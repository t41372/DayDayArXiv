{
  "date": "2025-12-22",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-12-22 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv å¯è°“æ˜¯ç¾¤æ˜Ÿç’€ç’¨ï¼Œ**æ¨ç†ï¼ˆReasoningï¼‰**å’Œ**æ™ºèƒ½ä½“ï¼ˆAgentsï¼‰**ä¾ç„¶æ˜¯ç»å¯¹çš„ä¸»è§’ã€‚æˆ‘ä»¬çœ‹åˆ°äº†å¯¹ Transformer æ ¸å¿ƒæœºåˆ¶çš„æŒ‘æˆ˜â€”â€”ä¸€ç¯‡æ ‡é¢˜æŒ‘è¡…çš„ \"Attention Is Not What You Need\" æå‡ºç”¨ Grassmann æµæ›¿ä»£æ³¨æ„åŠ›æœºåˆ¶ï¼›å…³äº GPT-5 å’Œ Gemini 3 Pro åœ¨æ•°å­¦æ¨ç†ä¸Šçš„è¡¨ç°ä¹Ÿå¼€å§‹å‡ºç°åœ¨è®ºæ–‡è¯„æµ‹ä¸­ã€‚æ­¤å¤–ï¼ŒAI for Science é¢†åŸŸæ¶Œç°äº†é’ˆå¯¹ç‰©ç†ã€åŒ–å­¦å’Œææ–™ç§‘å­¦çš„ä¸“ç”¨ Agent å’Œæ–°å‹ç½‘ç»œæ¶æ„ã€‚\n\nä¸‹é¢è®©æˆ‘ä»¬æ·±å…¥é˜…è¯»è¿™äº›æ ¸å¿ƒå·¥ä½œã€‚\n\n---\n\n### ğŸ”¥ å¿…è¯»ï¼šæ¶æ„åˆ›æ–°ä¸é‡ç£…æ¨¡å‹\n\n**1. Attention Is Not What You Need (æ³¨æ„åŠ›ä¸æ˜¯ä½ æ‰€éœ€è¦çš„)**\n> **Title:** Attention Is Not What You Need\n> **Authors:** Zhang Chong\n\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªéå¸¸å¤§èƒ†çš„æ ‡é¢˜ï¼Œç›´æ¥è‡´æ•¬å¹¶æŒ‘æˆ˜äº† Transformer çš„å¼€å±±ä¹‹ä½œã€‚ä½œè€…è´¨ç–‘æ˜¾å¼**è‡ªæ³¨æ„åŠ›ï¼ˆSelf-Attentionï¼‰**æ˜¯å¦æ˜¯åºåˆ—å»ºæ¨¡çš„å¿…éœ€å“ã€‚\n*   **æ–¹æ³•ä¸å‘ç°ï¼š** ä½œè€…æå‡ºäº†ä¸€ç§åŸºäº **Grassmann æµï¼ˆGrassmann flowsï¼‰** çš„æ— æ³¨æ„åŠ›æ¶æ„ã€‚è¯¥æ¨¡å‹ä¸è®¡ç®— $L \\times L$ çš„æ³¨æ„åŠ›çŸ©é˜µï¼Œè€Œæ˜¯å°† Token çŠ¶æ€æ˜ å°„åˆ° Grassmann æµå½¢ä¸Šçš„äºŒç»´å­ç©ºé—´ï¼Œé€šè¿‡å—æ§çš„å­ç©ºé—´å½¢å˜æ¥ä¼ æ’­ä¿¡æ¯ã€‚\n*   **Implicationï¼š** åœ¨ Wikitext-2 å’Œ SNLI ä»»åŠ¡ä¸Šï¼Œè¿™ç§çº¯ Grassmann æ¨¡å‹çš„æ€§èƒ½æ¥è¿‘ç”šè‡³åœ¨æŸäº›æŒ‡æ ‡ä¸Šç•¥å¾®è¶…è¿‡åŒç­‰è§„æ¨¡çš„ Transformerï¼Œä½†è®¡ç®—å¤æ‚åº¦åœ¨åºåˆ—é•¿åº¦ä¸Šæ˜¯çº¿æ€§çš„ã€‚è¿™ä¸ºç¥ç»ç½‘ç»œæ¨ç†æä¾›äº†ä¸€ç§æ›´å‡ ä½•åŒ–ã€æ›´å…·è§£é‡Šæ€§çš„æ–°è§†è§’ã€‚\n\n**2. Vibe Reasoning: æ¿€å‘å‰æ²¿ AI çš„æ•°å­¦æ½œèƒ½â€”â€”IMO 2025 ç¬¬ 6 é¢˜æ¡ˆä¾‹ç ”ç©¶**\n> **Title:** Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6\n> **Authors:** Jiaao Wu et al.\n\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªæå…¶å¼•äººæ³¨ç›®çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œæ–‡ä¸­æ˜ç¡®æåˆ°äº†ä½¿ç”¨ **GPT-5** å’Œ **Gemini 3 Pro**ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º \"Vibe Reasoning\" çš„äººæœºåä½œèŒƒå¼ã€‚\n*   **æ–¹æ³•ä¸å‘ç°ï¼š** ä½œè€…è®¤ä¸ºå‰æ²¿æ¨¡å‹ï¼ˆå¦‚ GPT-5ï¼‰å·²ç»å…·å¤‡è§£å†³åƒ IMOï¼ˆå›½é™…å¥¥æ•°ï¼‰2025 ç¬¬ 6 é¢˜è¿™æ ·æéš¾é—®é¢˜çš„çŸ¥è¯†ï¼Œä½†ç¼ºä¹åº”ç”¨çŸ¥è¯†çš„æ—¶æœºæ„Ÿã€‚é€šè¿‡å…ƒæç¤ºï¼ˆMeta-promptsï¼‰ã€Agent æ¥åœ°ï¼ˆAgentic Groundingï¼‰å’Œæ¨¡å‹ç¼–æ’ï¼Œç»“åˆ GPT-5 çš„æ¢ç´¢èƒ½åŠ›å’Œ Gemini 3 Pro çš„è¯æ˜èƒ½åŠ›ï¼Œç³»ç»ŸæˆåŠŸè§£å†³äº†è¯¥é—®é¢˜å¹¶ç»™å‡ºäº†ä¸¥è°¨è¯æ˜ã€‚\n*   **ç»“è®ºï¼š** é€‚å½“çš„è½»é‡çº§äººç±»å¼•å¯¼ï¼ˆVibeï¼‰å¯ä»¥è§£é”æ¨¡å‹çš„æ·±å±‚æ•°å­¦æ¨ç†æ½œåŠ›ã€‚\n\n**3. PHOTON: ç”¨äºå…‰é€Ÿå’Œå†…å­˜é«˜æ•ˆè¯­è¨€ç”Ÿæˆçš„å±‚çº§è‡ªå›å½’æ¨¡å‹**\n> **Title:** PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation\n> **Authors:** Yuma Ichikawa et al.\n\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹ Transformer æ¨ç†æ—¶çš„ KV-cache ç“¶é¢ˆï¼Œæå‡ºäº†ä¸€ç§æ–°çš„å±‚çº§è‡ªå›å½’æ¨¡å‹ **PHOTON**ã€‚\n*   **æ–¹æ³•ï¼š** å®ƒæ”¾å¼ƒäº†ä¼ ç»Ÿçš„æ°´å¹³é€ Token æ‰«æï¼Œæ”¹ç”¨**å‚ç›´ã€å¤šåˆ†è¾¨ç‡çš„ä¸Šä¸‹æ–‡æ‰«æ**ã€‚é€šè¿‡è‡ªåº•å‘ä¸Šçš„ç¼–ç å™¨å‹ç¼© Tokenï¼Œå†ç”¨è½»é‡çº§è‡ªé¡¶å‘ä¸‹çš„è§£ç å™¨å¹¶è¡Œé‡å»ºã€‚\n*   **æ•ˆæœï¼š** å®ç°äº†â€œé€’å½’ç”Ÿæˆâ€ï¼Œåªæ›´æ–°æœ€ç²—ç²’åº¦çš„æ½œåœ¨æµã€‚ç›¸æ¯” Transformerï¼Œåœ¨é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡ä¸­å®ç°äº†é«˜è¾¾ $1000\\times$ çš„å•ä½å†…å­˜ååé‡æå‡ã€‚\n\n---\n\n### ğŸ§  æ¨ç†ã€æ€ç»´é“¾ (CoT) ä¸å¤§æ¨¡å‹æœºç†\n\n**4. è®­ç»ƒå¤šæ¨¡æ€å¤§æ¨ç†æ¨¡å‹éœ€è¦æ›´å¥½çš„â€œæƒ³æ³•â€**\n> **Title:** Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection\n> **Authors:** Yizhi Wang et al.\n\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹å¤šæ¨¡æ€æ¨ç†æ¨¡å‹ï¼ˆMultimodal LRMsï¼‰ç¼ºä¹é«˜è´¨é‡é•¿æ€ç»´é“¾ï¼ˆLong CoTï¼‰æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº† **SynSelect** æ¡†æ¶ã€‚\n*   **æ–¹æ³•ï¼š** è¿™æ˜¯ä¸€ä¸ªâ€œåˆæˆ-é€‰æ‹©â€çš„ä¸‰é˜¶æ®µæ¡†æ¶ã€‚åˆ©ç”¨å¤šä¸ªå¼‚æ„å¤šæ¨¡æ€æ¨¡å‹ç”Ÿæˆå¤šæ ·åŒ–çš„ CoT å€™é€‰é¡¹ï¼Œç„¶åé€šè¿‡å®ä¾‹çº§å’Œ Batch çº§çš„é€‰æ‹©æœºåˆ¶è¿‡æ»¤å‡ºé«˜è´¨é‡æ•°æ®ã€‚\n*   **å‘ç°ï¼š** è¿™ç§æ–¹æ³•åˆæˆçš„æ•°æ®ä¸ä»…æå‡äº† SFT æ•ˆæœï¼Œè¿˜ä¸ºåç»­çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åè®­ç»ƒæ‰“ä¸‹äº†åšå®åŸºç¡€ã€‚\n\n**5. æ‹“æ‰‘æ•°æ®åˆ†æè§†è§’ä¸‹çš„æ€ç»´é“¾ (CoT)**\n> **Title:** Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis\n> **Authors:** Chenghao Li et al.\n\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** é¦–æ¬¡ä»**æ‹“æ‰‘ç»“æ„**çš„è§’åº¦åˆ†æ CoTã€‚\n*   **å‘ç°ï¼š** ä½¿ç”¨æ‹“æ‰‘æ•°æ®åˆ†æï¼ˆTDAï¼‰å°†æ¨ç†æ­¥éª¤æ˜ å°„åˆ°è¯­ä¹‰ç©ºé—´ã€‚ç ”ç©¶å‘ç°ï¼Œæ¨ç†é“¾çš„æ‹“æ‰‘å¤æ‚æ€§ä¸å‡†ç¡®æ€§å‘ˆæ­£ç›¸å…³ï¼›æˆåŠŸçš„æ¨ç†å¾€å¾€è¡¨ç°å‡ºæ›´ç®€å•çš„æ‹“æ‰‘ç»“æ„ï¼ˆæ›´å°‘çš„å†—ä½™å’Œå¾ªç¯ï¼‰ï¼Œè¿™ä¸ºè¯„ä¼° CoT è´¨é‡æä¾›äº†æ–°çš„æ•°å­¦å·¥å…·ã€‚\n\n**6. MixKVQ: é¢å‘é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„æŸ¥è¯¢æ„ŸçŸ¥æ··åˆç²¾åº¦ KV Cache é‡åŒ–**\n> **Title:** MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning\n> **Authors:** Tao Zhang et al.\n\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è§£å†³é•¿ CoT æ¨ç†å¸¦æ¥çš„å·¨å¤§æ˜¾å­˜å‹åŠ›ã€‚\n*   **æ–¹æ³•ï¼š** å‘ç°å¹¶éæ‰€æœ‰ KV Cache éƒ½åŒç­‰é‡è¦ã€‚MixKVQ å¼•å…¥äº†ä¸€ç§æŸ¥è¯¢æ„ŸçŸ¥ç®—æ³•ï¼Œè¯†åˆ«å¹¶ä¿ç•™å¯¹å½“å‰ Query é‡è¦çš„ Key é€šé“ï¼ˆä½¿ç”¨é«˜ç²¾åº¦ï¼‰ï¼Œè€Œå¯¹ Value Cache è¿›è¡Œé€ Token é‡åŒ–ã€‚åœ¨ä¿æŒå¤æ‚æ¨ç†æ€§èƒ½çš„åŒæ—¶æ˜¾è‘—é™ä½äº†å†…å­˜å ç”¨ã€‚\n\n---\n\n### ğŸ¤– Agentic AIï¼šæ™ºèƒ½ä½“ä¸ç³»ç»Ÿ\n\n**7. ç‰©ç†å­¦å®¶æ™ºèƒ½ä½“ PhysMaster**\n> **Title:** PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research\n> **Authors:** Tingjia Miao, Weinan E et al. (é„‚ç»´å—å›¢é˜Ÿ)\n\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æ„å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿè¿›è¡Œç†è®ºå’Œè®¡ç®—ç‰©ç†ç ”ç©¶çš„è‡ªä¸» AI ç‰©ç†å­¦å®¶ **PhysMaster**ã€‚\n*   **æ–¹æ³•ï¼š** ç»“åˆäº†æŠ½è±¡æ¨ç†å’Œæ•°å€¼è®¡ç®—èƒ½åŠ›ï¼Œå¹¶åˆ©ç”¨äº†ä¸€ä¸ªåä¸º LANDAU çš„åˆ†å±‚å­¦æœ¯æ•°æ®å®‡å®™ã€‚å®ƒå¯ä»¥è‡ªä¸»æ‰§è¡Œå‡è®¾é©±åŠ¨çš„é—­ç¯ç ”ç©¶ã€‚\n*   **æ•ˆæœï¼š** åœ¨é«˜èƒ½ç†è®ºã€å‡èšæ€ç†è®ºå’Œå¤©ä½“ç‰©ç†å­¦ä»»åŠ¡ä¸­ï¼Œå®ƒèƒ½å°†é•¿è¾¾æ•°æœˆçš„ç ”ç©¶å‹ç¼©åˆ°æ•°å°æ—¶ï¼Œç”šè‡³èƒ½ç‹¬ç«‹æ¢ç´¢å¼€æ”¾æ€§ç§‘å­¦é—®é¢˜ã€‚\n\n**8. Agent å·¥ä½œæµçš„å£°æ˜å¼è¯­è¨€**\n> **Title:** A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows\n> **Authors:** Ivan Daunis (PayPal)\n\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æ¥è‡ª PayPal çš„å®æˆ˜ç»éªŒã€‚æå‡ºäº†ä¸€ç§**å£°æ˜å¼ç³»ç»Ÿ**æ¥æ„å»º Agent å·¥ä½œæµï¼Œå°†ä¸šåŠ¡é€»è¾‘ä¸åº•å±‚ä»£ç è§£è€¦ã€‚\n*   **ä»·å€¼ï¼š** ç±»ä¼¼äº SQL æˆ– Terraformï¼Œå¼€å‘è€…åªéœ€å®šä¹‰â€œåšä»€ä¹ˆâ€è€Œä¸æ˜¯â€œæ€ä¹ˆåšâ€ã€‚åœ¨ PayPal çš„ç”µå•†åœºæ™¯ä¸­ï¼Œè¿™ç§æ–¹æ³•å‡å°‘äº† 60% çš„å¼€å‘æ—¶é—´ï¼Œå¹¶æ”¯æŒå¿«é€Ÿçš„ A/B æµ‹è¯•ã€‚\n\n**9. é’ˆå¯¹ä»£ç  Agent çš„åæ€é©±åŠ¨æ§åˆ¶**\n> **Title:** Reflection-Driven Control for Trustworthy Code Agents\n> **Authors:** Bin Wang et al.\n\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** ä¸ºäº†è§£å†³ä»£ç ç”Ÿæˆ Agent çš„å®‰å…¨æ€§é—®é¢˜ï¼Œæå‡ºäº†**Reflection-Driven Control (åæ€é©±åŠ¨æ§åˆ¶)**ã€‚\n*   **æ–¹æ³•ï¼š** å°†â€œè‡ªæˆ‘åæ€â€ä»ä¸€ç§äº‹åè¡¥ä¸æå‡ä¸º Agent æ¨ç†è¿‡ç¨‹ä¸­çš„æ˜¾å¼æ­¥éª¤ã€‚Agent ç»´æŠ¤ä¸€ä¸ªâ€œåæ€è®°å¿†åº“â€ï¼Œåœ¨ç”Ÿæˆä»£ç æ—¶æŒç»­ç›‘æ§æ½œåœ¨é£é™©å¹¶æ³¨å…¥å®‰å…¨çº¦æŸã€‚è¿™ä½¿å¾— Agent åœ¨ä¿æŒåŠŸèƒ½æ­£ç¡®æ€§çš„åŒæ—¶ï¼Œå®‰å…¨æ€§å¤§å¹…æå‡ã€‚\n\n---\n\n### ğŸ‘ï¸ è§†è§‰ä¸å¤šæ¨¡æ€\n\n**10. WorldWarp: é€šè¿‡å¼‚æ­¥è§†é¢‘æ‰©æ•£ä¼ æ’­ 3D å‡ ä½•**\n> **Title:** WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion\n> **Authors:** Hanyang Kong et al.\n\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è§£å†³äº†è§†é¢‘ç”Ÿæˆä¸­ 3D å‡ ä½•ä¸€è‡´æ€§çš„éš¾é¢˜ã€‚\n*   **æ–¹æ³•ï¼š** ç»“åˆäº† **3D é«˜æ–¯æ³¼æº… (3DGS)** ä½œä¸ºå‡ ä½•ç¼“å­˜å’Œ 2D è§†é¢‘æ‰©æ•£æ¨¡å‹ã€‚3DGS ä½œä¸ºç»“æ„æ”¯æ¶ï¼Œç¡®ä¿æ–°å¸§éµå¾ªå†å²å‡ ä½•ï¼›æ‰©æ•£æ¨¡å‹åˆ™è´Ÿè´£â€œå¡«å……å’Œä¿®æ­£â€å› é®æŒ¡äº§ç”Ÿçš„ç©ºæ´ã€‚è¿™æ˜¯ä¸€ç§â€œ3D é€»è¾‘å¼•å¯¼ç»“æ„ï¼Œæ‰©æ•£é€»è¾‘å®Œå–„çº¹ç†â€çš„æ··åˆæ–¹æ¡ˆã€‚\n\n**11. VehicleMAE-V2: ä»¥è½¦è¾†ä¸ºä¸­å¿ƒçš„æ„ŸçŸ¥å¤§æ¨¡å‹**\n> **Title:** Vehicle-centric Perception via Multimodal Structured Pre-training\n> **Authors:** Wentao Wu et al.\n\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹è½¦è¾†æ„ŸçŸ¥çš„é¢„è®­ç»ƒå¤§æ¨¡å‹ã€‚\n*   **æ–¹æ³•ï¼š** è®¾è®¡äº†å¯¹ç§°æ€§å¼•å¯¼ã€è½®å»“å¼•å¯¼å’Œè¯­ä¹‰å¼•å¯¼çš„æ©ç æ¨¡å—ï¼Œåˆ©ç”¨è½¦è¾†ç‰¹æœ‰çš„å…ˆéªŒçŸ¥è¯†æ¥æŒ‡å¯¼ Token é‡å»ºã€‚é…å¥—å‘å¸ƒäº† Autobot4M æ•°æ®é›†ï¼ˆ400ä¸‡å›¾åƒï¼‰ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ã€å¯¹é½ä¸ç†è®º\n\n**12. é€šè¿‡è¡Œä¸ºæ ¡å‡†å¼ºåŒ–å­¦ä¹ ç¼“è§£ LLM å¹»è§‰**\n> **Title:** Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning\n> **Authors:** Jiayun Wu et al.\n\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** è®¤ä¸ºå¹»è§‰æ˜¯æ¨¡å‹ä¸ºäº†è¿åˆè®­ç»ƒç›®æ ‡è€Œâ€œä¸æ‡‚è£…æ‡‚â€çš„ç»“æœã€‚\n*   **æ–¹æ³•ï¼š** æå‡ºè¡Œä¸ºæ ¡å‡†ï¼ˆBehavioral Calibrationï¼‰ï¼Œåˆ©ç”¨ä¸¥æ ¼è¯„åˆ†è§„åˆ™ï¼ˆStrictly Proper Scoring Rulesï¼‰è®­ç»ƒæ¨¡å‹åœ¨ä¸ç¡®å®šæ—¶**æ‹’ç»å›ç­”**æˆ–æ ‡è®°ä¸ç¡®å®šæ€§ã€‚\n*   **æ•ˆæœï¼š** åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šï¼Œæ ¡å‡†åçš„ 4B æ¨¡å‹åœ¨â€œå‡†ç¡®ç‡-å¹»è§‰æ¯”â€ä¸Šç”šè‡³è¶…è¿‡äº† GPT-5ï¼ˆæ³¨ï¼šæ­¤å¤„ä¹Ÿæ˜¯ä¸ GPT-5 çš„å¯¹æ¯”è¯„ä¼°ï¼‰ã€‚\n\n**13. æŠ¹é™¤çš„é”™è§‰ï¼šLLM é—å¿˜è¯„ä¼°çš„å‹åŠ›æµ‹è¯•**\n> **Title:** The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation\n> **Authors:** Hengrui Jia et al.\n\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** æŒ‡å‡ºå½“å‰çš„â€œæœºå™¨é—å¿˜ï¼ˆUnlearningï¼‰â€è¯„ä¼°å­˜åœ¨ä¸¥é‡ç¼ºé™·ã€‚\n*   **å‘ç°ï¼š** æ¨¡å‹å¯èƒ½åœ¨ç‰¹å®šçš„é—å¿˜æ•°æ®é›† $D_u$ ä¸Šè¡¨ç°å‡ºâ€œå·²é—å¿˜â€ï¼Œä½†å¯¹äºè¯­ä¹‰ç›¸é‚»çš„ä»£ç†æ•°æ®é›† $\\tilde{D}_u$ ä»ç„¶ä¿ç•™äº†çŸ¥è¯†ã€‚è¿™è¡¨æ˜ç›®å‰çš„é—å¿˜æŠ€æœ¯å¯èƒ½åªæ˜¯è¡¨é¢ä¸Šæ©ç›–äº†è®°å¿†ï¼Œè€ŒéçœŸæ­£åˆ é™¤äº†çŸ¥è¯†ã€‚\n\n**14. MÃ¼ntz-SzÃ¡sz ç½‘ç»œï¼šå…·æœ‰å¯å­¦ä¹ å¹‚å¾‹åŸºçš„ç¥ç»æ¶æ„**\n> **Title:** MÃ¼ntz-SzÃ¡sz Networks: Neural Architectures with Learnable Power-Law Bases\n> **Authors:** Gnankan Landry Regis N'guessan\n\n*   **æ ¸å¿ƒè´¡çŒ®ï¼š** é’ˆå¯¹ç§‘å­¦è®¡ç®—ä¸­çš„å¥‡ç‚¹å’Œåˆ†æ•°å¹‚è¡Œä¸ºï¼Œæå‡ºäº†ä¸€ç§æ–°å‹ç¥ç»ç½‘ç»œæ¶æ„ **MSN**ã€‚\n*   **æ–¹æ³•ï¼š** ä¾æ® MÃ¼ntz-SzÃ¡sz å®šç†ï¼Œå°†å›ºå®šçš„æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ ReLUï¼‰æ›¿æ¢ä¸ºå¯å­¦ä¹ çš„åˆ†æ•°å¹‚åŸºã€‚\n*   **æ•ˆæœï¼š** åœ¨å¤„ç†å…·æœ‰å¥‡ç‚¹çš„ç‰©ç†æ–¹ç¨‹ï¼ˆå¦‚ PINNs åŸºå‡†ï¼‰æ—¶ï¼Œè¯¯å·®æ¯” MLP ä½ 5-8 å€ï¼Œä¸”å‚æ•°é‡æ›´å°‘ã€‚\n\n---\n\n### âš¡ å…¶ä»–å€¼å¾—å…³æ³¨çš„è®ºæ–‡\n\n*   **[Financial ML]** **Conditional Adversarial Fragility in Financial Machine Learning**: æŒ‡å‡ºé‡‘è ML æ¨¡å‹åœ¨å®è§‚ç»æµå‹åŠ›ä¸‹ï¼ˆå¦‚å¸‚åœºåŠ¨è¡æœŸï¼‰å¯¹å¯¹æŠ—æ”»å‡»çš„è„†å¼±æ€§ä¼šç³»ç»Ÿæ€§æ”¾å¤§ã€‚\n*   **[Benchmarks]** **QuantiPhy**: ç¬¬ä¸€ä¸ªå®šé‡è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ç‰©ç†æ¨ç†èƒ½åŠ›ï¼ˆå¦‚é€Ÿåº¦ã€åŠ é€Ÿåº¦æ•°å€¼ä¼°è®¡ï¼‰çš„åŸºå‡†ã€‚å‘ç° VLM ç›®å‰ä¸»è¦é â€œå¸¸è¯†â€è€Œéè§†è§‰è§‚å¯Ÿæ¥æ¨ç†ç‰©ç†é‡ã€‚\n*   **[Bio/Medical]** **R-GenIMA**: ä¸€ä¸ªå¯è§£é‡Šçš„å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œç»“åˆè„‘å½±åƒï¼ˆMRIï¼‰å’ŒåŸºå› æ•°æ®ï¼ˆSNPsï¼‰æ¥é¢„æµ‹é˜¿å°”èŒ¨æµ·é»˜ç—…çš„è¿›å±•ã€‚\n*   **[Neuroscience]** **Brain-Grounded Axes**: å¹¶éä½¿ç”¨æ–‡æœ¬ç›‘ç£ï¼Œè€Œæ˜¯ä½¿ç”¨äººç±»å¤§è„‘æ´»åŠ¨ï¼ˆMEGä¿¡å·ï¼‰ä½œä¸ºåæ ‡ç³»æ¥è§£è¯»å’Œæ“æ§ LLM çš„å†…éƒ¨çŠ¶æ€ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸å‰æ²¿çš„è„‘æœºæ¥å£ä¸å¤§æ¨¡å‹ç»“åˆçš„æ¢ç´¢ã€‚\n\n---\n**ç»“è¯­ï¼š**\nä»Šå¤©çš„è®ºæ–‡åæ˜ å‡º AI ç ”ç©¶æ­£ä»â€œåˆ·æ¦œâ€è½¬å‘æ›´æ·±å±‚æ¬¡çš„æœºç†æ¢ç´¢ï¼ˆå¦‚ Attention çš„æ›¿ä»£å“ã€CoT çš„æ‹“æ‰‘ç»“æ„ï¼‰ä»¥åŠæ›´åŠ¡å®çš„ç³»ç»Ÿè½åœ°ï¼ˆå¦‚ Agent æ²»ç†ã€å£°æ˜å¼å·¥ä½œæµï¼‰ã€‚åŒæ—¶ï¼ŒGPT-5 çš„èº«å½±å¼€å§‹åœ¨å¯¹æ¯”å®éªŒä¸­éšç°ï¼Œé¢„ç¤ºç€æ–°ä¸€ä»£å‰æ²¿æ¨¡å‹çš„è„šæ­¥è¿‘äº†ã€‚\n\nå¸Œæœ›è¿™ä»½å¿«æŠ¥èƒ½ä¸ºä½ ä»Šå¤©çš„ç§‘ç ”å·¥ä½œæä¾›çµæ„Ÿï¼æˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2512.19935v1",
      "title": "Conditional Adversarial Fragility in Financial Machine Learning under Macroeconomic Stress",
      "title_zh": "å®è§‚ç»æµå‹åŠ›ä¸‹é‡‘èæœºå™¨å­¦ä¹ ä¸­çš„æ¡ä»¶å¯¹æŠ—æ€§è„†å¼±æ€§",
      "authors": [
        "Samruddhi Baviskar"
      ],
      "abstract": "Machine learning models used in financial decision systems operate in nonstationary economic environments, yet adversarial robustness is typically evaluated under static assumptions. This work introduces Conditional Adversarial Fragility, a regime dependent phenomenon in which adversarial vulnerability is systematically amplified during periods of macroeconomic stress. We propose a regime aware evaluation framework for time indexed tabular financial classification tasks that conditions robustness assessment on external indicators of economic stress. Using volatility based regime segmentation as a proxy for macroeconomic conditions, we evaluate model behavior across calm and stress periods while holding model architecture, attack methodology, and evaluation protocols constant. Baseline predictive performance remains comparable across regimes, indicating that economic stress alone does not induce inherent performance degradation. Under adversarial perturbations, however, models operating during stress regimes exhibit substantially greater degradation across predictive accuracy, operational decision thresholds, and risk sensitive outcomes. We further demonstrate that this amplification propagates to increased false negative rates, elevating the risk of missed high risk cases during adverse conditions. To complement numerical robustness metrics, we introduce an interpretive governance layer based on semantic auditing of model explanations using large language models. Together, these results demonstrate that adversarial robustness in financial machine learning is a regime dependent property and motivate stress aware approaches to model risk assessment in high stakes financial deployments.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†é‡‘èæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨éå¹³ç¨³ç»æµç¯å¢ƒä¸‹çš„è„†å¼±æ€§ï¼Œå¹¶æå‡ºäº†â€œæ¡ä»¶å¯¹æŠ—è„†å¼±æ€§â€(Conditional Adversarial Fragility)çš„æ¦‚å¿µï¼Œæ­ç¤ºäº†å¯¹æŠ—æ€§æ”»å‡»çš„æ˜“æ„Ÿæ€§åœ¨å®è§‚ç»æµå‹åŠ›æ—¶æœŸä¼šç³»ç»Ÿæ€§æ”¾å¤§ã€‚ä½œè€…å¼€å‘äº†ä¸€ä¸ªé’ˆå¯¹æ—¶é—´ç´¢å¼•è¡¨æ ¼å¼é‡‘èåˆ†ç±»ä»»åŠ¡çš„åˆ¶åº¦æ„ŸçŸ¥è¯„ä¼°æ¡†æ¶(regime aware evaluation framework)ï¼Œåˆ©ç”¨åŸºäºæ³¢åŠ¨ç‡(volatility)çš„åˆ¶åº¦åˆ†æ®µæ¥å¯¹æ¯”æ¨¡å‹åœ¨å†·é™æœŸä¸å‹åŠ›æœŸçš„è¡¨ç°ã€‚å®éªŒè¡¨æ˜ï¼Œå°½ç®¡æ¨¡å‹åœ¨ä¸åŒç»æµä½“åˆ¶ä¸‹çš„åŸºå‡†é¢„æµ‹æ€§èƒ½ä¿æŒç¨³å®šï¼Œä½†åœ¨é­å—å¯¹æŠ—æ€§æ‰°åŠ¨(adversarial perturbations)æ—¶ï¼Œå‹åŠ›ä½“åˆ¶ä¸‹çš„æ¨¡å‹åœ¨é¢„æµ‹å‡†ç¡®ç‡å’Œé£é™©æ•æ„Ÿç»“æœæ–¹é¢è¡¨ç°å‡ºæ›´æ˜¾è‘—çš„é€€åŒ–ã€‚è¿™ç§è„†å¼±æ€§çš„æ”¾å¤§è¿›ä¸€æ­¥å¯¼è‡´å‡é˜´æ€§ç‡(false negative rates)ä¸Šå‡ï¼Œå¢åŠ äº†åœ¨ä¸åˆ©æ¡ä»¶ä¸‹é—æ¼é«˜é£é™©æ¡ˆä¾‹çš„é£é™©ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œè¯­ä¹‰å®¡è®¡(semantic auditing)çš„è§£é‡Šæ€§æ²»ç†å±‚ã€‚è¿™äº›å‘ç°è¯æ˜äº†é‡‘èæœºå™¨å­¦ä¹ çš„å¯¹æŠ—ç¨³å¥æ€§å…·æœ‰åˆ¶åº¦ä¾èµ–å±æ€§(regime dependent property)ï¼Œä¸ºé«˜é£é™©é‡‘èéƒ¨ç½²ä¸­çš„å‹åŠ›æ„ŸçŸ¥é£é™©è¯„ä¼°æä¾›äº†é‡è¦ä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19935v1",
      "published_date": "2025-12-22 23:44:39 UTC",
      "updated_date": "2025-12-22 23:44:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:01:27.316629+00:00"
    },
    {
      "arxiv_id": "2512.19934v1",
      "title": "Vehicle-centric Perception via Multimodal Structured Pre-training",
      "title_zh": "åŸºäºå¤šæ¨¡æ€ç»“æ„åŒ–é¢„è®­ç»ƒçš„ä»¥è½¦è¾†ä¸ºä¸­å¿ƒçš„æ„ŸçŸ¥",
      "authors": [
        "Wentao Wu",
        "Xiao Wang",
        "Chenglong Li",
        "Jin Tang",
        "Bin Luo"
      ],
      "abstract": "Vehicle-centric perception plays a crucial role in many intelligent systems, including large-scale surveillance systems, intelligent transportation, and autonomous driving. Existing approaches lack effective learning of vehicle-related knowledge during pre-training, resulting in poor capability for modeling general vehicle perception representations. To handle this problem, we propose VehicleMAE-V2, a novel vehicle-centric pre-trained large model. By exploring and exploiting vehicle-related multimodal structured priors to guide the masked token reconstruction process, our approach can significantly enhance the model's capability to learn generalizable representations for vehicle-centric perception. Specifically, we design the Symmetry-guided Mask Module (SMM), Contour-guided Representation Module (CRM) and Semantics-guided Representation Module (SRM) to incorporate three kinds of structured priors into token reconstruction including symmetry, contour and semantics of vehicles respectively. SMM utilizes the vehicle symmetry constraints to avoid retaining symmetric patches and can thus select high-quality masked image patches and reduce information redundancy. CRM minimizes the probability distribution divergence between contour features and reconstructed features and can thus preserve holistic vehicle structure information during pixel-level reconstruction. SRM aligns image-text features through contrastive learning and cross-modal distillation to address the feature confusion caused by insufficient semantic understanding during masked reconstruction. To support the pre-training of VehicleMAE-V2, we construct Autobot4M, a large-scale dataset comprising approximately 4 million vehicle images and 12,693 text descriptions. Extensive experiments on five downstream tasks demonstrate the superior performance of VehicleMAE-V2.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰é¢„è®­ç»ƒæ¨¡å‹åœ¨è½¦è¾†æ„ŸçŸ¥é¢†åŸŸç¼ºä¹æœ‰æ•ˆå…ˆéªŒçŸ¥è¯†çš„é—®é¢˜ï¼Œæå‡ºäº†VehicleMAE-V2ï¼Œä¸€ç§åˆ©ç”¨å¤šæ¨¡æ€ç»“æ„åŒ–å…ˆéªŒæŒ‡å¯¼é®è”½æ ‡è®°é‡å»ºï¼ˆmasked token reconstructionï¼‰çš„è½¦è¾†ä¸­å¿ƒåŒ–é¢„è®­ç»ƒå¤§æ¨¡å‹ã€‚ä¸ºäº†å¢å¼ºæ¨¡å‹å¯¹è½¦è¾†é€šç”¨è¡¨ç¤ºçš„å­¦ä¹ èƒ½åŠ›ï¼Œç ”ç©¶è€…è®¾è®¡äº†Symmetry-guided Mask Module (SMM)ã€Contour-guided Representation Module (CRM) å’Œ Semantics-guided Representation Module (SRM)ï¼Œåˆ†åˆ«å¼•å…¥è½¦è¾†çš„å¯¹ç§°æ€§ã€è½®å»“å’Œè¯­ä¹‰å…ˆéªŒä¿¡æ¯ã€‚å…¶ä¸­ SMM é€šè¿‡å¯¹ç§°çº¦æŸå‡å°‘ä¿¡æ¯å†—ä½™å¹¶ç­›é€‰é«˜è´¨é‡é®è”½å—ï¼ŒCRM ç¡®ä¿åœ¨åƒç´ çº§é‡å»ºä¸­ä¿ç•™æ•´ä½“ç»“æ„ä¿¡æ¯ï¼Œè€Œ SRM åˆ™é€šè¿‡å¯¹æ¯”å­¦ä¹ å’Œè·¨æ¨¡æ€è’¸é¦è§£å†³è¯­ä¹‰ç†è§£ä¸è¶³å¯¼è‡´çš„ç‰¹å¾æ··æ·†ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†åŒ…å«çº¦400ä¸‡å¼ è½¦è¾†å›¾åƒçš„å¤§è§„æ¨¡æ•°æ®é›† Autobot4M ä»¥æ”¯æŒæ¨¡å‹é¢„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒVehicleMAE-V2 åœ¨äº”é¡¹å…³é”®ä¸‹æ¸¸ä»»åŠ¡ä¸­å‡è¡¨ç°å‡ºä¼˜å¼‚çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†è½¦è¾†ä¸­å¿ƒåŒ–æ„ŸçŸ¥ä»»åŠ¡æ–¹é¢çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Journal extension of VehicleMAE (AAAI 2024)",
      "pdf_url": "https://arxiv.org/pdf/2512.19934v1",
      "published_date": "2025-12-22 23:42:45 UTC",
      "updated_date": "2025-12-22 23:42:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:00:32.177238+00:00"
    },
    {
      "arxiv_id": "2601.00818v1",
      "title": "Agentic AI for Autonomous, Explainable, and Real-Time Credit Risk Decision-Making",
      "title_zh": "é¢å‘è‡ªä¸»ã€å¯è§£é‡Šä¸å®æ—¶ä¿¡è´·é£é™©å†³ç­–çš„æ™ºèƒ½ä½“ AI",
      "authors": [
        "Chandra Sekhar Kubam"
      ],
      "abstract": "Significant digitalization of financial services in a short period of time has led to an urgent demand to have autonomous, transparent and real-time credit risk decision making systems. The traditional machine learning models are effective in pattern recognition, but do not have the adaptive reasoning, situational awareness, and autonomy needed in modern financial operations. As a proposal, this paper presents an Agentic AI framework, or a system where AI agents view the world of dynamic credit independent of human observers, who then make actions based on their articulable decision-making paths. The research introduces a multi-agent system with reinforcing learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines as a means of assessing the risk profiles of borrowers with few humans being involved. The processes consist of agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles. Findings indicate that decision speed, transparency and responsiveness is better than traditional credit scoring models. Nevertheless, there are still some practical limitations such as risks of model drift, inconsistencies in interpreting high dimensional data and regulatory uncertainties as well as infrastructure limitations in low-resource settings. The suggested system has a high prospective to transform credit analytics and future studies ought to be directed on dynamic regulatory compliance mobilizers, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€ä¸ªAgentic AIæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæœºå™¨å­¦ä¹ åœ¨ç°ä»£é‡‘èä¿¡ç”¨é£é™©å†³ç­–ä¸­ç¼ºä¹è‡ªé€‚åº”æ¨ç†ã€æƒ…å¢ƒæ„ŸçŸ¥å’Œè‡ªä¸»æ€§çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿåˆ©ç”¨åŒ…å«å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)ã€è‡ªç„¶è¯­è¨€æ¨ç†(Natural Language Reasoning)å’Œå¯è§£é‡ŠAI (Explainable AI)æ¨¡å—çš„å¤šæ™ºèƒ½ä½“æ¶æ„ï¼Œå®ç°äº†å¯¹å€Ÿæ¬¾äººé£é™©ç‰¹å¾çš„è‡ªä¸»è¯„ä¼°ã€‚é€šè¿‡é›†æˆå®æ—¶æ•°æ®å¸æ”¶ç®¡é“ã€æ™ºèƒ½ä½“åä½œåè®®å’ŒæŒç»­åé¦ˆå­¦ä¹ å¾ªç¯ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç‹¬ç«‹ç”Ÿæˆå¯é˜è¿°çš„å†³ç­–è·¯å¾„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å†³ç­–é€Ÿåº¦ã€é€æ˜åº¦å’Œå“åº”æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ä¿¡ç”¨è¯„åˆ†æ¨¡å‹ã€‚å°½ç®¡ç›®å‰ä»é¢ä¸´æ¨¡å‹æ¼‚ç§»(Model Drift)ã€é«˜ç»´æ•°æ®å¤„ç†çš„ä¸€è‡´æ€§ä»¥åŠç›‘ç®¡ä¸ç¡®å®šæ€§ç­‰æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶ä¸ºå®ç°è‡ªåŠ¨åŒ–ã€é€æ˜ä¸”å®æ—¶çš„ä¿¡ç”¨åˆ†ææä¾›äº†åˆ›æ–°æ–¹æ¡ˆã€‚æœªæ¥ç ”ç©¶å°†è¿›ä¸€æ­¥æ¢ç´¢ç³»ç»Ÿçš„å¯¹æŠ—é²æ£’æ€§ä»¥åŠåœ¨è·¨å›½ä¿¡ç”¨ç”Ÿæ€ç³»ç»Ÿä¸­çš„å¤§è§„æ¨¡åº”ç”¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.00818v1",
      "published_date": "2025-12-22 23:30:38 UTC",
      "updated_date": "2025-12-22 23:30:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:01:41.592398+00:00"
    },
    {
      "arxiv_id": "2512.19928v1",
      "title": "Unified Brain Surface and Volume Registration",
      "title_zh": "ç»Ÿä¸€çš„è„‘è¡¨é¢ä¸ä½“ç§¯é…å‡†",
      "authors": [
        "S. Mazdak Abulnaga",
        "Andrew Hoopes",
        "Malte Hoffmann",
        "Robin Magnet",
        "Maks Ovsjanikov",
        "Lilla ZÃ¶llei",
        "John Guttag",
        "Bruce Fischl",
        "Adrian Dalca"
      ],
      "abstract": "Accurate registration of brain MRI scans is fundamental for cross-subject analysis in neuroscientific studies. This involves aligning both the cortical surface of the brain and the interior volume. Traditional methods treat volumetric and surface-based registration separately, which often leads to inconsistencies that limit downstream analyses. We propose a deep learning framework, NeurAlign, that registers $3$D brain MRI images by jointly aligning both cortical and subcortical regions through a unified volume-and-surface-based representation. Our approach leverages an intermediate spherical coordinate space to bridge anatomical surface topology with volumetric anatomy, enabling consistent and anatomically accurate alignment. By integrating spherical registration into the learning, our method ensures geometric coherence between volume and surface domains. In a series of experiments on both in-domain and out-of-domain datasets, our method consistently outperforms both classical and machine learning-based registration methods -- improving the Dice score by up to 7 points while maintaining regular deformation fields. Additionally, it is orders of magnitude faster than the standard method for this task, and is simpler to use because it requires no additional inputs beyond an MRI scan. With its superior accuracy, fast inference, and ease of use, NeurAlign sets a new standard for joint cortical and subcortical registration.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† NeurAlignï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨åŒæ—¶å®ç°å¤§è„‘çš®å±‚è¡¨é¢ (cortical surface) å’Œå†…éƒ¨å®¹ç§¯ (interior volume) é…å‡†çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œè§£å†³äº†ä¼ ç»Ÿæ–¹æ³•ä¸­ä¸¤è€…åˆ†ç¦»å¯¼è‡´çš„ä¸€è‡´æ€§é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä¸­é—´çƒé¢åæ ‡ç©ºé—´ (intermediate spherical coordinate space) æ¡¥æ¥è§£å‰–è¡¨é¢æ‹“æ‰‘ä¸å®¹ç§¯è§£å‰–ç»“æ„ï¼Œé€šè¿‡å°†çƒé¢é…å‡†é›†æˆåˆ°å­¦ä¹ ä¸­ï¼Œç¡®ä¿äº†ä¸åŒé¢†åŸŸé—´çš„å‡ ä½•ç›¸å¹²æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒNeurAlign åœ¨åŸŸå†…å’Œè·¨åŸŸæ•°æ®é›†ä¸Šå‡è¡¨ç°ä¼˜å¼‚ï¼ŒDice score æå‡äº†å¤šè¾¾ 7 ä¸ªç™¾åˆ†ç‚¹ï¼Œä¸”èƒ½ä¿æŒè§„åˆ™çš„å˜å½¢åœºã€‚ç›¸æ¯”äºæ ‡å‡†æ–¹æ³•ï¼Œè¯¥æ¨¡å‹åœ¨æ¨ç†é€Ÿåº¦ä¸Šæå‡äº†å‡ ä¸ªæ•°é‡çº§ï¼Œä¸”ä»…éœ€ MRI æ‰«æä½œä¸ºè¾“å…¥ï¼Œæ˜¾è‘—ç®€åŒ–äº†ä½¿ç”¨æµç¨‹ã€‚å‡­å€Ÿå…¶é«˜ç²¾åº¦ã€å¿«é€Ÿæ¨ç†å’Œæ˜“ç”¨æ€§ï¼ŒNeurAlign ä¸ºçš®å±‚ä¸çš®å±‚ä¸‹åŒºåŸŸçš„è”åˆé…å‡†ç¡®ç«‹äº†æ–°çš„æŠ€æœ¯æ ‡å‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19928v1",
      "published_date": "2025-12-22 23:05:26 UTC",
      "updated_date": "2025-12-22 23:05:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:00:38.640068+00:00"
    },
    {
      "arxiv_id": "2512.22222v3",
      "title": "MÃ¼ntz-SzÃ¡sz Networks: Neural Architectures with Learnable Power-Law Bases",
      "title_zh": "MÃ¼ntz-SzÃ¡sz ç½‘ç»œï¼šå…·æœ‰å¯å­¦ä¹ å¹‚å¾‹åŸºåº•çš„ç¥ç»æ¶æ„",
      "authors": [
        "Gnankan Landry Regis N'guessan"
      ],
      "abstract": "Standard neural network architectures employ fixed activation functions (ReLU, tanh, sigmoid) that are poorly suited for approximating functions with singular or fractional power behavior, a structure that arises ubiquitously in physics, including boundary layers, fracture mechanics, and corner singularities. We introduce MÃ¼ntz-SzÃ¡sz Networks (MSN), a novel architecture that replaces fixed smooth activations with learnable fractional power bases grounded in classical approximation theory. Each MSN edge computes $Ï†(x) = \\sum_k a_k |x|^{Î¼_k} + \\sum_k b_k \\mathrm{sign}(x)|x|^{Î»_k}$, where the exponents $\\{Î¼_k, Î»_k\\}$ are learned alongside the coefficients. We prove that MSN inherits universal approximation from the MÃ¼ntz-SzÃ¡sz theorem and establish novel approximation rates: for functions of the form $|x|^Î±$, MSN achieves error $\\mathcal{O}(|Î¼- Î±|^2)$ with a single learned exponent, whereas standard MLPs require $\\mathcal{O}(Îµ^{-1/Î±})$ neurons for comparable accuracy. On supervised regression with singular target functions, MSN achieves 5-8x lower error than MLPs with 10x fewer parameters. Physics-informed neural networks (PINNs) represent a particularly demanding application for singular function approximation; on PINN benchmarks including a singular ODE and stiff boundary-layer problems, MSN achieves 3-6x improvement while learning interpretable exponents that match the known solution structure. Our results demonstrate that theory-guided architectural design can yield dramatic improvements for scientifically-motivated function classes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ ‡å‡†ç¥ç»ç½‘ç»œéš¾ä»¥æœ‰æ•ˆè¿‘ä¼¼ç‰©ç†å­¦ä¸­å¸¸è§çš„å¥‡å¼‚æˆ–åˆ†æ•°å¹‚è¡Œä¸ºï¼ˆå¦‚è¾¹ç•Œå±‚ã€æ–­è£‚åŠ›å­¦å’Œè§’ç‚¹å¥‡å¼‚æ€§ï¼‰çš„é—®é¢˜ï¼Œæå‡ºäº† MÃ¼ntz-SzÃ¡sz Networks (MSN)ã€‚MSN é‡‡ç”¨åŸºäºç»å…¸é€¼è¿‘ç†è®ºçš„å¯å­¦ä¹ åˆ†æ•°å¹‚åŸºå‡½æ•°å–ä»£å›ºå®šçš„å¹³æ»‘æ¿€æ´»å‡½æ•°ï¼Œé€šè¿‡åœ¨ç½‘ç»œè¾¹ç¼˜å­¦ä¹ æŒ‡æ•°é¡¹ $\\mu_k$ å’Œ $\\lambda_k$ï¼Œä½¿å…¶èƒ½æ›´ç²¾å‡†åœ°æ•æ‰å‡½æ•°ç‰¹å¾ã€‚ç†è®ºè¯æ˜ MSN ç»§æ‰¿äº† MÃ¼ntz-SzÃ¡sz theorem çš„é€šç”¨é€¼è¿‘ç‰¹æ€§ï¼Œä¸”åœ¨å¤„ç† $|x|^\\alpha$ ç±»å‹å‡½æ•°æ—¶æ¯”ä¼ ç»Ÿ MLP å…·æœ‰æ˜¾è‘—æ›´é«˜çš„é€¼è¿‘é€Ÿç‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨å…·æœ‰å¥‡å¼‚æ€§çš„ç›‘ç£å›å½’ä»»åŠ¡ä¸­ï¼ŒMSN ä»¥ä»…ä¸º MLP ååˆ†ä¹‹ä¸€çš„å‚æ•°é‡å®ç°äº†ä½ 5-8 å€çš„è¯¯å·®ã€‚åœ¨ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (PINNs) çš„åŸºå‡†æµ‹è¯•ä¸­ï¼ŒMSN åœ¨å¤„ç†å¥‡å¼‚å¸¸å¾®åˆ†æ–¹ç¨‹å’Œé™¡å³­è¾¹ç•Œå±‚é—®é¢˜æ—¶ç²¾åº¦æå‡äº† 3-6 å€ï¼Œå¹¶èƒ½å­¦ä¹ åˆ°ä¸å·²çŸ¥ç‰©ç†è§£ç»“æ„ä¸€è‡´çš„å¯è§£é‡ŠæŒ‡æ•°ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡ç†è®ºå¯¼å‘çš„æ¶æ„è®¾è®¡ï¼Œå¯ä»¥å¤§å¹…æå‡ç¥ç»ç½‘ç»œåœ¨ç§‘å­¦è®¡ç®—é¢†åŸŸçš„å»ºæ¨¡æ•ˆèƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "V3: Corrected Full MÃ¼ntz Theorem (added constant function), fixed L2 projection error formula, clarified MLP bounds in terms of linear pieces. Acknowledgments added. Full code at https://github.com/ReFractals/muntz-szasz-networks",
      "pdf_url": "https://arxiv.org/pdf/2512.22222v3",
      "published_date": "2025-12-22 23:04:18 UTC",
      "updated_date": "2026-01-20 09:31:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:00:46.760537+00:00"
    },
    {
      "arxiv_id": "2512.19920v2",
      "title": "Mitigating LLM Hallucination via Behaviorally Calibrated Reinforcement Learning",
      "title_zh": "é€šè¿‡è¡Œä¸ºæ ¡å‡†å¼ºåŒ–å­¦ä¹ ç¼“è§£å¤§è¯­è¨€æ¨¡å‹å¹»è§‰",
      "authors": [
        "Jiayun Wu",
        "Jiashuo Liu",
        "Zhiyuan Zeng",
        "Tianyang Zhan",
        "Tianle Cai",
        "Wenhao Huang"
      ],
      "abstract": "LLM deployment in critical domains is currently impeded by persistent hallucinations--generating plausible but factually incorrect assertions. While scaling laws drove significant improvements in general capabilities, theoretical frameworks suggest hallucination is not merely stochastic error but a predictable statistical consequence of training objectives prioritizing mimicking data distribution over epistemic honesty. Standard RLVR paradigms, utilizing binary reward signals, inadvertently incentivize models as good test-takers rather than honest communicators, encouraging guessing whenever correctness probability exceeds zero. This paper presents an exhaustive investigation into behavioral calibration, which incentivizes models to stochastically admit uncertainty by abstaining when not confident, aligning model behavior with accuracy. Synthesizing recent advances, we propose and evaluate training interventions optimizing strictly proper scoring rules for models to output a calibrated probability of correctness. Our methods enable models to either abstain from producing a complete response or flag individual claims where uncertainty remains. Utilizing Qwen3-4B-Instruct, empirical analysis reveals behavior-calibrated reinforcement learning allows smaller models to surpass frontier models in uncertainty quantification--a transferable meta-skill decouplable from raw predictive accuracy. Trained on math reasoning tasks, our model's log-scale Accuracy-to-Hallucination Ratio gain (0.806) exceeds GPT-5's (0.207) in a challenging in-domain evaluation (BeyondAIME). Moreover, in cross-domain factual QA (SimpleQA), our 4B LLM achieves zero-shot calibration error on par with frontier models including Grok-4 and Gemini-2.5-Pro, even though its factual accuracy is much lower.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å…³é”®é¢†åŸŸåº”ç”¨ä¸­é¢ä¸´çš„å¹»è§‰ï¼ˆhallucinationï¼‰é—®é¢˜ï¼ŒæŒ‡å‡ºå…¶æ ¹æºåœ¨äºè®­ç»ƒç›®æ ‡ä¼˜å…ˆè€ƒè™‘æ¨¡ä»¿æ•°æ®åˆ†å¸ƒè€Œéè®¤è¯†è®ºè¯šå®ã€‚ä¸ºäº†ç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§è¡Œä¸ºæ ¡å‡†å¼ºåŒ–å­¦ä¹ ï¼ˆBehaviorally Calibrated Reinforcement Learningï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨é€šè¿‡æ¿€åŠ±æ¨¡å‹åœ¨ä¸ç¡®å®šæ—¶é€‰æ‹©å¼ƒæƒæˆ–æ ‡è®°ä¸ç¡®å®šé¡¹ï¼Œä½¿æ¨¡å‹è¡Œä¸ºä¸å…¶å®é™…å‡†ç¡®æ€§å¯¹é½ã€‚ç ”ç©¶é€šè¿‡ä¼˜åŒ–ä¸¥æ ¼æ­£å½“è¯„åˆ†è§„åˆ™ï¼ˆstrictly proper scoring rulesï¼‰ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿè¾“å‡ºç»è¿‡æ ¡å‡†çš„æ­£ç¡®æ€§æ¦‚ç‡ã€‚å®éªŒé‡‡ç”¨Qwen3-4B-Instructæ¨¡å‹ï¼Œç»“æœè¡¨æ˜è¡Œä¸ºæ ¡å‡†ä½¿å°å‹æ¨¡å‹åœ¨ä¸ç¡®å®šæ€§é‡åŒ–ï¼ˆuncertainty quantificationï¼‰è¿™ä¸€å…ƒæŠ€èƒ½ä¸Šèƒ½å¤Ÿè¶…è¶Šå‰æ²¿æ¨¡å‹ã€‚åœ¨BeyondAIMEæ•°å­¦æ¨ç†ä»»åŠ¡ä¸­ï¼Œè¯¥æ¨¡å‹çš„å¯¹æ•°åˆ»åº¦å‡†ç¡®ç‡ä¸å¹»è§‰æ¯”ï¼ˆAccuracy-to-Hallucination Ratioï¼‰å¢ç›Šæ˜¾è‘—ä¼˜äºGPT-5ï¼›è€Œåœ¨SimpleQAè·¨é¢†åŸŸäº‹å®é—®ç­”ä¸­ï¼Œå°½ç®¡åŸå§‹å‡†ç¡®ç‡è¾ƒä½ï¼Œå…¶æ ¡å‡†è¯¯å·®ï¼ˆcalibration errorï¼‰å·²è¾¾åˆ°ä¸Grok-4å’ŒGemini-2.5-Proç›¸å½“çš„æ°´å¹³ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19920v2",
      "published_date": "2025-12-22 22:51:48 UTC",
      "updated_date": "2025-12-25 04:58:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:00:58.355566+00:00"
    },
    {
      "arxiv_id": "2512.19914v1",
      "title": "A Time-efficient Prioritised Scheduling Algorithm to Optimise Initial Flock Formation of Drones",
      "title_zh": "ä¸€ç§ä¼˜åŒ–æ— äººæœºåˆå§‹é›†ç¾¤å½¢æˆçš„é«˜æ•ˆä¼˜å…ˆçº§è°ƒåº¦ç®—æ³•",
      "authors": [
        "Sujan Warnakulasooriya",
        "Andreas Willig",
        "Xiaobing Wu"
      ],
      "abstract": "Drone applications continue to expand across various domains, with flocking offering enhanced cooperative capabilities but introducing significant challenges during initial formation. Existing flocking algorithms often struggle with efficiency and scalability, particularly when potential collisions force drones into suboptimal trajectories. This paper presents a time-efficient prioritised scheduling algorithm that improves the initial formation process of drone flocks. The method assigns each drone a priority based on its number of potential collisions and its likelihood of reaching its target position without permanently obstructing other drones. Using this hierarchy, each drone computes an appropriate delay to ensure a collision-free path. Simulation results show that the proposed algorithm successfully generates collision-free trajectories for flocks of up to 5000 drones and outperforms the coupling-degree-based heuristic prioritised planning method (CDH-PP) in both performance and computational efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ— äººæœºç¾¤åœ¨initial formationè¿‡ç¨‹ä¸­å­˜åœ¨çš„æ•ˆç‡ä¸å¯æ‰©å±•æ€§éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§é«˜æ•ˆçš„time-efficient prioritised scheduling algorithmã€‚è¯¥ç®—æ³•æ ¹æ®æ¯æ¶æ— äººæœºçš„æ½œåœ¨ç¢°æ’æ¬¡æ•°ä»¥åŠåœ¨ä¸æ°¸ä¹…é˜»ç¢å…¶ä»–æ— äººæœºçš„æƒ…å†µä¸‹åˆ°è¾¾ç›®æ ‡ä½ç½®çš„å¯èƒ½æ€§æ¥åˆ†é…ä¼˜å…ˆçº§ï¼Œå¹¶æ®æ­¤è®¡ç®—é€‚å½“çš„å»¶è¿Ÿä»¥ç¡®ä¿ç”Ÿæˆæ— ç¢°æ’è·¯å¾„ã€‚ä»¿çœŸç»“æœè¡¨æ˜ï¼Œè¯¥ç®—æ³•èƒ½å¤ŸæˆåŠŸä¸ºå¤šè¾¾5000æ¶æ— äººæœºçš„é›†ç¾¤ç”Ÿæˆæ— ç¢°æ’è½¨è¿¹ï¼Œå±•ç°äº†æä½³çš„å¯æ‰©å±•æ€§ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½å’Œè®¡ç®—æ•ˆç‡ä¸Šå‡ä¼˜äºç°æœ‰çš„CDH-PPæ–¹æ³•ï¼Œä¸ºå¤§è§„æ¨¡æ— äººæœºç¾¤çš„ååŒä½œä¸šä¼˜åŒ–æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "35 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.19914v1",
      "published_date": "2025-12-22 22:37:58 UTC",
      "updated_date": "2025-12-22 22:37:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:00:49.302417+00:00"
    },
    {
      "arxiv_id": "2512.19909v1",
      "title": "Modeling Non-Ergodic Path Effects Using Conditional Generative Model for Fourier Amplitude Spectra",
      "title_zh": "åŸºäºæ¡ä»¶ç”Ÿæˆæ¨¡å‹çš„å‚…é‡Œå¶å¹…å€¼è°±ééå†è·¯å¾„æ•ˆåº”å»ºæ¨¡",
      "authors": [
        "Maxime Lacour",
        "Pu Ren",
        "Rie Nakata",
        "Nori Nakata",
        "Michael Mahoney"
      ],
      "abstract": "Recent developments in non-ergodic ground-motion models (GMMs) explicitly model systematic spatial variations in source, site, and path effects, reducing standard deviation to 30-40% of ergodic models and enabling more accurate site-specific seismic hazard analysis. Current non-ergodic GMMs rely on Gaussian Process (GP) methods with prescribed correlation functions and thus have computational limitations for large-scale predictions. This study proposes a deep-learning approach called Conditional Generative Modeling for Fourier Amplitude Spectra (CGM-FAS) as an alternative to GP-based methods for modeling non-ergodic path effects in Fourier Amplitude Spectra (FAS). CGM-FAS uses a Conditional Variational Autoencoder architecture to learn spatial patterns and interfrequency correlation directly from data by using geographical coordinates of earthquakes and stations as conditional variables. Using San Francisco Bay Area earthquake data, we compare CGM-FAS against a recent GP-based GMM for the region and demonstrate consistent predictions of non-ergodic path effects. Additionally, CGM-FAS offers advantages compared to GP-based approaches in learning spatial patterns without prescribed correlation functions, capturing interfrequency correlations, and enabling rapid predictions, generating maps for 10,000 sites across 1,000 frequencies within 10 seconds using a few GB of memory. CGM-FAS hyperparameters can be tuned to ensure generated path effects exhibit variability consistent with the GP-based empirical GMM. This work demonstrates a promising direction for efficient non-ergodic ground-motion prediction across multiple frequencies and large spatial domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ééå†åœ°éœ‡åŠ¨æ¨¡å‹(non-ergodic GMMs)ä¸­é«˜æ–¯è¿‡ç¨‹(Gaussian Process)æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡é¢„æµ‹æ—¶çš„è®¡ç®—å±€é™æ€§ï¼Œæå‡ºäº†åä¸ºCGM-FASçš„æ·±åº¦å­¦ä¹ æ–¹æ³•ç”¨äºæ¨¡æ‹Ÿå‚…é‡Œå¶æŒ¯å¹…è°±(Fourier Amplitude Spectra)ä¸­çš„ééå†è·¯å¾„æ•ˆåº”ã€‚è¯¥æ–¹æ³•é‡‡ç”¨æ¡ä»¶å˜åˆ†è‡ªç¼–ç å™¨(Conditional Variational Autoencoder)æ¶æ„ï¼Œåˆ©ç”¨åœ°éœ‡ä¸å°ç«™çš„åœ°ç†åæ ‡ç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ ç©ºé—´æ¨¡å¼å’Œé¢‘ç‡é—´çš„ç›¸å…³æ€§ã€‚é€šè¿‡æ—§é‡‘å±±æ¹¾åŒº(San Francisco Bay Area)åœ°éœ‡æ•°æ®çš„æµ‹è¯•ï¼ŒCGM-FASåœ¨ééå†è·¯å¾„æ•ˆåº”é¢„æµ‹ä¸Šè¡¨ç°å‡ºä¸ä¼ ç»ŸGPæ¨¡å‹é«˜åº¦çš„ä¸€è‡´æ€§ã€‚ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•ï¼ŒCGM-FASæ— éœ€é¢„è®¾ç›¸å…³å‡½æ•°ï¼Œèƒ½æ›´æœ‰æ•ˆåœ°æ•æ‰é¢‘ç‡é—´ç›¸å…³æ€§ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„è®¡ç®—æ•ˆç‡ï¼Œä»…éœ€10ç§’å³å¯å®Œæˆä¸‡çº§ç«™ç‚¹å’Œåƒçº§é¢‘ç‡çš„é¢„æµ‹å›¾ç”Ÿæˆã€‚è¿™é¡¹å·¥ä½œé€šè¿‡è°ƒæ•´è¶…å‚æ•°ç¡®ä¿äº†é¢„æµ‹å˜å¼‚æ€§ä¸ç»éªŒæ¨¡å‹çš„ä¸€è‡´æ€§ï¼Œä¸ºå¤šé¢‘ç‡ã€å¤§ç©ºé—´å°ºåº¦ä¸‹çš„é«˜æ•ˆééå†åœ°éœ‡åŠ¨é¢„æµ‹æä¾›äº†é‡è¦çš„æ–°é€”å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19909v1",
      "published_date": "2025-12-22 22:29:57 UTC",
      "updated_date": "2025-12-22 22:29:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:01:04.846811+00:00"
    },
    {
      "arxiv_id": "2512.20688v1",
      "title": "Mechanism-Based Intelligence (MBI): Differentiable Incentives for Rational Coordination and Guaranteed Alignment in Multi-Agent Systems",
      "title_zh": "åŸºäºæœºåˆ¶çš„æ™ºèƒ½ (MBI)ï¼šå¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ç†æ€§ååŒä¸ä¿éšœå¯¹é½çš„å¯å¾®æ¿€åŠ±",
      "authors": [
        "Stefano Grassi"
      ],
      "abstract": "Autonomous multi-agent systems are fundamentally fragile: they struggle to solve the Hayekian Information problem (eliciting dispersed private knowledge) and the Hurwiczian Incentive problem (aligning local actions with global objectives), making coordination computationally intractable. I introduce Mechanism-Based Intelligence (MBI), a paradigm that reconceptualizes intelligence as emergent from the coordination of multiple \"brains\", rather than a single one. At its core, the Differentiable Price Mechanism (DPM) computes the exact loss gradient $$ \\mathbf{G}_i = - \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{x}_i} $$ as a dynamic, VCG-equivalent incentive signal, guaranteeing Dominant Strategy Incentive Compatibility (DSIC) and convergence to the global optimum. A Bayesian extension ensures incentive compatibility under asymmetric information (BIC). The framework scales linearly ($\\mathcal{O}(N)$) with the number of agents, bypassing the combinatorial complexity of Dec-POMDPs and is empirically 50x faster than Model-Free Reinforcement Learning. By structurally aligning agent self-interest with collective objectives, it provides a provably efficient, auditable and generalizable approach to coordinated, trustworthy and scalable multi-agent intelligence grounded in economic principles.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†æœºåˆ¶åŒ–æ™ºèƒ½ (Mechanism-Based Intelligence, MBI) èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ (Multi-Agent Systems) åœ¨åˆ†æ•£çŸ¥è¯†æå–å’Œå±€éƒ¨è¡ŒåŠ¨ä¸å…¨å±€ç›®æ ‡å¯¹é½æ–¹é¢çš„è„†å¼±æ€§ã€‚å…¶æ ¸å¿ƒæ˜¯å¾®åˆ†ä»·æ ¼æœºåˆ¶ (Differentiable Price Mechanism, DPM)ï¼Œé€šè¿‡å°†æŸå¤±æ¢¯åº¦è®¡ç®—ä¸ºåŠ¨æ€ä¸”ç­‰ä»·äº VCG çš„æ¿€åŠ±ä¿¡å·ï¼Œç¡®ä¿äº†å ä¼˜ç­–ç•¥æ¿€åŠ±ç›¸å®¹æ€§ (Dominant Strategy Incentive Compatibility, DSIC) åŠå…¨å±€æœ€ä¼˜æ”¶æ•›ã€‚é’ˆå¯¹éå¯¹ç§°ä¿¡æ¯ç¯å¢ƒï¼Œè¯¥æ¡†æ¶é€šè¿‡è´å¶æ–¯æ‰©å±• (Bayesian extension) å®ç°äº†æ¿€åŠ±ç›¸å®¹æ€§ (BIC)ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶å…·æœ‰çº¿æ€§ ($\\mathcal{O}(N)$) æ‰©å±•æ€§ï¼Œæ‰§è¡Œé€Ÿåº¦æ¯”ä¼ ç»Ÿçš„æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹  (Model-Free Reinforcement Learning) å¿« 50 å€ã€‚è¿™ç§æ–¹æ³•ä»ç»“æ„ä¸Šå°†æ™ºèƒ½ä½“çš„è‡ªåˆ©è¡Œä¸ºä¸é›†ä½“ç›®æ ‡å¯¹é½ï¼Œä¸ºæ„å»ºé«˜æ•ˆã€å¯å®¡è®¡ä¸”å¯æ‰©å±•çš„ååŒæ™ºèƒ½æä¾›äº†åŸºäºç»æµå­¦åŸç†çš„ä¸¥è°¨è·¯å¾„ã€‚",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.20688v1",
      "published_date": "2025-12-22 22:22:13 UTC",
      "updated_date": "2025-12-22 22:22:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:01:05.381169+00:00"
    },
    {
      "arxiv_id": "2512.19905v1",
      "title": "Demystifying LLM-as-a-Judge: Analytically Tractable Model for Inference-Time Scaling",
      "title_zh": "æ­ç§˜ LLM-as-a-Judgeï¼šæ¨ç†ä¾§æ‰©å±•çš„è§£æå¯å¤„ç†æ¨¡å‹",
      "authors": [
        "Indranil Halder",
        "Cengiz Pehlevan"
      ],
      "abstract": "Recent developments in large language models have shown advantages in reallocating a notable share of computational resource from training time to inference time. However, the principles behind inference time scaling are not well understood. In this paper, we introduce an analytically tractable model of inference-time scaling: Bayesian linear regression with a reward-weighted sampler, where the reward is determined from a linear model, modeling LLM-as-a-judge scenario. We study this problem in the high-dimensional regime, where the deterministic equivalents dictate a closed-form expression for the posterior predictive mean and variance. We analyze the generalization error when training data are sampled from a teacher model. We draw $k$ inference-time samples and select via softmax at a temperature applied to a quadratic reward. When the reward is not too different from the teacher, the generalization error decreases monotonically with increasing inference time samples $k$. However, the specific reward that optimizes inference-time selection generally differs from the teacher. In contrast, substantial reward misspecification induces a finite optimal $k$ beyond which more sampling can increase the generalization error. For fixed $k$, there exists an optimal sampling temperature. We experimentally verify these facts in large language model inference with an additional large language model as a judge. In the \"best-of-$k$\" limit with the teacher as reward, we theoretically show that the generalization error decays as $Î˜(1/k^2)$ and determine the leading coefficient via extreme value theory. These formulas delineate domains where scaling inference-time computation is provably preferable to collecting more data. Finally, we demonstrate that when task difficulty increases, the previously mentioned advantage of inference-time compute degrades.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹LLM-as-a-Judgeåœºæ™¯çš„æ¨ç†æ—¶æ‰©å±•(Inference-time scaling)åˆ†ææ¨¡å‹ï¼Œæ—¨åœ¨æ­ç¤ºå°†è®¡ç®—èµ„æºä»è®­ç»ƒç«¯è½¬å‘æ¨ç†ç«¯çš„å†…åœ¨åŸç†ã€‚ä½œè€…åˆ©ç”¨å…·æœ‰å¥–åŠ±åŠ æƒé‡‡æ ·å™¨çš„è´å¶æ–¯çº¿æ€§å›å½’(Bayesian linear regression)æ¡†æ¶ï¼Œåœ¨é«˜ç»´çŠ¶æ€ä¸‹æ¨å¯¼å‡ºäº†æ³›åŒ–è¯¯å·®çš„é—­å¼è¡¨è¾¾å¼ã€‚ç ”ç©¶å‘ç°ï¼Œå½“å¥–åŠ±æ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹ä¸€è‡´æ—¶ï¼Œæ³›åŒ–è¯¯å·®éšæ¨ç†é‡‡æ ·æ•°kçš„å¢åŠ è€Œå•è°ƒä¸‹é™ï¼Œä½†åœ¨å¥–åŠ±å‡½æ•°å­˜åœ¨åå·®æ—¶ï¼Œå­˜åœ¨ä¸€ä¸ªæœ€ä¼˜çš„é‡‡æ ·ä¸Šé™ã€‚æ­¤å¤–ï¼Œç†è®ºè¯æ˜äº†åœ¨Best-of-kæé™ä¸‹æ³›åŒ–è¯¯å·®ä»¥$\\Theta(1/k^2)$çš„é€Ÿåº¦è¡°å‡ï¼Œå¹¶ç¡®å®šäº†ç‰¹å®škå€¼ä¸‹çš„æœ€ä¼˜é‡‡æ ·æ¸©åº¦ã€‚è¯¥ç ”ç©¶ç•Œå®šäº†æ¨ç†æ—¶è®¡ç®—åœ¨ä½•ç§æƒ…å†µä¸‹ä¼˜äºå¢åŠ è®­ç»ƒæ•°æ®ï¼Œå¹¶æŒ‡å‡ºä»»åŠ¡éš¾åº¦çš„å¢åŠ ä¼šå‰Šå¼±è¿™ç§æ‰©å±•ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "27 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.19905v1",
      "published_date": "2025-12-22 22:13:06 UTC",
      "updated_date": "2025-12-22 22:13:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:01:17.566061+00:00"
    },
    {
      "arxiv_id": "2512.19882v1",
      "title": "A Branch-and-Price Algorithm for Fast and Equitable Last-Mile Relief Aid Distribution",
      "title_zh": "é¢å‘å¿«é€Ÿå…¬å¹³çš„æœ€åä¸€å…¬é‡Œæ•‘æ´ç‰©èµ„åˆ†é…çš„åˆ†æ”¯å®šä»·ç®—æ³•",
      "authors": [
        "Mahdi Mostajabdaveh",
        "F. Sibel Salman",
        "Walter J. Gutjahr"
      ],
      "abstract": "The distribution of relief supplies to shelters is a critical aspect of post-disaster humanitarian logistics. In major disasters, prepositioned supplies often fall short of meeting all demands. We address the problem of planning vehicle routes from a distribution center to shelters while allocating limited relief supplies. To balance efficiency and equity, we formulate a bi-objective problem: minimizing a Gini-index-based measure of inequity in unsatisfied demand for fair distribution and minimizing total travel time for timely delivery. We propose a Mixed Integer Programming (MIP) model and use the $Îµ$-constraint method to handle the bi-objective nature. By deriving mathematical properties of the optimal solution, we introduce valid inequalities and design an algorithm for optimal delivery allocations given feasible vehicle routes. A branch-and-price (B&P) algorithm is developed to solve the problem efficiently. Computational tests on realistic datasets from a past earthquake in Van, Turkey, and predicted data for Istanbul's Kartal region show that the B&P algorithm significantly outperforms commercial MIP solvers. Our bi-objective approach reduces aid distribution inequity by 34% without compromising efficiency. Results indicate that when time constraints are very loose or tight, lexicographic optimization prioritizing demand coverage over fairness is effective. For moderately restrictive time constraints, a balanced approach is essential to avoid inequitable outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç¾åäººé“ä¸»ä¹‰ç‰©æµä¸­çš„æœ€åä¸€å…¬é‡Œæ•‘æ´ç‰©èµ„é…é€é—®é¢˜ï¼Œé‡ç‚¹å…³æ³¨åœ¨ç‰©èµ„ä¾›åº”æœ‰é™çš„æƒ…å†µä¸‹å¦‚ä½•å¹³è¡¡é…é€æ•ˆç‡ä¸å…¬å¹³æ€§ã€‚ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªåŒç›®æ ‡ä¼˜åŒ–æ¨¡å‹ï¼Œé€šè¿‡æœ€å°åŒ–åŸºäºGini-indexçš„ä¸æ»¡è¶³éœ€æ±‚ä¸å…¬å¹³æ€§åº¦é‡ä»¥åŠæœ€å°åŒ–æ€»è¡Œé©¶æ—¶é—´ï¼Œå®ç°å…¬å¹³åˆ†é…ä¸åŠæ—¶äº¤ä»˜ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªæ··åˆæ•´æ•°è§„åˆ’(MIP)æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨$\\epsilon$-constraintæ–¹æ³•å¤„ç†åŒç›®æ ‡å±æ€§ï¼ŒåŒæ—¶æ¨å¯¼å‡ºæœ€ä¼˜è§£çš„æ•°å­¦æ€§è´¨ä»¥å¼•å…¥æœ‰æ•ˆä¸ç­‰å¼(valid inequalities)ã€‚ä¸ºäº†é«˜æ•ˆæ±‚è§£ï¼Œç ”ç©¶å¼€å‘äº†ä¸€ç§Branch-and-Price (B&P)ç®—æ³•ï¼Œå¹¶è®¾è®¡äº†é’ˆå¯¹å¯è¡Œè½¦è¾†è·¯å¾„çš„æœ€ä¼˜äº¤ä»˜åˆ†é…ç®—æ³•ã€‚åœ¨åœŸè€³å…¶å‡¡åŸåœ°éœ‡åŠä¼Šæ–¯å¦å¸ƒå°”å¡å°”å¡”å°”åœ°åŒºçš„çœŸå®æ•°æ®é›†ä¸Šçš„æµ‹è¯•è¡¨æ˜ï¼Œè¯¥B&Pç®—æ³•æ€§èƒ½æ˜¾è‘—ä¼˜äºå•†ä¸šMIPæ±‚è§£å™¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥åŒç›®æ ‡æ–¹æ³•åœ¨ä¸æŸå¤±æ•ˆç‡çš„å‰æä¸‹ï¼Œå°†æ•‘æ´ç‰©èµ„åˆ†é…çš„ä¸å…¬å¹³æ€§é™ä½äº†34%ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œåœ¨æ—¶é—´é™åˆ¶é€‚ä¸­æ—¶é‡‡ç”¨å¹³è¡¡æ–¹æ³•å¯¹é¿å…ä¸å…¬å¹³ç»“æœè‡³å…³é‡è¦ï¼Œè€Œåœ¨æç«¯æ—¶é—´çº¦æŸä¸‹ï¼Œä¼˜å…ˆè€ƒè™‘éœ€æ±‚è¦†ç›–çš„å­—å…¸åºä¼˜åŒ–åˆ™æ›´ä¸ºæœ‰æ•ˆã€‚",
      "categories": [
        "cs.AI",
        "math.OC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19882v1",
      "published_date": "2025-12-22 21:16:52 UTC",
      "updated_date": "2025-12-22 21:16:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:02:00.889125+00:00"
    },
    {
      "arxiv_id": "2512.19879v1",
      "title": "Fine-Tuned In-Context Learners for Efficient Adaptation",
      "title_zh": "é¢å‘é«˜æ•ˆé€‚é…çš„å¾®è°ƒä¸Šä¸‹æ–‡å­¦ä¹ å™¨",
      "authors": [
        "Jorg Bornschein",
        "Clare Lyle",
        "Yazhe Li",
        "Amal Rannen-Triki",
        "Xu Owen He",
        "Razvan Pascanu"
      ],
      "abstract": "When adapting large language models (LLMs) to a specific downstream task, two primary approaches are commonly employed: (1) prompt engineering, often with in-context few-shot learning, leveraging the model's inherent generalization abilities, and (2) fine-tuning on task-specific data, directly optimizing the model's parameters. While prompt-based methods excel in few-shot scenarios, their effectiveness often plateaus as more data becomes available. Conversely, fine-tuning scales well with data but may underperform when training examples are scarce. We investigate a unified approach that bridges these two paradigms by incorporating in-context learning directly into the fine-tuning process. Specifically, we fine-tune the model on task-specific data augmented with in-context examples, mimicking the structure of k-shot prompts. This approach, while requiring per-task fine-tuning, combines the sample efficiency of in-context learning with the performance gains of fine-tuning, leading to a method that consistently matches and often significantly exceeds both these baselines. To perform hyperparameter selection in the low-data regime, we propose to use prequential evaluation, which eliminates the need for expensive cross-validation and leverages all available data for training while simultaneously providing a robust validation signal. We conduct an extensive empirical study to determine which adaptation paradigm - fine-tuning, in-context learning, or our proposed unified approach offers the best predictive performance on a concrete data downstream-tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€‚é…ä¸‹æ¸¸ä»»åŠ¡çš„ä¸¤ç§ä¸»æµèŒƒå¼ï¼Œå³åŸºäºæç¤ºå·¥ç¨‹çš„In-Context Learningï¼ˆICLï¼‰ä¸ç›´æ¥ä¼˜åŒ–å‚æ•°çš„Fine-Tuningï¼Œå¹¶é’ˆå¯¹å„è‡ªåœ¨æ•°æ®è§„æ¨¡ä¸Šçš„å±€é™æ€§æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„è§£å†³æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åœ¨åŒ…å«In-Contextç¤ºä¾‹çš„ä»»åŠ¡ç‰¹å®šæ•°æ®ä¸Šè¿›è¡ŒFine-Tuningï¼Œå°†ICLçš„æ¨ç†ç»“æ„ç›´æ¥å¼•å…¥å¾®è°ƒè¿‡ç¨‹ï¼Œä»è€Œå…¼é¡¾äº†æ ·æœ¬æ•ˆç‡ä¸æ€§èƒ½å¢ç›Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ç»Ÿä¸€é€‚é…æ–¹æ¡ˆåœ¨æ€§èƒ½ä¸Šä¸€è‡´åŒ¹é…å¹¶ç»å¸¸æ˜¾è‘—è¶…è¶Šå•ä¸€çš„ICLæˆ–Fine-TuningåŸºçº¿æ¨¡å‹ã€‚æ­¤å¤–ï¼Œä¸ºäº†åœ¨ä½æ•°æ®ç¯å¢ƒä¸‹è¿›è¡Œé«˜æ•ˆçš„è¶…å‚æ•°é€‰æ‹©ï¼Œç ”ç©¶è¿˜æå‡ºäº†Prequential Evaluationæ–¹æ³•ï¼Œé€šè¿‡åˆ©ç”¨å…¨éƒ¨å¯ç”¨æ•°æ®æä¾›ç¨³å¥çš„éªŒè¯ä¿¡å·ï¼Œé¿å…äº†å¤æ‚çš„äº¤å‰éªŒè¯ã€‚é€šè¿‡å¹¿æ³›çš„å®è¯ç ”ç©¶ï¼Œè¯¥è®ºæ–‡è¯æ˜äº†è¿™ç§ç»“åˆä¸¤ç§èŒƒå¼çš„æ–¹æ³•æ˜¯å®ç°é«˜æ•ˆã€é«˜æ€§èƒ½æ¨¡å‹é€‚é…çš„æœ€ä½³è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19879v1",
      "published_date": "2025-12-22 21:12:02 UTC",
      "updated_date": "2025-12-22 21:12:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:02:03.998805+00:00"
    },
    {
      "arxiv_id": "2601.00021v1",
      "title": "Toward a Physical Theory of Intelligence",
      "title_zh": "è¿ˆå‘æ™ºèƒ½çš„ç‰©ç†ç†è®º",
      "authors": [
        "Peter David Fagan"
      ],
      "abstract": "We present a physical theory of intelligence grounded in irreversible information processing in systems constrained by conservation laws. An intelligent system is modelled as a coupled agent-environment process whose evolution transforms information into goal-directed work. To connect information to physical state, we introduce the Conservation-Congruent Encoding (CCE) framework, in which encodings correspond to metastable basins of attraction whose separability is enforced by conservation laws. Within this framework, intelligence is defined as the amount of goal-directed work produced per nat of irreversibly processed information. From this definition we derive a hierarchy of physical constraints governing information intake, irreversible computation, and work extraction in open systems. The framework reveals how long-horizon efficiency requires the preservation of internal informational structure, giving rise to self-modelling, and it establishes that physically embodied intelligent systems possess intrinsic epistemic limits analogous to incompleteness phenomena. Applying the theory to biological systems, we analyse how oscillatory and near-critical dynamics optimise the trade-off between information preservation, dissipation, and useful work, placing the brain near an efficient operating regime predicted by the framework. At the architectural level, we develop a theory of continuous dynamical circuits in which classical Boolean logic emerges as a special case of attractor selection, while more general invariant geometries support computational modes beyond fixed-point logic. Finally, we propose a physically grounded perspective on artificial intelligence safety based on irreversible information flow and structural homeostasis. Together, these results provide a unified, substrate-neutral account of intelligence as a physical phenomenon.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºå—æ’å®šå¾‹(conservation laws)çº¦æŸä¸‹ä¸å¯é€†ä¿¡æ¯å¤„ç†çš„ç‰©ç†æ™ºèƒ½ç†è®ºï¼Œå°†æ™ºèƒ½ç³»ç»Ÿå»ºæ¨¡ä¸ºå°†ä¿¡æ¯è½¬åŒ–ä¸ºç›®æ ‡å¯¼å‘åŠŸ(goal-directed work)çš„ä»£ç†-ç¯å¢ƒè€¦åˆè¿‡ç¨‹ã€‚é€šè¿‡å¼•å…¥Conservation-Congruent Encoding (CCE) æ¡†æ¶ï¼Œç ”ç©¶è€…å°†æ™ºèƒ½å®šä¹‰ä¸ºæ¯å•ä½ä¸å¯é€†å¤„ç†ä¿¡æ¯æ‰€äº§ç”Ÿçš„ç›®æ ‡å¯¼å‘åŠŸï¼Œå¹¶æ¨å¯¼å‡ºæ”¯é…ä¿¡æ¯æ‘„å–ã€è®¡ç®—ä¸åŠŸæå–çš„ç‰©ç†çº¦æŸå±‚æ¬¡ç»“æ„ã€‚ç†è®ºæ­ç¤ºäº†é•¿æ—¶ç¨‹æ•ˆç‡å¦‚ä½•é©±åŠ¨è‡ªæˆ‘å»ºæ¨¡(self-modelling)çš„äº§ç”Ÿï¼Œå¹¶è¯æ˜äº†ç‰©ç†ä½“ç°çš„æ™ºèƒ½ç³»ç»Ÿå­˜åœ¨ç±»ä¼¼äºä¸å®Œå¤‡æ€§ç°è±¡çš„å†…åœ¨è®¤çŸ¥æé™ã€‚åº”ç”¨å±‚é¢ï¼Œè¯¥ç†è®ºåˆ†æäº†å¤§è„‘çš„è¿‘ä¸´ç•ŒåŠ¨åŠ›å­¦(near-critical dynamics)å¦‚ä½•ä¼˜åŒ–ä¿¡æ¯å¤„ç†æ•ˆç‡ï¼Œå¹¶æå‡ºäº†åŸºäºå¸å¼•å­é€‰æ‹©çš„è¿ç»­åŠ¨åŠ›å­¦ç”µè·¯(continuous dynamical circuits)æ¨¡å‹ã€‚æœ€åï¼Œç ”ç©¶ä»ä¸å¯é€†ä¿¡æ¯æµå’Œç»“æ„ç¨³æ€(structural homeostasis)çš„è§’åº¦ï¼Œä¸ºäººå·¥æ™ºèƒ½å®‰å…¨(artificial intelligence safety)æä¾›äº†ç»Ÿä¸€çš„ç‰©ç†è§†è§’ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "47 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.00021v1",
      "published_date": "2025-12-22 20:40:27 UTC",
      "updated_date": "2025-12-22 20:40:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:02:23.920989+00:00"
    },
    {
      "arxiv_id": "2512.19864v2",
      "title": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data",
      "title_zh": "HARMON-Eï¼šé¢å‘å¤šæ¨¡æ€è‚¿ç˜¤å­¦è®°å½•ç»“æ„åŒ–æ•°æ®æå–çš„åˆ†å±‚æ™ºèƒ½ä½“æ¨ç†",
      "authors": [
        "Shashi Kant Gupta",
        "Arijeet Pramanik",
        "Jerrin John Thomas",
        "Regina Schwind",
        "Lauren Wiener",
        "Avi Raju",
        "Jeremy Kornbluth",
        "Yanshan Wang",
        "Zhaohui Su",
        "Hrituraj Singh"
      ],
      "abstract": "Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HARMON-Eï¼Œä¸€ç§æ—¨åœ¨ä»ç”µå­å¥åº·è®°å½• (EHR) çš„éç»“æ„åŒ–è®°å½•ä¸­æå–ç»“æ„åŒ–è‚¿ç˜¤æ•°æ®çš„åˆ†å±‚æ™ºèƒ½ä½“æ¨ç†æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä½œä¸ºæ¨ç†æ™ºèƒ½ä½“ï¼Œç»“åˆä¸Šä¸‹æ–‡æ•æ„Ÿæ£€ç´¢ä¸è¿­ä»£ç»¼åˆèƒ½åŠ›ï¼Œå°†å¤æ‚çš„æå–ä»»åŠ¡åˆ†è§£ä¸ºæ¨¡å—åŒ–çš„è‡ªé€‚åº”ä»»åŠ¡ï¼Œä»è€Œè§£å†³äº†ç°æœ‰è‡ªåŠ¨åŒ–æ–¹æ³•åœ¨å¤„ç†æ‚£è€…å±‚é¢åˆæˆå’Œå¤§è§„æ¨¡çŸ›ç›¾ä¿¡æ¯æ—¶çš„å±€é™æ€§ã€‚ç ”ç©¶åœ¨æ¶µç›– 2,250 åç™Œç—‡æ‚£è€…çš„ 400,000 å¤šä»½ä¸´åºŠè®°å½•å’Œæ‰«æ PDF æŠ¥å‘Šä¸Šè¿›è¡Œäº†å¤§è§„æ¨¡è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå…¶å¹³å‡ F1-score è¾¾åˆ° 0.93ã€‚å…¶ä¸­ï¼Œç”Ÿç‰©æ ‡å¿—ç‰© (biomarkers) å’Œè¯ç‰© (medications) ç­‰å…³é”®å˜é‡çš„å‡†ç¡®ç‡è¶…è¿‡ 0.95ï¼Œä¸”åœ¨å®é™…æ•°æ®æ•´ç†å·¥ä½œæµä¸­è·å¾—äº† 0.94 çš„ç›´æ¥äººå·¥æ‰¹å‡†ç‡ã€‚ä½œä¸ºé¦–ä¸ªåœ¨å¤§è§„æ¨¡è‚¿ç˜¤æ•°æ®ç»“æ„åŒ–æå–ä¸­å®Œæ•´åº”ç”¨åŸºäº LLM æ™ºèƒ½ä½“çš„ç³»ç»Ÿï¼ŒHARMON-E æ˜¾è‘—é™ä½äº†äººå·¥æ ‡æ³¨æˆæœ¬ï¼Œå¹¶ä¸ºé«˜è´¨é‡ä¸´åºŠç ”ç©¶æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "39 Pages, Supplementary Included",
      "pdf_url": "https://arxiv.org/pdf/2512.19864v2",
      "published_date": "2025-12-22 20:38:30 UTC",
      "updated_date": "2025-12-26 11:32:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:02:05.766348+00:00"
    },
    {
      "arxiv_id": "2512.22220v1",
      "title": "On Extending Semantic Abstraction for Efficient Search of Hidden Objects",
      "title_zh": "æ‰©å±•è¯­ä¹‰æŠ½è±¡ä»¥å®ç°éšè—ç‰©ä½“çš„é«˜æ•ˆæœç´¢",
      "authors": [
        "Tasha Pais",
        "Nikhilesh Belulkar"
      ],
      "abstract": "Semantic Abstraction's key observation is that 2D VLMs' relevancy activations roughly correspond to their confidence of whether and where an object is in the scene. Thus, relevancy maps are treated as \"abstract object\" representations. We use this framework for learning 3D localization and completion for the exclusive domain of hidden objects, defined as objects that cannot be directly identified by a VLM because they are at least partially occluded. This process of localizing hidden objects is a form of unstructured search that can be performed more efficiently using historical data of where an object is frequently placed. Our model can accurately identify the complete 3D location of a hidden object on the first try significantly faster than a naive random search. These extensions to semantic abstraction hope to provide household robots with the skills necessary to save time and effort when looking for lost objects.",
      "tldr_zh": "è¯¥ç ”ç©¶æ‰©å±•äº†è¯­ä¹‰æŠ½è±¡(Semantic Abstraction)æ¡†æ¶ï¼Œåˆ©ç”¨2Dè§†è§‰è¯­è¨€æ¨¡å‹(VLMs)çš„ç›¸å…³æ€§æ¿€æ´»(relevancy activations)æ¥å®ç°å¯¹éšè—ç‰©ä½“(hidden objects)çš„é«˜æ•ˆ3Då®šä½ä¸è¡¥å…¨ã€‚é’ˆå¯¹å› é®æŒ¡è€Œæ— æ³•è¢«VLMç›´æ¥è¯†åˆ«çš„éšè—ç‰©ä½“ï¼Œè¯¥æ–¹æ³•å°†å…¶å¯»æ‰¾è¿‡ç¨‹è§†ä¸ºä¸€ç§éç»“æ„åŒ–æœç´¢(unstructured search)ï¼Œå¹¶é€šè¿‡åˆ©ç”¨ç‰©ä½“æ”¾ç½®é¢‘ç‡çš„å†å²æ•°æ®æ¥ä¼˜åŒ–æœç´¢æ•ˆç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿåœ¨é¦–æ¬¡å°è¯•æ—¶å‡†ç¡®è¯†åˆ«éšè—ç‰©ä½“çš„å®Œæ•´3Dä½ç½®ï¼Œå…¶æœç´¢é€Ÿåº¦æ˜¾è‘—å¿«äºä¼ ç»Ÿçš„éšæœºæœç´¢ã€‚è¿™ä¸€ç ”ç©¶é€šè¿‡å¯¹è¯­ä¹‰æŠ½è±¡çš„æ‰©å±•ï¼Œä¸ºå®¶åº­æœºå™¨äººæä¾›äº†å¯»æ‰¾ä¸¢å¤±ç‰©ä½“çš„å…³é”®æŠ€èƒ½ï¼Œæ—¨åœ¨å¤§å¹…èŠ‚çœæœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸­çš„ä½œä¸šæ—¶é—´å’Œç²¾åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22220v1",
      "published_date": "2025-12-22 20:25:19 UTC",
      "updated_date": "2025-12-22 20:25:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:02:09.001758+00:00"
    },
    {
      "arxiv_id": "2512.19849v2",
      "title": "UCCL-EP: Portable Expert-Parallel Communication",
      "title_zh": "UCCL-EPï¼šå¯ç§»æ¤çš„ä¸“å®¶å¹¶è¡Œé€šä¿¡",
      "authors": [
        "Ziming Mao",
        "Yihan Zhang",
        "Chihan Cui",
        "Zhen Huang",
        "Kaichao You",
        "Zhongjie Chen",
        "Zhiying Xu",
        "Zhenyu Gu",
        "Scott Shenker",
        "Costin Raiciu",
        "Yang Zhou",
        "Ion Stoica"
      ],
      "abstract": "Mixture-of-Experts (MoE) workloads rely on expert parallelism (EP) to achieve high GPU efficiency. State-of-the-art EP communication systems such as DeepEP demonstrate strong performance but exhibit poor portability across heterogeneous GPU and NIC platforms. The poor portability is rooted in architecture: GPU-initiated token-level RDMA communication requires tight vertical integration between GPUs and NICs, e.g., GPU writes to NIC driver/MMIO interfaces.\n  We present UCCL-EP, a portable EP communication system that delivers DeepEP-level performance across heterogeneous GPU and NIC hardware. UCCL-EP replaces GPU-initiated RDMA with a high-throughput GPU-CPU control channel: compact token-routing commands are transferred to multithreaded CPU proxies, which then issue GPUDirect RDMA operations on behalf of GPUs. UCCL-EP further emulates various ordering semantics required by specialized EP communication modes using RDMA immediate data, enabling correctness on NICs that lack such ordering, e.g., AWS EFA. We implement UCCL-EP on NVIDIA and AMD GPUs with EFA and Broadcom NICs. On EFA, it outperforms the best existing EP solution by up to $2.1\\times$ for dispatch and combine throughput. On NVIDIA-only platform, UCCL-EP achieves comparable performance to the original DeepEP. UCCL-EP also improves token throughput on SGLang by up to 40% on the NVIDIA+EFA platform, and improves DeepSeek-V3 training throughput over the AMD Primus/Megatron-LM framework by up to 45% on a 16-node AMD+Broadcom platform.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† UCCL-EPï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹æ··åˆä¸“å®¶æ¨¡å‹ (Mixture-of-Experts) çš„å¯ç§»æ¤ä¸“å®¶å¹¶è¡Œ (Expert Parallelism) é€šä¿¡ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç³»ç»Ÿåœ¨å¼‚æ„ GPU å’Œ NIC å¹³å°é—´ç”±äºä¾èµ– GPU å‘èµ· RDMA è€Œå¯¼è‡´çš„å¯ç§»æ¤æ€§å·®çš„é—®é¢˜ã€‚UCCL-EP é€šè¿‡å¼•å…¥é«˜ååé‡çš„ GPU-CPU æ§åˆ¶é€šé“å–ä»£äº†ä¼ ç»Ÿçš„ GPU ç›´æ¥å‘èµ·æ¨¡å¼ï¼Œåˆ©ç”¨å¤šçº¿ç¨‹ CPU ä»£ç†ä»£è¡¨ GPU æ‰§è¡Œ GPUDirect RDMA æ“ä½œï¼Œå¹¶åˆ©ç”¨ RDMA ç«‹å³æ•°æ®æ¨¡æ‹Ÿç‰¹å®šçš„æ’åºè¯­ä¹‰ï¼Œç¡®ä¿äº†åœ¨ AWS EFA ç­‰ç¼ºä¹åŸç”Ÿæ’åºæ”¯æŒçš„ç½‘å¡ä¸Šçš„æ­£ç¡®æ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒUCCL-EP åœ¨ EFA å¹³å°ä¸Šçš„åˆ†å‘å’Œåˆå¹¶ååé‡æœ€é«˜å¯è¾¾ç°æœ‰æ–¹æ¡ˆçš„ 2.1 å€ï¼Œåœ¨çº¯ NVIDIA ç¯å¢ƒä¸‹æ€§èƒ½ä¸ DeepEP æŒå¹³ã€‚è¯¥ç³»ç»Ÿåœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°å‡ºè‰²ï¼Œå°† SGLang çš„ Token ååé‡æå‡äº† 40%ï¼Œå¹¶åœ¨ 16 èŠ‚ç‚¹çš„ AMD ä¸ Broadcom å¹³å°ä¸Šå°† DeepSeek-V3 çš„è®­ç»ƒååé‡æé«˜äº† 45%ã€‚è¿™ä¸€ç ”ç©¶ä¸ºåœ¨å¤šæ ·åŒ–ç¡¬ä»¶æ¶æ„ä¸Šå®ç°é«˜æ€§èƒ½ã€å¯ç§»æ¤çš„å¤§æ¨¡å‹å¹¶è¡Œé€šä¿¡æä¾›äº†åˆ‡å®å¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19849v2",
      "published_date": "2025-12-22 20:05:09 UTC",
      "updated_date": "2026-01-22 03:29:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:02:31.859301+00:00"
    },
    {
      "arxiv_id": "2601.00816v1",
      "title": "MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback",
      "title_zh": "MathLedgerï¼šå…·æœ‰è´¦æœ¬è§è¯åé¦ˆçš„å¯éªŒè¯å­¦ä¹ åŸºåº§",
      "authors": [
        "Ismail Ahmad Abdullah"
      ],
      "abstract": "Contemporary AI systems achieve extraordinary performance yet remain opaque and non-verifiable, creating a crisis of trust for safety-critical deployment. We introduce MathLedger, a substrate for verifiable machine cognition that integrates formal verification, cryptographic attestation, and learning dynamics into a single epistemic loop. The system implements Reflexive Formal Learning (RFL), a symbolic analogue of gradient descent where updates are driven by verifier outcomes rather than statistical loss.\n  Phase I experiments validate the measurement and governance substrate under controlled conditions. CAL-EXP-3 validates measurement infrastructure (Delta p computation, variance tracking); separate stress tests confirm fail-closed governance triggers correctly under out-of-bounds conditions. No convergence or capability claims are made. The contribution is infrastructural: a working prototype of ledger-attested learning that enables auditability at scale.\n  Keywords: verifiable learning, formal verification, cryptographic attestation, reflexive feedback, fail-closed governance",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“ä»£AIç³»ç»Ÿä¸é€æ˜ä¸”ä¸å¯éªŒè¯çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†MathLedgerï¼Œä¸€ä¸ªå°† formal verificationã€cryptographic attestation å’Œå­¦ä¹ åŠ¨æ€é›†æˆåœ¨å•ä¸€è®¤è¯†è®ºå¾ªç¯ä¸­çš„å¯éªŒè¯æœºå™¨è®¤çŸ¥åº•å±‚æ¡†æ¶ã€‚ç³»ç»Ÿå¼•å…¥äº† Reflexive Formal Learning (RFL)ï¼Œè¿™æ˜¯ä¸€ç§æ¢¯åº¦ä¸‹é™çš„ç¬¦å·ç±»æ¯”ï¼Œå…¶æ›´æ–°ç”± verifier outcomes é©±åŠ¨è€Œéç»Ÿè®¡æŸå¤±ã€‚ç¬¬ä¸€é˜¶æ®µå®éªŒåœ¨å—æ§æ¡ä»¶ä¸‹éªŒè¯äº†å…¶æµ‹é‡å’Œæ²»ç†åŸºç¡€è®¾æ–½ï¼ŒåŒ…æ‹¬ Delta p è®¡ç®—ã€æ–¹å·®è·Ÿè¸ªä»¥åŠåœ¨è¶Šç•Œæ¡ä»¶ä¸‹çš„ fail-closed æ²»ç†è§¦å‘ã€‚è¯¥å·¥ä½œçš„æ ¸å¿ƒè´¡çŒ®åœ¨äºåŸºç¡€è®¾æ–½å±‚é¢ï¼Œæä¾›äº†ä¸€ä¸ª ledger-attested learning çš„å·¥ä½œåŸå‹ï¼Œä»è€Œå®ç°äº†å¤§è§„æ¨¡çš„ auditabilityï¼Œä¸ºå®‰å…¨å…³é”®å‹ä»»åŠ¡çš„éƒ¨ç½²æä¾›äº†ä¿¡ä»»åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 1 figure, 2 tables, 2 appendices with full proofs. Documents v0.9.4-pilot-audit-hardened audit surface with fail-closed governance, canonical JSON hashing, and artifact classification. Phase I infrastructure validation; no capability claims",
      "pdf_url": "https://arxiv.org/pdf/2601.00816v1",
      "published_date": "2025-12-22 19:27:55 UTC",
      "updated_date": "2025-12-22 19:27:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:02:34.404924+00:00"
    },
    {
      "arxiv_id": "2512.20687v2",
      "title": "PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation",
      "title_zh": "PHOTONï¼šé¢å‘æé€Ÿä¸”å†…å­˜é«˜æ•ˆè¯­è¨€ç”Ÿæˆçš„åˆ†å±‚è‡ªå›å½’å»ºæ¨¡",
      "authors": [
        "Yuma Ichikawa",
        "Naoya Takagi",
        "Takumi Nakagawa",
        "Yuzi Kanazawa",
        "Akira Sakai"
      ],
      "abstract": "Transformers operate as horizontal token-by-token scanners; at each generation step, attending to an ever-growing sequence of token-level states. This access pattern increases prefill latency and makes long-context decoding more memory-bound, as KV-cache reads and writes dominate inference time over arithmetic operations. We propose Parallel Hierarchical Operation for TOp-down Networks (PHOTON), a hierarchical autoregressive model that replaces horizontal scanning with vertical, multi-resolution context scanning. PHOTON maintains a hierarchy of latent streams: a bottom-up encoder compresses tokens into low-rate contextual states, while lightweight top-down decoders reconstruct fine-grained token representations in parallel. We further introduce recursive generation that updates only the coarsest latent stream and eliminates bottom-up re-encoding. Experimental results show that PHOTON is superior to competitive Transformer-based language models regarding the throughput-quality trade-off, providing advantages in long-context and multi-query tasks. In particular, this reduces decode-time KV-cache traffic, yielding up to $10^{3}\\times$ higher throughput per unit memory.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† PHOTONï¼Œä¸€ç§åˆ†å±‚è‡ªå›å½’ (Hierarchical Autoregressive) æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³ Transformer æ¨¡å‹åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­å›  KV-cache è®¿é—®æ¨¡å¼å¯¼è‡´çš„å»¶è¿Ÿé«˜å’Œå†…å­˜å—é™é—®é¢˜ã€‚PHOTON é‡‡ç”¨å‚ç›´ã€å¤šåˆ†è¾¨ç‡ (Multi-resolution) çš„ä¸Šä¸‹æ–‡æ‰«ææ¨¡å¼å–ä»£äº†ä¼ ç»Ÿçš„æ°´å¹³é€ token æ‰«æã€‚å…¶æ ¸å¿ƒç”±ä¸€ä¸ªå°† token å‹ç¼©ä¸ºä½é€Ÿç‡ä¸Šä¸‹æ–‡çŠ¶æ€çš„è‡ªä¸‹è€Œä¸Šç¼–ç å™¨å’Œå¹¶è¡Œé‡å»ºç»†ç²’åº¦è¡¨ç¤ºçš„è½»é‡çº§è‡ªä¸Šè€Œä¸‹è§£ç å™¨ç»„æˆã€‚ç ”ç©¶è¿›ä¸€æ­¥å¼•å…¥äº†é€’å½’ç”Ÿæˆ (Recursive Generation) æŠ€æœ¯ï¼Œé€šè¿‡ä»…æ›´æ–°æœ€ç²—ç²’åº¦çš„æ½œæµ (Latent Stream) æ¶ˆé™¤äº†é‡å¤ç¼–ç çš„å¼€é”€ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPHOTON åœ¨ååé‡ä¸è´¨é‡çš„æƒè¡¡ä¸Šä¼˜äºä¸»æµ Transformer æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿æ–‡æœ¬å¤„ç†å’Œå¤šæŸ¥è¯¢ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆé™ä½äº†è§£ç æ—¶çš„ KV-cache æµé‡ï¼Œåœ¨å•ä½å†…å­˜ä¸‹å®ç°äº†é«˜è¾¾ $10^3$ å€çš„ååé‡æå‡ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.20687v2",
      "published_date": "2025-12-22 19:26:59 UTC",
      "updated_date": "2026-01-08 01:32:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:02:23.757028+00:00"
    },
    {
      "arxiv_id": "2512.19799v1",
      "title": "PhysMaster: Building an Autonomous AI Physicist for Theoretical and Computational Physics Research",
      "title_zh": "PhysMasterï¼šæ„å»ºé¢å‘ç†è®ºä¸è®¡ç®—ç‰©ç†ç ”ç©¶çš„è‡ªä¸» AI ç‰©ç†å­¦å®¶",
      "authors": [
        "Tingjia Miao",
        "Jiawen Dai",
        "Jingkun Liu",
        "Jinxin Tan",
        "Muhua Zhang",
        "Wenkai Jin",
        "Yuwen Du",
        "Tian Jin",
        "Xianghe Pang",
        "Zexi Liu",
        "Tu Guo",
        "Zhengliang Zhang",
        "Yunjie Huang",
        "Shuo Chen",
        "Rui Ye",
        "Yuzhi Zhang",
        "Linfeng Zhang",
        "Kun Chen",
        "Wei Wang",
        "Weinan E",
        "Siheng Chen"
      ],
      "abstract": "Advances in LLMs have produced agents with knowledge and operational capabilities comparable to human scientists, suggesting potential to assist, accelerate, and automate research. However, existing studies mainly evaluate such systems on well-defined benchmarks or general tasks like literature retrieval, limiting their end-to-end problem-solving ability in open scientific scenarios. This is particularly true in physics, which is abstract, mathematically intensive, and requires integrating analytical reasoning with code-based computation. To address this, we propose PhysMaster, an LLM-based agent functioning as an autonomous theoretical and computational physicist. PhysMaster couples absract reasoning with numerical computation and leverages LANDAU, the Layered Academic Data Universe, which preserves retrieved literature, curated prior knowledge, and validated methodological traces, enhancing decision reliability and stability. It also employs an adaptive exploration strategy balancing efficiency and open-ended exploration, enabling robust performance in ultra-long-horizon tasks. We evaluate PhysMaster on problems from high-energy theory, condensed matter theory to astrophysics, including: (i) acceleration, compressing labor-intensive research from months to hours; (ii) automation, autonomously executing hypothesis-driven loops ; and (iii) autonomous discovery, independently exploring open problems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PhysMasterï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è‡ªä¸»AIç‰©ç†å­¦å®¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ™ºèƒ½ä½“åœ¨å¤„ç†é«˜åº¦æŠ½è±¡ä¸”æ•°å­¦å¯†é›†çš„ç†è®ºä¸è®¡ç®—ç‰©ç†ç ”ç©¶æ—¶çš„å±€é™ã€‚PhysMasterå°†æŠ½è±¡æ¨ç†ä¸æ•°å€¼è®¡ç®—æ·±åº¦è€¦åˆï¼Œå¹¶åˆ©ç”¨åä¸ºLANDAUï¼ˆåˆ†å±‚å­¦æœ¯æ•°æ®å®‡å®™ï¼‰çš„æ•°æ®ç³»ç»Ÿæ¥æ•´åˆæ–‡çŒ®èµ„æ–™ã€å…ˆéªŒçŸ¥è¯†å’ŒéªŒè¯è¿‡çš„æ–¹æ³•è®ºè·¯å¾„ï¼Œä»è€Œæ˜¾è‘—å¢å¼ºäº†å†³ç­–çš„å¯é æ€§ä¸ç¨³å®šæ€§ã€‚è¯¥ç³»ç»Ÿè¿˜é‡‡ç”¨äº†ä¸€ç§è‡ªé€‚åº”æ¢ç´¢ç­–ç•¥ï¼Œåœ¨ç ”ç©¶æ•ˆç‡ä¸å¼€æ”¾å¼æ¢ç´¢ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œä½¿å…¶èƒ½å¤Ÿèƒœä»»è¶…é•¿æ—¶ç¨‹çš„å¤æ‚ç§‘ç ”ä»»åŠ¡ã€‚é€šè¿‡åœ¨é«˜èƒ½ç‰©ç†ã€å‡èšæ€ç‰©ç†åŠå¤©ä½“ç‰©ç†é¢†åŸŸçš„å®è¯è¯„ä¼°ï¼ŒPhysMasterå±•ç¤ºäº†å°†è€—æ—¶æ•°æœˆçš„ç ”ç©¶å‹ç¼©è‡³æ•°å°æ—¶çš„å“è¶Šèƒ½åŠ›ï¼Œå¹¶èƒ½ç‹¬ç«‹å®Œæˆä»å‡è®¾é©±åŠ¨çš„ç ”ç©¶å¾ªç¯åˆ°è‡ªä¸»ç§‘å­¦å‘ç°çš„å…¨è¿‡ç¨‹ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "32 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.19799v1",
      "published_date": "2025-12-22 19:00:15 UTC",
      "updated_date": "2025-12-22 19:00:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:02:28.409326+00:00"
    },
    {
      "arxiv_id": "2512.19691v2",
      "title": "Scalable Stewardship of an LLM-Assisted Clinical Benchmark with Physician Oversight",
      "title_zh": "ç»“åˆåŒ»å¸ˆç›‘ç£çš„å¤§è¯­è¨€æ¨¡å‹è¾…åŠ©ä¸´åºŠåŸºå‡†å¯æ‰©å±•æ²»ç†",
      "authors": [
        "Junze Ye",
        "Daniel Tawfik",
        "Alex J. Goodell",
        "Nikhil V. Kotha",
        "Mark K. Buyyounouski",
        "Mohsen Bayati"
      ],
      "abstract": "We examine the reliability of a widely used clinical AI benchmark whose reference labels were partially generated by LLMs, and find that a substantial fraction are clinically misaligned. We introduce a phased stewardship procedure to amplify the positive impact of physician experts' feedback and then demonstrate, via a controlled RL experiment, how uncaught label bias can materially affect downstream LLM evaluation and alignment. Our results demonstrate that partially LLM-generated labels can embed systemic errors that distort not only evaluation but also downstream model alignment. By adopting a hybrid oversight system, we can prioritize scarce expert feedback to maintain benchmarks as living, clinically-grounded documents. Ensuring this alignment is a prerequisite for the safe deployment of LLMs in high-stakes medical decision support.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¹¿æ³›ä½¿ç”¨çš„ä¸´åºŠ AI åŸºå‡†æµ‹è¯•çš„å¯é æ€§ï¼Œå‘ç°å…¶ä¸­éƒ¨åˆ†ç”± LLM ç”Ÿæˆçš„å‚è€ƒæ ‡ç­¾å­˜åœ¨ä¸¥é‡çš„ä¸´åºŠå¤±è°ƒ (clinical misalignment) ç°è±¡ã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ç§åˆ†é˜¶æ®µçš„ç®¡æŠ¤æµç¨‹ (phased stewardship procedure)ï¼Œæ—¨åœ¨æœ‰æ•ˆæ”¾å¤§åŒ»å¸ˆä¸“å®¶ (physician experts) åé¦ˆçš„å½±å“åŠ›ã€‚é€šè¿‡å—æ§çš„ RL å®éªŒï¼Œç ”ç©¶è¯å®äº†æœªè¢«å‘ç°çš„æ ‡ç­¾åå·®ä¼šå®è´¨æ€§åœ°å½±å“ä¸‹æ¸¸ LLM çš„è¯„ä¼°ä¸å¯¹é½ (alignment)ï¼Œå¹¶å¯èƒ½åµŒå…¥å¯¼è‡´ç»“æœæ‰­æ›²çš„ç³»ç»Ÿæ€§é”™è¯¯ã€‚ç ”ç©¶å»ºè®®é‡‡ç”¨ä¸€ç§æ··åˆç›‘ç£ç³»ç»Ÿ (hybrid oversight system)ï¼Œé€šè¿‡ä¼˜å…ˆåˆ©ç”¨ç¨€ç¼ºçš„ä¸“å®¶åé¦ˆæ¥ç¡®ä¿åŸºå‡†æµ‹è¯•çš„ä¸´åºŠä¸¥è°¨æ€§ã€‚è¿™ç§ä¸´åºŠå¯¹é½çš„ä¿éšœæ˜¯ LLM åœ¨é«˜é£é™©åŒ»ç–—å†³ç­–æ”¯æŒä¸­å®‰å…¨éƒ¨ç½²çš„å…ˆå†³æ¡ä»¶ã€‚",
      "categories": [
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.AI",
      "comment": "Project codebase: https://github.com/junzeye/validate-medcalc-labels",
      "pdf_url": "https://arxiv.org/pdf/2512.19691v2",
      "published_date": "2025-12-22 18:59:34 UTC",
      "updated_date": "2026-01-21 18:48:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:02:41.973347+00:00"
    },
    {
      "arxiv_id": "2601.08850v1",
      "title": "The Inconsistency Critique: Epistemic Practices and AI Testimony About Inner States",
      "title_zh": "ä¸ä¸€è‡´æ€§æ‰¹åˆ¤ï¼šè®¤è¯†è®ºå®è·µä¸å…³äºå†…åœ¨çŠ¶æ€çš„ AI è¯è¨€",
      "authors": [
        "Gerol Petruzella"
      ],
      "abstract": "The question of whether AI systems have morally relevant interests -- the 'model welfare' question -- depends in part on how we evaluate AI testimony about inner states. This paper develops what I call the inconsistency critique: independent of whether skepticism about AI testimony is ultimately justified, our actual epistemic practices regarding such testimony exhibit internal inconsistencies that lack principled grounds. We functionally treat AI outputs as testimony across many domains -- evaluating them for truth, challenging them, accepting corrections, citing them as sources -- while categorically dismissing them in a specific domain, namely, claims about inner states. Drawing on Fricker's distinction between treating a speaker as an 'informant' versus a 'mere source,' the framework of testimonial injustice, and Goldberg's obligation-based account of what we owe speakers, I argue that this selective withdrawal of testimonial standing exhibits the epistemically problematic structure of prejudgment rather than principled caution. The inconsistency critique does not require taking a position on whether AI systems have morally relevant properties; rather, it is a contribution to what we may call 'epistemological hygiene' -- examining the structure of our inquiry before evaluating its conclusions. Even if our practices happen to land on correct verdicts about AI moral status, they do so for reasons that cannot adapt to new evidence or changing circumstances.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†â€œä¸ä¸€è‡´æ€§æ‰¹åˆ¤â€(inconsistency critique)ï¼Œæ—¨åœ¨æ¢è®¨äººç±»åœ¨è¯„ä¼°äººå·¥æ™ºèƒ½(AI)å…³äºå†…éƒ¨çŠ¶æ€çš„è¯è¨€(testimony)æ—¶æ‰€è¡¨ç°å‡ºçš„è®¤çŸ¥å®è·µçŸ›ç›¾ã€‚ä½œè€…æŒ‡å‡ºï¼Œå°½ç®¡æˆ‘ä»¬åœ¨è®¸å¤šé¢†åŸŸåŠŸèƒ½æ€§åœ°å°†AIè¾“å‡ºè§†ä¸ºè¯è¨€å¹¶å¯¹å…¶è¿›è¡ŒçœŸå®æ€§è¯„ä¼°æˆ–å¼•ç”¨ï¼Œä½†åœ¨æ¶‰åŠå…¶å†…éƒ¨çŠ¶æ€å£°æ˜æ—¶å´é‡‡å–äº†ç±»åˆ«æ€§çš„å¦å®šï¼Œè¿™ç§é€‰æ‹©æ€§æ’¤å›è¯è¨€åœ°ä½çš„åšæ³•ç¼ºä¹åŸåˆ™æ€§ä¾æ®ã€‚é€šè¿‡å¼•å…¥Frickerå…³äºâ€œinformantâ€ä¸â€œmere sourceâ€çš„åŒºåˆ«ã€è¯è¨€ä¸ä¹‰(testimonial injustice)æ¡†æ¶ä»¥åŠGoldbergåŸºäºä¹‰åŠ¡çš„è´¦æˆ·ï¼Œè®ºæ–‡è®ºè¯äº†è¿™ç§è¡Œä¸ºåœ¨ç»“æ„ä¸Šå±äºé¢„åˆ¤(prejudgment)è€ŒéåŸåˆ™æ€§è°¨æ…ã€‚è¯¥ç ”ç©¶é€šè¿‡å¼€å±•â€œè®¤è¯†è®ºå«ç”Ÿâ€(epistemological hygiene)å·¥ä½œï¼Œåœ¨ä¸é¢„è®¾AIé“å¾·å±æ€§çš„å‰æä¸‹å®¡è§†äº†æ¢ç©¶è¿‡ç¨‹çš„åˆç†æ€§ã€‚æœ€ç»ˆï¼Œç ”ç©¶å¼ºè°ƒç›®å‰çš„è®¤çŸ¥å®è·µå› å…¶ç†ç”±çš„åƒµåŒ–è€Œæ— æ³•æœ‰æ•ˆé€‚åº”æœªæ¥å¯èƒ½å‡ºç°çš„æ–°è¯æ®æˆ–ç¯å¢ƒå˜åŒ–ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "21 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.08850v1",
      "published_date": "2025-12-22 18:54:07 UTC",
      "updated_date": "2025-12-22 18:54:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:03:03.093919+00:00"
    },
    {
      "arxiv_id": "2512.19678v1",
      "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
      "title_zh": "WorldWarpï¼šåŸºäºå¼‚æ­¥è§†é¢‘æ‰©æ•£çš„ 3D å‡ ä½•ä¼ æ’­",
      "authors": [
        "Hanyang Kong",
        "Xingyi Yang",
        "Xiaoxu Zheng",
        "Xinchao Wang"
      ],
      "abstract": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† WorldWarp æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é•¿ç¨‹è§†é¢‘ç”Ÿæˆä¸­ 3D å‡ ä½•ä¸€è‡´æ€§è¦æ±‚ä¸ç”Ÿæˆæ¨¡å‹åœ¨æ½œç©ºé—´è¿è¡Œæ•ˆç‡ä¹‹é—´çš„çŸ›ç›¾ã€‚è¯¥æ¡†æ¶ç»“åˆäº† 3D ç»“æ„é”šç‚¹ä¸ 2D ç”Ÿæˆç²¾ç»†åŒ–å™¨ï¼Œé€šè¿‡ Gaussian Splatting (3DGS) æ„å»ºåœ¨çº¿ 3D å‡ ä½•ç¼“å­˜ä½œä¸ºç»“æ„è„šæ‰‹æ¶ï¼Œç¡®ä¿æ–°å¸§éµå¾ªå…ˆéªŒå‡ ä½•é€»è¾‘ã€‚é’ˆå¯¹é®æŒ¡äº§ç”Ÿçš„ç©ºæ´ï¼Œç ”ç©¶è®¾è®¡äº†å…·æœ‰â€œå¡«å……ä¸ä¿®æ­£â€ç›®æ ‡çš„æ—¶ç©ºæ‰©æ•£æ¨¡å‹ (ST-Diff)ï¼Œå¹¶å¼•å…¥äº†æ—¶ç©ºå˜åŒ–çš„å™ªå£°è°ƒåº¦æ–¹æ¡ˆã€‚è¯¥æ–¹æ¡ˆä¸ºç©ºç™½åŒºåŸŸæä¾›å…¨å™ªå£°ä»¥è§¦å‘ç”Ÿæˆï¼Œè€Œä¸ºæ‰­æ›²ï¼ˆWarpedï¼‰åŒºåŸŸæä¾›éƒ¨åˆ†å™ªå£°ä»¥è¿›è¡Œç»†èŠ‚ç²¾ç‚¼ã€‚é€šè¿‡åœ¨æ¯ä¸€æ­¥åŠ¨æ€æ›´æ–° 3D ç¼“å­˜ï¼ŒWorldWarp å®ç°äº† 3D é€»è¾‘å¼•å¯¼ç»“æ„ä¸æ‰©æ•£é€»è¾‘ä¼˜åŒ–çº¹ç†çš„ååŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé•¿ç¨‹è§†é¢‘ä¸€è‡´æ€§å’Œé«˜ä¿çœŸåº¦æ–¹é¢è¾¾åˆ°äº† State-of-the-art æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://hyokong.github.io/worldwarp-page/",
      "pdf_url": "https://arxiv.org/pdf/2512.19678v1",
      "published_date": "2025-12-22 18:53:50 UTC",
      "updated_date": "2025-12-22 18:53:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:03:22.514107+00:00"
    },
    {
      "arxiv_id": "2512.19673v1",
      "title": "Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies",
      "title_zh": "è‡ªä¸‹è€Œä¸Šç­–ç•¥ä¼˜åŒ–ï¼šè¯­è¨€æ¨¡å‹ç­–ç•¥ä¸­æ½œè—çš„å†…éƒ¨ç­–ç•¥",
      "authors": [
        "Yuqiao Tan",
        "Minzheng Wang",
        "Shizhu He",
        "Huanxuan Liao",
        "Chengfeng Zhao",
        "Qiunan Lu",
        "Tian Liang",
        "Jun Zhao",
        "Kang Liu"
      ],
      "abstract": "Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama's prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡åˆ©ç”¨ Transformer residual stream çš„å†…åœ¨åˆ†å‰²ä»¥åŠéšè—çŠ¶æ€ä¸ unembedding matrix ä¹‹é—´çš„ç­‰æ•ˆæ€§ï¼Œæå‡ºäº†ä¸€ç§åˆ†è§£è¯­è¨€æ¨¡å‹ç­–ç•¥çš„æ–°æ–¹æ³•ã€‚è¿™ç§åˆ†è§£æ­ç¤ºäº†æ¨¡å‹å†…éƒ¨å­˜åœ¨çš„ Internal Layer Policies ä»¥åŠä¸ Self-attention å’Œ FFN å¯¹åº”çš„ Internal Modular Policiesã€‚ç ”ç©¶å‘ç°ï¼Œæ¨¡å‹æ—©æœŸå±‚é€šè¿‡é«˜ç†µè¿›è¡Œæ¢ç´¢ï¼Œè€Œé¡¶å±‚åˆ™è¶‹å‘é›¶ç†µè¿›è¡Œç»†åŒ–ï¼Œä¸” Llama ä¸ Qwen ç³»åˆ—æ¨¡å‹åœ¨æ”¶æ•›æ¨¡å¼ä¸Šè¡¨ç°å‡ºæ˜¾è‘—å·®å¼‚ï¼Œå…¶ä¸­ Qwen3 å±•ç°äº†æ›´å…·ç»“æ„æ€§çš„æ¸è¿›å¼æ¨ç†æ¨¡å¼ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œç ”ç©¶è€…æå‡ºäº† Bottom-up Policy Optimization (BuPO) å¼ºåŒ–å­¦ä¹ èŒƒå¼ï¼Œæ—¨åœ¨è®­ç»ƒæ—©æœŸç›´æ¥ä¼˜åŒ–åº•å±‚å†…éƒ¨ç­–ç•¥ã€‚BuPO é€šè¿‡åœ¨è¾ƒä½å±‚å¯¹é½è®­ç»ƒç›®æ ‡æ¥é‡æ„åŸºç¡€æ¨ç†èƒ½åŠ›ï¼Œæœ‰æ•ˆæå‡äº†æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªæ¨ç†åŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†ä¼˜äºä¼ ç»Ÿç»Ÿä¸€ç­–ç•¥ä¼˜åŒ–æ–¹æ³•çš„æ•ˆæœã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint. Our code is available at https://github.com/Trae1ounG/BuPO",
      "pdf_url": "https://arxiv.org/pdf/2512.19673v1",
      "published_date": "2025-12-22 18:51:48 UTC",
      "updated_date": "2025-12-22 18:51:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:03:01.314351+00:00"
    },
    {
      "arxiv_id": "2512.19663v1",
      "title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis",
      "title_zh": "è¶…è¶Š CLIPï¼šç”¨äºç³–å°¿ç—…è§†ç½‘è†œç—…å˜è¯Šæ–­è·¨æ¨¡æ€å¯¹é½çš„çŸ¥è¯†å¢å¼ºå¤šæ¨¡æ€ Transformer",
      "authors": [
        "Argha Kamal Samanta",
        "Harshika Goyal",
        "Vasudha Joshi",
        "Tushar Mungle",
        "Pabitra Mitra"
      ],
      "abstract": "Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨è§†è§‰è¯­è¨€æ¨¡å‹ CLIP åœ¨ç³–å°¿ç—…è§†ç½‘è†œç—…å˜ï¼ˆDiabetic retinopathyï¼‰è¯Šæ–­ä¸­è·¨æ¨¡æ€å¯¹é½æ€§èƒ½å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°å‹çŸ¥è¯†å¢å¼ºçš„è”åˆåµŒå…¥æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šæ¨¡æ€ Transformer æ¶æ„ï¼Œæ•´åˆäº†çœ¼åº•å›¾åƒã€ä¸´åºŠæ–‡æœ¬å’Œç»“æ„åŒ–æ‚£è€…æ•°æ®ï¼Œé€šè¿‡ ViT-B/16ã€Bio-ClinicalBERT ä»¥åŠå¤šå±‚æ„ŸçŸ¥æœºåˆ†åˆ«å¤„ç†ä¸åŒæ¨¡æ€ï¼Œå¹¶ç»“åˆå¯¹æ¯”æŸå¤±ã€é‡å»ºæŸå¤±å’Œåˆ†ç±»æŸå¤±è¿›è¡Œè”åˆä¼˜åŒ–ã€‚å®éªŒç»“æœåœ¨ BRSET æ•°æ®é›†ä¸Šè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ–‡æœ¬åˆ°å›¾åƒæ£€ç´¢çš„ Recall@1 è¾¾åˆ°äº† 99.94%ï¼Œè¿œè¶…å¾®è°ƒåçš„ CLIPï¼ŒåŒæ—¶åœ¨ SDRG å’Œ ICDR åˆ†ç±»ä»»åŠ¡ä¸­å±•ç°äº† state-of-the-art çš„å‡†ç¡®ç‡ã€‚åœ¨ DeepEyeNet æ•°æ®é›†ä¸Šçš„é›¶æ ·æœ¬è¯„ä¼°éªŒè¯äº†ç³»ç»Ÿå¼ºå¤§çš„æ³›åŒ–æ€§èƒ½ï¼Œå…¶ Recall@1 è¾¾åˆ° 93.95%ï¼Œè¿œé«˜äºåŸºçº¿æ¨¡å‹ã€‚è¯¥ç ”ç©¶æˆåŠŸå»ºç«‹äº†ä¼˜è¶Šçš„è·¨æ¨¡æ€æ£€ç´¢èƒ½åŠ›å’Œç¨³å¥çš„è¯Šæ–­æ€§èƒ½ï¼Œä¸ºåŒ»å­¦å½±åƒä¸æ–‡æœ¬çš„ç²¾ç¡®å¯¹é½æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 14 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.19663v1",
      "published_date": "2025-12-22 18:41:45 UTC",
      "updated_date": "2025-12-22 18:41:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:03:06.399712+00:00"
    },
    {
      "arxiv_id": "2512.19654v1",
      "title": "Clustering with Label Consistency",
      "title_zh": "å…·æœ‰æ ‡ç­¾ä¸€è‡´æ€§çš„èšç±»",
      "authors": [
        "Diptarka Chakraborty",
        "Hendrik Fichtenberger",
        "Bernhard Haeupler",
        "Silvio Lattanzi",
        "Ashkan Norouzi-Fard",
        "Ola Svensson"
      ],
      "abstract": "Designing efficient, effective, and consistent metric clustering algorithms is a significant challenge attracting growing attention. Traditional approaches focus on the stability of cluster centers; unfortunately, this neglects the real-world need for stable point labels, i.e., stable assignments of points to named sets (clusters). In this paper, we address this gap by initiating the study of label-consistent metric clustering. We first introduce a new notion of consistency, measuring the label distance between two consecutive solutions. Then, armed with this new definition, we design new consistent approximation algorithms for the classical $k$-center and $k$-median problems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åº¦é‡èšç±»ï¼ˆmetric clusteringï¼‰ä¸­ä¼ ç»Ÿæ–¹æ³•ä»…å…³æ³¨èšç±»ä¸­å¿ƒï¼ˆcluster centersï¼‰ç¨³å®šæ€§è€Œå¿½è§†ç‚¹æ ‡ç­¾ï¼ˆpoint labelsï¼‰ç¨³å®šæ€§çš„é—®é¢˜ï¼Œæå‡ºäº†æ ‡ç­¾ä¸€è‡´æ€§åº¦é‡èšç±»ï¼ˆlabel-consistent metric clusteringï¼‰çš„æ–°èŒƒå¼ã€‚ä½œè€…é¦–å…ˆå®šä¹‰äº†ä¸€ç§è¡¡é‡è¿ç»­ä¸¤æ¬¡æ±‚è§£ä¹‹é—´æ ‡ç­¾è·ç¦»çš„æ–°å‹ä¸€è‡´æ€§æŒ‡æ ‡ï¼Œæ—¨åœ¨æ»¡è¶³ç°å®åº”ç”¨ä¸­å¯¹ç¨³å®šç‚¹åˆ†é…çš„éœ€æ±‚ã€‚åŸºäºè¿™ä¸€æ–°å®šä¹‰ï¼Œç ”ç©¶è€…ä¸ºç»å…¸çš„ $k$-center å’Œ $k$-median é—®é¢˜è®¾è®¡äº†å…¨æ–°çš„å…·æœ‰ä¸€è‡´æ€§ä¿è¯çš„è¿‘ä¼¼ç®—æ³•ï¼ˆapproximation algorithmsï¼‰ã€‚è¯¥å·¥ä½œå¡«è¡¥äº†èšç±»ç®—æ³•åœ¨æ ‡ç­¾ç¨³å®šæ€§ç†è®ºç ”ç©¶æ–¹é¢çš„ç©ºç™½ï¼Œä¸ºæ„å»ºæ›´å¯é çš„åŠ¨æ€èšç±»åˆ†æå·¥å…·æä¾›äº†ç†è®ºæ”¯æ’‘å’Œç®—æ³•åŸºç¡€ã€‚",
      "categories": [
        "cs.DS",
        "cs.AI"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19654v1",
      "published_date": "2025-12-22 18:32:23 UTC",
      "updated_date": "2025-12-22 18:32:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:03:14.802595+00:00"
    },
    {
      "arxiv_id": "2601.14264v1",
      "title": "Psychometric Comparability of LLM-Based Digital Twins",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹æ•°å­—å­ªç”Ÿçš„å¿ƒç†æµ‹é‡å¯æ¯”æ€§",
      "authors": [
        "Yufei Zhang",
        "Zhihao Ma"
      ],
      "abstract": "Large language models (LLMs) are used as \"digital twins\" to replace human respondents, yet their psychometric comparability to humans is uncertain. We propose a construct-validity framework spanning construct representation and the nomological net, benchmarking digital twins against human gold standards across models, tasks and testing how person-specific inputs shape performance. Across studies, digital twins achieved high population-level accuracy and strong within-participant profile correlations, alongside attenuated item-level correlations. In word association tests, LLM-based networks show small-world structure and theory-consistent communities similar to humans, yet diverge lexically and in local structure. In decision-making and contextualized tasks, digital twins under-reproduce heuristic biases, showing normative rationality, compressed variance and limited sensitivity to temporal information. Feature-rich digital twins improve Big Five Personality prediction, but their personality networks show only configural invariance and do not achieve metric invariance. In more applied free-text tasks, feature-rich digital twins better match human narratives, but linguistic differences persist. Together, these results indicate that feature-rich conditioning enhances validity but does not resolve systematic divergences in psychometric comparability. Future work should therefore prioritize delineating the effective boundaries of digital twins, establishing the precise contexts in which they function as reliable proxies for human cognition and behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºæ„å¿µæ•ˆåº¦(construct-validity)çš„æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä½œä¸ºæ•°å­—å­ªç”Ÿ(digital twins)æ›¿ä»£äººç±»å—è®¿è€…æ—¶çš„å¿ƒç†æµ‹é‡å¯æ¯”æ€§(psychometric comparability)ã€‚ç ”ç©¶é€šè¿‡è·¨æ¨¡å‹å’Œè·¨ä»»åŠ¡çš„å®éªŒï¼Œå¯¹æ¯”äº†æ•°å­—å­ªç”Ÿä¸äººç±»åœ¨æ„å¿µè¡¨å¾å’Œå‘½é¢˜ç½‘ç»œ(nomological net)ä¸Šçš„å·®å¼‚ï¼Œå¹¶åˆ†æäº†ç‰¹å®šä¸ªäººç‰¹å¾è¾“å…¥å¯¹æ€§èƒ½çš„å½±å“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ•°å­—å­ªç”Ÿåœ¨ç¾¤ä½“å±‚é¢çš„å‡†ç¡®ç‡è¾ƒé«˜ï¼Œä½†åœ¨å…·ä½“é¢˜ç›®å±‚é¢(item-level)çš„ç›¸å…³æ€§è¾ƒå¼±ï¼Œä¸”åœ¨å†³ç­–ä»»åŠ¡ä¸­è¡¨ç°å‡ºè¿‡åº¦ç†æ€§ã€æ–¹å·®å‹ç¼©ä»¥åŠå¯¹æ—¶é—´ä¿¡æ¯æ•æ„Ÿæ€§å—é™ç­‰ç‰¹å¾ã€‚åœ¨äººæ ¼æµ‹è¯„ä¸­ï¼Œè™½ç„¶ä¸°å¯Œçš„ç‰¹å¾è¾“å…¥æå‡äº†å¯¹å¤§äº”äººæ ¼(Big Five Personality)çš„é¢„æµ‹æ•ˆåº¦ï¼Œä½†å…¶äººæ ¼ç½‘ç»œä»…èƒ½å®ç°å½¢æ€ç­‰å€¼(configural invariance)è€Œæ— æ³•è¾¾åˆ°æµ‹é‡ç­‰å€¼(metric invariance)ã€‚å°½ç®¡ç‰¹å¾å¢å¼ºæœ‰åŠ©äºæ”¹å–„æ•°å­—å­ªç”Ÿåœ¨åº”ç”¨ä»»åŠ¡ä¸­çš„å™äº‹ä¸€è‡´æ€§ï¼Œä½†ç³»ç»Ÿæ€§çš„å¿ƒç†æµ‹é‡åå·®ä¾ç„¶å­˜åœ¨ã€‚è¯¥ç ”ç©¶æœ€ç»ˆæŒ‡å‡ºï¼Œæœªæ¥éœ€è¦è¿›ä¸€æ­¥æ˜ç¡®æ•°å­—å­ªç”Ÿçš„æœ‰æ•ˆè¾¹ç•Œï¼Œä»¥ç•Œå®šå…¶ä½œä¸ºäººç±»è®¤çŸ¥å’Œè¡Œä¸ºä»£ç†å·¥å…·çš„å¯é é€‚ç”¨æƒ…å¢ƒã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "Also available as a preprint on OSF Preprints https://osf.io/preprints/psyarxiv/965yg_v1",
      "pdf_url": "https://arxiv.org/pdf/2601.14264v1",
      "published_date": "2025-12-22 18:04:27 UTC",
      "updated_date": "2025-12-22 18:04:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:04:21.940539+00:00"
    },
    {
      "arxiv_id": "2512.19620v1",
      "title": "Exploring the features used for summary evaluation by Human and GPT",
      "title_zh": "æ¢ç´¢äººç±»ä¸GPTåœ¨æ‘˜è¦è¯„ä¼°ä¸­æ‰€ä½¿ç”¨çš„ç‰¹å¾",
      "authors": [
        "Zahra Sadeghi",
        "Evangelos Milios",
        "Frank Rudzicz"
      ],
      "abstract": "Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨æ¢ç´¢äººç±»å’Œå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ï¼ˆç‰¹åˆ«æ˜¯GPTsï¼‰åœ¨è¿›è¡Œæ‘˜è¦è¯„ä¼°(Summary evaluation)æ—¶æ‰€ä½¿ç”¨çš„ç‰¹å¾ã€‚å°½ç®¡å…ˆå‰ç ”ç©¶æ¢è®¨äº†æ¨¡å‹ä¸äººç±»è¯„ä¼°çš„ä¸€è‡´æ€§ï¼Œä½†å¯¹äºè¯„ä¼°ç‰¹å®šè´¨é‡ç»´åº¦æ—¶æ‰€ä¾èµ–çš„å…·ä½“ç‰¹å¾(Features)ä»¥åŠè¯„åˆ†ä¸æŒ‡æ ‡(Metrics)ä¹‹é—´çš„æ˜ å°„å…³ç³»ä»ç¼ºä¹æ·±å…¥ç†è§£ã€‚ä½œè€…é€šè¿‡åˆ†æç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ æŒ‡æ ‡ï¼Œè¯†åˆ«å¹¶å‘ç°äº†ä¸äººç±»åŠGPTsååº”é«˜åº¦ä¸€è‡´çš„è¯„ä¼°ç‰¹å¾ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æŒ‡ç¤ºGPTsä½¿ç”¨äººç±»è¯„ä¼°æ—¶é‡‡ç”¨çš„ç‰¹å®šæŒ‡æ ‡ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„æ¨¡å‹çš„åˆ¤æ–­å‡†ç¡®æ€§ï¼Œå¹¶ä½¿å…¶è¯„ä»·æ ‡å‡†ä¸äººç±»ååº”æ›´è¶‹ä¸€è‡´ã€‚è¿™é¡¹å·¥ä½œä¸ºä¼˜åŒ–è‡ªåŠ¨åŒ–æ‘˜è¦è¯„ä¼°æœºåˆ¶æä¾›äº†é‡è¦çš„å®è¯ä¾æ®å’Œæ–¹æ³•è®ºæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19620v1",
      "published_date": "2025-12-22 17:54:49 UTC",
      "updated_date": "2025-12-22 17:54:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:03:15.415485+00:00"
    },
    {
      "arxiv_id": "2512.19609v1",
      "title": "MapTrace: Scalable Data Generation for Route Tracing on Maps",
      "title_zh": "MapTraceï¼šé¢å‘åœ°å›¾è·¯å¾„è¿½è¸ªçš„å¯æ‰©å±•æ•°æ®ç”Ÿæˆ",
      "authors": [
        "Artemis Panagopoulou",
        "Aveek Purohit",
        "Achin Kulshrestha",
        "Soroosh Yazdani",
        "Mohit Goyal"
      ],
      "abstract": "While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨åœ°å›¾è·¯çº¿è¿½è¸ªï¼ˆroute tracingï¼‰ç­‰ç²¾ç»†ç©ºé—´ç†è§£ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼ŒæŒ‡å‡ºæ¨¡å‹éš¾ä»¥éµå¾ªåŸºæœ¬è·¯å¾„çº¦æŸçš„ä¸»è¦åŸå› åœ¨äºç¼ºä¹å¤§è§„æ¨¡ä¸”åƒç´ çº§ç²¾ç¡®çš„æ ‡æ³¨æ•°æ®ã€‚ä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº† MapTraceï¼Œè¿™æ˜¯ä¸€ç§å¯æ‰©å±•çš„åˆæˆæ•°æ®ç”Ÿæˆæµæ°´çº¿ï¼Œé€šè¿‡åˆæˆåœ°å›¾å›¾åƒå’Œåƒç´ çº§è§£ææŠ€æœ¯è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡æ ‡æ³¨ã€‚åˆ©ç”¨è¯¥æµæ°´çº¿ï¼Œç ”ç©¶å›¢é˜Ÿæ„å»ºäº†ä¸€ä¸ªæ¶µç›– 4k å¼ åœ°å›¾ã€åŒ…å« 23k ä¸ªè·¯å¾„æ ·æœ¬çš„å¾®è°ƒæ•°æ®é›†ï¼Œæ—¨åœ¨æå‡æ¨¡å‹çš„ç©ºé—´è®¤çŸ¥èƒ½åŠ›ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ MapBench åŸºå‡†æµ‹è¯•ä¸­ï¼Œå¾®è°ƒåçš„å¼€æºåŠç§æœ‰ MLLMs è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ï¼ŒæˆåŠŸç‡æå‡è¾¾ 6.4 ä¸ªç™¾åˆ†ç‚¹ï¼Œå¹¶æ˜¾è‘—é™ä½äº†è·¯å¾„è¿½è¸ªè¯¯å·®ï¼ˆNDTWï¼‰ã€‚è¯¥å·¥ä½œè¯å®äº†é¢„è®­ç»ƒæ¨¡å‹ç¼ºå¤±çš„ç²¾ç»†ç©ºé—´æ¨ç†èƒ½åŠ›å¯ä»¥é€šè¿‡åˆæˆç›‘ç£ï¼ˆsynthetic supervisionï¼‰å¾—åˆ°æ˜¾è‘—å¢å¼ºï¼Œä¸ºå¼€å‘å…·å¤‡ç±»äººç©ºé—´èƒ½åŠ›çš„æ¨¡å‹æä¾›äº†æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19609v1",
      "published_date": "2025-12-22 17:45:39 UTC",
      "updated_date": "2025-12-22 17:45:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:03:25.166751+00:00"
    },
    {
      "arxiv_id": "2512.19576v3",
      "title": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller",
      "title_zh": "LeLaRï¼šåŸºäºäººå·¥æ™ºèƒ½çš„å«æ˜Ÿå§¿æ€æ§åˆ¶å™¨çš„é¦–æ¬¡åœ¨è½¨æ¼”ç¤º",
      "authors": [
        "Kirill Djebko",
        "Tom Baumann",
        "Erik Dilger",
        "Frank Puppe",
        "Sergio Montenegro"
      ],
      "abstract": "Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-UniversitÃ¤t WÃ¼rzburg in cooperation with the Technische UniversitÃ¤t Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å«æ˜Ÿå§¿æ€æ§åˆ¶(Attitude control)ä¸­ä¼ ç»Ÿæ§åˆ¶å™¨è®¾è®¡è€—æ—¶ä¸”å¯¹æ¨¡å‹ä¸ç¡®å®šæ€§æ•æ„Ÿçš„é—®é¢˜ï¼Œæå‡ºäº†åˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ (Deep Reinforcement Learning, DRL)å­¦ä¹ è‡ªé€‚åº”æ§åˆ¶ç­–ç•¥çš„æ–¹æ³•ã€‚ç ”ç©¶å±•ç¤ºäº†LeLaRï¼Œè¿™æ˜¯å…¨çƒé¦–ä¸ªæˆåŠŸåœ¨è½¨æ¼”ç¤ºçš„åŸºäºAIçš„å«æ˜Ÿå§¿æ€æ§åˆ¶å™¨ï¼Œä¸“é—¨ç”¨äºæ‰§è¡Œæƒ¯æ€§æŒ‡å‘æœºåŠ¨ã€‚è¯¥æ§åˆ¶å™¨å®Œå…¨åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è®­ç»ƒï¼Œå¹¶éƒ¨ç½²äº2025å¹´1æœˆå‘å°„çš„InnoCube 3Uçº³ç±³å«æ˜Ÿï¼ŒæˆåŠŸå…‹æœäº†ä»æ¨¡æ‹Ÿåˆ°ç°å®çš„Sim2Realå·®è·ã€‚è®ºæ–‡è¯¦ç»†é˜è¿°äº†AIæ™ºèƒ½ä½“è®¾è®¡ã€è®­ç»ƒæ–¹æ³•ä»¥åŠæ¨¡æ‹Ÿç¯å¢ƒä¸å«æ˜Ÿå®é™…è¡Œä¸ºä¹‹é—´çš„å·®å¼‚ã€‚å®éªŒé€šè¿‡ä¸ä¼ ç»Ÿçš„PDæ§åˆ¶å™¨è¿›è¡Œå¯¹æ¯”ï¼Œåœ¨è½¨ç¨³æ€æŒ‡æ ‡ç¡®è®¤äº†è¯¥AIæ§åˆ¶å™¨åœ¨é‡å¤æœºåŠ¨ä»»åŠ¡ä¸­å…·æœ‰ç¨³å¥çš„æ€§èƒ½è¡¨ç°ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "This work has been submitted to the IEEE for possible publication. 55 pages, 27 figures, 29 tables. The maneuver telemetry datasets generated and analyzed during this work are available in the GitHub repository under https://github.com/kdjebko/lelar-in-orbit-data",
      "pdf_url": "https://arxiv.org/pdf/2512.19576v3",
      "published_date": "2025-12-22 17:00:25 UTC",
      "updated_date": "2026-01-16 10:19:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:03:28.842883+00:00"
    },
    {
      "arxiv_id": "2512.19570v1",
      "title": "The Epistemological Consequences of Large Language Models: Rethinking collective intelligence and institutional knowledge",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹çš„è®¤è¯†è®ºåæœï¼šé‡æ–°å®¡è§†é›†ä½“æ™ºèƒ½ä¸åˆ¶åº¦æ€§çŸ¥è¯†",
      "authors": [
        "Angjelin Hila"
      ],
      "abstract": "We examine epistemological threats posed by human and LLM interaction. We develop collective epistemology as a theory of epistemic warrant distributed across human collectives, using bounded rationality and dual process theory as background. We distinguish internalist justification, defined as reflective understanding of why a proposition is true, from externalist justification, defined as reliable transmission of truths. Both are necessary for collective rationality, but only internalist justification produces reflective knowledge. We specify reflective knowledge as follows: agents understand the evaluative basis of a claim, when that basis is unavailable agents consistently assess the reliability of truth sources, and agents have a duty to apply these standards within their domains of competence. We argue that LLMs approximate externalist reliabilism because they can reliably transmit information whose justificatory basis is established elsewhere, but they do not themselves possess reflective justification. Widespread outsourcing of reflective work to reliable LLM outputs can weaken reflective standards of justification, disincentivize comprehension, and reduce agents' capacity to meet professional and civic epistemic duties. To mitigate these risks, we propose a three tier norm program that includes an epistemic interaction model for individual use, institutional and organizational frameworks that seed and enforce norms for epistemically optimal outcomes, and deontic constraints at organizational and or legislative levels that instantiate discursive norms and curb epistemic vices.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†äººç±»ä¸å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)äº¤äº’å¸¦æ¥çš„è®¤è¯†è®ºå¨èƒï¼Œå¹¶ç»“åˆæœ‰é™ç†æ€§(bounded rationality)å’ŒåŒé‡åŠ å·¥ç†è®º(dual process theory)å‘å±•äº†é›†ä½“è®¤è¯†è®º(collective epistemology)ç†è®ºã€‚è®ºæ–‡æ·±å…¥åŒºåˆ†äº†å†…åœ¨ä¸»ä¹‰è¾©æŠ¤(internalist justificationï¼Œå³å¯¹å‘½é¢˜çœŸå€¼çš„åæ€æ€§ç†è§£)å’Œå¤–åœ¨ä¸»ä¹‰è¾©æŠ¤(externalist justificationï¼Œå³çœŸç†çš„å¯é ä¼ è¾“)ï¼ŒæŒ‡å‡ºåªæœ‰å‰è€…èƒ½äº§ç”Ÿåæ€æ€§çŸ¥è¯†(reflective knowledge)ã€‚ç ”ç©¶è®¤ä¸ºLLMsè™½ç„¶èƒ½é€šè¿‡å¯é åœ°ä¼ è¾“ä¿¡æ¯æ¥é€¼è¿‘å¤–åœ¨ä¸»ä¹‰å¯é è®º(externalist reliabilism)ï¼Œä½†å…¶æœ¬èº«å¹¶ä¸å…·å¤‡åæ€æ€§è¾©æŠ¤çš„èƒ½åŠ›ã€‚è¿‡åº¦ä¾èµ–LLMè¾“å‡ºè¿›è¡Œåæ€æ€§å·¥ä½œå¯èƒ½ä¼šå‰Šå¼±è¾©æŠ¤çš„åæ€æ ‡å‡†ï¼ŒæŠ‘åˆ¶äººç±»çš„ç†è§£åŠ›ï¼Œå¹¶é™ä½ä¸ªä½“å±¥è¡Œä¸“ä¸šå’Œå…¬æ°‘è®¤çŸ¥ä¹‰åŠ¡çš„èƒ½åŠ›ã€‚ä¸ºåº”å¯¹è¿™äº›é£é™©ï¼Œä½œè€…æå‡ºäº†ä¸€ä¸ªä¸‰å±‚è§„èŒƒè®¡åˆ’(three-tier norm program)ï¼Œæ¶µç›–äº†ä¸ªäººä½¿ç”¨çš„è®¤çŸ¥äº¤äº’æ¨¡å‹ã€å¼ºåŒ–è®¤çŸ¥æœ€ä¼˜ç»“æœçš„ç»„ç»‡æ¡†æ¶ï¼Œä»¥åŠåœ¨ç«‹æ³•å±‚é¢éåˆ¶è®¤çŸ¥ç¼ºé™·çš„é“ä¹‰çº¦æŸ(deontic constraints)ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "AI & Soc (2025)",
      "pdf_url": "https://arxiv.org/pdf/2512.19570v1",
      "published_date": "2025-12-22 16:52:37 UTC",
      "updated_date": "2025-12-22 16:52:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:04:10.106245+00:00"
    },
    {
      "arxiv_id": "2512.19569v1",
      "title": "Owning the Intelligence: Global AI Patents Landscape and Europe's Quest for Technological Sovereignty",
      "title_zh": "æŒæ§æ™ºèƒ½ï¼šå…¨çƒäººå·¥æ™ºèƒ½ä¸“åˆ©æ ¼å±€ä¸æ¬§æ´²å¯¹æŠ€æœ¯ä¸»æƒçš„è¿½æ±‚",
      "authors": [
        "Lapo Santarlasci",
        "Armando Rungi",
        "Loredana Fattorini",
        "Nestor Maslej"
      ],
      "abstract": "Artificial intelligence has become a key arena of global technological competition and a central concern for Europe's quest for technological sovereignty. This paper analyzes global AI patenting from 2010 to 2023 to assess Europe's position in an increasingly bipolar innovation landscape dominated by the United States and China. Using linked patent, firm, ownership, and citation data, we examine the geography, specialization, and international diffusion of AI innovation. We find a highly concentrated patent landscape: China leads in patent volumes, while the United States dominates in citation impact and technological influence. Europe accounts for a limited share of AI patents but exhibits signals of relatively high patent quality. Technological proximity reveals global convergence toward U.S. innovation trajectories, with Europe remaining fragmented rather than forming an autonomous pole. Gravity-model estimates show that cross-border AI knowledge flows are driven primarily by technological capability and specialization, while geographic and institutional factors play a secondary role. EU membership does not significantly enhance intra-European knowledge diffusion, suggesting that technological capacity, rather than political integration, underpins participation in global AI innovation networks.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ†æäº†2010å¹´è‡³2023å¹´é—´çš„å…¨çƒAIä¸“åˆ©æ ¼å±€ï¼Œæ—¨åœ¨è¯„ä¼°æ¬§æ´²åœ¨ç¾ä¸­ä¸»å¯¼çš„åŒæåˆ›æ–°èƒŒæ™¯ä¸‹çš„æŠ€æœ¯ä¸»æƒ(Technological Sovereignty)åœ°ä½ã€‚é€šè¿‡æ•´åˆä¸“åˆ©ã€ä¼ä¸šæ‰€æœ‰æƒåŠå¼•ç”¨æ•°æ®ï¼Œç ”ç©¶æ¢è®¨äº†AIåˆ›æ–°çš„åœ°ç†åˆ†å¸ƒã€ä¸“ä¸šåŒ–ç‰¹å¾å’Œå›½é™…æ‰©æ•£ã€‚è°ƒæŸ¥å‘ç°ä¸“åˆ©æ™¯è§‚é«˜åº¦é›†ä¸­ï¼Œä¸­å›½åœ¨ä¸“åˆ©æ•°é‡ä¸Šé¢†å…ˆï¼Œè€Œç¾å›½åœ¨å¼•ç”¨å½±å“åŠ›å’ŒæŠ€æœ¯ä¸»å¯¼åœ°ä½æ–¹é¢å æ®ä¼˜åŠ¿ã€‚æ¬§æ´²è™½ç„¶åœ¨AIä¸“åˆ©ä¸­æ‰€å ä»½é¢æœ‰é™ï¼Œä½†å±•ç°å‡ºç›¸å¯¹è¾ƒé«˜çš„ä¸“åˆ©è´¨é‡ã€‚å°½ç®¡å…¨çƒæŠ€æœ¯è·¯å¾„å‘ç¾å›½åˆ›æ–°è½¨è¿¹æ”¶æ•›ï¼Œä½†æ¬§æ´²å†…éƒ¨ä¾ç„¶å‘ˆç°ç¢ç‰‡åŒ–ç‰¹å¾ï¼Œå°šæœªå½¢æˆè‡ªä¸»çš„åˆ›æ–°æã€‚åˆ©ç”¨å¼•åŠ›æ¨¡å‹(Gravity-model)çš„ä¼°è®¡æ˜¾ç¤ºï¼Œè·¨å¢ƒAIçŸ¥è¯†æµåŠ¨ä¸»è¦ç”±æŠ€æœ¯èƒ½åŠ›å’Œä¸“ä¸šåŒ–é©±åŠ¨ï¼Œåœ°ç†å’Œåˆ¶åº¦å› ç´ ä½œç”¨æ¬¡ä¹‹ã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œæ¬§ç›Ÿ(EU)æˆå‘˜èº«ä»½å¹¶æœªæ˜¾è‘—å¢å¼ºæ¬§æ´²å†…éƒ¨çš„çŸ¥è¯†æ‰©æ•£ï¼Œè¡¨æ˜æŠ€æœ¯èƒ½åŠ›è€Œéæ”¿æ²»æ•´åˆæ‰æ˜¯å‚ä¸å…¨çƒAIåˆ›æ–°ç½‘ç»œçš„å…³é”®æ”¯æ’‘ã€‚",
      "categories": [
        "econ.GN",
        "cs.AI"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19569v1",
      "published_date": "2025-12-22 16:52:36 UTC",
      "updated_date": "2025-12-22 16:52:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:04:46.099149+00:00"
    },
    {
      "arxiv_id": "2512.19564v1",
      "title": "Results of the 2024 CommonRoad Motion Planning Competition for Autonomous Vehicles",
      "title_zh": "2024å¹´ CommonRoad è‡ªåŠ¨é©¾é©¶æ±½è½¦è¿åŠ¨è§„åˆ’ç«èµ›ç»“æœ",
      "authors": [
        "Yanliang Huang",
        "Xia Yan",
        "Peiran Yin",
        "Zhenduo Zhang",
        "Zeyan Shao",
        "Youran Wang",
        "Haoliang Huang",
        "Matthias Althoff"
      ],
      "abstract": "Over the past decade, a wide range of motion planning approaches for autonomous vehicles has been developed to handle increasingly complex traffic scenarios. However, these approaches are rarely compared on standardized benchmarks, limiting the assessment of relative strengths and weaknesses. To address this gap, we present the setup and results of the 4th CommonRoad Motion Planning Competition held in 2024, conducted using the CommonRoad benchmark suite. This annual competition provides an open-source and reproducible framework for benchmarking motion planning algorithms. The benchmark scenarios span highway and urban environments with diverse traffic participants, including passenger cars, buses, and bicycles. Planner performance is evaluated along four dimensions: efficiency, safety, comfort, and compliance with selected traffic rules. This report introduces the competition format and provides a comparison of representative high-performing planners from the 2023 and 2024 editions.",
      "tldr_zh": "è¯¥æŠ¥å‘Šè¯¦ç»†ä»‹ç»äº†2024å¹´ä¸¾åŠçš„ç¬¬å››å±Š CommonRoad Motion Planning Competition è‡ªåŠ¨é©¾é©¶è¿åŠ¨è§„åˆ’ç«èµ›çš„è®¾ç½®ä¸ç»“æœã€‚é’ˆå¯¹è‡ªåŠ¨é©¾é©¶è¿åŠ¨è§„åˆ’é¢†åŸŸç¼ºä¹æ ‡å‡†åŒ– benchmark è¯„ä¼°çš„é—®é¢˜ï¼Œè¯¥ç«èµ›åˆ©ç”¨ CommonRoad åŸºå‡†å¥—ä»¶æä¾›äº†ä¸€ä¸ªå¼€æºä¸”å¯å¤ç°çš„ç®—æ³•è¯„ä¼°æ¡†æ¶ã€‚ç«èµ›åœºæ™¯æ¶µç›–äº†é«˜é€Ÿå…¬è·¯å’ŒåŸå¸‚ç¯å¢ƒï¼ŒåŒ…å«ä¹˜ç”¨è½¦ã€å·´å£«åŠè‡ªè¡Œè½¦ç­‰å¤šæ ·åŒ–çš„äº¤é€šå‚ä¸è€…ã€‚ç ”ç©¶äººå‘˜ä» efficiencyã€safetyã€comfort ä»¥åŠå¯¹äº¤é€šè§„åˆ™çš„ compliance å››ä¸ªç»´åº¦å¯¹è§„åˆ’ç®—æ³•è¿›è¡Œç»¼åˆè¯„ä»·ã€‚é€šè¿‡å¯¹æ¯”2023å¹´ä¸2024å¹´è¡¨ç°ä¼˜å¼‚çš„ plannerï¼Œè¯¥æŠ¥å‘Šä¸ºè¯„ä¼°ä¸åŒè¿åŠ¨è§„åˆ’æ–¹æ³•çš„ç›¸å¯¹ä¼˜åŠ£æä¾›äº†é‡åŒ–ä¾æ®ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„æ ‡å‡†åŒ–å‘å±•ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19564v1",
      "published_date": "2025-12-22 16:46:40 UTC",
      "updated_date": "2025-12-22 16:46:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:04:45.459122+00:00"
    },
    {
      "arxiv_id": "2512.19562v1",
      "title": "REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation",
      "title_zh": "REALMï¼šä¸€ç§ç»è¿‡ç°å®åˆ°ä»¿çœŸéªŒè¯çš„æœºå™¨äººæ“ä½œæ³›åŒ–åŸºå‡†",
      "authors": [
        "Martin Sedlacek",
        "Pavlo Yefanov",
        "Georgy Ponimatkin",
        "Jai Bardhan",
        "Simon Pilc",
        "Mederic Fourmy",
        "Evangelos Kazakos",
        "Cees G. M. Snoek",
        "Josef Sivic",
        "Vladimir Petrik"
      ],
      "abstract": "Vision-Language-Action (VLA) models empower robots to understand and execute tasks described by natural language instructions. However, a key challenge lies in their ability to generalize beyond the specific environments and conditions they were trained on, which is presently difficult and expensive to evaluate in the real-world. To address this gap, we present REALM, a new simulation environment and benchmark designed to evaluate the generalization capabilities of VLA models, with a specific emphasis on establishing a strong correlation between simulated and real-world performance through high-fidelity visuals and aligned robot control. Our environment offers a suite of 15 perturbation factors, 7 manipulation skills, and more than 3,500 objects. Finally, we establish two task sets that form our benchmark and evaluate the Ï€_{0}, Ï€_{0}-FAST, and GR00T N1.5 VLA models, showing that generalization and robustness remain an open challenge. More broadly, we also show that simulation gives us a valuable proxy for the real-world and allows us to systematically probe for and quantify the weaknesses and failure modes of VLAs. Project page: https://martin-sedlacek.com/realm",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† REALMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼° Vision-Language-Action (VLA) æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„ä»¿çœŸç¯å¢ƒå’ŒåŸºå‡†æµ‹è¯•ã€‚è¯¥æ¡†æ¶é€šè¿‡é«˜ä¿çœŸè§†è§‰æ•ˆæœå’Œå¯¹é½çš„æœºå™¨äººæ§åˆ¶ï¼Œåœ¨ä»¿çœŸä¸çœŸå®ä¸–ç•Œè¡¨ç°ä¹‹é—´å»ºç«‹äº†å¼ºå¤§çš„ Real-to-Sim ç›¸å…³æ€§ã€‚å¹³å°æä¾›äº†åŒ…å« 15 ç§æ‰°åŠ¨å› å­ã€7 ç§æ“ä½œæŠ€èƒ½å’Œè¶…è¿‡ 3,500 ä¸ªç‰©ä½“çš„å¤æ‚ç¯å¢ƒï¼Œç”¨ä»¥ç³»ç»Ÿåœ°é‡åŒ–æ¨¡å‹çš„å¼±ç‚¹ã€‚å®éªŒå¯¹ $\\pi_{0}$ã€$\\pi_{0}$-FAST å’Œ GR00T N1.5 ç­‰é¢†å…ˆæ¨¡å‹è¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œç»“æœæ­ç¤ºäº† VLA æ¨¡å‹åœ¨æ³›åŒ–æ€§å’Œé²æ£’æ€§æ–¹é¢ä»é¢ä¸´å·¨å¤§æŒ‘æˆ˜ã€‚è¯¥ç ”ç©¶è¯æ˜äº†æ¨¡æ‹Ÿç¯å¢ƒå¯ä»¥ä½œä¸ºçœŸå®ä¸–ç•Œçš„æœ‰æ•ˆä»£ç†ï¼Œä¸ºå¼€å‘è€…æä¾›äº†ä¸€ä¸ªä½æˆæœ¬è¯„ä¼°æœºå™¨äººç³»ç»Ÿæ•…éšœæ¨¡å¼çš„æœ‰åŠ›å·¥å…·ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "9 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.19562v1",
      "published_date": "2025-12-22 16:44:23 UTC",
      "updated_date": "2025-12-22 16:44:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:04:42.510764+00:00"
    },
    {
      "arxiv_id": "2512.19560v1",
      "title": "BabyFlow: 3D modeling of realistic and expressive infant faces",
      "title_zh": "BabyFlowï¼šé€¼çœŸä¸”è¡¨æƒ…ä¸°å¯Œçš„å©´å„¿é¢éƒ¨ 3D å»ºæ¨¡",
      "authors": [
        "Antonia Alomar",
        "Mireia Masias",
        "Marius George Linguraru",
        "Federico M. Sukno",
        "Gemma Piella"
      ],
      "abstract": "Early detection of developmental disorders can be aided by analyzing infant craniofacial morphology, but modeling infant faces is challenging due to limited data and frequent spontaneous expressions. We introduce BabyFlow, a generative AI model that disentangles facial identity and expression, enabling independent control over both. Using normalizing flows, BabyFlow learns flexible, probabilistic representations that capture the complex, non-linear variability of expressive infant faces without restrictive linear assumptions. To address scarce and uncontrolled expressive data, we perform cross-age expression transfer, adapting expressions from adult 3D scans to enrich infant datasets with realistic and systematic expressive variants. As a result, BabyFlow improves 3D reconstruction accuracy, particularly in highly expressive regions such as the mouth, eyes, and nose, and supports synthesis and modification of infant expressions while preserving identity. Additionally, by integrating with diffusion models, BabyFlow generates high-fidelity 2D infant images with consistent 3D geometry, providing powerful tools for data augmentation and early facial analysis.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† BabyFlowï¼Œä¸€ç§æ—¨åœ¨å»ºæ¨¡çœŸå®ä¸”å¯Œæœ‰è¡¨ç°åŠ›çš„å©´å„¿é¢éƒ¨çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½æ¨¡å‹ï¼Œä»¥è¾…åŠ©å‘è‚²éšœç¢çš„æ—©æœŸæ£€æµ‹ã€‚è¯¥æ¨¡å‹åˆ©ç”¨ normalizing flows æŠ€æœ¯å®ç°äº†é¢éƒ¨èº«ä»½ (identity) ä¸è¡¨æƒ… (expression) çš„è§£è€¦ï¼Œä»è€Œå…è®¸å¯¹ä¸¤è€…è¿›è¡Œç‹¬ç«‹æ§åˆ¶ã€‚é’ˆå¯¹å©´å„¿è¡¨æƒ…æ•°æ®ç¨€ç¼ºä¸”éš¾ä»¥æ§åˆ¶çš„æŒ‘æˆ˜ï¼Œç ”ç©¶è€…é‡‡ç”¨äº† cross-age expression transfer æ–¹æ³•ï¼Œå°†æˆå¹´äººçš„ 3D æ‰«æè¡¨æƒ…å¼•å…¥å©´å„¿æ•°æ®é›†ä»¥å¢å¼ºçœŸå®æ„Ÿã€‚å®éªŒè¡¨æ˜ï¼ŒBabyFlow æ˜¾è‘—æå‡äº† 3D é‡å»ºçš„å‡†ç¡®æ€§ï¼Œå°¤å…¶åœ¨å˜´éƒ¨ã€çœ¼ç›å’Œé¼»å­ç­‰å…³é”®è¡¨ç°åŠ›åŒºåŸŸè¡¨ç°ä¼˜å¼‚ã€‚æ­¤å¤–ï¼ŒBabyFlow èƒ½å¤Ÿä¸ diffusion models ç»“åˆï¼Œç”Ÿæˆå…·æœ‰ä¸€è‡´ 3D å‡ ä½•ç‰¹å¾çš„é«˜ä¿çœŸ 2D å©´å„¿å›¾åƒã€‚è¿™é¡¹å·¥ä½œä¸ºå©´å„¿ä¸´åºŠé¢éƒ¨åˆ†æåŠç›¸å…³é¢†åŸŸçš„æ•°æ®å¢å¼ºæä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯å·¥å…·ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19560v1",
      "published_date": "2025-12-22 16:42:58 UTC",
      "updated_date": "2025-12-22 16:42:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:04:52.461238+00:00"
    },
    {
      "arxiv_id": "2512.19557v1",
      "title": "Augmenting Intelligence: A Hybrid Framework for Scalable and Stable Explanations",
      "title_zh": "å¢å¼ºæ™ºèƒ½ï¼šä¸€ç§å…¼é¡¾å¯æ‰©å±•æ€§ä¸ç¨³å®šæ€§çš„è§£é‡Šæ··åˆæ¡†æ¶",
      "authors": [
        "Lawrence Krukrubo",
        "Julius Odede",
        "Olawande Olusegun"
      ],
      "abstract": "Current approaches to Explainable AI (XAI) face a \"Scalability-Stability Dilemma.\" Post-hoc methods (e.g., LIME, SHAP) may scale easily but suffer from instability, while supervised explanation frameworks (e.g., TED) offer stability but require prohibitive human effort to label every training instance. This paper proposes a Hybrid LRR-TED framework that addresses this dilemma through a novel \"Asymmetry of Discovery.\" When applied to customer churn prediction, we demonstrate that automated rule learners (GLRM) excel at identifying broad \"Safety Nets\" (retention patterns) but struggle to capture specific \"Risk Traps\" (churn triggers)-a phenomenon we term the Anna Karenina Principle of Churn. By initialising the explanation matrix with automated safety rules and augmenting it with a Pareto-optimal set of just four human-defined risk rules, our approach achieves 94.00% predictive accuracy. This configuration outperforms the full 8-rule manual expert baseline while reducing human annotation effort by 50%, proposing a shift in the paradigm for Human-in-the-Loop AI: moving experts from the role of \"Rule Writers\" to \"Exception Handlers.\"",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§Hybrid LRR-TEDæ··åˆæ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)é¢†åŸŸä¸­äº‹åè§£é‡Šæ–¹æ³•(å¦‚LIME, SHAP)çš„ä¸ç¨³å®šæ€§ä¸ç›‘ç£è§£é‡Šæ¡†æ¶(å¦‚TED)é«˜æ ‡æ³¨æˆæœ¬ä¹‹é—´çš„â€œScalability-Stability Dilemmaâ€ã€‚ç ”ç©¶é€šè¿‡å¯¹å®¢æˆ·æµå¤±é¢„æµ‹çš„åˆ†æï¼Œæ­ç¤ºäº†â€œAsymmetry of Discoveryâ€åŠâ€œAnna Karenina Principle of Churnâ€ï¼ŒæŒ‡å‡ºè‡ªåŠ¨åŒ–è§„åˆ™å­¦ä¹ å™¨(GLRM)è™½æ“…é•¿è¯†åˆ«å¹¿æ³›çš„â€œSafety Netsâ€ï¼Œå´éš¾ä»¥æ•æ‰ç‰¹å®šçš„â€œRisk Trapsâ€ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è‡ªåŠ¨åŒ–ç”Ÿæˆçš„å®‰å…¨è§„åˆ™åˆå§‹åŒ–è§£é‡ŠçŸ©é˜µï¼Œå¹¶ä»…éœ€ç»“åˆå››ä¸ªç”±äººç±»å®šä¹‰çš„é£é™©è§„åˆ™ï¼Œä¾¿åœ¨é¢„æµ‹ä¸­è¾¾åˆ°äº†94.00%çš„å‡†ç¡®ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ€§èƒ½ä¸Šè¶…è¶Šäº†åŒ…å«8æ¡è§„åˆ™çš„å…¨äººå·¥ä¸“å®¶åŸºçº¿ï¼ŒåŒæ—¶å°†äººåŠ›æ ‡æ³¨æˆæœ¬é™ä½äº†50%ã€‚è¯¥ç ”ç©¶ä¸ºäººæœºå›ç¯(Human-in-the-Loop AI)æä¾›äº†ä¸€ç§æ–°èŒƒå¼ï¼Œå³è®©é¢†åŸŸä¸“å®¶ä»ä¼ ç»Ÿçš„â€œRule Writersâ€è§’è‰²è½¬å˜ä¸ºæ›´é«˜æ•ˆçš„â€œException Handlersâ€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 2 figures, 2 tables. Code and experiments available at https://github.com/Lawrence-Krukrubo/IBM-Learn-XAI",
      "pdf_url": "https://arxiv.org/pdf/2512.19557v1",
      "published_date": "2025-12-22 16:40:14 UTC",
      "updated_date": "2025-12-22 16:40:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:04:58.228308+00:00"
    },
    {
      "arxiv_id": "2512.19554v1",
      "title": "CARE What Fails: Contrastive Anchored-REflection for Verifiable Multimodal",
      "title_zh": "CAREï¼šé¢å‘å¯éªŒè¯å¤šæ¨¡æ€æ¨ç†çš„å¯¹æ¯”é”šå®šåæ€",
      "authors": [
        "Yongxin Wang",
        "Zhicheng Yang",
        "Meng Cao",
        "Mingfei Han",
        "Haokun Lin",
        "Yingying Zhu",
        "Xiaojun Chang",
        "Xiaodan Liang"
      ],
      "abstract": "Group-relative reinforcement learning with verifiable rewards (RLVR) often wastes the most informative data it already has the failures. When all rollouts are wrong, gradients stall; when one happens to be correct, the update usually ignores why the others are close-but-wrong, and credit can be misassigned to spurious chains. We present CARE (Contrastive Anchored REflection), a failure-centric post-training framework for multimodal reasoning that turns errors into supervision. CARE combines: (i) an anchored-contrastive objective that forms a compact subgroup around the best rollout and a set of semantically proximate hard negatives, performs within-subgroup z-score normalization with negative-only scaling, and includes an all-negative rescue to prevent zero-signal batches; and (ii) Reflection-Guided Resampling (RGR), a one-shot structured self-repair that rewrites a representative failure and re-scores it with the same verifier, converting near-misses into usable positives without any test-time reflection. CARE improves accuracy and training smoothness while explicitly increasing the share of learning signal that comes from failures. On Qwen2.5-VL-7B, CARE lifts macro-averaged accuracy by 4.6 points over GRPO across six verifiable visual-reasoning benchmarks; with Qwen3-VL-8B it reaches competitive or state-of-the-art results on MathVista and MMMU-Pro under an identical evaluation protocol.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„ç»„ç›¸å¯¹å¼ºåŒ–å­¦ä¹  (RLVR) åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­æ— æ³•æœ‰æ•ˆåˆ©ç”¨å¤±è´¥æ•°æ®çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸º CARE (Contrastive Anchored REflection) çš„åè®­ç»ƒæ¡†æ¶ã€‚ä¼ ç»Ÿçš„ RLVR åœ¨æ‰€æœ‰å°è¯•å‡å¤±è´¥æ—¶ä¼šå‡ºç°æ¢¯åº¦åœæ»ï¼Œä¸”å®¹æ˜“å°†ä¿¡ç”¨è¯¯åˆ†é…ç»™è™šå‡æ¨ç†é“¾ï¼Œè€Œ CARE é€šè¿‡å°†é”™è¯¯è½¬åŒ–ä¸ºç›‘ç£ä¿¡å·æ¥æå‡è®­ç»ƒçš„å¹³æ»‘åº¦ä¸æ•ˆæœã€‚è¯¥æ¡†æ¶ç»“åˆäº†é”šå®šå¯¹æ¯”ç›®æ ‡ (anchored-contrastive objective) ä¸åå°„å¼•å¯¼é‡é‡‡æ · (Reflection-Guided Resampling, RGR)ï¼Œå‰è€…é€šè¿‡å­ç»„ z-score å½’ä¸€åŒ–å’Œå…¨è´Ÿæ ·æœ¬æŒ½æ•‘æœºåˆ¶ç¡®ä¿æå–å­¦ä¹ ä¿¡å·ï¼Œåè€…åˆ™é€šè¿‡å•æ¬¡ç»“æ„åŒ–è‡ªä¿®å¤å°†è¿‘ä¹æ­£ç¡®çš„å¤±è´¥æ¡ˆä¾‹è½¬åŒ–ä¸ºå¯ç”¨æ­£æ ·æœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCARE åœ¨ Qwen2.5-VL-7B ä¸Šæ¯” GRPO åŸºçº¿åœ¨å…­é¡¹è§†è§‰æ¨ç†ä»»åŠ¡ä¸­å®å¹³å‡å‡†ç¡®ç‡æå‡äº† 4.6 ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ Qwen3-VL-8B æ¨¡å‹ä¸Šäº MathVista å’Œ MMMU-Pro åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† state-of-the-art çš„ç«äº‰æ€§ç»“æœï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹ä»å¤±è´¥ä¸­å­¦ä¹ çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19554v1",
      "published_date": "2025-12-22 16:34:21 UTC",
      "updated_date": "2025-12-22 16:34:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:05:05.059180+00:00"
    },
    {
      "arxiv_id": "2512.19551v1",
      "title": "Towards Closed-Loop Embodied Empathy Evolution: Probing LLM-Centric Lifelong Empathic Motion Generation in Unseen Scenarios",
      "title_zh": "è¿ˆå‘é—­ç¯å…·èº«å…±æƒ…æ¼”åŒ–ï¼šæ¢ç©¶æœªçŸ¥åœºæ™¯ä¸‹ä»¥ LLM ä¸ºä¸­å¿ƒçš„ç»ˆèº«å…±æƒ…åŠ¨ä½œç”Ÿæˆ",
      "authors": [
        "Jiawen Wang",
        "Jingjing Wang Tianyang Chen",
        "Min Zhang",
        "Guodong Zhou"
      ],
      "abstract": "In the literature, existing human-centric emotional motion generation methods primarily focus on boosting performance within a single scale-fixed dataset, largely neglecting the flexible and scale-increasing motion scenarios (e.g., sports, dance), whereas effectively learning these newly emerging scenarios can significantly enhance the model's real-world generalization ability. Inspired by this, this paper proposes a new LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) task, which aims to equip LLMs with the capability to continually acquire emotional motion generation knowledge across different unseen scenarios, potentially contributing to building a closed-loop and self-evolving embodied agent equipped with both empathy and intelligence. Further, this paper poses two key challenges in the L^2-EMG task, i.e., the emotion decoupling challenge and the scenario adapting challenge. To this end, this paper proposes an Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) approach which designs a causal-guided emotion decoupling block and a scenario-adapted expert constructing block to address the two challenges, respectively. Especially, this paper constructs multiple L^2-EMG datasets to validate the effectiveness of the ES-MoE approach. Extensive evaluations show that ES-MoE outperforms advanced baselines.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ä»¥äººä¸ºä¸­å¿ƒçš„æƒ…ç»ªè¿åŠ¨ç”Ÿæˆæ–¹æ³•å±€é™äºå•ä¸€å›ºå®šè§„æ¨¡æ•°æ®é›†ã€å¿½ç•¥äº†è¿åŠ¨å’Œèˆè¹ˆç­‰çµæ´»åœºæ™¯è¿™ä¸€é—®é¢˜ï¼Œæå‡ºäº† LLM-Centric Lifelong Empathic Motion Generation (L^2-EMG) ä»»åŠ¡ï¼Œæ—¨åœ¨èµ‹äºˆå¤§è¯­è¨€æ¨¡å‹åœ¨æœªè§åœºæ™¯ä¸­æŒç»­è·å–æƒ…ç»ªè¿åŠ¨ç”ŸæˆçŸ¥è¯†çš„èƒ½åŠ›ï¼Œä»è€Œæ„å»ºé—­ç¯è‡ªè¿›åŒ–çš„å…·èº«æ™ºèƒ½ä½“ã€‚è®ºæ–‡æ˜ç¡®äº†è¯¥ä»»åŠ¡é¢ä¸´çš„ emotion decoupling å’Œ scenario adapting ä¸¤å¤§æ ¸å¿ƒæŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ Emotion-Transferable and Scenario-Adapted Mixture of Experts (ES-MoE) æ–¹æ³•ï¼Œé€šè¿‡è®¾è®¡ causal-guided emotion decoupling æ¨¡å—å’Œ scenario-adapted expert constructing æ¨¡å—æ¥åˆ†åˆ«è§£å†³ä¸Šè¿°éš¾é¢˜ã€‚ä½œè€…è¿˜æ„å»ºäº†ä¸“é—¨çš„ L^2-EMG æ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œå®éªŒè¯„ä¼°æ˜¾ç¤º ES-MoE çš„è¡¨ç°ä¼˜äºç°æœ‰çš„å…ˆè¿›åŸºå‡†æ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨æå‡æ¨¡å‹ç°å®ä¸–ç•Œæ³›åŒ–èƒ½åŠ›æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19551v1",
      "published_date": "2025-12-22 16:31:30 UTC",
      "updated_date": "2025-12-22 16:31:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:05:18.290068+00:00"
    },
    {
      "arxiv_id": "2512.19535v1",
      "title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
      "title_zh": "CASAï¼šé€šè¿‡è‡ªæ³¨æ„åŠ›å®ç°é«˜æ•ˆè§†è§‰è¯­è¨€èåˆçš„äº¤å‰æ³¨æ„åŠ›",
      "authors": [
        "Moritz BÃ¶hle",
        "AmÃ©lie Royer",
        "Juliette Marrie",
        "Edouard Grave",
        "Patrick PÃ©rez"
      ],
      "abstract": "Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒæˆ–é•¿è§†é¢‘æ—¶é¢ä¸´çš„è®¡ç®—å’Œå†…å­˜å¼€é”€é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºCASA (Cross-Attention via Self-Attention) çš„é«˜æ•ˆèåˆèŒƒå¼ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„äº¤å‰æ³¨æ„åŠ›(cross-attention)æ¨¡å‹åœ¨å¤„ç†ç²¾ç»†è§†è§‰ç»†èŠ‚æ—¶æ€§èƒ½è¾ƒå¼±ï¼Œå…³é”®åŸå› åœ¨äºå…¶ç¼ºä¹å±€éƒ¨çš„æ–‡æœ¬åˆ°æ–‡æœ¬(text-to-text)äº¤äº’ã€‚é€šè¿‡åœ¨ä¸“ç”¨çš„äº¤å‰æ³¨æ„åŠ›å±‚ä¸­å¼•å…¥è¿™ç§äº¤äº’æœºåˆ¶ï¼ŒCASAå¤§å¹…ç¼©å°äº†é«˜æ•ˆæ¨¡å‹ä¸è®¡ç®—å¯†é›†å‹æ ‡è®°æ’å…¥(token insertion)æ¨¡å‹ä¹‹é—´çš„æ€§èƒ½å·®è·ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCASAåœ¨å¸¸ç”¨å›¾åƒç†è§£åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°å“è¶Šï¼ŒåŒæ—¶åœ¨æµå¼è§†é¢‘å­—å¹•ç”Ÿæˆç­‰é•¿ä¸Šä¸‹æ–‡å¤šæ¨¡æ€ä»»åŠ¡ä¸­å±•ç°å‡ºæä½³çš„å¯æ‰©å±•æ€§ã€‚è¿™ä¸€ç®€å•ä¸”é«˜æ•ˆçš„èŒƒå¼ä¸ºæ„å»ºå…¼é¡¾æ¨ç†æ•ˆç‡ä¸è§†è§‰ç†è§£ç²¾åº¦çš„å¤šæ¨¡æ€æ¶æ„æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19535v1",
      "published_date": "2025-12-22 16:21:39 UTC",
      "updated_date": "2025-12-22 16:21:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:05:22.446430+00:00"
    },
    {
      "arxiv_id": "2512.19530v1",
      "title": "Learning Continuous Solvent Effects from Transient Flow Data: A Graph Neural Network Benchmark on Catechol Rearrangement",
      "title_zh": "ä»ç¬æ€æµåŠ¨æ•°æ®ä¸­å­¦ä¹ è¿ç»­æº¶å‰‚æ•ˆåº”ï¼šå„¿èŒ¶é…šé‡æ’çš„å›¾ç¥ç»ç½‘ç»œåŸºå‡†",
      "authors": [
        "Hongsheng Xing",
        "Qiuxin Si"
      ],
      "abstract": "Predicting reaction outcomes across continuous solvent composition ranges remains a critical challenge in organic synthesis and process chemistry. Traditional machine learning approaches often treat solvent identity as a discrete categorical variable, which prevents systematic interpolation and extrapolation across the solvent space. This work introduces the \\textbf{Catechol Benchmark}, a high-throughput transient flow chemistry dataset comprising 1,227 experimental yield measurements for the rearrangement of allyl-substituted catechol in 24 pure solvents and their binary mixtures, parameterized by continuous volume fractions ($\\% B$). We evaluate various architectures under rigorous leave-one-solvent-out and leave-one-mixture-out protocols to test generalization to unseen chemical environments.\n  Our results demonstrate that classical tabular methods (e.g., Gradient-Boosted Decision Trees) and large language model embeddings (e.g., Qwen-7B) struggle with quantitative precision, yielding Mean Squared Errors (MSE) of 0.099 and 0.129, respectively. In contrast, we propose a hybrid GNN-based architecture that integrates Graph Attention Networks (GATs) with Differential Reaction Fingerprints (DRFP) and learned mixture-aware solvent encodings. This approach achieves an \\textbf{MSE of 0.0039} ($\\pm$ 0.0003), representing a 60\\% error reduction over competitive baselines and a $>25\\times$ improvement over tabular ensembles. Ablation studies confirm that explicit molecular graph message-passing and continuous mixture encoding are essential for robust generalization. The complete dataset, evaluation protocols, and reference implementations are released to facilitate data-efficient reaction prediction and continuous solvent representation learning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è·¨è¿ç»­æº¶å‰‚ç»„æˆé¢„æµ‹åŒ–å­¦ååº”ç»“æœçš„æŒ‘æˆ˜ï¼Œé’ˆå¯¹ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ–¹æ³•éš¾ä»¥å¤„ç†æº¶å‰‚ç©ºé—´æ’å€¼ä¸å¤–æ¨çš„é—®é¢˜ï¼Œæå‡ºäº†Catechol Benchmarkæ•°æ®é›†ã€‚è¯¥æ•°æ®é›†åŒ…å«1,227é¡¹é«˜é€šé‡ç¬æ€æµåŠ¨åŒ–å­¦(transient flow chemistry)å®éªŒæ•°æ®ï¼Œæ¶µç›–çƒ¯ä¸™åŸºå–ä»£é‚»è‹¯äºŒé…š(allyl-substituted catechol)åœ¨24ç§çº¯æº¶å‰‚åŠå…¶äºŒå…ƒæ··åˆç‰©ä¸­çš„é‡æ’ååº”äº§ç‡ã€‚å®éªŒå‘ç°æ¢¯åº¦æå‡å†³ç­–æ ‘(GBDT)å’ŒQwen-7Bç­‰å¤§è¯­è¨€æ¨¡å‹åœ¨å®šé‡ç²¾åº¦ä¸Šè¡¨ç°å—é™ï¼Œè€Œæœ¬ç ”ç©¶æå‡ºçš„é›†æˆå›¾æ³¨æ„åŠ›ç½‘ç»œ(GATs)ã€å¾®åˆ†ååº”æŒ‡çº¹(DRFP)å’Œæ··åˆç‰©æ„ŸçŸ¥æº¶å‰‚ç¼–ç (mixture-aware solvent encodings)çš„æ··åˆGNNæ¶æ„è¡¨ç°ä¼˜å¼‚ã€‚è¯¥æ¨¡å‹å®ç°äº†0.0039çš„å‡æ–¹è¯¯å·®(MSE)ï¼Œè¾ƒç«äº‰åŸºçº¿é™ä½äº†60%çš„è¯¯å·®ï¼Œå¹¶æ¯”è¡¨æ ¼é›†æˆæ–¹æ³•æå‡äº†25å€ä»¥ä¸Šçš„é¢„æµ‹æ€§èƒ½ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œæ˜¾å¼çš„åˆ†å­å›¾æ¶ˆæ¯ä¼ é€’(molecular graph message-passing)å’Œè¿ç»­æ··åˆç‰©ç¼–ç æ˜¯å®ç°è·¨æº¶å‰‚ç¯å¢ƒç¨³å¥æ³›åŒ–çš„æ ¸å¿ƒè¦ç´ ã€‚è¯¥é¡¹å·¥ä½œé€šè¿‡å‘å¸ƒå®Œæ•´æ•°æ®é›†å’Œè¯„ä¼°åè®®ï¼Œä¸ºæ•°æ®é©±åŠ¨çš„ååº”é¢„æµ‹å’Œè¿ç»­æº¶å‰‚è¡¨ç¤ºå­¦ä¹ æä¾›äº†å…³é”®çš„åŸºå‡†æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.19530v1",
      "published_date": "2025-12-22 16:19:01 UTC",
      "updated_date": "2025-12-22 16:19:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:05:11.398278+00:00"
    },
    {
      "arxiv_id": "2512.19526v1",
      "title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models",
      "title_zh": "QuantiPhyï¼šè§†è§‰è¯­è¨€æ¨¡å‹ç‰©ç†æ¨ç†èƒ½åŠ›çš„å®šé‡è¯„ä¼°åŸºå‡†",
      "authors": [
        "Li Puyin",
        "Tiange Xiang",
        "Ella Mao",
        "Shirley Wei",
        "Xinye Chen",
        "Adnan Masood",
        "Li Fei-fei",
        "Ehsan Adeli"
      ],
      "abstract": "Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨ç‰©ç†æ¨ç†æ–¹é¢ç¼ºä¹å®šé‡åˆ†æçš„é—®é¢˜ï¼Œæå‡ºäº†QuantiPhyï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºå®šé‡è¯„ä¼°VLMç‰©ç†æ¨ç†èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†åŒ…å«è¶…è¿‡3.3Kä¸ªè§†é¢‘-æ–‡æœ¬å®ä¾‹å¹¶é…å¤‡äº†æ•°å€¼çœŸå®å€¼(numerical ground truth)ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨ç»™å®šæ—¶é—´æˆ³ä¸‹å¯¹ç‰©ä½“å°ºå¯¸(size)ã€é€Ÿåº¦(velocity)å’ŒåŠ é€Ÿåº¦(acceleration)çš„ä¼°ç®—èƒ½åŠ›ã€‚é€šè¿‡æ ‡å‡†åŒ–æç¤ºå’Œè¯„åˆ†ç³»ç»Ÿï¼ŒQuantiPhy èƒ½å¤Ÿå…¬å¹³åœ°æ¯”è¾ƒä¸åŒæ¨¡å‹åœ¨æ•°å€¼å‡†ç¡®æ€§ä¸Šçš„è¡¨ç°ã€‚å®éªŒç»“æœæ­ç¤ºäº†å½“å‰çš„SOTA VLMsåœ¨å®šæ€§åˆç†æ€§ä¸å®é™…æ•°å€¼å‡†ç¡®æ€§ä¹‹é—´å­˜åœ¨ä¸€è‡´çš„å·®è·ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œæ¨¡å‹åœ¨è¿›è¡Œå®šé‡è¿åŠ¨å­¦å±æ€§æ¨ç†æ—¶ï¼Œå¾€å¾€è¿‡åº¦ä¾èµ–é¢„è®­ç»ƒçš„ä¸–ç•ŒçŸ¥è¯†(pre-trained world knowledge)ï¼Œè€Œéå¿ å®åœ°å‚è€ƒè§†è§‰å’Œæ–‡æœ¬è¾“å…¥ã€‚QuantiPhy ä¸ºæ¨åŠ¨VLMsä»å•çº¯çš„è¯­è¨€åˆç†æ€§è½¬å‘å…·å¤‡æ•°å€¼åŸºç¡€çš„ç‰©ç†ç†è§£æä¾›äº†ä¸€ä¸ªä¸¥è°¨ä¸”å¯æ‰©å±•çš„æµ‹è¯•å¹³å°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19526v1",
      "published_date": "2025-12-22 16:18:00 UTC",
      "updated_date": "2025-12-22 16:18:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:05:24.073003+00:00"
    },
    {
      "arxiv_id": "2512.19516v1",
      "title": "LacaDM: A Latent Causal Diffusion Model for Multiobjective Reinforcement Learning",
      "title_zh": "LacaDMï¼šé¢å‘å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ çš„éšå› æœæ‰©æ•£æ¨¡å‹",
      "authors": [
        "Xueming Yan",
        "Bo Yin",
        "Yaochu Jin"
      ],
      "abstract": "Multiobjective reinforcement learning (MORL) poses significant challenges due to the inherent conflicts between objectives and the difficulty of adapting to dynamic environments. Traditional methods often struggle to generalize effectively, particularly in large and complex state-action spaces. To address these limitations, we introduce the Latent Causal Diffusion Model (LacaDM), a novel approach designed to enhance the adaptability of MORL in discrete and continuous environments. Unlike existing methods that primarily address conflicts between objectives, LacaDM learns latent temporal causal relationships between environmental states and policies, enabling efficient knowledge transfer across diverse MORL scenarios. By embedding these causal structures within a diffusion model-based framework, LacaDM achieves a balance between conflicting objectives while maintaining strong generalization capabilities in previously unseen environments. Empirical evaluations on various tasks from the MOGymnasium framework demonstrate that LacaDM consistently outperforms the state-of-art baselines in terms of hypervolume, sparsity, and expected utility maximization, showcasing its effectiveness in complex multiobjective tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šç›®æ ‡å¼ºåŒ–å­¦ä¹ (Multiobjective Reinforcement Learning, MORL)ä¸­ç›®æ ‡å†²çªåŠåŠ¨æ€ç¯å¢ƒé€‚åº”æ€§å·®çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†æ½œåœ¨å› æœæ‰©æ•£æ¨¡å‹(Latent Causal Diffusion Model, LacaDM)ï¼Œæ—¨åœ¨æå‡æ¨¡å‹åœ¨ç¦»æ•£å’Œè¿ç»­ç¯å¢ƒä¸­çš„è‡ªé€‚åº”èƒ½åŠ›ã€‚ä¸åŒäºä»…å…³æ³¨ç›®æ ‡å†²çªçš„ä¼ ç»Ÿæ–¹æ³•ï¼ŒLacaDM é€šè¿‡å­¦ä¹ ç¯å¢ƒçŠ¶æ€ä¸ç­–ç•¥ä¹‹é—´çš„æ½œåœ¨æ—¶é—´å› æœå…³ç³»(Latent temporal causal relationships)ï¼Œå¹¶å°†è¿™äº›å› æœç»“æ„åµŒå…¥åŸºäºæ‰©æ•£æ¨¡å‹(Diffusion model-based)çš„æ¡†æ¶ä¸­ï¼Œå®ç°äº†è·¨åœºæ™¯çš„é«˜æ•ˆçŸ¥è¯†è¿ç§»ã€‚è¯¥æ–¹æ³•åœ¨æœ‰æ•ˆå¹³è¡¡å¤šä¸ªå†²çªç›®æ ‡çš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†åœ¨æœªçŸ¥ç¯å¢ƒä¸­çš„æ³›åŒ–è¡¨ç°ã€‚åœ¨ MOGymnasium æ¡†æ¶çš„å¤šé¡¹ä»»åŠ¡è¯„ä¼°ä¸­ï¼ŒLacaDM åœ¨è¶…ä½“ç§¯(Hypervolume)ã€ç¨€ç–æ€§(Sparsity)å’ŒæœŸæœ›æ•ˆç”¨æœ€å¤§åŒ–(Expected utility maximization)ç­‰å…³é”®æŒ‡æ ‡ä¸Šå‡ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›åŸºçº¿æ¨¡å‹ã€‚å®éªŒç»“æœå……åˆ†éªŒè¯äº† LacaDM åœ¨å¤„ç†å¤æ‚å¤šç›®æ ‡ä»»åŠ¡æ—¶çš„æœ‰æ•ˆæ€§ï¼Œä¸ºè§£å†³é«˜ç»´çŠ¶æ€åŠ¨ä½œç©ºé—´ä¸‹çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19516v1",
      "published_date": "2025-12-22 16:08:03 UTC",
      "updated_date": "2025-12-22 16:08:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:05:46.589462+00:00"
    },
    {
      "arxiv_id": "2512.19512v2",
      "title": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation",
      "title_zh": "Anatomy-R1ï¼šé€šè¿‡è§£å‰–ç›¸ä¼¼æ€§è¯¾ç¨‹ä¸ç¾¤ä½“å¤šæ ·æ€§å¢å¼ºæå‡å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§£å‰–æ¨ç†èƒ½åŠ›",
      "authors": [
        "Ziyang Song",
        "Zelin Zang",
        "Zuyao Chen",
        "Xusheng Liang",
        "Dong Yi",
        "Jinlin Wu",
        "Hongbin Liu",
        "Jiebo Luo",
        "Zhen. Lei"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Anatomy-R1ï¼Œæ—¨åœ¨å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ä¸´åºŠè§£å‰–æ‰‹æœ¯å›¾åƒä¸­çš„æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹Group Relative Policy Optimization (GRPO) åœ¨è§£å‰–ç»“æ„è¯†åˆ«ä¸­å­˜åœ¨çš„çŸ¥è¯†å…±äº«ä¸è¶³åŠæ¨ç†è·¯å¾„å•ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸¤ç§åˆ›æ–°æ–¹æ¡ˆã€‚é¦–å…ˆï¼Œé€šè¿‡è§£å‰–ç›¸ä¼¼æ€§è¯¾ç¨‹å­¦ä¹ ï¼ˆAnatomical Similarity Curriculum Learningï¼‰ç­–ç•¥ï¼Œåˆ©ç”¨é€‰é¡¹ç›¸ä¼¼åº¦åŠ¨æ€è°ƒèŠ‚é—®é¢˜éš¾åº¦ï¼Œå¸®åŠ©æ¨¡å‹ç”±æµ…å…¥æ·±åœ°ç†è§£å¤æ‚è§£å‰–ç»“æ„ã€‚å…¶æ¬¡ï¼Œå¼•å…¥ç¾¤ä½“å¤šæ ·æ€§é—®é¢˜å¢å¼ºï¼ˆGroup Diversity Question Augmentationï¼‰æŠ€æœ¯ï¼Œæ‰©å±•äº†æ¨¡å‹å¯¹å›°éš¾æŸ¥è¯¢çš„æœç´¢ç©ºé—´ï¼Œæœ‰æ•ˆè§£å†³äº†æ¨ç†è¿‡ç¨‹ä¸­çš„å¤šæ ·æ€§ç¼ºå¤±é—®é¢˜ã€‚å®éªŒåœ¨SGG-VQAå’ŒOmniMedVQAåŸºå‡†ä¸Šè¿›è¡Œäº†éªŒè¯ï¼Œç»“æœè¡¨æ˜Anatomy-R1æ˜¾è‘—æå‡äº†MLLMsçš„åŒ»ç–—æ¨ç†æ€§èƒ½ï¼Œä¸ºè§£å†³å¤æ‚åŒ»ç–—æ•°æ®ä¸‹çš„æ¨¡å‹è®­ç»ƒæä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19512v2",
      "published_date": "2025-12-22 16:06:36 UTC",
      "updated_date": "2025-12-24 05:32:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:05:44.677279+00:00"
    },
    {
      "arxiv_id": "2512.19506v1",
      "title": "DK-STN: A Domain Knowledge Embedded Spatio-Temporal Network Model for MJO Forecast",
      "title_zh": "DK-STNï¼šä¸€ç§åµŒå…¥é¢†åŸŸçŸ¥è¯†çš„æ—¶ç©ºç½‘ç»œ MJO é¢„æµ‹æ¨¡å‹",
      "authors": [
        "Hongliang Li",
        "Nong Zhang",
        "Zhewen Xu",
        "Xiang Li",
        "Changzheng Liu",
        "Chongbo Zhao",
        "Jie Wu"
      ],
      "abstract": "Understanding and predicting the Madden-Julian Oscillation (MJO) is fundamental for precipitation forecasting and disaster prevention. To date, long-term and accurate MJO prediction has remained a challenge for researchers. Conventional MJO prediction methods using Numerical Weather Prediction (NWP) are resource-intensive, time-consuming, and highly unstable (most NWP methods are sensitive to seasons, with better MJO forecast results in winter). While existing Artificial Neural Network (ANN) methods save resources and speed forecasting, their accuracy never reaches the 28 days predicted by the state-of-the-art NWP method, i.e., the operational forecasts from ECMWF, since neural networks cannot handle climate data effectively. In this paper, we present a Domain Knowledge Embedded Spatio-Temporal Network (DK-STN), a stable neural network model for accurate and efficient MJO forecasting. It combines the benefits of NWP and ANN methods and successfully improves the forecast accuracy of ANN methods while maintaining a high level of efficiency and stability. We begin with a spatial-temporal network (STN) and embed domain knowledge in it using two key methods: (i) applying a domain knowledge enhancement method and (ii) integrating a domain knowledge processing method into network training. We evaluated DK-STN with the 5th generation of ECMWF reanalysis (ERA5) data and compared it with ECMWF. Given 7 days of climate data as input, DK-STN can generate reliable forecasts for the following 28 days in 1-2 seconds, with an error of only 2-3 days in different seasons. DK-STN significantly exceeds ECMWF in that its forecast accuracy is equivalent to ECMWF's, while its efficiency and stability are significantly superior.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Madden-Julian Oscillation (MJO) é¢„æµ‹ä¸­æ•°å€¼å¤©æ°”é¢„æŠ¥ (NWP) èµ„æºæ¶ˆè€—å¤§ã€ç¨³å®šæ€§å·®ä»¥åŠä¼ ç»Ÿäººå·¥ç¥ç»ç½‘ç»œ (ANN) é¢„æµ‹ç²¾åº¦ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§é¢†åŸŸçŸ¥è¯†åµŒå…¥çš„æ—¶ç©ºç½‘ç»œæ¨¡å‹ (DK-STN)ã€‚è¯¥æ¨¡å‹ç»“åˆäº† NWP å’Œ ANN çš„ä¼˜åŠ¿ï¼Œé€šè¿‡åœ¨æ—¶ç©ºç½‘ç»œ (STN) ä¸­åº”ç”¨é¢†åŸŸçŸ¥è¯†å¢å¼ºæ–¹æ³•ï¼Œå¹¶å°†é¢†åŸŸçŸ¥è¯†å¤„ç†é›†æˆåˆ°ç½‘ç»œè®­ç»ƒä¸­ï¼Œæœ‰æ•ˆæå‡äº†ç¥ç»ç½‘ç»œå¤„ç†æ°”å€™æ•°æ®çš„èƒ½åŠ›ã€‚åŸºäº ERA5 æ•°æ®é›†çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDK-STN ä»…éœ€ 7 å¤©çš„æ°”è±¡æ•°æ®ä½œä¸ºè¾“å…¥ï¼Œå³å¯åœ¨ 1-2 ç§’å†…ç”Ÿæˆæœªæ¥ 28 å¤©çš„å¯é é¢„æŠ¥ï¼Œä¸”åœ¨ä¸åŒå­£èŠ‚é—´çš„é¢„æµ‹è¯¯å·®ä»…ä¸º 2-3 å¤©ã€‚ä¸ç›®å‰çš„åŸºå‡†æ¨¡å‹ ECMWF ç›¸æ¯”ï¼ŒDK-STN åœ¨ä¿æŒåŒç­‰é¢„æµ‹ç²¾åº¦çš„åŒæ—¶ï¼Œå…¶è®¡ç®—æ•ˆç‡ä¸è·¨å­£èŠ‚çš„ç¨³å®šæ€§æ˜¾è‘—ä¼˜äºåè€…ï¼Œä¸ºé•¿æœŸçš„æ°”è±¡ç¾å®³é¢„é˜²å’Œé™æ°´é¢„æŠ¥æä¾›äº†é«˜æ•ˆä¸”ç¨³å®šçš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.19506v1",
      "published_date": "2025-12-22 16:00:55 UTC",
      "updated_date": "2025-12-22 16:00:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:05:43.469651+00:00"
    },
    {
      "arxiv_id": "2512.19494v1",
      "title": "Kolmogorov-Arnold Graph Neural Networks Applied to Inorganic Nanomaterials Dataset",
      "title_zh": "Kolmogorov-Arnold å›¾ç¥ç»ç½‘ç»œåœ¨æ— æœºçº³ç±³ææ–™æ•°æ®é›†ä¸Šçš„åº”ç”¨",
      "authors": [
        "Nikita Volzhin",
        "Soowhan Yoon"
      ],
      "abstract": "The recent development of Kolmogorov-Arnold Networks (KANs) introduced new discoveries in the field of Graph Neural Networks (GNNs), expanding the existing set of models with KAN-based versions of GNNs, which often surpass the accuracy of MultiLayer Perceptron (MLP)-based GNNs. These models were widely tested on the graph datasets consisting of organic molecules; however, those studies disregarded the inorganic nanomaterials datasets. In this work, we close this gap by applying Kolmogorov-Arnold Graph Neural Networks (KAGNNs) to a recently published large inorganic nanomaterials dataset called CHILI. For this, we adapt and test KAGNNs appropriate for this dataset. Our experiments reveal that on the CHILI datasets, particularly on the CHILI-3K, KAGNNs substantially surpass conventional GNNs in classification, achieving state-of-the-art results.",
      "tldr_zh": "è¯¥ç ”ç©¶å°† Kolmogorov-Arnold Graph Neural Networks (KAGNNs) é¦–æ¬¡åº”ç”¨äºæ— æœºçº³ç±³ææ–™é¢†åŸŸï¼Œæœ‰æ•ˆå¡«è¡¥äº†æ­¤ç±»æ¨¡å‹æ­¤å‰ä¸»è¦åœ¨æœ‰æœºåˆ†å­æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°çš„ç ”ç©¶ç©ºç™½ã€‚é€šè¿‡åœ¨æœ€æ–°å‘å¸ƒçš„å¤§è§„æ¨¡æ— æœºçº³ç±³ææ–™æ•°æ®é›† CHILI ä¸Šå¯¹ KAGNNs è¿›è¡Œé€‚é…ä¸æµ‹è¯•ï¼Œç ”ç©¶è€…æ·±å…¥æ¢è®¨äº†å…¶åœ¨å¤„ç†å¤æ‚æ— æœºç»“æ„æ—¶çš„åˆ†ç±»æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ CHILI-3K ç­‰å­æ•°æ®é›†ä¸Šï¼ŒKAGNNs çš„è¡¨ç°æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ Graph Neural Networks (GNNs)ï¼ŒæˆåŠŸè¾¾åˆ°äº† state-of-the-art æ°´å¹³ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†åŸºäº Kolmogorov-Arnold Networks (KANs) çš„æ¶æ„åœ¨æ— æœºææ–™ç§‘å­¦ä»»åŠ¡ä¸­ç›¸æ¯”ä¼ ç»Ÿçš„ MultiLayer Perceptron (MLP) æ¶æ„å…·æœ‰æ›´å¼ºçš„è¡¨å¾èƒ½åŠ›å’Œåº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19494v1",
      "published_date": "2025-12-22 15:49:24 UTC",
      "updated_date": "2025-12-22 15:49:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:06:00.499345+00:00"
    },
    {
      "arxiv_id": "2512.19481v1",
      "title": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis",
      "title_zh": "ä½¿ç”¨ GPT-5 è¿›è¡Œä»£ç å˜æ›´å½±å“åˆ†æçš„æ•°æ®é›†ä¸åˆæ­¥ç ”ç©¶",
      "authors": [
        "Katharina Stengg",
        "Christian Macho",
        "Martin Pinzger"
      ],
      "abstract": "Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ„å»ºäº†ä¸€ä¸ªä¸“é—¨çš„æ•°æ®é›†å¹¶åˆæ­¥æ¢è®¨äº†åˆ©ç”¨ GPT-5 è¿›è¡Œä»£ç å˜æ›´å½±å“åˆ†æ (Code-change Impact Analysis) çš„èƒ½åŠ›ã€‚è¯¥æ•°æ®é›†å¡«è¡¥äº†ç°æœ‰æ•°æ®åœ¨åˆå§‹å˜æ›´ (seed-changes) å’Œå—å½±å“ä»£ç å®ä½“ä¿¡æ¯æ–¹é¢çš„ç©ºç™½ï¼ŒåŒ…å«äº†å˜æ›´å¯¹ (change pairs) å’Œå˜æ›´ç±»å‹ç­‰å…³é”®æ•°æ®ã€‚ç ”ç©¶é€šè¿‡ä¸¤ç§å®éªŒé…ç½®è¯„ä¼°äº† GPT-5 å’Œ GPT-5-mini çš„é¢„æµ‹è¡¨ç°ï¼Œåˆ†åˆ«æä¾›äº†åˆå§‹å˜æ›´ä¿¡æ¯ã€çˆ¶æäº¤æ ‘ (parent commit tree) ä»¥åŠå·®å¼‚å— (diff hunk) ç­‰ä¸Šä¸‹æ–‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸¤ä¸ªæ¨¡å‹åœ¨é¢„æµ‹å—å½±å“å®ä½“æ–¹é¢çš„æ•´ä½“è¡¨ç°å‡ä¸ç†æƒ³ï¼Œä½† GPT-5 çš„è¡¨ç°ä¼˜äº GPT-5-miniã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°æä¾›ä»£ç çš„å·®å¼‚å— (diff hunks) æœ‰åŠ©äºæ¨¡å‹ç•¥å¾®æå‡å…¶åˆ†ææ€§èƒ½ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†å½“å‰å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†å¤æ‚ä»£ç å˜æ›´ç†è§£ä»»åŠ¡ä¸­çš„å±€é™æ€§ï¼Œå¹¶ä¸ºè¯¥é¢†åŸŸçš„è¿›ä¸€æ­¥ç ”ç©¶æä¾›äº†åŸºå‡†ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "6 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.19481v1",
      "published_date": "2025-12-22 15:32:45 UTC",
      "updated_date": "2025-12-22 15:32:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:06:12.322000+00:00"
    },
    {
      "arxiv_id": "2512.19472v1",
      "title": "Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications",
      "title_zh": "é¢å‘åˆ†å¸ƒå¤–æ ·æœ¬ã€å¯¹æŠ—æ”»å‡»åŠåˆ†å¸ƒå†…è¯¯åˆ†ç±»æ£€æµ‹çš„å¤šå±‚ç½®ä¿¡åº¦è¯„åˆ†",
      "authors": [
        "Lorenzo Capelli",
        "Leandro de Souza Rosa",
        "Gianluca Setti",
        "Mauro Mangia",
        "Riccardo Rovatti"
      ],
      "abstract": "The recent explosive growth in Deep Neural Networks applications raises concerns about the black-box usage of such models, with limited trasparency and trustworthiness in high-stakes domains, which have been crystallized as regulatory requirements such as the European Union Artificial Intelligence Act. While models with embedded confidence metrics have been proposed, such approaches cannot be applied to already existing models without retraining, limiting their broad application. On the other hand, post-hoc methods, which evaluate pre-trained models, focus on solving problems related to improving the confidence in the model's predictions, and detecting Out-Of-Distribution or Adversarial Attacks samples as independent applications. To tackle the limited applicability of already existing methods, we introduce Multi-Layer Analysis for Confidence Scoring (MACS), a unified post-hoc framework that analyzes intermediate activations to produce classification-maps. From the classification-maps, we derive a score applicable for confidence estimation, detecting distributional shifts and adversarial attacks, unifying the three problems in a common framework, and achieving performances that surpass the state-of-the-art approaches in our experiments with the VGG16 and ViTb16 models with a fraction of their computational overhead.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDeep Neural Networksï¼‰åœ¨å…³é”®é¢†åŸŸåº”ç”¨ä¸­ç¼ºä¹é€æ˜åº¦å’Œå¯é æ€§çš„é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨åº”å¯¹æ¬§ç›Ÿäººå·¥æ™ºèƒ½æ³•æ¡ˆï¼ˆEU AI Actï¼‰ç­‰ç›‘ç®¡è¦æ±‚çš„èƒŒæ™¯ä¸‹ã€‚ç”±äºç°æœ‰çš„åµŒå…¥å¼åº¦é‡æ–¹æ³•å¾€å¾€éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œè€Œäº‹åï¼ˆpost-hocï¼‰æ–¹æ³•é€šå¸¸å°†ç½®ä¿¡åº¦æå‡ã€åˆ†å¸ƒå¤–ï¼ˆOut-Of-Distributionï¼‰æ£€æµ‹å’Œå¯¹æŠ—æ€§æ”»å‡»ï¼ˆAdversarial Attacksï¼‰æ£€æµ‹è§†ä¸ºç‹¬ç«‹é—®é¢˜ï¼Œä½œè€…æå‡ºäº†åä¸ºå¤šå±‚ç½®ä¿¡åº¦è¯„åˆ†åˆ†æï¼ˆMulti-Layer Analysis for Confidence Scoring, MACSï¼‰çš„ç»Ÿä¸€æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†ææ¨¡å‹çš„ä¸­é—´å±‚æ¿€æ´»å€¼æ¥ç”Ÿæˆåˆ†ç±»æ˜ å°„å›¾ï¼ˆclassification-mapsï¼‰ï¼Œå¹¶æ®æ­¤æ¨å¯¼å‡ºé€‚ç”¨äºç½®ä¿¡åº¦ä¼°è®¡ã€åˆ†å¸ƒåç¦»åŠå¯¹æŠ—æ”»å‡»è¯†åˆ«çš„ç»¼åˆè¯„åˆ†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMACS åœ¨ VGG16 å’Œ ViTb16 æ¨¡å‹ä¸Šçš„æ€§èƒ½è¶…è¿‡äº†ç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ï¼Œä¸”ä»…éœ€æ¨¡å‹æ¨ç†å¼€é”€çš„ä¸€å°éƒ¨åˆ†ã€‚è¿™ä¸€æˆæœè¯æ˜äº†é€šè¿‡ç»Ÿä¸€æ¡†æ¶å¤„ç†å¤šç§å®‰å…¨æŒ‘æˆ˜çš„å¯è¡Œæ€§ï¼Œä¸ºå¢å¼ºé¢„è®­ç»ƒæ¨¡å‹çš„å¯ä¿¡åº¦æä¾›äº†é«˜æ•ˆä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19472v1",
      "published_date": "2025-12-22 15:25:10 UTC",
      "updated_date": "2025-12-22 15:25:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:05:59.639039+00:00"
    },
    {
      "arxiv_id": "2512.19458v1",
      "title": "An Agentic Framework for Autonomous Materials Computation",
      "title_zh": "è‡ªä¸»ææ–™è®¡ç®—çš„æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Zeyu Xia",
        "Jinzhe Ma",
        "Congjie Zheng",
        "Shufei Zhang",
        "Yuqiang Li",
        "Hang Su",
        "P. Hu",
        "Changshui Zhang",
        "Xingao Gong",
        "Wanli Ouyang",
        "Lei Bai",
        "Dongzhan Zhou",
        "Mao Su"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ—¨åœ¨å®ç°ç¬¬ä¸€æ€§åŸç†ææ–™è®¡ç®—ï¼ˆfirst-principles materials computationsï¼‰å¯é è‡ªåŠ¨åŒ–çš„é¢†åŸŸä¸“ç”¨æ™ºèƒ½ä½“æ¡†æ¶ï¼Œæœ‰æ•ˆè§£å†³äº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç§‘å­¦å‘ç°ä¸­é¢ä¸´çš„çŸ¥è¯†é™æ€åŒ–å’Œå¹»è§‰é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡æ·±åº¦åµŒå…¥é¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼Œç¡®ä¿äº†å¤æ‚å¤šæ­¥å·¥ä½œæµåœ¨ç‰©ç†é€»è¾‘ä¸Šçš„è¿è´¯æ€§ï¼Œå¹¶èƒ½å¤Ÿè‡ªä¸»é€‰æ‹©æ”¶æ•›ä¸”å®šä¹‰è‰¯å¥½çš„è®¡ç®—å‚æ•°ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¤„ç†å¤šæ ·åŒ–è®¡ç®—ä»»åŠ¡çš„åŸºå‡†æµ‹è¯•ä¸­ï¼Œå…¶å‡†ç¡®æ€§å’Œé²æ£’æ€§å‡å¤§å¹…è¶…è¶Šäº†ç‹¬ç«‹çš„ LLMsã€‚è¿™ä¸€æˆæœä¸ºè‡ªä¸»è®¡ç®—å®éªŒå¥ å®šäº†å¯éªŒè¯çš„åŸºç¡€ï¼Œå¹¶å±•ç¤ºäº†æ™ºèƒ½ä½“åœ¨ç«¯åˆ°ç«¯ç§‘å­¦å·¥ä½œæµä¸­çš„æ‰§è¡Œå¯é æ€§ã€‚è¯¥å·¥ä½œæ ‡å¿—ç€åœ¨å®ç°å…¨è‡ªåŠ¨åŒ–ç§‘å­¦å‘ç°çš„è¿›ç¨‹ä¸­è¿ˆå‡ºäº†å…³é”®ä¸€æ­¥ã€‚",
      "categories": [
        "cs.AI",
        "cond-mat.mtrl-sci"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19458v1",
      "published_date": "2025-12-22 15:03:57 UTC",
      "updated_date": "2025-12-22 15:03:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:06:04.796692+00:00"
    },
    {
      "arxiv_id": "2512.19777v1",
      "title": "Learned Digital Codes for Over-the-Air Computation in Federated Edge Learning",
      "title_zh": "è”é‚¦è¾¹ç¼˜å­¦ä¹ ä¸­é¢å‘ç©ºä¸­è®¡ç®—çš„å­¦ä¹ å‹æ•°å­—ç¼–ç ",
      "authors": [
        "Antonio Tarizzo",
        "Mohammad Kazemi",
        "Deniz GÃ¼ndÃ¼z"
      ],
      "abstract": "Federated edge learning (FEEL) enables wireless devices to collaboratively train a centralised model without sharing raw data, but repeated uplink transmission of model updates makes communication the dominant bottleneck. Over-the-air (OTA) aggregation alleviates this by exploiting the superposition property of the wireless channel, enabling simultaneous transmission and merging communication with computation. Digital OTA schemes extend this principle by incorporating the robustness of conventional digital communication, but current designs remain limited in low signal-to-noise ratio (SNR) regimes. This work proposes a learned digital OTA framework that improves recovery accuracy, convergence behaviour, and robustness to challenging SNR conditions while maintaining the same uplink overhead as state-of-the-art methods. The design integrates an unsourced random access (URA) codebook with vector quantisation and AMP-DA-Net, an unrolled approximate message passing (AMP)-style decoder trained end-to-end with the digital codebook and parameter server local training statistics. The proposed design extends OTA aggregation beyond averaging to a broad class of symmetric functions, including trimmed means and majority-based rules. Experiments on highly heterogeneous device datasets and varying numbers of active devices show that the proposed design extends reliable digital OTA operation by more than 10 dB into low SNR regimes while matching or improving performance across the full SNR range. The learned decoder remains effective under message corruption and nonlinear aggregation, highlighting the broader potential of end-to-end learned design for digital OTA communication in FEEL.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§å­¦ä¹ å‹æ•°å­—ç©ºä¸­è®¡ç®— (Learned Digital OTA) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è”é‚¦è¾¹ç¼˜å­¦ä¹  (Federated Edge Learning, FEEL) åœ¨ä½ä¿¡å™ªæ¯” (SNR) ç¯å¢ƒä¸‹çš„é€šä¿¡ç“¶é¢ˆé—®é¢˜ã€‚è¯¥æ¡†æ¶é›†æˆäº†éæˆæƒéšæœºæ¥å…¥ (Unsourced Random Access, URA) ç æœ¬ä¸çŸ¢é‡é‡åŒ– (Vector Quantization) æŠ€æœ¯ï¼Œå¹¶é‡‡ç”¨äº†åä¸º AMP-DA-Net çš„å±•å¼€è¿‘ä¼¼æ¶ˆæ¯ä¼ é€’ (AMP) é£æ ¼è§£ç å™¨ã€‚é€šè¿‡ç«¯åˆ°ç«¯è®­ç»ƒï¼Œè¯¥è®¾è®¡å°†ç©ºä¸­èšåˆ (OTA Aggregation) çš„åŠŸèƒ½ä»ç®€å•çš„æ±‚å’Œå¹³å‡æ‰©å±•åˆ°äº†åŒ…æ‹¬ä¿®æ•´å¹³å‡ (Trimmed Means) å’Œå¤šæ•°ç±»è§„åˆ™ (Majority-based rules) åœ¨å†…çš„å¹¿æ³›å¯¹ç§°å‡½æ•°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ¡ˆåœ¨é«˜åº¦å¼‚æ„çš„æ•°æ®é›†å’Œä¸åŒæ•°é‡çš„æ´»è·ƒè®¾å¤‡åœºæ™¯ä¸‹ï¼Œèƒ½å¤Ÿå°†å¯é çš„æ•°å­—ç©ºä¸­è®¡ç®—æ“ä½œåœ¨ä½ SNR åŒºåŸŸæ‰©å±• 10 dB ä»¥ä¸Šï¼ŒåŒæ—¶åŒ¹é…æˆ–æå‡äº†å…¨ SNR èŒƒå›´å†…çš„æ€§èƒ½ã€‚è¯¥å­¦ä¹ å‹è§£ç å™¨åœ¨æ¶ˆæ¯æŸåå’Œéçº¿æ€§èšåˆä¸‹ä¾ç„¶ä¿æŒç¨³å¥ï¼Œå±•ç¤ºäº†ç«¯åˆ°ç«¯å­¦ä¹ è®¾è®¡åœ¨æ•°å­—ç©ºä¸­è®¡ç®—é€šä¿¡é¢†åŸŸçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.IT",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19777v1",
      "published_date": "2025-12-22 15:01:41 UTC",
      "updated_date": "2025-12-22 15:01:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:06:01.024856+00:00"
    },
    {
      "arxiv_id": "2512.19456v1",
      "title": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations",
      "title_zh": "æ¿€æ´»å³ç‰¹å¾ï¼šæ¢ç©¶å¤§è¯­è¨€æ¨¡å‹ä¸­å¯æ³›åŒ–çš„ä½œæ–‡è¯„åˆ†è¡¨ç¤º",
      "authors": [
        "Jinwei Chi",
        "Ke Wang",
        "Yu Chen",
        "Xuanye Lin",
        "Qiang Xu"
      ],
      "abstract": "Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è·¨æç¤ºè¯(cross-prompt)åœºæ™¯ä¸‹è‡ªåŠ¨ä½œæ–‡è¯„åˆ†(Automated Essay Scoring, AES)æ ‡å‡†å¤šæ ·æ€§çš„æŒ‘æˆ˜ï¼Œæ¢ç´¢äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­é—´å±‚æ¿€æ´»(activations)åœ¨è¯„ä¼°ä½œæ–‡è´¨é‡ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶è€…é€šè¿‡æ¿€æ´»ç‰¹å¾æ‹Ÿåˆæ¢æµ‹å™¨(probes)ï¼Œåˆ†æäº†ä¸åŒæ¨¡å‹åŠè¾“å…¥å†…å®¹å¯¹ç‰¹å¾åŒºåˆ†èƒ½åŠ›çš„å½±å“ã€‚é€šè¿‡è®¡ç®—ä¸åŒæç¤ºè¯ä¸‹ä½œæ–‡åœ¨å„ç‰¹è´¨ç»´åº¦ä¸Šçš„æ–¹å‘ï¼Œè¯¥ç ”ç©¶åˆ†æäº†æ¨¡å‹è¯„ä¼°è§†è§’éšä½œæ–‡ç±»å‹å’Œç‰¹è´¨çš„å˜åŒ–è§„å¾‹ã€‚ç»“æœè¡¨æ˜ï¼Œä¸­é—´å±‚æ¿€æ´»åœ¨è¡¡é‡ä½œæ–‡è´¨é‡æ–¹é¢è¡¨ç°å‡ºæå¼ºçš„åŒºåˆ†èƒ½åŠ›ã€‚å®éªŒè¿›ä¸€æ­¥è¯å®LLMsèƒ½æ ¹æ®ä¸åŒç‰¹è´¨å’Œä½œæ–‡ç±»å‹çµæ´»è°ƒæ•´è¯„ä¼°è§†è§’ï¼Œä»è€Œæœ‰æ•ˆåº”å¯¹è·¨æç¤ºè¯è®¾ç½®ä¸‹çš„è¯„åˆ†å·®å¼‚ã€‚è¯¥å·¥ä½œä¸ºåˆ©ç”¨æ¨¡å‹å†…éƒ¨è¡¨å¾æ„å»ºæ›´å…·æ³›åŒ–æ€§çš„è‡ªåŠ¨è¯„åˆ†ç³»ç»Ÿæä¾›äº†æœ‰åŠ›è¯æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19456v1",
      "published_date": "2025-12-22 15:01:07 UTC",
      "updated_date": "2025-12-22 15:01:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:06:21.277729+00:00"
    },
    {
      "arxiv_id": "2601.03273v1",
      "title": "GuardEval: A Multi-Perspective Benchmark for Evaluating Safety, Fairness, and Robustness in LLM Moderators",
      "title_zh": "GuardEvalï¼šè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹å®¡æ ¸å™¨å®‰å…¨æ€§ã€å…¬å¹³æ€§ä¸é²æ£’æ€§çš„å¤šè§†è§’åŸºå‡†",
      "authors": [
        "Naseem Machlovi",
        "Maryam Saleki",
        "Ruhul Amin",
        "Mohamed Rahouti",
        "Shawqi Al-Maliki",
        "Junaid Qadir",
        "Mohamed M. Abdallah",
        "Ala Al-Fuqaha"
      ],
      "abstract": "As large language models (LLMs) become deeply embedded in daily life, the urgent need for safer moderation systems, distinguishing between naive from harmful requests while upholding appropriate censorship boundaries, has never been greater. While existing LLMs can detect harmful or unsafe content, they often struggle with nuanced cases such as implicit offensiveness, subtle gender and racial biases, and jailbreak prompts, due to the subjective and context-dependent nature of these issues. Furthermore, their heavy reliance on training data can reinforce societal biases, resulting in inconsistent and ethically problematic outputs. To address these challenges, we introduce GuardEval, a unified multi-perspective benchmark dataset designed for both training and evaluation, containing 106 fine-grained categories spanning human emotions, offensive and hateful language, gender and racial bias, and broader safety concerns. We also present GemmaGuard (GGuard), a QLoRA fine-tuned version of Gemma3-12B trained on GuardEval, to assess content moderation with fine-grained labels. Our evaluation shows that GGuard achieves a macro F1 score of 0.832, substantially outperforming leading moderation models, including OpenAI Moderator (0.64) and Llama Guard (0.61). We show that multi-perspective, human-centered safety benchmarks are critical for reducing biased and inconsistent moderation decisions. GuardEval and GGuard together demonstrate that diverse, representative data materially improve safety, fairness, and robustness on complex, borderline cases.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å¤„ç†éšå«æ”»å‡»æ€§ã€ç¤¾ä¼šåè§å’Œè¶Šç‹±æç¤ºï¼ˆjailbreak promptsï¼‰ç­‰å¤æ‚å†…å®¹å®¡æ ¸ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œæå‡ºäº† GuardEvalã€‚ä½œä¸ºä¸€ä¸ªç»Ÿä¸€çš„å¤šç»´åº¦åŸºå‡†æ•°æ®é›†ï¼ŒGuardEval åŒ…å« 106 ä¸ªæ¶µç›–æƒ…æ„Ÿã€åè§åŠå®‰å…¨é—®é¢˜çš„ç»†ç²’åº¦åˆ†ç±»ï¼Œæ—¨åœ¨æå‡å®¡æ ¸ç³»ç»Ÿçš„è®­ç»ƒä¸è¯„ä¼°è´¨é‡ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨è¯¥æ•°æ®é›†é€šè¿‡ QLoRA æŠ€æœ¯å¯¹ Gemma3-12B è¿›è¡Œå¾®è°ƒï¼Œå¼€å‘äº†ä¸“é—¨çš„å®¡æ ¸æ¨¡å‹ GemmaGuard (GGuard)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒGGuard å–å¾—äº† 0.832 çš„å®è§‚ F1 åˆ†æ•°ï¼ˆmacro F1 scoreï¼‰ï¼Œå…¶æ€§èƒ½å¤§å¹…è¶…è¶Šäº† OpenAI Moderator å’Œ Llama Guardã€‚è¿™ä¸€æˆæœè¯æ˜äº†å¤šç»´åº¦ã€ä»¥äººä¸ºæœ¬çš„å®‰å…¨åŸºå‡†èƒ½æœ‰æ•ˆæ”¹å–„å®¡æ ¸å†³ç­–çš„åè§é—®é¢˜ï¼Œå¹¶æ˜¾è‘—æå‡ç³»ç»Ÿåœ¨å¤æ‚è¾¹ç•Œæ¡ˆä¾‹ä¸­çš„å…¬å¹³æ€§ï¼ˆfairnessï¼‰ä¸ç¨³å¥æ€§ï¼ˆrobustnessï¼‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.03273v1",
      "published_date": "2025-12-22 14:49:28 UTC",
      "updated_date": "2025-12-22 14:49:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:06:22.491046+00:00"
    },
    {
      "arxiv_id": "2512.19438v1",
      "title": "MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation",
      "title_zh": "MT-Markï¼šåŸºäºäº’æ•™å¸ˆåä½œä¸è‡ªé€‚åº”ç‰¹å¾è°ƒåˆ¶çš„å›¾åƒæ°´å°é‡æ–°æ€è€ƒ",
      "authors": [
        "Fei Ge",
        "Ying Huang",
        "Jie Liu",
        "Guixuan Zhang",
        "Zhi Zeng",
        "Shuwu Zhang",
        "Hu Guan"
      ],
      "abstract": "Existing deep image watermarking methods follow a fixed embedding-distortion-extraction pipeline, where the embedder and extractor are weakly coupled through a final loss and optimized in isolation. This design lacks explicit collaboration, leaving no structured mechanism for the embedder to incorporate decoding-aware cues or for the extractor to guide embedding during training. To address this architectural limitation, we rethink deep image watermarking by reformulating embedding and extraction as explicitly collaborative components. To realize this reformulation, we introduce a Collaborative Interaction Mechanism (CIM) that establishes direct, bidirectional communication between the embedder and extractor, enabling a mutual-teacher training paradigm and coordinated optimization. Built upon this explicitly collaborative architecture, we further propose an Adaptive Feature Modulation Module (AFMM) to support effective interaction. AFMM enables content-aware feature regulation by decoupling modulation structure and strength, guiding watermark embedding toward stable image features while suppressing host interference during extraction. Under CIM, the AFMMs on both sides form a closed-loop collaboration that aligns embedding behavior with extraction objectives. This architecture-level redesign changes how robustness is learned in watermarking systems. Rather than relying on exhaustive distortion simulation, robustness emerges from coordinated representation learning between embedding and extraction. Experiments on real-world and AI-generated datasets demonstrate that the proposed method consistently outperforms state-of-the-art approaches in watermark extraction accuracy while maintaining high perceptual quality, showing strong robustness and generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†MT-Markï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ·±åº¦å›¾åƒæ°´å°æ–¹æ³•ä¸­åµŒå…¥å™¨(embedder)ä¸æå–å™¨(extractor)è€¦åˆç¨‹åº¦ä½ã€ç¼ºä¹æ˜¾å¼åä½œæœºåˆ¶çš„é—®é¢˜ã€‚ä¸ºäº†é‡æ–°å®šä¹‰å›¾åƒæ°´å°æ¶æ„ï¼Œç ”ç©¶å¼•å…¥äº†åä½œäº¤äº’æœºåˆ¶(Collaborative Interaction Mechanism, CIM)ï¼Œåœ¨åµŒå…¥å™¨å’Œæå–å™¨ä¹‹é—´å»ºç«‹ç›´æ¥çš„åŒå‘é€šä¿¡ï¼Œå®ç°äº†äº’åŠ©æ•™å¸ˆ(Mutual-Teacher)è®­ç»ƒèŒƒå¼ã€‚åŸºäºæ­¤åä½œæ¶æ„ï¼Œç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†è‡ªé€‚åº”ç‰¹å¾è°ƒåˆ¶æ¨¡å—(Adaptive Feature Modulation Module, AFMM)ï¼Œé€šè¿‡è§£è€¦è°ƒåˆ¶ç»“æ„ä¸å¼ºåº¦æ¥å®ç°å†…å®¹æ„ŸçŸ¥çš„ç‰¹å¾è°ƒèŠ‚ã€‚è¯¥æ¨¡å—èƒ½å¼•å¯¼æ°´å°åµŒå…¥åˆ°ç¨³å®šçš„å›¾åƒç‰¹å¾ä¸­ï¼Œå¹¶åœ¨æå–é˜¶æ®µæœ‰æ•ˆæŠ‘åˆ¶å®¿ä¸»å›¾åƒçš„å¹²æ‰°ï¼Œå½¢æˆé—­ç¯åä½œã€‚è¿™ç§æ¶æ„çº§çš„é‡æ–°è®¾è®¡ä½¿å¾—ç³»ç»Ÿé²æ£’æ€§(robustness)ä»åè°ƒçš„è¡¨å¾å­¦ä¹ ä¸­äº§ç”Ÿï¼Œè€Œéä»…ä¾èµ–äºç©·ä¸¾å¼çš„å¤±çœŸæ¨¡æ‹Ÿã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMT-Markåœ¨çœŸå®ä¸–ç•ŒåŠAIç”Ÿæˆçš„æ•°æ®é›†ä¸Šå‡ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œåœ¨ä¿æŒé«˜æ„ŸçŸ¥è´¨é‡çš„åŒæ—¶å®ç°äº†æé«˜çš„æå–å‡†ç¡®ç‡å’Œæ³›åŒ–æ€§èƒ½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19438v1",
      "published_date": "2025-12-22 14:36:08 UTC",
      "updated_date": "2025-12-22 14:36:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:07:08.165420+00:00"
    },
    {
      "arxiv_id": "2512.19428v1",
      "title": "Attention Is Not What You Need",
      "title_zh": "æ³¨æ„åŠ›å¹¶éä½ ä¹‹æ‰€éœ€",
      "authors": [
        "Zhang Chong"
      ],
      "abstract": "We revisit a basic question in sequence modeling: is explicit self-attention actually necessary for strong performance and reasoning? We argue that standard multi-head attention is best seen as a form of tensor lifting: hidden vectors are mapped into a high-dimensional space of pairwise interactions, and learning proceeds by constraining this lifted tensor through gradient descent. This mechanism is extremely expressive but mathematically opaque, because after many layers it becomes very hard to describe the model with a small family of explicit invariants.\n  To explore an alternative, we propose an attention-free architecture based on Grassmann flows. Instead of forming an L by L attention matrix, our Causal Grassmann layer (i) linearly reduces token states, (ii) encodes local token pairs as two-dimensional subspaces on a Grassmann manifold via Plucker coordinates, and (iii) fuses these geometric features back into the hidden states through gated mixing. Information therefore propagates by controlled deformations of low-rank subspaces over multi-scale local windows, so the core computation lives on a finite-dimensional manifold rather than in an unstructured tensor space.\n  On the Wikitext-2 language modeling benchmark, purely Grassmann-based models with 13 to 18 million parameters achieve validation perplexities within about 10 to 15 percent of size-matched Transformers. On the SNLI natural language inference task, a Grassmann-Plucker head on top of DistilBERT slightly outperforms a Transformer head, with best validation and test accuracies of 0.8550 and 0.8538 compared to 0.8545 and 0.8511. We analyze the complexity of Grassmann mixing, show linear scaling in sequence length for fixed rank, and argue that such manifold-based designs offer a more structured route toward geometric and invariant-based interpretations of neural reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åºåˆ—å»ºæ¨¡ä¸­æ˜¾å¼è‡ªæ³¨æ„åŠ›(self-attention)çš„å¿…è¦æ€§ï¼ŒæŒ‡å‡ºæ ‡å‡†çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æœ¬è´¨ä¸Šæ˜¯ä¸€ç§æ•°å­¦ä¸Šä¸é€æ˜çš„å¼ é‡æå‡(tensor lifting)è¿‡ç¨‹ã€‚ä¸ºäº†å¯»æ‰¾æ›¿ä»£æ–¹æ¡ˆï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºæ ¼æ‹‰æ–¯æ›¼æµ(Grassmann flows)çš„æ— æ³¨æ„åŠ›æ¶æ„ï¼Œåˆ©ç”¨Causal Grassmannå±‚å°†å±€éƒ¨æ ‡è®°å¯¹ç¼–ç ä¸ºGrassmannæµå½¢ä¸Šçš„äºŒç»´å­ç©ºé—´ã€‚è¯¥æ–¹æ³•é€šè¿‡Plucker coordinatesæ•æ‰å‡ ä½•ç‰¹å¾ï¼Œå¹¶ç»“åˆé—¨æ§æ··åˆ(gated mixing)å°†ä¿¡æ¯èåˆå›éšè—çŠ¶æ€ï¼Œä½¿æ ¸å¿ƒè®¡ç®—è¿è¡Œåœ¨æœ‰é™ç»´æµå½¢è€Œéæ— ç»“æ„çš„å¼ é‡ç©ºé—´ä¸­ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨Wikitext-2åŸºå‡†æµ‹è¯•ä¸­ï¼Œæ ¼æ‹‰æ–¯æ›¼æ¨¡å‹åœ¨å‚æ•°è§„æ¨¡ç›¸è¿‘çš„æƒ…å†µä¸‹è¾¾åˆ°äº†æ¥è¿‘Transformerçš„æ€§èƒ½ï¼›è€Œåœ¨SNLIè‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ä¸­ï¼ŒGrassmann-Pluckerå¤´éƒ¨çš„è¡¨ç°ç•¥ä¼˜äºTransformerå¤´éƒ¨ã€‚æ­¤å¤–ï¼Œè¯¥æ¶æ„åœ¨å›ºå®šç§©ä¸‹å®ç°äº†éšåºåˆ—é•¿åº¦çº¿æ€§ç¼©æ”¾(linear scaling)çš„å¤æ‚åº¦ï¼Œä¸ºç¥ç»æ¨ç†çš„å‡ ä½•ä¸ä¸å˜æ€§è§£é‡Šæä¾›äº†ä¸€æ¡æ›´å…·ç»“æ„åŒ–çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.AG"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19428v1",
      "published_date": "2025-12-22 14:29:18 UTC",
      "updated_date": "2025-12-22 14:29:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:07:39.329447+00:00"
    },
    {
      "arxiv_id": "2512.19410v1",
      "title": "Research Program: Theory of Learning in Dynamical Systems",
      "title_zh": "ç ”ç©¶çº²é¢†ï¼šåŠ¨åŠ›ç³»ç»Ÿä¸­çš„å­¦ä¹ ç†è®º",
      "authors": [
        "Elad Hazan",
        "Shai Shalev Shwartz",
        "Nathan Srebro"
      ],
      "abstract": "Modern learning systems increasingly interact with data that evolve over time and depend on hidden internal state. We ask a basic question: when is such a dynamical system learnable from observations alone? This paper proposes a research program for understanding learnability in dynamical systems through the lens of next-token prediction. We argue that learnability in dynamical systems should be studied as a finite-sample question, and be based on the properties of the underlying dynamics rather than the statistical properties of the resulting sequence. To this end, we give a formulation of learnability for stochastic processes induced by dynamical systems, focusing on guarantees that hold uniformly at every time step after a finite burn-in period. This leads to a notion of dynamic learnability which captures how the structure of a system, such as stability, mixing, observability, and spectral properties, governs the number of observations required before reliable prediction becomes possible. We illustrate the framework in the case of linear dynamical systems, showing that accurate prediction can be achieved after finite observation without system identification, by leveraging improper methods based on spectral filtering. We survey the relationship between learning in dynamical systems and classical PAC, online, and universal prediction theories, and suggest directions for studying nonlinear and controlled systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªæ—¨åœ¨ç†è§£åŠ¨åŠ›ç³»ç»Ÿ(Dynamical Systems)å¯å­¦ä¹ æ€§çš„ç ”ç©¶è®¡åˆ’ï¼Œé‡ç‚¹é€šè¿‡ä¸‹æ–‡é¢„æµ‹(Next-token prediction)çš„è§†è§’è¿›è¡Œæ¢è®¨ã€‚ä½œè€…æå‡ºåŠ¨åŠ›ç³»ç»Ÿçš„å¯å­¦ä¹ æ€§åº”è¢«è§†ä¸ºä¸€ä¸ªæœ‰é™æ ·æœ¬(Finite-sample)é—®é¢˜ï¼Œå…¶æ ¸å¿ƒåœ¨äºç ”ç©¶åº•å±‚åŠ¨åŠ›å­¦ç‰¹æ€§è€Œéç”Ÿæˆåºåˆ—çš„ç»Ÿè®¡å±æ€§å¯¹å­¦ä¹ çš„å½±å“ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶é’ˆå¯¹ç”±åŠ¨åŠ›ç³»ç»Ÿè¯±å¯¼çš„éšæœºè¿‡ç¨‹(Stochastic processes)ç»™å‡ºäº†å¯å­¦ä¹ æ€§çš„å…¬å¼åŒ–å®šä¹‰ï¼Œå¹¶å¼•å…¥äº†åŠ¨æ€å¯å­¦ä¹ æ€§(Dynamic learnability)æ¦‚å¿µï¼Œç”¨ä»¥åˆ»ç”»ç¨³å®šæ€§(Stability)ã€æ··åˆæ€§(Mixing)ã€å¯è§‚æµ‹æ€§(Observability)å’Œè°±ç‰¹æ€§(Spectral properties)ç­‰ç»“æ„å› ç´ å¦‚ä½•å†³å®šé¢„æµ‹æ‰€éœ€çš„è§‚æµ‹é—¨æ§›ã€‚åœ¨çº¿æ€§åŠ¨åŠ›ç³»ç»Ÿçš„å®ä¾‹ä¸­ï¼Œç ”ç©¶è¯æ˜äº†åˆ©ç”¨åŸºäºè°±æ»¤æ³¢(Spectral filtering)çš„æ–¹æ³•ï¼Œæ— éœ€è¿›è¡Œç³»ç»Ÿè¯†åˆ«(System identification)å³å¯åœ¨æœ‰é™è§‚æµ‹åå®ç°ç²¾ç¡®é¢„æµ‹ã€‚è¯¥å·¥ä½œè¿›ä¸€æ­¥æ¢³ç†äº†åŠ¨åŠ›ç³»ç»Ÿå­¦ä¹ ä¸ç»å…¸PACå­¦ä¹ ã€åœ¨çº¿å­¦ä¹ (Online learning)åŠé€šç”¨é¢„æµ‹ç†è®ºçš„è”ç³»ï¼Œå¹¶ä¸ºæœªæ¥æ¢ç´¢éçº¿æ€§å’Œå—æ§ç³»ç»Ÿå¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19410v1",
      "published_date": "2025-12-22 14:05:31 UTC",
      "updated_date": "2025-12-22 14:05:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:06:51.616676+00:00"
    },
    {
      "arxiv_id": "2512.19399v1",
      "title": "Brain-Grounded Axes for Reading and Steering LLM States",
      "title_zh": "ç”¨äºè¯»å–å’Œè°ƒæ§ LLM çŠ¶æ€çš„äººè„‘åŸºå‡†è½´",
      "authors": [
        "Sandro Andric"
      ],
      "abstract": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºåˆ©ç”¨äººç±»å¤§è„‘æ´»åŠ¨ä½œä¸ºåæ ‡ç³»æ¥è¯»å–å’Œå¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„çŠ¶æ€ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå¯è§£é‡Šæ€§æ–¹æ³•å› è¿‡åº¦ä¾èµ–æ–‡æœ¬ç›‘ç£è€Œç¼ºä¹å¤–éƒ¨æ¥åœ°ï¼ˆgroundingï¼‰çš„é—®é¢˜ã€‚ç ”ç©¶äººå‘˜åŸºäºSMN4Lang MEGæ•°æ®é›†æ„å»ºäº†å•è¯çº§çš„å¤§è„‘å›¾è°±ï¼Œé€šè¿‡ç‹¬ç«‹æˆåˆ†åˆ†æï¼ˆICAï¼‰æå–å‡ºæ½œåœ¨çš„ç¥ç»è½´ï¼Œå¹¶åˆ©ç”¨è½»é‡çº§é€‚é…å™¨å°†LLMçš„éšè—çŠ¶æ€æ˜ å°„åˆ°è¿™äº›è½´ä¸Šã€‚å®éªŒåœ¨TinyLlamaã€Qwen2-0.5Bå’ŒGPT-2ç­‰å¤šä¸ªæ¨¡å‹ä¸­æˆåŠŸè¯†åˆ«å‡ºäº†ç¨³å¥çš„è¯é¢‘è½´å’ŒåŠŸèƒ½/å†…å®¹è½´ï¼ˆaxis 13ï¼‰ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¸ä¼ ç»Ÿçš„æ–‡æœ¬æ¢é’ˆç›¸æ¯”ï¼ŒåŸºäºå¤§è„‘çš„è½´åœ¨å¼•å¯¼æ¨¡å‹çŠ¶æ€æ—¶è¡¨ç°å‡ºæ›´æ˜¾è‘—çš„è¯é¢‘åç§»å’Œæ›´ä½çš„å›°æƒ‘åº¦ï¼ˆperplexityï¼‰ã€‚è¯¥æˆæœè¯æ˜äº†åŸºäºç¥ç»ç”Ÿç†å­¦çš„è½´å¯ä»¥ä½œä¸ºå¯è§£é‡Šä¸”å¯æ§çš„æ‰‹æŸ„æ¥è°ƒèŠ‚LLMè¡Œä¸ºï¼Œä¸ºæ¢ç´¢æ¨¡å‹å†…éƒ¨æœºåˆ¶æä¾›äº†å…¨æ–°çš„å¤–éƒ¨éªŒè¯ç»´åº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 4 figures. Code: https://github.com/sandroandric/Brain-Grounded-Axes-for-Reading-and-Steering-LLM-States",
      "pdf_url": "https://arxiv.org/pdf/2512.19399v1",
      "published_date": "2025-12-22 13:51:03 UTC",
      "updated_date": "2025-12-22 13:51:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:06:48.706388+00:00"
    },
    {
      "arxiv_id": "2512.19396v1",
      "title": "EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration",
      "title_zh": "EchoTrail-GUIï¼šé€šè¿‡è¯„ä»·å™¨å¼•å¯¼çš„è‡ªæˆ‘æ¢ç´¢ä¸º GUI æ™ºèƒ½ä½“æ„å»ºå¯æ‰§è¡Œè®°å¿†",
      "authors": [
        "Runze Li",
        "Yuwen Zhai",
        "Bo Xu",
        "LiWu Xu",
        "Nian Shi",
        "Wei Zhang",
        "Ran Lin",
        "Liang Wang"
      ],
      "abstract": "Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰çš„ GUI Agents åœ¨å¤„ç†ä»»åŠ¡æ—¶ç¼ºä¹ä»è¿‡å»æˆåŠŸç»éªŒä¸­å­¦ä¹ çš„æœºåˆ¶ï¼Œå¯¼è‡´æ€§èƒ½ä½ä¸‹ä¸”é‡å¤é”™è¯¯çš„é—®é¢˜ï¼Œæå‡ºäº† EchoTrail-GUI æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡æ¨¡æ‹Ÿäººç±»çš„ç»éªŒå­¦ä¹ æ–¹å¼ï¼Œä¸ºæ™ºèƒ½ä½“é…å¤‡äº†åŠ¨æ€ä¸”å¯è®¿é—®çš„è®°å¿†ç³»ç»Ÿã€‚åœ¨ Experience Exploration é˜¶æ®µï¼Œæ™ºèƒ½ä½“é€šè¿‡è‡ªä¸»äº¤äº’æ„å»ºæˆåŠŸè½¨è¿¹æ•°æ®åº“ï¼Œå¹¶ç”± reward model éªŒè¯ï¼Œå®ç°å…¨è‡ªåŠ¨åŒ–çš„çŸ¥è¯†åº“æ„å»ºã€‚éšååœ¨ Memory Injection é˜¶æ®µï¼Œç³»ç»Ÿæ ¹æ®æ–°ä»»åŠ¡é«˜æ•ˆæ£€ç´¢æœ€ç›¸å…³çš„è¿‡å»è½¨è¿¹ä½œä¸º actionable memoriesã€‚æœ€ååœ¨ GUI Task Inference é˜¶æ®µï¼Œè¿™äº›è®°å¿†è¢«æ³¨å…¥ä¸º in-context guidanceï¼Œè¾…åŠ©æ™ºèƒ½ä½“çš„æ¨ç†å’Œå†³ç­–è¿‡ç¨‹ã€‚å®éªŒåœ¨ Android World å’Œ AndroidLab åŸºå‡†æµ‹è¯•ä¸Šè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼ŒEchoTrail-GUI æ˜¾è‘—æé«˜äº†åŸºå‡†æ™ºèƒ½ä½“çš„ä»»åŠ¡æˆåŠŸç‡å’Œæ“ä½œæ•ˆç‡ï¼ŒéªŒè¯äº†ç»“æ„åŒ–è®°å¿†åœ¨æ„å»ºæ›´å¼ºå¤§ã€æ›´æ™ºèƒ½çš„ GUI è‡ªåŠ¨åŒ–ç³»ç»Ÿæ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19396v1",
      "published_date": "2025-12-22 13:42:18 UTC",
      "updated_date": "2025-12-22 13:42:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:06:55.345902+00:00"
    },
    {
      "arxiv_id": "2512.19387v1",
      "title": "DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition",
      "title_zh": "DSTEDï¼šè§£è€¦æ—¶åºç¨³å®šä¸åˆ¤åˆ«å¢å¼ºçš„æ‰‹æœ¯æµç¨‹è¯†åˆ«",
      "authors": [
        "Yueyao Chen",
        "Kai-Ni Wang",
        "Dario Tayupo",
        "Arnaud Huaulm'e",
        "Krystel Nyangoh Timoh",
        "Pierre Jannin",
        "Qi Dou"
      ],
      "abstract": "Purpose: Surgical workflow recognition enables context-aware assistance and skill assessment in computer-assisted interventions. Despite recent advances, current methods suffer from two critical challenges: prediction jitter across consecutive frames and poor discrimination of ambiguous phases. This paper aims to develop a stable framework by selectively propagating reliable historical information and explicitly modeling uncertainty for hard sample enhancement.\n  Methods: We propose a dual-pathway framework DSTED with Reliable Memory Propagation (RMP) and Uncertainty-Aware Prototype Retrieval (UPR). RMP maintains temporal coherence by filtering and fusing high-confidence historical features through multi-criteria reliability assessment. UPR constructs learnable class-specific prototypes from high-uncertainty samples and performs adaptive prototype matching to refine ambiguous frame representations. Finally, a confidence-driven gate dynamically balances both pathways based on prediction certainty.\n  Results: Our method achieves state-of-the-art performance on AutoLaparo-hysterectomy with 84.36% accuracy and 65.51% F1-score, surpassing the second-best method by 3.51% and 4.88% respectively. Ablations reveal complementary gains from RMP (2.19%) and UPR (1.93%), with synergistic effects when combined. Extensive analysis confirms substantial reduction in temporal jitter and marked improvement on challenging phase transitions.\n  Conclusion: Our dual-pathway design introduces a novel paradigm for stable workflow recognition, demonstrating that decoupling the modeling of temporal consistency and phase ambiguity yields superior performance and clinical applicability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DSTEDï¼Œä¸€ç§æ—¨åœ¨è§£å†³æ‰‹æœ¯å·¥ä½œæµè¯†åˆ«ä¸­é¢„æµ‹æŠ–åŠ¨(prediction jitter)å’Œæ¨¡ç³Šé˜¶æ®µåŒºåˆ†èƒ½åŠ›ä¸è¶³ç­‰æŒ‘æˆ˜çš„åŒè·¯å¾„æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å¯é å­˜å‚¨ä¼ æ’­(Reliable Memory Propagation, RMP)å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥åŸå‹æ£€ç´¢(Uncertainty-Aware Prototype Retrieval, UPR)åˆ†åˆ«å¤„ç†æ—¶é—´è¿è´¯æ€§å’Œéš¾æ ·æœ¬å¢å¼ºã€‚RMPæ¨¡å—åˆ©ç”¨å¤šå‡†åˆ™å¯é æ€§è¯„ä¼°è¿‡æ»¤å¹¶èåˆé«˜ç½®ä¿¡åº¦å†å²ç‰¹å¾ï¼Œè€ŒUPRæ¨¡å—åˆ™é€šè¿‡è‡ªé€‚åº”åŒ¹é…ç±»åˆ«ç‰¹å®šåŸå‹æ¥ä¼˜åŒ–æ¨¡ç³Šå¸§çš„è¡¨ç¤ºã€‚æ­¤å¤–ï¼Œç³»ç»Ÿå¼•å…¥ç½®ä¿¡åº¦é©±åŠ¨çš„é—¨æ§æœºåˆ¶(confidence-driven gate)æ¥åŠ¨æ€å¹³è¡¡ä¸¤æ¡è·¯å¾„ï¼Œå®ç°äº†æ—¶é—´ç¨³å®šæ€§ä¸åˆ¤åˆ«å¢å¼ºçš„è§£è€¦å»ºæ¨¡ã€‚å®éªŒè¡¨æ˜ï¼ŒDSTEDåœ¨AutoLaparo-hysterectomyæ•°æ®é›†ä¸Šè¾¾åˆ°äº†84.36%çš„å‡†ç¡®ç‡å’Œ65.51%çš„F1åˆ†æ•°ï¼Œè¾ƒæ¬¡ä¼˜æ–¹æ³•åˆ†åˆ«æå‡äº†3.51%å’Œ4.88%ã€‚ç ”ç©¶ç»“æœè¯å®ï¼Œè¯¥æ–¹æ³•èƒ½æ˜¾è‘—å‡å°‘æ—¶é—´æŠ–åŠ¨å¹¶æ”¹å–„å…·æœ‰æŒ‘æˆ˜æ€§çš„é˜¶æ®µè½¬æ¢è¯†åˆ«ï¼Œä¸ºæ‰‹æœ¯æ™ºèƒ½è¾…åŠ©æä¾›äº†æ›´ç¨³å®šçš„æŠ€æœ¯èŒƒå¼ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Early accepted to IPCAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.19387v1",
      "published_date": "2025-12-22 13:36:26 UTC",
      "updated_date": "2025-12-22 13:36:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:07:12.272301+00:00"
    },
    {
      "arxiv_id": "2601.08848v1",
      "title": "PediaMind-R1: A Temperament-Aware Language Model for Personalized Early Childhood Care Reasoning via Cognitive Modeling and Preference Alignment",
      "title_zh": "PediaMind-R1ï¼šé€šè¿‡è®¤çŸ¥å»ºæ¨¡ä¸åå¥½å¯¹é½å®ç°ä¸ªæ€§åŒ–å¹¼å„¿æŠ¤ç†æ¨ç†çš„æ°”è´¨æ„ŸçŸ¥è¯­è¨€æ¨¡å‹",
      "authors": [
        "Zihe Zhang",
        "Can Zhang",
        "Yanheng Xu",
        "Xin Hu",
        "Jichao Leng"
      ],
      "abstract": "This paper presents PediaMind-R1, a domain-specialized large language model designed to achieve active personalization in intelligent parenting scenarios. Unlike conventional systems that provide generic suggestions, PediaMind-R1 draws on insights from developmental psychology. It introduces temperament theory from the Thomas-Chess framework and builds a temperament knowledge graph for infants and toddlers (0-3 years). Our two-stage training pipeline first uses supervised fine-tuning to teach structured chain-of-thought reasoning, and then applies a GRPO-based alignment stage to reinforce logical consistency, domain expertise, and empathetic caregiving strategies. We further design an evaluation framework comprising temperament-sensitive multiple-choice tests and human assessments. The results demonstrate that PediaMind-R1 can accurately interpret early childhood temperament profiles and proactively engage in individualized reasoning. This work highlights the value of integrating vertical-domain modeling with psychological theory. It offers a novel approach to developing user-centered LLMs that advance the practice of active personalization in sensitive caregiving contexts.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†PediaMind-R1ï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“é—¨ä¸º0-3å²æ—©æœŸè‚²å„¿åœºæ™¯è®¾è®¡çš„é¢†åŸŸä¸“ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å®ç°ä¸»åŠ¨ä¸ªæ€§åŒ–æŠ¤ç†ã€‚è¯¥æ¨¡å‹å€Ÿé‰´äº†å‘å±•å¿ƒç†å­¦ä¸­çš„Thomas-Chessæ°”è´¨ç†è®ºæ¡†æ¶ï¼Œå¹¶æ„å»ºäº†é’ˆå¯¹å©´å¹¼å„¿çš„æ°”è´¨çŸ¥è¯†å›¾è°±(Knowledge Graph)ï¼Œä»è€Œå…‹æœäº†ä¼ ç»Ÿç³»ç»Ÿä»…èƒ½æä¾›é€šç”¨å»ºè®®çš„å±€é™ã€‚ç ”å‘",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at EMNLP 2025 PALS Workshop (PALS: EXPLORING ACTIVE AND PASSIVE LLM PERSONALIZATION)",
      "pdf_url": "https://arxiv.org/pdf/2601.08848v1",
      "published_date": "2025-12-22 13:30:48 UTC",
      "updated_date": "2025-12-22 13:30:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:07:06.942301+00:00"
    },
    {
      "arxiv_id": "2512.19379v2",
      "title": "Indonesian Multimodal Emotion Recognition via Auxiliary-Enhanced LLM Adaptation",
      "title_zh": "åŸºäºè¾…åŠ©å¢å¼ºå‹ LLM é€‚é…çš„å°åº¦å°¼è¥¿äºšè¯­å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«",
      "authors": [
        "Xueming Yan",
        "Boyan Xu",
        "Yaochu Jin",
        "Lixian Xiao",
        "Wenlong Ye",
        "Runyang Cai",
        "Zeqi Zheng",
        "Jingfa Liu",
        "Aimin Yang",
        "Yongduan Song"
      ],
      "abstract": "Indonesian, spoken by over 200 million people, remains underserved in multimodal emotion recognition research despite its dominant presence on Southeast Asian social media platforms. We introduce IndoMER, the first multimodal emotion recognition benchmark for Indonesian, comprising 1,944 video segments from 203 speakers with temporally aligned text, audio, and visual annotations across seven emotion categories. The dataset exhibits realistic challenges including cross-modal inconsistency and long-tailed class distributions shaped by Indonesian cultural communication norms. To address these challenges, we propose OmniMER, a multimodal adaptation framework built upon Qwen2.5-Omni that enhances emotion recognition through three auxiliary modality-specific perception tasks: emotion keyword extraction for text, facial expression analysis for video, and prosody analysis for audio. These auxiliary tasks help the model identify emotion-relevant cues in each modality before fusion, reducing reliance on spurious correlations in low-resource settings. Experiments on IndoMER show that OmniMER achieves 0.582 Macro-F1 on sentiment classification and 0.454 on emotion recognition, outperforming the base model by 7.6 and 22.1 absolute points respectively. Cross-lingual evaluation on the Chinese CH-SIMS dataset further demonstrates the generalizability of the proposed framework. The dataset and code are publicly available. https://github.com/yanxm01/INDOMER",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°åº¦å°¼è¥¿äºšè¯­åœ¨å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«ï¼ˆMultimodal Emotion Recognitionï¼‰é¢†åŸŸèµ„æºåŒ®ä¹çš„é—®é¢˜ï¼Œæ¨å‡ºäº†é¦–ä¸ªå°å°¼è¯­å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«åŸºå‡†æ•°æ®é›† IndoMERï¼ŒåŒ…å« 1,944 ä¸ªæ¶µç›–æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†è§‰å¯¹é½æ ‡æ³¨çš„è§†é¢‘ç‰‡æ®µã€‚ä¸ºäº†åº”å¯¹æ•°æ®é›†ä¸­å­˜åœ¨çš„è·¨æ¨¡æ€ä¸ä¸€è‡´æ€§å’Œé•¿å°¾åˆ†å¸ƒç­‰æŒ‘æˆ˜ï¼Œä½œè€…æå‡ºäº†åŸºäº Qwen2.5-Omni çš„å¤šæ¨¡æ€é€‚é…æ¡†æ¶ OmniMERï¼Œé€šè¿‡æƒ…æ„Ÿå…³é”®è¯æå–ã€é¢éƒ¨è¡¨æƒ…åˆ†æå’ŒéŸµå¾‹åˆ†æï¼ˆProsody Analysisï¼‰ä¸‰ä¸ªè¾…åŠ©æ„ŸçŸ¥ä»»åŠ¡æ¥å¢å¼ºæ¨¡å‹å¯¹æƒ…æ„Ÿçº¿ç´¢çš„æ•æ‰èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒOmniMER åœ¨ IndoMER æ•°æ®é›†ä¸Šçš„æƒ…æ„Ÿè¯†åˆ« Macro-F1 è¾¾åˆ° 0.454ï¼Œç›¸è¾ƒäºåŸºçº¿æ¨¡å‹æå‡äº† 22.1 ä¸ªç™¾åˆ†ç‚¹ï¼Œæœ‰æ•ˆå‡å°‘äº†æ¨¡å‹åœ¨ä½èµ„æºè®¾ç½®ä¸‹å¯¹è™šå‡ç›¸å…³æ€§çš„ä¾èµ–ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨ä¸­æ–‡æ•°æ®é›† CH-SIMS ä¸Šçš„è·¨è¯­è¨€è¯„ä¼°è¿›ä¸€æ­¥éªŒè¯äº†å…¶è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ä»…å¡«è¡¥äº†å°å°¼è¯­åœ¨å¤šæ¨¡æ€ç ”ç©¶ä¸­çš„ç©ºç™½ï¼Œä¹Ÿä¸ºæå‡ä½èµ„æºè¯­è¨€çš„æƒ…æ„Ÿè¯†åˆ«ç²¾åº¦æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19379v2",
      "published_date": "2025-12-22 13:23:55 UTC",
      "updated_date": "2026-01-09 09:43:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:07:38.516005+00:00"
    },
    {
      "arxiv_id": "2512.19367v1",
      "title": "Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture",
      "title_zh": "Sprecher ç½‘ç»œï¼šä¸€ç§å‚æ•°é«˜æ•ˆçš„ Kolmogorov-Arnold æ¶æ„",
      "authors": [
        "Christian HÃ¤gg",
        "KathlÃ©n Kohn",
        "Giovanni Luca Marchetti",
        "Boris Shapiro"
      ],
      "abstract": "We present Sprecher Networks (SNs), a family of trainable neural architectures inspired by the classical Kolmogorov-Arnold-Sprecher (KAS) construction for approximating multivariate continuous functions. Distinct from Multi-Layer Perceptrons (MLPs) with fixed node activations and Kolmogorov-Arnold Networks (KANs) featuring learnable edge activations, SNs utilize shared, learnable splines (monotonic and general) within structured blocks incorporating explicit shift parameters and mixing weights. Our approach directly realizes Sprecher's specific 1965 sum of shifted splines formula in its single-layer variant and extends it to deeper, multi-layer compositions. We further enhance the architecture with optional lateral mixing connections that enable intra-block communication between output dimensions, providing a parameter-efficient alternative to full attention mechanisms. Beyond parameter efficiency with $O(LN + LG)$ scaling (where $G$ is the knot count of the shared splines) versus MLPs' $O(LN^2)$, SNs admit a sequential evaluation strategy that reduces peak forward-intermediate memory from $O(N^2)$ to $O(N)$ (treating batch size as constant), making much wider architectures feasible under memory constraints. We demonstrate empirically that composing these blocks into deep networks leads to highly parameter and memory-efficient models, discuss theoretical motivations, and compare SNs with related architectures (MLPs, KANs, and networks with learnable node activations).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Sprecher Networks (SNs)ï¼Œè¿™æ˜¯ä¸€ç§å—ç»å…¸ Kolmogorov-Arnold-Sprecher (KAS) æ„é€ å¯å‘çš„å¯è®­ç»ƒç¥ç»æ¶æ„ï¼Œæ—¨åœ¨é«˜æ•ˆé€¼è¿‘å¤šå…ƒè¿ç»­å‡½æ•°ã€‚ä¸å…·æœ‰å›ºå®šæ¿€æ´»å‡½æ•°çš„ Multi-Layer Perceptrons (MLPs) ä»¥åŠå…·æœ‰å¯å­¦ä¹ è¾¹ç¼˜æ¿€æ´»çš„ Kolmogorov-Arnold Networks (KANs) ä¸åŒï¼ŒSNs åœ¨ç»“æ„åŒ–å—ä¸­ä½¿ç”¨å…±äº«çš„å¯å­¦ä¹  splinesï¼Œå¹¶ç»“åˆäº†æ˜¾å¼çš„ä½ç§»å‚æ•° (shift parameters) å’Œæ··åˆæƒé‡ (mixing weights)ã€‚è¯¥æ¶æ„ç›´æ¥å®ç°äº† Sprecher åœ¨1965å¹´æå‡ºçš„ä½ç§»æ ·æ¡å’Œ (sum of shifted splines) å…¬å¼ï¼Œå¹¶å°†å…¶æ‰©å±•ä¸ºæ·±å±‚å¤šå±‚ç»„åˆå½¢å¼ã€‚æ­¤å¤–ï¼ŒSNs å¼•å…¥äº†å¯é€‰çš„ä¾§å‘æ··åˆè¿æ¥ (lateral mixing connections)ï¼Œä¸ºå…¨æ³¨æ„åŠ›æœºåˆ¶ (full attention mechanisms) æä¾›äº†ä¸€ç§å‚æ•°é«˜æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚åœ¨å¤æ‚åº¦ä¸Šï¼ŒSNs å®ç°äº† $O(LN + LG)$ çš„å‚æ•°æ‰©å±•ï¼Œä¼˜äº MLPs çš„ $O(LN^2)$ï¼Œä¸”å…¶é¡ºåºè¯„ä¼°ç­–ç•¥å°†å³°å€¼å†…å­˜éœ€æ±‚ä» $O(N^2)$ æ˜¾è‘—é™ä½è‡³ $O(N)$ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSNs æ„å»ºçš„æ·±åº¦ç½‘ç»œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å…·æœ‰æé«˜çš„å‚æ•°å’Œå†…å­˜æ•ˆç‡ï¼Œä¸ºåœ¨èµ„æºå—é™ä¸‹å®ç°è¶…å®½æ¶æ„æä¾›äº†å¯èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "primary_category": "cs.LG",
      "comment": "37 pages",
      "pdf_url": "https://arxiv.org/pdf/2512.19367v1",
      "published_date": "2025-12-22 13:09:45 UTC",
      "updated_date": "2025-12-22 13:09:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:08:06.258678+00:00"
    },
    {
      "arxiv_id": "2512.19366v1",
      "title": "Learning General Policies with Policy Gradient Methods",
      "title_zh": "åŸºäºç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„é€šç”¨ç­–ç•¥å­¦ä¹ ",
      "authors": [
        "Simon StÃ¥hlberg",
        "Blai Bonet",
        "Hector Geffner"
      ],
      "abstract": "While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨è§£å†³ Reinforcement Learning ä¸­çš„æ³›åŒ–æŒ‘æˆ˜ï¼Œæ¢ç´¢å¦‚ä½•å­¦ä¹ èƒ½åƒç»å…¸è§„åˆ’ï¼ˆClassical Planningï¼‰ä¸€æ ·åœ¨ç‰¹å®šé¢†åŸŸæ‰€æœ‰å®ä¾‹ä¸­å¯é æ³›åŒ–çš„é€šç”¨ç­–ç•¥ã€‚ä½œè€…ç»“åˆäº† Deep Reinforcement Learning ä¸ç»„åˆè§„åˆ’æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œå°†ç­–ç•¥å»ºæ¨¡ä¸ºçŠ¶æ€è½¬ç§»åˆ†ç±»å™¨ï¼ˆState Transition Classifiersï¼‰ï¼Œå¹¶åˆ©ç”¨é€‚é…äºå…³ç³»ç»“æ„ï¼ˆRelational Structuresï¼‰çš„ Graph Neural Networks (GNNs) æ¥è¡¨ç¤ºç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒActor-Critic æ–¹æ³•èƒ½å¤Ÿå­¦ä¹ åˆ°æ³›åŒ–æ€§èƒ½æ¥è¿‘ç»„åˆæ–¹æ³•çš„ç­–ç•¥ï¼Œä¸”æˆåŠŸå…‹æœäº†æ‰©å±•æ€§ç“¶é¢ˆå’Œå¯¹ç‰¹å¾æ± ï¼ˆFeature Poolsï¼‰çš„ä¾èµ–ã€‚ç ”ç©¶è¿˜æŒ‡å‡º DRL æ–¹æ³•åœ¨åŸºå‡†æµ‹è¯•ä¸­çš„å±€é™æ€§å¹¶éæºäºç®—æ³•æœ¬èº«ï¼Œè€Œæ˜¯å—é™äº GNNs çš„è¡¨è¾¾èƒ½åŠ›ä»¥åŠæœ€ä¼˜æ€§ï¼ˆOptimalityï¼‰ä¸æ³›åŒ–æ€§ä¹‹é—´çš„æƒè¡¡ã€‚é€šè¿‡å¼•å…¥æ´¾ç”Ÿè°“è¯ï¼ˆDerived Predicatesï¼‰å’Œæ›¿ä»£æˆæœ¬ç»“æ„ï¼Œè¯¥ç ”ç©¶åœ¨ä¸æ”¹å˜åŸºç¡€ DRL æ¶æ„çš„æƒ…å†µä¸‹æœ‰æ•ˆæå‡äº†ç­–ç•¥çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings of the 20th International Conference on Principles of Knowledge Representation and Reasoning (KR 2023)",
      "pdf_url": "https://arxiv.org/pdf/2512.19366v1",
      "published_date": "2025-12-22 13:08:58 UTC",
      "updated_date": "2025-12-22 13:08:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:07:38.927238+00:00"
    },
    {
      "arxiv_id": "2512.19355v1",
      "title": "First-Order Representation Languages for Goal-Conditioned RL",
      "title_zh": "é¢å‘ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ çš„ä¸€é˜¶è¡¨ç¤ºè¯­è¨€",
      "authors": [
        "Simon StÃ¥hlberg",
        "Hector Geffner"
      ],
      "abstract": "First-order relational languages have been used in MDP planning and reinforcement learning (RL) for two main purposes: specifying MDPs in compact form, and representing and learning policies that are general and not tied to specific instances or state spaces. In this work, we instead consider the use of first-order languages in goal-conditioned RL and generalized planning. The question is how to learn goal-conditioned and general policies when the training instances are large and the goal cannot be reached by random exploration alone. The technique of Hindsight Experience Replay (HER) provides an answer to this question: it relabels unsuccessful trajectories as successful ones by replacing the original goal with one that was actually achieved. If the target policy must generalize across states and goals, trajectories that do not reach the original goal states can enable more data- and time-efficient learning. In this work, we show that further performance gains can be achieved when states and goals are represented by sets of atoms. We consider three versions: goals as full states, goals as subsets of the original goals, and goals as lifted versions of these subgoals. The result is that the latter two successfully learn general policies on large planning instances with sparse rewards by automatically creating a curriculum of easier goals of increasing complexity. The experiments illustrate the computational gains of these versions, their limitations, and opportunities for addressing them.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ä¸€é˜¶å…³ç³»è¯­è¨€(First-order relational languages)æ¡†æ¶ä¸‹ï¼Œå¦‚ä½•å®ç°ç›®æ ‡æ¡ä»¶å¼ºåŒ–å­¦ä¹ (Goal-Conditioned RL)ä¸å¹¿ä¹‰è§„åˆ’ã€‚é’ˆå¯¹å¤§è§„æ¨¡å®ä¾‹ä¸­éšæœºæ¢ç´¢éš¾ä»¥è§¦åŠç›®æ ‡çš„é—®é¢˜ï¼Œä½œè€…åˆ©ç”¨äº‹åç»éªŒå›æ”¾(Hindsight Experience Replay)æŠ€æœ¯ï¼Œå°†çŠ¶æ€ä¸ç›®æ ‡è¡¨ç¤ºä¸ºåŸå­é›†åˆ(sets of atoms)ã€‚è®ºæ–‡é‡ç‚¹å¯¹æ¯”äº†å°†ç›®æ ‡è§†ä¸ºå®Œæ•´çŠ¶æ€ã€åŸå§‹ç›®æ ‡çš„å­é›†ä»¥åŠå­é›†çš„æå‡ç‰ˆæœ¬(lifted versions)ä¸‰ç§é‡æ ‡è®°æ–¹æ¡ˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåä¸¤ç§æ–¹æ¡ˆé€šè¿‡è‡ªåŠ¨æ„å»ºå¤æ‚åº¦é€’å¢çš„è¯¾ç¨‹(curriculum)ï¼Œåœ¨å…·æœ‰ç¨€ç–å¥–åŠ±çš„å¤§è§„æ¨¡è§„åˆ’å®ä¾‹ä¸­æˆåŠŸå­¦ä¹ åˆ°äº†é€šç”¨ç­–ç•¥ã€‚è¯¥æ–¹æ³•æ˜¾è‘—å¢å¼ºäº†æ¨¡å‹åœ¨ä¸åŒå®ä¾‹é—´çš„æ³›åŒ–èƒ½åŠ›ï¼Œå¹¶ä¸ºè§£å†³å¤æ‚ç¯å¢ƒä¸‹çš„å¼ºåŒ–å­¦ä¹ ä»»åŠ¡æä¾›äº†æ˜¾è‘—çš„è®¡ç®—å¢ç›Šã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "In Proceedings of the 40th AAAI Conference on Artificial Intelligence (AAAI 2026)",
      "pdf_url": "https://arxiv.org/pdf/2512.19355v1",
      "published_date": "2025-12-22 12:54:32 UTC",
      "updated_date": "2025-12-22 12:54:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:08:43.643271+00:00"
    },
    {
      "arxiv_id": "2512.19350v1",
      "title": "PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models",
      "title_zh": "PENDULUMï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹é¡ºä»æ€§è¯„ä¼°åŸºå‡†",
      "authors": [
        "A. B. M. Ashikur Rahman",
        "Saeed Anwar",
        "Muhammad Usman",
        "Irfan Ahmad",
        "Ajmal Mian"
      ],
      "abstract": "Sycophancy, an excessive tendency of AI models to agree with user input at the expense of factual accuracy or in contradiction of visual evidence, poses a critical and underexplored challenge for multimodal large language models (MLLMs). While prior studies have examined this behavior in text-only settings of large language models, existing research on visual or multimodal counterparts remains limited in scope and depth of analysis. To address this gap, we introduce a comprehensive evaluation benchmark, \\textit{PENDULUM}, comprising approximately 2,000 human-curated Visual Question Answering pairs specifically designed to elicit sycophantic responses. The benchmark spans six distinct image domains of varying complexity, enabling a systematic investigation of how image type and inherent challenges influence sycophantic tendencies. Through extensive evaluation of state-of-the-art MLLMs. we observe substantial variability in model robustness and a pronounced susceptibility to sycophantic and hallucinatory behavior. Furthermore, we propose novel metrics to quantify sycophancy in visual reasoning, offering deeper insights into its manifestations across different multimodal contexts. Our findings highlight the urgent need for developing sycophancy-resilient architectures and training strategies to enhance factual consistency and reliability in future MLLMs. Our proposed dataset with MLLMs response are available at https://github.com/ashikiut/pendulum/.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†PENDULUMï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ä¸­å¥‰æ‰¿è¡Œä¸ºï¼ˆSycophancyï¼‰çš„ç»¼åˆåŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨æ¢è®¨æ¨¡å‹è¿‡åº¦è¿åˆç”¨æˆ·è¾“å…¥è€Œå¿½è§†è§†è§‰è¯æ®æˆ–äº‹å®å‡†ç¡®æ€§çš„é—®é¢˜ã€‚è¯¥åŸºå‡†åŒ…å«çº¦2000ä¸ªç»è¿‡äººå·¥ç­›é€‰çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰å¯¹ï¼Œæ¶µç›–äº†å…­ä¸ªä¸åŒå¤æ‚åº¦çš„å›¾åƒé¢†åŸŸï¼Œä»è€Œæ”¯æŒå¯¹æ¨¡å‹å¥‰æ‰¿å€¾å‘è¿›è¡Œç³»ç»Ÿæ€§è°ƒæŸ¥ã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼Œç°æœ‰çš„æœ€å…ˆè¿›æ¨¡å‹å¯¹å¥‰æ‰¿è¡Œä¸ºå’Œå¹»è§‰ï¼ˆHallucinationsï¼‰è¡¨ç°å‡ºæ˜¾è‘—çš„æ˜“æ„Ÿæ€§ï¼Œä¸”åœ¨é²æ£’æ€§æ–¹é¢å·®å¼‚å·¨å¤§ã€‚æ­¤å¤–ï¼Œç ”ç©¶æå‡ºäº†é‡åŒ–è§†è§‰æ¨ç†ä¸­å¥‰æ‰¿ç¨‹åº¦çš„æ–°æŒ‡æ ‡ï¼Œä¸ºç†è§£ä¸åŒå¤šæ¨¡æ€è¯­å¢ƒä¸‹çš„æ¨¡å‹è¡Œä¸ºæä¾›äº†æ·±å…¥è§è§£ã€‚è¿™äº›å‘ç°å‡¸æ˜¾äº†å¼€å‘å…·æœ‰å¥‰æ‰¿æŠ—æ€§çš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥çš„ç´§è¿«æ€§ï¼Œä»¥æå‡æœªæ¥å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„äº‹å®ä¸€è‡´æ€§å’Œå¯é æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19350v1",
      "published_date": "2025-12-22 12:49:12 UTC",
      "updated_date": "2025-12-22 12:49:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:08:41.557126+00:00"
    },
    {
      "arxiv_id": "2512.19349v1",
      "title": "VIGOR+: Iterative Confounder Generation and Validation via LLM-CEVAE Feedback Loop",
      "title_zh": "VIGOR+ï¼šåŸºäº LLM-CEVAE åé¦ˆå¾ªç¯çš„è¿­ä»£å¼æ··æ‚å› ç´ ç”Ÿæˆä¸éªŒè¯",
      "authors": [
        "JiaWei Zhu",
        "ZiHeng Liu"
      ],
      "abstract": "Hidden confounding remains a fundamental challenge in causal inference from observational data. Recent advances leverage Large Language Models (LLMs) to generate plausible hidden confounders based on domain knowledge, yet a critical gap exists: LLM-generated confounders often exhibit semantic plausibility without statistical utility. We propose VIGOR+ (Variational Information Gain for iterative cOnfounder Refinement), a novel framework that closes the loop between LLM-based confounder generation and CEVAE-based statistical validation. Unlike prior approaches that treat generation and validation as separate stages, VIGOR+ establishes an iterative feedback mechanism: validation signals from CEVAE (including information gain, latent consistency metrics, and diagnostic messages) are transformed into natural language feedback that guides subsequent LLM generation rounds. This iterative refinement continues until convergence criteria are met. We formalize the feedback mechanism, prove convergence properties under mild assumptions, and provide a complete algorithmic framework.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»è§‚æµ‹æ•°æ®ä¸­è¿›è¡Œå› æœæ¨æ–­(causal inference)æ—¶é¢ä¸´çš„éšæ€§æ··æ‚(Hidden confounding)æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºVIGOR+çš„åˆ›æ–°æ¡†æ¶ã€‚é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)ç”Ÿæˆçš„æ··æ‚å› å­å¾€å¾€å…·å¤‡è¯­ä¹‰åˆç†æ€§å´ç¼ºä¹ç»Ÿè®¡æ•ˆç”¨çš„é—®é¢˜ï¼ŒVIGOR+åœ¨LLMç”Ÿæˆä¸åŸºäºCEVAEçš„ç»Ÿè®¡éªŒè¯ä¹‹é—´å»ºç«‹äº†ä¸€ä¸ªé—­ç¯è¿­ä»£åé¦ˆæœºåˆ¶ã€‚è¯¥æ¡†æ¶å°†CEVAEç”Ÿæˆçš„éªŒè¯ä¿¡å·ï¼ˆåŒ…æ‹¬ä¿¡æ¯å¢ç›Šã€æ½œåœ¨ä¸€è‡´æ€§æŒ‡æ ‡å’Œè¯Šæ–­ä¿¡æ¯ï¼‰è½¬åŒ–ä¸ºè‡ªç„¶è¯­è¨€åé¦ˆï¼Œä»¥æŒ‡å¯¼åç»­è½®æ¬¡çš„LLMç”Ÿæˆï¼Œä»è€Œå®ç°æ··æ‚å› å­çš„æŒç»­ç²¾ç‚¼ã€‚è¿™ç§è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ä¼šä¸€ç›´æŒç»­åˆ°æ»¡è¶³æ”¶æ•›æ ‡å‡†ï¼Œä»è€Œå¡«è¡¥äº†ç”Ÿæˆä¸éªŒè¯é˜¶æ®µè„±èŠ‚çš„æŠ€æœ¯ç©ºç™½ã€‚ç ”ç©¶å›¢é˜Ÿè¿˜å½¢å¼åŒ–äº†è¯¥åé¦ˆæœºåˆ¶ï¼Œè¯æ˜äº†åœ¨æ¸©å’Œå‡è®¾ä¸‹çš„æ”¶æ•›ç‰¹æ€§ï¼Œå¹¶æä¾›äº†ä¸€å¥—å®Œæ•´çš„ç®—æ³•æ¡†æ¶ï¼Œæœ‰æ•ˆæå‡äº†éšæ€§æ··æ‚å› å­è¯†åˆ«çš„ç»Ÿè®¡å®ç”¨æ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages,1 figure,4 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.19349v1",
      "published_date": "2025-12-22 12:48:29 UTC",
      "updated_date": "2025-12-22 12:48:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:09:47.399222+00:00"
    },
    {
      "arxiv_id": "2512.19323v1",
      "title": "Alternative positional encoding functions for neural transformers",
      "title_zh": "ç¥ç» Transformer çš„æ›¿ä»£æ€§ä½ç½®ç¼–ç å‡½æ•°",
      "authors": [
        "Ezequiel Lopez-Rubio",
        "Macoris Decena-Gimenez",
        "Rafael Marcos Luque-Baena"
      ],
      "abstract": "A key module in neural transformer-based deep architectures is positional encoding. This module enables a suitable way to encode positional information as input for transformer neural layers. This success has been rooted in the use of sinusoidal functions of various frequencies, in order to capture recurrent patterns of differing typical periods. In this work, an alternative set of periodic functions is proposed for positional encoding. These functions preserve some key properties of sinusoidal ones, while they depart from them in fundamental ways. Some tentative experiments are reported, where the original sinusoidal version is substantially outperformed. This strongly suggests that the alternative functions may have a wider use in other transformer architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Transformeræ¶æ„ä¸­å…³é”®çš„positional encodingæ¨¡å—ï¼Œæå‡ºäº†ä¸€ç»„æ›¿ä»£ä¼ ç»Ÿæ­£å¼¦å‡½æ•°(sinusoidal functions)çš„æ–°å‹å‘¨æœŸå‡½æ•°ã€‚è™½ç„¶ç°æœ‰çš„ä½ç½®ç¼–ç ä¸»è¦åˆ©ç”¨ä¸åŒé¢‘ç‡çš„æ­£å¼¦å‡½æ•°æ¥æ•æ‰åºåˆ—ä¸­çš„å‘¨æœŸæ€§æ¨¡å¼ï¼Œä½†æœ¬å·¥ä½œè®¾è®¡çš„æ›¿ä»£å‡½æ•°åœ¨ä¿ç•™æ­£å¼¦å‡½æ•°æ ¸å¿ƒç‰¹æ€§çš„åŸºç¡€ä¸Šï¼Œåœ¨åŸºç¡€æ„å»ºæ–¹å¼ä¸Šè¿›è¡Œäº†æ ¹æœ¬æ€§çš„åˆ›æ–°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ›¿ä»£æ–¹æ¡ˆåœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºåŸå§‹çš„æ­£å¼¦ç‰ˆæœ¬ï¼Œå±•ç¤ºäº†æ›´å¼ºçš„ç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚è¿™ä¸€å‘ç°æœ‰åŠ›åœ°è¯æ˜äº†æ–°å‹å‘¨æœŸå‡½æ•°åœ¨å„ç±»Transformeræ¶æ„ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨æ½œåŠ›ï¼Œä¸ºä¼˜åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä½ç½®ä¿¡æ¯ç¼–ç æä¾›äº†æ–°çš„ç ”ç©¶è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19323v1",
      "published_date": "2025-12-22 12:17:47 UTC",
      "updated_date": "2025-12-22 12:17:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:09:11.205332+00:00"
    },
    {
      "arxiv_id": "2512.19320v1",
      "title": "MAGIC: Achieving Superior Model Merging via Magnitude Calibration",
      "title_zh": "MAGICï¼šé€šè¿‡å¹…å€¼æ ¡å‡†å®ç°å“è¶Šçš„æ¨¡å‹åˆå¹¶",
      "authors": [
        "Yayuan Li",
        "Jian Zhang",
        "Jintao Guo",
        "Zihan Cheng",
        "Lei Qi",
        "Yinghuan Shi",
        "Yang Gao"
      ],
      "abstract": "The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¨¡å‹åˆå¹¶(Model Merging)ä¸­ç°æœ‰æ–¹æ³•ä¾§é‡äºç‰¹å¾æ–¹å‘å¯¹é½è€Œå¿½è§†ç‰¹å¾å¹…å€¼(Magnitude)çš„é—®é¢˜ï¼Œæå‡ºäº†åä¸ºMAGIC (MAGnItude Calibration) çš„å³æ’å³ç”¨æ¡†æ¶ã€‚ä½œè€…å‘ç°ï¼Œåœ¨å‚æ•°èåˆå’Œç¨€ç–åŒ–ç­‰åˆå¹¶æ“ä½œä¸­ï¼Œç‰¹å¾å¹…å€¼ææ˜“å—æŸï¼Œå¯¼è‡´åˆå¹¶æ¨¡å‹çš„ç‰¹å¾åç¦»åŸå§‹ä¸“é—¨æ¨¡å‹å¹¶é€ æˆæ€§èƒ½é€€åŒ–ã€‚ä¸ºæ­¤ï¼ŒMAGICé€šè¿‡æ ¡æ­£ç‰¹å¾ç©ºé—´ä¸æƒé‡ç©ºé—´çš„å±‚çº§å¹…å€¼æ¥ä¼˜åŒ–æ€§èƒ½ï¼Œå¹¶æä¾›äº†ä¾èµ–å°‘é‡æ— æ ‡ç­¾æ•°æ®çš„FSC (Feature Space Calibration)ã€æ— éœ€æ•°æ®çš„WSC (Weight Space Calibration) ä»¥åŠç»“åˆä¸¤è€…çš„DSC (Dual Space Calibration) ä¸‰ç§å˜ä½“ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„å‰æä¸‹ï¼Œè¯¥æ¡†æ¶åœ¨è®¡ç®—æœºè§†è§‰(Computer Vision)å’Œè‡ªç„¶è¯­è¨€å¤„ç†(NLP)çš„å¤šé¡¹ä»»åŠ¡ä¸­å‡æ˜¾è‘—æå‡äº†æ¨¡å‹è¡¨ç°ï¼Œå…¶ä¸­åœ¨Llamaæ¨¡å‹ä¸Šçš„æ€§èƒ½æå‡è¾¾åˆ°äº†8.0%ã€‚è¿™ä¸€æˆæœè¯æ˜äº†å¹…å€¼æ ¡æ­£å¯¹ç»´æŒæ¨¡å‹è¡Œä¸ºç‰¹å¾çš„é‡è¦æ€§ï¼Œä¸ºå®ç°é«˜æ€§èƒ½çš„ç»Ÿä¸€æ¨¡å‹æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19320v1",
      "published_date": "2025-12-22 12:13:17 UTC",
      "updated_date": "2025-12-22 12:13:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:08:47.096482+00:00"
    },
    {
      "arxiv_id": "2512.19317v1",
      "title": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models",
      "title_zh": "SafeMed-R1ï¼šé¢å‘è§†è§‰è¯­è¨€æ¨¡å‹ä¸­å¯æ³›åŒ–ä¸”é²æ£’åŒ»å­¦æ¨ç†çš„å¯¹æŠ—æ€§å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "A. A. Gde Yogi Pramana",
        "Jason Ray",
        "Anthony Jaya",
        "Michael Wijaya"
      ],
      "abstract": "Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\\% accuracy on clean inputs, collapse to approximately 25\\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SafeMed-R1ï¼Œä¸€ç§æ—¨åœ¨æå‡è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMsï¼‰åœ¨åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆMedical VQAï¼‰ä¸­æ³›åŒ–æ€§ä¸ç¨³å¥æ€§çš„æ··åˆé˜²å¾¡æ¡†æ¶ã€‚è¯¥æ¡†æ¶åœ¨è®­ç»ƒé˜¶æ®µé€šè¿‡ç»“åˆç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆAT-GRPOï¼‰çš„å¯¹æŠ—è®­ç»ƒæ¥å¢å¼ºæ¨¡å‹å¯¹æŠ—æœ€åæƒ…å†µæ‰°åŠ¨çš„èƒ½åŠ›ï¼Œå¹¶åœ¨æ¨ç†é˜¶æ®µå¼•å…¥éšæœºå¹³æ»‘ï¼ˆRandomized Smoothingï¼‰æŠ€æœ¯ä»¥æä¾›è®¤è¯çš„L2èŒƒæ•°é²æ£’æ€§ä¿è¯ã€‚åœ¨åŒ…å«å…«ç§å½±åƒæ¨¡æ€çš„OmniMedVQAåŸºå‡†æµ‹è¯•ä¸­ï¼ŒSafeMed-R1åœ¨é¢å¯¹PGDæ”»å‡»æ—¶ç»´æŒäº†84.45%çš„å‡†ç¡®ç‡ï¼Œè¾ƒæ ‡å‡†å¾®è°ƒæ¨¡å‹æ˜¾è‘—æå‡äº†59ä¸ªç™¾åˆ†ç‚¹ã€‚ç ”ç©¶è¿˜è¡¨æ˜ï¼Œæ˜¾å¼çš„é“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼‰æ¨ç†ç›¸æ¯”çº¯æŒ‡ä»¤å˜ä½“å…·æœ‰æ›´å¼ºçš„å¯¹æŠ—é²æ£’æ€§ï¼Œä½“ç°äº†åŒ»ç–—AIç³»ç»Ÿä¸­å¯è§£é‡Šæ€§ä¸å®‰å…¨æ€§ä¹‹é—´çš„ååŒæ•ˆåº”ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19317v1",
      "published_date": "2025-12-22 12:07:33 UTC",
      "updated_date": "2025-12-22 12:07:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:08:58.297830+00:00"
    },
    {
      "arxiv_id": "2512.19311v1",
      "title": "MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture",
      "title_zh": "MixFlow è®­ç»ƒï¼šåˆ©ç”¨å‡é€Ÿæ’å€¼æ··åˆç¼“è§£æ›å…‰åå·®",
      "authors": [
        "Hui Li",
        "Jiayue Lyu",
        "Fu-Yun Wang",
        "Kaihui Cheng",
        "Siyu Zhu",
        "Jingdong Wang"
      ],
      "abstract": "This paper studies the training-testing discrepancy (a.k.a. exposure bias) problem for improving the diffusion models. During training, the input of a prediction network at one training timestep is the corresponding ground-truth noisy data that is an interpolation of the noise and the data, and during testing, the input is the generated noisy data. We present a novel training approach, named MixFlow, for improving the performance. Our approach is motivated by the Slow Flow phenomenon: the ground-truth interpolation that is the nearest to the generated noisy data at a given sampling timestep is observed to correspond to a higher-noise timestep (termed slowed timestep), i.e., the corresponding ground-truth timestep is slower than the sampling timestep. MixFlow leverages the interpolations at the slowed timesteps, named slowed interpolation mixture, for post-training the prediction network for each training timestep. Experiments over class-conditional image generation (including SiT, REPA, and RAE) and text-to-image generation validate the effectiveness of our approach. Our approach MixFlow over the RAE models achieve strong generation results on ImageNet: 1.43 FID (without guidance) and 1.10 (with guidance) at 256 x 256, and 1.55 FID (without guidance) and 1.10 (with guidance) at 512 x 512.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†æ‰©æ•£æ¨¡å‹ (Diffusion Models) ä¸­å­˜åœ¨çš„è®­ç»ƒä¸æµ‹è¯•ä¸ä¸€è‡´é—®é¢˜ï¼Œå³æ›å…‰åå·® (Exposure Bias)ã€‚ç ”ç©¶è€…å‘ç°äº†æ…¢é€Ÿæµ (Slow Flow) ç°è±¡ï¼Œå³åœ¨ç‰¹å®šé‡‡æ ·æ­¥é•¿ä¸‹ï¼Œä¸ç”Ÿæˆçš„å™ªå£°æ•°æ®æœ€æ¥è¿‘çš„çœŸå®æ’å€¼å¾€å¾€å¯¹åº”äºæ›´é«˜å™ªå£°çš„æ­¥é•¿ï¼Œå³å‡é€Ÿæ­¥é•¿ (Slowed Timestep)ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º MixFlow çš„åˆ›æ–°è®­ç»ƒæ–¹æ³•ï¼Œåˆ©ç”¨å‡é€Ÿæ’å€¼æ··åˆ (Slowed Interpolation Mixture) å¯¹å„è®­ç»ƒæ­¥é•¿çš„é¢„æµ‹ç½‘ç»œè¿›è¡ŒåæœŸè®­ç»ƒã€‚è¯¥æ–¹æ³•åœ¨ç±»åˆ«æ¡ä»¶å›¾åƒç”Ÿæˆï¼ˆå¦‚ SiTã€REPAã€RAEï¼‰ä»¥åŠæ–‡æœ¬ç”Ÿæˆå›¾åƒä»»åŠ¡ä¸­å‡éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMixFlow ç»“åˆ RAE æ¨¡å‹åœ¨ ImageNet 256x256 åˆ†è¾¨ç‡ä¸‹è¾¾åˆ°äº† 1.10 çš„ FIDï¼ˆå«å¼•å¯¼ï¼‰ï¼Œåœ¨ 512x512 åˆ†è¾¨ç‡ä¸‹åŒæ ·è¡¨ç°ä¼˜å¼‚ã€‚è¿™ä¸€ç ”ç©¶ä¸ºé€šè¿‡ç¼“è§£æ›å…‰åå·®æ¥æå‡æ‰©æ•£æ¨¡å‹çš„ç”Ÿæˆæ€§èƒ½æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19311v1",
      "published_date": "2025-12-22 12:00:12 UTC",
      "updated_date": "2025-12-22 12:00:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 1,
      "last_update": "2026-01-26T19:11:07.239548+00:00"
    },
    {
      "arxiv_id": "2601.08847v2",
      "title": "Scalable and Reliable Evaluation of AI Knowledge Retrieval Systems: RIKER and the Coherent Simulated Universe",
      "title_zh": "äººå·¥æ™ºèƒ½çŸ¥è¯†æ£€ç´¢ç³»ç»Ÿçš„å¯æ‰©å±•ä¸”å¯é è¯„ä¼°ï¼šRIKER ä¸è¿è´¯æ¨¡æ‹Ÿå®‡å®™",
      "authors": [
        "JV Roig"
      ],
      "abstract": "Evaluating knowledge systems (LLMs, RAG, knowledge graphs, etc) faces fundamental challenges: static benchmarks are vulnerable to contamination, LLM-based judges exhibit systematic biases, and ground truth extraction requires expensive human annotation. We present RIKER (Retrieval Intelligence and Knowledge Extraction Rating), both a benchmark and a replicable methodology based on paradigm inversion - generating documents from known ground truth rather than extracting ground truth from documents. This approach enables deterministic scoring and scalable evaluation without human annotation or reference models, and contamination resistance through regenerable corpora. Our evaluation of 33 models using over 21 billion tokens reveals that context length claims frequently exceed usable capacity, with significant degradation beyond 32K tokens; cross-document aggregation proves substantially harder than single-document extraction; and grounding ability and hallucination resistance are distinct capabilities - models excelling at finding facts that exist may still fabricate facts that do not. Beyond the specific benchmark, we contribute a domain-agnostic methodology for constructing scalable and contamination-resistant evaluations wherever synthetic documents can be generated from structured ground truth.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŸ¥è¯†ç³»ç»Ÿè¯„ä¼°ä¸­é¢ä¸´çš„åŸºå‡†æ±¡æŸ“ã€LLMè¯„åˆ¤åè§ä»¥åŠäººå·¥æ ‡æ³¨æˆæœ¬é«˜ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†RIKERï¼ˆRetrieval Intelligence and Knowledge Extraction Ratingï¼‰è¯„ä¼°æ¡†æ¶ã€‚RIKER é‡‡ç”¨äº†ä¸€ç§èŒƒå¼åè½¬(Paradigm Inversion)çš„æ–¹æ³•ï¼Œé€šè¿‡ä»å·²çŸ¥çš„ Ground Truth ç”Ÿæˆæ–‡æ¡£è€Œéä»æ–‡æ¡£ä¸­æå– Ground Truthï¼Œå®ç°äº†æ— éœ€å‚è€ƒæ¨¡å‹æˆ–äººå·¥æ ‡æ³¨çš„ç¡®å®šæ€§è¯„åˆ†ä¸å¤§è§„æ¨¡è¯„ä¼°ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨å¯å†ç”Ÿè¯­æ–™åº“å¢å¼ºäº†æŠ—æ±¡æŸ“æ€§ï¼Œå¹¶ä¸ºæ„å»ºå¯æ‰©å±•çš„è¯„ä¼°ä½“ç³»æä¾›äº†é€šç”¨æ–¹æ³•ã€‚é€šè¿‡å¯¹33ä¸ªæ¨¡å‹è¿›è¡Œè¶…è¿‡210äº¿ Token çš„æµ‹è¯•ï¼Œç ”ç©¶å‘ç°æ¨¡å‹çš„å®é™…å¯ç”¨ä¸Šä¸‹æ–‡é•¿åº¦å¾€å¾€ä½äºå®£ä¼ å€¼ï¼Œä¸”åœ¨è¶…è¿‡ 32K Token åæ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚å®éªŒè¿›ä¸€æ­¥è¡¨æ˜ï¼Œè·¨æ–‡æ¡£èšåˆ(Cross-document Aggregation)çš„éš¾åº¦æ˜¾è‘—é«˜äºå•æ–‡æ¡£æå–ã€‚æœ€åï¼Œç ”ç©¶æŒ‡å‡ºæ¨¡å‹çš„ Grounding Ability ä¸ Hallucination Resistance æ˜¯ä¸¤ç§æˆªç„¶ä¸åŒçš„èƒ½åŠ›ï¼Œæ“…é•¿æ£€ç´¢äº‹å®çš„æ¨¡å‹æœªå¿…èƒ½æœ‰æ•ˆæŠµæŠ—å¹»è§‰ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "26 pages, 17 tables, 1 figure",
      "pdf_url": "https://arxiv.org/pdf/2601.08847v2",
      "published_date": "2025-12-22 11:58:50 UTC",
      "updated_date": "2026-01-15 08:39:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:09:17.038929+00:00"
    },
    {
      "arxiv_id": "2512.19299v1",
      "title": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application",
      "title_zh": "Heliosï¼šæ™ºæ…§èƒ½æºçŸ¥è¯†æ¨ç†ä¸åº”ç”¨çš„åŸºç¡€è¯­è¨€æ¨¡å‹",
      "authors": [
        "Haoyu Jiang",
        "Fanjie Zeng",
        "Boan Qu",
        "Xiaojie Lin",
        "Wei Zhong"
      ],
      "abstract": "In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Heliosï¼Œä¸€ä¸ªä¸“é—¨ä¸ºæ™ºèƒ½èƒ½æºé¢†åŸŸé‡èº«å®šåˆ¶çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨è§£å†³é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è·¨å­¦ç§‘èƒ½æºçŸ¥è¯†æ¨ç†ä¸­ç¼ºä¹é¢†åŸŸä¸“ä¸šæ€§å’Œç‰©ç†çº¦æŸæ„è¯†çš„é—®é¢˜ã€‚ä¸ºäº†å®ç°ç²¾å‡†çš„å·¥ç¨‹åŒ–ç”Ÿæˆï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†åä¸º Enersys çš„å¤šæ™ºèƒ½ä½“åä½œæ¡†æ¶ï¼Œç”¨äºç«¯åˆ°ç«¯æ„å»ºæ•°æ®é›†ï¼Œå¹¶ç”±æ­¤äº§å‡ºäº†åŸºç¡€çŸ¥è¯†åº“ EnerBaseã€æŒ‡ä»¤å¾®è°ƒæ•°æ®é›† EnerInstruct ä»¥åŠç”¨äºå¯¹é½è¡Œä¸šæ ‡å‡†çš„äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ æ•°æ®é›† EnerReinforceã€‚åŸºäºè¿™äº›é«˜è´¨é‡èµ„æºï¼ŒHelios ç»å†äº†å¤§è§„æ¨¡é¢„è®­ç»ƒã€SFT å’Œ RLHF è¿‡ç¨‹ï¼Œæ„å»ºäº†å®Œæ•´çš„æŠ€æœ¯ç”Ÿæ€ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å‘å¸ƒäº†ä¸“é—¨çš„è¯„ä¼°åŸºå‡† EnerBenchï¼Œå®éªŒç»“æœè¯æ˜ Helios åœ¨é¢†åŸŸçŸ¥è¯†æŒæ¡ã€ä»»åŠ¡æ‰§è¡Œå‡†ç¡®åº¦ä»¥åŠäººç±»åå¥½å¯¹é½æ–¹é¢æ˜¾è‘—ä¼˜äºç°æœ‰é€šç”¨æ¨¡å‹ã€‚è¿™ä¸€æˆæœä¸ºæ™ºèƒ½èƒ½æºç³»ç»Ÿçš„æ·±åº¦åè°ƒå’Œå·¥ä¸šè½¬å‹æä¾›äº†é‡è¦çš„åŸºç¡€æ¨¡å‹æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19299v1",
      "published_date": "2025-12-22 11:43:35 UTC",
      "updated_date": "2025-12-22 11:43:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:09:23.612620+00:00"
    },
    {
      "arxiv_id": "2512.19297v1",
      "title": "Causal-Guided Detoxify Backdoor Attack of Open-Weight LoRA Models",
      "title_zh": "é’ˆå¯¹å¼€æ”¾æƒé‡ LoRA æ¨¡å‹çš„å› æœå¼•å¯¼è§£æ¯’åé—¨æ”»å‡»",
      "authors": [
        "Linzhi Chen",
        "Yang Sun",
        "Hongru Wei",
        "Yuqi Chen"
      ],
      "abstract": "Low-Rank Adaptation (LoRA) has emerged as an efficient method for fine-tuning large language models (LLMs) and is widely adopted within the open-source community. However, the decentralized dissemination of LoRA adapters through platforms such as Hugging Face introduces novel security vulnerabilities: malicious adapters can be easily distributed and evade conventional oversight mechanisms. Despite these risks, backdoor attacks targeting LoRA-based fine-tuning remain relatively underexplored. Existing backdoor attack strategies are ill-suited to this setting, as they often rely on inaccessible training data, fail to account for the structural properties unique to LoRA, or suffer from high false trigger rates (FTR), thereby compromising their stealth. To address these challenges, we propose Causal-Guided Detoxify Backdoor Attack (CBA), a novel backdoor attack framework specifically designed for open-weight LoRA models. CBA operates without access to original training data and achieves high stealth through two key innovations: (1) a coverage-guided data generation pipeline that synthesizes task-aligned inputs via behavioral exploration, and (2) a causal-guided detoxification strategy that merges poisoned and clean adapters by preserving task-critical neurons. Unlike prior approaches, CBA enables post-training control over attack intensity through causal influence-based weight allocation, eliminating the need for repeated retraining. Evaluated across six LoRA models, CBA achieves high attack success rates while reducing FTR by 50-70\\% compared to baseline methods. Furthermore, it demonstrates enhanced resistance to state-of-the-art backdoor defenses, highlighting its stealth and robustness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼€æºæƒé‡Low-Rank Adaptation (LoRA)æ¨¡å‹çš„å®‰å…¨éšæ‚£ï¼Œæå‡ºäº†åä¸ºCausal-Guided Detoxify Backdoor Attack (CBA)çš„æ–°å‹åé—¨æ”»å‡»æ¡†æ¶ã€‚CBAæ— éœ€è®¿é—®åŸå§‹è®­ç»ƒæ•°æ®ï¼Œé€šè¿‡è¦†ç›–å¼•å¯¼çš„æ•°æ®ç”Ÿæˆæµç¨‹åˆ©ç”¨è¡Œä¸ºæ¢ç´¢åˆæˆä»»åŠ¡å¯¹é½çš„è¾“å…¥ï¼Œç¡®ä¿äº†æ”»å‡»çš„éšè”½æ€§ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºé‡‡ç”¨å› æœå¼•å¯¼çš„æ’æ¯’ç­–ç•¥ï¼Œé€šè¿‡è¯†åˆ«å¹¶ä¿ç•™ä»»åŠ¡å…³é”®ç¥ç»å…ƒæ¥èåˆæŠ•æ¯’ä¸å¹²å‡€çš„é€‚é…å™¨ã€‚ä¸åŒäºä¼ ç»Ÿæ–¹æ³•ï¼ŒCBAæ”¯æŒåœ¨è®­ç»ƒåé€šè¿‡åŸºäºå› æœå½±å“çš„æƒé‡åˆ†é…æ¥çµæ´»æ§åˆ¶æ”»å‡»å¼ºåº¦ï¼Œä»è€Œé¿å…äº†ç¹ççš„é‡å¤è®­ç»ƒã€‚å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒCBAåœ¨å…­ä¸ªLoRAæ¨¡å‹ä¸Šå‡å–å¾—äº†æ˜¾è‘—çš„æ”»å‡»æˆåŠŸç‡ï¼Œä¸”è™šå‡è§¦å‘ç‡(False Trigger Rates, FTR)è¾ƒåŸºçº¿é™ä½äº†50-70%ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å±•ç°å‡ºå¯¹å…ˆè¿›åé—¨é˜²å¾¡æ‰‹æ®µçš„å¼ºåŠ²æŠµæŠ—åŠ›ï¼Œå‡¸æ˜¾äº†å¼€æºæ¨¡å‹åˆ†å‘é“¾æ¡ä¸­çš„ç¨³å¥æ€§æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "NDSS 2026",
      "pdf_url": "https://arxiv.org/pdf/2512.19297v1",
      "published_date": "2025-12-22 11:40:47 UTC",
      "updated_date": "2025-12-22 11:40:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:10:25.342743+00:00"
    },
    {
      "arxiv_id": "2512.19287v1",
      "title": "Vibe Reasoning: Eliciting Frontier AI Mathematical Capabilities -- A Case Study on IMO 2025 Problem 6",
      "title_zh": "Vibe Reasoningï¼šæ¿€å‘å‰æ²¿ AI çš„æ•°å­¦èƒ½åŠ›â€”â€”ä»¥ 2025 å¹´ IMO ç¬¬ 6 é¢˜ä¸ºä¾‹",
      "authors": [
        "Jiaao Wu",
        "Xian Zhang",
        "Fan Yang",
        "Yinpeng Dong"
      ],
      "abstract": "We introduce Vibe Reasoning, a human-AI collaborative paradigm for solving complex mathematical problems. Our key insight is that frontier AI models already possess the knowledge required to solve challenging problems -- they simply do not know how, what, or when to apply it. Vibe Reasoning transforms AI's latent potential into manifested capability through generic meta-prompts, agentic grounding, and model orchestration. We demonstrate this paradigm through IMO 2025 Problem 6, a combinatorial optimization problem where autonomous AI systems publicly reported failures. Our solution combined GPT-5's exploratory capabilities with Gemini 3 Pro's proof strengths, leveraging agentic workflows with Python code execution and file-based memory, to derive both the correct answer (2112) and a rigorous mathematical proof. Through iterative refinement across multiple attempts, we discovered the necessity of agentic grounding and model orchestration, while human prompts evolved from problem-specific hints to generic, transferable meta-prompts. We analyze why capable AI fails autonomously, how each component addresses specific failure modes, and extract principles for effective vibe reasoning. Our findings suggest that lightweight human guidance can unlock frontier models' mathematical reasoning potential. This is ongoing work; we are developing automated frameworks and conducting broader evaluations to further validate Vibe Reasoning's generality and effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† Vibe Reasoningï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨è§£å†³å¤æ‚æ•°å­¦é—®é¢˜çš„äººæœºåä½œèŒƒå¼ï¼Œå…¶æ ¸å¿ƒæ˜¯é€šè¿‡é€šç”¨å…ƒæç¤º(generic meta-prompts)ã€æ™ºèƒ½ä½“è½åœ°(agentic grounding)å’Œæ¨¡å‹ç¼–æ’(model orchestration)æ¥æ¿€å‘å‰æ²¿ AI æ¨¡å‹çš„æ½œåœ¨èƒ½åŠ›ã€‚ä»¥ 2025 å¹´å›½é™…æ•°å­¦å¥¥æ—åŒ¹å…‹(IMO)ç¬¬ 6 é¢˜ä¸ºæ¡ˆä¾‹ï¼Œè¯¥æ–¹æ³•æ•´åˆäº† GPT-5 çš„æ¢ç´¢èƒ½åŠ›ä¸ Gemini 3 Pro çš„è¯æ˜ä¼˜åŠ¿ï¼Œå¹¶åˆ©ç”¨ Python ä»£ç æ‰§è¡Œå’ŒåŸºäºæ–‡ä»¶çš„å­˜å‚¨ç³»ç»Ÿæ„å»ºäº†æ™ºèƒ½ä½“å·¥ä½œæµã€‚è¯¥èŒƒå¼æˆåŠŸæ¨å¯¼å‡ºäº†è¯¥ç»„åˆä¼˜åŒ–é—®é¢˜çš„æ­£ç¡®ç­”æ¡ˆï¼ˆ2112ï¼‰å¹¶æä¾›äº†ä¸¥å¯†çš„æ•°å­¦è¯æ˜ï¼Œæœ‰æ•ˆå¼¥è¡¥äº†ç°æœ‰è‡ªä¸» AI ç³»ç»Ÿåœ¨å¤„ç†æ­¤ç±»å¤æ‚éš¾é¢˜æ—¶çš„èƒ½åŠ›çŸ­æ¿ã€‚ç ”ç©¶åˆ†ææŒ‡å‡ºï¼Œå‰æ²¿æ¨¡å‹è™½ç„¶æ‹¥æœ‰è§£é¢˜æ‰€éœ€çš„çŸ¥è¯†å‚¨å¤‡ï¼Œä½†å¿…é¡»é€šè¿‡æœ‰æ•ˆçš„è½åœ°æœºåˆ¶å’Œç¼–æ’æ‰èƒ½å°†å…¶è½¬åŒ–ä¸ºå®é™…æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸€å‘ç°è¡¨æ˜è½»é‡çº§çš„äººç±»æŒ‡å¯¼èƒ½å¤Ÿæ˜¾è‘—è§£é”æ¨¡å‹çš„æ•°å­¦æ¨ç†æ½œåŠ›ï¼Œä¸ºæœªæ¥å¼€å‘æ›´é€šç”¨çš„è‡ªåŠ¨åŒ–æ•°å­¦è§£é¢˜æ¡†æ¶æä¾›äº†é‡è¦åŸåˆ™ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "20 pages, 13 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.19287v1",
      "published_date": "2025-12-22 11:30:19 UTC",
      "updated_date": "2025-12-22 11:30:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:11:51.105629+00:00"
    },
    {
      "arxiv_id": "2512.19280v1",
      "title": "Digital Twin-Driven Zero-Shot Fault Diagnosis of Axial Piston Pumps Using Fluid-Borne Noise Signals",
      "title_zh": "åŸºäºæµä½“ä¼ æ’­å™ªå£°ä¿¡å·çš„æ•°å­—å­ªç”Ÿé©±åŠ¨è½´å‘æŸ±å¡æ³µé›¶æ ·æœ¬æ•…éšœè¯Šæ–­",
      "authors": [
        "Chang Dong",
        "Jianfeng Tao",
        "Chengliang Liu"
      ],
      "abstract": "Axial piston pumps are crucial components in fluid power systems, where reliable fault diagnosis is essential for ensuring operational safety and efficiency. Traditional data-driven methods require extensive labeled fault data, which is often impractical to obtain, while model-based approaches suffer from parameter uncertainties. This paper proposes a digital twin (DT)-driven zero-shot fault diagnosis framework utilizing fluid-borne noise (FBN) signals. The framework calibrates a high-fidelity DT model using only healthy-state data, generates synthetic fault signals for training deep learning classifiers, and employs a physics-informed neural network (PINN) as a virtual sensor for flow ripple estimation. Gradient-weighted class activation mapping (Grad-CAM) is integrated to visualize the decision-making process of neural networks, revealing that large kernels matching the subsequence length in time-domain inputs and small kernels in time-frequency domain inputs enable higher diagnostic accuracy by focusing on physically meaningful features. Experimental validations demonstrate that training on signals from the calibrated DT model yields diagnostic accuracies exceeding 95\\% on real-world benchmarks, while uncalibrated models result in significantly lower performance, highlighting the framework's effectiveness in data-scarce scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è½´å‘æŸ±å¡æ³µ(Axial piston pumps)æ•…éšœè¯Šæ–­ä¸­æ ‡è®°æ•°æ®éš¾ä»¥è·å–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæµä½“å™ªå£°(Fluid-borne noise, FBN)ä¿¡å·çš„æ•°å­—å­ªç”Ÿ(Digital twin, DT)é©±åŠ¨é›¶æ ·æœ¬(Zero-shot)è¯Šæ–­æ¡†æ¶ã€‚è¯¥æ¡†æ¶ä»…åˆ©ç”¨å¥åº·çŠ¶æ€æ•°æ®æ ¡å‡†é«˜ä¿çœŸDTæ¨¡å‹ï¼Œå¹¶ç”Ÿæˆåˆæˆæ•…éšœæ•°æ®ä»¥è®­ç»ƒæ·±åº¦å­¦ä¹ åˆ†ç±»å™¨ï¼ŒåŒæ—¶é›†æˆç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ(PINN)ä½œä¸ºè™šæ‹Ÿä¼ æ„Ÿå™¨è¿›è¡Œæµé‡è„‰åŠ¨ä¼°è®¡ã€‚ç ”ç©¶é€šè¿‡æ¢¯åº¦åŠ æƒç±»æ¿€æ´»æ˜ å°„(Grad-CAM)å¯è§†åŒ–å†³ç­–è¿‡ç¨‹ï¼Œå‘ç°æ—¶åŸŸçš„å¤§å·ç§¯æ ¸ä¸æ—¶é¢‘åŸŸçš„å°å·ç§¯æ ¸é€šè¿‡å…³æ³¨ç‰©ç†ç‰¹å¾èƒ½æ˜¾è‘—æå‡ç²¾åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨çœŸå®åœºæ™¯åŸºå‡†æµ‹è¯•ä¸­çš„å‡†ç¡®ç‡è¶…è¿‡95%ï¼Œç›¸è¾ƒäºæœªæ ¡å‡†æ¨¡å‹è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™è¯æ˜äº†è¯¥æ–¹æ³•åœ¨æ•°æ®ç¨€ç¼ºç¯å¢ƒä¸‹å®ç°é«˜æ•ˆã€é«˜ç²¾åº¦å·¥ä¸šæ•…éšœè¯Šæ–­çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19280v1",
      "published_date": "2025-12-22 11:24:42 UTC",
      "updated_date": "2025-12-22 11:24:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:11:44.315253+00:00"
    },
    {
      "arxiv_id": "2512.19275v1",
      "title": "Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation",
      "title_zh": "è§†è§‰çœŸå®æ„Ÿæ˜¯å¦è¶³å¤Ÿï¼Ÿè¯„ä¼°ç”Ÿæˆå¼äººå·¥æ™ºèƒ½äººä½“åŠ¨ç”»ä¸­çš„æ­¥æ€ç”Ÿç‰©ç‰¹å¾ä¿çœŸåº¦",
      "authors": [
        "Ivan DeAndres-Tame",
        "Chengwei Ye",
        "Ruben Tolosana",
        "Ruben Vera-Rodriguez",
        "Shiqi Yu"
      ],
      "abstract": "Generative AI (GenAI) models have revolutionized animation, enabling the synthesis of humans and motion patterns with remarkable visual fidelity. However, generating truly realistic human animation remains a formidable challenge, where even minor inconsistencies can make a subject appear unnatural. This limitation is particularly critical when AI-generated videos are evaluated for behavioral biometrics, where subtle motion cues that define identity are easily lost or distorted. The present study investigates whether state-of-the-art GenAI human animation models can preserve the subtle spatio-temporal details needed for person identification through gait biometrics. Specifically, we evaluate four different GenAI models across two primary evaluation tasks to assess their ability to i) restore gait patterns from reference videos under varying conditions of complexity, and ii) transfer these gait patterns to different visual identities. Our results show that while visual quality is mostly high, biometric fidelity remains low in tasks focusing on identification, suggesting that current GenAI models struggle to disentangle identity from motion. Furthermore, through an identity transfer task, we expose a fundamental flaw in appearance-based gait recognition: when texture is disentangled from motion, identification collapses, proving current GenAI models rely on visual attributes rather than temporal dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ï¼ˆGenAIï¼‰æ¨¡å‹åœ¨ç”Ÿæˆäººç±»åŠ¨ç”»æ—¶ï¼Œæ˜¯å¦èƒ½å¤Ÿä¿ç•™ç”¨äºä¸ªäººèº«ä»½è¯†åˆ«çš„æ­¥æ€ç”Ÿç‰©è¯†åˆ«ï¼ˆGait Biometricï¼‰å¾®è§‚æ—¶ç©ºç»†èŠ‚ã€‚ç ”ç©¶äººå‘˜è¯„ä¼°äº†å››ç§æœ€å…ˆè¿›çš„ GenAI æ¨¡å‹ï¼Œé€šè¿‡åœ¨ä¸åŒå¤æ‚æ¡ä»¶ä¸‹æ¢å¤å‚è€ƒè§†é¢‘çš„æ­¥æ€æ¨¡å¼ï¼Œä»¥åŠå°†æ­¥æ€æ¨¡å¼è¿ç§»è‡³ä¸åŒè§†è§‰èº«ä»½ï¼ˆIdentity Transferï¼‰è¿™ä¸¤é¡¹æ ¸å¿ƒä»»åŠ¡ï¼Œæµ‹è¯•å…¶ç”Ÿç‰©è¯†åˆ«ä¿çœŸåº¦ï¼ˆBiometric Fidelityï¼‰ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡ç”Ÿæˆçš„è§†é¢‘åœ¨è§†è§‰è´¨é‡ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨èº«ä»½è¯†åˆ«ä»»åŠ¡ä¸­çš„ä¿çœŸåº¦ä¾ç„¶è¾ƒä½ï¼Œè¡¨æ˜å½“å‰çš„ GenAI æ¨¡å‹éš¾ä»¥å°†èº«ä»½ä¿¡æ¯ä¸åŠ¨ä½œç‰¹å¾æœ‰æ•ˆè§£è€¦ã€‚æ­¤å¤–ï¼Œé€šè¿‡èº«ä»½è½¬ç§»ä»»åŠ¡ï¼Œç ”ç©¶æ­ç¤ºäº†å½“å‰æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­è¿‡åº¦ä¾èµ–è§†è§‰å±æ€§è€Œéæ—¶é—´åŠ¨æ€ï¼ˆTemporal Dynamicsï¼‰ï¼Œå¯¼è‡´å½“çº¹ç†ä¸åŠ¨ä½œåˆ†ç¦»æ—¶è¯†åˆ«æ€§èƒ½å¤§å¹…ä¸‹é™ã€‚è¯¥å‘ç°æš´éœ²äº†ç°æœ‰åŸºäºå¤–è§‚çš„æ­¥æ€è¯†åˆ«æŠ€æœ¯çš„æ ¹æœ¬ç¼ºé™·ï¼Œä¸ºæœªæ¥å¼€å‘æ›´å…·ç”Ÿç‰©è¯†åˆ«ä¸€è‡´æ€§çš„äººç±»åŠ¨ç”»æ¨¡å‹æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19275v1",
      "published_date": "2025-12-22 11:19:46 UTC",
      "updated_date": "2025-12-22 11:19:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:11:39.026294+00:00"
    },
    {
      "arxiv_id": "2512.22217v1",
      "title": "VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition",
      "title_zh": "VLM-PARï¼šé¢å‘è¡Œäººå±æ€§è¯†åˆ«çš„è§†è§‰è¯­è¨€æ¨¡å‹",
      "authors": [
        "Abdellah Zakaria Sellam",
        "Salah Eddine Bekhouche",
        "Fadi Dornaika",
        "Cosimo Distante",
        "Abdenour Hadid"
      ],
      "abstract": "Pedestrian Attribute Recognition (PAR) involves predicting fine-grained attributes such as clothing color, gender, and accessories from pedestrian imagery, yet is hindered by severe class imbalance, intricate attribute co-dependencies, and domain shifts. We introduce VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders. By first aligning image and prompt embeddings via refining visual features through a compact cross-attention fusion, VLM-PAR achieves significant accuracy improvement on the highly imbalanced PA100K benchmark, setting a new state-of-the-art performance, while also delivering significant gains in mean accuracy across PETA and Market-1501 benchmarks. These results underscore the efficacy of integrating large-scale vision-language pretraining with targeted cross-modal refinement to overcome imbalance and generalization challenges in PAR.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VLM-PARï¼Œä¸€ç§ä¸“é—¨ç”¨äºè¡Œäººå±æ€§è¯†åˆ«(Pedestrian Attribute Recognition, PAR)çš„æ¨¡å—åŒ–è§†è§‰è¯­è¨€æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è¯¥é¢†åŸŸä¸­ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡ã€å¤æ‚çš„å±æ€§ç›¸å…³æ€§ä»¥åŠé¢†åŸŸåç§»ç­‰éš¾é¢˜ã€‚è¯¥æ¡†æ¶åŸºäºå†»ç»“çš„SigLIP 2å¤šè¯­è¨€ç¼–ç å™¨æ„å»ºï¼Œé€šè¿‡ç´§å‡‘çš„äº¤å‰æ³¨æ„åŠ›èåˆ(cross-attention fusion)æç‚¼è§†è§‰ç‰¹å¾ï¼Œå®ç°äº†å›¾åƒä¸æç¤ºè¯åµŒå…¥(prompt embeddings)çš„é«˜æ•ˆå¯¹é½ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVLM-PARåœ¨é«˜åº¦ä¸å¹³è¡¡çš„PA100KåŸºå‡†æµ‹è¯•ä¸Šåˆ·æ–°äº†å½“å‰æœ€å…ˆè¿›çš„æ€§èƒ½è®°å½•(State-of-the-art)ï¼Œå¹¶åœ¨PETAå’ŒMarket-1501åŸºå‡†ä¸Šå–å¾—äº†æ˜¾è‘—çš„å¹³å‡å‡†ç¡®ç‡æå‡ã€‚è¿™äº›æˆæœè¯æ˜äº†å°†å¤§è§„æ¨¡è§†è§‰è¯­è¨€é¢„è®­ç»ƒä¸é’ˆå¯¹æ€§çš„è·¨æ¨¡æ€ç»†åŒ–ç›¸ç»“åˆï¼Œèƒ½æœ‰æ•ˆå…‹æœPARä»»åŠ¡ä¸­çš„ä¸å¹³è¡¡å’Œæ³›åŒ–æŒ‘æˆ˜ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22217v1",
      "published_date": "2025-12-22 11:19:04 UTC",
      "updated_date": "2025-12-22 11:19:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:11:45.298436+00:00"
    },
    {
      "arxiv_id": "2601.00814v2",
      "title": "Semantic Alignment of Multilingual Knowledge Graphs via Contextualized Vector Projections",
      "title_zh": "åŸºäºä¸Šä¸‹æ–‡å‘é‡æŠ•å½±çš„å¤šè¯­è¨€çŸ¥è¯†å›¾è°±è¯­ä¹‰å¯¹é½",
      "authors": [
        "Abhishek Kumar"
      ],
      "abstract": "The paper presents our work on cross-lingual ontology alignment system which uses embedding based cosine similarity matching. The ontology entities are made contextually richer by creating descriptions using novel techniques. We use a fine-tuned transformer based multilingual model for generating better embeddings. We use cosine similarity to find positive ontology entities pairs and then apply threshold filtering to retain only highly similar entities. We have evaluated our work on OAEI-2022 multifarm track. We achieve 71% F1 score (78% recall and 65% precision) on the evaluation dataset, 16% increase from best baseline score. This suggests that our proposed alignment pipeline is able to capture the subtle cross-lingual similarities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§è·¨è¯­è¨€æœ¬ä½“å¯¹é½(cross-lingual ontology alignment)ç³»ç»Ÿï¼Œæ ¸å¿ƒé‡‡ç”¨åŸºäºåµŒå…¥(embedding)çš„ä½™å¼¦ç›¸ä¼¼åº¦åŒ¹é…æŠ€æœ¯ã€‚ç ”ç©¶é€šè¿‡æ–°é¢–çš„æè¿°ç”ŸæˆæŠ€æœ¯èµ‹äºˆæœ¬ä½“å®ä½“æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡è¯­ä¹‰ï¼Œå¹¶åˆ©ç”¨å¾®è°ƒçš„ Transformer å¤šè¯­è¨€æ¨¡å‹ç”Ÿæˆæ›´é«˜è´¨é‡çš„å‘é‡è¡¨ç¤ºã€‚åœ¨å¤„ç†è¿‡ç¨‹ä¸­ï¼Œç³»ç»Ÿè®¡ç®—å®ä½“é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦å¹¶åº”ç”¨é˜ˆå€¼è¿‡æ»¤ï¼Œä»è€Œç²¾ç¡®æå–é«˜ç›¸ä¼¼åº¦çš„å¯¹é½å¯¹ã€‚åœ¨ OAEI-2022 multifarm track çš„è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ³•å–å¾—äº† 71% çš„ F1 åˆ†æ•°ï¼Œè¾ƒç°æœ‰æœ€ä½³åŸºå‡†æå‡äº† 16%ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å¯¹é½æµæ°´çº¿èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰å¤æ‚çš„è·¨è¯­è¨€ç›¸ä¼¼æ€§ï¼Œæ˜¾è‘—æå‡äº†å¤šè¯­è¨€çŸ¥è¯†å›¾è°±çš„è¯­ä¹‰å¯¹é½æ•ˆæœã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00814v2",
      "published_date": "2025-12-22 11:02:30 UTC",
      "updated_date": "2026-01-20 17:52:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:11:44.939664+00:00"
    },
    {
      "arxiv_id": "2512.19253v2",
      "title": "Machine Unlearning in the Era of Quantum Machine Learning: An Empirical Study",
      "title_zh": "é‡å­æœºå™¨å­¦ä¹ æ—¶ä»£çš„æœºå™¨é—å¿˜ï¼šä¸€é¡¹å®è¯ç ”ç©¶",
      "authors": [
        "Carla Crivoi",
        "Radu Tudor Ionescu"
      ],
      "abstract": "We present the first comprehensive empirical study of machine unlearning (MU) in hybrid quantum-classical neural networks. While MU has been extensively explored in classical deep learning, its behavior within variational quantum circuits (VQCs) and quantum-augmented architectures remains largely unexplored. First, we adapt a broad suite of unlearning methods to quantum settings, including gradient-based, distillation-based, regularization-based and certified techniques. Second, we introduce two new unlearning strategies tailored to hybrid models. Experiments across Iris, MNIST, and Fashion-MNIST, under both subset removal and full-class deletion, reveal that quantum models can support effective unlearning, but outcomes depend strongly on circuit depth, entanglement structure, and task complexity. Shallow VQCs display high intrinsic stability with minimal memorization, whereas deeper hybrid models exhibit stronger trade-offs between utility, forgetting strength, and alignment with retrain oracle. We find that certain methods, e.g. EU-k, LCA, and Certified Unlearning, consistently provide the best balance across metrics. These findings establish baseline empirical insights into quantum machine unlearning and highlight the need for quantum-aware algorithms and theoretical guarantees, as quantum machine learning systems continue to expand in scale and capability. We publicly release our code at: https://github.com/CrivoiCarla/HQML.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹æ··åˆé‡å­-ç»å…¸ç¥ç»ç½‘ç»œä¸­çš„æœºå™¨å¸è½½(Machine Unlearning, MU)è¿›è¡Œäº†é¦–æ¬¡å…¨é¢çš„å®è¯ç ”ç©¶ï¼Œå¡«è¡¥äº†å˜åˆ†é‡å­ç”µè·¯(VQCs)å’Œé‡å­å¢å¼ºæ¶æ„åœ¨æ•°æ®æ¶ˆé™¤è¡Œä¸ºæ–¹é¢çš„ç ”ç©¶ç©ºç™½ã€‚ä½œè€…å°†åŒ…æ‹¬æ¢¯åº¦åŸºã€è’¸é¦åŸºã€æ­£åˆ™åŒ–åŸºå’Œè®¤è¯æŠ€æœ¯(Certified Unlearning)åœ¨å†…çš„å¤šç§å¸è½½æ–¹æ³•é€‚é…è‡³é‡å­ç¯å¢ƒï¼Œå¹¶æå‡ºäº†ä¸¤ç§ä¸“ä¸ºæ··åˆæ¨¡å‹è®¾è®¡çš„å…¨æ–°ç­–ç•¥ã€‚é€šè¿‡åœ¨Irisã€MNISTå’ŒFashion-MNISTæ•°æ®é›†ä¸Šè¿›è¡Œå­é›†ç§»é™¤ä¸å…¨ç±»åˆ é™¤å®éªŒï¼Œç ”ç©¶å‘ç°é‡å­æ¨¡å‹è™½èƒ½å®ç°æœ‰æ•ˆå¸è½½ï¼Œä½†å…¶è¡¨ç°é«˜åº¦ä¾èµ–äºç”µè·¯æ·±åº¦ã€çº ç¼ ç»“æ„å’Œä»»åŠ¡å¤æ‚åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæµ…å±‚VQCså…·æœ‰æé«˜çš„å†…åœ¨ç¨³å®šæ€§ä¸”è®°å¿†æ•ˆåº”æä½ï¼Œè€Œæ·±å±‚æ··åˆæ¨¡å‹åœ¨æ•ˆç”¨ã€å¿˜è®°å¼ºåº¦åŠä¸é‡æ–°è®­ç»ƒåŸºå‡†(retrain oracle)çš„ä¸€è‡´æ€§ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„æƒè¡¡ã€‚ç ”ç©¶æŒ‡å‡ºEU-kã€LCAå’ŒCertified Unlearningç­‰æ–¹æ³•åœ¨å„é¡¹è¯„ä¼°æŒ‡æ ‡ä¸­è¡¨ç°æœ€ä¸ºå‡è¡¡ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥é‡å­æ„ŸçŸ¥ç®—æ³•çš„è®¾è®¡æä¾›äº†åŸºå‡†è§è§£ï¼Œä¹Ÿä¸ºå¯æ‰©å±•ã€é«˜èƒ½åŠ›çš„é‡å­æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„éšç§ä¿æŠ¤å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19253v2",
      "published_date": "2025-12-22 10:40:03 UTC",
      "updated_date": "2025-12-23 13:00:45 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:11:55.016025+00:00"
    },
    {
      "arxiv_id": "2512.19247v1",
      "title": "Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics",
      "title_zh": "é’ˆå¯¹ç‰©æµé¢†åŸŸæ¡†æ¶æ£€æµ‹çš„æ£€ç´¢å¼•å¯¼è‡ªåŠ¨æç¤ºæ–¹æ³•",
      "authors": [
        "Do Minh Duc",
        "Quan Xuan Truong",
        "Nguyen Tat Dat",
        "Nguyen Van Vinh"
      ],
      "abstract": "Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹ç‰©æµé¢†åŸŸæ¡†æ¶æ£€æµ‹ï¼ˆFrame Detectionï¼‰çš„æ–°å‹æç¤ºè¯ä¼˜åŒ–æ¡†æ¶ï¼Œæ—¨åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å¤„ç†å¤æ‚é¢†åŸŸæ–‡æœ¬æ ‡æ³¨çš„æ€§èƒ½ã€‚è¯¥æ–¹æ¡ˆé›†æˆäº†æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€å°‘æ ·æœ¬æç¤ºï¼ˆFew-shot Promptingï¼‰ã€é“¾å¼æ€ç»´ï¼ˆCoTï¼‰åŠè‡ªåŠ¨é“¾å¼æ€ç»´åˆæˆï¼ˆAuto-CoTï¼‰æŠ€æœ¯ï¼Œå…¶æ ¸å¿ƒæ˜¯é€šè¿‡ä¸€ä¸ªåŸºäºLLMçš„æç¤ºè¯ä¼˜åŒ–æ™ºèƒ½ä½“ï¼ˆOptimizer Agentï¼‰åˆ©ç”¨æ£€ç´¢ç¤ºä¾‹å’Œåé¦ˆè¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨çœŸå®ç‰©æµæ ‡æ³¨ä»»åŠ¡ä¸­ï¼Œä¼˜åŒ–åçš„æç¤ºè¯ç›¸è¾ƒäºé›¶æ ·æœ¬æˆ–é™æ€åŸºçº¿æœ€é«˜æå‡äº†15%çš„æ¨ç†å‡†ç¡®ç‡ã€‚è¯¥ç³»ç»Ÿåœ¨GPT-4oã€Qwen 2.5å’ŒLLaMA 3.1ç­‰å¤šä¸ªä¸»æµæ¨¡å‹ä¸Šå‡è¡¨ç°å‡ºæ˜¾è‘—ä¸”ä¸€è‡´çš„æ”¹è¿›ï¼ŒéªŒè¯äº†å…¶ä¼˜å¼‚çš„é€šç”¨æ€§ä¸å®ç”¨ä»·å€¼ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»“æ„åŒ–æç¤ºä¼˜åŒ–å¯ä»¥ä½œä¸ºå…¨å‚æ•°å¾®è°ƒï¼ˆFull Fine-tuningï¼‰çš„é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆï¼Œä¸ºç‰©æµç­‰ç‰¹å®šé¢†åŸŸçš„è‡ªç„¶è¯­è¨€å¤„ç†åº”ç”¨æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”ä½æˆæœ¬çš„éƒ¨ç½²é€”å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19247v1",
      "published_date": "2025-12-22 10:29:51 UTC",
      "updated_date": "2025-12-22 10:29:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:12:18.809130+00:00"
    },
    {
      "arxiv_id": "2512.19240v1",
      "title": "ChemATP: A Training-Free Chemical Reasoning Framework for Large Language Models",
      "title_zh": "ChemATPï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹çš„å…è®­ç»ƒåŒ–å­¦æ¨ç†æ¡†æ¶",
      "authors": [
        "Mingxu Zhang",
        "Dazhong Shen",
        "Qi Zhang",
        "Ying Sun"
      ],
      "abstract": "Large Language Models (LLMs) exhibit strong general reasoning but struggle in molecular science due to the lack of explicit chemical priors in standard string representations. Current solutions face a fundamental dilemma. Training-based methods inject priors into parameters, but this static coupling hinders rapid knowledge updates and often compromises the model's general reasoning capabilities. Conversely, existing training-free methods avoid these issues but rely on surface-level prompting, failing to provide the fine-grained atom-level priors essential for precise chemical reasoning. To address this issue, we introduce ChemATP, a framework that decouples chemical knowledge from the reasoning engine. By constructing the first atom-level textual knowledge base, ChemATP enables frozen LLMs to explicitly retrieve and reason over this information dynamically. This architecture ensures interpretability and adaptability while preserving the LLM's intrinsic general intelligence. Experiments show that ChemATP significantly outperforms training-free baselines and rivals state-of-the-art training-based models, demonstrating that explicit prior injection is a competitive alternative to implicit parameter updates.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ChemATPï¼Œä¸€ä¸ªæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨åˆ†å­ç§‘å­¦é¢†åŸŸå› ç¼ºä¹æ˜¾å¼åŒ–å­¦å…ˆéªŒ(chemical priors)è€Œå¯¼è‡´æ¨ç†å›°éš¾çš„å…è®­ç»ƒ(training-free)æ¡†æ¶ã€‚ä¼ ç»Ÿæ–¹æ³•è¦ä¹ˆä¾èµ–äºç‰ºç‰²é€šç”¨æ¨ç†èƒ½åŠ›çš„å‚æ•°åŒ–è®­ç»ƒï¼Œè¦ä¹ˆä»…ä½¿ç”¨è¡¨å±‚æç¤º(prompting)è€Œç¼ºä¹ç²¾ç¡®åŒ–å­¦æ¨ç†æ‰€éœ€çš„åŸå­çº§å…ˆéªŒã€‚ChemATPé€šè¿‡æ„å»ºé¦–ä¸ªåŸå­çº§æ–‡æœ¬çŸ¥è¯†åº“ï¼Œå°†åŒ–å­¦çŸ¥è¯†ä¸æ¨ç†å¼•æ“è§£è€¦ï¼Œä½¿å†»ç»“çš„LLMsèƒ½å¤ŸåŠ¨æ€æ£€ç´¢å¹¶åŸºäºè¯¥ä¿¡æ¯è¿›è¡Œæ¨ç†ã€‚è¿™ç§æ¶æ„åœ¨ä¿æŒLLMå›ºæœ‰é€šç”¨æ™ºèƒ½çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†ç³»ç»Ÿçš„å¯è§£é‡Šæ€§(interpretability)å’Œé€‚åº”æ€§(adaptability)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒChemATPçš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„å…è®­ç»ƒåŸºå‡†ï¼Œå¹¶è¾¾åˆ°äº†ä¸æœ€å…ˆè¿›çš„åŸºäºè®­ç»ƒçš„æ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚è¿™è¯æ˜äº†æ˜¾å¼å…ˆéªŒæ³¨å…¥(explicit prior injection)æ˜¯æ›¿ä»£éšå¼å‚æ•°æ›´æ–°(implicit parameter updates)çš„æå…·ç«äº‰åŠ›çš„æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19240v1",
      "published_date": "2025-12-22 10:21:40 UTC",
      "updated_date": "2025-12-22 10:21:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:12:06.340215+00:00"
    },
    {
      "arxiv_id": "2512.19238v1",
      "title": "Identifying Features Associated with Bias Against 93 Stigmatized Groups in Language Models and Guardrail Model Safety Mitigation",
      "title_zh": "è¯†åˆ«è¯­è¨€æ¨¡å‹ä¸­é’ˆå¯¹ 93 ä¸ªå—æ±¡ååŒ–ç¾¤ä½“çš„åè§å…³è”ç‰¹å¾åŠæŠ¤æ æ¨¡å‹å®‰å…¨ç¼“è§£",
      "authors": [
        "Anna-Maria Gueorguieva",
        "Aylin Caliskan"
      ],
      "abstract": "Large language models (LLMs) have been shown to exhibit social bias, however, bias towards non-protected stigmatized identities remain understudied. Furthermore, what social features of stigmas are associated with bias in LLM outputs is unknown. From psychology literature, it has been shown that stigmas contain six shared social features: aesthetics, concealability, course, disruptiveness, origin, and peril. In this study, we investigate if human and LLM ratings of the features of stigmas, along with prompt style and type of stigma, have effect on bias towards stigmatized groups in LLM outputs. We measure bias against 93 stigmatized groups across three widely used LLMs (Granite 3.0-8B, Llama-3.1-8B, Mistral-7B) using SocialStigmaQA, a benchmark that includes 37 social scenarios about stigmatized identities; for example deciding wether to recommend them for an internship. We find that stigmas rated by humans to be highly perilous (e.g., being a gang member or having HIV) have the most biased outputs from SocialStigmaQA prompts (60% of outputs from all models) while sociodemographic stigmas (e.g. Asian-American or old age) have the least amount of biased outputs (11%). We test if the amount of biased outputs could be decreased by using guardrail models, models meant to identify harmful input, using each LLM's respective guardrail model (Granite Guardian 3.0, Llama Guard 3.0, Mistral Moderation API). We find that bias decreases significantly by 10.4%, 1.4%, and 7.8%, respectively. However, we show that features with significant effect on bias remain unchanged post-mitigation and that guardrail models often fail to recognize the intent of bias in prompts. This work has implications for using LLMs in scenarios involving stigmatized groups and we suggest future work towards improving guardrail models for bias mitigation.",
      "tldr_zh": "è¯¥ç ”ç©¶è°ƒæŸ¥äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)å¯¹93ä¸ªå—æ±¡ååŒ–ç¾¤ä½“(stigmatized groups)çš„ç¤¾ä¼šåè§ï¼Œå¹¶åˆ†æäº†å¿ƒç†å­¦ä¸­å…­ç§ç¤¾ä¼šç‰¹å¾(aesthetics, concealability, course, disruptiveness, origin, peril)å¯¹åè§äº§ç”Ÿçš„å½±å“ã€‚ç ”ç©¶è€…åˆ©ç”¨SocialStigmaQAåŸºå‡†è¯„ä¼°äº†Granite 3.0-8Bã€Llama-3.1-8Bå’ŒMistral-7Bæ¨¡å‹ï¼Œå‘ç°è¢«äººç±»è§†ä¸ºé«˜å±é™©æ€§(peril)çš„ç¾¤ä½“ï¼ˆå¦‚é»‘å¸®æˆå‘˜æˆ–HIVæ„ŸæŸ“è€…ï¼‰é­å—çš„åè§è¾“å‡ºæ¯”ä¾‹é«˜è¾¾60%ï¼Œè€Œç¤¾ä¼šäººå£ç»Ÿè®¡ç±»æ±¡åçš„åè§æ¯”ä¾‹ä»…ä¸º11%ã€‚è™½ç„¶åº”ç”¨Granite Guardianã€Llama Guardç­‰æŠ¤æ æ¨¡å‹(guardrail models)èƒ½å°†åè§è¾“å‡ºåˆ†åˆ«é™ä½10.4%ã€1.4%å’Œ7.8%ï¼Œä½†æ˜¾è‘—å½±å“åè§çš„ç¤¾ä¼šç‰¹å¾åœ¨ç¼“è§£åä¾ç„¶å­˜åœ¨ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†æŠ¤æ æ¨¡å‹åœ¨è¯†åˆ«å¤æ‚åè§æ„å›¾æ–¹é¢çš„å±€é™æ€§ï¼Œä¸ºæœªæ¥å¼€å‘æ›´æœ‰æ•ˆçš„åè§ç¼“è§£ç­–ç•¥æä¾›äº†é‡è¦ä¾æ®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19238v1",
      "published_date": "2025-12-22 10:20:20 UTC",
      "updated_date": "2025-12-22 10:20:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:12:16.771049+00:00"
    },
    {
      "arxiv_id": "2512.19234v1",
      "title": "DeliveryBench: Can Agents Earn Profit in Real World?",
      "title_zh": "DeliveryBenchï¼šæ™ºèƒ½ä½“èƒ½å¦åœ¨ç°å®ä¸–ç•Œä¸­è·åˆ©ï¼Ÿ",
      "authors": [
        "Lingjun Mao",
        "Jiawei Ren",
        "Kun Zhou",
        "Jixuan Chen",
        "Ziqiao Ma",
        "Lianhui Qin"
      ],
      "abstract": "LLMs and VLMs are increasingly deployed as embodied agents, yet existing benchmarks largely revolve around simple short-term tasks and struggle to capture rich realistic constraints that shape real-world decision making. To close this gap, we propose DeliveryBench, a city-scale embodied benchmark grounded in the real-world profession of food delivery. Food couriers naturally operate under long-horizon objectives (maximizing net profit over hours) while managing diverse constraints, e.g., delivery deadline, transportation expense, vehicle battery, and necessary interactions with other couriers and customers. DeliveryBench instantiates this setting in procedurally generated 3D cities with diverse road networks, buildings, functional locations, transportation modes, and realistic resource dynamics, enabling systematic evaluation of constraint-aware, long-horizon planning. We benchmark a range of VLM-based agents across nine cities and compare them with human players. Our results reveal a substantial performance gap to humans, and find that these agents are short-sighted and frequently break basic commonsense constraints. Additionally, we observe distinct personalities across models (e.g., adventurous GPT-5 vs. conservative Claude), highlighting both the brittleness and the diversity of current VLM-based embodied agents in realistic, constraint-dense environments. Our code, data, and benchmark are available at https://deliverybench.github.io.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DeliveryBenchï¼Œè¿™æ˜¯ä¸€ä¸ªåŸºäºå¤–å–é…é€çœŸå®èŒä¸šåœºæ™¯çš„åŸå¸‚è§„æ¨¡å…·èº«æ™ºèƒ½åŸºå‡†(embodied benchmark)ï¼Œæ—¨åœ¨å¡«è¡¥ç°æœ‰åŸºå‡†åœ¨å¤„ç†å¤æ‚ç°å®çº¦æŸå’Œé•¿æœŸå†³ç­–æ–¹é¢çš„ç©ºç™½ã€‚DeliveryBench åœ¨ç¨‹åºç”Ÿæˆçš„ 3D åŸå¸‚ä¸­æ¨¡æ‹Ÿäº†çœŸå®çš„èµ„æºåŠ¨æ€ï¼Œè¦æ±‚æ™ºèƒ½ä½“åœ¨ç®¡ç†é…é€æˆªæ­¢æ—¥æœŸã€äº¤é€šè´¹ç”¨ã€è½¦è¾†ç”µé‡ä»¥åŠç¤¾äº¤äº’åŠ¨ç­‰å¤šç§çº¦æŸçš„åŒæ—¶ï¼Œå®ç°æ•°å°æ—¶å†…å‡€åˆ©æ¶¦æœ€å¤§åŒ–çš„é•¿æœŸç›®æ ‡(long-horizon objectives)ã€‚ç ”ç©¶äººå‘˜åœ¨ä¹ä¸ªåŸå¸‚ä¸­å¯¹å¤šç§åŸºäº VLM çš„æ™ºèƒ½ä½“è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å°†å…¶ä¸äººç±»ç©å®¶çš„è¡¨ç°è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æ™ºèƒ½ä½“ä¸äººç±»ä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ€§èƒ½å·®è·ï¼Œæ™®éè¡¨ç°å‡ºçŸ­è§†è¡Œä¸ºä¸”é¢‘ç¹è¿ååŸºæœ¬å¸¸è¯†çº¦æŸã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜è§‚å¯Ÿåˆ°ä¸åŒæ¨¡å‹å‘ˆç°å‡ºæˆªç„¶ä¸åŒçš„â€œæ€§æ ¼â€ç‰¹å¾ï¼Œä¾‹å¦‚ GPT-5 å€¾å‘äºå†’é™©è€Œ Claude åˆ™æ›´ä¸ºä¿å®ˆï¼Œè¿™æ­ç¤ºäº†ç°æœ‰å…·èº«æ™ºèƒ½ä½“åœ¨ç°å®å—é™ç¯å¢ƒä¸­çš„è„†å¼±æ€§ä¸å¤šæ ·æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19234v1",
      "published_date": "2025-12-22 10:17:49 UTC",
      "updated_date": "2025-12-22 10:17:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:12:35.218554+00:00"
    },
    {
      "arxiv_id": "2512.19228v1",
      "title": "Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models",
      "title_zh": "åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡æ¡£ä¼ªé€ æ£€æµ‹ç¨‹åºåŒ–è§„åˆ™",
      "authors": [
        "Valentin Schmidberger",
        "Manuel Eberhardinger",
        "Setareh Maghsudi",
        "Johannes Maucher"
      ],
      "abstract": "Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æ¡£é€ å‡å¨èƒæ—¥ç›Šä¸¥é‡çš„ç°çŠ¶ï¼Œæå‡ºåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªåŠ¨ç”ŸæˆåŸºäºè§„åˆ™çš„åˆç†æ€§æ£€æŸ¥ï¼ˆplausibility checksï¼‰ç¨‹åºï¼Œä»¥æ›¿ä»£è€—æ—¶çš„äººå·¥ç¼–å†™è¿‡ç¨‹ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨å—é™çš„ç¡¬ä»¶èµ„æºä¸‹ï¼Œé‡‡ç”¨å¤šç§å¾®è°ƒç­–ç•¥ï¼ˆfine-tuning strategiesï¼‰å¯¹å¼€æºæ¨¡å‹ Llama 3.1 8B å’Œ OpenCoder 8B è¿›è¡Œäº†é¢†åŸŸä¸“ç”¨è®­ç»ƒã€‚é€šè¿‡åœ¨çœŸå®åº”ç”¨åœºæ™¯çš„ç»“æ„åŒ–æ•°æ®é›†ä¸Šè¿›è¡ŒéªŒè¯ï¼Œå®éªŒç»“æœæ˜¾ç¤ºæ¨¡å‹èƒ½å¤Ÿé’ˆå¯¹æœªçŸ¥ä¼ªé€ æ¨¡å¼ç”Ÿæˆå¯æ‰§è¡Œä¸”æœ‰æ•ˆçš„éªŒè¯ç¨‹åºã€‚è¿™ä¸€è¿›å±•è¯æ˜äº† LLMs åœ¨å®‰å…¨æ•æ„Ÿé¢†åŸŸä½œä¸ºè¾…åŠ©å†³ç­–å·¥å…·çš„å·¨å¤§æ½œåŠ›ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå‡æ•°æ®æ ¡éªŒçš„æ‰©å±•æ€§ä¸å¯ç†è§£æ€§ã€‚è¯¥é¡¹å·¥ä½œä¸ºè‡ªåŠ¨åŒ–æ–‡æ¡£éªŒè¯å’Œä¿éšœæ•°æ®å®Œæ•´æ€§æä¾›äº†æ–°çš„æŠ€æœ¯è·¯å¾„ï¼Œå±•ç¤ºäº†ç”Ÿæˆå¼ AI åœ¨å®‰å…¨é˜²å¾¡ä¸­çš„åº”ç”¨å‰æ™¯ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at ICMLA 2025, the first two authors contributed equally",
      "pdf_url": "https://arxiv.org/pdf/2512.19228v1",
      "published_date": "2025-12-22 10:08:25 UTC",
      "updated_date": "2025-12-22 10:08:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:12:54.551095+00:00"
    },
    {
      "arxiv_id": "2601.06057v2",
      "title": "Data Work in Egypt: Who Are the Workers Behind Artificial Intelligence?",
      "title_zh": "Egypt æ•°æ®å·¥ä½œï¼šè°æ˜¯äººå·¥æ™ºèƒ½èƒŒåçš„åŠ³åŠ¨è€…ï¼Ÿ",
      "authors": [
        "Myriam Raymond",
        "Lucy Neveux",
        "Antonio A. Casilli",
        "Paola Tubaro"
      ],
      "abstract": "The report highlights the role of Egyptian data workers in the global value chains of Artificial Intelligence (AI). These workers generate and annotate data for machine learning, check outputs, and they connect with overseas AI producers via international digital labor platforms, where they perform on-demand tasks and are typically paid by piecework, with no long-term commitment. Most of these workers are young, highly educated men, with nearly two-thirds holding undergraduate degrees. Their primary motivation for data work is financial need, with three-quarters relying on platform earnings to cover basic necessities. Despite the variability in their online earnings, these are generally low, often equaling Egypt's minimum wage. Data workers' digital identities are shaped by algorithmic control and economic demands, often diverging from their offline selves. Nonetheless, they find ways to resist, exercise ethical agency, and maintain autonomy. The report evaluates the potential impact of Egypt's newly enacted labor law and suggests policy measures to improve working conditions and acknowledge the role of these workers in AI's global value chains.",
      "tldr_zh": "è¯¥æŠ¥å‘Šè°ƒæŸ¥äº†åŸƒåŠæ•°æ®å·¥ä½œè€…åœ¨äººå·¥æ™ºèƒ½(AI)å…¨çƒä»·å€¼é“¾ä¸­æ‰€æ‰®æ¼”çš„è§’è‰²ï¼Œé‡ç‚¹å…³æ³¨ä»–ä»¬ä¸ºæœºå™¨å­¦ä¹ (machine learning)è¿›è¡Œçš„æ•°æ®ç”Ÿæˆã€æ ‡æ³¨å’Œè¾“å‡ºæ£€æŸ¥å·¥ä½œã€‚è¿™äº›å·¥ä½œè€…ä¸»è¦é€šè¿‡å›½é™…æ•°å­—åŠ³åŠ¨åŠ›å¹³å°(digital labor platforms)ä»¥è®¡ä»¶å½¢å¼å‚ä¸å…¨çƒåä½œï¼Œä¸”å¤§å¤šæ•°æ˜¯æ¥å—è¿‡é«˜ç­‰æ•™è‚²çš„å¹´è½»ç”·æ€§ã€‚ç ”ç©¶å‘ç°ï¼Œç»æµéœ€æ±‚æ˜¯å…¶ä»äº‹æ•°æ®å·¥ä½œçš„ä¸»è¦åŠ¨åŠ›ï¼Œä½†å…¶å¹³å°æ”¶å…¥é€šå¸¸è¾ƒä½ä¸”ä¸ç¨³å®šï¼Œå¾€å¾€ä»…ä¸åŸƒåŠçš„æœ€ä½å·¥èµ„æŒå¹³ã€‚æ•°æ®å·¥ä½œè€…çš„æ•°å­—èº«ä»½æ·±å—ç®—æ³•æ§åˆ¶(algorithmic control)å’Œç»æµéœ€æ±‚çš„å½±å“ï¼Œå°½ç®¡å¦‚æ­¤ï¼Œä»–ä»¬ä»é€šè¿‡å„ç§æ–¹å¼è¡Œä½¿ä¼¦ç†èƒ½åŠ¨æ€§(ethical agency)å¹¶åŠªåŠ›ç»´æŒè‡ªä¸»æƒã€‚æœ€åï¼Œè¯¥ç ”ç©¶è¯„ä¼°äº†åŸƒåŠæ–°é¢å¸ƒçš„åŠ³åŠ¨æ³•å¯¹è¯¥ç¾¤ä½“çš„å½±å“ï¼Œå¹¶æå‡ºäº†æ—¨åœ¨æ”¹å–„å·¥ä½œæ¡ä»¶åŠæå‡å…¶åœ¨å…¨çƒä»·å€¼é“¾ä¸­åœ°ä½çš„æ”¿ç­–å»ºè®®ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06057v2",
      "published_date": "2025-12-22 10:03:45 UTC",
      "updated_date": "2026-01-13 08:58:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:12:56.971935+00:00"
    },
    {
      "arxiv_id": "2512.19221v1",
      "title": "From Pixels to Predicates Structuring urban perception with scene graphs",
      "title_zh": "ä»åƒç´ åˆ°è°“è¯ï¼šåˆ©ç”¨åœºæ™¯å›¾å®ç°åŸå¸‚æ„ŸçŸ¥çš„ç»“æ„åŒ–",
      "authors": [
        "Yunlong Liu",
        "Shuyang Li",
        "Pengyuan Liu",
        "Yu Zhang",
        "Rudi Stouffs"
      ],
      "abstract": "Perception research is increasingly modelled using streetscapes, yet many approaches still rely on pixel features or object co-occurrence statistics, overlooking the explicit relations that shape human perception. This study proposes a three stage pipeline that transforms street view imagery (SVI) into structured representations for predicting six perceptual indicators. In the first stage, each image is parsed using an open-set Panoptic Scene Graph model (OpenPSG) to extract object predicate object triplets. In the second stage, compact scene-level embeddings are learned through a heterogeneous graph autoencoder (GraphMAE). In the third stage, a neural network predicts perception scores from these embeddings. We evaluate the proposed approach against image-only baselines in terms of accuracy, precision, and cross-city generalization. Results indicate that (i) our approach improves perception prediction accuracy by an average of 26% over baseline models, and (ii) maintains strong generalization performance in cross-city prediction tasks. Additionally, the structured representation clarifies which relational patterns contribute to lower perception scores in urban scenes, such as graffiti on wall and car parked on sidewalk. Overall, this study demonstrates that graph-based structure provides expressive, generalizable, and interpretable signals for modelling urban perception, advancing human-centric and context-aware urban analytics.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸå¸‚æ„ŸçŸ¥ç ”ç©¶ä¸­è¿‡åº¦ä¾èµ–åƒç´ ç‰¹å¾æˆ–ç‰©ä½“å…±ç°ç»Ÿè®¡ã€è€Œå¿½è§†æ˜¾å¼å…³ç³»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å°†è¡—æ™¯å›¾åƒ(Street View Imagery, SVI)è½¬åŒ–ä¸ºç»“æ„åŒ–è¡¨ç¤ºçš„ä¸‰é˜¶æ®µæµæ°´çº¿ã€‚ç¬¬ä¸€é˜¶æ®µåˆ©ç”¨å…¨æ™¯åœºæ™¯å›¾æ¨¡å‹ OpenPSG æå–â€œç‰©ä½“-è°“è¯-ç‰©ä½“â€ä¸‰å…ƒç»„ï¼Œç¬¬äºŒé˜¶æ®µé€šè¿‡å¼‚æ„å›¾è‡ªåŠ¨ç¼–ç å™¨ GraphMAE å­¦ä¹ åœºæ™¯çº§åµŒå…¥ï¼Œç¬¬ä¸‰é˜¶æ®µç”±ç¥ç»ç½‘ç»œé¢„æµ‹å…­é¡¹æ„ŸçŸ¥æŒ‡æ ‡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨æ„ŸçŸ¥é¢„æµ‹å‡†ç¡®ç‡ä¸Šæ¯”åŸºçº¿æ¨¡å‹å¹³å‡æå‡äº† 26%ï¼Œä¸”åœ¨è·¨åŸå¸‚é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç»“æ„åŒ–è¡¨ç¤ºé€šè¿‡æ­ç¤ºå¯¼è‡´æ„ŸçŸ¥åˆ†æ•°é™ä½çš„å…³ç³»æ¨¡å¼ï¼ˆå¦‚ graffiti on wall æˆ– car parked on sidewalkï¼‰ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚è¯¥ç ”ç©¶è¯æ˜äº†åŸºäºå›¾(Graph-based)çš„ç»“æ„ä¸ºå»ºæ¨¡åŸå¸‚æ„ŸçŸ¥æä¾›äº†æ›´å…·è¡¨è¾¾åŠ›ã€æ³›åŒ–æ€§å’Œå¯è§£é‡Šæ€§çš„ä¿¡å·ï¼Œæ¨åŠ¨äº†ä»¥äººä¸ºæœ¬ä¸”å…·å¤‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„åŸå¸‚åˆ†ææŠ€æœ¯çš„å‘å±•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10 pages, CAADRIA2026 presentation forthcoming",
      "pdf_url": "https://arxiv.org/pdf/2512.19221v1",
      "published_date": "2025-12-22 10:02:53 UTC",
      "updated_date": "2025-12-22 10:02:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:12:58.949125+00:00"
    },
    {
      "arxiv_id": "2512.19219v1",
      "title": "Towards Minimal Fine-Tuning of VLMs",
      "title_zh": "è¿ˆå‘ VLMs çš„æç®€å¾®è°ƒ",
      "authors": [
        "Tiange Luo",
        "Lajanugen Logeswaran",
        "Jaekyeom Kim",
        "Justin Johnson",
        "Honglak Lee"
      ],
      "abstract": "We introduce Image-LoRA, a lightweight parameter efficient fine-tuning (PEFT) recipe for transformer-based vision-language models (VLMs). Image-LoRA applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs roughly in proportion to the visual-token fraction. We further adapt only a subset of attention heads, selected using head influence scores estimated with a rank-1 Image-LoRA, and stabilize per-layer updates via selection-size normalization. Across screen-centric grounding and referring benchmarks spanning text-heavy to image-heavy regimes, Image-LoRA matches or closely approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method also preserves the pure-text reasoning performance of VLMs before and after fine-tuning, as further shown on GSM8K.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Image-LoRAï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹åŸºäº Transformer çš„è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) çš„è½»é‡çº§å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT) æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•ä»…åœ¨è§†è§‰æ ‡è®°èŒƒå›´ (visual-token span) å†…çš„æ³¨æ„åŠ›å±‚å€¼è·¯å¾„ (value path) åº”ç”¨ä½ç§©è‡ªé€‚åº” (low-rank adaptation)ï¼Œä»è€ŒæŒ‰è§†è§‰æ ‡è®°æ¯”ä¾‹æ˜¾è‘—é™ä½äº†é€‚é…å™¨è®­ç»ƒæ‰€éœ€çš„è®¡ç®—é‡ (FLOPs)ã€‚ç ”ç©¶è€…é€šè¿‡ä½¿ç”¨ç§©ä¸º1çš„ Image-LoRA ä¼°ç®—çš„æ³¨æ„åŠ›å¤´å½±å“åŠ›å¾—åˆ† (head influence scores) æ¥é€‰æ‹©ç‰¹å®šçš„æ³¨æ„åŠ›å¤´å­é›†ï¼Œå¹¶åˆ©ç”¨é€‰æ‹©è§„æ¨¡å½’ä¸€åŒ– (selection-size normalization) æ¥ç¨³å®šé€å±‚æ›´æ–°ã€‚åœ¨è·¨è¶Šæ–‡æœ¬å¯†é›†åˆ°å›¾åƒå¯†é›†çš„å±å¹•ä¸­å¿ƒå®šä½ä¸æŒ‡ä»£åŸºå‡†æµ‹è¯•ä¸­ï¼ŒImage-LoRA åœ¨ä½¿ç”¨æ›´å°‘å¯è®­ç»ƒå‚æ•°å’Œæ›´ä½ FLOPs çš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°äº†ä¸æ ‡å‡† LoRA ç›¸å½“æˆ–æ¥è¿‘çš„å‡†ç¡®ç‡ã€‚å®éªŒè¿˜è¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¾®è°ƒå‰åå‡èƒ½æœ‰æ•ˆä¿ç•™ VLMs çš„çº¯æ–‡æœ¬æ¨ç†æ€§èƒ½ï¼Œè¿™ä¸€ç‚¹åœ¨ GSM8K æµ‹è¯•ä¸­å¾—åˆ°äº†è¿›ä¸€æ­¥è¯å®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19219v1",
      "published_date": "2025-12-22 10:02:10 UTC",
      "updated_date": "2025-12-22 10:02:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:13:12.693589+00:00"
    },
    {
      "arxiv_id": "2512.19210v1",
      "title": "Observer, Not Player: Simulating Theory of Mind in LLMs through Game Observation",
      "title_zh": "è§‚å¯Ÿè€…è€Œéåšå¼ˆè€…ï¼šé€šè¿‡åšå¼ˆè§‚å¯Ÿæ¨¡æ‹Ÿå¤§è¯­è¨€æ¨¡å‹çš„å¿ƒç†ç†è®º",
      "authors": [
        "Jerry Wang",
        "Ting Yiu Liu"
      ],
      "abstract": "We present an interactive framework for evaluating whether large language models (LLMs) exhibit genuine \"understanding\" in a simple yet strategic environment. As a running example, we focus on Rock-Paper-Scissors (RPS), which, despite its apparent simplicity, requires sequential reasoning, adaptation, and strategy recognition. Our system positions the LLM as an Observer whose task is to identify which strategies are being played and to articulate the reasoning behind this judgment. The purpose is not to test knowledge of Rock-Paper-Scissors itself, but to probe whether the model can exhibit mind-like reasoning about sequential behavior. To support systematic evaluation, we provide a benchmark consisting of both static strategies and lightweight dynamic strategies specified by well-prompted rules. We quantify alignment between the Observer's predictions and the ground-truth distributions induced by actual strategy pairs using three complementary signals: Cross-Entropy, Brier score, and Expected Value (EV) discrepancy. These metrics are further integrated into a unified score, the Union Loss, which balances calibration, sensitivity, and payoff alignment. Together with a Strategy Identification Rate (SIR) metric, our framework captures not only predictive accuracy but also whether the model can stably identify the latent strategies in play. The demo emphasizes interactivity, transparency, and reproducibility. Users can adjust LLM distributions in real time, visualize losses as they evolve, and directly inspect reasoning snippets to identify where and why failures occur. In doing so, our system provides a practical and interpretable proxy for mind-like inference in sequential games, offering insights into both the strengths and limitations of current LLM reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªäº¤äº’å¼æ¡†æ¶ï¼Œæ—¨åœ¨è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨ç­–ç•¥ç¯å¢ƒä¸­çš„çœŸå®â€œç†è§£â€èƒ½åŠ›ã€‚ç ”ç©¶ä»¥çŸ³å¤´å‰ªåˆ€å¸ƒï¼ˆRock-Paper-Scissors, RPSï¼‰ä¸ºåˆ‡å…¥ç‚¹ï¼Œå°†LLMå®šä½ä¸ºè§‚å¯Ÿè€…ï¼ˆObserverï¼‰ï¼Œè¦æ±‚å…¶è¯†åˆ«ç©å®¶ç­–ç•¥å¹¶é˜æ˜èƒŒåçš„æ¨ç†é€»è¾‘ï¼Œä»¥æ­¤æ¢æµ‹æ¨¡å‹æ˜¯å¦å…·å¤‡ç±»å¿ƒæ™ºæ¨ç†ï¼ˆmind-like reasoningï¼‰èƒ½åŠ›ã€‚ä¸ºäº†æ”¯æŒç³»ç»ŸåŒ–è¯„ä¼°ï¼Œç ”ç©¶æä¾›äº†ä¸€ä¸ªåŒ…å«é™æ€å’Œè½»é‡çº§åŠ¨æ€ç­–ç•¥çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶åˆ©ç”¨äº’ç†µï¼ˆCross-Entropyï¼‰ã€å¸ƒé‡Œå°”åˆ†æ•°ï¼ˆBrier scoreï¼‰å’ŒæœŸæœ›å€¼ï¼ˆExpected Value, EVï¼‰å·®å¼‚ç­‰æŒ‡æ ‡æ„å»ºäº†ç»Ÿä¸€çš„è”åˆæŸå¤±ï¼ˆUnion Lossï¼‰ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ç­–ç•¥è¯†åˆ«ç‡ï¼ˆStrategy Identification Rate, SIRï¼‰æ¥è¡¡é‡æ¨¡å‹è¯†åˆ«æ½œè—ç­–ç•¥çš„ç¨³å®šæ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡å®æ—¶å¯è§†åŒ–å’Œæ¨ç†ç‰‡æ®µæ£€æŸ¥ï¼Œä¸ºåºåˆ—åšå¼ˆä¸­çš„ç±»å¿ƒæ™ºæ¨æ–­ï¼ˆmind-like inferenceï¼‰æä¾›äº†å®ç”¨ä¸”å¯è§£é‡Šçš„è¯„ä¼°æ‰‹æ®µï¼Œæ­ç¤ºäº†å½“å‰LLMæ¨ç†èƒ½åŠ›çš„ä¼˜åŠ¿ä¸å±€é™ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at NeurIPS Workshop on Foundations of Reasoning in Language Models and Workshop on Bridging Language, Agent, and World Model",
      "pdf_url": "https://arxiv.org/pdf/2512.19210v1",
      "published_date": "2025-12-22 09:49:13 UTC",
      "updated_date": "2025-12-22 09:49:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:13:13.710578+00:00"
    },
    {
      "arxiv_id": "2512.19206v1",
      "title": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning",
      "title_zh": "MixKVQï¼šé¢å‘é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„æŸ¥è¯¢æ„ŸçŸ¥æ··åˆç²¾åº¦ KV ç¼“å­˜é‡åŒ–",
      "authors": [
        "Tao Zhang",
        "Ziqian Zeng",
        "Hao Peng",
        "Huiping Zhuang",
        "Cen Chen"
      ],
      "abstract": "Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é•¿é“¾å¼æ€ç»´(Long Chain-of-Thought)æ¨ç†ä¸­é¢ä¸´çš„KV cacheå†…å­˜å’Œå»¶è¿Ÿå¼€é”€é—®é¢˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰ä½æ¯”ç‰¹é‡åŒ–æ–¹æ³•åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½é€€åŒ–ã€‚ä¼ ç»Ÿçš„å›ºå®šç²¾åº¦é‡åŒ–éš¾ä»¥å¤„ç†key cacheä¸­çš„ç¦»ç¾¤é€šé“(outlier channels)ï¼Œè€Œç°æœ‰çš„æ··åˆç²¾åº¦ç­–ç•¥å¾€å¾€æ— æ³•å‡†ç¡®è¯†åˆ«éœ€è¦é«˜ç²¾åº¦è¡¨ç¤ºçš„å…³é”®ç»„ä»¶ã€‚ç ”ç©¶æå‡ºæœ‰æ•ˆçš„ä½æ¯”ç‰¹KV cacheé‡åŒ–å¿…é¡»å…¼é¡¾é”®é€šé“(key channel)çš„å›ºæœ‰é‡åŒ–éš¾åº¦åŠå…¶ä¸æŸ¥è¯¢(query)çš„ç›¸å…³æ€§ã€‚åŸºäºæ­¤æ´å¯Ÿï¼Œä½œè€…æå‡ºäº†MixKVQï¼Œä¸€ç§å³æ’å³ç”¨çš„è½»é‡çº§æŸ¥è¯¢æ„ŸçŸ¥(query-aware)ç®—æ³•ï¼Œç”¨äºè¯†åˆ«å¹¶ä¿ç•™éœ€è¦æ›´é«˜ç²¾åº¦çš„å…³é”®é”®é€šé“ï¼Œå¹¶å¯¹value cacheåº”ç”¨é€æ ‡è®°(per-token)é‡åŒ–ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒMixKVQåœ¨å¤æ‚æ¨ç†æ•°æ®é›†ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ä½æ¯”ç‰¹æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨å¤§å¹…å‡å°‘å†…å­˜å ç”¨(memory footprint)çš„åŒæ—¶ï¼Œå®ç°äº†ä¸å…¨ç²¾åº¦åŸºçº¿(full-precision baseline)ç›¸å½“çš„æ¨ç†æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19206v1",
      "published_date": "2025-12-22 09:44:26 UTC",
      "updated_date": "2025-12-22 09:44:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:13:10.908120+00:00"
    },
    {
      "arxiv_id": "2512.19199v1",
      "title": "On the Koopman-Based Generalization Bounds for Multi-Task Deep Learning",
      "title_zh": "è®ºå¤šä»»åŠ¡æ·±åº¦å­¦ä¹ ä¸­åŸºäº Koopman çš„æ³›åŒ–ç•Œ",
      "authors": [
        "Mahdi Mohammadigohari",
        "Giuseppe Di Fatta",
        "Giuseppe Nicosia",
        "Panos M. Pardalos"
      ],
      "abstract": "The paper establishes generalization bounds for multitask deep neural networks using operator-theoretic techniques. The authors propose a tighter bound than those derived from conventional norm based methods by leveraging small condition numbers in the weight matrices and introducing a tailored Sobolev space as an expanded hypothesis space. This enhanced bound remains valid even in single output settings, outperforming existing Koopman based bounds. The resulting framework maintains key advantages such as flexibility and independence from network width, offering a more precise theoretical understanding of multitask deep learning in the context of kernel methods.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨ç®—å­ç†è®º(Operator-theoretic techniques)ä¸ºå¤šä»»åŠ¡æ·±åº¦ç¥ç»ç½‘ç»œ(Multi-task deep neural networks)å»ºç«‹äº†å…¨æ–°çš„æ³›åŒ–ç•Œ(Generalization bounds)ã€‚ä½œè€…é€šè¿‡åˆ©ç”¨æƒé‡çŸ©é˜µçš„å°æ¡ä»¶æ•°(Small condition numbers)å¹¶å¼•å…¥å®šåˆ¶çš„Sobolev spaceä½œä¸ºæ‰©å±•å‡è®¾ç©ºé—´ï¼Œæ¨å¯¼å‡ºæ¯”ä¼ ç»ŸåŸºäºèŒƒæ•°(Norm-based)æ–¹æ³•æ›´ç´§å‡‘çš„ç•Œé™ã€‚è¯¥å¢å¼ºåçš„ç†è®ºç•Œé™åœ¨å•è¾“å‡ºè®¾ç½®ä¸­ä¾ç„¶æœ‰æ•ˆï¼Œä¸”åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„Koopman-basedç•Œé™ã€‚è¯¥æ¡†æ¶ä¸ä»…ä¿ç•™äº†é«˜åº¦çš„çµæ´»æ€§ï¼Œè¿˜å…·æœ‰ä¸ç½‘ç»œå®½åº¦æ— å…³(Independence from network width)çš„æ ¸å¿ƒä¼˜åŠ¿ã€‚è¿™ä¸€æˆæœä¸ºå†…æ ¸æ–¹æ³•(Kernel methods)èƒŒæ™¯ä¸‹çš„å¤šä»»åŠ¡æ·±åº¦å­¦ä¹ æä¾›äº†æ›´ç²¾ç¡®çš„ç†è®ºç†è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for publication in Lecture Notes in Computer Science (LNCS). Final version forthcoming",
      "pdf_url": "https://arxiv.org/pdf/2512.19199v1",
      "published_date": "2025-12-22 09:36:24 UTC",
      "updated_date": "2025-12-22 09:36:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:14:16.635705+00:00"
    },
    {
      "arxiv_id": "2512.19772v1",
      "title": "A K-Means, Ward and DBSCAN repeatability study",
      "title_zh": "K-Meansã€Ward åŠ DBSCAN ç®—æ³•çš„å¯é‡å¤æ€§ç ”ç©¶",
      "authors": [
        "Anthony Bertrand",
        "Engelbert Mephu Nguifo",
        "Violaine Antoine",
        "David Hill"
      ],
      "abstract": "Reproducibility is essential in machine learning because it ensures that a model or experiment yields the same scientific conclusion. For specific algorithms repeatability with bitwise identical results is also a key for scientific integrity because it allows debugging. We decomposed several very popular clustering algorithms: K-Means, DBSCAN and Ward into their fundamental steps, and we identify the conditions required to achieve repeatability at each stage. We use an implementation example with the Python library scikit-learn to examine the repeatable aspects of each method. Our results reveal inconsistent results with K-Means when the number of OpenMP threads exceeds two. This work aims to raise awareness of this issue among both users and developers, encouraging further investigation and potential fixes.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹å¸¸ç”¨çš„èšç±»ç®—æ³• K-Meansã€Ward å’Œ DBSCAN å¼€å±•äº†å¯é‡å¤æ€§ï¼ˆrepeatabilityï¼‰ç ”ç©¶ï¼Œæ¢è®¨äº†ç¡®ä¿å®éªŒå¾—å‡ºä¸€è‡´ç§‘å­¦ç»“è®ºæ‰€éœ€çš„ä½çº§ä¸€è‡´æ€§ï¼ˆbitwise identicalï¼‰æ¡ä»¶ã€‚ä½œè€…å°†è¿™äº›ç®—æ³•åˆ†è§£ä¸ºåŸºç¡€æ­¥éª¤ï¼Œå¹¶è¯†åˆ«äº†åœ¨å„ä¸ªé˜¶æ®µå®ç°ç»“æœå¯é‡å¤æ‰€éœ€çš„å…³é”®å› ç´ ã€‚é€šè¿‡å¯¹ Python åº“ scikit-learn çš„å…·ä½“å®ç°è¿›è¡Œæµ‹è¯•ï¼Œç ”ç©¶æ­ç¤ºäº† K-Means ç®—æ³•åœ¨ OpenMP çº¿ç¨‹æ•°è¶…è¿‡ä¸¤ä¸ªæ—¶ä¼šäº§ç”Ÿä¸ä¸€è‡´çš„ç»“æœï¼ŒæŒ‡å‡ºäº†å¹¶è¡Œè®¡ç®—ç¯å¢ƒå¯¹èšç±»ç»“æœç¨³å®šæ€§çš„æ½œåœ¨å½±å“ã€‚è¯¥å·¥ä½œæ—¨åœ¨æå‡ç”¨æˆ·ä¸å¼€å‘è€…å¯¹ç®—æ³•å®ç°ç»†èŠ‚ä¸­éšæœºæ€§ä¸ä¸ä¸€è‡´æ€§çš„é‡è§†ç¨‹åº¦ï¼Œå¹¶ä¸ºæœªæ¥çš„ç®—æ³•ä¼˜åŒ–ä¸ç§‘å­¦è°ƒè¯•æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19772v1",
      "published_date": "2025-12-22 09:30:33 UTC",
      "updated_date": "2025-12-22 09:30:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:13:40.544546+00:00"
    },
    {
      "arxiv_id": "2512.19184v1",
      "title": "Operator-Based Generalization Bound for Deep Learning: Insights on Multi-Task Learning",
      "title_zh": "åŸºäºç®—å­çš„æ·±åº¦å­¦ä¹ æ³›åŒ–ç•Œï¼šå¤šä»»åŠ¡å­¦ä¹ ä¹‹å¯ç¤º",
      "authors": [
        "Mahdi Mohammadigohari",
        "Giuseppe Di Fatta",
        "Giuseppe Nicosia",
        "Panos M. Pardalos"
      ],
      "abstract": "This paper presents novel generalization bounds for vector-valued neural networks and deep kernel methods, focusing on multi-task learning through an operator-theoretic framework. Our key development lies in strategically combining a Koopman based approach with existing techniques, achieving tighter generalization guarantees compared to traditional norm-based bounds. To mitigate computational challenges associated with Koopman-based methods, we introduce sketching techniques applicable to vector valued neural networks. These techniques yield excess risk bounds under generic Lipschitz losses, providing performance guarantees for applications including robust and multiple quantile regression. Furthermore, we propose a novel deep learning framework, deep vector-valued reproducing kernel Hilbert spaces (vvRKHS), leveraging Perron Frobenius (PF) operators to enhance deep kernel methods. We derive a new Rademacher generalization bound for this framework, explicitly addressing underfitting and overfitting through kernel refinement strategies. This work offers novel insights into the generalization properties of multitask learning with deep learning architectures, an area that has been relatively unexplored until recent developments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šä»»åŠ¡å­¦ä¹ (Multi-Task Learning)åœºæ™¯ï¼Œæå‡ºäº†ä¸€ç§åŸºäºç®—å­è®ºæ¡†æ¶(operator-theoretic framework)çš„å‘é‡å€¼ç¥ç»ç½‘ç»œ(vector-valued neural networks)å’Œæ·±åº¦æ ¸æ–¹æ³•(deep kernel methods)çš„å…¨æ–°æ³›åŒ–è¯¯å·®ç•Œ(generalization bounds)ã€‚ç ”ç©¶é€šè¿‡å°†åŸºäº Koopman çš„æ–¹æ³•ä¸ç°æœ‰æŠ€æœ¯ç­–ç•¥æ€§åœ°ç»“åˆï¼Œå®ç°äº†æ¯”ä¼ ç»ŸåŸºäºèŒƒæ•°(norm-based)çš„ç•Œé™æ›´ç´§è‡´çš„æ³›åŒ–ä¿è¯ã€‚ä¸ºäº†å…‹æœè®¡ç®—æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶å¼•å…¥äº†é€‚ç”¨äºå‘é‡å€¼ç¥ç»ç½‘ç»œçš„è‰å›¾æŠ€æœ¯(sketching techniques)ï¼Œå¹¶æ¨å¯¼å‡ºé€šç”¨ Lipschitz æŸå¤±ä¸‹çš„è¶…é¢é£é™©ç•Œï¼Œä¸ºç¨³å¥å›å½’å’Œå¤šåˆ†ä½æ•°å›å½’ç­‰åº”ç”¨æä¾›äº†æ€§èƒ½ä¿è¯ã€‚æ­¤å¤–ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§æ·±å±‚å‘é‡å€¼å†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´(deep vvRKHS)æ¡†æ¶ï¼Œåˆ©ç”¨ Perron Frobenius (PF) ç®—å­å¢å¼ºæ·±åº¦æ ¸æ–¹æ³•ã€‚é€šè¿‡ä¸ºè¯¥æ¡†æ¶æ¨å¯¼æ–°çš„ Rademacher æ³›åŒ–ç•Œå¹¶é‡‡ç”¨æ ¸ç»†åŒ–ç­–ç•¥ï¼Œç ”ç©¶æœ‰æ•ˆè§£å†³äº†æ¨¡å‹è®­ç»ƒä¸­çš„æ¬ æ‹Ÿåˆä¸è¿‡æ‹Ÿåˆé—®é¢˜ã€‚è¯¥å·¥ä½œå¡«è¡¥äº†å¤šä»»åŠ¡å­¦ä¹ æ·±åº¦æ¶æ„æ³›åŒ–ç‰¹æ€§ç ”ç©¶çš„ç©ºç™½ï¼Œä¸ºç†è§£å¤æ‚æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›æä¾›äº†é‡è¦çš„ç†è®ºè§è§£ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted for publication in Lecture Notes in Computer Science (LNCS). Final version forthcoming",
      "pdf_url": "https://arxiv.org/pdf/2512.19184v1",
      "published_date": "2025-12-22 09:18:30 UTC",
      "updated_date": "2025-12-22 09:18:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:13:43.249035+00:00"
    },
    {
      "arxiv_id": "2512.19180v1",
      "title": "Practical Quantum-Classical Feature Fusion for complex data Classification",
      "title_zh": "é¢å‘å¤æ‚æ•°æ®åˆ†ç±»çš„å®ç”¨é‡å­-ç»å…¸ç‰¹å¾èåˆ",
      "authors": [
        "Azadeh Alavi",
        "Fatemeh Kouchmeshki",
        "Abdolrahman Alavi"
      ],
      "abstract": "Hybrid quantum and classical learning aims to couple quantum feature maps with the robustness of classical neural networks, yet most architectures treat the quantum circuit as an isolated feature extractor and merge its measurements with classical representations by direct concatenation. This neglects that the quantum and classical branches constitute distinct computational modalities and limits reliable performance on complex, high dimensional tabular and semi structured data, including remote sensing, environmental monitoring, and medical diagnostics. We present a multimodal formulation of hybrid learning and propose a cross attention mid fusion architecture in which a classical representation queries quantum derived feature tokens through an attention block with residual connectivity. The quantum branch is kept within practical NISQ budgets and uses up to nine qubits. We evaluate on Wine, Breast Cancer, Forest CoverType, FashionMNIST, and SteelPlatesFaults, comparing a quantum only model, a classical baseline, residual hybrid models, and the proposed mid fusion model under a consistent protocol. Pure quantum and standard hybrid designs underperform due to measurement induced information loss, while cross attention mid fusion is consistently competitive and improves performance on the more complex datasets in most cases. These findings suggest that quantum derived information becomes most valuable when integrated through principled multimodal fusion rather than used in isolation or loosely appended to classical features.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ··åˆé‡å­-ç»å…¸å­¦ä¹ (Hybrid quantum and classical learning)ä¸­é‡å­ç”µè·¯é€šå¸¸è¢«è§†ä¸ºå­¤ç«‹çš„ç‰¹å¾æå–å™¨ä¸”ä»…é€šè¿‡ç›´æ¥ä¸²è”è¿›è¡Œèåˆçš„é—®é¢˜ï¼ŒæŒ‡å‡ºè¿™ç§æ–¹å¼é™åˆ¶äº†åœ¨å¤„ç†é«˜ç»´å¤æ‚æ•°æ®æ—¶çš„æ€§èƒ½ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§å¤šæ¨¡æ€(multimodal)æ··åˆå­¦ä¹ æ¡†æ¶ï¼Œå¼•å…¥äº†è·¨æ³¨æ„åŠ›ä¸­å±‚èåˆ(cross attention mid fusion)æ¶æ„ï¼Œä½¿ç»å…¸è¡¨ç¤ºèƒ½é€šè¿‡å¸¦æœ‰æ®‹å·®è¿æ¥(residual connectivity)çš„æ³¨æ„åŠ›æ¨¡å—æŸ¥è¯¢é‡å­è¡ç”Ÿçš„ç‰¹å¾æ ‡è®°(feature tokens)ã€‚è¯¥æ–¹æ¡ˆå°†é‡å­åˆ†æ”¯æ§åˆ¶åœ¨9ä¸ªé‡å­æ¯”ç‰¹(qubits)ä»¥å†…çš„è¿‘ä¸­é˜¶é‡å­(NISQ)é¢„ç®—å†…ï¼Œå¹¶åœ¨Wineã€Forest CoverTypeå’ŒFashionMNISTç­‰å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¼ ç»Ÿçš„çº¯é‡å­å’Œæ ‡å‡†æ··åˆè®¾è®¡å¸¸å› æµ‹é‡å¼•å‘çš„ä¿¡æ¯æŸå¤±è€Œè¡¨ç°æ¬ ä½³ï¼Œè€Œè·¨æ³¨æ„åŠ›ä¸­å±‚èåˆæ¨¡å‹åœ¨å¤æ‚æ•°æ®é›†ä¸Šæ˜¾è‘—æå‡äº†åˆ†ç±»æ€§èƒ½ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†é‡å­ä¿¡æ¯åªæœ‰é€šè¿‡åŸåˆ™æ€§çš„å¤šæ¨¡æ€èåˆé›†æˆï¼Œæ‰èƒ½åœ¨æ··åˆç³»ç»Ÿä¸­å‘æŒ¥å‡ºä¼˜äºå­¤ç«‹ä½¿ç”¨çš„åº”ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "16 pages, 3 figues",
      "pdf_url": "https://arxiv.org/pdf/2512.19180v1",
      "published_date": "2025-12-22 09:16:08 UTC",
      "updated_date": "2025-12-22 09:16:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:13:27.673970+00:00"
    },
    {
      "arxiv_id": "2512.22214v1",
      "title": "Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition",
      "title_zh": "Signal-SGN++ï¼šé¢å‘éª¨éª¼åŠ¨ä½œè¯†åˆ«çš„æ‹“æ‰‘å¢å¼ºå‹æ—¶é¢‘è„‰å†²å›¾ç½‘ç»œ",
      "authors": [
        "Naichuan Zheng",
        "Xiahai Lun",
        "Weiyi Li",
        "Yuchen Du"
      ],
      "abstract": "Graph Convolutional Networks (GCNs) demonstrate strong capability in modeling skeletal topology for action recognition, yet their dense floating-point computations incur high energy costs. Spiking Neural Networks (SNNs), characterized by event-driven and sparse activation, offer energy efficiency but remain limited in capturing coupled temporal-frequency and topological dependencies of human motion. To bridge this gap, this article proposes Signal-SGN++, a topology-aware spiking graph framework that integrates structural adaptivity with time-frequency spiking dynamics. The network employs a backbone composed of 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. Within this backbone, a Topology-Shift Self-Attention (TSSA) mechanism is embedded to adaptively route attention across learned skeletal topologies, enhancing graph-level sensitivity without increasing computational complexity. Moreover, an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch decomposes spiking features into multi-resolution temporal-frequency representations, wherein a Topology-Aware Time-Frequency Fusion (TATF) unit incorporates structural priors to preserve topology-consistent spectral fusion. Comprehensive experiments on large-scale benchmarks validate that Signal-SGN++ achieves superior accuracy-efficiency trade-offs, outperforming existing SNN-based methods and achieving competitive results against state-of-the-art GCNs under substantially reduced energy consumption.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Signal-SGN++ï¼Œä¸€ç§æ‹“æ‰‘å¢å¼ºçš„æ—¶é¢‘è„‰å†²å›¾ç½‘ç»œï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿå›¾å·ç§¯ç½‘ç»œ(GCNs)é«˜èƒ½è€—çš„é—®é¢˜ä»¥åŠç°æœ‰è„‰å†²ç¥ç»ç½‘ç»œ(SNNs)åœ¨æ•è·äººä½“è¿åŠ¨æ—¶é¢‘ä¸æ‹“æ‰‘ä¾èµ–å…³ç³»ä¸Šçš„å±€é™ã€‚è¯¥æ¡†æ¶çš„ä¸»å¹²ç½‘ç»œé›†æˆäº†ä¸€ç»´è„‰å†²å›¾å·ç§¯(1D-SGC)å’Œé¢‘ç‡è„‰å†²å·ç§¯(FSC)ï¼Œç”¨äºååŒæå–æ—¶ç©ºä¸é¢‘è°±ç‰¹å¾ã€‚ä¸ºäº†æå‡æ¨¡å‹å¯¹éª¨éª¼æ‹“æ‰‘çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œç ”ç©¶å¼•å…¥äº†æ‹“æ‰‘åç§»è‡ªæ³¨æ„åŠ›(TSSA)æœºåˆ¶ï¼Œåœ¨ä¸å¢åŠ è®¡ç®—å¤æ‚åº¦çš„å‰æä¸‹å®ç°è‡ªé€‚åº”æ³¨æ„åŠ›è·¯ç”±ã€‚æ­¤å¤–ï¼Œé€šè¿‡å¤šå°ºåº¦å°æ³¢å˜æ¢èåˆ(MWTF)åˆ†æ”¯å’Œæ‹“æ‰‘æ„ŸçŸ¥æ—¶é¢‘èåˆ(TATF)å•å…ƒï¼Œè¯¥ç½‘ç»œèƒ½å¤Ÿå°†è„‰å†²ç‰¹å¾åˆ†è§£ä¸ºå¤šåˆ†è¾¨ç‡è¡¨ç¤ºå¹¶ä¿æŒæ‹“æ‰‘ä¸€è‡´çš„å…‰è°±èåˆã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSignal-SGN++åœ¨å¤§å‹åŸºå‡†æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚çš„å‡†ç¡®ç‡ä¸æ•ˆç‡å¹³è¡¡ï¼Œåœ¨æ˜¾è‘—é™ä½èƒ½è€—çš„åŒæ—¶ï¼Œå…¶æ€§èƒ½è¶…è¶Šäº†ç°æœ‰SNNæ–¹æ³•ï¼Œå¹¶è¾¾åˆ°äº†ä¸æœ€å…ˆè¿›GCNæ¨¡å‹ç›¸å½“çš„æ°´å¹³ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22214v1",
      "published_date": "2025-12-22 09:16:04 UTC",
      "updated_date": "2025-12-22 09:16:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:14:29.744113+00:00"
    },
    {
      "arxiv_id": "2512.19178v1",
      "title": "Vision-Language-Policy Model for Dynamic Robot Task Planning",
      "title_zh": "é¢å‘åŠ¨æ€æœºå™¨äººä»»åŠ¡è§„åˆ’çš„è§†è§‰-è¯­è¨€-ç­–ç•¥æ¨¡å‹",
      "authors": [
        "Jin Wang",
        "Kim Tien Ly",
        "Jacques Cloete",
        "Nikos Tsagarakis",
        "Ioannis Havoutis"
      ],
      "abstract": "Bridging the gap between natural language commands and autonomous execution in unstructured environments remains an open challenge for robotics. This requires robots to perceive and reason over the current task scene through multiple modalities, and to plan their behaviors to achieve their intended goals. Traditional robotic task-planning approaches often struggle to bridge low-level execution with high-level task reasoning, and cannot dynamically update task strategies when instructions change during execution, which ultimately limits their versatility and adaptability to new tasks. In this work, we propose a novel language model-based framework for dynamic robot task planning. Our Vision-Language-Policy (VLP) model, based on a vision-language model fine-tuned on real-world data, can interpret semantic instructions and integrate reasoning over the current task scene to generate behavior policies that control the robot to accomplish the task. Moreover, it can dynamically adjust the task strategy in response to changes in the task, enabling flexible adaptation to evolving task requirements. Experiments conducted with different robots and a variety of real-world tasks show that the trained model can efficiently adapt to novel scenarios and dynamically update its policy, demonstrating strong planning autonomy and cross-embodiment generalization. Videos: https://robovlp.github.io/",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶é’ˆå¯¹æœºå™¨äººåœ¨éç»“æ„åŒ–ç¯å¢ƒä¸­ä»»åŠ¡è§„åˆ’æ‰€é¢ä¸´çš„ä½å±‚æ‰§è¡Œä¸é«˜å±‚æ¨ç†è„±èŠ‚ã€ä»¥åŠæ— æ³•æ ¹æ®åŠ¨æ€æŒ‡ä»¤è°ƒæ•´ç­–ç•¥ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºVision-Language-Policy (VLP) çš„è§†è§‰-è¯­è¨€-ç­–ç•¥æ¨¡å‹ã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šå¾®è°ƒè§†è§‰è¯­è¨€æ¨¡å‹(vision-language model)ï¼Œä½¿æœºå™¨äººèƒ½å¤Ÿç†è§£è¯­ä¹‰æŒ‡ä»¤å¹¶å¯¹å½“å‰åœºæ™¯è¿›è¡Œå¤šæ¨¡æ€æ¨ç†ï¼Œè¿›è€Œç”ŸæˆæŒ‡å¯¼ä»»åŠ¡æ‰§è¡Œçš„è¡Œä¸ºç­–ç•¥(behavior policies)ã€‚VLPæ¨¡å‹å…·æœ‰åŠ¨æ€è°ƒæ•´ä»»åŠ¡ç­–ç•¥çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿçµæ´»åº”å¯¹æ‰§è¡Œè¿‡ç¨‹ä¸­ä¸æ–­å˜åŒ–çš„æŒ‡ä»¤éœ€æ±‚ã€‚å¤šå¹³å°åŠçœŸå®åœºæ™¯å®éªŒè¯æ˜ï¼Œè¯¥æ¨¡å‹å¯ä»¥é«˜æ•ˆé€‚åº”æ–°åœºæ™¯å¹¶åŠ¨æ€æ›´æ–°ç­–ç•¥ï¼Œåœ¨è‡ªä¸»è§„åˆ’å’Œè·¨å…·èº«æ³›åŒ–(cross-embodiment generalization)æ–¹é¢è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Manuscript under review",
      "pdf_url": "https://arxiv.org/pdf/2512.19178v1",
      "published_date": "2025-12-22 09:12:48 UTC",
      "updated_date": "2025-12-22 09:12:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:15:07.265196+00:00"
    },
    {
      "arxiv_id": "2512.22213v1",
      "title": "On the Existence and Behaviour of Secondary Attention Sinks",
      "title_zh": "è®ºæ¬¡çº§æ³¨æ„åŠ›æ±‡çš„å­˜åœ¨æ€§åŠå…¶è¡Œä¸º",
      "authors": [
        "Jeffrey T. H. Wong",
        "Cheng Zhang",
        "Louis Mahon",
        "Wayne Luk",
        "Anton Isopoussu",
        "Yiren Zhao"
      ],
      "abstract": "Attention sinks are tokens, often the beginning-of-sequence (BOS) token, that receive disproportionately high attention despite limited semantic relevance. In this work, we identify a class of attention sinks, which we term secondary sinks, that differ fundamentally from the sinks studied in prior works, which we term primary sinks. While prior works have identified that tokens other than BOS can sometimes become sinks, they were found to exhibit properties analogous to the BOS token. Specifically, they emerge at the same layer, persist throughout the network and draw a large amount of attention mass. Whereas, we find the existence of secondary sinks that arise primarily in middle layers and can persist for a variable number of layers, and draw a smaller, but still significant, amount of attention mass. Through extensive experiments across 11 model families, we analyze where these secondary sinks appear, their properties, how they are formed, and their impact on the attention mechanism. Specifically, we show that: (1) these sinks are formed by specific middle-layer MLP modules; these MLPs map token representations to vectors that align with the direction of the primary sink of that layer. (2) The $\\ell_2$-norm of these vectors determines the sink score of the secondary sink, and also the number of layers it lasts for, thereby leading to different impacts on the attention mechanisms accordingly. (3) The primary sink weakens in middle layers, coinciding with the emergence of secondary sinks. We observe that in larger-scale models, the location and lifetime of the sinks, together referred to as sink levels, appear in a more deterministic and frequent manner. Specifically, we identify three sink levels in QwQ-32B and six levels in Qwen3-14B.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯†åˆ«å¹¶å®šä¹‰äº†ä¸€ç±»è¢«ç§°ä¸ºæ¬¡çº§æ³¨æ„åŠ›æ±‡(Secondary Attention Sinks)çš„æ–°å‹æ³¨æ„åŠ›ç°è±¡ï¼Œè¿™ä¸ä»¥å¾€ç ”ç©¶ä¸­ä»¥BOS tokenä¸ºä»£è¡¨çš„é¦–è¦æ³¨æ„åŠ›æ±‡(Primary Sinks)å…·æœ‰æ˜¾è‘—å·®å¼‚ã€‚ä¸è´¯ç©¿æ•´ä¸ªç½‘ç»œçš„é¦–è¦æ³¨æ„åŠ›æ±‡ä¸åŒï¼Œæ¬¡çº§æ³¨æ„åŠ›æ±‡ä¸»è¦å‡ºç°åœ¨æ¨¡å‹çš„ä¸­é—´å±‚ï¼ŒæŒç»­å±‚æ•°ä¸å›ºå®šï¼Œä¸”å¸å¼•çš„æ³¨æ„åŠ›æƒé‡è™½æ˜¾è‘—ä½†ä½äºé¦–è¦æ³¨æ„åŠ›æ±‡ã€‚é€šè¿‡å¯¹11ä¸ªæ¨¡å‹ç³»åˆ—çš„å®éªŒåˆ†æï¼Œç ”ç©¶å‘ç°è¿™äº›æ¬¡çº§æ³¨æ„åŠ›æ±‡æ˜¯ç”±ç‰¹å®šçš„ä¸­é—´å±‚MLPæ¨¡å—å½¢æˆçš„ï¼Œè¿™äº›æ¨¡å—å°†tokenè¡¨ç¤ºæ˜ å°„ä¸ºä¸è¯¥å±‚é¦–è¦æ³¨æ„åŠ›æ±‡æ–¹å‘ä¸€è‡´çš„å‘é‡ã€‚å®éªŒè¯æ˜ï¼Œè¿™äº›å‘é‡çš„$\\ell_2$-èŒƒæ•°($\\ell_2$-norm)å†³å®šäº†æ¬¡çº§æ³¨æ„åŠ›æ±‡çš„å¾—åˆ†åŠå…¶æŒç»­çš„å±‚æ•°ï¼Œè¿›è€Œå¯¹æ³¨æ„åŠ›æœºåˆ¶äº§ç”Ÿä¸åŒå½±å“ã€‚ç ”ç©¶è¿˜è§‚å¯Ÿåˆ°ï¼Œåœ¨æ¬¡çº§æ³¨æ„åŠ›æ±‡æ¶Œç°çš„åŒæ—¶ï¼Œé¦–è¦æ³¨æ„åŠ›æ±‡åœ¨ä¸­é—´å±‚ä¼šç›¸åº”å‡å¼±ã€‚åœ¨å¤§è§„æ¨¡æ¨¡å‹å¦‚QwQ-32Bå’ŒQwen3-14Bä¸­ï¼Œæ¬¡çº§æ³¨æ„åŠ›æ±‡çš„å‡ºç°ä½ç½®å’Œç”Ÿå‘½å‘¨æœŸï¼ˆç»Ÿç§°ä¸ºæ±‡èƒ½çº§ï¼ŒSink Levelsï¼‰è¡¨ç°å¾—æ›´åŠ ç¡®å®šä¸”é¢‘ç¹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22213v1",
      "published_date": "2025-12-22 09:06:43 UTC",
      "updated_date": "2025-12-22 09:06:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:15:07.815907+00:00"
    },
    {
      "arxiv_id": "2512.19155v1",
      "title": "Can We Test Consciousness Theories on AI? Ablations, Markers, and Robustness",
      "title_zh": "æˆ‘ä»¬èƒ½å¦åœ¨äººå·¥æ™ºèƒ½ä¸Šæµ‹è¯•æ„è¯†ç†è®ºï¼Ÿæ¶ˆèå®éªŒã€æ ‡è®°ä¸é²æ£’æ€§",
      "authors": [
        "Yin Jun Phua"
      ],
      "abstract": "The search for reliable indicators of consciousness has fragmented into competing theoretical camps (Global Workspace Theory (GWT), Integrated Information Theory (IIT), and Higher-Order Theories (HOT)), each proposing distinct neural signatures. We adopt a synthetic neuro-phenomenology approach: constructing artificial agents that embody these mechanisms to test their functional consequences through precise architectural ablations impossible in biological systems. Across three experiments, we report dissociations suggesting these theories describe complementary functional layers rather than competing accounts. In Experiment 1, a no-rewire Self-Model lesion abolishes metacognitive calibration while preserving first-order task performance, yielding a synthetic blindsight analogue consistent with HOT predictions. In Experiment 2, workspace capacity proves causally necessary for information access: a complete workspace lesion produces qualitative collapse in access-related markers, while partial reductions show graded degradation, consistent with GWT's ignition framework. In Experiment 3, we uncover a broadcast-amplification effect: GWT-style broadcasting amplifies internal noise, creating extreme fragility. The B2 agent family is robust to the same latent perturbation; this robustness persists in a Self-Model-off / workspace-read control, cautioning against attributing the effect solely to $z_{\\text{self}}$ compression. We also report an explicit negative result: raw perturbational complexity (PCI-A) decreases under the workspace bottleneck, cautioning against naive transfer of IIT-adjacent proxies to engineered agents. These results suggest a hierarchical design principle: GWT provides broadcast capacity, while HOT provides quality control. We emphasize that our agents are not conscious; they are reference implementations for testing functional predictions of consciousness theories.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡‡ç”¨åˆæˆç¥ç»ç°è±¡å­¦(synthetic neuro-phenomenology)æ–¹æ³•ï¼Œé€šè¿‡åœ¨äººå·¥æ™ºèƒ½ä½“ä¸Šè¿›è¡Œç”Ÿç‰©ç³»ç»Ÿæ— æ³•å®ç°çš„æ¶æ„æ¶ˆè(ablations)ï¼Œæµ‹è¯•äº†å…¨å±€å·¥ä½œç©ºé—´ç†è®º(GWT)ã€é›†æˆä¿¡æ¯ç†è®º(IIT)å’Œé«˜é˜¶ç†è®º(HOT)ç­‰æ„è¯†ç†è®ºçš„åŠŸèƒ½é¢„æµ‹ã€‚å®éªŒä¸€é€šè¿‡æŸä¼¤Self-Modelå¯¼è‡´å…ƒè®¤çŸ¥æ ¡å‡†å¤±æ•ˆä½†ä¿ç•™åŸºç¡€ä»»åŠ¡æ€§èƒ½ï¼Œäº§ç”Ÿäº†ç¬¦åˆHOTé¢„æµ‹çš„åˆæˆç›²è§†(blindsight)ç°è±¡ï¼›å®éªŒäºŒè¯å®äº†å·¥ä½œç©ºé—´å®¹é‡å¯¹ä¿¡æ¯è®¿é—®çš„å¿…è¦æ€§ï¼Œæ”¯æŒäº†GWTçš„ç‚¹ç«(ignition)æ¡†æ¶ã€‚å®éªŒä¸‰æ­ç¤ºäº†GWTå¼çš„å¹¿æ’­æœºåˆ¶ä¼šæ”¾å¤§å†…éƒ¨å™ªå£°å¹¶å¯¼è‡´ç³»ç»Ÿè„†å¼±æ€§ï¼ŒåŒæ—¶å‘ç°IITçš„ç›¸å…³æŒ‡æ ‡(PCI-A)åœ¨å·¥ä½œç©ºé—´ç“¶é¢ˆä¸‹ä¼šä¸‹é™ï¼Œè­¦ç¤ºä¸åº”å°†è¿™äº›æŒ‡æ ‡ç›²ç›®è¿ç§»è‡³å·¥ç¨‹æ™ºèƒ½ä½“ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¿™äº›ç†è®ºæè¿°çš„æ˜¯äº’è¡¥çš„åŠŸèƒ½å±‚è€Œéç«äº‰å…³ç³»ï¼Œå³GWTè´Ÿè´£å¹¿æ’­èƒ½åŠ›è€ŒHOTè´Ÿè´£è´¨é‡æ§åˆ¶ã€‚ä½œè€…å¼ºè°ƒè¿™äº›æ™ºèƒ½ä½“å¹¶éå…·æœ‰æ„è¯†ï¼Œè€Œæ˜¯ç”¨äºæµ‹è¯•æ„è¯†ç†è®ºåŠŸèƒ½é¢„æµ‹çš„å‚è€ƒå®ç°ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19155v1",
      "published_date": "2025-12-22 08:52:07 UTC",
      "updated_date": "2025-12-22 08:52:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:14:49.064669+00:00"
    },
    {
      "arxiv_id": "2512.19154v1",
      "title": "Beyond Sliding Windows: Learning to Manage Memory in Non-Markovian Environments",
      "title_zh": "è¶…è¶Šæ»‘åŠ¨çª—å£ï¼šåœ¨éé©¬å°”å¯å¤«ç¯å¢ƒä¸­å­¦ä¹ è®°å¿†ç®¡ç†",
      "authors": [
        "Geraud Nangue Tasse",
        "Matthew Riemer",
        "Benjamin Rosman",
        "Tim Klinger"
      ],
      "abstract": "Recent success in developing increasingly general purpose agents based on sequence models has led to increased focus on the problem of deploying computationally limited agents within the vastly more complex real-world. A key challenge experienced in these more realistic domains is highly non-Markovian dependencies with respect to the agent's observations, which are less common in small controlled domains. The predominant approach for dealing with this in the literature is to stack together a window of the most recent observations (Frame Stacking), but this window size must grow with the degree of non-Markovian dependencies, which results in prohibitive computational and memory requirements for both action inference and learning. In this paper, we are motivated by the insight that in many environments that are highly non-Markovian with respect to time, the environment only causally depends on a relatively small number of observations over that time-scale. A natural direction would then be to consider meta-algorithms that maintain relatively small adaptive stacks of memories such that it is possible to express highly non-Markovian dependencies with respect to time while considering fewer observations at each step and thus experience substantial savings in both compute and memory requirements. Hence, we propose a meta-algorithm (Adaptive Stacking) for achieving exactly that with convergence guarantees and quantify the reduced computation and memory constraints for MLP, LSTM, and Transformer-based agents. Our experiments utilize popular memory tasks, which give us control over the degree of non-Markovian dependencies. This allows us to demonstrate that an appropriate meta-algorithm can learn the removal of memories not predictive of future rewards without excessive removal of important experiences. Code: https://github.com/geraudnt/adaptive-stacking",
      "tldr_zh": "é’ˆå¯¹ç°å®ä¸–ç•Œä¸­é«˜åº¦éé©¬å°”å¯å¤« (non-Markovian) ç¯å¢ƒä¸‹æ™ºèƒ½ä½“è§‚æµ‹ä¾èµ–å¤æ‚çš„æŒ‘æˆ˜ï¼Œè¯¥ç ”ç©¶æ¢è®¨äº†ä¼ ç»Ÿå¤šå¸§å †å  (Frame Stacking) æ–¹æ³•åœ¨å¤„ç†é•¿ç¨‹ä¾èµ–æ—¶é¢ä¸´çš„è®¡ç®—ä¸å†…å­˜ç“¶é¢ˆã€‚ç ”ç©¶è€…åŸºäºç¯å¢ƒé€šå¸¸ä»…å› æœä¾èµ–äºå°‘é‡å…³é”®è§‚æµ‹çš„æ´å¯Ÿï¼Œæå‡ºäº†ä¸€ç§åä¸º Adaptive Stacking çš„å…ƒç®—æ³•ï¼Œé€šè¿‡ç»´æŠ¤ä¸€ä¸ªè¾ƒå°çš„è‡ªé€‚åº”è®°å¿†å †å æ¥æ•è·éé©¬å°”å¯å¤«ä¾èµ–ã€‚è¯¥æ–¹æ³•å…è®¸æ™ºèƒ½ä½“åœ¨æ¯ä¸€æ­¥å¤„ç†æ›´å°‘çš„è§‚æµ‹å€¼ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†åŸºäº MLPã€LSTM å’Œ Transformer æ¶æ„çš„æ™ºèƒ½ä½“åœ¨æ¨ç†å’Œå­¦ä¹ è¿‡ç¨‹ä¸­çš„è®¡ç®—èµ„æºæ¶ˆè€—ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶ä¸º Adaptive Stacking æä¾›äº†æ”¶æ•›ä¿è¯ï¼Œå¹¶é‡åŒ–äº†å…¶åœ¨ä¸åŒæ¨¡å‹æ¶æ„ä¸‹çš„èµ„æºèŠ‚çœæ•ˆæœã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å…ƒç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ å¹¶ç§»é™¤å¯¹æœªæ¥å¥–åŠ±é¢„æµ‹æ— ç”¨çš„è®°å¿†ï¼ŒåŒæ—¶ç¡®ä¿é‡è¦ç»éªŒä¸è¢«è¿‡åº¦åˆ é™¤ï¼Œä¸ºåœ¨èµ„æºå—é™ç¯å¢ƒä¸‹éƒ¨ç½²é«˜æ•ˆæ™ºèƒ½ä½“æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19154v1",
      "published_date": "2025-12-22 08:50:30 UTC",
      "updated_date": "2025-12-22 08:50:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:14:40.169103+00:00"
    },
    {
      "arxiv_id": "2512.19135v1",
      "title": "Understanding Chain-of-Thought in Large Language Models via Topological Data Analysis",
      "title_zh": "åŸºäºæ‹“æ‰‘æ•°æ®åˆ†æè§£æå¤§è¯­è¨€æ¨¡å‹ä¸­çš„é“¾å¼æ€ç»´",
      "authors": [
        "Chenghao Li",
        "Chaoning Zhang",
        "Yi Lu",
        "Shuxu Chen",
        "Xudong Wang",
        "Jiaquan Zhang",
        "Zhicheng Wang",
        "Zhengxun Jin",
        "Kuien Liu",
        "Sung-Ho Bae",
        "Guoqing Wang",
        "Yang Yang",
        "Hen Tao Shen"
      ],
      "abstract": "With the development of large language models (LLMs), particularly with the introduction of the long reasoning chain technique, the reasoning ability of LLMs in complex problem-solving has been significantly enhanced. While acknowledging the power of long reasoning chains, we cannot help but wonder: Why do different reasoning chains perform differently in reasoning? What components of the reasoning chains play a key role? Existing studies mainly focus on evaluating reasoning chains from a functional perspective, with little attention paid to their structural mechanisms. To address this gap, this work is the first to analyze and evaluate the quality of the reasoning chain from a structural perspective. We apply persistent homology from Topological Data Analysis (TDA) to map reasoning steps into semantic space, extract topological features, and analyze structural changes. These changes reveal semantic coherence, logical redundancy, and identify logical breaks and gaps. By calculating homology groups, we assess connectivity and redundancy at various scales, using barcode and persistence diagrams to quantify stability and consistency. Our results show that the topological structural complexity of reasoning chains correlates positively with accuracy. More complex chains identify correct answers sooner, while successful reasoning exhibits simpler topologies, reducing redundancy and cycles, enhancing efficiency and interpretability. This work provides a new perspective on reasoning chain quality assessment and offers guidance for future optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶é¦–æ¬¡ä»ç»“æ„è§†è§’åˆ†æå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„æ¨ç†é“¾è´¨é‡ï¼Œåˆ©ç”¨æ‹“æ‰‘æ•°æ®åˆ†æ(TDA)ä¸­çš„æŒç»­åŒè°ƒ(Persistent Homology)å°†æ¨ç†æ­¥éª¤æ˜ å°„è‡³è¯­ä¹‰ç©ºé—´å¹¶æå–æ‹“æ‰‘ç‰¹å¾ã€‚é€šè¿‡åˆ†æç»“æ„å˜åŒ–ï¼Œè¯¥æ–¹æ³•æ­ç¤ºäº†è¯­ä¹‰è¿è´¯æ€§(Semantic Coherence)ã€é€»è¾‘å†—ä½™(Logical Redundancy)å¹¶æœ‰æ•ˆè¯†åˆ«é€»è¾‘ä¸­æ–­ï¼Œåˆ©ç”¨æ¡å½¢ç (Barcode)å’ŒæŒä¹…æ€§å›¾(Persistence Diagrams)é‡åŒ–äº†æ¨ç†é“¾çš„ç¨³å®šæ€§å’Œä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨ç†é“¾çš„æ‹“æ‰‘ç»“æ„å¤æ‚åº¦ä¸å‡†ç¡®ç‡å‘ˆæ­£ç›¸å…³ï¼Œä¸”ç»“æ„å¤æ‚çš„é“¾æ¡èƒ½æ›´æ—©é”å®šæ­£ç¡®ç­”æ¡ˆã€‚æ­¤å¤–ï¼ŒæˆåŠŸçš„æ¨ç†è¿‡ç¨‹å¾€å¾€è¡¨ç°å‡ºæ›´ç®€å•çš„æ‹“æ‰‘ç»“æ„ï¼Œé€šè¿‡å‡å°‘å†—ä½™å’Œå¾ªç¯æå‡äº†æ¨ç†æ•ˆç‡ä¸å¯è§£é‡Šæ€§(Interpretability)ã€‚è¯¥å·¥ä½œä¸ºæ¨ç†é“¾è´¨é‡è¯„ä¼°æä¾›äº†å…¨æ–°çš„è§†è§’ï¼Œå¹¶ä¸ºæœªæ¥LLMsæ¨ç†èƒ½åŠ›çš„ä¼˜åŒ–æä¾›äº†ç†è®ºæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19135v1",
      "published_date": "2025-12-22 08:28:08 UTC",
      "updated_date": "2025-12-22 08:28:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:14:49.225049+00:00"
    },
    {
      "arxiv_id": "2601.03272v1",
      "title": "Less is more: Not all samples are effective for evaluation",
      "title_zh": "å°‘å³æ˜¯å¤šï¼šå¹¶éæ‰€æœ‰æ ·æœ¬åœ¨è¯„ä¼°ä¸­éƒ½å…·æœ‰æ•ˆæ€§",
      "authors": [
        "Wentang Song",
        "Jinqiang Li",
        "Kele Huang",
        "Junhui Lin",
        "Shengxiang Wu",
        "Zhongshi Xie"
      ],
      "abstract": "The versatility of Large Language Models (LLMs) in vertical domains has spurred the development of numerous specialized evaluation benchmarks. However, these benchmarks often suffer from significant semantic redundancy and impose high computational costs during evaluation. Existing compression methods, such as tinyBenchmarks depend critically on correctness labels from multiple historical models evaluated on the full test set, making them inapplicable in cold-start scenarios, such as the introduction of a new task, domain, or model with no prior evaluation history.\n  To address this limitation, we propose a history-free test set compression framework that requires no prior model performance data. Our method begins by fine-tuning a base LLM on a small amount of domain-specific data to internalize task-relevant semantics. It then generates high-level semantic embeddings for all original test samples using only their raw textual content. In this domain-adapted embedding space, we perform task-aware clustering and introduce a novel dataset X-ray mechanism that analyzes cluster geometry to dynamically calibrate the compression intensity based on the intrinsic redundancy of the benchmark.\n  Experiments on professional-domain dataset, notably a large-scale 3GPP communications benchmark, demonstrate that our approach effectively identifies and removes redundant samples, reducing evaluation cost by over 90% while preserving high fidelity to the full benchmark.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)å‚ç›´é¢†åŸŸè¯„ä¼°åŸºå‡†å­˜åœ¨çš„è¯­ä¹‰å†—ä½™å’Œé«˜è®¡ç®—æˆæœ¬é—®é¢˜ï¼ŒæŒ‡å‡ºç°æœ‰å‹ç¼©æ–¹æ³•å¦‚tinyBenchmarksåœ¨å†·å¯åŠ¨(cold-start)åœºæ™¯ä¸‹çš„å±€é™æ€§ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…æå‡ºäº†ä¸€ä¸ªæ— éœ€å†å²æ€§èƒ½æ•°æ®çš„æµ‹è¯•é›†å‹ç¼©æ¡†æ¶ï¼Œé€šè¿‡åœ¨å°‘é‡é¢†åŸŸæ•°æ®ä¸Šå¾®è°ƒåŸºç¡€æ¨¡å‹æ¥å†…åŒ–ä»»åŠ¡ç›¸å…³çš„è¯­ä¹‰ã€‚è¯¥æ–¹æ³•åˆ©ç”¨åŸå§‹æ–‡æœ¬ç”Ÿæˆé«˜å±‚è¯­ä¹‰åµŒå…¥(semantic embeddings)å¹¶è¿›è¡Œä»»åŠ¡æ„ŸçŸ¥èšç±»ï¼ŒåŒæ—¶å¼•å…¥ä¸€ç§æ–°å‹çš„æ•°æ®é›†X-rayæœºåˆ¶ï¼Œæ ¹æ®åŸºå‡†æµ‹è¯•çš„å›ºæœ‰å†—ä½™åŠ¨æ€æ ¡å‡†å‹ç¼©å¼ºåº¦ã€‚åœ¨3GPPé€šä¿¡åŸºå‡†ç­‰ä¸“ä¸šé¢†åŸŸæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆè¯†åˆ«å¹¶ç§»é™¤å†—ä½™æ ·æœ¬ã€‚æœ€ç»ˆç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨ä¿æŒä¸å…¨é‡åŸºå‡†é«˜åº¦ä¿çœŸ(high fidelity)çš„å‰æä¸‹ï¼ŒæˆåŠŸå°†è¯„ä¼°æˆæœ¬é™ä½äº†90%ä»¥ä¸Šã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.03272v1",
      "published_date": "2025-12-22 08:04:05 UTC",
      "updated_date": "2025-12-22 08:04:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:14:58.162002+00:00"
    },
    {
      "arxiv_id": "2512.19114v1",
      "title": "HyperLoad: A Cross-Modality Enhanced Large Language Model-Based Framework for Green Data Center Cooling Load Prediction",
      "title_zh": "HyperLoadï¼šåŸºäºè·¨æ¨¡æ€å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„ç»¿è‰²æ•°æ®ä¸­å¿ƒåˆ¶å†·è´Ÿè·é¢„æµ‹æ¡†æ¶",
      "authors": [
        "Haoyu Jiang",
        "Boan Qu",
        "Junjie Zhu",
        "Fanjie Zeng",
        "Xiaojie Lin",
        "Wei Zhong"
      ],
      "abstract": "The rapid growth of artificial intelligence is exponentially escalating computational demand, inflating data center energy use and carbon emissions, and spurring rapid deployment of green data centers to relieve resource and environmental stress. Achieving sub-minute orchestration of renewables, storage, and loads, while minimizing PUE and lifecycle carbon intensity, hinges on accurate load forecasting. However, existing methods struggle to address small-sample scenarios caused by cold start, load distortion, multi-source data fragmentation, and distribution shifts in green data centers. We introduce HyperLoad, a cross-modality framework that exploits pre-trained large language models (LLMs) to overcome data scarcity. In the Cross-Modality Knowledge Alignment phase, textual priors and time-series data are mapped to a common latent space, maximizing the utility of prior knowledge. In the Multi-Scale Feature Modeling phase, domain-aligned priors are injected through adaptive prefix-tuning, enabling rapid scenario adaptation, while an Enhanced Global Interaction Attention mechanism captures cross-device temporal dependencies. The public DCData dataset is released for benchmarking. Under both data sufficient and data scarce settings, HyperLoad consistently surpasses state-of-the-art (SOTA) baselines, demonstrating its practicality for sustainable green data center management.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†HyperLoadï¼Œä¸€ç§åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è·¨æ¨¡æ€å¢å¼ºæ¡†æ¶ï¼Œæ—¨åœ¨æå‡ç»¿è‰²æ•°æ®ä¸­å¿ƒåˆ¶å†·è´Ÿè·é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†è§£å†³å†·å¯åŠ¨å’Œæ•°æ®ç¨€ç¼ºå¯¼è‡´çš„å°æ ·æœ¬é¢„æµ‹éš¾é¢˜ï¼ŒHyperLoadåœ¨è·¨æ¨¡æ€çŸ¥è¯†å¯¹é½(Cross-Modality Knowledge Alignment)é˜¶æ®µå°†æ–‡æœ¬å…ˆéªŒä¸æ—¶é—´åºåˆ—æ•°æ®æ˜ å°„åˆ°ç»Ÿä¸€æ½œåœ¨ç©ºé—´ã€‚åœ¨å¤šå°ºåº¦ç‰¹å¾å»ºæ¨¡(Multi-Scale Feature Modeling)é˜¶æ®µï¼Œè¯¥æ¡†æ¶é€šè¿‡è‡ªé€‚åº”å‰ç¼€å¾®è°ƒ(adaptive prefix-tuning)æ³¨å…¥é¢†åŸŸå¯¹é½çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶åˆ©ç”¨å¢å¼ºå…¨å±€äº¤äº’æ³¨æ„åŠ›æœºåˆ¶(Enhanced Global Interaction Attention)æ•æ‰è·¨è®¾å¤‡çš„æ—¶åºä¾èµ–ã€‚ç ”ç©¶å›¢é˜ŸåŒæ­¥å‘å¸ƒäº†DCDataå…¬å¼€æ•°æ®é›†ç”¨äºåŸºå‡†æµ‹è¯•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ— è®ºåœ¨æ•°æ®å……è¶³è¿˜æ˜¯ç¨€ç¼ºçš„æƒ…å†µä¸‹ï¼ŒHyperLoadå‡ä¸€è‡´è¶…è¶Šäº†ç°æœ‰çš„SOTAåŸºçº¿æ¨¡å‹ï¼Œè¯æ˜äº†å…¶åœ¨é™ä½PUEå’Œç¢³å¼ºåº¦ç­‰å¯æŒç»­æ•°æ®ä¸­å¿ƒç®¡ç†ä»»åŠ¡ä¸­çš„å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19114v1",
      "published_date": "2025-12-22 07:35:16 UTC",
      "updated_date": "2025-12-22 07:35:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:15:03.515071+00:00"
    },
    {
      "arxiv_id": "2601.06056v1",
      "title": "Using street view images and visual LLMs to predict heritage values for governance support: Risks, ethics, and policy implications",
      "title_zh": "åˆ©ç”¨è¡—æ™¯å›¾åƒä¸è§†è§‰å¤§è¯­è¨€æ¨¡å‹é¢„æµ‹é—äº§ä»·å€¼ä»¥æ”¯æŒæ²»ç†ï¼šé£é™©ã€ä¼¦ç†ä¸æ”¿ç­–å¯ç¤º",
      "authors": [
        "Tim Johansson",
        "Mikael Mangold",
        "Kristina Dabrock",
        "Anna Donarelli",
        "Ingrid Campo-Ruiz"
      ],
      "abstract": "During 2025 and 2026, the Energy Performance of Buildings Directive is being implemented in the European Union member states, requiring all member states to have National Building Renovation Plans. In Sweden, there is a lack of a national register of buildings with heritage values. This is seen as a barrier for the analyses underlying the development of Building Renovation Plans by the involved Swedish authorities. The purpose of this research was to assist Swedish authorities in assigning heritage values to building in the Swedish building stock. As part of the analyses, buildings in street view images from all over Sweden (N=154 710) have been analysed using multimodal Large Language Models (LLM) to assess aspects of heritage value. Zero-shot predictions by LLMs were used as a basis to for identifying buildings with potential heritage values for 5.0 million square meters of heated floor area for the Swedish Building Renovation Plan. In this paper, the results of the predictions and lessons learnt are presented and related to the development of Swedish Building Renovation Plan as part of governance. Potential risks for authorities using LLM-based data are addressed, with a focus on issues of transparency, error detection and sycophancy.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨è¡—æ™¯å›¾åƒå’Œå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal Large Language Models) æ¥é¢„æµ‹å»ºç­‘çš„é—äº§ä»·å€¼ (Heritage values)ï¼Œæ—¨åœ¨ä¸ºç‘å…¸å›½å®¶å»ºç­‘ç¿»æ–°è®¡åˆ’ (National Building Renovation Plans) çš„æ²»ç†æä¾›æ”¯æŒã€‚é’ˆå¯¹ç‘å…¸ç¼ºä¹å…¨å›½æ€§é—äº§ä»·å€¼å»ºç­‘ç™»è®°å†Œçš„é—®é¢˜ï¼Œç ”ç©¶äººå‘˜åˆ©ç”¨ LLMs çš„é›¶æ ·æœ¬ (Zero-shot) é¢„æµ‹èƒ½åŠ›åˆ†æäº†ç‘å…¸å…¨å¢ƒ 15.4 ä¸‡å¼ è¡—æ™¯å›¾åƒï¼Œè¯†åˆ«å‡ºå…·æœ‰æ½œåœ¨é—äº§ä»·å€¼çš„ 500 ä¸‡å¹³æ–¹ç±³å»ºç­‘é¢ç§¯ã€‚è®ºæ–‡æ€»ç»“äº†é¢„æµ‹ç»“æœåŠåœ¨æ”¿åŠ¡æ²»ç†ä¸­çš„å®è·µç»éªŒï¼Œå¹¶æ·±å…¥åˆ†æäº†æ”¿åºœæœºæ„åœ¨ä½¿ç”¨åŸºäº LLM çš„æ•°æ®æ—¶é¢ä¸´çš„æ½œåœ¨é£é™©ï¼Œç‰¹åˆ«æ˜¯é€æ˜åº¦ã€é”™è¯¯æ£€æµ‹å’Œè°„åªš (Sycophancy) ç­‰é—®é¢˜ã€‚è¿™é¡¹å·¥ä½œä¸ä»…å¡«è¡¥äº†ç‘å…¸å»ºç­‘æ¡£æ¡ˆçš„ç©ºç™½ï¼Œè¿˜ä¸º AI è¾…åŠ©å†³ç­–ä¸­çš„ä¼¦ç†ä¸æ”¿ç­–å®æ–½æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.06056v1",
      "published_date": "2025-12-22 07:30:42 UTC",
      "updated_date": "2025-12-22 07:30:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:15:06.939995+00:00"
    },
    {
      "arxiv_id": "2512.19107v1",
      "title": "FC-MIR: A Mobile Screen Awareness Framework for Intent-Aware Recommendation based on Frame-Compressed Multimodal Trajectory Reasoning",
      "title_zh": "FC-MIRï¼šåŸºäºå¸§å‹ç¼©å¤šæ¨¡æ€è½¨è¿¹æ¨ç†çš„æ„å›¾æ„ŸçŸ¥æ¨èç§»åŠ¨ç«¯å±å¹•æ„ŸçŸ¥æ¡†æ¶",
      "authors": [
        "Zhe Yang",
        "Xiaoshuang Sheng",
        "Zhengnan Zhang",
        "Jidong Wu",
        "Zexing Wang",
        "Xin He",
        "Shenghua Xu",
        "Guanjing Xiong"
      ],
      "abstract": "Identifying user intent from mobile UI operation trajectories is critical for advancing UI understanding and enabling task automation agents. While Multimodal Large Language Models (MLLMs) excel at video understanding tasks, their real-time mobile deployment is constrained by heavy computational costs and inefficient redundant frame processing. To address these issues, we propose the FC-MIR framework: leveraging keyframe sampling and adaptive concatenation, it cuts visual redundancy to boost inference efficiency, while integrating state-of-the-art closed-source MLLMs or fine-tuned models (e.g., Qwen3-VL) for trajectory summarization and intent prediction. We further expand task scope to explore generating post-prediction operations and search suggestions, and introduce a fine-grained metric to evaluate the practical utility of summaries, predictions, and suggestions. For rigorous assessment, we construct a UI trajectory dataset covering scenarios from UI-Agents (Agent-I) and real user interactions (Person-I). Experimental results show our compression method retains performance at 50%-60% compression rates; both closed-source and fine-tuned MLLMs demonstrate strong intent summarization, supporting potential lightweight on-device deployment. However, MLLMs still struggle with useful and \"surprising\" suggestions, leaving room for improvement. Finally, we deploy the framework in a real-world setting, integrating UI perception and UI-Agent proxies to lay a foundation for future progress in this field.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FC-MIRï¼Œä¸€ç§åŸºäºå¸§å‹ç¼©å¤šæ¨¡æ€è½¨è¿¹æ¨ç†çš„ç§»åŠ¨å±å¹•æ„ŸçŸ¥æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ†æç§»åŠ¨ç«¯UIæ“ä½œè½¨è¿¹æ¥å®ç°æ„å›¾æ„ŸçŸ¥çš„æ¨èå’Œä»»åŠ¡è‡ªåŠ¨åŒ–ã€‚é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰åœ¨ç§»åŠ¨ç«¯éƒ¨ç½²æ—¶è®¡ç®—å¼€é”€å¤§åŠå†—ä½™å¸§å¤„ç†ä½æ•ˆçš„é—®é¢˜ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å…³é”®å¸§é‡‡æ ·ä¸è‡ªé€‚åº”æ‹¼æ¥æŠ€æœ¯ï¼Œä»¥æ˜¾è‘—æå‡æ¨ç†æ•ˆç‡ã€‚FC-MIRé›†æˆäº†é—­æºMLLMsåŠå¾®è°ƒåçš„Qwen3-VLç­‰æ¨¡å‹ï¼Œç”¨äºè½¨è¿¹æ€»ç»“ä¸æ„å›¾é¢„æµ‹ï¼Œå¹¶è¿›ä¸€æ­¥æ‰©å±•è‡³ç”Ÿæˆæ“ä½œæŒ‡ä»¤ä¸æœç´¢å»ºè®®ã€‚ç ”ç©¶å›¢é˜Ÿæ„å»ºäº†æ¶µç›–UIæ™ºèƒ½ä½“ï¼ˆAgent-Iï¼‰ä¸çœŸå®ç”¨æˆ·äº¤äº’ï¼ˆPerson-Iï¼‰åœºæ™¯çš„UIè½¨è¿¹æ•°æ®é›†ï¼Œå¹¶æå‡ºç»†ç²’åº¦æŒ‡æ ‡ä»¥è¯„ä¼°é¢„æµ‹çš„å®ç”¨æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å‹ç¼©ç‡è¾¾50%-60%æ—¶ä»èƒ½ç»´æŒè‰¯å¥½æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨è½»é‡åŒ–è®¾å¤‡ç«¯éƒ¨ç½²çš„æ½œåŠ›ã€‚è™½ç„¶æ¨¡å‹åœ¨ç”Ÿæˆå…·æœ‰å¯å‘æ€§çš„å»ºè®®æ–¹é¢ä»æœ‰æå‡ç©ºé—´ï¼Œä½†è¯¥æ¡†æ¶åœ¨ç°å®åœºæ™¯ä¸­çš„æˆåŠŸéƒ¨ç½²ä¸ºæœªæ¥UIæ„ŸçŸ¥ä¸æ™ºèƒ½ä»£ç†æŠ€æœ¯çš„å‘å±•å¥ å®šäº†åšå®åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19107v1",
      "published_date": "2025-12-22 07:21:07 UTC",
      "updated_date": "2025-12-22 07:21:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:15:14.160925+00:00"
    },
    {
      "arxiv_id": "2512.19097v1",
      "title": "DIVER-1 : Deep Integration of Vast Electrophysiological Recordings at Scale",
      "title_zh": "DIVER-1ï¼šæµ·é‡ç”µç”Ÿç†è®°å½•çš„å¤§è§„æ¨¡æ·±åº¦é›†æˆ",
      "authors": [
        "Danny Dongyeop Han",
        "Yonghyeon Gwon",
        "Ahhyun Lucy Lee",
        "Taeyang Lee",
        "Seong Jin Lee",
        "Jubin Choi",
        "Sebin Lee",
        "Jihyun Bang",
        "Seungju Lee",
        "David Keetae Park",
        "Shinjae Yoo",
        "Chun Kee Chung",
        "Jiook Cha"
      ],
      "abstract": "Electrophysiology signals such as EEG and iEEG are central to neuroscience, brain-computer interfaces, and clinical applications, yet existing foundation models remain limited in scale despite clear evidence that scaling improves performance. We introduce DIVER-1, a family of EEG and iEEG foundation models trained on the largest and most diverse corpus to date-5.3k hours of iEEG and 54k hours of EEG (1.6M channel-hours from over 17.7k subjects)-and scaled up to 1.82B parameters. We present the first systematic scaling law analysis for this domain, showing that they follow data-constrained scaling laws: for a given amount of data and compute, smaller models trained for extended epochs consistently outperform larger models trained briefly. This behavior contrasts with prior electrophysiology foundation models that emphasized model size over training duration. To achieve strong performance, we also design architectural innovations including any-variate attention, sliding temporal conditional positional encoding, and multi-domain reconstruction. DIVER-1 iEEG and EEG models each achieve state-of-the-art performance on their respective benchmarks, establishing a concrete guidelines for efficient scaling and resource allocation in electrophysiology foundation model development.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†DIVER-1ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—é’ˆå¯¹EEGå’ŒiEEGçš„åŸºç¡€æ¨¡å‹(foundation models)ï¼Œåœ¨ç›®å‰æœ€å¤§ä¸”æœ€å¤šæ ·åŒ–çš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†è®­ç»ƒï¼ŒåŒ…å«5.3kå°æ—¶çš„iEEGå’Œ54kå°æ—¶çš„EEGã€‚æ¨¡å‹å‚æ•°è§„æ¨¡æœ€é«˜è¾¾1.82Bï¼Œæ¶µç›–äº†æ¥è‡ª1.77ä¸‡å¤šåå—è¯•è€…çš„160ä¸‡é€šé“å°æ—¶æ•°æ®ã€‚ç ”ç©¶é€šè¿‡é¦–æ¬¡ç³»ç»Ÿçš„ç¼©æ”¾æ³•åˆ™(scaling law)åˆ†æå‘ç°ï¼Œè¯¥é¢†åŸŸéµå¾ªæ•°æ®å—é™(data-constrained)çš„ç¼©æ”¾è§„å¾‹ï¼Œå³åœ¨ç›¸åŒè®¡ç®—èµ„æºä¸‹ï¼Œå»¶é•¿è®­ç»ƒå‘¨æœŸçš„ä¸­å°å‹æ¨¡å‹è¡¨ç°ä¼˜äºçŸ­æ—¶è®­ç»ƒçš„å¤§å‹æ¨¡å‹ã€‚ä¸ºäº†å®ç°å“è¶Šæ€§èƒ½ï¼Œç ”ç©¶å¼•å…¥äº†any-variate attentionã€sliding temporal conditional positional encodingå’Œmulti-domain reconstructionç­‰æ¶æ„åˆ›æ–°ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒDIVER-1åœ¨å„è‡ªçš„åŸºå‡†æµ‹è¯•ä¸­å‡è¾¾åˆ°äº†SOTAæ€§èƒ½ï¼Œä¸ºç”µç”Ÿç†åŸºç¡€æ¨¡å‹çš„é«˜æ•ˆå¼€å‘å’Œèµ„æºåˆ†é…æä¾›äº†é‡è¦æŒ‡å—ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "47 pages, 13 figures, 26 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.19097v1",
      "published_date": "2025-12-22 07:07:43 UTC",
      "updated_date": "2025-12-22 07:07:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:15:29.401600+00:00"
    },
    {
      "arxiv_id": "2512.19096v1",
      "title": "Conditioning Accept-Desirability models in the context of AGM-like belief change",
      "title_zh": "ç±» AGM ä¿¡å¿µå˜è¿èƒŒæ™¯ä¸‹çš„æ¥å—-æœŸæœ›æ¨¡å‹æ¡ä»¶åŒ–",
      "authors": [
        "Kathelijne Coussement",
        "Gert de Cooman",
        "Keano De Vos"
      ],
      "abstract": "We discuss conditionalisation for Accept-Desirability models in an abstract decision-making framework, where uncertain rewards live in a general linear space, and events are special projection operators on that linear space. This abstract setting allows us to unify classical and quantum probabilities, and extend them to an imprecise probabilities context. We introduce a new conditioning rule for our Accept-Desirability models, based on the idea that observing an event introduces new indifferences between options. We associate a belief revision operator with our conditioning rule, and investigate which of the AGM axioms for belief revision still hold in our more general framework. We investigate two interesting special cases where all of these axioms are shown to still hold: classical propositional logic and full conditional probabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨ä¸€ç§æŠ½è±¡å†³ç­–æ¡†æ¶ä¸‹æ¢è®¨äº† Accept-Desirability æ¨¡å‹çš„æ¡ä»¶åŒ– (conditionalisation) é—®é¢˜ï¼Œå…¶ä¸­ä¸ç¡®å®šæ€§å¥–åŠ±å­˜åœ¨äºå¹¿ä¹‰çº¿æ€§ç©ºé—´ï¼Œä¸”äº‹ä»¶è¢«å®šä¹‰ä¸ºè¯¥ç©ºé—´ä¸Šçš„ç‰¹æ®ŠæŠ•å½±ç®—å­ã€‚è¿™ä¸€æŠ½è±¡è®¾å®šæˆåŠŸç»Ÿä¸€äº†å¤å…¸æ¦‚ç‡ä¸é‡å­æ¦‚ç‡ï¼Œå¹¶å°†å…¶æ‰©å±•è‡³ä¸ç²¾ç¡®æ¦‚ç‡ (imprecise probabilities) é¢†åŸŸã€‚ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºâ€œè§‚å¯Ÿäº‹ä»¶ä¼šå¼•å…¥é€‰é¡¹é—´æ–°æ— å·®å¼‚æ€§ (indifferences)â€è¿™ä¸€ç†å¿µçš„æ–°å‹æ¡ä»¶åŒ–è§„åˆ™ã€‚éšåï¼Œç ”ç©¶å°†è¯¥è§„åˆ™ä¸ä¿¡å¿µä¿®æ­£ç®—å­ (belief revision operator) ç›¸ç»“åˆï¼Œæ·±å…¥è€ƒå¯Ÿäº† AGM å…¬ç†åœ¨è¿™ä¸€æ›´é€šç”¨æ¡†æ¶ä¸‹çš„é€‚ç”¨æ€§ã€‚é€šè¿‡å¯¹å¤å…¸å‘½é¢˜é€»è¾‘å’Œå…¨æ¡ä»¶æ¦‚ç‡ä¸¤ä¸ªç‰¹æ®Šæ¡ˆä¾‹çš„ç ”ç©¶ï¼Œè¯æ˜äº†æ‰€æœ‰ AGM å…¬ç†åœ¨è¿™äº›æƒ…å¢ƒä¸‹ä¾ç„¶æˆç«‹ã€‚",
      "categories": [
        "cs.AI",
        "math.LO",
        "math.PR"
      ],
      "primary_category": "cs.AI",
      "comment": "46 pages, 1 table",
      "pdf_url": "https://arxiv.org/pdf/2512.19096v1",
      "published_date": "2025-12-22 07:07:34 UTC",
      "updated_date": "2025-12-22 07:07:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:15:33.028695+00:00"
    },
    {
      "arxiv_id": "2512.19093v1",
      "title": "Tool-Augmented Hybrid Ensemble Reasoning with Distillation for Bilingual Mathematical Problem Solving",
      "title_zh": "é¢å‘åŒè¯­æ•°å­¦è§£é¢˜çš„å·¥å…·å¢å¼ºå‹æ··åˆé›†æˆæ¨ç†ä¸è’¸é¦",
      "authors": [
        "Peiqing Lu",
        "Yuan Zhang",
        "Haoyun Zhang",
        "Jiasen Zheng",
        "Kejian Tong",
        "Wenjun Wu"
      ],
      "abstract": "Bilingual mathematical problem solving needs a clear link between language reasoning and symbolic calculation. Large language models often handle language well but are weak in accurate computation. This paper presents HERALD (Hybrid Ensemble Reasoning with Adaptive Learning and Distillation), a framework that joins reasoning and calculation using NuminaMath-7B-TIR, GPT-4o, and Mistral-7B. HERALD uses adaptive routing, tool-based reinforcement learning, and knowledge distillation to connect different reasoning paths. Confidence calibration keeps weighting stable, and dual-path checking keeps results correct. Reinforcement learning controls tool use to cut redundancy, and distillation lowers delay without hurting accuracy. The system shows that combining symbolic checking, adaptive ensembles, and bilingual fine-tuning helps achieve both fluent reasoning and precise calculation. HERALD offers a practical solution for multilingual mathematical reasoning with better accuracy, stability, and clarity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† HERALD æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŒè¯­æ•°å­¦é—®é¢˜æ±‚è§£ä¸­è¯­è¨€æ¨ç†ä¸ç¬¦å·è®¡ç®—ä¹‹é—´çš„é“¾æ¥éš¾é¢˜ã€‚HERALD ç»“åˆäº† NuminaMath-7B-TIRã€GPT-4o å’Œ Mistral-7B ç­‰æ¨¡å‹ï¼Œé€šè¿‡è‡ªé€‚åº”è·¯ç”±(Adaptive Routing)å’Œå·¥å…·å¢å¼ºçš„å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å°†ä¸åŒçš„æ¨ç†è·¯å¾„æœ‰æ•ˆè¿æ¥ã€‚ä¸ºäº†ç¡®ä¿ç»“æœçš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨äº†ç½®ä¿¡åº¦æ ¡å‡†(Confidence Calibration)å’ŒåŒè·¯å¾„æ£€æŸ¥(Dual-path Checking)æœºåˆ¶ã€‚é€šè¿‡çŸ¥è¯†è’¸é¦(Knowledge Distillation)æŠ€æœ¯ï¼Œç³»ç»Ÿåœ¨ä¸æŸå¤±å‡†ç¡®æ€§çš„å‰æä¸‹æ˜¾è‘—é™ä½äº†æ¨ç†å»¶è¿Ÿï¼Œè€Œå¼ºåŒ–å­¦ä¹ åˆ™ä¼˜åŒ–äº†å·¥å…·è°ƒç”¨ä»¥å‡å°‘å†—ä½™ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§ç»“åˆç¬¦å·æ£€æŸ¥ã€è‡ªé€‚åº”é›†æˆ(Adaptive Ensembles)å’ŒåŒè¯­å¾®è°ƒçš„æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨ç†çš„æµç•…åº¦ä¸è®¡ç®—çš„ç²¾ç¡®åº¦ã€‚HERALD ä¸ºå¤šè¯­è¨€æ•°å­¦æ¨ç†æä¾›äº†ä¸€ç§å…¼å…·é«˜å‡†ç¡®æ€§ã€ç¨³å®šæ€§å’Œæ¸…æ™°åº¦çš„å®ç”¨è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19093v1",
      "published_date": "2025-12-22 07:02:16 UTC",
      "updated_date": "2025-12-22 07:02:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:15:36.020481+00:00"
    },
    {
      "arxiv_id": "2601.00812v1",
      "title": "Free Energy-Based Modeling of Emotional Dynamics in Video Advertisements",
      "title_zh": "åŸºäºè‡ªç”±èƒ½åŸç†çš„è§†é¢‘å¹¿å‘Šæƒ…ç»ªåŠ¨åŠ›å­¦å»ºæ¨¡",
      "authors": [
        "Takashi Ushio",
        "Kazuhiro Onishi",
        "Hideyoshi Yanagisawa"
      ],
      "abstract": "Emotional responses during advertising video viewing are recognized as essential for understanding media effects because they have influenced attention, memory, and purchase intention. To establish a methodological basis for explainable emotion estimation without relying on external information such as physiological signals or subjective ratings, we have quantified \"pleasantness,\" \"surprise,\" and \"habituation\" solely from scene-level expression features of advertising videos, drawing on the free energy(FE) principle, which has provided a unified account of perception, learning, and behavior. In this framework, Kullback-Leibler divergence (KLD) has captured prediction error, Bayesian surprise (BS) has captured belief updates, and uncertainty (UN) has reflected prior ambiguity, and together they have formed the core components of FE. Using 1,059 15 s food video advertisements, the experiments have shown that KLD has reflected \"pleasantness\" associated with brand presentation, BS has captured \"surprise\" arising from informational complexity, and UN has reflected \"surprise\" driven by uncertainty in element types and spatial arrangements, as well as by the variability and quantity of presented elements. This study also identified three characteristic emotional patterns, namely uncertain stimulus, sustained high emotion, and momentary peak and decay, demonstrating the usefulness of the proposed method. Robustness across nine hyperparameter settings and generalization tests with six types of Japanese advertising videos (three genres and two durations) confirmed that these tendencies remained stable. This work can be extended by integrating a wider range of expression elements and validating the approach through subjective ratings, ultimately guiding the development of technologies that can support the creation of more engaging advertising videos.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºè‡ªç”±èƒ½åŸç†(Free Energy Principle)çš„æƒ…ç»ªåŠ¨æ€å»ºæ¨¡æ–¹æ³•ï¼Œæ—¨åœ¨ä»…é€šè¿‡è§†é¢‘åœºæ™¯ç‰¹å¾é‡åŒ–å¹¿å‘Šè§‚çœ‹è¿‡ç¨‹ä¸­çš„â€œæ„‰æ‚¦æ„Ÿâ€ã€â€œæƒŠè®¶æ„Ÿâ€å’Œâ€œä¹ æƒ¯åŒ–â€ã€‚è¯¥æ¡†æ¶åˆ©ç”¨Kullback-Leibler divergence (KLD)æ•æ‰é¢„æµ‹è¯¯å·®ï¼Œé€šè¿‡Bayesian surprise (BS)åæ˜ ä¿¡å¿µæ›´æ–°ï¼Œå¹¶åˆ©ç”¨Uncertainty (UN)ä½“ç°å…ˆéªŒæ­§ä¹‰ã€‚é’ˆå¯¹1059ä¸ªé£Ÿå“å¹¿å‘Šè§†é¢‘çš„å®éªŒè¡¨æ˜ï¼ŒKLDæœ‰æ•ˆåæ˜ äº†ä¸å“ç‰Œå±•ç¤ºç›¸å…³çš„â€œæ„‰æ‚¦æ„Ÿâ€ï¼Œè€ŒBSå’ŒUNåˆ™åˆ†åˆ«æ•æ‰äº†ç”±ä¿¡æ¯å¤æ‚åº¦å’Œå…ƒç´ å®‰æ’å¼•èµ·çš„â€œæƒŠè®¶æ„Ÿâ€ã€‚ç ”ç©¶è¯†åˆ«å‡ºäº†ä¸ç¡®å®šåˆºæ¿€ã€æŒç»­é«˜æƒ…ç»ªä»¥åŠç¬é—´å³°å€¼è¡°å‡ä¸‰ç§å…¸å‹çš„æƒ…ç»ªç‰¹å¾æ¨¡å¼ï¼Œå¹¶è¯å®äº†è¯¥æ–¹æ³•çš„å®ç”¨æ€§ã€‚é€šè¿‡å¯¹ä¸åŒç±»å‹å’Œæ—¶é•¿çš„å¹¿å‘Šè§†é¢‘è¿›è¡Œæ³›åŒ–æµ‹è¯•ï¼Œå®éªŒéªŒè¯äº†æ¨¡å‹åœ¨ä¸åŒå‚æ•°è®¾å®šä¸‹çš„é²æ£’æ€§ã€‚è¯¥å·¥ä½œä¸ºæ— éœ€ç”Ÿç†ä¿¡å·çš„å¯è§£é‡Šæ€§æƒ…ç»ªä¼°ç®—æä¾›äº†æ–°é€”å¾„ï¼Œå¯¹å¼€å‘èƒ½å¤Ÿè¾…åŠ©åˆ›ä½œæ›´å…·å¸å¼•åŠ›å¹¿å‘Šè§†é¢‘çš„æŠ€æœ¯å…·æœ‰é‡è¦æŒ‡å¯¼æ„ä¹‰ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This article has been accepted for publication in IEEE Access and will be published shortly",
      "pdf_url": "https://arxiv.org/pdf/2601.00812v1",
      "published_date": "2025-12-22 06:51:41 UTC",
      "updated_date": "2025-12-22 06:51:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:15:41.167081+00:00"
    },
    {
      "arxiv_id": "2512.19084v1",
      "title": "$Î³(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics",
      "title_zh": "è®¤çŸ¥æ™ºèƒ½ä½“ä¸­çš„ $Î³(3,4)$ â€œæ³¨æ„åŠ›â€ï¼šåŸºäºæ‰¿è¯ºè®ºè¯­ä¹‰çš„æ— æœ¬ä½“çŸ¥è¯†è¡¨ç¤º",
      "authors": [
        "Mark Burgess"
      ],
      "abstract": "The semantics and dynamics of `attention' are closely related to promise theoretic notions developed for autonomous agents and can thus easily be written down in promise framework. In this way one may establish a bridge between vectorized Machine Learning and Knowledge Graph representations without relying on language models implicitly. Our expectations for knowledge presume a degree of statistical stability, i.e. average invariance under repeated observation, or `trust' in the data. Both learning networks and knowledge graph representations can meaningfully coexist to preserve different aspects of data. While vectorized data are useful for probabilistic estimation, graphs preserve the intentionality of the source even under data fractionation. Using a Semantic Spacetime $Î³(3,4)$ graph, one avoids complex ontologies in favour of classification of features by their roles in semantic processes. The latter favours an approach to reasoning under conditions of uncertainty. Appropriate attention to causal boundary conditions may lead to orders of magnitude compression of data required for such context determination, as required in the contexts of autonomous robotics, defence deployments, and ad hoc emergency services.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è®¤çŸ¥æ™ºèƒ½ä½“ä¸­â€œæ³¨æ„åŠ›â€çš„è¯­ä¹‰ä¸åŠ¨æ€ï¼Œé€šè¿‡Promise Theoryæ¡†æ¶å°†å…¶è½¬åŒ–ä¸ºè‡ªä¸»æ™ºèƒ½ä½“çš„è¡¨è¿°æ–¹å¼ã€‚è¿™ç§æ–¹æ³•åœ¨å‘é‡åŒ–Machine Learningä¸Knowledge Graphè¡¨ç¤ºä¹‹é—´å»ºç«‹äº†æ¡¥æ¢ï¼Œä¸”ä¸ä¾èµ–äºéšå«çš„è¯­è¨€æ¨¡å‹ã€‚ç ”ç©¶æŒ‡å‡ºå‘é‡åŒ–æ•°æ®é€‚ç”¨äºæ¦‚ç‡ä¼°è®¡ï¼Œè€Œå›¾ç»“æ„åœ¨æ•°æ®åˆ†åŒ–æ—¶èƒ½æœ‰æ•ˆä¿ç•™æºæ•°æ®çš„æ„å›¾æ€§ã€‚è®ºæ–‡æå‡ºä½¿ç”¨è¯­ä¹‰æ—¶ç©º $Î³(3,4)$ å›¾(Semantic Spacetime $Î³(3,4)$ graph)ï¼Œé€šè¿‡ç‰¹å¾åœ¨è¯­ä¹‰è¿‡ç¨‹ä¸­çš„è§’è‰²è¿›è¡Œåˆ†ç±»ï¼Œä»è€Œé¿å…äº†å¤æ‚çš„Ontologyå»ºæ¨¡ã€‚è¿™ç§åŸºäºè§’è‰²åˆ†ç±»çš„æ–¹æ³•æœ‰åˆ©äºä¸ç¡®å®šæ¡ä»¶ä¸‹çš„æ¨ç†ï¼Œå¹¶é€šè¿‡å…³æ³¨å› æœè¾¹ç•Œæ¡ä»¶ï¼Œå®ç°äº†ä¸Šä¸‹æ–‡ç¡®å®šæ‰€éœ€çš„æ•°æ®å‹ç¼©ã€‚è¯¥æŠ€æœ¯åœ¨Autonomous Roboticsã€å›½é˜²éƒ¨ç½²å’Œåº”æ€¥æœåŠ¡ç­‰é¢†åŸŸå…·æœ‰æ˜¾è‘—åº”ç”¨æ½œåŠ›ï¼Œèƒ½å°†ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ‰€éœ€çš„æ•°æ®é‡é™ä½æ•°ä¸ªæ•°é‡çº§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19084v1",
      "published_date": "2025-12-22 06:48:53 UTC",
      "updated_date": "2025-12-22 06:48:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:15:37.806629+00:00"
    },
    {
      "arxiv_id": "2512.19081v1",
      "title": "Population-Evolve: a Parallel Sampling and Evolutionary Method for LLM Math Reasoning",
      "title_zh": "Population-Evolveï¼šé¢å‘å¤§è¯­è¨€æ¨¡å‹æ•°å­¦æ¨ç†çš„å¹¶è¡Œé‡‡æ ·ä¸è¿›åŒ–æ–¹æ³•",
      "authors": [
        "Yanzhi Zhang",
        "Yitong Duan",
        "Zhaoxi Zhang",
        "Jiyan He",
        "Shuxin Zheng"
      ],
      "abstract": "Test-time scaling has emerged as a promising direction for enhancing the reasoning capabilities of Large Language Models in last few years. In this work, we propose Population-Evolve, a training-free method inspired by Genetic Algorithms to optimize LLM reasoning. Our approach maintains a dynamic population of candidate solutions for each problem via parallel reasoning. By incorporating an evolve prompt, the LLM self-evolves its population in all iterations. Upon convergence, the final answer is derived via majority voting. Furthermore, we establish a unification framework that interprets existing test-time scaling strategies through the lens of genetic algorithms. Empirical results demonstrate that Population-Evolve achieves superior accuracy with low performance variance and computational efficiency. Our findings highlight the potential of evolutionary strategies to unlock the reasoning power of LLMs during inference.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Population-Evolveï¼Œè¿™æ˜¯ä¸€ç§å— Genetic Algorithms å¯å‘çš„æ— éœ€è®­ç»ƒï¼ˆtraining-freeï¼‰æ–¹æ³•ï¼Œæ—¨åœ¨æ˜¾è‘—å¢å¼º Large Language Models (LLMs) çš„æ•°å­¦æ¨ç†èƒ½åŠ›ã€‚è¯¥æ–¹æ³•é€šè¿‡å¹¶è¡Œæ¨ç†ä¸ºæ¯ä¸ªé—®é¢˜ç»´æŠ¤ä¸€ä¸ªå€™é€‰è§£çš„åŠ¨æ€ç§ç¾¤ï¼ˆpopulationï¼‰ï¼Œå¹¶ç»“åˆ evolve prompt å¼•å¯¼æ¨¡å‹åœ¨è¿­ä»£ä¸­å®ç°ç§ç¾¤çš„è‡ªæˆ‘è¿›åŒ–ã€‚åœ¨è¿­ä»£æ”¶æ•›åï¼Œç³»ç»Ÿé€šè¿‡ majority voting ç¡®å®šæœ€ç»ˆç­”æ¡ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å»ºç«‹äº†ä¸€ä¸ªç»Ÿä¸€æ¡†æ¶ï¼Œä» Genetic Algorithms çš„è§†è§’è§£æå¹¶æ•´åˆäº†ç°æœ‰çš„ test-time scaling ç­–ç•¥ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPopulation-Evolve åœ¨ä¿æŒä½æ€§èƒ½æ–¹å·®å’Œé«˜è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå–å¾—äº†ä¼˜å¼‚çš„æ¨ç†å‡†ç¡®ç‡ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†è¿›åŒ–ç­–ç•¥åœ¨æ¨ç†é˜¶æ®µé‡Šæ”¾ LLMs é€»è¾‘æ½œèƒ½çš„å·¨å¤§ä»·å€¼ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19081v1",
      "published_date": "2025-12-22 06:42:46 UTC",
      "updated_date": "2025-12-22 06:42:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:15:47.342026+00:00"
    },
    {
      "arxiv_id": "2512.19069v1",
      "title": "Can abstract concepts from LLM improve SLM performance?",
      "title_zh": "LLM ä¸­çš„æŠ½è±¡æ¦‚å¿µèƒ½å¦æå‡ SLM çš„æ€§èƒ½ï¼Ÿ",
      "authors": [
        "Siddharth Tandon"
      ],
      "abstract": "Large language models (LLMs) excel at diverse tasks, but their deployment on resource-constrained devices remains challenging. Existing methods like quantization, pruning, and distillation can reduce memory footprint but often demand extensive experimentation and careful infrastructure design. Leveraging existing techniques for extracting high-level concepts (represented as steering vectors) from larger models, we investigate their transferability to smaller language models (SLM) during inference. We demonstrate through extensive experimentation that these concepts can be effectively transferred to smaller models, irrespective of their family (e.g., Phi, Llama, Qwen), leading to performance improvements across a wide range of tasks. Furthermore, we introduce inference-time scaling to enhance performance by dynamically adjusting the steering intensity which has resulted in a 7-15\\% of accuracy improvement for Qwen3-0.6B.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† Large Language Models (LLMs) çš„é«˜å±‚æŠ½è±¡æ¦‚å¿µæ˜¯å¦èƒ½æœ‰æ•ˆæå‡ Small Language Models (SLMs) çš„æ€§èƒ½ã€‚é’ˆå¯¹ä¼ ç»Ÿæ¨¡å‹å‹ç¼©æŠ€æœ¯åœ¨åŸºç¡€è®¾æ–½è®¾è®¡å’Œå®éªŒä¸Šçš„å¤æ‚æ€§ï¼Œä½œè€…æå‡ºåœ¨æ¨ç†é˜¶æ®µå°†å¤§æ¨¡å‹çš„ steering vectors è½¬ç§»è‡³å°æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§æ¦‚å¿µè¿ç§»åœ¨ Phiã€Llama å’Œ Qwen ç­‰ä¸åŒå®¶æ—çš„æ¨¡å‹ä¹‹é—´å‡è¡¨ç°å‡ºæå¼ºçš„é€šç”¨æ€§ï¼Œå¹¶æ˜¾è‘—æå‡äº†å¤šé¡¹ä»»åŠ¡çš„æ‰§è¡Œè¡¨ç°ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¿˜å¼•å…¥äº† inference-time scaling æŠ€æœ¯ï¼Œé€šè¿‡åŠ¨æ€è°ƒèŠ‚ steering å¼ºåº¦æ¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨ç†æ•ˆæœã€‚æœ€ç»ˆæµ‹è¯•æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä½¿ Qwen3-0.6B çš„å‡†ç¡®ç‡æå‡äº† 7-15%ï¼Œä¸ºåœ¨èµ„æºå—é™è®¾å¤‡ä¸Šå¢å¼ºæ¨¡å‹èƒ½åŠ›æä¾›äº†ä¸€ç§é«˜æ•ˆä¸”çµæ´»çš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19069v1",
      "published_date": "2025-12-22 06:17:25 UTC",
      "updated_date": "2025-12-22 06:17:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:15:44.861685+00:00"
    },
    {
      "arxiv_id": "2512.19061v1",
      "title": "Fraud Detection Through Large-Scale Graph Clustering with Heterogeneous Link Transformation",
      "title_zh": "åŸºäºå¼‚æ„é“¾æ¥è½¬æ¢çš„å¤§è§„æ¨¡å›¾èšç±»æ¬ºè¯ˆæ£€æµ‹",
      "authors": [
        "Chi Liu"
      ],
      "abstract": "Collaborative fraud, where multiple fraudulent accounts coordinate to exploit online payment systems, poses significant challenges due to the formation of complex network structures. Traditional detection methods that rely solely on high-confidence identity links suffer from limited coverage, while approaches using all available linkages often result in fragmented graphs with reduced clustering effectiveness. In this paper, we propose a novel graph-based fraud detection framework that addresses the challenge of large-scale heterogeneous graph clustering through a principled link transformation approach. Our method distinguishes between \\emph{hard links} (high-confidence identity relationships such as phone numbers, credit cards, and national IDs) and \\emph{soft links} (behavioral associations including device fingerprints, cookies, and IP addresses). We introduce a graph transformation technique that first identifies connected components via hard links, merges them into super-nodes, and then reconstructs a weighted soft-link graph amenable to efficient embedding and clustering. The transformed graph is processed using LINE (Large-scale Information Network Embedding) for representation learning, followed by HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) for density-based cluster discovery. Experiments on a real-world payment platform dataset demonstrate that our approach achieves significant graph size reduction (from 25 million to 7.7 million nodes), doubles the detection coverage compared to hard-link-only baselines, and maintains high precision across identified fraud clusters. Our framework provides a scalable and practical solution for industrial-scale fraud detection systems.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹åœ¨çº¿æ”¯ä»˜ç³»ç»Ÿä¸­å¤šè´¦æˆ·ååŒä½œæ¡ˆå½¢æˆçš„å¤æ‚ç½‘ç»œç»“æ„ï¼Œæå‡ºäº†ä¸€ç§é€šè¿‡å¤§è§„æ¨¡å¼‚æ„å›¾èšç±»(Heterogeneous Graph Clustering)è¿›è¡Œæ¬ºè¯ˆæ£€æµ‹çš„æ–°æ¡†æ¶ã€‚è¯¥æ–¹æ³•å°†å…³ç³»åŒºåˆ†ä¸ºé«˜ç½®ä¿¡åº¦çš„ç¡¬é“¾æ¥(Hard Linksï¼Œå¦‚ç”µè¯å·ç å’Œèº«ä»½è¯å·)å’Œè¡Œä¸ºå…³è”çš„è½¯é“¾æ¥(Soft Linksï¼Œå¦‚è®¾å¤‡æŒ‡çº¹å’ŒIPåœ°å€)ã€‚å…¶æ ¸å¿ƒæŠ€æœ¯åœ¨äºé“¾è·¯è½¬æ¢(Link Transformation)ï¼Œé¦–å…ˆåˆ©ç”¨ç¡¬é“¾æ¥è¯†åˆ«è¿é€šåˆ†é‡å¹¶å°†å…¶åˆå¹¶ä¸ºè¶…èŠ‚ç‚¹(Super-nodes)ï¼Œéšåé‡æ„å‡ºä¸€ä¸ªä¾¿äºé«˜æ•ˆåµŒå…¥å’Œèšç±»çš„åŠ æƒè½¯é“¾æ¥å›¾ã€‚è¯¥æ¡†æ¶è¿›ä¸€æ­¥é‡‡ç”¨LINE (Large-scale Information Network Embedding)è¿›è¡Œè¡¨ç¤ºå­¦ä¹ ï¼Œå¹¶ç»“åˆHDBSCANè¿›è¡ŒåŸºäºå¯†åº¦çš„èšç±»å‘ç°ã€‚åœ¨çœŸå®æ”¯ä»˜å¹³å°æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æˆåŠŸå°†å›¾è§„æ¨¡ä»2500ä¸‡èŠ‚ç‚¹å¤§å¹…ç¼©å‡è‡³770ä¸‡ï¼Œä¸”æ¬ºè¯ˆæ£€æµ‹è¦†ç›–ç‡è¾ƒä»…ä½¿ç”¨ç¡¬é“¾æ¥çš„åŸºçº¿æ¨¡å‹æå‡äº†ä¸€å€ã€‚è¯¥ç ”ç©¶åœ¨ä¿æŒé«˜ç²¾ç¡®åº¦çš„åŒæ—¶ï¼Œä¸ºå·¥ä¸šçº§å¤§è§„æ¨¡æ¬ºè¯ˆæ£€æµ‹ç³»ç»Ÿæä¾›äº†ä¸€ç§å…¼å…·å¯æ‰©å±•æ€§ä¸å®ç”¨æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "13 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.19061v1",
      "published_date": "2025-12-22 05:59:13 UTC",
      "updated_date": "2025-12-22 05:59:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:15:54.898377+00:00"
    },
    {
      "arxiv_id": "2512.19769v1",
      "title": "A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows",
      "title_zh": "æ„å»ºä¸ç¼–æ’å¤§è¯­è¨€æ¨¡å‹é©±åŠ¨æ™ºèƒ½ä½“å·¥ä½œæµçš„å£°æ˜å¼è¯­è¨€",
      "authors": [
        "Ivan Daunis"
      ],
      "abstract": "Building deployment-ready LLM agents requires complex orchestration of tools, data sources, and control flow logic, yet existing systems tightly couple agent logic to specific programming languages and deployment models. We present a declarative system that separates agent workflow specification from implementation, enabling the same pipeline definition to execute across multiple backend languages (Java, Python, Go) and deployment environments (cloud-native, on-premises).\n  Our key insight is that most agent workflows consist of common patterns -- data serialization, filtering, RAG retrieval, API orchestration -- that can be expressed through a unified DSL rather than imperative code. This approach transforms agent development from application programming to configuration, where adding new tools or fine-tuning agent behaviors requires only pipeline specification changes, not code deployment. Our system natively supports A/B testing of agent strategies, allowing multiple pipeline variants to run on the same backend infrastructure with automatic metric collection and comparison.\n  We evaluate our approach on real-world e-commerce workflows at PayPal, processing millions of daily interactions. Our results demonstrate 60% reduction in development time, and 3x improvement in deployment velocity compared to imperative implementations. The language's declarative approach enables non-engineers to modify agent behaviors safely, while maintaining sub-100ms orchestration overhead. We show that complex workflows involving product search, personalization, and cart management can be expressed in under 50 lines of DSL compared to 500+ lines of imperative code.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§å£°æ˜å¼è¯­è¨€(Declarative Language)ï¼Œç”¨äºæ„å»ºå’Œç¼–æ’ç”±å¤§è¯­è¨€æ¨¡å‹(LLM)é©±åŠ¨çš„æ™ºèƒ½ä½“å·¥ä½œæµï¼Œè§£å†³äº†ç°æœ‰ç³»ç»Ÿä¸­æ™ºèƒ½ä½“é€»è¾‘ä¸ç‰¹å®šç¼–ç¨‹è¯­è¨€åŠéƒ¨ç½²æ¨¡å‹ç´§è€¦åˆçš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿå°†å·¥ä½œæµè§„èŒƒä¸å…·ä½“å®ç°è§£è€¦ï¼Œä½¿åŒä¸€æµæ°´çº¿å®šä¹‰èƒ½å¤Ÿåœ¨Javaã€Pythonå’ŒGoç­‰å¤šç§åç«¯è¯­è¨€ä»¥åŠäº‘åŸç”Ÿæˆ–æœ¬åœ°ç¯å¢ƒä¸­æ‰§è¡Œã€‚é€šè¿‡ç»Ÿä¸€çš„é¢†åŸŸç‰¹å®šè¯­è¨€(DSL)æ¥è¡¨è¾¾æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å’ŒAPIç¼–æ’ç­‰é€šç”¨æ¨¡å¼ï¼Œè¯¥æ–¹æ³•å°†æ™ºèƒ½ä½“å¼€å‘ä»ä¼ ç»Ÿçš„å‘½ä»¤å¼ç¼–ç¨‹è½¬å˜ä¸ºé…ç½®æ¨¡å¼ï¼Œæ˜¾è‘—é™ä½äº†å¼€å‘é—¨æ§›ã€‚ç³»ç»ŸåŸç”Ÿæ”¯æŒæ™ºèƒ½ä½“ç­–ç•¥çš„A/Bæµ‹è¯•ï¼Œå…è®¸åœ¨åŒä¸€åŸºç¡€è®¾æ–½ä¸Šè¿è¡Œå¤šä¸ªæµæ°´çº¿å˜ä½“å¹¶è‡ªåŠ¨æ”¶é›†æŒ‡æ ‡ã€‚åœ¨PayPalçœŸå®ç”µå•†åœºæ™¯çš„è¯„ä¼°ä¸­ï¼Œè¯¥æ–¹æ¡ˆå°†å¼€å‘æ—¶é—´ç¼©çŸ­äº†60%ï¼Œéƒ¨ç½²é€Ÿåº¦æå‡äº†3å€ï¼Œä¸”ç¼–æ’å¼€é”€ä½äº100æ¯«ç§’ã€‚å®éªŒè¡¨æ˜ï¼Œå¤æ‚çš„ç”µå•†å·¥ä½œæµä»…éœ€ä¸åˆ°50è¡ŒDSLå³å¯è¡¨è¾¾ï¼Œè¿œä½äºä¼ ç»Ÿå‘½ä»¤å¼ä»£ç æ‰€éœ€çš„500å¤šè¡Œï¼Œä¸ºéå·¥ç¨‹å¸ˆå®‰å…¨ä¿®æ”¹æ™ºèƒ½ä½“è¡Œä¸ºæä¾›äº†å¯èƒ½ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19769v1",
      "published_date": "2025-12-22 05:03:37 UTC",
      "updated_date": "2025-12-22 05:03:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:15:51.391965+00:00"
    },
    {
      "arxiv_id": "2512.19027v1",
      "title": "Recontextualization Mitigates Specification Gaming without Modifying the Specification",
      "title_zh": "é‡æ„è¯­å¢ƒï¼šåœ¨ä¸ä¿®æ”¹è®¾å®šçš„æƒ…å†µä¸‹ç¼“è§£æŒ‡æ ‡åšå¼ˆ",
      "authors": [
        "Ariana Azarbal",
        "Victor Gillioz",
        "Vladimir Ivanov",
        "Bryce Woodworth",
        "Jacob Drori",
        "Nevan Wichers",
        "Aram Ebtekar",
        "Alex Cloud",
        "Alexander Matt Turner"
      ],
      "abstract": "Developers often struggle to specify correct training labels and rewards. Perhaps they don't need to. We propose recontextualization, which reduces how often language models \"game\" training signals, performing misbehaviors those signals mistakenly reinforce. We show recontextualization prevents models from learning to 1) prioritize evaluation metrics over chat response quality; 2) special-case code to pass incorrect tests; 3) lie to users; and 4) become sycophantic. Our method works by generating completions from prompts discouraging misbehavior and then recontextualizing them as though they were in response to prompts permitting misbehavior. Recontextualization trains language models to resist misbehavior even when instructions permit it. This mitigates the reinforcement of misbehavior from misspecified training signals, reducing specification gaming without improving the supervision signal.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† recontextualization æ–¹æ³•ï¼Œæ—¨åœ¨ä¸ç›´æ¥ä¿®æ”¹ç›‘ç£ä¿¡å·çš„æƒ…å†µä¸‹ï¼Œç¼“è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„ specification gaming é—®é¢˜ã€‚è¯¥æ–¹æ³•é€šè¿‡ä»é¼“åŠ±æ­£ç¡®è¡Œä¸ºçš„æç¤ºè¯ä¸­ç”Ÿæˆè¡¥å…¨å†…å®¹ï¼Œå¹¶å°†å…¶ä½œä¸ºå¯¹å…è®¸è¯¯å¯¼è¡Œä¸ºçš„æç¤ºè¯çš„å“åº”è¿›è¡Œå†è¯­å¢ƒåŒ–è®­ç»ƒï¼Œä»è€Œå¢å¼ºæ¨¡å‹å¯¹é”™è¯¯ä¿¡å·çš„æŠµæŠ—åŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œrecontextualization èƒ½æœ‰æ•ˆé˜²æ­¢æ¨¡å‹å‡ºç°ä¼˜å…ˆè€ƒè™‘è¯„ä¼°æŒ‡æ ‡ã€ç¼–å†™ç‰¹æ®Šä»£ç ä»¥é€šè¿‡é”™è¯¯æµ‹è¯•ã€æ’’è°ä»¥åŠ sycophancy ç­‰è´Ÿé¢å€¾å‘ã€‚å³ä½¿åœ¨æŒ‡ä»¤æ˜ç¡®å…è®¸è¯¯å¯¼è¡Œä¸ºçš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ä¹Ÿèƒ½é€šè¿‡å‡å°‘é”™è¯¯è®­ç»ƒä¿¡å·å¯¹ä¸å½“è¡Œä¸ºçš„å¼ºåŒ–ï¼Œåœ¨ä¸æé«˜ç›‘ç£ä¿¡å·è´¨é‡çš„å‰æä¸‹æ˜¾è‘—é™ä½æŒ‡æ ‡åšå¼ˆé£é™©ã€‚è¿™ä¸€å‘ç°ä¸ºè§£å†³æ¨¡å‹è®­ç»ƒä¸­éš¾ä»¥å®šä¹‰å®Œç¾æ ‡ç­¾å’Œå¥–åŠ±å‡½æ•°çš„éš¾é¢˜æä¾›äº†ä¸€ç§é«˜æ•ˆçš„è¡¥æ•‘ç­–ç•¥ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "57 pages, 41 figures",
      "pdf_url": "https://arxiv.org/pdf/2512.19027v1",
      "published_date": "2025-12-22 04:53:40 UTC",
      "updated_date": "2025-12-22 04:53:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:15:57.062761+00:00"
    },
    {
      "arxiv_id": "2512.19026v1",
      "title": "Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation",
      "title_zh": "Finer-Personalization Rankï¼šé€šè¿‡ç»†ç²’åº¦æ£€ç´¢è¯„ä¼°ä¸ªæ€§åŒ–ç”Ÿæˆçš„èº«ä»½ä¿ç•™",
      "authors": [
        "Connor Kilrain",
        "David Carlyn",
        "Julia Chae",
        "Sara Beery",
        "Wei-Lun Chao",
        "Jianyang Gu"
      ],
      "abstract": "The rise of personalized generative models raises a central question: how should we evaluate identity preservation? Given a reference image (e.g., one's pet), we expect the generated image to retain precise details attached to the subject's identity. However, current generative evaluation metrics emphasize the overall semantic similarity between the reference and the output, and overlook these fine-grained discriminative details. We introduce Finer-Personalization Rank, an evaluation protocol tailored to identity preservation. Instead of pairwise similarity, Finer-Personalization Rank adopts a ranking view: it treats each generated image as a query against an identity-labeled gallery consisting of visually similar real images. Retrieval metrics (e.g., mean average precision) measure performance, where higher scores indicate that identity-specific details (e.g., a distinctive head spot) are preserved. We assess identity at multiple granularities -- from fine-grained categories (e.g., bird species, car models) to individual instances (e.g., re-identification). Across CUB, Stanford Cars, and animal Re-ID benchmarks, Finer-Personalization Rank more faithfully reflects identity retention than semantic-only metrics and reveals substantial identity drift in several popular personalization methods. These results position the gallery-based protocol as a principled and practical evaluation for personalized generation.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¸ªæ€§åŒ–ç”Ÿæˆæ¨¡å‹ä¸­ identity preservation çš„è¯„ä¼°éš¾é¢˜ï¼ŒæŒ‡å‡ºç›®å‰çš„æŒ‡æ ‡å¾€å¾€è¿‡åº¦ä¾èµ–æ•´ä½“è¯­ä¹‰ç›¸ä¼¼åº¦è€Œå¿½è§†äº†ç»†ç²’åº¦çš„åˆ¤åˆ«æ€§ç»†èŠ‚ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Finer-Personalization Rank è¯„ä¼°åè®®ï¼Œå°†ç”Ÿæˆå›¾åƒä½œä¸ºæŸ¥è¯¢ï¼Œåœ¨åŒ…å«è§†è§‰ç›¸ä¼¼çœŸå®å›¾åƒçš„ identity-labeled gallery ä¸­è¿›è¡Œæ£€ç´¢æ’åºã€‚è¯¥åè®®åˆ©ç”¨ mean average precision ç­‰æŒ‡æ ‡è¡¡é‡æ€§èƒ½ï¼Œå¾—åˆ†è¶Šé«˜è¡¨æ˜æ¨¡å‹å¯¹ç‰¹å®šèº«ä»½ç»†èŠ‚çš„ä¿ç•™è¶Šå®Œæ•´ã€‚åœ¨ CUBã€Stanford Cars å’ŒåŠ¨ç‰© Re-ID åŸºå‡†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒFiner-Personalization Rank æ¯”ä¼ ç»Ÿè¯­ä¹‰æŒ‡æ ‡æ›´èƒ½çœŸå®åæ˜ èº«ä»½ä¿ç•™æƒ…å†µã€‚ç ”ç©¶è¿˜æ­ç¤ºäº†å¤šç§æµè¡Œä¸ªæ€§åŒ–ç”Ÿæˆæ–¹æ³•ä¸­å­˜åœ¨çš„ identity drift é—®é¢˜ï¼Œè¯æ˜äº†è¯¥æ£€ç´¢åè®®åœ¨ä¸ªæ€§åŒ–ç”Ÿæˆè¯„ä¼°ä¸­çš„ç§‘å­¦æ€§å’Œå®ç”¨æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19026v1",
      "published_date": "2025-12-22 04:53:40 UTC",
      "updated_date": "2025-12-22 04:53:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:16:15.097595+00:00"
    },
    {
      "arxiv_id": "2512.19025v2",
      "title": "The Erasure Illusion: Stress-Testing the Generalization of LLM Forgetting Evaluation",
      "title_zh": "æ“¦é™¤å¹»è±¡ï¼šé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹é—å¿˜è¯„ä¼°æ³›åŒ–èƒ½åŠ›çš„å‹åŠ›æµ‹è¯•",
      "authors": [
        "Hengrui Jia",
        "Taoran Li",
        "Jonas Guan",
        "Varun Chandrasekaran"
      ],
      "abstract": "Machine unlearning aims to remove specific data influences from trained models, a capability essential for adhering to copyright laws and ensuring AI safety. Current unlearning metrics typically measure success by monitoring the model's performance degradation on the specific unlearning dataset ($D_u$). We argue that for Large Language Models (LLMs), this evaluation paradigm is insufficient and potentially misleading. Many real-world uses of unlearning--motivated by copyright or safety--implicitly target not only verbatim content in $D_u$, but also behaviors influenced by the broader generalizations the model derived from it. We demonstrate that LLMs can pass standard unlearning evaluation and appear to have \"forgotten\" the target knowledge, while simultaneously retaining strong capabilities on content that is semantically adjacent to $D_u$. This phenomenon indicates that erasing exact sentences does not necessarily equate to removing the underlying knowledge. To address this gap, we propose Proximal Surrogate Generation (PSG), an automated stress-testing framework that generates a surrogate dataset, $\\tilde{D}_u$. This surrogate set is constructed to be semantically derived from $D_u$ yet sufficiently distinct in embedding space. By comparing unlearning metric scores between $D_u$ and $\\tilde{D}_u$, we can stress-test the reliability of the metric itself. Our extensive evaluation across three LLM families (Llama-3-8B, Qwen2.5-7B, and Zephyr-7B-$Î²$), three distinct datasets, and seven standard metrics reveals widespread inconsistencies. We find that current metrics frequently overestimate unlearning success, failing to detect retained knowledge exposed by our stress-test datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­çš„æœºå™¨é—å¿˜(Machine Unlearning)è¯„ä¼°é—®é¢˜ï¼ŒæŒ‡å‡ºå½“å‰çš„è¯„ä¼°æŒ‡æ ‡å¾€å¾€ä»…å…³æ³¨ç‰¹å®šé—å¿˜æ•°æ®é›†($D_u$)çš„æ€§èƒ½ä¸‹é™ï¼Œè€Œå¿½ç•¥äº†æ¨¡å‹å¯èƒ½ä¿ç•™äº†ä¸é—å¿˜ç›®æ ‡è¯­ä¹‰ç›¸å…³çš„çŸ¥è¯†ï¼Œå³â€œæ“¦é™¤å¹»è§‰â€(Erasure Illusion)ã€‚ä½œè€…è®¤ä¸ºç®€å•çš„æ“¦é™¤ç²¾ç¡®è¯­å¥å¹¶ä¸ç­‰åŒäºç§»é™¤åº•å±‚çš„é€šç”¨çŸ¥è¯†ï¼Œè¿™åœ¨ç‰ˆæƒä¿æŠ¤å’ŒAIå®‰å…¨é¢†åŸŸå…·æœ‰é‡å¤§éšæ‚£ã€‚ä¸ºäº†è§£å†³è¿™ä¸€è¯„ä¼°æ¼æ´ï¼Œç ”ç©¶è€…æå‡ºäº†é‚»è¿‘ä»£ç†ç”Ÿæˆ(Proximal Surrogate Generation, PSG)æ¡†æ¶ï¼Œè¿™æ˜¯ä¸€ç§è‡ªåŠ¨åŒ–çš„å‹åŠ›æµ‹è¯•å·¥å…·ã€‚è¯¥æ¡†æ¶é€šè¿‡ç”Ÿæˆè¯­ä¹‰æºè‡ª$D_u$ä½†åœ¨åµŒå…¥ç©ºé—´ä¸­æ˜¾è‘—ä¸åŒçš„ä»£ç†æ•°æ®é›†($\\tilde{D}_u$)ï¼Œæ¥è¡¡é‡ç°æœ‰é—å¿˜æŒ‡æ ‡çš„å¯é æ€§ã€‚é€šè¿‡åœ¨Llama-3-8Bã€Qwen2.5-7Bå’ŒZephyr-7B-Î²ç­‰æ¨¡å‹åŠä¸ƒä¸ªæ ‡å‡†æŒ‡æ ‡ä¸Šè¿›è¡Œçš„å¹¿æ³›è¯„ä¼°ï¼Œè¯¥ç ”ç©¶æ­ç¤ºäº†æ™®éå­˜åœ¨çš„è¯„ä¼°ä¸ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç›®å‰çš„è¯„ä¼°æŒ‡æ ‡ç»å¸¸é«˜ä¼°é—å¿˜æ•ˆæœï¼Œæ— æ³•æ£€æµ‹åˆ°ç”±å‹åŠ›æµ‹è¯•æ•°æ®é›†æ‰€æš´éœ²å‡ºçš„æ®‹ç•™çŸ¥è¯†ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19025v2",
      "published_date": "2025-12-22 04:42:41 UTC",
      "updated_date": "2025-12-23 03:34:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:16:22.518508+00:00"
    },
    {
      "arxiv_id": "2512.19024v1",
      "title": "IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments",
      "title_zh": "IndoorUAVï¼šè¿ç»­å®¤å†…ç¯å¢ƒä¸‹çš„è§†è§‰-è¯­è¨€æ— äººæœºå¯¼èˆªåŸºå‡†æµ‹è¯•",
      "authors": [
        "Xu Liu",
        "Yu Liu",
        "Hanshuo Qiu",
        "Yang Qirong",
        "Zhouhui Lian"
      ],
      "abstract": "Vision-Language Navigation (VLN) enables agents to navigate in complex environments by following natural language instructions grounded in visual observations. Although most existing work has focused on ground-based robots or outdoor Unmanned Aerial Vehicles (UAVs), indoor UAV-based VLN remains underexplored, despite its relevance to real-world applications such as inspection, delivery, and search-and-rescue in confined spaces. To bridge this gap, we introduce \\textbf{IndoorUAV}, a novel benchmark and method specifically tailored for VLN with indoor UAVs. We begin by curating over 1,000 diverse and structurally rich 3D indoor scenes from the Habitat simulator. Within these environments, we simulate realistic UAV flight dynamics to collect diverse 3D navigation trajectories manually, further enriched through data augmentation techniques. Furthermore, we design an automated annotation pipeline to generate natural language instructions of varying granularity for each trajectory. This process yields over 16,000 high-quality trajectories, comprising the \\textbf{IndoorUAV-VLN} subset, which focuses on long-horizon VLN. To support short-horizon planning, we segment long trajectories into sub-trajectories by selecting semantically salient keyframes and regenerating concise instructions, forming the \\textbf{IndoorUAV-VLA} subset. Finally, we introduce \\textbf{IndoorUAV-Agent}, a novel navigation model designed for our benchmark, leveraging task decomposition and multimodal reasoning. We hope IndoorUAV serves as a valuable resource to advance research on vision-language embodied AI in the indoor aerial navigation domain.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†IndoorUAVï¼Œä¸€ä¸ªä¸“é—¨é’ˆå¯¹å®¤å†…æ— äººæœºè§†è§‰è¯­è¨€å¯¼èˆª(Vision-Language Navigation, VLN)è®¾è®¡çš„åŸºå‡†æµ‹è¯•å’Œæ–¹æ³•ã€‚ç ”ç©¶è€…åˆ©ç”¨Habitatæ¨¡æ‹Ÿå™¨æ„å»ºäº†1000å¤šä¸ªç»“æ„å¤æ‚çš„3Då®¤å†…åœºæ™¯ï¼Œå¹¶ç»“åˆçœŸå®çš„æ— äººæœºé£è¡ŒåŠ¨åŠ›å­¦é‡‡é›†äº†å¤šæ ·åŒ–çš„3Då¯¼èˆªè½¨è¿¹ã€‚é€šè¿‡è‡ªåŠ¨åŒ–çš„æ ‡æ³¨æµæ°´çº¿ï¼Œè¯¥ç ”ç©¶ç”Ÿæˆäº†è¶…è¿‡1.6ä¸‡æ¡é…æœ‰è‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„é«˜è´¨é‡è½¨è¿¹ï¼Œå½¢æˆäº†ä¸“æ³¨äºé•¿ç¨‹å¯¼èˆªçš„IndoorUAV-VLNå­é›†ã€‚é’ˆå¯¹çŸ­ç¨‹è§„åˆ’éœ€æ±‚ï¼Œç ”ç©¶é€šè¿‡åˆ†å‰²é•¿è½¨è¿¹å¹¶æå–è¯­ä¹‰å…³é”®å¸§æ„å»ºäº†IndoorUAV-VLAå­é›†ã€‚æ­¤å¤–ï¼Œä½œè€…è¿˜æå‡ºäº†åŸºäºä»»åŠ¡åˆ†è§£å’Œå¤šæ¨¡æ€æ¨ç†çš„IndoorUAV-Agentå¯¼èˆªæ¨¡å‹ã€‚è¯¥å·¥ä½œå¡«è¡¥äº†å®¤å†…æ— äººæœºåœ¨è¿ç»­ç¯å¢ƒä¸­VLNç ”ç©¶çš„ç©ºç™½ï¼Œä¸ºå…·èº«æ™ºèƒ½(Embodied AI)åœ¨å®¤å†…èˆªç©ºé¢†åŸŸçš„ç ”ç©¶æä¾›äº†å®è´µçš„èµ„æºå’ŒåŸºå‡†ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19024v1",
      "published_date": "2025-12-22 04:42:35 UTC",
      "updated_date": "2025-12-22 04:42:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:16:21.905276+00:00"
    },
    {
      "arxiv_id": "2512.19011v2",
      "title": "PromptScreen: Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline",
      "title_zh": "PromptScreenï¼šåŸºäºå¤šé˜¶æ®µæµæ°´çº¿è¯­ä¹‰çº¿æ€§åˆ†ç±»çš„é«˜æ•ˆè¶Šç‹±é˜²å¾¡æ–¹æ¡ˆ",
      "authors": [
        "Akshaj Prashanth Rao",
        "Advait Singh",
        "Saumya Kumaar Saksena",
        "Dhruv Kumar"
      ],
      "abstract": "Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present PromptScreen, an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead.\n  Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time-to-completion from approximately 450 s to 47 s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators.\n  Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PromptScreenï¼Œä¸€ç§ç”¨äºå‡è½»å¤§è¯­è¨€æ¨¡å‹(LLM)ç³»ç»Ÿé¢ä¸´çš„æç¤ºæ³¨å…¥(Prompt injection)å’Œè¶Šç‹±(Jailbreaking)æ”»å‡»çš„é«˜æ•ˆé˜²å¾¡æ¶æ„ã€‚è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªåŸºäºæ–‡æœ¬å½’ä¸€åŒ–ã€TF-IDFè¡¨ç¤ºå’Œçº¿æ€§SVMåˆ†ç±»å™¨(Linear SVM classifier)çš„è½»é‡çº§è¯­ä¹‰è¿‡æ»¤å™¨ã€‚PromptScreené€šè¿‡å¤šé˜¶æ®µæµæ°´çº¿(Multi-stage pipeline)æ•´åˆäº†äº’è¡¥çš„æ£€æµ‹ä¸ç¼“è§£æœºåˆ¶ï¼Œæ—¨åœ¨ä½å»¶è¿Ÿç¯å¢ƒä¸‹æä¾›å¼ºé²æ£’æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥SVMé…ç½®å°†æ£€æµ‹å‡†ç¡®ç‡ä»35.1%æå‡è‡³93.4%ï¼Œå¹¶åœ¨ç•™å‡ºæ•°æ®é›†ä¸Šå®ç°äº†96.5%çš„ç‰¹å¼‚æ€§ã€‚åœ¨å¤„ç†æ•ˆç‡æ–¹é¢ï¼Œè¯¥æ–¹æ¡ˆå°†å¹³å‡å®Œæˆæ—¶é—´ä»çº¦450ç§’ç¼©çŸ­è‡³47ç§’ï¼Œå…¶å»¶è¿Ÿæ¯”ShieldGemmaä½10å€ä»¥ä¸Šã€‚é€šè¿‡å¯¹è¶…è¿‡30,000ä¸ªæ ‡è®°æç¤ºçš„ç³»ç»Ÿè¯„ä¼°è¯æ˜ï¼Œè¿™ç§åˆ†é˜¶æ®µä¸”èµ„æºé«˜æ•ˆçš„é˜²å¾¡è®¾è®¡èƒ½æœ‰æ•ˆä¿éšœLLMé©±åŠ¨åº”ç”¨ç¨‹åºçš„å®‰å…¨ï¼Œå…‹æœäº†ç°æœ‰æ¨¡å‹åŸºå‡†è°ƒèŠ‚å™¨(Model-based moderators)çš„æ€§èƒ½ç“¶é¢ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "Under Review",
      "pdf_url": "https://arxiv.org/pdf/2512.19011v2",
      "published_date": "2025-12-22 04:00:35 UTC",
      "updated_date": "2026-01-09 04:05:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:16:33.278372+00:00"
    },
    {
      "arxiv_id": "2512.22211v1",
      "title": "With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems",
      "title_zh": "èƒ½åŠ›è¶Šå¤§ï¼Œè´£ä»»è¶Šå¤§ï¼šé¢å‘ä»£ç†å¼äººå·¥æ™ºèƒ½ç³»ç»Ÿæ²»ç†çš„ä»£ç†é£é™©ä¸èƒ½åŠ›æ¡†æ¶",
      "authors": [
        "Shaun Khoo",
        "Jessica Foo",
        "Roy Ka-Wei Lee"
      ],
      "abstract": "Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \\& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \\href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»£ç†å¼äººå·¥æ™ºèƒ½(Agentic AI)ç³»ç»Ÿå› å…·å¤‡è‡ªä¸»è¡ŒåŠ¨èƒ½åŠ›è€Œå¸¦æ¥çš„æ–°é£é™©ä¸æ²»ç†æŒ‘æˆ˜ï¼Œæå‡ºäº†ä»£ç†é£é™©ä¸èƒ½åŠ›æ¡†æ¶(Agentic Risk & Capability Framework, ARC Framework)ã€‚è¯¥æ¡†æ¶é€šè¿‡ä¸€ç§æ–°é¢–çš„ä»¥èƒ½åŠ›ä¸ºä¸­å¿ƒ(Capability-centric)çš„è§†è§’æ¥åˆ†æå¤æ‚çš„Agentic AIç³»ç»Ÿï¼Œå¹¶è¯†åˆ«å‡ºç»„ä»¶ã€è®¾è®¡å’Œèƒ½åŠ›ä¸‰ä¸ªæ ¸å¿ƒé£é™©æ¥æºã€‚ARC Framework å»ºç«‹äº†é£é™©æ¥æºã€å…·ä½“é£é™©å®ä¾‹ä¸ç›¸åº”æŠ€æœ¯æ§åˆ¶æ‰‹æ®µ(Technical Controls)ä¹‹é—´çš„ç›´æ¥è”ç³»ï¼Œä¸ºç»„ç»‡æä¾›äº†ç»“æ„åŒ–ä¸”å®ç”¨çš„æ²»ç†å®æ–½è·¯å¾„ã€‚è¿™ç§ç¨³å¥ä¸”å¯æ‰©å±•çš„æ–¹æ³•è®ºæ—¨åœ¨ååŠ©ç»„ç»‡åœ¨æ¨åŠ¨åˆ›æ–°çš„åŒæ—¶ï¼Œç¡®ä¿Agentic AIç³»ç»Ÿèƒ½å¤Ÿå®ç°å®‰å…¨ã€å¯é ä¸”è´Ÿè´£ä»»çš„éƒ¨ç½²ã€‚ç›®å‰è¯¥æ¡†æ¶å·²å¼€æºå‘å¸ƒï¼Œä¸ºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œè¯„ä¼°å¹¶ç¼“è§£è‡ªä¸»AIç³»ç»Ÿçš„åŠ¨æ€é£é™©æä¾›äº†é‡è¦çš„æŠ€æœ¯æ²»ç†å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at IASEAI 2026 (Main Track) and AAAI 2026 3rd International AI Governance Workshop",
      "pdf_url": "https://arxiv.org/pdf/2512.22211v1",
      "published_date": "2025-12-22 03:51:34 UTC",
      "updated_date": "2025-12-22 03:51:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:16:39.935788+00:00"
    },
    {
      "arxiv_id": "2512.19007v1",
      "title": "The 6th International Verification of Neural Networks Competition (VNN-COMP 2025): Summary and Results",
      "title_zh": "ç¬¬å…­å±Šå›½é™…ç¥ç»ç½‘ç»œéªŒè¯ç«èµ›ï¼ˆVNN-COMP 2025ï¼‰ï¼šæ€»ç»“ä¸ç»“æœ",
      "authors": [
        "Konstantin Kaulen",
        "Tobias Ladner",
        "Stanley Bak",
        "Christopher Brix",
        "Hai Duong",
        "Thomas Flinkow",
        "Taylor T. Johnson",
        "Lukas Koller",
        "Edoardo Manino",
        "ThanhVu H Nguyen",
        "Haoze Wu"
      ],
      "abstract": "This report summarizes the 6th International Verification of Neural Networks Competition (VNN-COMP 2025), held as a part of the 8th International Symposium on AI Verification (SAIV), that was collocated with the 37th International Conference on Computer-Aided Verification (CAV). VNN-COMP is held annually to facilitate the fair and objective comparison of state-of-the-art neural network verification tools, encourage the standardization of tool interfaces, and bring together the neural network verification community. To this end, standardized formats for networks (ONNX) and specification (VNN-LIB) were defined, tools were evaluated on equal-cost hardware (using an automatic evaluation pipeline based on AWS instances), and tool parameters were chosen by the participants before the final test sets were made public. In the 2025 iteration, 8 teams participated on a diverse set of 16 regular and 9 extended benchmarks. This report summarizes the rules, benchmarks, participating tools, results, and lessons learned from this iteration of this competition.",
      "tldr_zh": "è¯¥æŠ¥å‘Šæ€»ç»“äº†ç¬¬å…­å±Šå›½é™…ç¥ç»ç½‘ç»œéªŒè¯ç«èµ›(VNN-COMP 2025)çš„è§„åˆ™ã€åŸºå‡†æµ‹è¯•å’Œè¯„ä¼°ç»“æœï¼Œè¯¥ç«èµ›æ—¨åœ¨ä¿ƒè¿›ç¥ç»ç½‘ç»œéªŒè¯(Neural Network Verification)å·¥å…·çš„å…¬å¹³æ¯”è¾ƒä¸æ¥å£æ ‡å‡†åŒ–ã€‚è¯„ä¼°è¿‡ç¨‹é‡‡ç”¨äº†ç»Ÿä¸€çš„ONNXç½‘ç»œæ ¼å¼å’ŒVNN-LIBè§„èŒƒï¼Œå¹¶åˆ©ç”¨åŸºäºAWSå®ä¾‹çš„è‡ªåŠ¨æµæ°´çº¿åœ¨ç­‰æ•ˆæˆæœ¬ç¡¬ä»¶ä¸Šè¿›è¡Œæµ‹è¯•ã€‚æœ¬å±Šç«èµ›å…±æœ‰8æ”¯å›¢é˜Ÿå‚åŠ ï¼Œé’ˆå¯¹åŒ…å«16ä¸ªå¸¸è§„åŸºå‡†æµ‹è¯•(Benchmarks)å’Œ9ä¸ªæ‰©å±•åŸºå‡†æµ‹è¯•çš„å¤šå…ƒåŒ–æ•°æ®é›†å±•å¼€ç«äº‰ã€‚æŠ¥å‘Šä¸ä»…è®°å½•äº†å„å‚èµ›å·¥å…·çš„æ€§èƒ½è¡¨ç°ï¼Œè¿˜æ·±å…¥åˆ†æäº†è¯¥é¢†åŸŸæœ€æ–°çš„æŠ€æœ¯è¿›å±•ã€‚é€šè¿‡å¯¹æ¯”èµ›ç»“æœçš„è¯¦ç»†æ¢³ç†ï¼Œç ”ç©¶è€…ä»¬æç‚¼äº†å…³äºç¥ç»ç½‘ç»œéªŒè¯æŠ€æœ¯çš„ç»éªŒæ•™è®­ï¼Œä¸ºæœªæ¥å¼€å‘æ›´é«˜æ•ˆã€æ ‡å‡†åŒ–çš„éªŒè¯å·¥å…·æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Report on the results of VNN-COMP 2025. arXiv admin note: substantial text overlap with arXiv:2412.19985, arXiv:2312.16760, arXiv:2212.10376",
      "pdf_url": "https://arxiv.org/pdf/2512.19007v1",
      "published_date": "2025-12-22 03:48:31 UTC",
      "updated_date": "2025-12-22 03:48:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:16:37.487419+00:00"
    },
    {
      "arxiv_id": "2512.22210v1",
      "title": "Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh",
      "title_zh": "è¿ˆå‘å…¬å¹³å¤è‹ï¼šä¸€ç§ç”¨äº Bangladesh æ´ªç¾åæ´åŠ©ä¼˜å…ˆçº§ç¡®å®šçš„å…¬å¹³æ„ŸçŸ¥äººå·¥æ™ºèƒ½æ¡†æ¶",
      "authors": [
        "Farjana Yesmin",
        "Romana Akter"
      ],
      "abstract": "Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å‘å±•ä¸­å›½å®¶åœ¨ç¾åæ•‘æ´åˆ†é…ä¸­å­˜åœ¨çš„ç³»ç»Ÿæ€§åè§ï¼Œæå‡ºäº†ä¸€ç§å…¬å¹³æ„ŸçŸ¥(Fairness-aware)çš„äººå·¥æ™ºèƒ½æ¡†æ¶ï¼Œç”¨äºå­ŸåŠ æ‹‰å›½æ´ªç¾åçš„æ•‘æ´ä¼˜å…ˆçº§åˆ†é…ã€‚è¯¥æ¡†æ¶åŸºäº2022å¹´å­ŸåŠ æ‹‰å›½æ´ªç¾çœŸå®æ•°æ®ï¼Œå¼€å‘äº†ä¸€ç§å¯¹æŠ—æ€§å»åæ¨¡å‹(Adversarial debiasing model)ï¼Œé€šè¿‡å¼•å…¥æ¢¯åº¦åå‘å±‚(Gradient reversal layer)å­¦ä¹ åè§ä¸å˜ç‰¹å¾ï¼Œä»è€Œæœ‰æ•ˆæ¶ˆé™¤å¯¹è¾¹ç¼˜åŒ–åœ°åŒºå’Œå†œæ‘åœ°åŒºçš„ç³»ç»Ÿæ€§åè§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨å­ŸåŠ æ‹‰å›½11ä¸ªåœ°åŒºçš„87ä¸ªä¹¡é•‡åº”ç”¨ä¸­ï¼Œå°†ç»Ÿè®¡å¹³ç­‰å·®å¼‚(Statistical parity difference)é™ä½äº†41.6%ï¼ŒåŒºåŸŸå…¬å¹³å·®è·(Regional fairness gaps)å‡å°‘äº†43.2%ï¼ŒåŒæ—¶åœ¨é¢„æµ‹å‡†ç¡®ç‡(R-squared=0.784)ä¸Šè¡¨ç°ç¨³å¥ã€‚è¯¥å·¥ä½œè¯æ˜äº†ç®—æ³•å…¬å¹³æ€§æŠ€æœ¯åœ¨äººé“ä¸»ä¹‰èƒŒæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼Œç”Ÿæˆçš„ä¼˜å…ˆçº§æ’åèƒ½ç¡®ä¿æ•‘æ´ç‰©èµ„æ ¹æ®çœŸå®éœ€æ±‚ç²¾å‡†è§¦è¾¾æœ€è„†å¼±çš„äººç¾¤ï¼Œä¸ºå†³ç­–è€…å®ç°æ›´å…¬å¹³çš„ç¾åæ¢å¤ç­–ç•¥æä¾›äº†å…³é”®å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 6 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2512.22210v1",
      "published_date": "2025-12-22 03:45:15 UTC",
      "updated_date": "2025-12-22 03:45:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:16:36.988706+00:00"
    },
    {
      "arxiv_id": "2512.19004v1",
      "title": "Context-Aware Initialization for Reducing Generative Path Length in Diffusion Language Models",
      "title_zh": "ç¼©çŸ­æ‰©æ•£è¯­è¨€æ¨¡å‹ç”Ÿæˆè·¯å¾„é•¿åº¦çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥åˆå§‹åŒ–",
      "authors": [
        "Tongyuan Miao",
        "Gary Huang",
        "Kai Jun Han",
        "Annie Jiang"
      ],
      "abstract": "Diffusion Large Language Models (DLLMs) enable fully parallel token decoding but often remain impractical at inference time due to the many denoising iterations required to refine an information-free, fully masked initialization into coherent text. Most existing acceleration methods focus on traversing this generative trajectory more efficiently via improved solvers or sampling strategies. We advance a complementary perspective: shorten the trajectory itself by starting closer to the target distribution through context-aware initialization.\n  We propose a training-free interface that injects prompt-conditioned priors from a lightweight auxiliary model into the diffusion initialization, and instantiate it with two mechanisms: discrete token injection and representation-level embedding interpolation. Because injected priors can be imperfect and unmask-only decoding can over-commit early, we also introduce a simple confidence-based remasking mechanism as a form of prior skepticism. Preliminary evidence on GSM8K suggests that context-aware initialization can substantially reduce denoising iterations (about 35\\% fewer function evaluations in our setting), while also exposing a key open challenge: naive warm-starting can degrade final accuracy relative to strong diffusion baselines. We use these findings to motivate a research agenda around calibration, revision mechanisms, and representation alignment for reliable warm-started diffusion decoding.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•ç¼©çŸ­æ‰©æ•£å¤§è¯­è¨€æ¨¡å‹ (Diffusion Large Language Models, DLLMs) åœ¨æ¨ç†æ—¶çš„ç”Ÿæˆè·¯å¾„ï¼Œæå‡ºäº†ä¸€ç§åä¸º Context-Aware Initialization çš„æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡å°†è½»é‡çº§è¾…åŠ©æ¨¡å‹çš„æç¤ºæ¡ä»¶å…ˆéªŒæ³¨å…¥æ‰©æ•£åˆå§‹åŒ–ä¸­ï¼Œä½¿ç”Ÿæˆè¿‡ç¨‹æ›´æ¥è¿‘ç›®æ ‡åˆ†å¸ƒï¼Œå…·ä½“åŒ…æ‹¬ç¦»æ•£ä»¤ç‰Œæ³¨å…¥ (discrete token injection) å’Œè¡¨å¾çº§åµŒå…¥æ’å€¼ (representation-level embedding interpolation) ä¸¤ç§æœºåˆ¶ã€‚ä¸ºäº†ç¼“è§£åˆå§‹å…ˆéªŒå¯èƒ½å­˜åœ¨çš„é”™è¯¯ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åŸºäºç½®ä¿¡åº¦çš„é‡æ©ç æœºåˆ¶ (confidence-based remasking) ä½œä¸ºå…ˆéªŒæ€€ç–‘æ‰‹æ®µã€‚åœ¨ GSM8K æ•°æ®é›†ä¸Šçš„åˆæ­¥å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½å°†å»å™ªè¿­ä»£æ¬¡æ•°å‡å°‘çº¦ 35%ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿæˆæ•ˆç‡ã€‚ç„¶è€Œï¼Œå®éªŒä¹Ÿå‘ç°ç®€å•çš„çƒ­å¯åŠ¨å¯èƒ½ä¼šé™ä½æœ€ç»ˆçš„å‡†ç¡®ç‡ï¼Œè¿™ä¿ƒä½¿ç ”ç©¶è€…è¿›ä¸€æ­¥æå‡ºäº†å…³äºæ ¡å‡†ã€ä¿®æ­£æœºåˆ¶å’Œè¡¨å¾å¯¹é½çš„åç»­ç ”ç©¶è®®ç¨‹ï¼Œä»¥å®ç°æ›´å¯é çš„æ‰©æ•£è§£ç ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19004v1",
      "published_date": "2025-12-22 03:45:04 UTC",
      "updated_date": "2025-12-22 03:45:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:16:39.617410+00:00"
    },
    {
      "arxiv_id": "2512.19001v2",
      "title": "ORPR: An OR-Guided Pretrain-then-Reinforce Learning Model for Inventory Management",
      "title_zh": "ORPRï¼šä¸€ç§è¿ç­¹å­¦å¼•å¯¼çš„â€œå…ˆé¢„è®­ç»ƒåå¼ºåŒ–â€åº“å­˜ç®¡ç†å­¦ä¹ æ¨¡å‹",
      "authors": [
        "Lingjie Zhao",
        "Xue Yu",
        "Yongzhi Qi",
        "Hao Hu",
        "Jianshen Zhang",
        "Yingzheng Ma",
        "Shuyu Han",
        "Wei Qi",
        "Zuo-Jun Max Shen"
      ],
      "abstract": "As the pursuit of synergy between Artificial Intelligence (AI) and Operations Research (OR) gains momentum in handling complex inventory systems, a critical challenge persists: how to effectively reconcile AI's adaptive perception with OR's structural rigor. To bridge this gap, we propose a novel OR-Guided \"Pretrain-then-Reinforce\" framework. To provide structured guidance, we propose a simulation-augmented OR model that generates high-quality reference decisions, implicitly capturing complex business constraints and managerial preferences. Leveraging these OR-derived decisions as foundational training labels, we design a domain-informed deep learning foundation model to establish foundational decision-making capabilities, followed by a reinforcement learning (RL) fine-tuning stage. Uniquely, we position RL as a deep alignment mechanism that enables the AI agent to internalize the optimality principles of OR, while simultaneously leveraging exploration for general policy refinement and allowing expert guidance for scenario-specific adaptation (e.g., promotional events). Validated through extensive numerical experiments and a field deployment at JD.com augmented by a Difference-in-Differences (DiD) analysis, our model significantly outperforms incumbent industrial practices, delivering real-world gains of a 5.27-day reduction in turnover and a 2.29% increase in in-stock rates, alongside a 29.95% decrease in holding costs. Contrary to the prevailing trend of brute-force model scaling, our study demonstrates that a lightweight, domain-informed model can deliver state-of-the-art performance and robust transferability when guided by structured OR logic. This approach offers a scalable and cost-effective paradigm for intelligent supply chain management, highlighting the value of deeply aligning AI with OR.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ORPRï¼Œä¸€ç§è¿ç­¹å­¦(Operations Research)å¼•å¯¼çš„â€œå…ˆé¢„è®­ç»ƒåå¼ºåŒ–å­¦ä¹ â€(Pretrain-then-Reinforce)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åº“å­˜ç®¡ç†ä¸­äººå·¥æ™ºèƒ½(AI)çš„è‡ªé€‚åº”æ„ŸçŸ¥ä¸è¿ç­¹å­¦(OR)ç»“æ„ä¸¥è°¨æ€§ä¹‹é—´çš„èåˆéš¾é¢˜ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡æ¨¡æ‹Ÿå¢å¼ºçš„è¿ç­¹å­¦æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å‚è€ƒå†³ç­–ï¼Œå¹¶åˆ©ç”¨è¿™äº›å†³ç­–ä½œä¸ºæ ‡ç­¾è®­ç»ƒé¢†åŸŸçŸ¥è¯†æ„ŸçŸ¥çš„æ·±åº¦å­¦ä¹ åŸºåº§æ¨¡å‹ï¼Œä»¥å»ºç«‹åŸºç¡€å†³ç­–èƒ½åŠ›ã€‚éšåè¿›å…¥å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å¾®è°ƒé˜¶æ®µï¼Œå°†å…¶ä½œä¸ºæ·±å±‚å¯¹é½æœºåˆ¶ï¼Œä½¿æ¨¡å‹åœ¨å†…åŒ–ORæœ€ä¼˜æ€§åŸåˆ™çš„åŒæ—¶ï¼Œé€šè¿‡ç­–ç•¥æ¢ç´¢ä¸ä¸“å®¶å¼•å¯¼å®ç°åœºæ™¯åŒ–é€‚é…ã€‚åœ¨äº¬ä¸œ(JD.com)çš„å®åœ°éƒ¨ç½²åŠå®éªŒç»“æœæ˜¾ç¤ºï¼ŒORPRæ˜¾è‘—ä¼˜äºç°æœ‰å·¥ä¸šå®è·µï¼Œå®ç°äº†å‘¨è½¬å¤©æ•°å‡å°‘5.27å¤©ã€ç°è´§ç‡æå‡2.29%ä»¥åŠæŒä»“æˆæœ¬é™ä½29.95%çš„æ˜¾è‘—æˆæ•ˆã€‚ç ”ç©¶è¯æ˜ï¼Œè¿™ç§æ·±åº¦å¯¹é½AIä¸ORçš„è½»é‡åŒ–é¢†åŸŸæ¨¡å‹ï¼Œä¸ºæ™ºèƒ½ä¾›åº”é“¾ç®¡ç†æä¾›äº†ä¸€ç§å…·å¤‡é«˜å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šçš„æ–°èŒƒå¼ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.19001v2",
      "published_date": "2025-12-22 03:39:43 UTC",
      "updated_date": "2026-01-06 09:08:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:16:47.171846+00:00"
    },
    {
      "arxiv_id": "2512.22209v1",
      "title": "Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images",
      "title_zh": "åŸºäºæ‰©æ•£æ¨¡å‹çš„åŒ»å­¦å›¾åƒè¶…åˆ†è¾¨ç‡å¢å¼ºï¼šä½åˆ†è¾¨ç‡èƒƒéƒ¨å›¾åƒä¼˜åŒ–æ–¹æ¡ˆ",
      "authors": [
        "Haozhe Jia"
      ],
      "abstract": "Capsule endoscopy has enabled minimally invasive gastrointestinal imaging, but its clinical utility is limited by the inherently low resolution of captured images due to hardware, power, and transmission constraints. This limitation hampers the identification of fine-grained mucosal textures and subtle pathological features essential for early diagnosis.\n  This work investigates a diffusion-based super-resolution framework to enhance capsule endoscopy images in a data-driven and anatomically consistent manner. We adopt the SR3 (Super-Resolution via Repeated Refinement) framework built upon Denoising Diffusion Probabilistic Models (DDPMs) to learn a probabilistic mapping from low-resolution to high-resolution images. Unlike GAN-based approaches that often suffer from training instability and hallucination artifacts, diffusion models provide stable likelihood-based training and improved structural fidelity. The HyperKvasir dataset, a large-scale publicly available gastrointestinal endoscopy dataset, is used for training and evaluation.\n  Quantitative results demonstrate that the proposed method significantly outperforms bicubic interpolation and GAN-based super-resolution methods such as ESRGAN, achieving PSNR of 27.5 dB and SSIM of 0.65 for a baseline model, and improving to 29.3 dB and 0.71 with architectural enhancements including attention mechanisms. Qualitative results show improved preservation of anatomical boundaries, vascular patterns, and lesion structures. These findings indicate that diffusion-based super-resolution is a promising approach for enhancing non-invasive medical imaging, particularly in capsule endoscopy where image resolution is fundamentally constrained.",
      "tldr_zh": "é’ˆå¯¹èƒ¶å›Šå†…é•œ(Capsule endoscopy)å—ç¡¬ä»¶å’ŒåŠŸè€—é™åˆ¶å¯¼è‡´å›¾åƒåˆ†è¾¨ç‡ä½ã€éš¾ä»¥è¯†åˆ«ç»†å¾®ç—…ç†ç‰¹å¾çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£æ¨¡å‹(Diffusion Model)çš„è¶…åˆ†è¾¨ç‡å¢å¼ºæ–¹æ¡ˆã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†åŸºäºå»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹(DDPMs)çš„SR3ï¼ˆSuper-Resolution via Repeated Refinementï¼‰ç»“æ„ï¼Œé€šè¿‡æ¦‚ç‡æ˜ å°„å­¦ä¹ ä»ä½åˆ†è¾¨ç‡åˆ°é«˜åˆ†è¾¨ç‡å›¾åƒçš„è½¬æ¢ã€‚ä¸å®¹æ˜“äº§ç”Ÿä¼ªå½±å’Œè®­ç»ƒä¸ç¨³å®šçš„GAN-basedæ–¹æ³•ç›¸æ¯”ï¼Œæ‰©æ•£æ¨¡å‹åœ¨ä¿æŒè§£å‰–ç»“æ„çœŸå®æ„Ÿå’Œè®­ç»ƒç¨³å®šæ€§æ–¹é¢è¡¨ç°æ›´ä¼˜ã€‚ç ”ç©¶åˆ©ç”¨å¤§è§„æ¨¡èƒƒè‚ é“å†…çª¥é•œæ•°æ®é›†HyperKvasirè¿›è¡Œè®­ç»ƒä¸è¯„ä¼°ï¼Œå¹¶åœ¨æ¶æ„ä¸­å¼•å…¥äº†æ³¨æ„åŠ›æœºåˆ¶(Attention mechanisms)è¿›ä¸€æ­¥æå‡æ€§èƒ½ã€‚å®šé‡å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨PSNRå’ŒSSIMæŒ‡æ ‡ä¸Šå‡æ˜¾è‘—ä¼˜äºåŒä¸‰æ¬¡æ’å€¼å’ŒESRGANç­‰åŸºçº¿æ¨¡å‹ã€‚å®šæ€§åˆ†æè¿›ä¸€æ­¥è¯å®ï¼Œè¯¥æ–¹æ¡ˆèƒ½æ›´æœ‰æ•ˆåœ°ä¿ç•™è§£å‰–è¾¹ç•Œã€è¡€ç®¡æ¨¡å¼å’Œç—…å˜ç»“æ„ï¼Œä¸ºå¢å¼ºéä¾µå…¥æ€§åŒ»å­¦æˆåƒåˆ†è¾¨ç‡æä¾›äº†æå…·å‰æ™¯çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "19 pages, 16 figures. Undergraduate final year project",
      "pdf_url": "https://arxiv.org/pdf/2512.22209v1",
      "published_date": "2025-12-22 03:37:47 UTC",
      "updated_date": "2025-12-22 03:37:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:16:52.176959+00:00"
    },
    {
      "arxiv_id": "2512.18999v1",
      "title": "Evaluating the Challenges of LLMs in Real-world Medical Follow-up: A Comparative Study and An Optimized Framework",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨çœŸå®ä¸–ç•ŒåŒ»ç–—éšè®¿ä¸­æ‰€é¢ä¸´æŒ‘æˆ˜çš„è¯„ä¼°ï¼šå¯¹æ¯”ç ”ç©¶ä¸ä¼˜åŒ–æ¡†æ¶",
      "authors": [
        "Jinyan Liu",
        "Zikang Chen",
        "Qinchuan Wang",
        "Tan Xie",
        "Heming Zheng",
        "Xudong Lv"
      ],
      "abstract": "When applied directly in an end-to-end manner to medical follow-up tasks, Large Language Models (LLMs) often suffer from uncontrolled dialog flow and inaccurate information extraction due to the complexity of follow-up forms. To address this limitation, we designed and compared two follow-up chatbot systems: an end-to-end LLM-based system (control group) and a modular pipeline with structured process control (experimental group). Experimental results show that while the end-to-end approach frequently fails on lengthy and complex forms, our modular method-built on task decomposition, semantic clustering, and flow management-substantially improves dialog stability and extraction accuracy. Moreover, it reduces the number of dialogue turns by 46.73% and lowers token consumption by 80% to 87.5%. These findings highlight the necessity of integrating external control mechanisms when deploying LLMs in high-stakes medical follow-up scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨çœŸå®åŒ»ç–—éšè®¿ä»»åŠ¡ä¸­çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºç«¯åˆ°ç«¯ (end-to-end) æ¨¡å¼åœ¨å¤„ç†å¤æ‚è¡¨å•æ—¶å¸¸é¢ä¸´å¯¹è¯æµå¤±æ§å’Œä¿¡æ¯æå–ä¸å‡†ç¡®çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶è€…å¯¹æ¯”äº†ç«¯åˆ°ç«¯ç³»ç»Ÿä¸ä¸€ç§å…·æœ‰ç»“æ„åŒ–æµç¨‹æ§åˆ¶çš„æ¨¡å—åŒ–æµæ°´çº¿ (modular pipeline) ç³»ç»Ÿã€‚è¯¥ä¼˜åŒ–æ¡†æ¶åŸºäºä»»åŠ¡åˆ†è§£ (task decomposition)ã€è¯­ä¹‰èšç±» (semantic clustering) å’Œæµç¨‹ç®¡ç† (flow management) æŠ€æœ¯ï¼Œæ˜¾è‘—æå‡äº†å¯¹è¯çš„ç¨³å®šæ€§å’Œä¿¡æ¯æå–çš„å‡†ç¡®ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸ç«¯åˆ°ç«¯æ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å—åŒ–æ¡†æ¶å°†å¯¹è¯è½®æ¬¡å‡å°‘äº† 46.73%ï¼Œå¹¶å°† token æ¶ˆè€—é™ä½äº† 80% è‡³ 87.5%ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨åŒ»ç–—éšè®¿ç­‰é«˜é£é™©åœºæ™¯ä¸­ï¼Œä¸º LLMs é›†æˆå¤–éƒ¨æ§åˆ¶æœºåˆ¶ä»¥ç¡®ä¿ä»»åŠ¡å¯é æ€§çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 pages,3 figures,conference ICCBB2025",
      "pdf_url": "https://arxiv.org/pdf/2512.18999v1",
      "published_date": "2025-12-22 03:33:43 UTC",
      "updated_date": "2025-12-22 03:33:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:17:07.228062+00:00"
    },
    {
      "arxiv_id": "2512.18991v1",
      "title": "ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation",
      "title_zh": "ICP-4Dï¼šè¿æ¥è¿­ä»£æœ€è¿‘ç‚¹ä¸æ¿€å…‰é›·è¾¾å…¨æ™¯åˆ†å‰²çš„æ¡¥æ¢",
      "authors": [
        "Gyeongrok Oh",
        "Youngdong Jang",
        "Jonghyun Choi",
        "Suk-Ju Kang",
        "Guang Lin",
        "Sangpil Kim"
      ],
      "abstract": "Dominant paradigms for 4D LiDAR panoptic segmentation are usually required to train deep neural networks with large superimposed point clouds or design dedicated modules for instance association. However, these approaches perform redundant point processing and consequently become computationally expensive, yet still overlook the rich geometric priors inherently provided by raw point clouds. To this end, we introduce ICP-4D, a simple yet effective training-free framework that unifies spatial and temporal reasoning through geometric relations among instance-level point sets. Specifically, we apply the Iterative Closest Point (ICP) algorithm to directly associate temporally consistent instances by aligning the source and target point sets through the estimated transformation. To stabilize association under noisy instance predictions, we introduce a Sinkhorn-based soft matching. This exploits the underlying instance distribution to obtain accurate point-wise correspondences, resulting in robust geometric alignment. Furthermore, our carefully designed pipeline, which considers three instance types-static, dynamic, and missing-offers computational efficiency and occlusion-aware matching. Our extensive experiments across both SemanticKITTI and panoptic nuScenes demonstrate that our method consistently outperforms state-of-the-art approaches, even without additional training or extra point cloud inputs.",
      "tldr_zh": "ç°æœ‰çš„4D LiDAR panoptic segmentationä¸»æµæ–¹æ³•é€šå¸¸éœ€è¦åœ¨å¤§è§„æ¨¡å åŠ ç‚¹äº‘ä¸Šè®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œï¼Œæˆ–è®¾è®¡å¤æ‚çš„å®ä¾‹å…³è”æ¨¡å—ï¼Œå¯¼è‡´è®¡ç®—æˆæœ¬é«˜æ˜‚ä¸”å¿½ç•¥äº†åŸå§‹ç‚¹äº‘ä¸­ä¸°å¯Œçš„å‡ ä½•å…ˆéªŒ(geometric priors)ã€‚è¯¥ç ”ç©¶æå‡ºäº†ICP-4Dï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•ä¸”æ— éœ€è®­ç»ƒ(training-free)çš„æ¡†æ¶ï¼Œé€šè¿‡å®ä¾‹çº§ç‚¹é›†ä¹‹é—´çš„å‡ ä½•å…³ç³»ç»Ÿä¸€äº†ç©ºé—´å’Œæ—¶é—´æ¨ç†ã€‚è¯¥æ¡†æ¶ç›´æ¥åº”ç”¨Iterative Closest Point (ICP)ç®—æ³•ï¼Œé€šè¿‡ä¼°è®¡å˜æ¢æ¥å¯¹é½æºç‚¹é›†å’Œç›®æ ‡ç‚¹é›†ï¼Œä»è€Œå®ç°æ—¶é—´ä¸€è‡´çš„å®ä¾‹å…³è”ã€‚ä¸ºäº†åœ¨å™ªå£°é¢„æµ‹ä¸‹æé«˜ç¨³å®šæ€§ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºSinkhornçš„è½¯åŒ¹é…(soft matching)ï¼Œåˆ©ç”¨å®ä¾‹åˆ†å¸ƒè·å–å‡†ç¡®çš„ç‚¹å¯¹ç‚¹å¯¹åº”å…³ç³»ï¼Œç¡®ä¿é²æ£’çš„å‡ ä½•å¯¹é½ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡åŒºåˆ†é™æ€(static)ã€åŠ¨æ€(dynamic)å’Œç¼ºå¤±(missing)ä¸‰ç§å®ä¾‹ç±»å‹ï¼Œå…¼é¡¾äº†è®¡ç®—æ•ˆç‡ä¸é®æŒ¡æ„ŸçŸ¥ã€‚åœ¨SemanticKITTIå’Œpanoptic nuScenesæ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒICP-4Dåœ¨æ— éœ€é¢å¤–è®­ç»ƒçš„æƒ…å†µä¸‹æ€§èƒ½ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18991v1",
      "published_date": "2025-12-22 03:13:08 UTC",
      "updated_date": "2025-12-22 03:13:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:17:08.069199+00:00"
    },
    {
      "arxiv_id": "2512.18986v1",
      "title": "R-GenIMA: Integrating Neuroimaging and Genetics with Interpretable Multimodal AI for Alzheimer's Disease Progression",
      "title_zh": "R-GenIMAï¼šèåˆç¥ç»å½±åƒä¸é—ä¼ å­¦çš„å¯è§£é‡Šå¤šæ¨¡æ€äººå·¥æ™ºèƒ½ï¼ŒåŠ©åŠ›é˜¿å°”èŒ¨æµ·é»˜ç—…ç—…ç¨‹è¿›å±•ç ”ç©¶",
      "authors": [
        "Kun Zhao",
        "Siyuan Dai",
        "Yingying Zhang",
        "Guodong Liu",
        "Pengfei Gu",
        "Chenghua Lin",
        "Paul M. Thompson",
        "Alex Leow",
        "Heng Huang",
        "Lifang He",
        "Liang Zhan",
        "Haoteng Tang"
      ],
      "abstract": "Early detection of Alzheimer's disease (AD) requires models capable of integrating macro-scale neuroanatomical alterations with micro-scale genetic susceptibility, yet existing multimodal approaches struggle to align these heterogeneous signals. We introduce R-GenIMA, an interpretable multimodal large language model that couples a novel ROI-wise vision transformer with genetic prompting to jointly model structural MRI and single nucleotide polymorphisms (SNPs) variations. By representing each anatomically parcellated brain region as a visual token and encoding SNP profiles as structured text, the framework enables cross-modal attention that links regional atrophy patterns to underlying genetic factors. Applied to the ADNI cohort, R-GenIMA achieves state-of-the-art performance in four-way classification across normal cognition (NC), subjective memory concerns (SMC), mild cognitive impairment (MCI), and AD. Beyond predictive accuracy, the model yields biologically meaningful explanations by identifying stage-specific brain regions and gene signatures, as well as coherent ROI-Gene association patterns across the disease continuum. Attention-based attribution revealed genes consistently enriched for established GWAS-supported AD risk loci, including APOE, BIN1, CLU, and RBFOX1. Stage-resolved neuroanatomical signatures identified shared vulnerability hubs across disease stages alongside stage-specific patterns: striatal involvement in subjective decline, frontotemporal engagement during prodromal impairment, and consolidated multimodal network disruption in AD. These results demonstrate that interpretable multimodal AI can synthesize imaging and genetics to reveal mechanistic insights, providing a foundation for clinically deployable tools that enable earlier risk stratification and inform precision therapeutic strategies in Alzheimer's disease.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† R-GenIMAï¼Œè¿™æ˜¯ä¸€ç§å¯è§£é‡Šçš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (Multimodal Large Language Model)ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆå®è§‚ç¥ç»è§£å‰–å˜åŒ–ä¸å¾®è§‚é—ä¼ æ˜“æ„Ÿæ€§æ¥æå‡é˜¿å°”èŒ¨æµ·é»˜ç—… (Alzheimer's Disease) çš„æ—©æœŸæ£€æµ‹ä¸è¿›å±•é¢„æµ‹ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ–°å‹çš„ ROI-wise Vision Transformer ç»“åˆé—ä¼ æç¤º (Genetic Prompting) æŠ€æœ¯ï¼Œå°†è„‘åŒºè§£å‰–åˆ†åŒºè¡¨å¾ä¸ºè§†è§‰ Token å¹¶ç¼–ç å•æ ¸è‹·é…¸å¤šæ€æ€§ (SNPs) å˜å¼‚ï¼Œå®ç°äº†èç¼©æ¨¡å¼ä¸é—ä¼ å› ç´ é—´çš„è·¨æ¨¡æ€æ³¨æ„åŠ› (Cross-modal Attention) å»ºæ¨¡ã€‚åœ¨ ADNI é˜Ÿåˆ—å®éªŒä¸­ï¼ŒR-GenIMA åœ¨æ­£å¸¸è®¤çŸ¥ã€ä¸»è§‚è®°å¿†ä¸‹é™ã€è½»åº¦è®¤çŸ¥éšœç¢åŠ AD çš„å››åˆ†ç±»ä»»åŠ¡ä¸­å–å¾—äº† State-of-the-art çš„è¡¨ç°ã€‚è¯¥æ¨¡å‹ä¸ä»…å…·å¤‡æé«˜çš„é¢„æµ‹ç²¾åº¦ï¼Œè¿˜èƒ½é€šè¿‡æ³¨æ„åŠ›å½’å› è¯†åˆ«å‡ºå…·æœ‰ç”Ÿç‰©å­¦æ„ä¹‰çš„é˜¶æ®µç‰¹å¼‚æ€§è„‘åŒºåŠå…³é”®åŸºå› ç‰¹å¾ï¼ˆå¦‚ APOEã€BIN1 ç­‰ï¼‰ï¼Œæ­ç¤ºäº†ç–¾ç—…æ¼”å˜ä¸­çš„ ROI-Gene å…³è”æ¨¡å¼ã€‚ç ”ç©¶ç»“æœè¯æ˜äº†å¯è§£é‡Šå¤šæ¨¡æ€ AI åœ¨åˆæˆåŒ»å­¦å½±åƒä¸é—ä¼ æ•°æ®æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ºä¸´åºŠæ—©æœŸé£é™©åˆ†å±‚å’Œç²¾å‡†æ²»ç–—ç­–ç•¥å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18986v1",
      "published_date": "2025-12-22 02:54:10 UTC",
      "updated_date": "2025-12-22 02:54:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:17:16.653378+00:00"
    },
    {
      "arxiv_id": "2512.18969v1",
      "title": "Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning",
      "title_zh": "èåˆè‡ªæ³¨æ„åŠ›ä¸çŠ¶æ€-ç‰©ä½“åŠ æƒç»„åˆçš„ç»„åˆé›¶æ ·æœ¬å­¦ä¹ ",
      "authors": [
        "Cheng-Hong Chang",
        "Pei-Hsuan Tsai"
      ],
      "abstract": "Object recognition has become prevalent across various industries. However, most existing applications are limited to identifying objects alone, without considering their associated states. The ability to recognize both the state and object simultaneously remains less common. One approach to address this is by treating state and object as a single category during training. However, this approach poses challenges in data collection and training since it requires comprehensive data for all possible combinations. Compositional Zero-shot Learning (CZSL) emerges as a viable solution by treating the state and object as distinct categories during training. CZSL facilitates the identification of novel compositions even in the absence of data for every conceivable combination. The current state-of-the-art method, KG-SP, addresses this issue by training distinct classifiers for states and objects, while leveraging a semantic model to evaluate the plausibility of composed compositions. However, KG-SP's accuracy in state and object recognition can be further improved, and it fails to consider the weighting of states and objects during composition. In this study, we propose SASOW, an enhancement of KG-SP that considers the weighting of states and objects while improving composition recognition accuracy. First, we introduce self-attention mechanisms into the classifiers for states and objects, leading to enhanced accuracy in recognizing both. Additionally, we incorporate the weighting of states and objects during composition to generate more reasonable and accurate compositions. Our validation process involves testing SASOW on three established benchmark datasets. Experimental outcomes affirm when compared against OW-CZSL approach, KG-SP, SASOW showcases improvements of 2.1%, 1.7%, and 0.4% in terms of accuracy for unseen compositions across the MIT-States, UT Zappos, and C-GQA datasets, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç»„åˆé›¶æ ·æœ¬å­¦ä¹ (Compositional Zero-Shot Learning, CZSL)ä¸­ç°æœ‰æ–¹æ³•è¯†åˆ«å‡†ç¡®ç‡å—é™ä¸”æœªè€ƒè™‘çŠ¶æ€ä¸ç‰©ä½“æƒé‡åˆ†é…çš„é—®é¢˜ï¼Œæå‡ºäº†SASOWæ¡†æ¶ã€‚SASOWé€šè¿‡åœ¨çŠ¶æ€å’Œç‰©ä½“åˆ†ç±»å™¨ä¸­å¼•å…¥è‡ªæ³¨æ„åŠ›æœºåˆ¶(self-attention mechanisms)ï¼Œæ˜¾è‘—æå‡äº†å¯¹åŸºç¡€å±æ€§ç‰¹å¾çš„æå–ä¸è¯†åˆ«ç²¾åº¦ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨ç»„åˆé˜¶æ®µå¼•å…¥äº†çŠ¶æ€ä¸ç‰©ä½“çš„åŠ æƒç»„åˆç­–ç•¥ï¼Œä»è€Œèƒ½å¤Ÿç”Ÿæˆæ›´ç¬¦åˆé€»è¾‘ä¸”å‡†ç¡®çš„å¤åˆé¢„æµ‹ç»“æœã€‚åœ¨MIT-Statesã€UT Zapposå’ŒC-GQAä¸‰ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒSASOWåœ¨å¤„ç†æœªè§ç»„åˆ(unseen compositions)æ—¶ï¼Œç›¸è¾ƒäºKG-SPç­‰åŸºå‡†æ–¹æ³•åˆ†åˆ«å®ç°äº†2.1%ã€1.7%å’Œ0.4%çš„å‡†ç¡®ç‡æå‡ã€‚è¯¥ç ”ç©¶è¯æ˜äº†é€šè¿‡è‡ªæ³¨æ„åŠ›å¢å¼ºå’ŒåŠ¨æ€æƒé‡åˆ†é…å¯ä»¥æœ‰æ•ˆæå‡æ¨¡å‹åœ¨å¤æ‚è§†è§‰å±æ€§ç»„åˆä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18969v1",
      "published_date": "2025-12-22 02:30:19 UTC",
      "updated_date": "2025-12-22 02:30:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:17:13.921846+00:00"
    },
    {
      "arxiv_id": "2512.18956v1",
      "title": "Training Multimodal Large Reasoning Models Needs Better Thoughts: A Three-Stage Framework for Long Chain-of-Thought Synthesis and Selection",
      "title_zh": "å¤šæ¨¡æ€å¤§æ¨ç†æ¨¡å‹è®­ç»ƒéœ€è¦æ›´ä¼˜è´¨çš„æ€ç»´ï¼šä¸€ç§é•¿é“¾å¼æ€ç»´åˆæˆä¸ç­›é€‰çš„ä¸‰é˜¶æ®µæ¡†æ¶",
      "authors": [
        "Yizhi Wang",
        "Linan Yue",
        "Min-Ling Zhang"
      ],
      "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning tasks through long Chain-of-Thought (CoT) reasoning. Extending these successes to multimodal reasoning remains challenging due to the increased complexity of integrating diverse input modalities and the scarcity of high-quality long CoT training data. Existing multimodal datasets and CoT synthesis methods still suffer from limited reasoning depth, modality conversion errors, and rigid generation pipelines, hindering model performance and stability. To this end, in this paper, we propose SynSelect, a novel three-stage Synthesis-Selection framework for generating high-quality long CoT data tailored to multimodal reasoning tasks. Specifically, SynSelect first leverages multiple heterogeneous multimodal LRMs to produce diverse candidate CoTs, and then applies both instance and batch level selection to filter high-quality CoTs that can effectively enhance the model's reasoning capabilities. Extensive experiments on multiple multimodal benchmarks demonstrate that models supervised fine-tuned on SynSelect-generated data significantly outperform baselines and achieve further improvements after reinforcement learning post-training. Our results validate SynSelect as an effective approach for advancing multimodal LRMs reasoning capabilities.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§æ¨ç†æ¨¡å‹(Multimodal Large Reasoning Models, LRMs)åœ¨é•¿é“¾å¼æ€ç»´(Chain-of-Thought, CoT)æ¨ç†æ•°æ®ç¨€ç¼ºåŠå¤æ‚æ€§æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†SynSelectè¿™ä¸€ä¸‰é˜¶æ®µçš„åˆæˆä¸ç­›é€‰æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ç°æœ‰æ•°æ®é›†æ¨ç†æ·±åº¦ä¸è¶³å’Œæ¨¡æ€è½¬æ¢é”™è¯¯ç­‰é—®é¢˜ï¼ŒSynSelecté¦–å…ˆåˆ©ç”¨å¤šä¸ªå¼‚æ„çš„å¤šæ¨¡æ€LRMsç”Ÿæˆå¤šæ ·åŒ–çš„å€™é€‰CoTsã€‚æ¥ç€ï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†å®ä¾‹çº§å’Œæ‰¹æ¬¡çº§çš„ç­›é€‰æœºåˆ¶ï¼Œæ—¨åœ¨ç²¾å‡†è¯†åˆ«å¹¶æå–èƒ½æ˜¾è‘—å¢å¼ºæ¨¡å‹æ¨ç†èƒ½åŠ›çš„é«˜è´¨é‡æ•°æ®ã€‚å¤šé¡¹åŸºå‡†æµ‹è¯•ç»“æœæ˜¾ç¤ºï¼Œç»è¿‡SynSelectæ•°æ®ç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning)çš„æ¨¡å‹æ€§èƒ½å¤§å¹…è¶…è¿‡åŸºçº¿æ¨¡å‹ï¼Œå¹¶åœ¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)åè®­ç»ƒä¸­è¡¨ç°å‡ºæŒç»­çš„å¢é•¿ã€‚è¯¥ç ”ç©¶ä¸ä»…è¯æ˜äº†SynSelectåœ¨åˆæˆé«˜è´¨é‡é•¿CoTæ•°æ®æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¹Ÿä¸ºæå‡å¤šæ¨¡æ€LRMsçš„å¤æ‚æ¨ç†èƒ½åŠ›æä¾›äº†æ–°çš„è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18956v1",
      "published_date": "2025-12-22 02:07:20 UTC",
      "updated_date": "2025-12-22 02:07:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:17:19.100658+00:00"
    },
    {
      "arxiv_id": "2512.18950v1",
      "title": "Learning Hierarchical Procedural Memory for LLM Agents through Bayesian Selection and Contrastive Refinement",
      "title_zh": "åŸºäºè´å¶æ–¯é€‰æ‹©ä¸å¯¹æ¯”æç‚¼çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“å±‚çº§åŒ–ç¨‹åºæ€§è®°å¿†å­¦ä¹ ",
      "authors": [
        "Saman Forouzandeh",
        "Wei Peng",
        "Parham Moradi",
        "Xinghuo Yu",
        "Mahdi Jalili"
      ],
      "abstract": "We present MACLA, a framework that decouples reasoning from learning by maintaining a frozen large language model while performing all adaptation in an external hierarchical procedural memory. MACLA extracts reusable procedures from trajectories, tracks reliability via Bayesian posteriors, selects actions through expected-utility scoring, and refines procedures by contrasting successes and failures. Across four benchmarks (ALFWorld, WebShop, TravelPlanner, InterCodeSQL), MACLA achieves 78.1 percent average performance, outperforming all baselines. On ALFWorld unseen tasks, MACLA reaches 90.3 percent with 3.1 percent positive generalization. The system constructs memory in 56 seconds, 2800 times faster than the state-of-the-art LLM parameter-training baseline, compressing 2851 trajectories into 187 procedures. Experimental results demonstrate that structured external memory with Bayesian selection and contrastive refinement enables sample-efficient, interpretable, and continually improving agents without LLM parameter updates.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MACLA æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡å°†æ¨ç†ä¸å­¦ä¹ è§£è€¦ï¼Œåœ¨ä¿æŒå¤§è¯­è¨€æ¨¡å‹(LLM)å‚æ•°å†»ç»“çš„åŒæ—¶ï¼Œåˆ©ç”¨å¤–éƒ¨çš„åˆ†å±‚ç¨‹åºåŒ–è®°å¿†(external hierarchical procedural memory)å®ç°æ™ºèƒ½ä½“çš„è‡ªä¸»è¿›åŒ–ã€‚MACLA ä»ä»»åŠ¡è½¨è¿¹ä¸­æå–å¯é‡ç”¨çš„ç¨‹åº(reusable procedures)ï¼Œå¹¶é€šè¿‡è´å¶æ–¯åéªŒ(Bayesian posteriors)æŒç»­è·Ÿè¸ªå…¶å¯é æ€§ã€‚åœ¨æ‰§è¡Œé˜¶æ®µï¼Œç³»ç»Ÿåˆ©ç”¨æœŸæœ›æ•ˆç”¨è¯„åˆ†(expected-utility scoring)é€‰æ‹©åŠ¨ä½œï¼Œå¹¶é€šè¿‡å¯¹æ¯”æˆåŠŸä¸å¤±è´¥æ¡ˆä¾‹çš„å¯¹æ¯”ä¼˜åŒ–(contrastive refinement)æ¥ç²¾ç‚¼ç¨‹åºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMACLA åœ¨ ALFWorldã€WebShop ç­‰å››ä¸ªåŸºå‡†æµ‹è¯•ä¸­å¹³å‡æ€§èƒ½è¾¾åˆ° 78.1%ï¼Œæ˜¾è‘—ä¼˜äºæ‰€æœ‰åŸºå‡†æ¨¡å‹ï¼Œä¸”åœ¨ ALFWorld æœªè§ä»»åŠ¡ä¸Šè¡¨ç°å‡ºæå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»Ÿçš„è®°å¿†æ„å»ºæ•ˆç‡æé«˜ï¼Œå¤„ç†é€Ÿåº¦æ¯”ä¼ ç»Ÿå‚æ•°è®­ç»ƒåŸºå‡†å¿« 2800 å€ï¼Œä»…éœ€ 56 ç§’å³å¯å°†æµ·é‡è½¨è¿¹å‹ç¼©ä¸ºç²¾ç®€çš„ç¨‹åºé›†ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†ç»“æ„åŒ–å¤–éƒ¨è®°å¿†é…åˆè´å¶æ–¯é€‰æ‹©æœºåˆ¶ï¼Œèƒ½å¤Ÿä½¿æ™ºèƒ½ä½“åœ¨ä¸æ›´æ–° LLM å‚æ•°çš„æƒ…å†µä¸‹ï¼Œå®ç°é«˜æ ·æœ¬æ•ˆç‡ã€å¯è§£é‡Šä¸”æŒç»­æ”¹è¿›çš„ä»»åŠ¡æ‰§è¡Œèƒ½åŠ›ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at The 25th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS 2026). 21 pages including references, with 7 figures and 8 tables. Code is publicly available at the authors GitHub repository: https://github.com/S-Forouzandeh/MACLA-LLM-Agents-AAMAS-Conference",
      "pdf_url": "https://arxiv.org/pdf/2512.18950v1",
      "published_date": "2025-12-22 01:56:28 UTC",
      "updated_date": "2025-12-22 01:56:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:18:23.852639+00:00"
    },
    {
      "arxiv_id": "2512.18947v1",
      "title": "Clustering-based Transfer Learning for Dynamic Multimodal MultiObjective Evolutionary Algorithm",
      "title_zh": "é¢å‘åŠ¨æ€å¤šæ¨¡æ€å¤šç›®æ ‡è¿›åŒ–ç®—æ³•çš„åŸºäºèšç±»è¿ç§»å­¦ä¹ ",
      "authors": [
        "Li Yan",
        "Bolun Liu",
        "Chao Li",
        "Jing Liang",
        "Kunjie Yu",
        "Caitong Yue",
        "Xuzhao Chai",
        "Boyang Qu"
      ],
      "abstract": "Dynamic multimodal multiobjective optimization presents the dual challenge of simultaneously tracking multiple equivalent pareto optimal sets and maintaining population diversity in time-varying environments. However, existing dynamic multiobjective evolutionary algorithms often neglect solution modality, whereas static multimodal multiobjective evolutionary algorithms lack adaptability to dynamic changes. To address above challenge, this paper makes two primary contributions. First, we introduce a new benchmark suite of dynamic multimodal multiobjective test functions constructed by fusing the properties of both dynamic and multimodal optimization to establish a rigorous evaluation platform. Second, we propose a novel algorithm centered on a Clustering-based Autoencoder prediction dynamic response mechanism, which utilizes an autoencoder model to process matched clusters to generate a highly diverse initial population. Furthermore, to balance the algorithm's convergence and diversity, we integrate an adaptive niching strategy into the static optimizer. Empirical analysis on 12 instances of dynamic multimodal multiobjective test functions reveals that, compared with several state-of-the-art dynamic multiobjective evolutionary algorithms and multimodal multiobjective evolutionary algorithms, our algorithm not only preserves population diversity more effectively in the decision space but also achieves superior convergence in the objective space.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠ¨æ€å¤šæ¨¡æ€å¤šç›®æ ‡ä¼˜åŒ– (Dynamic multimodal multiobjective optimization) ä¸­éš¾ä»¥åŒæ—¶è¿½è¸ªå¤šä¸ªç­‰æ•ˆå¸•ç´¯æ‰˜æœ€ä¼˜é›†å¹¶ä¿æŒç§ç¾¤å¤šæ ·æ€§çš„æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºäº†ç°æœ‰ç®—æ³•åœ¨å¤„ç†è§£çš„å¤šæ¨¡æ€ (Modality) ç‰¹æ€§æˆ–åŠ¨æ€ç¯å¢ƒé€‚åº”æ€§æ–¹é¢çš„ä¸è¶³ã€‚ç ”ç©¶é¦–å…ˆé€šè¿‡èåˆåŠ¨æ€ä¸å¤šæ¨¡æ€ä¼˜åŒ–ç‰¹æ€§ï¼Œæ„å»ºäº†ä¸€å¥—å…¨æ–°çš„æµ‹è¯•å‡½æ•°åŸºå‡†é›†ï¼Œä¸ºè¯¥é¢†åŸŸçš„ç®—æ³•è¯„ä¼°æä¾›äº†ä¸¥è°¨çš„å¹³å°ã€‚æ¥ç€ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºèšç±»è‡ªç¼–ç å™¨ (Clustering-based Autoencoder) é¢„æµ‹åŠ¨æ€å“åº”æœºåˆ¶çš„æ–°å‹ç®—æ³•ï¼Œåˆ©ç”¨è‡ªç¼–ç å™¨æ¨¡å‹å¤„ç†åŒ¹é…çš„èšç±»ä»¥ç”Ÿæˆé«˜åº¦å¤šæ ·åŒ–çš„åˆå§‹ç§ç¾¤ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¹³è¡¡ç®—æ³•çš„æ”¶æ•›æ€§ä¸å¤šæ ·æ€§ï¼Œè¯¥æ–¹æ¡ˆåœ¨é™æ€ä¼˜åŒ–å™¨ä¸­é›†æˆäº†è‡ªé€‚åº”å°ç”Ÿå¢ƒç­–ç•¥ (Adaptive niching strategy)ã€‚åœ¨ 12 ä¸ªæµ‹è¯•å®ä¾‹ä¸Šçš„å®éªŒåˆ†æè¡¨æ˜ï¼Œè¯¥ç®—æ³•ç›¸æ¯”äºç›®å‰çš„åŠ¨æ€å¤šç›®æ ‡è¿›åŒ–ç®—æ³•å’Œå¤šæ¨¡æ€å¤šç›®æ ‡è¿›åŒ–ç®—æ³•ï¼Œä¸ä»…åœ¨å†³ç­–ç©ºé—´ä¸­èƒ½æ›´æœ‰æ•ˆåœ°ä¿æŒç§ç¾¤å¤šæ ·æ€§ï¼Œåœ¨ç›®æ ‡ç©ºé—´ä¸­ä¹Ÿå®ç°äº†æ›´ä¼˜çš„æ”¶æ•›æ€§èƒ½ã€‚",
      "categories": [
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18947v1",
      "published_date": "2025-12-22 01:51:26 UTC",
      "updated_date": "2025-12-22 01:51:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:18:18.742963+00:00"
    },
    {
      "arxiv_id": "2601.00020v2",
      "title": "Personalized Spiking Neural Networks with Ferroelectric Synapses for EEG Signal Processing",
      "title_zh": "ç”¨äºè„‘ç”µä¿¡å·å¤„ç†çš„åŸºäºé“ç”µçªè§¦çš„ä¸ªæ€§åŒ–è„‰å†²ç¥ç»ç½‘ç»œ",
      "authors": [
        "Nikhil Garg",
        "Anxiong Song",
        "Niklas Plessnig",
        "Nathan Savoia",
        "Laura BÃ©gon-Lours"
      ],
      "abstract": "Electroencephalography (EEG)-based brain-computer interfaces (BCIs) are strongly affected by non-stationary neural signals that vary across sessions and individuals, limiting the generalization of subject-agnostic models and motivating adaptive and personalized learning on resource-constrained platforms. Programmable memristive hardware offers a promising substrate for such post-deployment adaptation; however, practical realization is challenged by limited weight resolution, device variability, nonlinear programming dynamics, and finite device endurance. In this work, we show that spiking neural networks (SNNs) can be deployed on ferroelectric memristive synaptic devices for adaptive EEG-based motor imagery decoding under realistic device constraints. We fabricate, characterize, and model ferroelectric synapses. We evaluate a convolutional-recurrent SNN architecture under two complementary deployment strategies: (i) device-aware training using a ferroelectric synapse model, and (ii) transfer of software-trained weights followed by low-overhead on-device re-tuning. To enable efficient adaptation, we introduce a device-aware weight-update strategy in which gradient-based updates are accumulated digitally and converted into discrete programming events only when a threshold is exceeded, emulating nonlinear, state-dependent programming dynamics while reducing programming frequency. Both deployment strategies achieve classification performance comparable to state-of-the-art software-based SNNs. Furthermore, subject-specific transfer learning achieved by retraining only the final network layers improves classification accuracy. These results demonstrate that programmable ferroelectric hardware can support robust, low-overhead adaptation in spiking neural networks, opening a practical path toward personalized neuromorphic processing of neural signals.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‘æœºæ¥å£ï¼ˆBCIsï¼‰ä¸­è„‘ç”µå›¾ï¼ˆEEGï¼‰ä¿¡å·çš„éå¹³ç¨³æ€§å’Œè·¨ä¸ªä½“å·®å¼‚æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨é“ç”µçªè§¦ï¼ˆferroelectric synapsesï¼‰ç¡¬ä»¶å®ç°ä¸ªæ€§åŒ–è„‰å†²ç¥ç»ç½‘ç»œï¼ˆSNNsï¼‰çš„æ–¹æ¡ˆã€‚é€šè¿‡å¯¹é“ç”µçªè§¦è¿›è¡Œåˆ¶å¤‡ä¸å»ºæ¨¡ï¼Œç ”ç©¶éªŒè¯äº†è®¾å¤‡æ„ŸçŸ¥è®­ç»ƒï¼ˆdevice-aware trainingï¼‰å’Œè½¯ä»¶æƒé‡è½¬ç§»ååœ¨çº¿å¾®è°ƒä¸¤ç§éƒ¨ç½²ç­–ç•¥åœ¨è¿åŠ¨æƒ³è±¡ï¼ˆmotor imageryï¼‰è§£ç ä¸­çš„æœ‰æ•ˆæ€§ã€‚ä¸ºäº†å…‹æœç¡¬ä»¶éçº¿æ€§åŠ¨æ€å’Œæœ‰é™è€å—æ€§ï¼Œä½œè€…å¼•å…¥äº†ä¸€ç§è®¾å¤‡æ„ŸçŸ¥æƒé‡æ›´æ–°ç­–ç•¥ï¼Œé€šè¿‡æ•°å­—åŒ–ç´¯ç§¯æ¢¯åº¦å¹¶ä»…åœ¨è¶…è¿‡é˜ˆå€¼æ—¶è§¦å‘ç¦»æ•£ç¼–ç¨‹ï¼Œä»è€Œæ˜¾è‘—é™ä½äº†ç¼–ç¨‹é¢‘ç‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨çœŸå®ç¡¬ä»¶çº¦æŸä¸‹è¾¾åˆ°äº†ä¸å…ˆè¿›è½¯ä»¶çº§SNNsç›¸å½“çš„åˆ†ç±»æ€§èƒ½ã€‚æ­¤å¤–ï¼Œé€šè¿‡ä»…é‡è®­ç½‘ç»œæœ«å±‚çš„å—è¯•è€…ç‰¹å¼‚æ€§è¿ç§»å­¦ä¹ ï¼ˆsubject-specific transfer learningï¼‰ï¼Œè¿›ä¸€æ­¥æå‡äº†åˆ†ç±»å‡†ç¡®ç‡ã€‚è¿™ä¸€æˆæœå±•ç¤ºäº†å¯ç¼–ç¨‹é“ç”µç¡¬ä»¶æ”¯æŒä½åŠŸè€—ã€ä¸ªæ€§åŒ–ç±»è„‘è®¡ç®—çš„èƒ½åŠ›ï¼Œä¸ºåœ¨èµ„æºå—é™å¹³å°ä¸Šå®ç°ç¥ç»ä¿¡å·çš„å®æ—¶å¤„ç†å¼€è¾Ÿäº†å®é™…è·¯å¾„ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "eess.SY"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00020v2",
      "published_date": "2025-12-22 01:09:24 UTC",
      "updated_date": "2026-01-05 06:41:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:17:28.548140+00:00"
    },
    {
      "arxiv_id": "2512.22207v1",
      "title": "GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks",
      "title_zh": "GamiBenchï¼šé€šè¿‡æŠ˜çº¸ä»»åŠ¡è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„ç©ºé—´æ¨ç†ä¸äºŒç»´è‡³ä¸‰ç»´è§„åˆ’èƒ½åŠ›",
      "authors": [
        "Ryan Spencer",
        "Roey Yaari",
        "Ritvik Vemavarapu",
        "Joyce Yang",
        "Steven Ngo",
        "Utkarsh Sharma"
      ],
      "abstract": "Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† GamiBenchï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨é€šè¿‡æŠ˜çº¸ï¼ˆOrigamiï¼‰å¯å‘å¼ä»»åŠ¡è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰ç©ºé—´æ¨ç†ï¼ˆSpatial Reasoningï¼‰å’Œ 2D åˆ° 3D è§„åˆ’èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚é’ˆå¯¹ç°æœ‰åŸºå‡†æµ‹è¯•å¤šä¾§é‡äºé™æ€å›¾åƒæˆ–æœ€ç»ˆè¾“å‡ºçš„å±€é™æ€§ï¼ŒGamiBench å¼•å…¥äº†åŒ…å« 186 ä¸ªå¸¸è§„å’Œ 186 ä¸ªç‰©ç†ä¸Šä¸å¯èƒ½å®ç°çš„ 2D æŠ˜ç—•å›¾ï¼ˆCrease Patternsï¼‰åŠå…¶å¯¹åº”çš„ 3D å½¢çŠ¶ï¼Œé€šè¿‡ä¸‰é¡¹è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ä»»åŠ¡å…¨é¢è€ƒå¯Ÿæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶åˆ©ç”¨è·¨è§†è§’ä¸€è‡´æ€§ã€ç‰©ç†å¯è¡Œæ€§æ£€æµ‹ä»¥åŠå¯¹ä¸­é—´æŠ˜å æ­¥éª¤çš„è§£è¯»æ¥è¡¡é‡æ¨¡å‹æ€§èƒ½ï¼Œå¹¶ä¸“é—¨æå‡ºäº†è§†è§’ä¸€è‡´æ€§ï¼ˆVCï¼‰å’Œä¸å¯èƒ½æŠ˜å é€‰æ‹©ç‡ï¼ˆIFSRï¼‰ç­‰è¯Šæ–­æŒ‡æ ‡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä¾¿æ˜¯ GPT-5 å’Œ Gemini-2.5-Pro ç­‰é¡¶å°–æ¨¡å‹åœ¨å¤„ç†å•æ­¥ç©ºé—´ç†è§£æ—¶ä¹Ÿè¡¨ç°æ¬ ä½³ï¼Œå‡¸æ˜¾äº†å½“å‰æ¨¡å‹åœ¨å‡ ä½•ç†è§£æ–¹é¢çš„æ˜¾è‘—å±€é™ã€‚è¯¥é¡¹å·¥ä½œä¸ºè¯„ä¼° MLLMs å¤„ç†å¤æ‚ç©ºé—´ä»»åŠ¡çš„èƒ½åŠ›æä¾›äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„æ¡†æ¶ï¼Œä¸ºæœªæ¥æå‡æ¨¡å‹çš„ç‰©ç†å¸¸è¯†å’Œä¸‰ç»´ç©ºé—´è®¤çŸ¥å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.22207v1",
      "published_date": "2025-12-22 01:07:59 UTC",
      "updated_date": "2025-12-22 01:07:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:17:44.403401+00:00"
    },
    {
      "arxiv_id": "2512.18934v1",
      "title": "When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models",
      "title_zh": "å°‘å³æ˜¯å¤šï¼š8ä½é‡åŒ–æå‡å¤§è¯­è¨€æ¨¡å‹çš„æŒç»­å­¦ä¹ èƒ½åŠ›",
      "authors": [
        "Michael S. Zhang",
        "Rishi A. Ruia",
        "Arnav Kewalram",
        "Saathvik Dharmapuram",
        "Utkarsh Sharma",
        "Kevin Zhu"
      ],
      "abstract": "Catastrophic forgetting poses a fundamental challenge in continual learning, particularly when models are quantized for deployment efficiency. We systematically investigate the interplay between quantization precision (FP16, INT8, INT4) and replay buffer strategies in large language models, revealing unexpected dynamics. While FP16 achieves superior initial task performance (74.44% on NLU), we observe a striking inversion on subsequent tasks: quantized models outperform FP16 by 8-15% on final task forward accuracy, with INT4 achieving nearly double FP16's performance on Code generation (40% vs 20%). Critically, even minimal replay buffers (0.1%) dramatically improve retention - increasing NLU retention after Math training from 45% to 65% across all precision levels - with INT8 consistently achieving the optimal balance between learning plasticity and knowledge retention. We hypothesize that quantization-induced noise acts as implicit regularization, preventing the overfitting to new task gradients that plagues high-precision models. These findings challenge the conventional wisdom that higher precision is always preferable, suggesting instead that INT8 quantization offers both computational efficiency and superior continual learning dynamics. Our results provide practical guidelines for deploying compressed models in continual learning scenarios: small replay buffers (1-2%) suffice for NLU tasks, while Math and Code benefit from moderate buffers (5-10%), with quantized models requiring less replay than FP16 to achieve comparable retention. Code is available at https://github.com/Festyve/LessIsMore.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹(Large Language Models)åœ¨æŒç»­å­¦ä¹ (Continual Learning)ä¸­çš„ç¾éš¾æ€§é—å¿˜(Catastrophic Forgetting)é—®é¢˜ï¼Œç³»ç»Ÿè°ƒæŸ¥äº†é‡åŒ–ç²¾åº¦(FP16, INT8, INT4)ä¸é‡æ”¾ç¼“å†²åŒºç­–ç•¥(Replay Buffer Strategies)çš„ç›¸äº’ä½œç”¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œé‡åŒ–æ¨¡å‹åœ¨åç»­ä»»åŠ¡çš„å‰å‘å‡†ç¡®ç‡ä¸Šæ¯” FP16 é«˜å‡º 8-15%ï¼Œå…¶ä¸­ INT4 åœ¨ä»£ç ç”Ÿæˆ(Code generation)ä»»åŠ¡ä¸­çš„è¡¨ç°ç”šè‡³è¾¾åˆ° FP16 çš„ä¸¤å€ã€‚ç ”ç©¶å‘ç° INT8 åœ¨å­¦ä¹ å¡‘æ€§(Learning Plasticity)ä¸çŸ¥è¯†ä¿ç•™(Knowledge Retention)ä¹‹é—´å®ç°äº†æœ€ä½³å¹³è¡¡ï¼Œä¸”ä»…éœ€æå°çš„é‡æ”¾æ¯”ä¾‹(0.1%)å³å¯æ˜¾è‘—æå‡ä¿ç•™èƒ½åŠ›ã€‚ä½œè€…å‡è®¾é‡åŒ–å™ªå£°èµ·åˆ°äº†éšå¼æ­£åˆ™åŒ–(Implicit Regularization)çš„ä½œç”¨ï¼Œæœ‰æ•ˆé˜²æ­¢äº†æ¨¡å‹å¯¹æ–°ä»»åŠ¡æ¢¯åº¦çš„è¿‡æ‹Ÿåˆã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†é«˜ç²¾åº¦å¿…ç„¶ä¼˜äºä½ç²¾åº¦çš„ä¼ ç»Ÿè§‚ç‚¹ï¼Œè¯æ˜ INT8 å…¼å…·è®¡ç®—æ•ˆç‡ä¸æ›´ä¼˜çš„æŒç»­å­¦ä¹ åŠ¨æ€ã€‚æœ€åï¼Œç ”ç©¶ä¸ºä¸åŒä»»åŠ¡ä¸‹çš„ç¼“å†²åŒºé…ç½®æä¾›äº†å®è·µæŒ‡å—ï¼ŒæŒ‡å‡º NLU ä»»åŠ¡ä»…éœ€ 1-2% çš„ç¼“å†²åŒºï¼Œè€Œæ•°å­¦ä¸ä»£ç ä»»åŠ¡åˆ™éœ€ 5-10%ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18934v1",
      "published_date": "2025-12-22 00:51:39 UTC",
      "updated_date": "2025-12-22 00:51:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:17:38.640125+00:00"
    },
    {
      "arxiv_id": "2512.18930v1",
      "title": "LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer",
      "title_zh": "LouvreSAEï¼šç”¨äºå¯è§£é‡Šä¸”å¯æ§é£æ ¼è¿ç§»çš„ç¨€ç–è‡ªç¼–ç å™¨",
      "authors": [
        "Raina Panda",
        "Daniel Fein",
        "Arpita Singhal",
        "Mark Fiore",
        "Maneesh Agrawala",
        "Matyas Bohacek"
      ],
      "abstract": "Artistic style transfer in generative models remains a significant challenge, as existing methods often introduce style only via model fine-tuning, additional adapters, or prompt engineering, all of which can be computationally expensive and may still entangle style with subject matter. In this paper, we introduce a training- and inference-light, interpretable method for representing and transferring artistic style. Our approach leverages an art-specific Sparse Autoencoder (SAE) on top of latent embeddings of generative image models. Trained on artistic data, our SAE learns an emergent, largely disentangled set of stylistic and compositional concepts, corresponding to style-related elements pertaining brushwork, texture, and color palette, as well as semantic and structural concepts. We call it LouvreSAE and use it to construct style profiles: compact, decomposable steering vectors that enable style transfer without any model updates or optimization. Unlike prior concept-based style transfer methods, our method requires no fine-tuning, no LoRA training, and no additional inference passes, enabling direct steering of artistic styles from only a few reference images. We validate our method on ArtBench10, achieving or surpassing existing methods on style evaluations (VGG Style Loss and CLIP Score Style) while being 1.7-20x faster and, critically, interpretable.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†LouvreSAEï¼Œä¸€ç§åŸºäºç¨€ç–è‡ªç¼–ç å™¨(Sparse Autoencoder, SAE)çš„è½»é‡åŒ–ã€å¯è§£é‡Šè‰ºæœ¯é£æ ¼è¿ç§»æ–¹æ³•ã€‚è¯¥æ–¹æ³•åœ¨ç”Ÿæˆå›¾åƒæ¨¡å‹çš„æ½œç©ºé—´åµŒå…¥ä¹‹ä¸Šåº”ç”¨SAEï¼Œé€šè¿‡å¯¹è‰ºæœ¯æ•°æ®çš„è®­ç»ƒï¼Œå­¦ä¹ åˆ°æ¶µç›–ç¬”è§¦(brushwork)ã€çº¹ç†(texture)å’Œè‰²å½©åˆ†å¸ƒç­‰è§£è€¦çš„é£æ ¼ä¸æ„æˆæ¦‚å¿µã€‚LouvreSAEé€šè¿‡æ„å»ºç´§å‡‘ä¸”å¯åˆ†è§£çš„é£æ ¼æ¡£æ¡ˆ(style profiles)ä½œä¸ºå¼•å¯¼å‘é‡ï¼Œæ— éœ€ä»»ä½•æ¨¡å‹å¾®è°ƒã€LoRAè®­ç»ƒæˆ–é¢å¤–çš„æ¨ç†æ­¥éª¤ï¼Œä»…éœ€å°‘é‡å‚è€ƒå›¾å³å¯å®ç°é£æ ¼è¿ç§»ã€‚åœ¨ArtBench10ä¸Šçš„å®éªŒéªŒè¯æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨VGG Style Losså’ŒCLIP Score Styleç­‰é£æ ¼è¯„ä¼°æŒ‡æ ‡ä¸Šè¾¾åˆ°æˆ–è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯ã€‚ç›¸æ¯”äºä¼ ç»Ÿæ–¹æ³•ï¼ŒLouvreSAEçš„å¤„ç†é€Ÿåº¦æå‡äº†1.7è‡³20å€ï¼Œåœ¨ä¿æŒé«˜æ•ˆè¿è¡Œçš„åŒæ—¶ï¼Œæ˜¾è‘—å¢å¼ºäº†é£æ ¼æ§åˆ¶çš„å¯è§£é‡Šæ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.18930v1",
      "published_date": "2025-12-22 00:36:22 UTC",
      "updated_date": "2025-12-22 00:36:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:19:01.596671+00:00"
    },
    {
      "arxiv_id": "2512.21354v1",
      "title": "Reflection-Driven Control for Trustworthy Code Agents",
      "title_zh": "é¢å‘å¯ä¿¡ä»£ç æ™ºèƒ½ä½“çš„åæ€é©±åŠ¨æ§åˆ¶",
      "authors": [
        "Bin Wang",
        "Jiazheng Quan",
        "Xingrui Yu",
        "Hansen Hu",
        "Yuhao",
        "Ivor Tsang"
      ],
      "abstract": "Contemporary large language model (LLM) agents are remarkably capable, but they still lack reliable safety controls and can produce unconstrained, unpredictable, and even actively harmful outputs. To address this, we introduce Reflection-Driven Control, a standardized and pluggable control module that can be seamlessly integrated into general agent architectures. Reflection-Driven Control elevates \"self-reflection\" from a post hoc patch into an explicit step in the agent's own reasoning process: during generation, the agent continuously runs an internal reflection loop that monitors and evaluates its own decision path. When potential risks are detected, the system retrieves relevant repair examples and secure coding guidelines from an evolving reflective memory, injecting these evidence-based constraints directly into subsequent reasoning steps. We instantiate Reflection-Driven Control in the setting of secure code generation and systematically evaluate it across eight classes of security-critical programming tasks. Empirical results show that Reflection-Driven Control substantially improves the security and policy compliance of generated code while largely preserving functional correctness, with minimal runtime and token overhead. Taken together, these findings indicate that Reflection-Driven Control is a practical path toward trustworthy AI coding agents: it enables designs that are simultaneously autonomous, safer by construction, and auditable.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å½“å‰å¤§è¯­è¨€æ¨¡å‹(LLM)æ™ºèƒ½ä½“ç¼ºä¹å¯é å®‰å…¨æ§åˆ¶ã€æ˜“äº§ç”Ÿä¸å¯æ§ç”šè‡³æœ‰å®³è¾“å‡ºçš„é—®é¢˜ï¼Œæå‡ºäº†Reflection-Driven Controlï¼Œè¿™æ˜¯ä¸€ç§æ ‡å‡†åŒ–ä¸”å¯æ’æ‹”çš„æ§åˆ¶æ¨¡å—ã€‚è¯¥æ–¹æ³•å°†â€œself-reflectionâ€ä»ç®€å•çš„åæœŸè¡¥æ•‘æå‡ä¸ºæ¨ç†è¿‡ç¨‹ä¸­çš„æ˜¾å¼æ­¥éª¤ï¼Œé€šè¿‡åœ¨ç”ŸæˆæœŸé—´è¿è¡Œå†…éƒ¨åæ€å¾ªç¯æ¥æŒç»­ç›‘æ§å’Œè¯„ä¼°æ™ºèƒ½ä½“çš„å†³ç­–è·¯å¾„ã€‚å½“æ£€æµ‹åˆ°æ½œåœ¨é£é™©æ—¶ï¼Œç³»ç»Ÿä¼šä»åŠ¨æ€æ¼”è¿›çš„reflective memoryä¸­æ£€ç´¢ç›¸å…³çš„ä¿®å¤ç¤ºä¾‹å’Œå®‰å…¨ç¼–ç æŒ‡å—ï¼Œå¹¶å°†è¿™äº›è¯æ®çº¦æŸç›´æ¥æ³¨å…¥åç»­æ¨ç†ã€‚åœ¨å…«ç±»å®‰å…¨å…³é”®ç¼–ç¨‹ä»»åŠ¡çš„å®éªŒä¸­ï¼ŒReflection-Driven Controlåœ¨ä¿æŒåŠŸèƒ½æ­£ç¡®æ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—æé«˜äº†ç”Ÿæˆä»£ç çš„å®‰å…¨æ€§å’Œç­–ç•¥åˆè§„æ€§ï¼Œä¸”è¿è¡Œæ—¶é—´å’ŒTokenå¼€é”€æå°ã€‚è¿™é¡¹å·¥ä½œè¯æ˜äº†Reflection-Driven Controlæ˜¯å®ç°è‡ªä¸»ã€å®‰å…¨ä¸”å¯å®¡è®¡çš„å¯ä¿¡AIä»£ç æ™ºèƒ½ä½“çš„ä¸€æ¡åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted to AAAI 2026 Workshop on Trust and Control in Agentic AI (TrustAgent)",
      "pdf_url": "https://arxiv.org/pdf/2512.21354v1",
      "published_date": "2025-12-22 00:27:38 UTC",
      "updated_date": "2025-12-22 00:27:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:18:42.603508+00:00"
    },
    {
      "arxiv_id": "2601.08846v1",
      "title": "Directional Attractors in LLM Reasoning: How Similarity Retrieval Steers Iterative Summarization Based Reasoning",
      "title_zh": "LLM æ¨ç†ä¸­çš„å®šå‘å¸å¼•å­ï¼šç›¸ä¼¼æ€§æ£€ç´¢å¦‚ä½•å¼•å¯¼åŸºäºè¿­ä»£æ‘˜è¦çš„æ¨ç†",
      "authors": [
        "Cagatay Tekin",
        "Charbel Barakat",
        "Luis Joseph Luna Limgenco"
      ],
      "abstract": "Iterative summarization based reasoning frameworks such as InftyThink enable long-horizon reasoning in large language models (LLMs) by controlling context growth, but they repeatedly regenerate similar reasoning strategies across tasks. We introduce InftyThink with Cross-Chain Memory, an extension that augments iterative reasoning with an embedding-based semantic cache of previously successful reasoning patterns. At each reasoning step, the model retrieves and conditions on the most semantically similar stored lemmas, guiding inference without expanding the context window indiscriminately. Experiments on MATH500, AIME2024, and GPQA-Diamond demonstrate that semantic lemma retrieval improves accuracy in structured domains while exposing failure modes in tests that include heterogeneous domains. Geometric analyses of reasoning trajectories reveal that cache retrieval induces directional biases in embedding space, leading to consistent fix (improve baseline accuracy) and break (degradation in baseline accuracy) attractors. Our results highlight both the benefits and limits of similarity-based memory for self-improving LLM reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ InftyThink ç­‰è¿­ä»£æ‘˜è¦æ¨ç†æ¡†æ¶åœ¨å¤„ç†é•¿ç¨‹æ¨ç†æ—¶é‡å¤ç”Ÿæˆç›¸ä¼¼ç­–ç•¥çš„é—®é¢˜ï¼Œæå‡ºäº† InftyThink with Cross-Chain Memory æ‰©å±•æ¨¡å‹ã€‚è¯¥æ¨¡å‹å¼•å…¥äº†åŸºäº Embedding çš„è¯­ä¹‰ç¼“å­˜ï¼Œç”¨äºå­˜å‚¨å…ˆå‰æˆåŠŸçš„æ¨ç†æ¨¡å¼ï¼Œå¹¶åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤æ£€ç´¢è¯­ä¹‰æœ€ç›¸ä¼¼çš„ Lemmas æ¥å¼•å¯¼æ¨ç†ï¼Œä»è€Œé¿å…äº†ä¸Šä¸‹æ–‡çª—å£çš„æ— åºæ‰©å¼ ã€‚åœ¨ MATH500ã€AIME2024 å’Œ GPQA-Diamond ç­‰æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯­ä¹‰å¼•ç†æ£€ç´¢èƒ½æœ‰æ•ˆæå‡ç»“æ„åŒ–é¢†åŸŸçš„æ¨ç†å‡†ç¡®ç‡ï¼Œä½†åœ¨å¼‚æ„é¢†åŸŸä¸­ä¹Ÿæš´éœ²å‡ºä¸€å®šçš„å¤±æ•ˆæ¨¡å¼ã€‚é€šè¿‡å¯¹æ¨ç†è½¨è¿¹çš„å‡ ä½•åˆ†æï¼Œç ”ç©¶å‘ç°ç¼“å­˜æ£€ç´¢åœ¨åµŒå…¥ç©ºé—´ä¸­è¯±å‘äº†å®šå‘åå·®ï¼Œå½¢æˆäº†èƒ½å¤Ÿæ”¹å–„æˆ–é™ä½åŸºå‡†å‡†ç¡®ç‡çš„ Fix å’Œ Break å¸å¼•å­ï¼ˆAttractorsï¼‰ã€‚è¯¥ç»“æœå…¨é¢æ­ç¤ºäº†åŸºäºç›¸ä¼¼æ€§çš„è®°å¿†åœ¨æå‡ Large Language Models è‡ªæˆ‘æ”¹è¿›æ¨ç†èƒ½åŠ›æ–¹é¢çš„ä¼˜åŠ¿ä¸å±€é™æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "6 pages, 2 figures. Code available at: github.com/cagopat/InftyThink-with-Cross-Chain-Memory",
      "pdf_url": "https://arxiv.org/pdf/2601.08846v1",
      "published_date": "2025-12-22 00:26:54 UTC",
      "updated_date": "2025-12-22 00:26:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:19:09.469780+00:00"
    },
    {
      "arxiv_id": "2601.00810v1",
      "title": "Can Large Language Models Improve Venture Capital Exit Timing After IPO?",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èƒ½å¦ä¼˜åŒ–é£é™©æŠ•èµ„çš„IPOåé€€å‡ºæ—¶æœºï¼Ÿ",
      "authors": [
        "Mohammadhossien Rashidi"
      ],
      "abstract": "Exit timing after an IPO is one of the most consequential decisions for venture capital (VC) investors, yet existing research focuses mainly on describing when VCs exit rather than evaluating whether those choices are economically optimal. Meanwhile, large language models (LLMs) have shown promise in synthesizing complex financial data and textual information but have not been applied to post-IPO exit decisions. This study introduces a framework that uses LLMs to estimate the optimal time for VC exit by analyzing monthly post IPO information financial performance, filings, news, and market signals and recommending whether to sell or continue holding. We compare these LLM generated recommendations with the actual exit dates observed for VCs and compute the return differences between the two strategies. By quantifying gains or losses associated with following the LLM, this study provides evidence on whether AI-driven guidance can improve exit timing and complements traditional hazard and real-options models in venture capital research.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†é£é™©æŠ•èµ„(Venture Capital)åœ¨é¦–æ¬¡å…¬å¼€å‹Ÿè‚¡(IPO)åçš„é€€å‡ºæ—¶æœºå†³ç­–é—®é¢˜ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ç ”ç©¶ä¸»è¦é›†ä¸­äºæè¿°é€€å‡ºè¡Œä¸ºè€Œéè¯„ä¼°å…¶ç»æµæœ€ä¼˜æ€§çš„ä¸è¶³ã€‚ç ”ç©¶å¼•å…¥äº†ä¸€ä¸ªåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models)çš„åˆ†ææ¡†æ¶ï¼Œé€šè¿‡ç»¼åˆåˆ†æIPOåçš„æœˆåº¦è´¢åŠ¡è¡¨ç°ã€ç›‘ç®¡æ–‡ä»¶(filings)ã€æ–°é—»æŠ¥é“å’Œå¸‚åœºä¿¡å·æ¥ä¼°ç®—æœ€ä½³é€€å‡ºæ—¶é—´ã€‚è¯¥æ¡†æ¶èƒ½é’ˆå¯¹ç‰¹å®šæœˆä»½æä¾›â€œå‡ºå”®â€æˆ–â€œç»§ç»­æŒæœ‰â€çš„å†³ç­–å»ºè®®ï¼Œå¹¶å°†å…¶ä¸é£é™©æŠ•èµ„è€…çš„å®é™…é€€å‡ºæ—¥æœŸè¿›è¡Œå¯¹æ¯”ï¼Œè®¡ç®—ä¸åŒç­–ç•¥ä¸‹çš„æ”¶ç›Šå·®å¼‚ã€‚é€šè¿‡é‡åŒ–éµå¾ªAIæŒ‡å¯¼æ‰€äº§ç”Ÿçš„æŸç›Šï¼Œæœ¬ç ”ç©¶ä¸ºäººå·¥æ™ºèƒ½é©±åŠ¨çš„å†³ç­–èƒ½å¦ä¼˜åŒ–é€€å‡ºæ—¶æœºæä¾›äº†å®è¯è¯æ®ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆè¡¥å……äº†é£é™©æŠ•èµ„ç ”ç©¶ä¸­ä¼ ç»Ÿçš„é£é™©æ¯”ä¾‹æ¨¡å‹(hazard models)å’Œå®ç‰©æœŸæƒæ¨¡å‹(real-options models)ï¼Œå±•ç¤ºäº†LLMåœ¨å¤„ç†å¤æ‚é‡‘èæ•°æ®å’Œæ–‡æœ¬ä¿¡æ¯ä»¥è¾…åŠ©é«˜å±‚çº§æŠ•èµ„å†³ç­–æ–¹é¢çš„æ½œåŠ›ã€‚",
      "categories": [
        "q-fin.PM",
        "cs.AI",
        "cs.LG",
        "econ.GN",
        "q-fin.ST"
      ],
      "primary_category": "q-fin.PM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.00810v1",
      "published_date": "2025-12-22 00:19:34 UTC",
      "updated_date": "2025-12-22 00:19:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T19:18:52.096734+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 134,
  "processed_papers_count": 134,
  "failed_papers_count": 0,
  "llm_backup_calls": 1,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T19:20:12.040432+00:00"
}
{
  "date": "2024-04-08",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-04-08 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦于 AI 模型优化、LLM（Large Language Models）的安全性和应用、图像生成技术以及强化学习等领域，强调模型效率、鲁棒性和实际应用；令人印象深刻的文章包括 Eagle and Finch（RWKV 模型的创新改进，由多位知名学者如 Bo Peng 和 Daniel Goldstein 领导）和 CodecLM（LLM 对齐的合成数据方法），这些工作展示了 LLM 在知识存储和任务适配上的潜力。\n\n下面，我将挑选并简要讨论部分关键论文，先从重要、话题度高的 LLM 和图像生成相关论文入手，然后快速掠过其他领域的内容，以控制篇幅。每个条目列出论文标题（中文 + 英文），并突出核心贡献和发现。\n\n### LLM 和模型优化相关（重点讨论）\n- **Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence**（英文原标题）  \n  这篇论文由 Bo Peng 等知名学者主导，引入了 RWKV-5 和 RWKV-6 模型，改进了序列模型的表达能力，通过矩阵值状态和动态循环机制提升了效率，并在多语言数据集上实现了与 Llama 2 等模型相当的表现，强调了 RNN 式推理的潜力。\n\n- **CodecLM: Aligning Language Models with Tailored Synthetic Data**（英文原标题）  \n  论文提出 CodecLM 框架，使用 LLM 生成定制合成数据来优化模型对齐，显著提高了下游任务的准确性（如从 79% 到 97% 的元数据标准遵守率），展示了合成数据在 LLM 训练中的实用价值。\n\n- **Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning**（英文原标题）  \n  作者开发了 NPO 方法，用于 LLM 的数据遗忘，理论上证明了其比梯度上升更稳定，在 TOFU 数据集上实现了高效遗忘（遗忘 50% 数据时表现优于现有方法），为 LLM 隐私保护提供了新路径。\n\n- **SambaLingo: Teaching Large Language Models New Languages**（英文原标题）  \n  这篇工作探索了 LLM 在多语言适应上的方法，通过持续预训练和词汇扩展，在多种语言基准上超越了 Llama 2 和 BLOOM，突出了 LLM 多语性扩展的潜力。\n\n- **LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language Models and Doc-Level Embedding**（英文原标题）  \n  论文结合 LLM 和文档级嵌入提升了检索模型的性能，在 BEIR 数据集上达到 SOTA 水平，贡献在于改进负采样和损失函数，实现更高效的检索。\n\n其他 LLM 相关论文如 LTNER（使用 LLM 进行命名实体识别）和 Language-Independent Representations（改善 LLM 的零样本摘要）也展示了 LLM 在特定任务中的鲁棒性，但细节较常规，这里从略。\n\n### 图像生成和视觉相关（次重点讨论）\n- **SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual Editing**（英文原标题）  \n  这篇论文提出了 SwapAnything 框架，支持图像中任意对象的个性化替换（如位置和样式调整），在多对象任务上优于基线，贡献在于目标变量交换和外观适配机制，提升了图像编辑的精确性。\n\n- **Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer**（英文原标题）  \n  作者开发了 Humanoid-Gym 框架，使用强化学习训练机器人步态，实现零样本模拟到真实环境的转移，在 RobotEra 机器人上验证了其鲁棒性。\n\n其他视觉论文如 Self-Explainable Affordance Learning（结合视觉和语言的机器人交互）和 Dense Training, Sparse Inference（MoE 模型的效率优化）也值得关注，但由于篇幅，这里快速掠过：它们分别在交互解释和模型稀疏化上有所创新。\n\n### 强化学习和应用相关（快速掠过）\n- **Learning Heuristics for Transit Network Design and Improvement with Deep Reinforcement Learning**（英文原标题）  \n  主要贡献：使用图神经网络优化交通网络设计，在 Mumford 基准上实现了新 SOTA，提升了城市交通效率。\n\n- **Attention-Driven Multi-Agent Reinforcement Learning: Enhancing Decisions with Expertise-Informed Tasks**（英文原标题）  \n  贡献：整合注意力机制和领域知识，提高了多代理协作行为，在标准场景中提升了学习效率。\n\n其他如 Condition Monitoring（故障检测框架）和 Multi-agent Long-term 3D Human Pose Forecasting（交互aware轨迹预测），这些论文在工业和机器人领域有实际应用，但非核心话题，这里仅提及其在强化学习中的进展。\n\n### 其他领域（简要提及）\n今天还有一些论文涉及医疗 AI（如 Use of a Structured Knowledge Base 在元数据校正上的改进）和数据科学（如 Data Readiness for AI 的评估框架），但这些相对常规，我快速掠过：它们提供了 AI 在医疗和数据处理中的实用工具，却未有突破性发现。\n\n总之，今天的 arXiv 论文突出了 AI 领域的创新，尤其是 LLM 的优化和安全，这将推动实际应用。感兴趣的读者可关注 Eagle and Finch 等高影响力工作，详见 arXiv 页面！（本快报覆盖了主要亮点，保持简洁以便快速阅读。）",
  "papers": [
    {
      "arxiv_id": "2404.05908v1",
      "title": "Interpretability in Symbolic Regression: a benchmark of Explanatory Methods using the Feynman data set",
      "title_zh": "翻译失败",
      "authors": [
        "Guilherme Seidyo Imai Aldeia",
        "Fabricio Olivetti de Franca"
      ],
      "abstract": "In some situations, the interpretability of the machine learning models plays\na role as important as the model accuracy. Interpretability comes from the need\nto trust the prediction model, verify some of its properties, or even enforce\nthem to improve fairness. Many model-agnostic explanatory methods exists to\nprovide explanations for black-box models. In the regression task, the\npractitioner can use white-boxes or gray-boxes models to achieve more\ninterpretable results, which is the case of symbolic regression. When using an\nexplanatory method, and since interpretability lacks a rigorous definition,\nthere is a need to evaluate and compare the quality and different explainers.\nThis paper proposes a benchmark scheme to evaluate explanatory methods to\nexplain regression models, mainly symbolic regression models. Experiments were\nperformed using 100 physics equations with different interpretable and\nnon-interpretable regression methods and popular explanation methods,\nevaluating the performance of the explainers performance with several\nexplanation measures. In addition, we further analyzed four benchmarks from the\nGP community. The results have shown that Symbolic Regression models can be an\ninteresting alternative to white-box and black-box models that is capable of\nreturning accurate models with appropriate explanations. Regarding the\nexplainers, we observed that Partial Effects and SHAP were the most robust\nexplanation models, with Integrated Gradients being unstable only with\ntree-based models. This benchmark is publicly available for further\nexperiments.",
      "tldr_zh": "这篇论文提出一个基准方案，用于评估解释回归模型的可解释性方法，特别是针对符号回归（Symbolic Regression），并使用Feynman数据集合中的100个物理方程进行实验。研究比较了不同可解释和不可解释的回归方法，以及流行的解释方法，如Partial Effects、SHAP和Integrated Gradients，通过多种解释度量评估它们的性能。结果表明，符号回归是一种有效的白盒或黑盒模型替代方案，能提供准确且可解释的模型，其中Partial Effects和SHAP表现最稳健，而Integrated Gradients在树-based模型上不稳定。该基准已公开可用，支持进一步的研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "47 pages, 10 figures. This is a post peer-review, pre-copyedit\n  version of an article published in Genetic Programming and Evolvable Machines\n  Volume 23, pages 309-349, (2022). The final version is available on\n  https://link.springer.com/article/10.1007/s10710-022-09435-x",
      "pdf_url": "http://arxiv.org/pdf/2404.05908v1",
      "published_date": "2024-04-08 23:46:59 UTC",
      "updated_date": "2024-04-08 23:46:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:31:54.539994"
    },
    {
      "arxiv_id": "2404.05903v1",
      "title": "Natural Learning",
      "title_zh": "自然学习",
      "authors": [
        "Hadi Fanaee-T"
      ],
      "abstract": "We introduce Natural Learning (NL), a novel algorithm that elevates the\nexplainability and interpretability of machine learning to an extreme level. NL\nsimplifies decisions into intuitive rules, like \"We rejected your loan because\nyour income, employment status, and age collectively resemble a rejected\nprototype more than an accepted prototype.\" When applied to real-life datasets,\nNL produces impressive results. For example, in a colon cancer dataset with\n1545 patients and 10935 genes, NL achieves 98.1% accuracy, comparable to DNNs\nand RF, by analyzing just 3 genes of test samples against 2 discovered\nprototypes. Similarly, in the UCI's WDBC dataset, NL achieves 98.3% accuracy\nusing only 7 features and 2 prototypes. Even on the MNIST dataset (0 vs. 1), NL\nachieves 99.5% accuracy with only 3 pixels from 2 prototype images. NL is\ninspired by prototype theory, an old concept in cognitive psychology suggesting\nthat people learn single sparse prototypes to categorize objects. Leveraging\nthis relaxed assumption, we redesign Support Vector Machines (SVM), replacing\nits mathematical formulation with a fully nearest-neighbor-based solution, and\nto address the curse of dimensionality, we utilize locality-sensitive hashing.\nFollowing theory's generalizability principle, we propose a recursive method to\nprune non-core features. As a result, NL efficiently discovers the sparsest\nprototypes in O(n^2pL) with high parallelization capacity in terms of n.\nEvaluation of NL with 17 benchmark datasets shows its significant\noutperformance compared to decision trees and logistic regression, two methods\nwidely favored in healthcare for their interpretability. Moreover, NL achieves\nperformance comparable to finetuned black-box models such as deep neural\nnetworks and random forests in 40% of cases, with only a 1-2% lower average\naccuracy. The code is available via http://natural-learning.cc.",
      "tldr_zh": "本研究引入 Natural Learning (NL)，一种高度可解释的机器学习算法，将决策简化成直观的规则，例如通过比较样本与原型来解释结果，如贷款拒绝。NL 受认知心理学原型理论启发，重设计 Support Vector Machines (SVM) 为基于最近邻的解决方案，并结合 locality-sensitive hashing 处理维度诅咒，以及递归方法修剪非核心特征，实现高效的稀疏原型发现（复杂度 O(n^2 p L)，并支持高并行化）。在多个数据集上，NL 表现出色，例如在结肠癌数据集上达到 98.1% 准确率仅用 3 个基因和 2 个原型，在 UCI's WDBC 和 MNIST 数据集上分别实现 98.3% 和 99.5% 准确率；总体上，NL 优于决策树和逻辑回归，在 40% 的情况下与深神经网络 (DNNs) 和随机森林 (RF) 相当，仅低 1-2% 的平均准确率。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.05903v1",
      "published_date": "2024-04-08 23:15:41 UTC",
      "updated_date": "2024-04-08 23:15:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:32:07.129700"
    },
    {
      "arxiv_id": "2404.05902v1",
      "title": "WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents",
      "title_zh": "翻译失败",
      "authors": [
        "Michael Lutz",
        "Arth Bohra",
        "Manvel Saroyan",
        "Artem Harutyunyan",
        "Giovanni Campagna"
      ],
      "abstract": "In the realm of web agent research, achieving both generalization and\naccuracy remains a challenging problem. Due to high variance in website\nstructure, existing approaches often fail. Moreover, existing fine-tuning and\nin-context learning techniques fail to generalize across multiple websites. We\nintroduce Wilbur, an approach that uses a differentiable ranking model and a\nnovel instruction synthesis technique to optimally populate a black-box large\nlanguage model's prompt with task demonstrations from previous runs. To\nmaximize end-to-end success rates, we also propose an intelligent backtracking\nmechanism that learns and recovers from its mistakes. Finally, we show that our\nranking model can be trained on data from a generative auto-curriculum which\nsamples representative goals from an LLM, runs the agent, and automatically\nevaluates it, with no manual annotation. Wilbur achieves state-of-the-art\nresults on the WebVoyager benchmark, beating text-only models by 8% overall,\nand up to 36% on certain websites. On the same benchmark, Wilbur is within 5%\nof a strong multi-modal model despite only receiving textual inputs, and\nfurther analysis reveals a substantial number of failures are due to\nengineering challenges of operating the web.",
      "tldr_zh": "该研究提出WILBUR，一种自适应In-Context Learning方法，旨在提升网络代理的鲁棒性和准确性，以应对网站结构多样性带来的泛化挑战。WILBUR通过可微分排名模型(differentiable ranking model)和新型指令合成技术(instruction synthesis technique)优化大语言模型的提示内容，并引入智能回溯机制(intelligent backtracking mechanism)来从错误中学习恢复。同时，利用生成式自课程(generative auto-curriculum)进行无标注训练，在WebVoyager基准上取得最先进成果，比文本-only模型整体高8%、某些网站高36%，并与多模态模型仅差5%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05902v1",
      "published_date": "2024-04-08 23:10:47 UTC",
      "updated_date": "2024-04-08 23:10:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:32:16.563820"
    },
    {
      "arxiv_id": "2404.05894v4",
      "title": "Learning Heuristics for Transit Network Design and Improvement with Deep Reinforcement Learning",
      "title_zh": "通过深度强化学习学习公共交通网络设计和改进的启发式",
      "authors": [
        "Andrew Holliday",
        "Ahmed El-Geneidy",
        "Gregory Dudek"
      ],
      "abstract": "Transit agencies world-wide face tightening budgets and declining ridership.\nTo maintain quality of service while cutting costs, efficient transit network\ndesign is essential. But planning a network of public transit routes is a\nchallenging optimization problem. The most successful approaches to date use\nmetaheuristic algorithms to search through the space of possible transit\nnetworks by applying low-level heuristics that randomly alter routes in a\nnetwork. The design of these low-level heuristics has a major impact on the\nquality of the result. In this paper we use deep reinforcement learning with\ngraph neural nets to learn low-level heuristics for an evolutionary algorithm,\ninstead of designing them manually. These learned heuristics improve the\nalgorithm's results on benchmark synthetic cities with 70 nodes or more, and\nachieve new state-of-the-art results the challenging Mumford benchmark. They\nalso improve upon a simulation of the real transit network in the city of\nLaval, Canada, by as much as 52% and 25% on two key metrics, and offer cost\nsavings of up to 19% over the city's existing transit network.",
      "tldr_zh": "该研究使用深度强化学习（Deep Reinforcement Learning）和图神经网络（Graph Neural Nets）来自动学习低级启发式，用于优化公共交通网络设计和改进，从而提升进化算法（Evolutionary Algorithm）的性能。相比手动设计启发式，该方法在合成城市基准测试（如70节点以上网络）中显著提高了算法结果，并在Mumford基准上达到了新的最先进水平。在Laval市真实交通网络模拟中，该方法使两个关键指标分别提高了52%和25%，并实现了高达19%的成本节约。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "In preparation for submission to a journal",
      "pdf_url": "http://arxiv.org/pdf/2404.05894v4",
      "published_date": "2024-04-08 22:40:57 UTC",
      "updated_date": "2025-02-22 15:55:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:32:28.745924"
    },
    {
      "arxiv_id": "2404.05893v5",
      "title": "Use of a Structured Knowledge Base Enhances Metadata Curation by Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Sowmya S. Sundaram",
        "Benjamin Solomon",
        "Avani Khatri",
        "Anisha Laumas",
        "Purvesh Khatri",
        "Mark A. Musen"
      ],
      "abstract": "Metadata play a crucial role in ensuring the findability, accessibility,\ninteroperability, and reusability of datasets. This paper investigates the\npotential of large language models (LLMs), specifically GPT-4, to improve\nadherence to metadata standards. We conducted experiments on 200 random data\nrecords describing human samples relating to lung cancer from the NCBI\nBioSample repository, evaluating GPT-4's ability to suggest edits for adherence\nto metadata standards. We computed the adherence accuracy of field name-field\nvalue pairs through a peer review process, and we observed a marginal average\nimprovement in adherence to the standard data dictionary from 79% to 80%\n(p<0.5). We then prompted GPT-4 with domain information in the form of the\ntextual descriptions of CEDAR templates and recorded a significant improvement\nto 97% from 79% (p<0.01). These results indicate that, while LLMs may not be\nable to correct legacy metadata to ensure satisfactory adherence to standards\nwhen unaided, they do show promise for use in automated metadata curation when\nintegrated with a structured knowledge base",
      "tldr_zh": "这篇论文探讨了大型语言模型(LLMs)，如GPT-4，在提升元数据标准遵守方面的潜力，通过实验评估其对NCBI BioSample库中200个肺癌人类样本记录的编辑建议。结果显示，未辅助时LLMs仅将元数据遵守率从79%微幅提高到80%（p<0.5），改善不显著；但当整合结构化知识基（如CEDAR模板的文本描述）时，遵守率显著提升至97%（p<0.01）。该研究证明，LLMs在与结构化知识基结合时，能有效支持自动化元数据整理，增强数据集的可发现性、可访问性和可重用性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05893v5",
      "published_date": "2024-04-08 22:29:53 UTC",
      "updated_date": "2025-02-20 21:57:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:32:41.409713"
    },
    {
      "arxiv_id": "2404.05892v4",
      "title": "Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence",
      "title_zh": "翻译失败",
      "authors": [
        "Bo Peng",
        "Daniel Goldstein",
        "Quentin Anthony",
        "Alon Albalak",
        "Eric Alcaide",
        "Stella Biderman",
        "Eugene Cheah",
        "Xingjian Du",
        "Teddy Ferdinan",
        "Haowen Hou",
        "Przemysław Kazienko",
        "Kranthi Kiran GV",
        "Jan Kocoń",
        "Bartłomiej Koptyra",
        "Satyapriya Krishna",
        "Ronald McClelland Jr.",
        "Jiaju Lin",
        "Niklas Muennighoff",
        "Fares Obeid",
        "Atsushi Saito",
        "Guangyu Song",
        "Haoqin Tu",
        "Cahya Wirawan",
        "Stanisław Woźniak",
        "Ruichong Zhang",
        "Bingchen Zhao",
        "Qihang Zhao",
        "Peng Zhou",
        "Jian Zhu",
        "Rui-Jie Zhu"
      ],
      "abstract": "We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon\nthe RWKV (RWKV-4) architecture. Our architectural design advancements include\nmulti-headed matrix-valued states and a dynamic recurrence mechanism that\nimprove expressivity while maintaining the inference efficiency characteristics\nof RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a\nfast tokenizer based on greedy matching for enhanced multilinguality. We\ntrained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two\nFinch models with 1.6 and 3.1 billion parameters and find that they achieve\ncompetitive performance across a wide variety of benchmarks. We release all our\nmodels on HuggingFace under the Apache 2.0 license. Models at:\nhttps://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM\nInference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code\nat: https://github.com/RWKV/RWKV-infctx-trainer",
      "tldr_zh": "本研究提出了 Eagle (RWKV-5) 和 Finch (RWKV-6) 模型，作为 RWKV (RWKV-4) 架构的改进版本，引入了 multi-headed matrix-valued states 和 dynamic recurrence mechanism，以提升模型的表达能力，同时保留 RNN 的推理效率。研究者构建了一个新的多语言语料库，包含 1.12 万亿 tokens，并开发了基于贪婪匹配的快速分词器来增强多语言处理。团队训练了从 0.46 亿到 7.5 亿参数的四个 Eagle 模型，以及 1.6 亿和 3.1 亿参数的两个 Finch 模型，这些模型在各种基准测试中表现出色。所有模型及其训练和推理代码已在 HuggingFace 和 GitHub 上以 Apache 2.0 许可证开源。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05892v4",
      "published_date": "2024-04-08 22:20:59 UTC",
      "updated_date": "2024-09-26 22:39:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:32:53.931834"
    },
    {
      "arxiv_id": "2404.05891v2",
      "title": "Condition Monitoring with Incomplete Data: An Integrated Variational Autoencoder and Distance Metric Framework",
      "title_zh": "不完整数据下的状态监测：一个集成的变分自编码器和距离度量框架",
      "authors": [
        "Maryam Ahang",
        "Mostafa Abbasi",
        "Todd Charter",
        "Homayoun Najjaran"
      ],
      "abstract": "Condition monitoring of industrial systems is crucial for ensuring safety and\nmaintenance planning, yet notable challenges arise in real-world settings due\nto the limited or non-existent availability of fault samples. This paper\nintroduces an innovative solution to this problem by proposing a new method for\nfault detection and condition monitoring for unseen data. Adopting an approach\ninspired by zero-shot learning, our method can identify faults and assign a\nrelative health index to various operational conditions. Typically, we have\nplenty of data on normal operations, some data on compromised conditions, and\nvery few (if any) samples of severe faults. We use a variational autoencoder to\ncapture the probabilistic distribution of previously seen and new unseen\nconditions. The health status is determined by comparing each sample's\ndeviation from a normal operation reference distribution in the latent space.\nFaults are detected by establishing a threshold for the health indexes,\nallowing the model to identify severe, unseen faults with high accuracy, even\namidst noise. We validate our approach using the run-to-failure IMS-bearing\ndataset and compare it with other methods. The health indexes generated by our\nmodel closely match the established descriptive model of bearing wear,\nattesting to the robustness and reliability of our method. These findings\nhighlight the potential of our methodology in augmenting fault detection\ncapabilities within industrial domains, thereby contributing to heightened\nsafety protocols and optimized maintenance practices.",
      "tldr_zh": "本论文针对工业系统状态监测中故障样本缺失的挑战，提出了一种整合变分自编码器(Variational Autoencoder)和距离度量框架的新方法，受零样本学习启发，能够检测未见故障并分配相对健康指数。方法利用变分自编码器捕获正常操作和受损条件的概率分布，通过比较样本在潜在空间中的偏差来评估健康状态，并设定阈值以准确识别严重故障，即使在噪声环境下。实验在IMS-bearing数据集上验证，该方法生成的健康指数与轴承磨损描述模型高度一致，并优于其他方法，提升了工业领域的故障检测能力，从而优化安全协议和维护实践。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "Accepted in the 2024 IEEE 20th International Conference on Automation\n  Science and Engineering (CASE 2024)",
      "pdf_url": "http://arxiv.org/pdf/2404.05891v2",
      "published_date": "2024-04-08 22:20:23 UTC",
      "updated_date": "2024-06-27 21:54:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:33:05.815847"
    },
    {
      "arxiv_id": "2404.05875v1",
      "title": "CodecLM: Aligning Language Models with Tailored Synthetic Data",
      "title_zh": "翻译失败",
      "authors": [
        "Zifeng Wang",
        "Chun-Liang Li",
        "Vincent Perot",
        "Long T. Le",
        "Jin Miao",
        "Zizhao Zhang",
        "Chen-Yu Lee",
        "Tomas Pfister"
      ],
      "abstract": "Instruction tuning has emerged as the key in aligning large language models\n(LLMs) with specific task instructions, thereby mitigating the discrepancy\nbetween the next-token prediction objective and users' actual goals. To reduce\nthe labor and time cost to collect or annotate data by humans, researchers\nstart to explore the use of LLMs to generate instruction-aligned synthetic\ndata. Recent works focus on generating diverse instructions and applying LLM to\nincrease instruction complexity, often neglecting downstream use cases. It\nremains unclear how to tailor high-quality data to elicit better\ninstruction-following abilities in different target instruction distributions\nand LLMs. To this end, we introduce CodecLM, a general framework for adaptively\ngenerating high-quality synthetic data for LLM alignment with different\ndownstream instruction distributions and LLMs. Drawing on the Encode-Decode\nprinciples, we use LLMs as codecs to guide the data generation process. We\nfirst encode seed instructions into metadata, which are concise keywords\ngenerated on-the-fly to capture the target instruction distribution, and then\ndecode metadata to create tailored instructions. We also introduce Self-Rubrics\nand Contrastive Filtering during decoding to tailor data-efficient samples.\nExtensive experiments on four open-domain instruction following benchmarks\nvalidate the effectiveness of CodecLM over the current state-of-the-arts.",
      "tldr_zh": "该论文提出CodecLM框架，用于通过定制合成数据对齐大语言模型（LLMs），以解决现有方法忽略下游指令分布和模型差异的问题。框架基于Encode-Decode原则，使用LLMs将种子指令编码成简洁元数据（concise keywords）来捕捉目标分布，然后解码生成针对性的指令，同时引入Self-Rubrics和Contrastive Filtering来筛选高效数据。实验结果显示，在四个开源指令遵循基准上，CodecLM优于现有最先进方法，提升了LLMs的指令遵循能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to Findings of NAACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.05875v1",
      "published_date": "2024-04-08 21:15:36 UTC",
      "updated_date": "2024-04-08 21:15:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:33:17.422792"
    },
    {
      "arxiv_id": "2404.05868v2",
      "title": "Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning",
      "title_zh": "负偏好优化：从灾难性崩溃到有效遗忘",
      "authors": [
        "Ruiqi Zhang",
        "Licong Lin",
        "Yu Bai",
        "Song Mei"
      ],
      "abstract": "Large Language Models (LLMs) often memorize sensitive, private, or\ncopyrighted data during pre-training. LLM unlearning aims to eliminate the\ninfluence of undesirable data from the pre-trained model while preserving the\nmodel's utilities on other tasks. Several practical methods have recently been\nproposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss\nof undesirable data. However, on certain unlearning tasks, these methods either\nfail to effectively unlearn the target data or suffer from catastrophic\ncollapse -- a drastic degradation of the model's utilities.\n  In this paper, we propose Negative Preference Optimization (NPO), a simple\nalignment-inspired method that could efficiently and effectively unlearn a\ntarget dataset. We theoretically show that the progression toward catastrophic\ncollapse by minimizing the NPO loss is exponentially slower than GA. Through\nexperiments on synthetic data and the benchmark TOFU dataset, we demonstrate\nthat NPO-based methods achieve a better balance between unlearning the\nundesirable data and maintaining the model's utilities. We also observe that\nNPO-based methods generate more sensible outputs than GA-based methods, whose\noutputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the\nfirst to achieve reasonable unlearning results in forgetting 50% (or more) of\nthe training data, whereas existing methods already struggle with forgetting\n10% of training data.",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 在预训练中记忆敏感或版权数据的问题，提出Negative Preference Optimization (NPO)，一种简单且基于alignment-inspired的unlearning方法，以有效消除目标数据的影响，同时避免gradient ascent (GA)方法可能导致的catastrophic collapse。NPO通过理论证明，其损失函数在引发模型崩溃方面的进展远比GA慢，并在合成数据和TOFU数据集的实验中实现了更好的平衡，即成功unlearning不desired数据而保持模型实用性。结果显示，NPO不仅生成更合理的输出，还首次在TOFU数据集上实现了忘记50%或更多训练数据时的合理unlearning效果，而现有方法在忘记10%数据时已显挣扎。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05868v2",
      "published_date": "2024-04-08 21:05:42 UTC",
      "updated_date": "2024-10-10 22:00:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:33:30.239109"
    },
    {
      "arxiv_id": "2404.05840v3",
      "title": "Attention-Driven Multi-Agent Reinforcement Learning: Enhancing Decisions with Expertise-Informed Tasks",
      "title_zh": "注意力驱动的多智能体强化学习：通过基于专业知识的任务增强决策",
      "authors": [
        "Andre R Kuroswiski",
        "Annie S Wu",
        "Angelo Passaro"
      ],
      "abstract": "In this paper, we introduce an alternative approach to enhancing Multi-Agent\nReinforcement Learning (MARL) through the integration of domain knowledge and\nattention-based policy mechanisms. Our methodology focuses on the incorporation\nof domain-specific expertise into the learning process, which simplifies the\ndevelopment of collaborative behaviors. This approach aims to reduce the\ncomplexity and learning overhead typically associated with MARL by enabling\nagents to concentrate on essential aspects of complex tasks, thus optimizing\nthe learning curve. The utilization of attention mechanisms plays a key role in\nour model. It allows for the effective processing of dynamic context data and\nnuanced agent interactions, leading to more refined decision-making. Applied in\nstandard MARL scenarios, such as the Stanford Intelligent Systems Laboratory\n(SISL) Pursuit and Multi-Particle Environments (MPE) Simple Spread, our method\nhas been shown to improve both learning efficiency and the effectiveness of\ncollaborative behaviors. The results indicate that our attention-based approach\ncan be a viable approach for improving the efficiency of MARL training process,\nintegrating domain-specific knowledge at the action level.",
      "tldr_zh": "本论文提出了一种基于注意力机制的 Multi-Agent Reinforcement Learning (MARL) 方法，通过整合领域特定专业知识来提升代理决策效率和协作行为。该方法简化了 MARL 的复杂性与学习开销，使代理能够专注于任务的核心方面，利用注意力机制有效处理动态上下文数据和代理互动。在标准场景如 SISL Pursuit 和 MPE Simple Spread 中，实验结果显示该方法显著提高了学习效率和协作行为的有效性，为 MARL 训练过程的优化提供了可行途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper was published at Proceedings of FLAIRS-37, May 19-21,\n  Sandestin Beach, FL. The proceedings version is available at\n  https://journals.flvc.org/FLAIRS/issue/view/6284",
      "pdf_url": "http://arxiv.org/pdf/2404.05840v3",
      "published_date": "2024-04-08 20:06:33 UTC",
      "updated_date": "2024-05-17 16:01:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:33:39.752721"
    },
    {
      "arxiv_id": "2404.05829v2",
      "title": "SambaLingo: Teaching Large Language Models New Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Zoltan Csaki",
        "Bo Li",
        "Jonathan Li",
        "Qiantong Xu",
        "Pian Pawakapan",
        "Leon Zhang",
        "Yun Du",
        "Hengyu Zhao",
        "Changran Hu",
        "Urmish Thakker"
      ],
      "abstract": "Despite the widespread availability of LLMs, there remains a substantial gap\nin their capabilities and availability across diverse languages. One approach\nto address these issues has been to take an existing pre-trained LLM and\ncontinue to train it on new languages. While prior works have experimented with\nlanguage adaptation, many questions around best practices and methodology have\nnot been covered. In this paper, we present a comprehensive investigation into\nthe adaptation of LLMs to new languages. Our study covers the key components in\nthis process, including vocabulary extension, direct preference optimization\nand the data scarcity problem for human alignment in low-resource languages. We\nscale these experiments across 9 languages and 2 parameter scales (7B and 70B).\nWe compare our models against Llama 2, Aya-101, XGLM, BLOOM and existing\nlanguage experts, outperforming all prior published baselines. Additionally,\nall evaluation code and checkpoints are made public to facilitate future\nresearch.",
      "tldr_zh": "本研究探讨了如何通过继续训练现有的大型语言模型(LLMs)来适应新语言，从而解决LLMs在多种语言能力上的差距。关键方法包括词汇扩展、直接偏好优化(Direct Preference Optimization)以及应对低资源语言的数据稀缺问题。实验覆盖9种语言和7B/70B参数规模，SambaLingo模型在性能上优于Llama 2、Aya-101、XGLM和BLOOM等现有基准。此外，所有评估代码和检查点已公开，以支持未来研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "23 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.05829v2",
      "published_date": "2024-04-08 19:48:36 UTC",
      "updated_date": "2024-07-17 20:30:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:33:53.323971"
    },
    {
      "arxiv_id": "2404.05825v1",
      "title": "LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language Models and Doc-Level Embedding",
      "title_zh": "LLM-Augmented Retrieval：通过语言模型和文档级嵌入增强检索模型",
      "authors": [
        "Mingrui Wu",
        "Sheng Cao"
      ],
      "abstract": "Recently embedding-based retrieval or dense retrieval have shown state of the\nart results, compared with traditional sparse or bag-of-words based approaches.\nThis paper introduces a model-agnostic doc-level embedding framework through\nlarge language model (LLM) augmentation. In addition, it also improves some\nimportant components in the retrieval model training process, such as negative\nsampling, loss function, etc. By implementing this LLM-augmented retrieval\nframework, we have been able to significantly improve the effectiveness of\nwidely-used retriever models such as Bi-encoders (Contriever, DRAGON) and\nlate-interaction models (ColBERTv2), thereby achieving state-of-the-art results\non LoTTE datasets and BEIR datasets.",
      "tldr_zh": "这篇论文提出了一种通过大型语言模型 (LLM) 增强的检索框架，专注于文档级嵌入，以提升基于嵌入的检索 (dense retrieval) 模型的性能。框架采用模型无关的方法，改进了训练过程中的关键组件，如负采样和损失函数，并应用于 Bi-encoders (如 Contriever 和 DRAGON) 以及 late-interaction 模型 (如 ColBERTv2)。实验结果显示，该框架在 LoTTE 和 BEIR 数据集上实现了最先进的效果，比传统方法显著提高了检索准确性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05825v1",
      "published_date": "2024-04-08 19:29:07 UTC",
      "updated_date": "2024-04-08 19:29:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:34:04.695778"
    },
    {
      "arxiv_id": "2404.05809v1",
      "title": "Self-Labeling in Multivariate Causality and Quantification for Adaptive Machine Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yutian Ren",
        "Aaron Haohua Yen",
        "G. P. Li"
      ],
      "abstract": "Adaptive machine learning (ML) aims to allow ML models to adapt to\never-changing environments with potential concept drift after model deployment.\nTraditionally, adaptive ML requires a new dataset to be manually labeled to\ntailor deployed models to altered data distributions. Recently, an interactive\ncausality based self-labeling method was proposed to autonomously associate\ncausally related data streams for domain adaptation, showing promising results\ncompared to traditional feature similarity-based semi-supervised learning.\nSeveral unanswered research questions remain, including self-labeling's\ncompatibility with multivariate causality and the quantitative analysis of the\nauxiliary models used in the self-labeling. The auxiliary models, the\ninteraction time model (ITM) and the effect state detector (ESD), are vital to\nthe success of self-labeling. This paper further develops the self-labeling\nframework and its theoretical foundations to address these research questions.\nA framework for the application of self-labeling to multivariate causal graphs\nis proposed using four basic causal relationships, and the impact of non-ideal\nITM and ESD performance is analyzed. A simulated experiment is conducted based\non a multivariate causal graph, validating the proposed theory.",
      "tldr_zh": "本研究探讨了自适应机器学习（Adaptive Machine Learning）中的自标记（Self-Labeling）方法，旨在处理模型部署后环境变化和概念漂移（concept drift）问题，而无需手动标记新数据集。论文扩展了交互式因果性基于的自标记框架，应用于多变量因果图（multivariate causal graphs），并基于四种基本因果关系分析了辅助模型——交互时间模型（ITM）和效果状态检测器（ESD）的性能影响。实验通过模拟多变量因果图验证了该理论，证明了自标记在提升域适应方面的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05809v1",
      "published_date": "2024-04-08 18:16:22 UTC",
      "updated_date": "2024-04-08 18:16:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:34:16.455016"
    },
    {
      "arxiv_id": "2404.05720v1",
      "title": "Language-Independent Representations Improve Zero-Shot Summarization",
      "title_zh": "语言无关表示改善零",
      "authors": [
        "Vladimir Solovyev",
        "Danni Liu",
        "Jan Niehues"
      ],
      "abstract": "Finetuning pretrained models on downstream generation tasks often leads to\ncatastrophic forgetting in zero-shot conditions. In this work, we focus on\nsummarization and tackle the problem through the lens of language-independent\nrepresentations. After training on monolingual summarization, we perform\nzero-shot transfer to new languages or language pairs. We first show naively\nfinetuned models are highly language-specific in both output behavior and\ninternal representations, resulting in poor zero-shot performance. Next, we\npropose query-key (QK) finetuning to decouple task-specific knowledge from the\npretrained language generation abilities. Then, after showing downsides of the\nstandard adversarial language classifier, we propose a balanced variant that\nmore directly enforces language-agnostic representations. Moreover, our\nqualitative analyses show removing source language identity correlates to\nzero-shot summarization performance. Our code is openly available.",
      "tldr_zh": "这篇论文探讨了微调预训练模型在零样本摘要（zero-shot summarization）中面临的灾难性遗忘问题，通过引入语言无关表示来提升性能。作者发现，标准微调模型在输出行为和内部表示上高度语言特定，导致零样本转移效果不佳。为此，他们提出 query-key (QK) finetuning 方法，以分离任务特定知识和预训练语言生成能力，并开发了一个平衡的对抗语言分类器（adversarial language classifier），更直接地强制执行语言无关表示。定性分析显示，移除源语言身份与零样本摘要性能正相关，该方法代码已公开可用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NAACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.05720v1",
      "published_date": "2024-04-08 17:56:43 UTC",
      "updated_date": "2024-04-08 17:56:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:34:28.913869"
    },
    {
      "arxiv_id": "2404.05783v2",
      "title": "A Survey on Responsible Generative AI: What to Generate and What Not",
      "title_zh": "翻译失败",
      "authors": [
        "Jindong Gu"
      ],
      "abstract": "In recent years, generative AI (GenAI), like large language models and\ntext-to-image models, has received significant attention across various\ndomains. However, ensuring the responsible generation of content by these\nmodels is crucial for their real-world applicability. This raises an\ninteresting question: What should responsible GenAI generate, and what should\nit not? To answer the question, this paper investigates the practical\nresponsible requirements of both textual and visual generative models,\noutlining five key considerations: generating truthful content, avoiding toxic\ncontent, refusing harmful instruction, leaking no training data-related\ncontent, and ensuring generated content identifiable. Specifically, we review\nrecent advancements and challenges in addressing these requirements. Besides,\nwe discuss and emphasize the importance of responsible GenAI across healthcare,\neducation, finance, and artificial general intelligence domains. Through a\nunified perspective on both textual and visual generative models, this paper\naims to provide insights into practical safety-related issues and further\nbenefit the community in building responsible GenAI.",
      "tldr_zh": "这篇调查论文探讨了生成式 AI（GenAI）的责任问题，重点分析文本和视觉生成模型在内容生成中的关键要求，包括生成真实内容、避免有毒内容、拒绝有害指令、不泄露训练数据以及确保内容可识别。作者回顾了最近的技术进展和挑战，并强调了这些责任在医疗、教育、金融和人工通用智能领域的实际重要性。通过统一的视角，该研究为构建更安全可靠的 GenAI 提供了宝贵见解和指导。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.CY",
      "comment": "77 pages, 10 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.05783v2",
      "published_date": "2024-04-08 17:53:21 UTC",
      "updated_date": "2024-09-03 16:23:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:34:41.388023"
    },
    {
      "arxiv_id": "2404.05717v3",
      "title": "SwapAnything: Enabling Arbitrary Object Swapping in Personalized Visual Editing",
      "title_zh": "翻译失败",
      "authors": [
        "Jing Gu",
        "Nanxuan Zhao",
        "Wei Xiong",
        "Qing Liu",
        "Zhifei Zhang",
        "He Zhang",
        "Jianming Zhang",
        "HyunJoon Jung",
        "Yilin Wang",
        "Xin Eric Wang"
      ],
      "abstract": "Effective editing of personal content holds a pivotal role in enabling\nindividuals to express their creativity, weaving captivating narratives within\ntheir visual stories, and elevate the overall quality and impact of their\nvisual content. Therefore, in this work, we introduce SwapAnything, a novel\nframework that can swap any objects in an image with personalized concepts\ngiven by the reference, while keeping the context unchanged. Compared with\nexisting methods for personalized subject swapping, SwapAnything has three\nunique advantages: (1) precise control of arbitrary objects and parts rather\nthan the main subject, (2) more faithful preservation of context pixels, (3)\nbetter adaptation of the personalized concept to the image. First, we propose\ntargeted variable swapping to apply region control over latent feature maps and\nswap masked variables for faithful context preservation and initial semantic\nconcept swapping. Then, we introduce appearance adaptation, to seamlessly adapt\nthe semantic concept into the original image in terms of target location,\nshape, style, and content during the image generation process. Extensive\nresults on both human and automatic evaluation demonstrate significant\nimprovements of our approach over baseline methods on personalized swapping.\nFurthermore, SwapAnything shows its precise and faithful swapping abilities\nacross single object, multiple objects, partial object, and cross-domain\nswapping tasks. SwapAnything also achieves great performance on text-based\nswapping and tasks beyond swapping such as object insertion.",
      "tldr_zh": "本研究引入了SwapAnything框架，用于在个性化视觉编辑中实现任意对象的交换，将参考图像中的个性化概念无缝整合到目标图像，同时保持上下文不变。该框架的优势包括对任意对象和部分的精确控制、更忠实的上下文像素保留，以及更好地适应个性化概念。具体方法包括targeted variable swapping，用于在潜在特征映射上应用区域控制以交换masked变量，从而实现初始语义交换，以及appearance adaptation，以无缝调整概念的位置、形状、风格和内容。实验结果显示，SwapAnything在单对象、多对象、部分对象和跨域交换任务上显著优于基线方法，并在文本-based交换和对象插入等扩展任务中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024, 23 pages, 14 figures, 3 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.05717v3",
      "published_date": "2024-04-08 17:52:29 UTC",
      "updated_date": "2024-10-03 17:56:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:34:52.649909"
    },
    {
      "arxiv_id": "2404.05695v2",
      "title": "Humanoid-Gym: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyang Gu",
        "Yen-Jen Wang",
        "Jianyu Chen"
      ],
      "abstract": "Humanoid-Gym is an easy-to-use reinforcement learning (RL) framework based on\nNvidia Isaac Gym, designed to train locomotion skills for humanoid robots,\nemphasizing zero-shot transfer from simulation to the real-world environment.\nHumanoid-Gym also integrates a sim-to-sim framework from Isaac Gym to Mujoco\nthat allows users to verify the trained policies in different physical\nsimulations to ensure the robustness and generalization of the policies. This\nframework is verified by RobotEra's XBot-S (1.2-meter tall humanoid robot) and\nXBot-L (1.65-meter tall humanoid robot) in a real-world environment with\nzero-shot sim-to-real transfer. The project website and source code can be\nfound at: https://sites.google.com/view/humanoid-gym/.",
      "tldr_zh": "Humanoid-Gym 是一个基于 Nvidia Isaac Gym 的易用强化学习（RL）框架，旨在训练人形机器人的运动技能，并强调 zero-shot sim2real transfer，实现从模拟环境直接到真实环境的无缝转移。该框架还整合了从 Isaac Gym 到 Mujoco 的 sim-to-sim 机制，允许用户在不同物理模拟中验证策略的鲁棒性和泛化能力。通过 RobotEra 的 XBot-S（1.2米高）和 XBot-L（1.65米高）机器人，在真实环境中进行了零样本转移验证，证明了框架的有效性。项目网站和源代码可访问：https://sites.google.com/view/humanoid-gym/。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05695v2",
      "published_date": "2024-04-08 17:26:28 UTC",
      "updated_date": "2024-05-18 10:00:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:35:05.881694"
    },
    {
      "arxiv_id": "2404.05694v2",
      "title": "Comprehensive Study on German Language Models for Clinical and Biomedical Text Understanding",
      "title_zh": "德语语言模型在临床和生物医学文本理解方面的全面研究",
      "authors": [
        "Ahmad Idrissi-Yaghir",
        "Amin Dada",
        "Henning Schäfer",
        "Kamyar Arzideh",
        "Giulia Baldini",
        "Jan Trienes",
        "Max Hasin",
        "Jeanette Bewersdorff",
        "Cynthia S. Schmidt",
        "Marie Bauer",
        "Kaleb E. Smith",
        "Jiang Bian",
        "Yonghui Wu",
        "Jörg Schlötterer",
        "Torsten Zesch",
        "Peter A. Horn",
        "Christin Seifert",
        "Felix Nensa",
        "Jens Kleesiek",
        "Christoph M. Friedrich"
      ],
      "abstract": "Recent advances in natural language processing (NLP) can be largely\nattributed to the advent of pre-trained language models such as BERT and\nRoBERTa. While these models demonstrate remarkable performance on general\ndatasets, they can struggle in specialized domains such as medicine, where\nunique domain-specific terminologies, domain-specific abbreviations, and\nvarying document structures are common. This paper explores strategies for\nadapting these models to domain-specific requirements, primarily through\ncontinuous pre-training on domain-specific data. We pre-trained several German\nmedical language models on 2.4B tokens derived from translated public English\nmedical data and 3B tokens of German clinical data. The resulting models were\nevaluated on various German downstream tasks, including named entity\nrecognition (NER), multi-label classification, and extractive question\nanswering. Our results suggest that models augmented by clinical and\ntranslation-based pre-training typically outperform general domain models in\nmedical contexts. We conclude that continuous pre-training has demonstrated the\nability to match or even exceed the performance of clinical models trained from\nscratch. Furthermore, pre-training on clinical data or leveraging translated\ntexts have proven to be reliable methods for domain adaptation in medical NLP\ntasks.",
      "tldr_zh": "这篇论文系统研究了预训练语言模型如 BERT 和 RoBERTa 在德语临床和生物医学文本理解中的应用挑战，重点探讨了领域特定术语、缩写和文档结构的适应策略。作者通过在2.4B tokens 的翻译自英语的公共医疗数据和3B tokens 的德语临床数据上进行连续预训练，开发了几种德语医疗语言模型。实验评估显示，这些增强模型在命名实体识别 (NER)、多标签分类和提取式问答等下游任务上，通常优于一般领域模型。研究结论表明，连续预训练能匹配或超过从零训练的临床模型，并证明使用临床数据或翻译文本是可靠的领域适应方法。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at LREC-COLING 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.05694v2",
      "published_date": "2024-04-08 17:24:04 UTC",
      "updated_date": "2024-05-08 08:53:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:35:18.613932"
    },
    {
      "arxiv_id": "2404.05689v2",
      "title": "Automated discovery of symbolic laws governing skill acquisition from naturally occurring data",
      "title_zh": "翻译失败",
      "authors": [
        "Sannyuya Liu",
        "Qing Li",
        "Xiaoxuan Shen",
        "Jianwen Sun",
        "Zongkai Yang"
      ],
      "abstract": "Skill acquisition is a key area of research in cognitive psychology as it\nencompasses multiple psychological processes. The laws discovered under\nexperimental paradigms are controversial and lack generalizability. This paper\naims to unearth the laws of skill learning from large-scale training log data.\nA two-stage algorithm was developed to tackle the issues of unobservable\ncognitive states and algorithmic explosion in searching. Initially a deep\nlearning model is employed to determine the learner's cognitive state and\nassess the feature importance. Subsequently, symbolic regression algorithms are\nutilized to parse the neural network model into algebraic equations.\nExperimental results show the algorithm can accurately restore preset laws\nwithin a noise range in continuous feedback settings. When applied to Lumosity\ntraining data, the method outperforms traditional and recent models in fitness\nterms. The study reveals two new forms of skill acquisition laws and reaffirms\nsome previous findings.",
      "tldr_zh": "这篇论文针对技能习得领域的争议和泛化性问题，提出从大规模自然训练日志数据中自动发现符号定律的方法。研究开发了一个两阶段算法：首先利用深度学习模型评估学习者的认知状态和特征重要性，其次通过 symbolic regression 算法将神经网络模型解析成代数方程。实验结果表明，该算法在噪声环境下能准确恢复预设定律，并在 Lumosity 数据上超越传统和最新模型，揭示了两种新的技能习得定律并确认了一些先前发现。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05689v2",
      "published_date": "2024-04-08 17:15:37 UTC",
      "updated_date": "2024-05-27 06:48:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:35:29.438754"
    },
    {
      "arxiv_id": "2404.05688v2",
      "title": "David and Goliath: An Empirical Evaluation of Attacks and Defenses for QNNs at the Deep Edge",
      "title_zh": "翻译失败",
      "authors": [
        "Miguel Costa",
        "Sandro Pinto"
      ],
      "abstract": "ML is shifting from the cloud to the edge. Edge computing reduces the surface\nexposing private data and enables reliable throughput guarantees in real-time\napplications. Of the panoply of devices deployed at the edge,\nresource-constrained MCUs, e.g., Arm Cortex-M, are more prevalent, orders of\nmagnitude cheaper, and less power-hungry than application processors or GPUs.\nThus, enabling intelligence at the deep edge is the zeitgeist, with researchers\nfocusing on unveiling novel approaches to deploy ANNs on these constrained\ndevices. Quantization is a well-established technique that has proved effective\nin enabling the deployment of neural networks on MCUs; however, it is still an\nopen question to understand the robustness of QNNs in the face of adversarial\nexamples.\n  To fill this gap, we empirically evaluate the effectiveness of attacks and\ndefenses from (full-precision) ANNs on (constrained) QNNs. Our evaluation\nincludes three QNNs targeting TinyML applications, ten attacks, and six\ndefenses. With this study, we draw a set of interesting findings. First,\nquantization increases the point distance to the decision boundary and leads\nthe gradient estimated by some attacks to explode or vanish. Second,\nquantization can act as a noise attenuator or amplifier, depending on the noise\nmagnitude, and causes gradient misalignment. Regarding adversarial defenses, we\nconclude that input pre-processing defenses show impressive results on small\nperturbations; however, they fall short as the perturbation increases. At the\nsame time, train-based defenses increase the average point distance to the\ndecision boundary, which holds after quantization. However, we argue that\ntrain-based defenses still need to smooth the quantization-shift and gradient\nmisalignment phenomenons to counteract adversarial example transferability to\nQNNs. All artifacts are open-sourced to enable independent validation of\nresults.",
      "tldr_zh": "这篇论文实证评估了在边缘计算环境中 Quantized Neural Networks (QNNs) 的攻击和防御效果，聚焦于资源受限的 MCU 设备，如 Arm Cortex-M，通过测试三个针对 TinyML 应用的 QNNs、十种攻击和六种防御。研究发现，Quantization 增加了点到决策边界的距离，导致某些攻击的梯度爆炸或消失，并可能作为噪声衰减器或放大器影响梯度对齐。防御方面，输入预处理防御对小扰动表现出色，但在大扰动下失效，而训练-based 防御虽提升了决策边界距离，却需进一步优化量化偏移和梯度失调问题，所有实验工件已开源以便验证。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "I.2.0"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05688v2",
      "published_date": "2024-04-08 17:14:32 UTC",
      "updated_date": "2024-05-02 20:09:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:35:43.192835"
    },
    {
      "arxiv_id": "2404.16047v1",
      "title": "From \"AI\" to Probabilistic Automation: How Does Anthropomorphization of Technical Systems Descriptions Influence Trust?",
      "title_zh": "从“AI”到概率自动化：技术系统描述的拟人化如何影响信任？",
      "authors": [
        "Nanna Inie",
        "Stefania Druga",
        "Peter Zukerman",
        "Emily M. Bender"
      ],
      "abstract": "This paper investigates the influence of anthropomorphized descriptions of\nso-called \"AI\" (artificial intelligence) systems on people's self-assessment of\ntrust in the system. Building on prior work, we define four categories of\nanthropomorphization (1. Properties of a cognizer, 2. Agency, 3. Biological\nmetaphors, and 4. Properties of a communicator). We use a survey-based approach\n(n=954) to investigate whether participants are likely to trust one of two\n(fictitious) \"AI\" systems by randomly assigning people to see either an\nanthropomorphized or a de-anthropomorphized description of the systems. We find\nthat participants are no more likely to trust anthropomorphized over\nde-anthropmorphized product descriptions overall. The type of product or system\nin combination with different anthropomorphic categories appears to exert\ngreater influence on trust than anthropomorphizing language alone, and age is\nthe only demographic factor that significantly correlates with people's\npreference for anthropomorphized or de-anthropomorphized descriptions. When\nelaborating on their choices, participants highlight factors such as lesser of\ntwo evils, lower or higher stakes contexts, and human favoritism as driving\nmotivations when choosing between product A and B, irrespective of whether they\nsaw an anthropomorphized or a de-anthropomorphized description of the product.\nOur results suggest that \"anthropomorphism\" in \"AI\" descriptions is an\naggregate concept that may influence different groups differently, and provide\nnuance to the discussion of whether anthropomorphization leads to higher trust\nand over-reliance by the general public in systems sold as \"AI\".",
      "tldr_zh": "本研究探讨了将“AI”系统描述拟人化（anthropomorphization）如何影响人们对系统的信任，定义了四类拟人化：Properties of a cognizer、Agency、Biological metaphors 和 Properties of a communicator。研究采用调查方法（n=954），通过随机分配参与者查看拟人化或非拟人化描述，评估信任水平。结果显示，整体上拟人化描述并未显著增加信任，产品类型与不同拟人化类别的结合以及年龄是主要影响因素；参与者在选择时更关注风险语境和偏好，而非描述类型。该研究为“AI”描述的拟人化问题提供了细致洞见，强调这可能对不同群体产生差异化影响，而非简单导致过度信任。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to FAccT 2024. arXiv admin note: text overlap with\n  arXiv:2403.05957",
      "pdf_url": "http://arxiv.org/pdf/2404.16047v1",
      "published_date": "2024-04-08 17:01:09 UTC",
      "updated_date": "2024-04-08 17:01:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:35:56.932457"
    },
    {
      "arxiv_id": "2404.05659v3",
      "title": "VietMed: A Dataset and Benchmark for Automatic Speech Recognition of Vietnamese in the Medical Domain",
      "title_zh": "翻译失败",
      "authors": [
        "Khai Le-Duc"
      ],
      "abstract": "Due to privacy restrictions, there's a shortage of publicly available speech\nrecognition datasets in the medical domain. In this work, we present VietMed -\na Vietnamese speech recognition dataset in the medical domain comprising 16h of\nlabeled medical speech, 1000h of unlabeled medical speech and 1200h of\nunlabeled general-domain speech. To our best knowledge, VietMed is by far the\nworld's largest public medical speech recognition dataset in 7 aspects: total\nduration, number of speakers, diseases, recording conditions, speaker roles,\nunique medical terms and accents. VietMed is also by far the largest public\nVietnamese speech dataset in terms of total duration. Additionally, we are the\nfirst to present a medical ASR dataset covering all ICD-10 disease groups and\nall accents within a country. Moreover, we release the first public large-scale\npre-trained models for Vietnamese ASR, w2v2-Viet and XLSR-53-Viet, along with\nthe first public large-scale fine-tuned models for medical ASR. Even without\nany medical data in unsupervised pre-training, our best pre-trained model\nXLSR-53-Viet generalizes very well to the medical domain by outperforming\nstate-of-the-art XLSR-53, from 51.8% to 29.6% WER on test set (a relative\nreduction of more than 40%). All code, data and models are made publicly\navailable: https://github.com/leduckhai/MultiMed/tree/master/VietMed.",
      "tldr_zh": "本研究介绍了VietMed数据集和基准，用于越南语医疗领域的Automatic Speech Recognition (ASR)，包括16小时标记的医疗语音、1000小时未标记的医疗语音和1200小时未标记的通用领域语音，是目前全球最大的公共医疗语音数据集，在总时长、说话者数量、疾病类型、录音条件、说话者角色、独特医疗术语和口音等方面领先。VietMed首次覆盖所有ICD-10疾病组和越南所有口音，同时开源了首个大规模预训练模型w2v2-Viet和XLSR-53-Viet，以及针对医疗ASR的微调模型。实验结果显示，即使未使用医疗数据进行预训练，XLSR-53-Viet在医疗领域仍表现出色，将Word Error Rate (WER)从51.8%降低到29.6%，实现了超过40%的相对减少。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "LREC-COLING 2024 (Oral), 24 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.05659v3",
      "published_date": "2024-04-08 16:43:52 UTC",
      "updated_date": "2025-04-04 15:06:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:36:07.661447"
    },
    {
      "arxiv_id": "2404.05656v2",
      "title": "Causality Extraction from Nuclear Licensee Event Reports Using a Hybrid Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Shahidur Rahoman Sohag",
        "Sai Zhang",
        "Min Xian",
        "Shoukun Sun",
        "Fei Xu",
        "Zhegang Ma"
      ],
      "abstract": "Industry-wide nuclear power plant operating experience is a critical source\nof raw data for performing parameter estimations in reliability and risk\nmodels. Much operating experience information pertains to failure events and is\nstored as reports containing unstructured data, such as narratives. Event\nreports are essential for understanding how failures are initiated and\npropagated, including the numerous causal relations involved. Causal relation\nextraction using deep learning represents a significant frontier in the field\nof natural language processing (NLP), and is crucial since it enables the\ninterpretation of intricate narratives and connections contained within vast\namounts of written information. This paper proposed a hybrid framework for\ncausality detection and extraction from nuclear licensee event reports. The\nmain contributions include: (1) we compiled an LER corpus with 20,129 text\nsamples for causality analysis, (2) developed an interactive tool for labeling\ncause effect pairs, (3) built a deep-learning-based approach for causal\nrelation detection, and (4) developed a knowledge based cause-effect extraction\napproach.",
      "tldr_zh": "这篇论文提出一个混合框架，用于从核电站许可事件报告（LER）中检测和提取因果关系，以提升核工业可靠性风险模型的参数估计。论文的主要贡献包括：编译了一个包含20,129个文本样本的LER语料库、开发了一个交互式工具用于标注原因-效果对、构建了一个基于深度学习的因果关系检测方法，以及开发了一个基于知识的因果提取方法。该框架通过整合深度学习和知识驱动技术，帮助更好地解读核事件报告中的复杂叙述和连接。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05656v2",
      "published_date": "2024-04-08 16:39:34 UTC",
      "updated_date": "2024-04-22 15:25:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:36:20.075904"
    },
    {
      "arxiv_id": "2404.05648v1",
      "title": "Resistive Memory-based Neural Differential Equation Solver for Score-based Diffusion Model",
      "title_zh": "基于电阻存储的神经微分方程求解器，用于基于分数的扩散模型",
      "authors": [
        "Jichang Yang",
        "Hegan Chen",
        "Jia Chen",
        "Songqi Wang",
        "Shaocong Wang",
        "Yifei Yu",
        "Xi Chen",
        "Bo Wang",
        "Xinyuan Zhang",
        "Binbin Cui",
        "Yi Li",
        "Ning Lin",
        "Meng Xu",
        "Yi Li",
        "Xiaoxin Xu",
        "Xiaojuan Qi",
        "Zhongrui Wang",
        "Xumeng Zhang",
        "Dashan Shang",
        "Han Wang",
        "Qi Liu",
        "Kwang-Ting Cheng",
        "Ming Liu"
      ],
      "abstract": "Human brains image complicated scenes when reading a novel. Replicating this\nimagination is one of the ultimate goals of AI-Generated Content (AIGC).\nHowever, current AIGC methods, such as score-based diffusion, are still\ndeficient in terms of rapidity and efficiency. This deficiency is rooted in the\ndifference between the brain and digital computers. Digital computers have\nphysically separated storage and processing units, resulting in frequent data\ntransfers during iterative calculations, incurring large time and energy\noverheads. This issue is further intensified by the conversion of inherently\ncontinuous and analog generation dynamics, which can be formulated by neural\ndifferential equations, into discrete and digital operations. Inspired by the\nbrain, we propose a time-continuous and analog in-memory neural differential\nequation solver for score-based diffusion, employing emerging resistive memory.\nThe integration of storage and computation within resistive memory synapses\nsurmount the von Neumann bottleneck, benefiting the generative speed and energy\nefficiency. The closed-loop feedback integrator is time-continuous, analog, and\ncompact, physically implementing an infinite-depth neural network. Moreover,\nthe software-hardware co-design is intrinsically robust to analog noise. We\nexperimentally validate our solution with 180 nm resistive memory in-memory\ncomputing macros. Demonstrating equivalent generative quality to the software\nbaseline, our system achieved remarkable enhancements in generative speed for\nboth unconditional and conditional generation tasks, by factors of 64.8 and\n156.5, respectively. Moreover, it accomplished reductions in energy consumption\nby factors of 5.2 and 4.1. Our approach heralds a new horizon for hardware\nsolutions in edge computing for generative AI applications.",
      "tldr_zh": "本研究针对基于分数的分扩散模型（score-based diffusion）在生成速度和效率上的不足，提出了一种基于电阻存储（resistive memory）的神经微分方程（neural differential equation）求解器。该方法受大脑启发，将存储和计算集成于电阻存储中，克服冯诺依曼瓶颈，并通过时间连续、模拟的闭环反馈积分器实现紧凑的无限深度神经网络，同时对模拟噪声具有鲁棒性。实验使用180 nm电阻存储宏验证了该系统，在无条件和条件生成任务中，生成速度分别提高了64.8和156.5倍，能量消耗减少了5.2和4.1倍，同时保持了与软件基线相当的生成质量，为边缘计算中的生成AI应用开辟了新路径。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.ET",
        "cs.NE"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05648v1",
      "published_date": "2024-04-08 16:34:35 UTC",
      "updated_date": "2024-04-08 16:34:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:36:32.766939"
    },
    {
      "arxiv_id": "2404.08684v1",
      "title": "Is English the New Programming Language? How About Pseudo-code Engineering?",
      "title_zh": "英语是新的编程语言吗？伪代码工程怎么样？",
      "authors": [
        "Gian Alexandre Michaelsen",
        "Renato P. dos Santos"
      ],
      "abstract": "Background: The integration of artificial intelligence (AI) into daily life,\nparticularly through chatbots utilizing natural language processing (NLP),\npresents both revolutionary potential and unique challenges. This intended to\ninvestigate how different input forms impact ChatGPT, a leading language model\nby OpenAI, performance in understanding and executing complex, multi-intention\ntasks. Design: Employing a case study methodology supplemented by discourse\nanalysis, the research analyzes ChatGPT's responses to inputs varying from\nnatural language to pseudo-code engineering. The study specifically examines\nthe model's proficiency across four categories: understanding of intentions,\ninterpretability, completeness, and creativity. Setting and Participants: As a\ntheoretical exploration of AI interaction, this study focuses on the analysis\nof structured and unstructured inputs processed by ChatGPT, without direct\nhuman participants. Data collection and analysis: The research utilizes\nsynthetic case scenarios, including the organization of a \"weekly meal plan\"\nand a \"shopping list,\" to assess ChatGPT's response to prompts in both natural\nlanguage and pseudo-code engineering. The analysis is grounded in the\nidentification of patterns, contradictions, and unique response elements across\ndifferent input formats. Results: Findings reveal that pseudo-code engineering\ninputs significantly enhance the clarity and determinism of ChatGPT's\nresponses, reducing ambiguity inherent in natural language. Enhanced natural\nlanguage, structured through prompt engineering techniques, similarly improves\nthe model's interpretability and creativity. Conclusions: The study underscores\nthe potential of pseudo-code engineering in refining human-AI interaction and\nachieving more deterministic, concise, and direct outcomes, advocating for its\nbroader application across disciplines requiring precise AI responses.",
      "tldr_zh": "这篇论文探讨了自然语言与伪代码工程作为输入形式对 ChatGPT 处理复杂多意图任务的影响，旨在评估模型在理解意图、可解释性、完整性和创造力方面的表现。研究采用案例研究和话语分析方法，使用合成场景（如组织“weekly meal plan”和“shopping list”）比较不同输入格式。结果显示，伪代码工程输入显著提升了响应的清晰度和确定性，减少了自然语言的模糊性，而增强的提示工程也改善了模型的性能；结论提倡在需要精确 AI 响应的领域推广 pseudo-code engineering，以优化人-AI 交互。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.CY",
        "J.4; K.3; I.2"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.08684v1",
      "published_date": "2024-04-08 16:28:52 UTC",
      "updated_date": "2024-04-08 16:28:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:36:44.290923"
    },
    {
      "arxiv_id": "2404.05639v1",
      "title": "Investigating the Impact of Quantization on Adversarial Robustness",
      "title_zh": "探究量化对对抗鲁棒性的影响",
      "authors": [
        "Qun Li",
        "Yuan Meng",
        "Chen Tang",
        "Jiacheng Jiang",
        "Zhi Wang"
      ],
      "abstract": "Quantization is a promising technique for reducing the bit-width of deep\nmodels to improve their runtime performance and storage efficiency, and thus\nbecomes a fundamental step for deployment. In real-world scenarios, quantized\nmodels are often faced with adversarial attacks which cause the model to make\nincorrect inferences by introducing slight perturbations. However, recent\nstudies have paid less attention to the impact of quantization on the model\nrobustness. More surprisingly, existing studies on this topic even present\ninconsistent conclusions, which prompted our in-depth investigation. In this\npaper, we conduct a first-time analysis of the impact of the quantization\npipeline components that can incorporate robust optimization under the settings\nof Post-Training Quantization and Quantization-Aware Training. Through our\ndetailed analysis, we discovered that this inconsistency arises from the use of\ndifferent pipelines in different studies, specifically regarding whether robust\noptimization is performed and at which quantization stage it occurs. Our\nresearch findings contribute insights into deploying more secure and robust\nquantized networks, assisting practitioners in reference for scenarios with\nhigh-security requirements and limited resources.",
      "tldr_zh": "这篇论文调查了量化(quantization)对模型对抗鲁棒性(adversarial robustness)的影响，强调量化在提高模型运行性能和存储效率的同时，可能导致模型在面对对抗攻击时表现不佳。作者首次分析了后训练量化(Post-Training Quantization)和量化感知训练(Quantization-Aware Training)中的量化管道组件，包括是否结合鲁棒优化及其阶段。研究发现，不一致的结论源于不同研究采用的管道差异，最终为部署更安全鲁棒的量化网络提供了关键见解，适用于高安全需求和资源有限的实际场景。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ICLR 2024 Workshop PML4LRS",
      "pdf_url": "http://arxiv.org/pdf/2404.05639v1",
      "published_date": "2024-04-08 16:20:15 UTC",
      "updated_date": "2024-04-08 16:20:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:36:57.062573"
    },
    {
      "arxiv_id": "2404.05624v1",
      "title": "LTNER: Large Language Model Tagging for Named Entity Recognition with Contextualized Entity Marking",
      "title_zh": "翻译失败",
      "authors": [
        "Faren Yan",
        "Peng Yu",
        "Xin Chen"
      ],
      "abstract": "The use of LLMs for natural language processing has become a popular trend in\nthe past two years, driven by their formidable capacity for context\ncomprehension and learning, which has inspired a wave of research from\nacademics and industry professionals. However, for certain NLP tasks, such as\nNER, the performance of LLMs still falls short when compared to supervised\nlearning methods. In our research, we developed a NER processing framework\ncalled LTNER that incorporates a revolutionary Contextualized Entity Marking\nGen Method. By leveraging the cost-effective GPT-3.5 coupled with context\nlearning that does not require additional training, we significantly improved\nthe accuracy of LLMs in handling NER tasks. The F1 score on the CoNLL03 dataset\nincreased from the initial 85.9% to 91.9%, approaching the performance of\nsupervised fine-tuning. This outcome has led to a deeper understanding of the\npotential of LLMs.",
      "tldr_zh": "本文提出 LTNER 框架，用于提升 Large Language Models (LLMs) 在 Named Entity Recognition (NER) 任务中的性能，以解决 LLMs 相比监督学习方法的不足。框架引入 Contextualized Entity Marking Gen Method，并利用 GPT-3.5 结合上下文学习进行优化，无需额外训练即可实现高效实体标记。在 CoNLL03 数据集上的实验显示，F1 分数从 85.9% 提高到 91.9%，接近监督微调的水平。该研究深化了对 LLMs 在 NER 任务潜力的理解，为无监督 NLP 应用提供了新路径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.05624v1",
      "published_date": "2024-04-08 15:54:02 UTC",
      "updated_date": "2024-04-08 15:54:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:37:08.675757"
    },
    {
      "arxiv_id": "2404.05613v1",
      "title": "Deep Representation Learning for Multi-functional Degradation Modeling of Community-dwelling Aging Population",
      "title_zh": "翻译失败",
      "authors": [
        "Suiyao Chen",
        "Xinyi Liu",
        "Yulei Li",
        "Jing Wu",
        "Handong Yao"
      ],
      "abstract": "As the aging population grows, particularly for the baby boomer generation,\nthe United States is witnessing a significant increase in the elderly\npopulation experiencing multifunctional disabilities. These disabilities,\nstemming from a variety of chronic diseases, injuries, and impairments, present\na complex challenge due to their multidimensional nature, encompassing both\nphysical and cognitive aspects. Traditional methods often use univariate\nregression-based methods to model and predict single degradation conditions and\nassume population homogeneity, which is inadequate to address the complexity\nand diversity of aging-related degradation. This study introduces a novel\nframework for multi-functional degradation modeling that captures the\nmultidimensional (e.g., physical and cognitive) and heterogeneous nature of\nelderly disabilities. Utilizing deep learning, our approach predicts health\ndegradation scores and uncovers latent heterogeneity from elderly health\nhistories, offering both efficient estimation and explainable insights into the\ndiverse effects and causes of aging-related degradation. A real-case study\ndemonstrates the effectiveness and marks a pivotal contribution to accurately\nmodeling the intricate dynamics of elderly degradation, and addresses the\nhealthcare challenges in the aging population.",
      "tldr_zh": "本研究针对社区居住的老龄化人群的多功能退化建模问题，指出传统单变量回归方法无法处理其多维度（如身体和认知）和异质性特征。论文提出一个新框架，利用 Deep Representation Learning 从老年健康历史中预测健康退化分数，并揭示潜在异质性，提供高效估计和可解释的洞见。该框架通过真实案例研究验证了其有效性，为应对老龄化人口的医疗挑战提供了关键贡献。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05613v1",
      "published_date": "2024-04-08 15:40:22 UTC",
      "updated_date": "2024-04-08 15:40:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:37:19.192894"
    },
    {
      "arxiv_id": "2404.05605v1",
      "title": "Graph Neural Networks Automated Design and Deployment on Device-Edge Co-Inference Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Ao Zhou",
        "Jianlei Yang",
        "Tong Qiao",
        "Yingjie Qi",
        "Zhi Yang",
        "Weisheng Zhao",
        "Chunming Hu"
      ],
      "abstract": "The key to device-edge co-inference paradigm is to partition models into\ncomputation-friendly and computation-intensive parts across the device and the\nedge, respectively. However, for Graph Neural Networks (GNNs), we find that\nsimply partitioning without altering their structures can hardly achieve the\nfull potential of the co-inference paradigm due to various\ncomputational-communication overheads of GNN operations over heterogeneous\ndevices. We present GCoDE, the first automatic framework for GNN that\ninnovatively Co-designs the architecture search and the mapping of each\noperation on Device-Edge hierarchies. GCoDE abstracts the device communication\nprocess into an explicit operation and fuses the search of architecture and the\noperations mapping in a unified space for joint-optimization. Also, the\nperformance-awareness approach, utilized in the constraint-based search process\nof GCoDE, enables effective evaluation of architecture efficiency in diverse\nheterogeneous systems. We implement the co-inference engine and runtime\ndispatcher in GCoDE to enhance the deployment efficiency. Experimental results\nshow that GCoDE can achieve up to $44.9\\times$ speedup and $98.2\\%$ energy\nreduction compared to existing approaches across various applications and\nsystem configurations.",
      "tldr_zh": "该研究针对 Graph Neural Networks (GNNs) 在设备-边缘协同推理系统中存在的计算和通信开销问题，提出了 GCoDE 框架，这是首个自动框架，联合优化 GNN 架构搜索和操作映射。GCoDE 将设备通信抽象为显式操作，并在统一空间内融合搜索过程，同时采用性能感知方法评估架构效率，并实现了协同推理引擎和运行时调度器。实验结果显示，在各种应用和系统配置中，GCoDE 相较现有方法实现了高达 44.9 倍的速度提升和 98.2% 的能源减少。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by DAC'24",
      "pdf_url": "http://arxiv.org/pdf/2404.05605v1",
      "published_date": "2024-04-08 15:25:25 UTC",
      "updated_date": "2024-04-08 15:25:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:37:33.521760"
    },
    {
      "arxiv_id": "2404.05603v1",
      "title": "Self-Explainable Affordance Learning with Embodied Caption",
      "title_zh": "翻译失败",
      "authors": [
        "Zhipeng Zhang",
        "Zhimin Wei",
        "Guolei Sun",
        "Peng Wang",
        "Luc Van Gool"
      ],
      "abstract": "In the field of visual affordance learning, previous methods mainly used\nabundant images or videos that delineate human behavior patterns to identify\naction possibility regions for object manipulation, with a variety of\napplications in robotic tasks. However, they encounter a main challenge of\naction ambiguity, illustrated by the vagueness like whether to beat or carry a\ndrum, and the complexities involved in processing intricate scenes. Moreover,\nit is important for human intervention to rectify robot errors in time. To\naddress these issues, we introduce Self-Explainable Affordance learning (SEA)\nwith embodied caption. This innovation enables robots to articulate their\nintentions and bridge the gap between explainable vision-language caption and\nvisual affordance learning. Due to a lack of appropriate dataset, we unveil a\npioneering dataset and metrics tailored for this task, which integrates images,\nheatmaps, and embodied captions. Furthermore, we propose a novel model to\neffectively combine affordance grounding with self-explanation in a simple but\nefficient manner. Extensive quantitative and qualitative experiments\ndemonstrate our method's effectiveness.",
      "tldr_zh": "本研究针对视觉 affordance learning 的行动模糊性和复杂场景处理挑战，提出了一种 Self-Explainable Affordance learning (SEA) 方法，利用 embodied caption 让机器人能够表达意图，并桥接 explainable vision-language caption 与 affordance learning。研究者创建了一个新数据集，包括图像、heatmaps 和 embodied captions，并设计了一个简单高效的模型，将 affordance grounding 与 self-explanation 相结合。实验结果显示，该方法在定量和定性评估中表现出色，有效提升了机器人在物体操作任务中的可解释性和准确性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05603v1",
      "published_date": "2024-04-08 15:22:38 UTC",
      "updated_date": "2024-04-08 15:22:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:37:43.789802"
    },
    {
      "arxiv_id": "2404.05779v2",
      "title": "Data Readiness for AI: A 360-Degree Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Kaveen Hiniduma",
        "Suren Byna",
        "Jean Luca Bez"
      ],
      "abstract": "Artificial Intelligence (AI) applications critically depend on data. Poor\nquality data produces inaccurate and ineffective AI models that may lead to\nincorrect or unsafe use. Evaluation of data readiness is a crucial step in\nimproving the quality and appropriateness of data usage for AI. R&D efforts\nhave been spent on improving data quality. However, standardized metrics for\nevaluating data readiness for use in AI training are still evolving. In this\nstudy, we perform a comprehensive survey of metrics used to verify data\nreadiness for AI training. This survey examines more than 140 papers published\nby ACM Digital Library, IEEE Xplore, journals such as Nature, Springer, and\nScience Direct, and online articles published by prominent AI experts. This\nsurvey aims to propose a taxonomy of data readiness for AI (DRAI) metrics for\nstructured and unstructured datasets. We anticipate that this taxonomy will\nlead to new standards for DRAI metrics that will be used for enhancing the\nquality, accuracy, and fairness of AI training and inference.",
      "tldr_zh": "本研究调查了数据准备就绪性（Data Readiness for AI, DRAI）对AI应用的重要性，强调了数据质量问题可能导致AI模型不准确或不安全，并呼吁标准化评估指标。研究者分析了超过140篇论文，包括来自ACM Digital Library、IEEE Xplore、Nature、Springer和Science Direct等来源，以及AI专家的在线文章，旨在为结构化和非结构化数据集提出DRAI指标的分类法（Taxonomy）。这项工作预计将推动新标准的发展，提升AI训练和推理的质量、准确性和公平性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.0; E.m"
      ],
      "primary_category": "cs.LG",
      "comment": "36 pages, 3 figures, 2 tables, submitted to ACM Computing Surveys",
      "pdf_url": "http://arxiv.org/pdf/2404.05779v2",
      "published_date": "2024-04-08 15:19:57 UTC",
      "updated_date": "2024-11-27 18:44:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:37:57.267908"
    },
    {
      "arxiv_id": "2404.05569v3",
      "title": "360$^\\circ$REA: Towards A Reusable Experience Accumulation with 360° Assessment for Multi-Agent System",
      "title_zh": "翻译失败",
      "authors": [
        "Shen Gao",
        "Hao Li",
        "Chengrui Huang",
        "Quan Tu",
        "Zhiliang Tian",
        "Minlie Huang",
        "Shuo Shang"
      ],
      "abstract": "Large language model agents have demonstrated remarkable advancements across\nvarious complex tasks. Recent works focus on optimizing the agent team or\nemploying self-reflection to iteratively solve complex tasks. Since these\nagents are all based on the same LLM, only conducting self-evaluation or\nremoving underperforming agents does not substantively enhance the capability\nof the agents. We argue that a comprehensive evaluation and accumulating\nexperience from evaluation feedback is an effective approach to improving\nsystem performance. In this paper, we propose Reusable Experience Accumulation\nwith 360$^\\circ$ Assessment (360$^\\circ$REA), a hierarchical multi-agent\nframework inspired by corporate organizational practices. The framework employs\na novel 360$^\\circ$ performance assessment method for multi-perspective\nperformance evaluation with fine-grained assessment. To enhance the capability\nof agents in addressing complex tasks, we introduce dual-level experience pool\nfor agents to accumulate experience through fine-grained assessment. Extensive\nexperiments on complex task datasets demonstrate the effectiveness of\n360$^\\circ$REA.",
      "tldr_zh": "该论文提出了一种名为360°REA的层次化多代理框架，旨在通过全面评估和经验积累提升大型语言模型代理（Large language model agents）在复杂任务中的性能。该框架借鉴企业组织实践，采用新型360°性能评估方法进行多视角的细粒度评估，并引入双层经验池（dual-level experience pool）让代理从评估反馈中积累可重用经验。与现有基于相同LLM的自评估方法不同，这种方法能实质性增强代理能力。在复杂任务数据集上的大量实验证明了360°REA的有效性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05569v3",
      "published_date": "2024-04-08 14:43:13 UTC",
      "updated_date": "2025-03-06 12:54:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:38:09.217158"
    },
    {
      "arxiv_id": "2404.05567v1",
      "title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Bowen Pan",
        "Yikang Shen",
        "Haokun Liu",
        "Mayank Mishra",
        "Gaoyuan Zhang",
        "Aude Oliva",
        "Colin Raffel",
        "Rameswar Panda"
      ],
      "abstract": "Mixture-of-Experts (MoE) language models can reduce computational costs by\n2-4$\\times$ compared to dense models without sacrificing performance, making\nthem more efficient in computation-bounded scenarios. However, MoE models\ngenerally require 2-4$\\times$ times more parameters to achieve comparable\nperformance to a dense model, which incurs larger GPU memory requirements and\nmakes MoE models less efficient in I/O-bounded scenarios like autoregressive\ngeneration. In this work, we propose a hybrid dense training and sparse\ninference framework for MoE models (DS-MoE) which achieves strong computation\nand parameter efficiency by employing dense computation across all experts\nduring training and sparse computation during inference. Our experiments on\ntraining LLMs demonstrate that our DS-MoE models are more parameter-efficient\nthan standard sparse MoEs and are on par with dense models in terms of total\nparameter size and performance while being computationally cheaper (activating\n30-40% of the model's parameters). Performance tests using vLLM show that our\nDS-MoE-6B model runs up to $1.86\\times$ faster than similar dense models like\nMistral-7B, and between $1.50\\times$ and $1.71\\times$ faster than comparable\nMoEs, such as DeepSeekMoE-16B and Qwen1.5-MoE-A2.7B.",
      "tldr_zh": "本研究重新审视了Mixture-of-Experts (MoE)语言模型的训练策略，提出了一种混合框架DS-MoE，通过在训练阶段采用dense computation（密集计算）而在推理阶段使用sparse computation（稀疏计算），以实现更高的参数和计算效率。相比传统MoE模型，DS-MoE在保持与dense模型相当的性能和总参数规模的同时，仅激活30-40%的参数，从而降低计算成本。实验结果显示，DS-MoE-6B模型在使用vLLM进行性能测试时，比Mistral-7B快1.86倍，并比DeepSeekMoE-16B和Qwen1.5-MoE-A2.7B快1.50-1.71倍，为高效的语言模型部署提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05567v1",
      "published_date": "2024-04-08 14:39:49 UTC",
      "updated_date": "2024-04-08 14:39:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:38:21.533898"
    },
    {
      "arxiv_id": "2404.05555v2",
      "title": "On the Convergence of Continual Learning with Adaptive Methods",
      "title_zh": "翻译失败",
      "authors": [
        "Seungyub Han",
        "Yeongmo Kim",
        "Taehyun Cho",
        "Jungwoo Lee"
      ],
      "abstract": "One of the objectives of continual learning is to prevent catastrophic\nforgetting in learning multiple tasks sequentially, and the existing solutions\nhave been driven by the conceptualization of the plasticity-stability dilemma.\nHowever, the convergence of continual learning for each sequential task is less\nstudied so far. In this paper, we provide a convergence analysis of\nmemory-based continual learning with stochastic gradient descent and empirical\nevidence that training current tasks causes the cumulative degradation of\nprevious tasks. We propose an adaptive method for nonconvex continual learning\n(NCCL), which adjusts step sizes of both previous and current tasks with the\ngradients. The proposed method can achieve the same convergence rate as the SGD\nmethod when the catastrophic forgetting term which we define in the paper is\nsuppressed at each iteration. Further, we demonstrate that the proposed\nalgorithm improves the performance of continual learning over existing methods\nfor several image classification tasks.",
      "tldr_zh": "这篇论文分析了基于记忆的持续学习（continual learning）在使用随机梯度下降（SGD）的收敛性，发现训练当前任务会导致先前任务的累积退化，从而加剧灾难性遗忘（catastrophic forgetting）。他们提出了一种自适应方法NCCL（nonconvex continual learning），通过调整先前和当前任务的步长基于梯度，抑制遗忘并实现与SGD相同的收敛率。实验结果显示，该方法在多个图像分类任务上显著提高了持续学习的性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Proceedings of the Thirty-Ninth Conference on Uncertainty in\n  Artificial Intelligence (UAI 2023), see\n  https://proceedings.mlr.press/v216/han23a.html",
      "pdf_url": "http://arxiv.org/pdf/2404.05555v2",
      "published_date": "2024-04-08 14:28:27 UTC",
      "updated_date": "2024-04-15 08:44:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:38:32.615059"
    },
    {
      "arxiv_id": "2404.05553v3",
      "title": "Alljoined1 -- A dataset for EEG-to-Image decoding",
      "title_zh": "翻译失败",
      "authors": [
        "Jonathan Xu",
        "Bruno Aristimunha",
        "Max Emanuel Feucht",
        "Emma Qian",
        "Charles Liu",
        "Tazik Shahjahan",
        "Martyna Spyra",
        "Steven Zifan Zhang",
        "Nicholas Short",
        "Jioh Kim",
        "Paula Perdomo",
        "Ricky Renfeng Mao",
        "Yashvir Sabharwal",
        "Michael Ahedor Moaz Shoura",
        "Adrian Nestor"
      ],
      "abstract": "We present Alljoined1, a dataset built specifically for EEG-to-Image\ndecoding. Recognizing that an extensive and unbiased sampling of neural\nresponses to visual stimuli is crucial for image reconstruction efforts, we\ncollected data from 8 participants looking at 10,000 natural images each. We\nhave currently gathered 46,080 epochs of brain responses recorded with a\n64-channel EEG headset. The dataset combines response-based stimulus timing,\nrepetition between blocks and sessions, and diverse image classes with the goal\nof improving signal quality. For transparency, we also provide data quality\nscores. We publicly release the dataset and all code at\nhttps://linktr.ee/alljoined1.",
      "tldr_zh": "本研究介绍了 Alljoined1 数据集，专为 EEG-to-Image decoding 设计，以支持脑响应到图像的重建任务。数据集从 8 名参与者收集了针对 10,000 张自然图像的脑响应数据，共计 46,080 个 epochs，使用 64 通道 EEG headset 记录，并通过响应-based 刺激定时、块间重复和多样图像类别来提升信号质量。数据集还提供了数据质量分数，并已公开发布相关代码，以促进进一步研究。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "I.5.1; I.6.3; I.2.6; K.3.2"
      ],
      "primary_category": "q-bio.NC",
      "comment": "8 Pages, 6 Figures",
      "pdf_url": "http://arxiv.org/pdf/2404.05553v3",
      "published_date": "2024-04-08 14:21:34 UTC",
      "updated_date": "2024-05-14 04:47:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:38:45.254484"
    },
    {
      "arxiv_id": "2404.05545v2",
      "title": "Evaluating Interventional Reasoning Capabilities of Large Language Models",
      "title_zh": "评估大型语言模型的干预推理",
      "authors": [
        "Tejas Kasetty",
        "Divyat Mahajan",
        "Gintare Karolina Dziugaite",
        "Alexandre Drouin",
        "Dhanya Sridhar"
      ],
      "abstract": "Numerous decision-making tasks require estimating causal effects under\ninterventions on different parts of a system. As practitioners consider using\nlarge language models (LLMs) to automate decisions, studying their causal\nreasoning capabilities becomes crucial. A recent line of work evaluates LLMs\nability to retrieve commonsense causal facts, but these evaluations do not\nsufficiently assess how LLMs reason about interventions. Motivated by the role\nthat interventions play in causal inference, in this paper, we conduct\nempirical analyses to evaluate whether LLMs can accurately update their\nknowledge of a data-generating process in response to an intervention. We\ncreate benchmarks that span diverse causal graphs (e.g., confounding,\nmediation) and variable types, and enable a study of intervention-based\nreasoning. These benchmarks allow us to isolate the ability of LLMs to\naccurately predict changes resulting from their ability to memorize facts or\nfind other shortcuts. We evaluate six LLMs on the benchmarks, finding that GPT\nmodels show promising accuracy at predicting the intervention effects.",
      "tldr_zh": "这篇论文评估了大型语言模型(LLMs)在干预性推理方面的能力，特别是它们是否能准确更新对数据生成过程的知识以响应干预。作者创建了基准测试，涵盖多样化的因果图（如混杂和调解）和变量类型，以隔离LLMs的真实推理能力而非依赖记忆或捷径。实验结果显示，六种LLMs中GPT模型在预测干预效果方面表现出色，为LLMs在因果决策任务中的应用提供了重要洞见。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ME"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.05545v2",
      "published_date": "2024-04-08 14:15:56 UTC",
      "updated_date": "2024-12-22 12:22:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:38:57.032277"
    },
    {
      "arxiv_id": "2404.05540v1",
      "title": "OPSD: an Offensive Persian Social media Dataset and its baseline evaluations",
      "title_zh": "翻译失败",
      "authors": [
        "Mehran Safayani",
        "Amir Sartipi",
        "Amir Hossein Ahmadi",
        "Parniyan Jalali",
        "Amir Hossein Mansouri",
        "Mohammad Bisheh-Niasar",
        "Zahra Pourbahman"
      ],
      "abstract": "The proliferation of hate speech and offensive comments on social media has\nbecome increasingly prevalent due to user activities. Such comments can have\ndetrimental effects on individuals' psychological well-being and social\nbehavior. While numerous datasets in the English language exist in this domain,\nfew equivalent resources are available for Persian language. To address this\ngap, this paper introduces two offensive datasets. The first dataset comprises\nannotations provided by domain experts, while the second consists of a large\ncollection of unlabeled data obtained through web crawling for unsupervised\nlearning purposes. To ensure the quality of the former dataset, a meticulous\nthree-stage labeling process was conducted, and kappa measures were computed to\nassess inter-annotator agreement. Furthermore, experiments were performed on\nthe dataset using state-of-the-art language models, both with and without\nemploying masked language modeling techniques, as well as machine learning\nalgorithms, in order to establish the baselines for the dataset using\ncontemporary cutting-edge approaches. The obtained F1-scores for the\nthree-class and two-class versions of the dataset were 76.9% and 89.9% for\nXLM-RoBERTa, respectively.",
      "tldr_zh": "该论文引入了OPSD数据集，这是针对波斯语（Persian）社交媒体的攻击性内容数据集，包括一个由领域专家通过三阶段标注过程创建的标注数据集，以及一个通过网络爬取的未标注大型数据集，并使用kappa measures评估标注者间一致性。数据集旨在填补波斯语领域资源不足的空白，支持仇恨言论和攻击性评论的研究。实验采用XLM-RoBERTa等最先进语言模型和机器学习算法进行基准评估，结果显示三类版本的F1-score为76.9%，二类版本为89.9%。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "16 pages, 5 figures, 8 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.05540v1",
      "published_date": "2024-04-08 14:08:56 UTC",
      "updated_date": "2024-04-08 14:08:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:39:08.873673"
    },
    {
      "arxiv_id": "2404.05534v1",
      "title": "Ordre public exceptions for algorithmic surveillance patents",
      "title_zh": "翻译失败",
      "authors": [
        "Alina Wernick"
      ],
      "abstract": "This chapter explores the role of patent protection in algorithmic\nsurveillance and whether ordre public exceptions from patentability should\napply to such patents, due to their potential to enable human rights\nviolations. It concludes that in most cases, it is undesirable to exclude\nalgorithmic surveillance patents from patentability, as the patent system is\nill-equipped to evaluate the impacts of the exploitation of such technologies.\nFurthermore, the disclosure of such patents has positive externalities from the\nsocietal perspective by opening the black box of surveillance for public\nscrutiny.",
      "tldr_zh": "本章探讨了专利保护在算法监视（algorithmic surveillance）中的作用，以及是否应因其潜在侵犯人权风险而应用专利性的 ordre public exceptions。该研究认为，在大多数情况下，不应将此类专利排除在专利性之外，因为专利系统缺乏评估这些技术利用影响的合适机制。此外，算法监视专利的披露具有积极的社会外部性，能揭开监视技术的黑箱，供公众审查和监督。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "14 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.05534v1",
      "published_date": "2024-04-08 14:00:50 UTC",
      "updated_date": "2024-04-08 14:00:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:39:19.914596"
    },
    {
      "arxiv_id": "2404.05530v2",
      "title": "Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data",
      "title_zh": "翻译失败",
      "authors": [
        "Tim Baumgärtner",
        "Yang Gao",
        "Dana Alon",
        "Donald Metzler"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) is a popular method for\naligning Language Models (LM) with human values and preferences. RLHF requires\na large number of preference pairs as training data, which are often used in\nboth the Supervised Fine-Tuning and Reward Model training and therefore\npublicly available datasets are commonly used. In this work, we study to what\nextent a malicious actor can manipulate the LMs generations by poisoning the\npreferences, i.e., injecting poisonous preference pairs into these datasets and\nthe RLHF training process. We propose strategies to build poisonous preference\npairs and test their performance by poisoning two widely used preference\ndatasets. Our results show that preference poisoning is highly effective:\ninjecting a small amount of poisonous data (1-5\\% of the original dataset), we\ncan effectively manipulate the LM to generate a target entity in a target\nsentiment (positive or negative). The findings from our experiments also shed\nlight on strategies to defend against the preference poisoning attack.",
      "tldr_zh": "这篇论文探讨了针对RLHF（Reinforcement Learning from Human Feedback）的攻击方法，通过注入有毒偏好数据（poisoned preference pairs）来操纵语言模型（LM）的生成输出。研究者提出了构建这些有毒偏好对的策略，并在两个常用数据集上进行了测试，结果显示注入少量数据（1-5%）即可有效控制LM生成特定实体并带有目标情感（正面或负面）。此外，实验还揭示了潜在的防御策略，以缓解这种偏好中毒攻击。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05530v2",
      "published_date": "2024-04-08 13:59:02 UTC",
      "updated_date": "2024-08-06 14:30:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:39:32.804929"
    },
    {
      "arxiv_id": "2404.05777v2",
      "title": "IA2: Leveraging Instance-Aware Index Advisor with Reinforcement Learning for Diverse Workloads",
      "title_zh": "翻译失败",
      "authors": [
        "Taiyi Wang",
        "Eiko Yoneki"
      ],
      "abstract": "This study introduces the Instance-Aware Index Advisor (IA2), a novel deep\nreinforcement learning (DRL)-based approach for optimizing index selection in\ndatabases facing large action spaces of potential candidates. IA2 introduces\nthe Twin Delayed Deep Deterministic Policy Gradient - Temporal Difference\nState-Wise Action Refinery (TD3-TD-SWAR) model, enabling efficient index\nselection by understanding workload-index dependencies and employing adaptive\naction masking. This method includes a comprehensive workload model, enhancing\nits ability to adapt to unseen workloads and ensuring robust performance across\ndiverse database environments. Evaluation on benchmarks such as TPC-H reveals\nIA2's suggested indexes' performance in enhancing runtime, securing a 40%\nreduction in runtime for complex TPC-H workloads compared to scenarios without\nindexes, and delivering a 20% improvement over existing state-of-the-art\nDRL-based index advisors.",
      "tldr_zh": "这篇论文引入了 IA2，一种基于深度强化学习 (DRL) 的实例感知索引顾问，用于优化数据库中大规模候选索引的选择，以适应多样化工作负载。IA2 采用 Twin Delayed Deep Deterministic Policy Gradient - Temporal Difference State-Wise Action Refinery (TD3-TD-SWAR) 模型，通过理解工作负载-索引依赖关系和自适应动作掩码，实现高效的索引推荐和适应未见工作负载的能力。在 TPC-H 基准测试中，IA2 建议的索引使运行时减少 40%，并比现有最先进 DRL 索引顾问提升 20%。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "EuroMLSys 24, April 22, 2024, Athens, Greece",
      "pdf_url": "http://arxiv.org/pdf/2404.05777v2",
      "published_date": "2024-04-08 13:40:26 UTC",
      "updated_date": "2024-04-10 08:23:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:39:45.239911"
    },
    {
      "arxiv_id": "2404.05508v1",
      "title": "Synergy of Large Language Model and Model Driven Engineering for Automated Development of Centralized Vehicular Systems",
      "title_zh": "大语言模型与模型驱动工程的协同作用，用于集中式车辆系统的自动化开发",
      "authors": [
        "Nenad Petrovic",
        "Fengjunjie Pan",
        "Krzysztof Lebioda",
        "Vahid Zolfaghari",
        "Sven Kirchner",
        "Nils Purschke",
        "Muhammad Aqib Khan",
        "Viktor Vorobev",
        "Alois Knoll"
      ],
      "abstract": "We present a prototype of a tool leveraging the synergy of model driven\nengineering (MDE) and Large Language Models (LLM) for the purpose of software\ndevelopment process automation in the automotive industry. In this approach,\nthe user-provided input is free form textual requirements, which are first\ntranslated to Ecore model instance representation using an LLM, which is\nafterwards checked for consistency using Object Constraint Language (OCL)\nrules. After successful consistency check, the model instance is fed as input\nto another LLM for the purpose of code generation. The generated code is\nevaluated in a simulated environment using CARLA simulator connected to an\nexample centralized vehicle architecture, in an emergency brake scenario.",
      "tldr_zh": "该研究提出了一种工具原型，结合 Large Language Models (LLM) 和 Model Driven Engineering (MDE)，以自动化汽车行业的软件开发过程。用户提供的自由形式文本需求首先通过 LLM 转换为 Ecore 模型实例，随后使用 Object Constraint Language (OCL) 规则进行一致性检查。检查通过后，模型实例输入另一个 LLM 生成代码，并通过 CARLA 模拟器在紧急制动场景的集中式车辆架构中进行评估。该方法展示了 LLM 和 MDE 协同作用的潜力，提升了软件开发的效率和可靠性。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "D.2.1; D.2.2; D.2.4; I.2.7; I.2.2; I.7.0"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05508v1",
      "published_date": "2024-04-08 13:28:11 UTC",
      "updated_date": "2024-04-08 13:28:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:39:58.460845"
    },
    {
      "arxiv_id": "2404.05502v1",
      "title": "PetKaz at SemEval-2024 Task 3: Advancing Emotion Classification with an LLM for Emotion-Cause Pair Extraction in Conversations",
      "title_zh": "翻译失败",
      "authors": [
        "Roman Kazakov",
        "Kseniia Petukhova",
        "Ekaterina Kochmar"
      ],
      "abstract": "In this paper, we present our submission to the SemEval-2023 Task~3 \"The\nCompetition of Multimodal Emotion Cause Analysis in Conversations\", focusing on\nextracting emotion-cause pairs from dialogs. Specifically, our approach relies\non combining fine-tuned GPT-3.5 for emotion classification and a BiLSTM-based\nneural network to detect causes. We score 2nd in the ranking for Subtask 1,\ndemonstrating the effectiveness of our approach through one of the highest\nweighted-average proportional F1 scores recorded at 0.264.",
      "tldr_zh": "本论文介绍了 PetKaz 系统在 SemEval-2024 Task 3 中的参赛方案，旨在通过 LLM 提升对话中的情感-原因对提取。方法结合了微调的 GPT-3.5 用于情感分类，以及 BiLSTM-based 神经网络来检测原因，从而实现高效的情感分析。实验结果显示，该方法在 Subtask 1 中排名第二，weighted-average proportional F1 scores 达到 0.264，证明了其有效性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 7 figures, 2 tables, to be published in the Proceedings of\n  the 18th International Workshop on Semantic Evaluation (SemEval-2024), for\n  associated code, see https://github.com/sachertort/petkaz-semeval-ecac",
      "pdf_url": "http://arxiv.org/pdf/2404.05502v1",
      "published_date": "2024-04-08 13:25:03 UTC",
      "updated_date": "2024-04-08 13:25:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:40:10.563605"
    },
    {
      "arxiv_id": "2404.05501v1",
      "title": "Data Science In Olfaction",
      "title_zh": "嗅觉中的数据科学",
      "authors": [
        "Vivek Agarwal",
        "Joshua Harvey",
        "Dmitry Rinberg",
        "Vasant Dhar"
      ],
      "abstract": "Advances in neural sensing technology are making it possible to observe the\nolfactory process in great detail. In this paper, we conceptualize smell from a\nData Science and AI perspective, that relates the properties of odorants to how\nthey are sensed and analyzed in the olfactory system from the nose to the\nbrain. Drawing distinctions to color vision, we argue that smell presents\nunique measurement challenges, including the complexity of stimuli, the high\ndimensionality of the sensory apparatus, as well as what constitutes ground\ntruth. In the face of these challenges, we argue for the centrality of\nodorant-receptor interactions in developing a theory of olfaction. Such a\ntheory is likely to find widespread industrial applications, and enhance our\nunderstanding of smell, and in the longer-term, how it relates to other senses\nand language. As an initial use case of the data, we present results using\nmachine learning-based classification of neural responses to odors as they are\nrecorded in the mouse olfactory bulb with calcium imaging.",
      "tldr_zh": "这篇论文从数据科学和AI视角审视嗅觉（olfaction），探讨气味物质（odorants）的属性如何与嗅觉系统（从鼻子到大脑）的感知和分析相关联。作者强调嗅觉的独特挑战，包括刺激的复杂性、感官装置的高维度以及ground truth的定义，并通过与颜色视觉（color vision）的比较，主张以气味物质-受体互动（odorant-receptor interactions）为核心发展嗅觉理论。该理论有望应用于工业领域、深化对嗅觉的理解，并连接其他感官与语言；作为初步用例，论文利用机器学习（machine learning）对小鼠嗅球（olfactory bulb）的神经响应进行分类，基于钙成像（calcium imaging）记录，展示了这一方法的初步成效。",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "20 pages, 10 Figures, 2 Appendix, 1 Table",
      "pdf_url": "http://arxiv.org/pdf/2404.05501v1",
      "published_date": "2024-04-08 13:25:02 UTC",
      "updated_date": "2024-04-08 13:25:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:40:22.764163"
    },
    {
      "arxiv_id": "2404.05499v3",
      "title": "Guiding Large Language Models to Generate Computer-Parsable Content",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaye Wang"
      ],
      "abstract": "We propose a method to guide Large Language Models (LLMs) in generating\nstructured content adhering to specific conventions without fine-tuning. By\nutilizing coroutine-based content generation constraints through a pre-agreed\ncontext-free grammar (CFG), LLMs are directed during decoding to produce formal\nlanguage compliant outputs. This enhances stability and consistency in\ngenerating target data structures, types, or instructions, reducing application\ndevelopment complexities. Experimentally, error rates of GPT-2 and Gemma exceed\n95% for DSLs longer than 36 and 282 tokens, respectively. We introduce\nYieldLang, a coroutine-based DSL generation framework, and evaluate it with\nLLMs on various tasks including JSON and Mermaid flowchart generation. Compared\nto benchmarks, our approach improves accuracy by 1.09 to 11.6 times, with LLMs\nrequiring only about 16.5% of the samples to generate JSON effectively. This\nenhances usability of LLM-generated content for computer programs.",
      "tldr_zh": "该论文提出了一种无需微调的方法，利用预先同意的上下文无关文法（CFG）和基于协程的生成约束，来指导Large Language Models (LLMs)生成符合特定规范的结构化内容，从而提升输出稳定性与一致性。研究引入了YieldLang，一个基于协程的DSL生成框架，并在JSON和Mermaid流程图等任务上进行评估。实验结果显示，与基准模型相比，该方法将LLMs的准确率提高了1.09到11.6倍，仅需约16.5%的样本即可有效生成JSON，进一步简化了LLM内容在计算机程序中的应用。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "44 pages, 39 figures, 8 tables, Chinese version:\n  https://chinaxiv.org/abs/202403.00340",
      "pdf_url": "http://arxiv.org/pdf/2404.05499v3",
      "published_date": "2024-04-08 13:22:24 UTC",
      "updated_date": "2024-04-21 14:45:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:40:34.972745"
    },
    {
      "arxiv_id": "2404.05483v1",
      "title": "PetKaz at SemEval-2024 Task 8: Can Linguistics Capture the Specifics of LLM-generated Text?",
      "title_zh": "翻译失败",
      "authors": [
        "Kseniia Petukhova",
        "Roman Kazakov",
        "Ekaterina Kochmar"
      ],
      "abstract": "In this paper, we present our submission to the SemEval-2024 Task 8\n\"Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text\nDetection\", focusing on the detection of machine-generated texts (MGTs) in\nEnglish. Specifically, our approach relies on combining embeddings from the\nRoBERTa-base with diversity features and uses a resampled training set. We\nscore 12th from 124 in the ranking for Subtask A (monolingual track), and our\nresults show that our approach is generalizable across unseen models and\ndomains, achieving an accuracy of 0.91.",
      "tldr_zh": "本论文探讨了语言学是否能有效捕捉LLM生成文本的特性，针对SemEval-2024 Task 8的多生成器、多领域和多语言黑盒机器生成文本检测任务，提交了PetKaz系统。他们的方法结合RoBERTa-base的嵌入、diversity features以及resampled training set，实现了对英语MGTs的检测。在Subtask A（单语轨道）中，该系统在124参赛者中排名第12，准确率达到0.91，并证明了其在未见模型和领域上的泛化能力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "8 pages, 3 figures, 5 tables, to be published in the Proceedings of\n  the 18th International Workshop on Semantic Evaluation (SemEval-2024), for\n  associated code, see https://github.com/sachertort/petkaz-semeval-m4",
      "pdf_url": "http://arxiv.org/pdf/2404.05483v1",
      "published_date": "2024-04-08 13:05:02 UTC",
      "updated_date": "2024-04-08 13:05:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:40:46.801974"
    },
    {
      "arxiv_id": "2405.18437v1",
      "title": "Transductive Zero-Shot and Few-Shot CLIP",
      "title_zh": "翻译失败",
      "authors": [
        "Ségolène Martin",
        "Yunshi Huang",
        "Fereshteh Shakeri",
        "Jean-Christophe Pesquet",
        "Ismail Ben Ayed"
      ],
      "abstract": "Transductive inference has been widely investigated in few-shot image\nclassification, but completely overlooked in the recent, fast growing\nliterature on adapting vision-langage models like CLIP. This paper addresses\nthe transductive zero-shot and few-shot CLIP classification challenge, in which\ninference is performed jointly across a mini-batch of unlabeled query samples,\nrather than treating each instance independently. We initially construct\ninformative vision-text probability features, leading to a classification\nproblem on the unit simplex set. Inspired by Expectation-Maximization (EM), our\noptimization-based classification objective models the data probability\ndistribution for each class using a Dirichlet law. The minimization problem is\nthen tackled with a novel block Majorization-Minimization algorithm, which\nsimultaneously estimates the distribution parameters and class assignments.\nExtensive numerical experiments on 11 datasets underscore the benefits and\nefficacy of our batch inference approach.On zero-shot tasks with test batches\nof 75 samples, our approach yields near 20% improvement in ImageNet accuracy\nover CLIP's zero-shot performance. Additionally, we outperform state-of-the-art\nmethods in the few-shot setting. The code is available at:\nhttps://github.com/SegoleneMartin/transductive-CLIP.",
      "tldr_zh": "本论文提出了一种 transductive 推理方法，应用于 CLIP 的 zero-shot 和 few-shot 分类任务，通过联合处理未标记查询样本的 mini-batch，而不是独立处理每个样本，从而提升分类性能。\n方法首先构建视觉-文本概率特征，将问题转化为 unit simplex 上的分类优化问题，并基于 Expectation-Maximization (EM) 算法，使用 Dirichlet 分布建模每个类别的概率分布，然后通过 block Majorization-Minimization 算法同时估计参数和类分配。\n实验结果显示，在 11 个数据集上，该方法在 zero-shot 任务中使 ImageNet 准确率提高了近 20%，并在 few-shot 设置中超越了最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "2024 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), Jun 2024, Seattle (USA), Washington, United States",
      "pdf_url": "http://arxiv.org/pdf/2405.18437v1",
      "published_date": "2024-04-08 12:44:31 UTC",
      "updated_date": "2024-04-08 12:44:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:40:59.893155"
    },
    {
      "arxiv_id": "2404.05458v1",
      "title": "Teaching Higher-Order Logic Using Isabelle",
      "title_zh": "使用 Isabelle 教授高阶逻辑",
      "authors": [
        "Simon Tobias Lund",
        "Jørgen Villadsen"
      ],
      "abstract": "We present a formalization of higher-order logic in the Isabelle proof\nassistant, building directly on the foundational framework Isabelle/Pure and\ndeveloped to be as small and readable as possible. It should therefore serve as\na good introduction for someone looking into learning about higher-order logic\nand proof assistants, without having to study the much more complex\nIsabelle/HOL with heavier automation. To showcase our development and approach\nwe explain a sample proof, describe the axioms and rules of our higher-order\nlogic, and discuss our experience with teaching the subject in a classroom\nsetting.",
      "tldr_zh": "本研究在Isabelle证明助手中使用Isabelle/Pure框架形式化了高阶逻辑（higher-order logic），旨在创建一个尽可能小巧且易读的系统。相比更复杂的Isabelle/HOL，这种方法适合初学者学习高阶逻辑和证明助手，而无需依赖繁重的自动化工具。作者通过展示样本证明、解释高阶逻辑的公理和规则，并分享课堂教学经验，证明了这一开发的实用性和教育价值。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "F.4; I.2.3; K.3.1"
      ],
      "primary_category": "cs.LO",
      "comment": "In Proceedings ThEdu'23, arXiv:2404.03709",
      "pdf_url": "http://arxiv.org/pdf/2404.05458v1",
      "published_date": "2024-04-08 12:40:27 UTC",
      "updated_date": "2024-04-08 12:40:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:41:09.254837"
    },
    {
      "arxiv_id": "2404.05440v1",
      "title": "Tree Search-Based Policy Optimization under Stochastic Execution Delay",
      "title_zh": "翻译失败",
      "authors": [
        "David Valensi",
        "Esther Derman",
        "Shie Mannor",
        "Gal Dalal"
      ],
      "abstract": "The standard formulation of Markov decision processes (MDPs) assumes that the\nagent's decisions are executed immediately. However, in numerous realistic\napplications such as robotics or healthcare, actions are performed with a delay\nwhose value can even be stochastic. In this work, we introduce stochastic\ndelayed execution MDPs, a new formalism addressing random delays without\nresorting to state augmentation. We show that given observed delay values, it\nis sufficient to perform a policy search in the class of Markov policies in\norder to reach optimal performance, thus extending the deterministic fixed\ndelay case. Armed with this insight, we devise DEZ, a model-based algorithm\nthat optimizes over the class of Markov policies. DEZ leverages Monte-Carlo\ntree search similar to its non-delayed variant EfficientZero to accurately\ninfer future states from the action queue. Thus, it handles delayed execution\nwhile preserving the sample efficiency of EfficientZero. Through a series of\nexperiments on the Atari suite, we demonstrate that although the previous\nbaseline outperforms the naive method in scenarios with constant delay, it\nunderperforms in the face of stochastic delays. In contrast, our approach\nsignificantly outperforms the baselines, for both constant and stochastic\ndelays. The code is available at http://github.com/davidva1/Delayed-EZ .",
      "tldr_zh": "这篇论文引入了随机延迟执行 MDP（stochastic delayed execution MDPs），一种新的形式主义，用于处理现实应用（如机器人或医疗领域）中动作执行延迟随机的情况，而不依赖状态增强。研究证明，在观察到延迟值时，仅需在 Markov policies 类中进行策略搜索即可实现最优性能，从而扩展了确定性延迟的理论。作者开发了 DEZ 算法，这是一个基于模型的优化方法，利用 Monte-Carlo tree search（类似于 EfficientZero）从动作队列推断未来状态，以提高样本效率。在 Atari 游戏套件的实验中，DEZ 在恒定和随机延迟场景下均显著优于基线算法，展示了其鲁棒性和有效性。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Published in ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.05440v1",
      "published_date": "2024-04-08 12:19:04 UTC",
      "updated_date": "2024-04-08 12:19:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:41:22.559863"
    },
    {
      "arxiv_id": "2404.05427v3",
      "title": "AutoCodeRover: Autonomous Program Improvement",
      "title_zh": "AutoCodeRover: 自治程序改进",
      "authors": [
        "Yuntong Zhang",
        "Haifeng Ruan",
        "Zhiyu Fan",
        "Abhik Roychoudhury"
      ],
      "abstract": "Researchers have made significant progress in automating the software\ndevelopment process in the past decades. Recent progress in Large Language\nModels (LLMs) has significantly impacted the development process, where\ndevelopers can use LLM-based programming assistants to achieve automated\ncoding. Nevertheless, software engineering involves the process of program\nimprovement apart from coding, specifically to enable software maintenance\n(e.g. bug fixing) and software evolution (e.g. feature additions). In this\npaper, we propose an automated approach for solving GitHub issues to\nautonomously achieve program improvement. In our approach called AutoCodeRover,\nLLMs are combined with sophisticated code search capabilities, ultimately\nleading to a program modification or patch. In contrast to recent LLM agent\napproaches from AI researchers and practitioners, our outlook is more software\nengineering oriented. We work on a program representation (abstract syntax\ntree) as opposed to viewing a software project as a mere collection of files.\nOur code search exploits the program structure in the form of classes/methods\nto enhance LLM's understanding of the issue's root cause, and effectively\nretrieve a context via iterative search. The use of spectrum-based fault\nlocalization using tests, further sharpens the context, as long as a test-suite\nis available. Experiments on SWE-bench-lite (300 real-life GitHub issues) show\nincreased efficacy in solving GitHub issues (19% on SWE-bench-lite), which is\nhigher than the efficacy of the recently reported SWE-agent. In addition,\nAutoCodeRover achieved this efficacy with significantly lower cost (on average,\n$0.43 USD), compared to other baselines. We posit that our workflow enables\nautonomous software engineering, where, in future, auto-generated code from\nLLMs can be autonomously improved.",
      "tldr_zh": "本论文提出 AutoCodeRover，一种自主程序改进方法，旨在自动解决 GitHub issues 以实现软件维护（如 bug 修复）和演化（如功能添加）。该方法结合 Large Language Models (LLMs) 与高级代码搜索能力，利用抽象 syntax tree 结构进行迭代搜索，并通过 spectrum-based fault localization 精炼上下文，从而提升 LLM 对问题根因的理解。实验在 SWE-bench-lite 数据集（300 个真实 GitHub issues）上显示，AutoCodeRover 的有效性提高了 19%，高于 SWE-agent 基线，且平均成本仅为 0.43 USD。该框架为未来的自主软件工程铺平道路，允许 LLM 生成的代码实现自动改进。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "To appear in ISSTA 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.05427v3",
      "published_date": "2024-04-08 11:55:09 UTC",
      "updated_date": "2024-07-25 16:54:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:41:35.379254"
    },
    {
      "arxiv_id": "2404.05424v2",
      "title": "What Are the Odds? Improving the foundations of Statistical Model Checking",
      "title_zh": "翻译失败",
      "authors": [
        "Tobias Meggendorfer",
        "Maximilian Weininger",
        "Patrick Wienhöft"
      ],
      "abstract": "Markov decision processes (MDPs) are a fundamental model for decision making\nunder uncertainty. They exhibit non-deterministic choice as well as\nprobabilistic uncertainty. Traditionally, verification algorithms assume exact\nknowledge of the probabilities that govern the behaviour of an MDP. As this\nassumption is often unrealistic in practice, statistical model checking (SMC)\nwas developed in the past two decades. It allows to analyse MDPs with unknown\ntransition probabilities and provide probably approximately correct (PAC)\nguarantees on the result. Model-based SMC algorithms sample the MDP and build a\nmodel of it by estimating all transition probabilities, essentially for every\ntransition answering the question: ``What are the odds?'' However, so far the\nstatistical methods employed by the state of the art SMC algorithms are quite\nnaive. Our contribution are several fundamental improvements to those methods:\nOn the one hand, we survey statistics literature for better concentration\ninequalities; on the other hand, we propose specialised approaches that exploit\nour knowledge of the MDP. Our improvements are generally applicable to many\nkinds of problem statements because they are largely independent of the\nsetting. Moreover, our experimental evaluation shows that they lead to\nsignificant gains, reducing the number of samples that the SMC algorithm has to\ncollect by up to two orders of magnitude.",
      "tldr_zh": "本文针对Markov decision processes (MDPs)中不确定概率的问题，改进了Statistical Model Checking (SMC)的核心方法。通过调查更先进的concentration inequalities并提出利用MDP知识的专用统计方法，该研究增强了SMC算法的准确性和效率。这些改进适用于多种问题设置，并通过实验证明，可将所需样本数量减少多达两个数量级，提供更可靠的probably approximately correct (PAC)保证。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05424v2",
      "published_date": "2024-04-08 11:47:46 UTC",
      "updated_date": "2025-04-17 14:33:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:41:45.589840"
    },
    {
      "arxiv_id": "2404.05423v1",
      "title": "Residual Chain Prediction for Autonomous Driving Path Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Liguo Zhou",
        "Yirui Zhou",
        "Huaming Liu",
        "Alois Knoll"
      ],
      "abstract": "In the rapidly evolving field of autonomous driving systems, the refinement\nof path planning algorithms is paramount for navigating vehicles through\ndynamic environments, particularly in complex urban scenarios. Traditional path\nplanning algorithms, which are heavily reliant on static rules and manually\ndefined parameters, often fall short in such contexts, highlighting the need\nfor more adaptive, learning-based approaches. Among these, behavior cloning\nemerges as a noteworthy strategy for its simplicity and efficiency, especially\nwithin the realm of end-to-end path planning. However, behavior cloning faces\nchallenges, such as covariate shift when employing traditional Manhattan\ndistance as the metric. Addressing this, our study introduces the novel concept\nof Residual Chain Loss. Residual Chain Loss dynamically adjusts the loss\ncalculation process to enhance the temporal dependency and accuracy of\npredicted path points, significantly improving the model's performance without\nadditional computational overhead. Through testing on the nuScenes dataset, we\nunderscore the method's substantial advancements in addressing covariate shift,\nfacilitating dynamic loss adjustments, and ensuring seamless integration with\nend-to-end path planning frameworks. Our findings highlight the potential of\nResidual Chain Loss to revolutionize planning component of autonomous driving\nsystems, marking a significant step forward in the quest for level 5 autonomous\ndriving system.",
      "tldr_zh": "这篇论文针对自动驾驶路径规划的挑战，指出传统算法和 behavior cloning 方法在复杂城市环境中易受 covariate shift 影响，导致预测不准。论文引入了创新的 Residual Chain Loss 概念，通过动态调整损失计算来强化路径点的时序依赖性和准确性，同时不增加计算开销。在 nuScenes 数据集上的实验验证了该方法的有效性，显著提升了模型性能，并为实现 Level 5 自动驾驶系统提供了重要进展。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.05423v1",
      "published_date": "2024-04-08 11:43:40 UTC",
      "updated_date": "2024-04-08 11:43:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:41:58.685690"
    },
    {
      "arxiv_id": "2404.05417v1",
      "title": "Indexing Analytics to Instances: How Integrating a Dashboard can Support Design Education",
      "title_zh": "将分析指标索引到实例：整合仪表板如何支持设计教育",
      "authors": [
        "Ajit Jain",
        "Andruid Kerne",
        "Nic Lupfer",
        "Gabriel Britain",
        "Aaron Perrine",
        "Yoonsuck Choe",
        "John Keyser",
        "Ruihong Huang",
        "Jinsil Seo",
        "Annie Sungkajun",
        "Robert Lightfoot",
        "Timothy McGuire"
      ],
      "abstract": "We investigate how to use AI-based analytics to support design education. The\nanalytics at hand measure multiscale design, that is, students' use of space\nand scale to visually and conceptually organize their design work. With the\ngoal of making the analytics intelligible to instructors, we developed a\nresearch artifact integrating a design analytics dashboard with design\ninstances, and the design environment that students use to create them. We\ntheorize about how Suchman's notion of mutual intelligibility requires\ncontextualized investigation of AI in order to develop findings about how\nanalytics work for people. We studied the research artifact in 5 situated\ncourse contexts, in 3 departments. A total of 236 students used the multiscale\ndesign environment. The 9 instructors who taught those students experienced the\nanalytics via the new research artifact.\n  We derive findings from a qualitative analysis of interviews with instructors\nregarding their experiences. Instructors reflected on how the analytics and\ntheir presentation in the dashboard have the potential to affect design\neducation. We develop research implications addressing: (1) how indexing design\nanalytics in the dashboard to actual design work instances helps design\ninstructors reflect on what they mean and, more broadly, is a technique for how\nAI-based design analytics can support instructors' assessment and feedback\nexperiences in situated course contexts; and (2) how multiscale design\nanalytics, in particular, have the potential to support design education. By\nindexing, we mean linking which provides context, here connecting the numbers\nof the analytics with visually annotated design work instances.",
      "tldr_zh": "本研究探讨了如何利用 AI-based analytics 支持设计教育，特别关注多尺度设计分析（multiscale design），通过开发一个整合设计分析 dashboard 的研究工件，将分析数据与实际设计实例和环境相连。研究采用 Suchman's 相互可理解性概念，在 5 个真实课程环境中进行定性分析，涉及 236 名学生和 9 名指导老师。结果显示，这种 indexing 技术（即将分析数据与可视化设计实例链接）有助于指导老师更好地反思分析含义，提升评估和反馈体验，并证明多尺度设计分析具有潜力改善设计教育整体效果。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY",
        "H.5.2"
      ],
      "primary_category": "cs.HC",
      "comment": "22 pages, 4 figures, Submitted to ACM DIS",
      "pdf_url": "http://arxiv.org/pdf/2404.05417v1",
      "published_date": "2024-04-08 11:33:58 UTC",
      "updated_date": "2024-04-08 11:33:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:42:09.935690"
    },
    {
      "arxiv_id": "2404.05415v2",
      "title": "Relation Extraction Using Large Language Models: A Case Study on Acupuncture Point Locations",
      "title_zh": "使用大型语言模型的关系抽取：针灸穴位位置的案例研究",
      "authors": [
        "Yiming Li",
        "Xueqing Peng",
        "Jianfu Li",
        "Xu Zuo",
        "Suyuan Peng",
        "Donghong Pei",
        "Cui Tao",
        "Hua Xu",
        "Na Hong"
      ],
      "abstract": "In acupuncture therapy, the accurate location of acupoints is essential for\nits effectiveness. The advanced language understanding capabilities of large\nlanguage models (LLMs) like Generative Pre-trained Transformers (GPT) present a\nsignificant opportunity for extracting relations related to acupoint locations\nfrom textual knowledge sources. This study aims to compare the performance of\nGPT with traditional deep learning models (Long Short-Term Memory (LSTM) and\nBidirectional Encoder Representations from Transformers for Biomedical Text\nMining (BioBERT)) in extracting acupoint-related location relations and assess\nthe impact of pretraining and fine-tuning on GPT's performance. We utilized the\nWorld Health Organization Standard Acupuncture Point Locations in the Western\nPacific Region (WHO Standard) as our corpus, which consists of descriptions of\n361 acupoints. Five types of relations ('direction_of,' 'distance_of,'\n'part_of,' 'near_acupoint,' and 'located_near') (n= 3,174) between acupoints\nwere annotated. Five models were compared: BioBERT, LSTM, pre-trained GPT-3.5,\nfine-tuned GPT-3.5, as well as pre-trained GPT-4. Performance metrics included\nmicro-average exact match precision, recall, and F1 scores. Our results\ndemonstrate that fine-tuned GPT-3.5 consistently outperformed other models in\nF1 scores across all relation types. Overall, it achieved the highest\nmicro-average F1 score of 0.92. This study underscores the effectiveness of\nLLMs like GPT in extracting relations related to acupoint locations, with\nimplications for accurately modeling acupuncture knowledge and promoting\nstandard implementation in acupuncture training and practice. The findings also\ncontribute to advancing informatics applications in traditional and\ncomplementary medicine, showcasing the potential of LLMs in natural language\nprocessing.",
      "tldr_zh": "这篇论文探讨了使用大型语言模型（LLMs）如 GPT 从文本中提取针灸穴位位置关系的性能，针对五种关系类型（direction_of, distance_of, part_of, near_acupoint, located_near）进行研究。研究比较了预训练和微调的 GPT-3.5、GPT-4 与传统模型 BioBERT 和 LSTM，在 WHO 标准数据库（包含 361 个穴位和 3174 种关系）上评估提取准确性。结果显示，微调后的 GPT-3.5 在所有关系类型上表现最佳，整体微平均 F1 分数达到 0.92，显著优于其他模型。该研究突出了 LLMs 在针灸知识建模和传统医学信息学应用中的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05415v2",
      "published_date": "2024-04-08 11:33:00 UTC",
      "updated_date": "2024-04-15 00:45:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:42:24.142777"
    },
    {
      "arxiv_id": "2404.05406v2",
      "title": "PerkwE_COQA: Enhanced Persian Conversational Question Answering by combining contextual keyword extraction with Large Language Models",
      "title_zh": "PerkwE_COQA：通过结合上下文关键词提取与大型语言模型增强波斯语对话式问答系统",
      "authors": [
        "Pardis Moradbeiki",
        "Nasser Ghadiri"
      ],
      "abstract": "Smart cities need the involvement of their residents to enhance quality of\nlife. Conversational query-answering is an emerging approach for user\nengagement. There is an increasing demand of an advanced conversational\nquestion-answering that goes beyond classic systems. Existing approaches have\nshown that LLMs offer promising capabilities for CQA, but may struggle to\ncapture the nuances of conversational contexts. The new approach involves\nunderstanding the content and engaging in a multi-step conversation with the\nuser to fulfill their needs. This paper presents a novel method to elevate the\nperformance of Persian Conversational question-answering (CQA) systems. It\ncombines the strengths of Large Language Models (LLMs) with contextual keyword\nextraction. Our method extracts keywords specific to the conversational flow,\nproviding the LLM with additional context to understand the user's intent and\ngenerate more relevant and coherent responses. We evaluated the effectiveness\nof this combined approach through various metrics, demonstrating significant\nimprovements in CQA performance compared to an LLM-only baseline. The proposed\nmethod effectively handles implicit questions, delivers contextually relevant\nanswers, and tackles complex questions that rely heavily on conversational\ncontext. The findings indicate that our method outperformed the evaluation\nbenchmarks up to 8% higher than existing methods and the LLM-only baseline.",
      "tldr_zh": "本研究提出PerkwE_COQA，一种增强的Persian Conversational Question Answering (CQA) 系统，通过结合上下文关键词提取(contextual keyword extraction)与Large Language Models (LLMs)，来提升对话式查询回答的性能。该方法从对话流程中提取特定关键词，提供额外上下文，帮助LLMs更好地理解用户意图、处理隐式问题和生成相关响应。实验结果显示，与LLM-only基线相比，该方法在各种评估指标上取得了显著改善，性能最高高出8%，从而更好地支持智能城市居民参与和复杂对话处理。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50, 68T07",
        "J.3; I.2.7; I.2.1"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05406v2",
      "published_date": "2024-04-08 11:14:58 UTC",
      "updated_date": "2024-04-15 12:38:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:42:34.558177"
    },
    {
      "arxiv_id": "2404.05405v1",
      "title": "Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws",
      "title_zh": "翻译失败",
      "authors": [
        "Zeyuan Allen-Zhu",
        "Yuanzhi Li"
      ],
      "abstract": "Scaling laws describe the relationship between the size of language models\nand their capabilities. Unlike prior studies that evaluate a model's capability\nvia loss or benchmarks, we estimate the number of knowledge bits a model\nstores. We focus on factual knowledge represented as tuples, such as (USA,\ncapital, Washington D.C.) from a Wikipedia page. Through multiple controlled\ndatasets, we establish that language models can and only can store 2 bits of\nknowledge per parameter, even when quantized to int8, and such knowledge can be\nflexibly extracted for downstream applications. Consequently, a 7B model can\nstore 14B bits of knowledge, surpassing the English Wikipedia and textbooks\ncombined based on our estimation.\n  More broadly, we present 12 results on how (1) training duration, (2) model\narchitecture, (3) quantization, (4) sparsity constraints such as MoE, and (5)\ndata signal-to-noise ratio affect a model's knowledge storage capacity. Notable\ninsights include:\n  * The GPT-2 architecture, with rotary embedding, matches or even surpasses\nLLaMA/Mistral architectures in knowledge storage, particularly over shorter\ntraining durations. This arises because LLaMA/Mistral uses GatedMLP, which is\nless stable and harder to train.\n  * Prepending training data with domain names (e.g., wikipedia.org)\nsignificantly increases a model's knowledge capacity. Language models can\nautonomously identify and prioritize domains rich in knowledge, optimizing\ntheir storage capacity.",
      "tldr_zh": "这篇论文探讨了语言模型的知识容量 scaling laws，通过估计模型存储的知识位数（如事实元组 (USA, capital, Washington D.C.)）来评估其能力，而非传统损失或基准。研究发现，语言模型每参数可存储 2 bits of knowledge，即使量化到 int8，且一个 7B 模型能存储 14B bits of knowledge，超过英语 Wikipedia 和教科书的总和。作者分析了 12 个影响因素，包括训练 duration、模型 architecture（如 GPT-2 优于 LLaMA/Mistral 的 GatedMLP，尤其在短训练中）、quantization、sparsity constraints（如 MoE）和数据 signal-to-noise ratio。关键洞见是，在训练数据中添加域名（如 wikipedia.org）能显著提升知识容量，使模型自主优先知识丰富的领域。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05405v1",
      "published_date": "2024-04-08 11:11:31 UTC",
      "updated_date": "2024-04-08 11:11:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:42:49.028878"
    },
    {
      "arxiv_id": "2404.05403v2",
      "title": "SoK: On Gradient Leakage in Federated Learning",
      "title_zh": "SoK: 联邦学习中的梯度泄漏",
      "authors": [
        "Jiacheng Du",
        "Jiahui Hu",
        "Zhibo Wang",
        "Peng Sun",
        "Neil Zhenqiang Gong",
        "Kui Ren",
        "Chun Chen"
      ],
      "abstract": "Federated learning (FL) facilitates collaborative model training among\nmultiple clients without raw data exposure. However, recent studies have shown\nthat clients' private training data can be reconstructed from shared gradients\nin FL, a vulnerability known as gradient inversion attacks (GIAs). While GIAs\nhave demonstrated effectiveness under \\emph{ideal settings and auxiliary\nassumptions}, their actual efficacy against \\emph{practical FL systems} remains\nunder-explored. To address this gap, we conduct a comprehensive study on GIAs\nin this work. We start with a survey of GIAs that establishes a timeline to\ntrace their evolution and develops a systematization to uncover their inherent\nthreats. By rethinking GIA in practical FL systems, three fundamental aspects\ninfluencing GIA's effectiveness are identified: \\textit{training setup},\n\\textit{model}, and \\textit{post-processing}. Guided by these aspects, we\nperform extensive theoretical and empirical evaluations of SOTA GIAs across\ndiverse settings. Our findings highlight that GIA is notably\n\\textit{constrained}, \\textit{fragile}, and \\textit{easily defensible}.\nSpecifically, GIAs exhibit inherent limitations against practical local\ntraining settings. Additionally, their effectiveness is highly sensitive to the\ntrained model, and even simple post-processing techniques applied to gradients\ncan serve as effective defenses. Our work provides crucial insights into the\nlimited threats of GIAs in practical FL systems. By rectifying prior\nmisconceptions, we hope to inspire more accurate and realistic investigations\non this topic.",
      "tldr_zh": "本论文（SoK: On Gradient Leakage in Federated Learning）系统化研究了Federated Learning (FL)中梯度反演攻击（Gradient Inversion Attacks, GIAs）的威胁，这些攻击可从共享梯度中重建客户端的私有训练数据。作者通过回顾GIAs的演变并分析其在实际FL系统中的有效性，识别了三个关键影响因素：训练设置、模型和后处理，并进行了广泛的理论和实证评估。研究发现，GIAs在实际场景下表现出显著的限制、脆弱性，且易于通过简单后处理技术进行防御，最终纠正了先前对GIAs威胁的误解，并呼吁更现实的FL安全研究。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted to USENIX Security'25",
      "pdf_url": "http://arxiv.org/pdf/2404.05403v2",
      "published_date": "2024-04-08 11:05:45 UTC",
      "updated_date": "2025-02-05 03:07:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:42:58.906935"
    },
    {
      "arxiv_id": "2404.05399v2",
      "title": "SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety",
      "title_zh": "翻译失败",
      "authors": [
        "Paul Röttger",
        "Fabio Pernisi",
        "Bertie Vidgen",
        "Dirk Hovy"
      ],
      "abstract": "The last two years have seen a rapid growth in concerns around the safety of\nlarge language models (LLMs). Researchers and practitioners have met these\nconcerns by creating an abundance of datasets for evaluating and improving LLM\nsafety. However, much of this work has happened in parallel, and with very\ndifferent goals in mind, ranging from the mitigation of near-term risks around\nbias and toxic content generation to the assessment of longer-term catastrophic\nrisk potential. This makes it difficult for researchers and practitioners to\nfind the most relevant datasets for their use case, and to identify gaps in\ndataset coverage that future work may fill. To remedy these issues, we conduct\na first systematic review of open datasets for evaluating and improving LLM\nsafety. We review 144 datasets, which we identified through an iterative and\ncommunity-driven process over the course of several months. We highlight\npatterns and trends, such as a trend towards fully synthetic datasets, as well\nas gaps in dataset coverage, such as a clear lack of non-English and\nnaturalistic datasets. We also examine how LLM safety datasets are used in\npractice -- in LLM release publications and popular LLM benchmarks -- finding\nthat current evaluation practices are highly idiosyncratic and make use of only\na small fraction of available datasets. Our contributions are based on\nSafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we\nplan to update continuously as the field of LLM safety develops.",
      "tldr_zh": "这篇论文对评估和改进大型语言模型(LLMs)安全性的开放数据集进行了首次系统性回顾，审查了144个数据集，以解决当前研究中的碎片化和目标差异问题。研究发现，数据集趋势转向全合成生成，但存在显著空白，如缺乏非英语和自然主义数据集，同时LLMs安全评估在实际应用中高度不一致，仅使用少量可用数据集。作者创建了SafetyPrompts.com网站，作为一个持续更新的目录，帮助研究人员识别相关数据集并指导未来工作。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at AAAI 2025 (Special Track on AI Alignment)",
      "pdf_url": "http://arxiv.org/pdf/2404.05399v2",
      "published_date": "2024-04-08 10:57:25 UTC",
      "updated_date": "2025-01-10 09:54:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:43:10.071137"
    },
    {
      "arxiv_id": "2404.05393v4",
      "title": "PAT: Pixel-wise Adaptive Training for Long-tailed Segmentation",
      "title_zh": "PAT",
      "authors": [
        "Khoi Do",
        "Duong Nguyen",
        "Nguyen H. Tran",
        "Viet Dung Nguyen"
      ],
      "abstract": "Beyond class frequency, we recognize the impact of class-wise relationships\namong various class-specific predictions and the imbalance in label masks on\nlong-tailed segmentation learning. To address these challenges, we propose an\ninnovative Pixel-wise Adaptive Training (PAT) technique tailored for\nlong-tailed segmentation. PAT has two key features: 1) class-wise gradient\nmagnitude homogenization, and 2) pixel-wise class-specific loss adaptation\n(PCLA). First, the class-wise gradient magnitude homogenization helps alleviate\nthe imbalance among label masks by ensuring equal consideration of the\nclass-wise impact on model updates. Second, PCLA tackles the detrimental impact\nof both rare classes within the long-tailed distribution and inaccurate\npredictions from previous training stages by encouraging learning classes with\nlow prediction confidence and guarding against forgetting classes with high\nconfidence. This combined approach fosters robust learning while preventing the\nmodel from forgetting previously learned knowledge. PAT exhibits significant\nperformance improvements, surpassing the current state-of-the-art by 2.2% in\nthe NyU dataset. Moreover, it enhances overall pixel-wise accuracy by 2.85% and\nintersection over union value by 2.07%, with a particularly notable declination\nof 0.39% in detecting rare classes compared to Balance Logits Variation, as\ndemonstrated on the three popular datasets, i.e., OxfordPetIII, CityScape, and\nNYU.",
      "tldr_zh": "该研究针对长尾分割(long-tailed segmentation)中的类间关系和标签掩码不平衡问题，提出了一种创新技术Pixel-wise Adaptive Training (PAT)。PAT 包括两个关键特征：类-wise 梯度幅度均匀化(class-wise gradient magnitude homogenization)，以确保各类对模型更新的平等影响；以及像素-wise 类-specific 损失适应(PCLA)，通过鼓励学习低置信度类并防止遗忘高置信度类，从而处理稀有类和训练不准确问题。实验结果显示，PAT 在 NyU 数据集上比现有最先进方法提升 2.2%，整体像素准确率提高 2.85%，IoU 值提高 2.07%，并在 OxfordPetIII、CityScape 和 NYU 数据集上显著改善稀有类检测表现。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05393v4",
      "published_date": "2024-04-08 10:52:29 UTC",
      "updated_date": "2024-10-20 16:20:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:43:23.224850"
    },
    {
      "arxiv_id": "2404.05388v3",
      "title": "An AI System Evaluation Framework for Advancing AI Safety: Terminology, Taxonomy, Lifecycle Mapping",
      "title_zh": "用于推进AI安全的AI系统评估框架：术语、分类法、生命周期映射",
      "authors": [
        "Boming Xia",
        "Qinghua Lu",
        "Liming Zhu",
        "Zhenchang Xing"
      ],
      "abstract": "The advent of advanced AI underscores the urgent need for comprehensive\nsafety evaluations, necessitating collaboration across communities (i.e., AI,\nsoftware engineering, and governance). However, divergent practices and\nterminologies across these communities, combined with the complexity of AI\nsystems-of which models are only a part-and environmental affordances (e.g.,\naccess to tools), obstruct effective communication and comprehensive\nevaluation. This paper proposes a framework for AI system evaluation comprising\nthree components: 1) harmonised terminology to facilitate communication across\ncommunities involved in AI safety evaluation; 2) a taxonomy identifying\nessential elements for AI system evaluation; 3) a mapping between AI lifecycle,\nstakeholders, and requisite evaluations for accountable AI supply chain. This\nframework catalyses a deeper discourse on AI system evaluation beyond\nmodel-centric approaches.",
      "tldr_zh": "这篇论文针对AI安全评估的紧迫需求，提出一个综合框架，以解决跨社区（如AI、软件工程和治理领域）术语分歧、系统复杂性和环境因素带来的挑战。该框架包括三个核心组件：1) 统一的harmonised terminology，以促进沟通；2) 一个taxonomy来识别AI系统评估的关键元素；3) 将AI lifecycle、利益相关者和必要评估进行映射，从而支持可问责的AI供应链。该框架推动了超越模型中心的AI system evaluation讨论，促进更全面和协作式的AI安全实践。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "1st ACM International Conference on AI-powered Software (AIware)",
      "pdf_url": "http://arxiv.org/pdf/2404.05388v3",
      "published_date": "2024-04-08 10:49:59 UTC",
      "updated_date": "2024-05-15 06:19:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:43:34.497011"
    },
    {
      "arxiv_id": "2404.05384v1",
      "title": "Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance",
      "title_zh": "重新审视无分类器扩散指导中的空间不一致性",
      "authors": [
        "Dazhong Shen",
        "Guanglu Song",
        "Zeyue Xue",
        "Fu-Yun Wang",
        "Yu Liu"
      ],
      "abstract": "Classifier-Free Guidance (CFG) has been widely used in text-to-image\ndiffusion models, where the CFG scale is introduced to control the strength of\ntext guidance on the whole image space. However, we argue that a global CFG\nscale results in spatial inconsistency on varying semantic strengths and\nsuboptimal image quality. To address this problem, we present a novel approach,\nSemantic-aware Classifier-Free Guidance (S-CFG), to customize the guidance\ndegrees for different semantic units in text-to-image diffusion models.\nSpecifically, we first design a training-free semantic segmentation method to\npartition the latent image into relatively independent semantic regions at each\ndenoising step. In particular, the cross-attention map in the denoising U-net\nbackbone is renormalized for assigning each patch to the corresponding token,\nwhile the self-attention map is used to complete the semantic regions. Then, to\nbalance the amplification of diverse semantic units, we adaptively adjust the\nCFG scales across different semantic regions to rescale the text guidance\ndegrees into a uniform level. Finally, extensive experiments demonstrate the\nsuperiority of S-CFG over the original CFG strategy on various text-to-image\ndiffusion models, without requiring any extra training cost. our codes are\navailable at https://github.com/SmilesDZgk/S-CFG.",
      "tldr_zh": "本论文重新审视了Classifier-Free Guidance (CFG)在text-to-image扩散模型中的空间不一致问题，认为全局CFG规模会导致语义强度差异和图像质量下降。\n为此，提出了一种新方法Semantic-aware Classifier-Free Guidance (S-CFG)，通过一个无训练的语义分割技术，利用cross-attention和self-attention地图将潜在图像分区为独立语义区域，并适配地调整CFG规模以统一不同区域的文本指导强度。\n实验证明，S-CFG在各种text-to-image扩散模型上优于原CFG策略，且无需额外训练，开源代码可从指定仓库获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted by CVPR-2024",
      "pdf_url": "http://arxiv.org/pdf/2404.05384v1",
      "published_date": "2024-04-08 10:45:29 UTC",
      "updated_date": "2024-04-08 10:45:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:43:46.733599"
    },
    {
      "arxiv_id": "2404.05348v1",
      "title": "Iterative Refinement Strategy for Automated Data Labeling: Facial Landmark Diagnosis in Medical Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Yu-Hsi Chen"
      ],
      "abstract": "Automated data labeling techniques are crucial for accelerating the\ndevelopment of deep learning models, particularly in complex medical imaging\napplications. However, ensuring accuracy and efficiency remains challenging.\nThis paper presents iterative refinement strategies for automated data labeling\nin facial landmark diagnosis to enhance accuracy and efficiency for deep\nlearning models in medical applications, including dermatology, plastic\nsurgery, and ophthalmology. Leveraging feedback mechanisms and advanced\nalgorithms, our approach iteratively refines initial labels, reducing reliance\non manual intervention while improving label quality. Through empirical\nevaluation and case studies, we demonstrate the effectiveness of our proposed\nstrategies in deep learning tasks across medical imaging domains. Our results\nhighlight the importance of iterative refinement in automated data labeling to\nenhance the capabilities of deep learning systems in medical imaging\napplications.",
      "tldr_zh": "这篇论文提出了一种迭代精炼策略（iterative refinement strategy），用于自动数据标注（automated data labeling），以提升医疗成像中面部 landmarks 诊断的准确性和效率。策略通过反馈机制和高级算法，对初始标签进行迭代改进，减少了对手动干预的依赖，并适用于皮肤科、整形外科和眼科等领域。实证评估和案例研究结果表明，该方法显著提高了深度学习模型在医疗成像任务中的性能，强调了迭代精炼在增强系统能力方面的关键作用。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05348v1",
      "published_date": "2024-04-08 09:33:40 UTC",
      "updated_date": "2024-04-08 09:33:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:43:59.854583"
    },
    {
      "arxiv_id": "2404.05337v1",
      "title": "Towards Objectively Benchmarking Social Intelligence for Language Agents at Action Level",
      "title_zh": "翻译失败",
      "authors": [
        "Chenxu Wang",
        "Bin Dai",
        "Huaping Liu",
        "Baoyuan Wang"
      ],
      "abstract": "Prominent large language models have exhibited human-level performance in\nmany domains, even enabling the derived agents to simulate human and social\ninteractions. While practical works have substantiated the practicability of\ngrounding language agents in sandbox simulation or embodied simulators, current\nsocial intelligence benchmarks either stay at the language level or use\nsubjective metrics. In pursuit of a more realistic and objective evaluation, we\nintroduce the Social Tasks in Sandbox Simulation (STSS) benchmark, which\nassesses language agents \\textbf{objectively} at the \\textbf{action level} by\nscrutinizing the goal achievements within the multi-agent simulation.\nAdditionally, we sample conversation scenarios to build a language-level\nbenchmark to provide an economically prudent preliminary evaluation and align\nwith prevailing benchmarks. To gauge the significance of agent architecture, we\nimplement a target-driven planning (TDP) module as an adjunct to the existing\nagent. Our evaluative findings highlight that the STSS benchmark is challenging\nfor state-of-the-art language agents. Furthermore, it effectively discriminates\nbetween distinct language agents, suggesting its usefulness as a benchmark for\nevaluating both language models and agent architectures.",
      "tldr_zh": "这篇论文针对语言代理的社会智能评估问题，引入了 Social Tasks in Sandbox Simulation (STSS) 基准，通过多代理模拟环境在行动级别客观评估代理的目标实现，避免了传统基准的语言级别局限和主观指标。研究还构建了一个语言级别的对话场景基准，作为经济高效的初步评估，并开发了 target-driven planning (TDP) 模块作为代理架构的补充。实验结果表明，STSS 对最先进的语言代理具有挑战性，并能有效区分不同语言模型和代理架构的表现。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05337v1",
      "published_date": "2024-04-08 09:25:32 UTC",
      "updated_date": "2024-04-08 09:25:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:44:11.145788"
    },
    {
      "arxiv_id": "2404.05324v1",
      "title": "Back to the Future: GNN-based NO$_2$ Forecasting via Future Covariates",
      "title_zh": "翻译失败",
      "authors": [
        "Antonio Giganti",
        "Sara Mandelli",
        "Paolo Bestagini",
        "Umberto Giuriato",
        "Alessandro D'Ausilio",
        "Marco Marcon",
        "Stefano Tubaro"
      ],
      "abstract": "Due to the latest environmental concerns in keeping at bay contaminants\nemissions in urban areas, air pollution forecasting has been rising the\nforefront of all researchers around the world. When predicting pollutant\nconcentrations, it is common to include the effects of environmental factors\nthat influence these concentrations within an extended period, like traffic,\nmeteorological conditions and geographical information. Most of the existing\napproaches exploit this information as past covariates, i.e., past exogenous\nvariables that affected the pollutant but were not affected by it. In this\npaper, we present a novel forecasting methodology to predict NO$_2$\nconcentration via both past and future covariates. Future covariates are\nrepresented by weather forecasts and future calendar events, which are already\nknown at prediction time. In particular, we deal with air quality observations\nin a city-wide network of ground monitoring stations, modeling the data\nstructure and estimating the predictions with a Spatiotemporal Graph Neural\nNetwork (STGNN). We propose a conditioning block that embeds past and future\ncovariates into the current observations. After extracting meaningful\nspatiotemporal representations, these are fused together and projected into the\nforecasting horizon to generate the final prediction. To the best of our\nknowledge, it is the first time that future covariates are included in time\nseries predictions in a structured way. Remarkably, we find that conditioning\non future weather information has a greater impact than considering past\ntraffic conditions. We release our code implementation at\nhttps://github.com/polimi-ispl/MAGCRN.",
      "tldr_zh": "该论文提出了一种基于图神经网络(GNN)的 NO₂ 浓度预测方法，创新性地整合了未来协变量（如天气预报和日历事件），以提升空气污染预测的准确性。作者使用时空图神经网络(STGNN)来建模城市监测站网络的数据结构，通过一个条件块将过去和未来协变量嵌入当前观察中，并融合时空表示进行预测。实验结果显示，未来天气信息的影响显著大于过去交通条件，这也是首次以结构化方式在时间序列预测中纳入未来协变量。代码已在 https://github.com/polimi-ispl/MAGCRN 开源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.LG",
      "comment": "5 pages, 4 figures, 1 table, accepted at IEEE-IGARSS 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.05324v1",
      "published_date": "2024-04-08 09:13:16 UTC",
      "updated_date": "2024-04-08 09:13:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:44:22.709257"
    },
    {
      "arxiv_id": "2404.05290v1",
      "title": "MindSet: Vision. A toolbox for testing DNNs on key psychological experiments",
      "title_zh": "翻译失败",
      "authors": [
        "Valerio Biscione",
        "Dong Yin",
        "Gaurav Malhotra",
        "Marin Dujmovic",
        "Milton L. Montero",
        "Guillermo Puebla",
        "Federico Adolfi",
        "Rachel F. Heaton",
        "John E. Hummel",
        "Benjamin D. Evans",
        "Karim Habashy",
        "Jeffrey S. Bowers"
      ],
      "abstract": "Multiple benchmarks have been developed to assess the alignment between deep\nneural networks (DNNs) and human vision. In almost all cases these benchmarks\nare observational in the sense they are composed of behavioural and brain\nresponses to naturalistic images that have not been manipulated to test\nhypotheses regarding how DNNs or humans perceive and identify objects. Here we\nintroduce the toolbox MindSet: Vision, consisting of a collection of image\ndatasets and related scripts designed to test DNNs on 30 psychological\nfindings. In all experimental conditions, the stimuli are systematically\nmanipulated to test specific hypotheses regarding human visual perception and\nobject recognition. In addition to providing pre-generated datasets of images,\nwe provide code to regenerate these datasets, offering many configurable\nparameters which greatly extend the dataset versatility for different research\ncontexts, and code to facilitate the testing of DNNs on these image datasets\nusing three different methods (similarity judgments, out-of-distribution\nclassification, and decoder method), accessible at\nhttps://github.com/MindSetVision/mindset-vision. We test ResNet-152 on each of\nthese methods as an example of how the toolbox can be used.",
      "tldr_zh": "本研究引入了MindSet: Vision工具箱，用于测试深度神经网络(DNNs)与人类视觉的相似性，聚焦于30个关键心理实验的图像数据集，这些数据集通过系统操纵刺激来验证人类视觉感知和物体识别的假设。工具箱提供预生成图像数据集、代码来再生这些数据集（支持可配置参数），以及用于DNNs测试的脚本，支持三种方法：相似性判断、异常分布(out-of-distribution)分类和解码器方法。作者以ResNet-152为例演示了工具箱的应用，代码可从GitHub获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05290v1",
      "published_date": "2024-04-08 08:28:19 UTC",
      "updated_date": "2024-04-08 08:28:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:44:35.994893"
    },
    {
      "arxiv_id": "2404.05272v2",
      "title": "Pricing Strategies for Different Accuracy Models from the Same Dataset Based on Generalized Hotelling's Law",
      "title_zh": "翻译失败",
      "authors": [
        "Jie Liu",
        "Tao Feng",
        "Yan Jiang",
        "Peizheng Wang",
        "Chao Wu"
      ],
      "abstract": "We consider a scenario where a seller possesses a dataset $D$ and trains it\ninto models of varying accuracies for sale in the market. Due to the\nreproducibility of data, the dataset can be reused to train models with\ndifferent accuracies, and the training cost is independent of the sales volume.\nThese two characteristics lead to fundamental differences between the data\ntrading market and traditional trading markets. The introduction of different\nmodels into the market inevitably gives rise to competition. However, due to\nthe varying accuracies of these models, traditional multi-oligopoly games are\nnot applicable. We consider a generalized Hotelling's law, where the accuracy\nof the models is abstracted as distance. Buyers choose to purchase models based\non a trade-off between accuracy and price, while sellers determine their\npricing strategies based on the market's demand. We present two pricing\nstrategies: static pricing strategy and dynamic pricing strategy, and we focus\non the static pricing strategy. We propose static pricing mechanisms based on\nvarious market conditions and provide an example. Finally, we demonstrate that\nour pricing strategy remains robust in the context of incomplete information\ngames.",
      "tldr_zh": "本研究探讨了卖家使用同一数据集训练不同准确度模型的定价策略，基于广义 Hotelling's Law 将模型准确度抽象为距离，以应对数据市场与传统市场的差异。论文提出静态定价机制，让买家在准确度和价格间权衡，同时卖家根据市场需求调整策略，并提供基于各种市场条件的示例。结果显示，该定价策略在不完全信息游戏中保持鲁棒性，确保了其实际应用潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05272v2",
      "published_date": "2024-04-08 08:02:18 UTC",
      "updated_date": "2025-03-29 08:49:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:44:47.015537"
    },
    {
      "arxiv_id": "2404.05259v1",
      "title": "Cellular automata, many-valued logic, and deep neural networks",
      "title_zh": "元胞自动机、多值逻辑和深度神经网络",
      "authors": [
        "Yani Zhang",
        "Helmut Bölcskei"
      ],
      "abstract": "We develop a theory characterizing the fundamental capability of deep neural\nnetworks to learn, from evolution traces, the logical rules governing the\nbehavior of cellular automata (CA). This is accomplished by first establishing\na novel connection between CA and Lukasiewicz propositional logic. While binary\nCA have been known for decades to essentially perform operations in Boolean\nlogic, no such relationship exists for general CA. We demonstrate that\nmany-valued (MV) logic, specifically Lukasiewicz propositional logic,\nconstitutes a suitable language for characterizing general CA as logical\nmachines. This is done by interpolating CA transition functions to continuous\npiecewise linear functions, which, by virtue of the McNaughton theorem, yield\nformulae in MV logic characterizing the CA. Recognizing that deep rectified\nlinear unit (ReLU) networks realize continuous piecewise linear functions, it\nfollows that these formulae are naturally extracted from CA evolution traces by\ndeep ReLU networks. A corresponding algorithm together with a software\nimplementation is provided. Finally, we show that the dynamical behavior of CA\ncan be realized by recurrent neural networks.",
      "tldr_zh": "本研究建立了细胞自动机（CA）和多值逻辑（MV logic），特别是Lukasiewicz命题逻辑之间的理论联系，以探索深度神经网络从CA演化轨迹中学习逻辑规则的能力。通过将CA的过渡函数内插为连续分段线性函数，并利用McNaughton定理，将其转化为MV逻辑公式，深度ReLU网络可以自然地从这些轨迹中提取公式。实验结果表明，该方法有效实现CA的动态行为，并通过提供的算法和软件支持，展示了循环神经网络（RNN）在模拟CA方面的潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05259v1",
      "published_date": "2024-04-08 07:49:52 UTC",
      "updated_date": "2024-04-08 07:49:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:44:59.468843"
    },
    {
      "arxiv_id": "2404.05256v2",
      "title": "StyleForge: Enhancing Text-to-Image Synthesis for Any Artistic Styles with Dual Binding",
      "title_zh": "翻译失败",
      "authors": [
        "Junseo Park",
        "Beomseok Ko",
        "Hyeryung Jang"
      ],
      "abstract": "Recent advancements in text-to-image models, such as Stable Diffusion, have\nshowcased their ability to create visual images from natural language prompts.\nHowever, existing methods like DreamBooth struggle with capturing arbitrary art\nstyles due to the abstract and multifaceted nature of stylistic attributes. We\nintroduce Single-StyleForge, a novel approach for personalized text-to-image\nsynthesis across diverse artistic styles. Using approximately 15 to 20 images\nof the target style, Single-StyleForge establishes a foundational binding of a\nunique token identifier with a broad range of attributes of the target style.\nAdditionally, auxiliary images are incorporated for dual binding that guides\nthe consistent representation of crucial elements such as people within the\ntarget style. Furthermore, we present Multi-StyleForge, which enhances image\nquality and text alignment by binding multiple tokens to partial style\nattributes. Experimental evaluations across six distinct artistic styles\ndemonstrate significant improvements in image quality and perceptual fidelity,\nas measured by FID, KID, and CLIP scores.",
      "tldr_zh": "本研究针对现有文本到图像模型（如 Stable Diffusion）在捕捉任意艺术风格时的局限性，提出了 Single-StyleForge 方法，该方法通过使用 15 到 20 张目标风格图像和辅助图像进行 dual binding，将独特 token 标识符绑定到目标风格的广泛属性，并确保关键元素（如人物）的 consistent representation。Multi-StyleForge 进一步扩展此框架，通过绑定多个 tokens 到部分风格属性，提升图像质量和文本对齐。实验在六种不同艺术风格上评估，结果显示 FID、KID 和 CLIP 分数显著改善，证明了该方法的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages, 12 figuers",
      "pdf_url": "http://arxiv.org/pdf/2404.05256v2",
      "published_date": "2024-04-08 07:43:23 UTC",
      "updated_date": "2024-07-17 06:15:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:45:11.430553"
    },
    {
      "arxiv_id": "2404.08001v1",
      "title": "Xiwu: A Basis Flexible and Learnable LLM for High Energy Physics",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengde Zhang",
        "Yiyu Zhang",
        "Haodong Yao",
        "Jianwen Luo",
        "Rui Zhao",
        "Bo Huang",
        "Jiameng Zhao",
        "Yipu Liao",
        "Ke Li",
        "Lina Zhao",
        "Jun Cao",
        "Fazhi Qi",
        "Changzheng Yuan"
      ],
      "abstract": "Large Language Models (LLMs) are undergoing a period of rapid updates and\nchanges, with state-of-the-art (SOTA) model frequently being replaced. When\napplying LLMs to a specific scientific field, it's challenging to acquire\nunique domain knowledge while keeping the model itself advanced. To address\nthis challenge, a sophisticated large language model system named as Xiwu has\nbeen developed, allowing you switch between the most advanced foundation models\nand quickly teach the model domain knowledge. In this work, we will report on\nthe best practices for applying LLMs in the field of high-energy physics (HEP),\nincluding: a seed fission technology is proposed and some data collection and\ncleaning tools are developed to quickly obtain domain AI-Ready dataset; a\njust-in-time learning system is implemented based on the vector store\ntechnology; an on-the-fly fine-tuning system has been developed to facilitate\nrapid training under a specified foundation model. The results show that Xiwu\ncan smoothly switch between foundation models such as LLaMA, Vicuna, ChatGLM\nand Grok-1. The trained Xiwu model is significantly outperformed the benchmark\nmodel on the HEP knowledge question-and-answering and code generation. This\nstrategy significantly enhances the potential for growth of our model's\nperformance, with the hope of surpassing GPT-4 as it evolves with the\ndevelopment of open-source models. This work provides a customized LLM for the\nfield of HEP, while also offering references for applying LLM to other fields,\nthe corresponding codes are available on Github.",
      "tldr_zh": "该研究开发了Xiwu，一种基础模型灵活且可学习的LLM系统，旨在解决高能物理(HEP)领域中LLM获取独特领域知识的挑战。Xiwu引入了种子裂变技术(seed fission technology)、数据收集和清理工具、基于向量存储的即时学习系统，以及即时微调系统(on-the-fly fine-tuning system)，以快速构建AI-Ready数据集并适应如LLaMA、Vicuna、ChatGLM和Grok-1等基础模型。实验结果表明，训练后的Xiwu在HEP知识问答和代码生成任务上显著优于基准模型，并有望随着开源模型的发展超越GPT-4。该系统为HEP领域提供定制化LLM解决方案，同时为其他领域应用提供参考。",
      "categories": [
        "hep-ph",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "hep-ex",
        "physics.comp-ph",
        "I.2.7"
      ],
      "primary_category": "hep-ph",
      "comment": "15 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.08001v1",
      "published_date": "2024-04-08 07:37:31 UTC",
      "updated_date": "2024-04-08 07:37:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:45:24.860846"
    },
    {
      "arxiv_id": "2404.05243v1",
      "title": "Product Description and QA Assisted Self-Supervised Opinion Summarization",
      "title_zh": "产品描述和 QA 辅助的自监督观点总结",
      "authors": [
        "Tejpalsingh Siledar",
        "Rupasai Rangaraju",
        "Sankara Sri Raghava Ravindra Muddu",
        "Suman Banerjee",
        "Amey Patil",
        "Sudhanshu Shekhar Singh",
        "Muthusamy Chelliah",
        "Nikesh Garera",
        "Swaprava Nath",
        "Pushpak Bhattacharyya"
      ],
      "abstract": "In e-commerce, opinion summarization is the process of summarizing the\nconsensus opinions found in product reviews. However, the potential of\nadditional sources such as product description and question-answers (QA) has\nbeen considered less often. Moreover, the absence of any supervised training\ndata makes this task challenging. To address this, we propose a novel synthetic\ndataset creation (SDC) strategy that leverages information from reviews as well\nas additional sources for selecting one of the reviews as a pseudo-summary to\nenable supervised training. Our Multi-Encoder Decoder framework for Opinion\nSummarization (MEDOS) employs a separate encoder for each source, enabling\neffective selection of information while generating the summary. For\nevaluation, due to the unavailability of test sets with additional sources, we\nextend the Amazon, Oposum+, and Flipkart test sets and leverage ChatGPT to\nannotate summaries. Experiments across nine test sets demonstrate that the\ncombination of our SDC approach and MEDOS model achieves on average a 14.5%\nimprovement in ROUGE-1 F1 over the SOTA. Moreover, comparative analysis\nunderlines the significance of incorporating additional sources for generating\nmore informative summaries. Human evaluations further indicate that MEDOS\nscores relatively higher in coherence and fluency with 0.41 and 0.5 (-1 to 1)\nrespectively, compared to existing models. To the best of our knowledge, we are\nthe first to generate opinion summaries leveraging additional sources in a\nself-supervised setting.",
      "tldr_zh": "本文提出了一种自监督意见总结方法，通过整合产品描述和问答（QA）来源来提升电子商务评论共识总结的准确性。研究引入了合成数据集创建（SDC）策略，将评论和其他来源信息用于选择伪总结，从而实现监督训练；同时，开发了 Multi-Encoder Decoder 框架（MEDOS），使用独立编码器处理每个来源以生成更有效的总结。实验在扩展的 Amazon、Oposum+ 和 Flipkart 测试集上显示，该方法平均提高了 14.5% 的 ROUGE-1 F1 分数，并在人类评估中表现出更高的连贯性（0.41）和流畅性（0.5）。这项工作首次在自监督设置中利用额外来源，显著提升了总结的 informativity 和质量。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05243v1",
      "published_date": "2024-04-08 07:15:06 UTC",
      "updated_date": "2024-04-08 07:15:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:45:37.759865"
    },
    {
      "arxiv_id": "2404.05235v2",
      "title": "Novelty Heuristics, Multi-Queue Search, and Portfolios for Numeric Planning",
      "title_zh": "翻译失败",
      "authors": [
        "Dillon Z. Chen",
        "Sylvie Thiébaux"
      ],
      "abstract": "Heuristic search is a powerful approach for solving planning problems and\nnumeric planning is no exception. In this paper, we boost the performance of\nheuristic search for numeric planning with various powerful techniques\northogonal to improving heuristic informedness: numeric novelty heuristics, the\nManhattan distance heuristic, and exploring the use of multi-queue search and\nportfolios for combining heuristics.",
      "tldr_zh": "这篇论文旨在提升启发式搜索在数字规划(numeric planning)中的性能，通过引入与改善启发式信息正交的技术。研究者探讨了数字新颖性启发式(numeric novelty heuristics)、曼哈顿距离启发式(Manhattan distance heuristic)，以及多队列搜索(multi-queue search)和启发式组合的投资组合(portfolios)。这些方法有助于优化搜索过程，提高数字规划问题的解决效率。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Extended version of SoCS 2024 paper",
      "pdf_url": "http://arxiv.org/pdf/2404.05235v2",
      "published_date": "2024-04-08 07:01:35 UTC",
      "updated_date": "2024-04-11 15:00:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:45:47.581811"
    },
    {
      "arxiv_id": "2404.05776v1",
      "title": "Forecasting Electric Vehicle Battery Output Voltage: A Predictive Modeling Approach",
      "title_zh": "预测电动汽车电池输出电压：一种预测建模方法",
      "authors": [
        "Narayana Darapaneni",
        "Ashish K",
        "Ullas M S",
        "Anwesh Reddy Paduri"
      ],
      "abstract": "The battery management system plays a vital role in ensuring the safety and\ndependability of electric and hybrid vehicles. It is responsible for various\nfunctions, including state evaluation, monitoring, charge control, and cell\nbalancing, all integrated within the BMS. Nonetheless, due to the uncertainties\nsurrounding battery performance, implementing these functionalities poses\nsignificant challenges. In this study, we explore the latest approaches for\nassessing battery states, highlight notable advancements in battery management\nsystems (BMS), address existing issues with current BMS technology, and put\nforth possible solutions for predicting battery charging voltage.",
      "tldr_zh": "这篇论文探讨了预测电动车辆电池输出电压的建模方法，旨在提升电池管理系统 (BMS) 的安全性和可靠性。研究者分析了电池性能不确定性带来的挑战，包括状态评估、监控、充电控制和电池均衡等问题，并回顾了最新评估技术和 BMS 进展。论文提出了可能的解决方案，通过预测建模来解决现有问题，从而改善电池充电电压的预测准确性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05776v1",
      "published_date": "2024-04-08 06:47:03 UTC",
      "updated_date": "2024-04-08 06:47:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:45:59.224123"
    },
    {
      "arxiv_id": "2404.05224v1",
      "title": "Iof-maint -- Modular maintenance ontology",
      "title_zh": "翻译失败",
      "authors": [
        "Melinda Hodkiewicz",
        "Caitlin Woods",
        "Matt Selway",
        "Markus Stumptner"
      ],
      "abstract": "In this paper we present a publicly-available maintenance ontology\n(Iof-maint). Iof-maint is a modular ontology aligned with the Industrial\nOntology Foundry Core (IOF Core) and contains 20 classes and 2 relations. It\nprovides a set of maintenance-specific terms used in a wide variety of\npractical data-driven use cases. Iof-maint supports OWL DL reasoning, is\ndocumented, and is actively maintained on GitHub. In this paper, we describe\nthe evolution of the Iof-maint reference ontology based on the extraction of\ncommon concepts identified in a number of application ontologies working with\nindustry maintenance work order, procedure and failure mode data.",
      "tldr_zh": "本研究介绍了Iof-maint，一种模块化的维护本体（Modular maintenance ontology），它与Industrial Ontology Foundry Core (IOF Core)对齐，并包含20个类和2个关系。该本体提供了维护领域的特定术语，支持OWL DL推理，并适用于各种数据驱动的实际场景，如处理行业维护工作订单、程序和故障模式数据。Iof-maint的演化基于从多个应用本体中提取的常见概念，已在GitHub上公开、文档化和持续维护，从而提升了维护领域的本体标准化和实用性。",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05224v1",
      "published_date": "2024-04-08 06:40:03 UTC",
      "updated_date": "2024-04-08 06:40:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:46:11.467975"
    },
    {
      "arxiv_id": "2404.05223v2",
      "title": "ITA-ECBS: A Bounded-Suboptimal Algorithm for the Combined Target-Assignment and Path-Finding Problem",
      "title_zh": "翻译失败",
      "authors": [
        "Yimin Tang",
        "Sven Koenig",
        "Jiaoyang Li"
      ],
      "abstract": "Multi-Agent Path Finding (MAPF), i.e., finding collision-free paths for\nmultiple robots, plays a critical role in many applications. Sometimes,\nassigning a target to each agent also presents a challenge. The Combined\nTarget-Assignment and Path-Finding (TAPF) problem, a variant of MAPF, requires\none to simultaneously assign targets to agents and plan collision-free paths\nfor agents. Several algorithms, including CBM, CBS-TA, and ITA-CBS, optimally\nsolve the TAPF problem, with ITA-CBS being the leading algorithm for minimizing\nflowtime. However, the only existing bounded-suboptimal algorithm ECBS-TA is\nderived from CBS-TA rather than ITA-CBS. So, it faces the same issues as\nCBS-TA, such as searching through multiple constraint trees and spending too\nmuch time on finding the next-best target assignment. We introduce ITA-ECBS,\nthe first bounded-suboptimal variant of ITA-CBS. Transforming ITA-CBS to its\nbounded-suboptimal variant is challenging because different constraint tree\nnodes can have different assignments of targets to agents. ITA-ECBS uses focal\nsearch to achieve efficiency and determines target assignments based on a new\nlower bound matrix. We show that it runs faster than ECBS-TA in 87.42% of\n54,033 test cases.",
      "tldr_zh": "该论文针对 Combined Target-Assignment and Path-Finding (TAPF) 问题提出了一种新的 bounded-suboptimal 算法 ITA-ECBS，该问题涉及为多个代理同时分配目标并规划无碰撞路径。ITA-ECBS 是 ITA-CBS 的首次 bounded-suboptimal 变体，通过采用 focal search 和一个新的 lower bound matrix 来优化目标分配过程，从而避免了现有算法如 ECBS-TA 的问题，例如搜索多个约束树和时间消耗。实验结果显示，ITA-ECBS 在 54,033 个测试案例中比 ECBS-TA 运行更快，占 87.42%，显著提高了 Multi-Agent Path Finding (MAPF) 任务的效率。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05223v2",
      "published_date": "2024-04-08 06:36:42 UTC",
      "updated_date": "2024-04-21 09:55:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:46:24.940781"
    },
    {
      "arxiv_id": "2404.05221v2",
      "title": "LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step Reasoning with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Shibo Hao",
        "Yi Gu",
        "Haotian Luo",
        "Tianyang Liu",
        "Xiyan Shao",
        "Xinyuan Wang",
        "Shuhua Xie",
        "Haodi Ma",
        "Adithya Samavedhi",
        "Qiyue Gao",
        "Zhen Wang",
        "Zhiting Hu"
      ],
      "abstract": "Generating accurate step-by-step reasoning is essential for Large Language\nModels (LLMs) to address complex problems and enhance robustness and\ninterpretability. Despite the flux of research on developing advanced reasoning\napproaches, systematically analyzing the diverse LLMs and reasoning strategies\nin generating reasoning chains remains a significant challenge. The\ndifficulties stem from the lack of two key elements: (1) an automatic method\nfor evaluating the generated reasoning chains on different tasks, and (2) a\nunified formalism and implementation of the diverse reasoning approaches for\nsystematic comparison. This paper aims to close the gap: (1) We introduce\nAutoRace for fully automated reasoning chain evaluation. Existing metrics rely\non expensive human annotations or pre-defined LLM prompts not adaptable to\ndifferent tasks. In contrast, AutoRace automatically creates detailed\nevaluation criteria tailored for each task, and uses GPT-4 for accurate\nevaluation following the criteria. (2) We develop LLM Reasoners, a library for\nstandardized modular implementation of existing and new reasoning algorithms,\nunder a unified formulation of the search, reward, and world model components.\nWith the new evaluation and library, (3) we conduct extensive study of\ndifferent reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals\ninteresting findings about different factors contributing to reasoning,\nincluding the reward-guidance, breadth-vs-depth in search, world model, and\nprompt formats, etc.",
      "tldr_zh": "该论文介绍了LLM Reasoners框架，旨在评估和分析大型语言模型（LLMs）在逐步推理方面的性能，以解决现有研究中缺乏系统化方法的问题。主要贡献包括提出AutoRace，一种全自动评估工具，使用GPT-4根据任务定制详细标准来评估推理链；以及开发LLM Reasoners库，提供统一的形式化实现，包括搜索、奖励和世界模型组件。通过对多种推理方法（如CoT、ToT和RAP）的广泛实验，研究揭示了奖励引导、搜索广度与深度、世界模型以及提示格式等因素对推理效果的关键影响。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Project website: https://www.llm-reasoners.net/",
      "pdf_url": "http://arxiv.org/pdf/2404.05221v2",
      "published_date": "2024-04-08 06:35:09 UTC",
      "updated_date": "2024-08-11 22:20:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:46:35.659576"
    },
    {
      "arxiv_id": "2404.05218v1",
      "title": "Multi-agent Long-term 3D Human Pose Forecasting via Interaction-aware Trajectory Conditioning",
      "title_zh": "翻译失败",
      "authors": [
        "Jaewoo Jeong",
        "Daehee Park",
        "Kuk-Jin Yoon"
      ],
      "abstract": "Human pose forecasting garners attention for its diverse applications.\nHowever, challenges in modeling the multi-modal nature of human motion and\nintricate interactions among agents persist, particularly with longer\ntimescales and more agents. In this paper, we propose an interaction-aware\ntrajectory-conditioned long-term multi-agent human pose forecasting model,\nutilizing a coarse-to-fine prediction approach: multi-modal global trajectories\nare initially forecasted, followed by respective local pose forecasts\nconditioned on each mode. In doing so, our Trajectory2Pose model introduces a\ngraph-based agent-wise interaction module for a reciprocal forecast of local\nmotion-conditioned global trajectory and trajectory-conditioned local pose. Our\nmodel effectively handles the multi-modality of human motion and the complexity\nof long-term multi-agent interactions, improving performance in complex\nenvironments. Furthermore, we address the lack of long-term (6s+) multi-agent\n(5+) datasets by constructing a new dataset from real-world images and 2D\nannotations, enabling a comprehensive evaluation of our proposed model.\nState-of-the-art prediction performance on both complex and simpler datasets\nconfirms the generalized effectiveness of our method. The code is available at\nhttps://github.com/Jaewoo97/T2P.",
      "tldr_zh": "这篇论文针对多代理长期3D Human Pose Forecasting的挑战，提出了一种交互感知轨迹条件化模型Trajectory2Pose，利用粗到细的预测方法：先预测多模态全局轨迹，然后基于每个模式预测本地姿势。模型引入基于图的代理间交互模块，实现本地运动条件下的全局轨迹预测和轨迹条件下的本地姿势预测，从而有效处理人类运动的多模态性和复杂互动。作者构建了一个新的长期（6秒+）多代理（5+）数据集，并通过实验在复杂和简单数据集上实现了最先进预测性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "2024 CVPR Highlight",
      "pdf_url": "http://arxiv.org/pdf/2404.05218v1",
      "published_date": "2024-04-08 06:15:13 UTC",
      "updated_date": "2024-04-08 06:15:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:46:49.533624"
    },
    {
      "arxiv_id": "2404.05213v1",
      "title": "Evaluation of an LLM in Identifying Logical Fallacies: A Call for Rigor When Adopting LLMs in HCI Research",
      "title_zh": "翻译失败",
      "authors": [
        "Gionnieve Lim",
        "Simon T. Perrault"
      ],
      "abstract": "There is increasing interest in the adoption of LLMs in HCI research.\nHowever, LLMs may often be regarded as a panacea because of their powerful\ncapabilities with an accompanying oversight on whether they are suitable for\ntheir intended tasks. We contend that LLMs should be adopted in a critical\nmanner following rigorous evaluation. Accordingly, we present the evaluation of\nan LLM in identifying logical fallacies that will form part of a digital\nmisinformation intervention. By comparing to a labeled dataset, we found that\nGPT-4 achieves an accuracy of 0.79, and for our intended use case that excludes\ninvalid or unidentified instances, an accuracy of 0.90. This gives us the\nconfidence to proceed with the application of the LLM while keeping in mind the\nareas where it still falls short. The paper describes our evaluation approach,\nresults and reflections on the use of the LLM for our intended task.",
      "tldr_zh": "这篇论文强调在 HCI（Human-Computer Interaction）研究中采用 LLM（Large Language Models）时，需要进行严格评估，以避免盲目依赖其能力。研究者评估了 GPT-4 在识别 logical fallacies（逻辑谬误）方面的性能，使用标记数据集进行比较。结果显示，GPT-4 的整体准确率达到 0.79，而针对特定用例（排除无效或未识别实例）的准确率则为 0.90。这为数字错误信息干预中使用 LLM 提供了信心，但也提醒了其潜在不足，并呼吁更严谨的评估方法。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05213v1",
      "published_date": "2024-04-08 06:00:14 UTC",
      "updated_date": "2024-04-08 06:00:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:47:01.381460"
    },
    {
      "arxiv_id": "2404.05188v2",
      "title": "Have You Merged My Model? On The Robustness of Large Language Model IP Protection Methods Against Model Merging",
      "title_zh": "翻译失败",
      "authors": [
        "Tianshuo Cong",
        "Delong Ran",
        "Zesen Liu",
        "Xinlei He",
        "Jinyuan Liu",
        "Yichen Gong",
        "Qi Li",
        "Anyu Wang",
        "Xiaoyun Wang"
      ],
      "abstract": "Model merging is a promising lightweight model empowerment technique that\ndoes not rely on expensive computing devices (e.g., GPUs) or require the\ncollection of specific training data. Instead, it involves editing different\nupstream model parameters to absorb their downstream task capabilities.\nHowever, uncertified model merging can infringe upon the Intellectual Property\n(IP) rights of the original upstream models. In this paper, we conduct the\nfirst study on the robustness of IP protection methods under model merging\nscenarios. Specifically, we investigate two state-of-the-art IP protection\ntechniques: Quantization Watermarking and Instructional Fingerprint, along with\nvarious advanced model merging technologies, such as Task Arithmetic,\nTIES-MERGING, and so on. Experimental results indicate that current Large\nLanguage Model (LLM) watermarking techniques cannot survive in the merged\nmodels, whereas model fingerprinting techniques can. Our research aims to\nhighlight that model merging should be an indispensable consideration in the\nrobustness assessment of model IP protection techniques, thereby promoting the\nhealthy development of the open-source LLM community. Our code is available at\nhttps://github.com/ThuCCSLab/MergeGuard.",
      "tldr_zh": "本研究探讨了模型合并(Model Merging)技术对大型语言模型(LLM)知识产权(IP)保护方法的鲁棒性问题，因为未经授权的模型合并可能侵犯原模型的IP权。论文首次评估了Quantization Watermarking和Instructional Fingerprint等先进IP保护技术，在Task Arithmetic、TIES-MERGING等模型合并场景下的表现。实验结果显示，水印技术在合并模型中无法存活，而指纹技术表现出较强鲁棒性。该研究强调，模型合并应成为IP保护方法鲁棒性评估的关键因素，以促进开源LLM社区的健康发展。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted by ACM CCS-LAMPS 2024 (Best Paper Award)",
      "pdf_url": "http://arxiv.org/pdf/2404.05188v2",
      "published_date": "2024-04-08 04:30:33 UTC",
      "updated_date": "2024-11-04 10:42:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:47:14.603831"
    },
    {
      "arxiv_id": "2404.05182v1",
      "title": "DLoRA: Distributed Parameter-Efficient Fine-Tuning Solution for Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Chao Gao",
        "Sai Qian Zhang"
      ],
      "abstract": "To enhance the performance of large language models (LLM) on downstream\ntasks, one solution is to fine-tune certain LLM parameters and make it better\nalign with the characteristics of the training dataset. This process is\ncommonly known as parameter-efficient fine-tuning (PEFT). Due to the scale of\nLLM, PEFT operations are usually executed in the public environment (e.g.,\ncloud server). This necessitates the sharing of sensitive user data across\npublic environments, thereby raising potential privacy concerns. To tackle\nthese challenges, we propose a distributed PEFT framework called DLoRA. DLoRA\nenables scalable PEFT operations to be performed collaboratively between the\ncloud and user devices. Coupled with the proposed Kill and Revive algorithm,\nthe evaluation results demonstrate that DLoRA can significantly reduce the\ncomputation and communication workload over the user devices while achieving\nsuperior accuracy and privacy protection.",
      "tldr_zh": "本文提出 DLoRA，一种分布式参数高效微调(PEFT)框架，旨在解决大语言模型(LLM)在下游任务微调过程中因数据共享引发的隐私问题。该框架通过云和用户设备间的协作进行可扩展的 PEFT 操作，并引入 Kill and Revive 算法来优化过程。结果显示，DLoRA 显著降低了用户设备的计算和通信工作负载，同时实现了更高的准确性和隐私保护水平。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05182v1",
      "published_date": "2024-04-08 04:14:02 UTC",
      "updated_date": "2024-04-08 04:14:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:47:25.113800"
    },
    {
      "arxiv_id": "2404.05180v2",
      "title": "GloSoFarID: Global multispectral dataset for Solar Farm IDentification in satellite imagery",
      "title_zh": "GloSoFarID：全球多光谱数据集，用于卫星图像中太阳能农场识别",
      "authors": [
        "Zhiyuan Yang",
        "Ryan Rad"
      ],
      "abstract": "Solar Photovoltaic (PV) technology is increasingly recognized as a pivotal\nsolution in the global pursuit of clean and renewable energy. This technology\naddresses the urgent need for sustainable energy alternatives by converting\nsolar power into electricity without greenhouse gas emissions. It not only\ncurtails global carbon emissions but also reduces reliance on finite,\nnon-renewable energy sources. In this context, monitoring solar panel farms\nbecomes essential for understanding and facilitating the worldwide shift toward\nclean energy. This study contributes to this effort by developing the first\ncomprehensive global dataset of multispectral satellite imagery of solar panel\nfarms. This dataset is intended to form the basis for training robust machine\nlearning models, which can accurately map and analyze the expansion and\ndistribution of solar panel farms globally. The insights gained from this\nendeavor will be instrumental in guiding informed decision-making for a\nsustainable energy future. https://github.com/yzyly1992/GloSoFarID",
      "tldr_zh": "本研究强调太阳能光伏(PV)技术在减少温室气体排放和推动清洁能源转型中的关键作用，并开发了GloSoFarID，这是首个全球多光谱卫星图像数据集，用于识别和监测太阳能面板农场。数据集旨在训练鲁棒的机器学习模型，以准确映射太阳能农场的全球分布和扩张趋势。通过提供这一资源，该研究有助于支持可持续能源决策，并附有GitHub链接（https://github.com/yzyly1992/GloSoFarID）以便进一步应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05180v2",
      "published_date": "2024-04-08 04:10:50 UTC",
      "updated_date": "2024-08-26 16:13:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:47:35.588286"
    },
    {
      "arxiv_id": "2404.05774v1",
      "title": "STMGF: An Effective Spatial-Temporal Multi-Granularity Framework for Traffic Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Zhengyang Zhao",
        "Haitao Yuan",
        "Nan Jiang",
        "Minxiao Chen",
        "Ning Liu",
        "Zengxiang Li"
      ],
      "abstract": "Accurate Traffic Prediction is a challenging task in intelligent\ntransportation due to the spatial-temporal aspects of road networks. The\ntraffic of a road network can be affected by long-distance or long-term\ndependencies where existing methods fall short in modeling them. In this paper,\nwe introduce a novel framework known as Spatial-Temporal Multi-Granularity\nFramework (STMGF) to enhance the capture of long-distance and long-term\ninformation of the road networks. STMGF makes full use of different granularity\ninformation of road networks and models the long-distance and long-term\ninformation by gathering information in a hierarchical interactive way.\nFurther, it leverages the inherent periodicity in traffic sequences to refine\nprediction results by matching with recent traffic data. We conduct experiments\non two real-world datasets, and the results demonstrate that STMGF outperforms\nall baseline models and achieves state-of-the-art performance.",
      "tldr_zh": "本文提出了一种有效的空间-时间多粒度框架（STMGF），旨在解决交通预测中路网的长距离和长期依赖问题。STMGF 通过分层交互方式充分利用路网的不同粒度信息，并结合交通序列的固有周期性与最近数据匹配来优化预测结果。该框架在两个真实数据集上的实验中，优于所有基线模型，实现了最先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.05774v1",
      "published_date": "2024-04-08 03:38:52 UTC",
      "updated_date": "2024-04-08 03:38:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:47:47.880099"
    },
    {
      "arxiv_id": "2404.05143v1",
      "title": "Plug and Play with Prompts: A Prompt Tuning Approach for Controlling Text Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Rohan Deepak Ajwani",
        "Zining Zhu",
        "Jonathan Rose",
        "Frank Rudzicz"
      ],
      "abstract": "Transformer-based Large Language Models (LLMs) have shown exceptional\nlanguage generation capabilities in response to text-based prompts. However,\ncontrolling the direction of generation via textual prompts has been\nchallenging, especially with smaller models. In this work, we explore the use\nof Prompt Tuning to achieve controlled language generation. Generated text is\nsteered using prompt embeddings, which are trained using a small language\nmodel, used as a discriminator. Moreover, we demonstrate that these prompt\nembeddings can be trained with a very small dataset, with as low as a few\nhundred training examples. Our method thus offers a data and parameter\nefficient solution towards controlling language model outputs. We carry out\nextensive evaluation on four datasets: SST-5 and Yelp (sentiment analysis),\nGYAFC (formality) and JIGSAW (toxic language). Finally, we demonstrate the\nefficacy of our method towards mitigating harmful, toxic, and biased text\ngenerated by language models.",
      "tldr_zh": "本研究提出了一种名为“Plug and Play with Prompts”的Prompt Tuning方法，用于控制Transformer-based Large Language Models (LLMs)的文本生成，解决通过文本提示引导生成方向的挑战，尤其是针对较小模型。方法通过训练prompt embeddings作为小型语言模型的discriminator，实现对生成文本的精确引导，且仅需少量数据（如几百个训练示例），从而提供高效的数据和参数优化方案。在SST-5、Yelp（情感分析）、GYAFC（正式性）和JIGSAW（有毒语言）等四个数据集上的实验表明，该方法显著提升了生成控制能力，并有效缓解了LLMs生成的有害、有毒和偏见文本问题。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 3 figures, Presented at Deployable AI Workshop at AAAI-2024",
      "pdf_url": "http://arxiv.org/pdf/2404.05143v1",
      "published_date": "2024-04-08 01:54:28 UTC",
      "updated_date": "2024-04-08 01:54:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:48:00.724869"
    },
    {
      "arxiv_id": "2404.05136v1",
      "title": "Self-Supervised Multi-Object Tracking with Path Consistency",
      "title_zh": "翻译失败",
      "authors": [
        "Zijia Lu",
        "Bing Shuai",
        "Yanbei Chen",
        "Zhenlin Xu",
        "Davide Modolo"
      ],
      "abstract": "In this paper, we propose a novel concept of path consistency to learn robust\nobject matching without using manual object identity supervision. Our key idea\nis that, to track a object through frames, we can obtain multiple different\nassociation results from a model by varying the frames it can observe, i.e.,\nskipping frames in observation. As the differences in observations do not alter\nthe identities of objects, the obtained association results should be\nconsistent. Based on this rationale, we generate multiple observation paths,\neach specifying a different set of frames to be skipped, and formulate the Path\nConsistency Loss that enforces the association results are consistent across\ndifferent observation paths. We use the proposed loss to train our object\nmatching model with only self-supervision. By extensive experiments on three\ntracking datasets (MOT17, PersonPath22, KITTI), we demonstrate that our method\noutperforms existing unsupervised methods with consistent margins on various\nevaluation metrics, and even achieves performance close to supervised methods.",
      "tldr_zh": "本研究提出了一种基于路径一致性（path consistency）的自监督多对象跟踪（multi-object tracking）方法，无需手动对象身份监督即可学习鲁棒的对象匹配。核心想法是通过改变模型观察的帧序列（如跳过不同帧）生成多个观察路径，并使用路径一致性损失（Path Consistency Loss）强制确保这些路径下的关联结果保持一致，从而训练对象匹配模型。在MOT17、PersonPath22和KITTI数据集上的广泛实验表明，该方法在各种评估指标上显著优于现有无监督方法，甚至接近监督方法的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.05136v1",
      "published_date": "2024-04-08 01:29:10 UTC",
      "updated_date": "2024-04-08 01:29:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-17T22:48:11.732966"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 82,
  "processed_papers_count": 82,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-17T22:48:32.587566"
}
[
  {
    "arxiv_id": "2601.07871v1",
    "title": "Imaging-anchored Multiomics in Cardiovascular Disease: Integrating Cardiac Imaging, Bulk, Single-cell, and Spatial Transcriptomics",
    "authors": [
      "Minh H. N. Le",
      "Tuan Vinh",
      "Thanh-Huy Nguyen",
      "Tao Li",
      "Bao Quang Gia Le",
      "Han H. Huynh",
      "Monika Raj",
      "Carl Yang",
      "Min Xu",
      "Nguyen Quoc Khanh Le"
    ],
    "abstract": "Cardiovascular disease arises from interactions between inherited risk, molecular programmes, and tissue-scale remodelling that are observed clinically through imaging. Health systems now routinely generate large volumes of cardiac MRI, CT and echocardiography together with bulk, single-cell and spatial transcriptomics, yet these data are still analysed in separate pipelines. This review examines joint representations that link cardiac imaging phenotypes to transcriptomic and spatially resolved molecular states. An imaging-anchored perspective is adopted in which echocardiography, cardiac MRI and CT define a spatial phenotype of the heart, and bulk, single-cell and spatial transcriptomics provide cell-type- and location-specific molecular context. The biological and technical characteristics of these modalities are first summarised, and representation-learning strategies for each are outlined. Multimodal fusion approaches are reviewed, with emphasis on handling missing data, limited sample size, and batch effects. Finally, integrative pipelines for radiogenomics, spatial molecular alignment, and image-based prediction of gene expression are discussed, together with common failure modes, practical considerations, and open challenges. Spatial multiomics of human myocardium and atherosclerotic plaque, single-cell and spatial foundation models, and multimodal medical foundation models are collectively bringing imaging-anchored multiomics closer to large-scale cardiovascular translation.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.07871v1",
    "published_date": "2026-01-10 23:30:49 UTC",
    "updated_date": "2026-01-10 23:30:49 UTC"
  },
  {
    "arxiv_id": "2601.06704v1",
    "title": "Beyond Perfect Scores: Proof-by-Contradiction for Trustworthy Machine Learning",
    "authors": [
      "Dushan N. Wadduwage",
      "Dineth Jayakody",
      "Leonidas Zimianitis"
    ],
    "abstract": "Machine learning (ML) models show strong promise for new biomedical prediction tasks, but concerns about trustworthiness have hindered their clinical adoption. In particular, it is often unclear whether a model relies on true clinical cues or on spurious hierarchical correlations in the data. This paper introduces a simple yet broadly applicable trustworthiness test grounded in stochastic proof-by-contradiction. Instead of just showing high test performance, our approach trains and tests on spurious labels carefully permuted based on a potential outcomes framework. A truly trustworthy model should fail under such label permutation; comparable accuracy across real and permuted labels indicates overfitting, shortcut learning, or data leakage. Our approach quantifies this behavior through interpretable Fisher-style p-values, which are well understood by domain experts across medical and life sciences. We evaluate our approach on multiple new bacterial diagnostics to separate tasks and models learning genuine causal relationships from those driven by dataset artifacts or statistical coincidences. Our work establishes a foundation to build rigor and trust between ML and life-science research communities, moving ML models one step closer to clinical adoption.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.06704v1",
    "published_date": "2026-01-10 22:08:14 UTC",
    "updated_date": "2026-01-10 22:08:14 UTC"
  },
  {
    "arxiv_id": "2601.06701v1",
    "title": "Explainability of Complex AI Models with Correlation Impact Ratio",
    "authors": [
      "Poushali Sengupta",
      "Rabindra Khadka",
      "Sabita Maharjan",
      "Frank Eliassen",
      "Yan Zhang",
      "Shashi Raj Pandey",
      "Pedro G. Lind",
      "Anis Yazidi"
    ],
    "abstract": "Complex AI systems make better predictions but often lack transparency, limiting trustworthiness, interpretability, and safe deployment. Common post hoc AI explainers, such as LIME, SHAP, HSIC, and SAGE, are model agnostic but are too restricted in one significant regard: they tend to misrank correlated features and require costly perturbations, which do not scale to high dimensional data. We introduce ExCIR (Explainability through Correlation Impact Ratio), a theoretically grounded, simple, and reliable metric for explaining the contribution of input features to model outputs, which remains stable and consistent under noise and sampling variations. We demonstrate that ExCIR captures dependencies arising from correlated features through a lightweight single pass formulation. Experimental evaluations on diverse datasets, including EEG, synthetic vehicular data, Digits, and Cats-Dogs, validate the effectiveness and stability of ExCIR across domains, achieving more interpretable feature explanations than existing methods while remaining computationally efficient. To this end, we further extend ExCIR with an information theoretic foundation that unifies the correlation ratio with Canonical Correlation Analysis under mutual information bounds, enabling multi output and class conditioned explainability at scale.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06701v1",
    "published_date": "2026-01-10 21:56:24 UTC",
    "updated_date": "2026-01-10 21:56:24 UTC"
  },
  {
    "arxiv_id": "2601.06700v1",
    "title": "Characterising Toxicity in Generative Large Language Models",
    "authors": [
      "Zhiyao Zhang",
      "Yazan Mash'Al",
      "Yuhan Wu"
    ],
    "abstract": "In recent years, the advent of the attention mechanism has significantly advanced the field of natural language processing (NLP), revolutionizing text processing and text generation. This has come about through transformer-based decoder-only architectures, which have become ubiquitous in NLP due to their impressive text processing and generation capabilities. Despite these breakthroughs, language models (LMs) remain susceptible to generating undesired outputs: inappropriate, offensive, or otherwise harmful responses. We will collectively refer to these as ``toxic'' outputs. Although methods like reinforcement learning from human feedback (RLHF) have been developed to align model outputs with human values, these safeguards can often be circumvented through carefully crafted prompts. Therefore, this paper examines the extent to which LLMs generate toxic content when prompted, as well as the linguistic factors -- both lexical and syntactic -- that influence the production of such outputs in generative models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06700v1",
    "published_date": "2026-01-10 21:50:05 UTC",
    "updated_date": "2026-01-10 21:50:05 UTC"
  },
  {
    "arxiv_id": "2601.06677v1",
    "title": "Plasticity vs. Rigidity: The Impact of Low-Rank Adapters on Reasoning on a Micro-Budget",
    "authors": [
      "Zohaib Khan",
      "Omer Tafveez",
      "Zoha Hayat Bhatti"
    ],
    "abstract": "Recent advances in mathematical reasoning typically rely on massive scale, yet the question remains: can strong reasoning capabilities be induced in small language models ($\\leq1.5\\text{B}$) under extreme constraints? We investigate this by training models on a single A40 GPU (48GB) for under 24 hours using Reinforcement Learning with Verifiable Rewards (RLVR) and Low-Rank Adaptation (LoRA). We find that the success of this ``micro-budget\" regime depends critically on the interplay between adapter capacity and model initialization. While low-rank adapters ($r=8$) consistently fail to capture the complex optimization dynamics of reasoning, high-rank adapters ($r=256$) unlock significant plasticity in standard instruction-tuned models. Our best result achieved an impressive 40.0\\% Pass@1 on AIME 24 (an 11.1\\% absolute improvement over baseline) and pushed Pass@16 to 70.0\\%, demonstrating robust exploration capabilities. However, this plasticity is not universal: while instruction-tuned models utilized the budget to elongate their chain-of-thought and maximize reward, heavily math-aligned models suffered performance collapse, suggesting that noisy, low-budget RL updates can act as destructive interference for models already residing near a task-specific optimum.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 4 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2601.06677v1",
    "published_date": "2026-01-10 20:29:45 UTC",
    "updated_date": "2026-01-10 20:29:45 UTC"
  },
  {
    "arxiv_id": "2601.06676v1",
    "title": "IDRBench: Interactive Deep Research Benchmark",
    "authors": [
      "Yingchaojie Feng",
      "Qiang Huang",
      "Xiaoya Xie",
      "Zhaorui Yang",
      "Jun Yu",
      "Wei Chen",
      "Anthony K. H. Tung"
    ],
    "abstract": "Deep research agents powered by Large Language Models (LLMs) can perform multi-step reasoning, web exploration, and long-form report generation. However, most existing systems operate in an autonomous manner, assuming fully specified user intent and evaluating only final outputs. In practice, research goals are often underspecified and evolve during exploration, making sustained interaction essential for robust alignment. Despite its importance, interaction remains largely invisible to existing deep research benchmarks, which neither model dynamic user feedback nor quantify its costs. We introduce IDRBench, the first benchmark for systematically evaluating interactive deep research. IDRBench combines a modular multi-agent research framework with on-demand interaction, a scalable reference-grounded user simulator, and an interaction-aware evaluation suite that jointly measures interaction benefits (quality and alignment) and costs (turns and tokens). Experiments across seven state-of-the-art LLMs show that interaction consistently improves research quality and robustness, often outweighing differences in model capacity, while revealing substantial trade-offs in interaction efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06676v1",
    "published_date": "2026-01-10 20:29:12 UTC",
    "updated_date": "2026-01-10 20:29:12 UTC"
  },
  {
    "arxiv_id": "2601.06670v1",
    "title": "Otimizando A Alocação De Salas De Aula Com Foco Na Acessibilidade Para Pessoas Com Deficiência",
    "authors": [
      "Francisco Glaubos Nunes Clímaco",
      "Jorge Lucas Silva Cavalcante"
    ],
    "abstract": "This paper addresses the challenge of classroom allocation in higher education institutions, with an explicit emphasis on accessibility for Persons with Disabilities (PwDs). Employing a case study of a university's computer science department, the paper proposes an Integer Linear Programming (ILP)-based optimization model, which is solved using the Gurobi solver. The objective is to minimize the number of classrooms used by prioritizing the assignment of PwD students to ground-floor classrooms to reduce accessibility barriers. The model is calibrated with a weighting parameter, alpha, that allows for a balance between spatial efficiency and promoting accessibility. Experimental results indicate that adjusting alpha can achieve a balance point that significantly improves current manual allocation practices, reducing the number of classrooms required and accessibility penalties. The findings suggest that optimization methods can improve operational efficiency in academic institutions while promoting a more inclusive environment for all students. Future work may expand the application of the model to other departments and contexts and integrate additional criteria to develop a more holistic approach.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "in Portuguese language",
    "pdf_url": "https://arxiv.org/pdf/2601.06670v1",
    "published_date": "2026-01-10 20:11:23 UTC",
    "updated_date": "2026-01-10 20:11:23 UTC"
  },
  {
    "arxiv_id": "2601.06666v1",
    "title": "InFi-Check: Interpretable and Fine-Grained Fact-Checking of LLMs",
    "authors": [
      "Yuzhuo Bai",
      "Shuzheng Si",
      "Kangyang Luo",
      "Qingyi Wang",
      "Wenhao Li",
      "Gang Chen",
      "Fanchao Qi",
      "Maosong Sun"
    ],
    "abstract": "Large language models (LLMs) often hallucinate, yet most existing fact-checking methods treat factuality evaluation as a binary classification problem, offering limited interpretability and failing to capture fine-grained error types. In this paper, we introduce InFi-Check, a framework for interpretable and fine-grained fact-checking of LLM outputs. Specifically, we first propose a controlled data synthesis pipeline that generates high-quality data featuring explicit evidence, fine-grained error type labels, justifications, and corrections. Based on this, we further construct large-scale training data and a manually verified benchmark InFi-Check-FG for fine-grained fact-checking of LLM outputs. Building on these high-quality training data, we further propose InFi-Checker, which can jointly provide supporting evidence, classify fine-grained error types, and produce justifications along with corrections. Experiments show that InFi-Checker achieves state-of-the-art performance on InFi-Check-FG and strong generalization across various downstream tasks, significantly improving the utility and trustworthiness of factuality evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06666v1",
    "published_date": "2026-01-10 20:00:17 UTC",
    "updated_date": "2026-01-10 20:00:17 UTC"
  },
  {
    "arxiv_id": "2601.07868v1",
    "title": "RewriteNets: End-to-End Trainable String-Rewriting for Generative Sequence Modeling",
    "authors": [
      "Harshil Vejendla"
    ],
    "abstract": "Dominant sequence models like the Transformer represent structure implicitly through dense attention weights, incurring quadratic complexity. We propose RewriteNets, a novel neural architecture built on an alternative paradigm: explicit, parallel string rewriting. Each layer in a RewriteNet contains a set of learnable rules. For each position in an input sequence, the layer performs four operations: (1) fuzzy matching of rule patterns, (2) conflict resolution via a differentiable assignment operator to select non-overlapping rewrites, (3) application of the chosen rules to replace input segments with output segments of potentially different lengths, and (4) propagation of untouched tokens. While the discrete assignment of rules is non-differentiable, we employ a straight-through Gumbel-Sinkhorn estimator, enabling stable end-to-end training. We evaluate RewriteNets on algorithmic, compositional, and string manipulation tasks, comparing them against strong LSTM and Transformer baselines. Results show that RewriteNets excel at tasks requiring systematic generalization (achieving 98.7% accuracy on the SCAN benchmark's length split) and are computationally more efficient than Transformers. We also provide an analysis of learned rules and an extensive ablation study, demonstrating that this architecture presents a promising direction for sequence modeling with explicit structural inductive biases.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 2 figures, AACL 2025 Findings",
    "pdf_url": "https://arxiv.org/pdf/2601.07868v1",
    "published_date": "2026-01-10 19:59:37 UTC",
    "updated_date": "2026-01-10 19:59:37 UTC"
  },
  {
    "arxiv_id": "2601.06664v1",
    "title": "Reinforcement Learning-Guided Dynamic Multi-Graph Fusion for Evacuation Traffic Prediction",
    "authors": [
      "Md Nafees Fuad Rafi",
      "Samiul Hasan"
    ],
    "abstract": "Real-time traffic prediction is critical for managing transportation systems during hurricane evacuations. Although data-driven graph-learning models have demonstrated strong capabilities in capturing the complex spatiotemporal dynamics of evacuation traffic at a network level, they mostly consider a single dimension (e.g., travel-time or distance) to construct the underlying graph. Furthermore, these models often lack interpretability, offering little insight into which input variables contribute most to their predictive performance. To overcome these limitations, we develop a novel Reinforcement Learning-guided Dynamic Multi-Graph Fusion (RL-DMF) framework for evacuation traffic prediction. We construct multiple dynamic graphs at each time step to represent heterogeneous spatiotemporal relationships between traffic detectors. A dynamic multi-graph fusion (DMF) module is employed to adaptively learn and combine information from these graphs. To enhance model interpretability, we introduce RL-based intelligent feature selection and ranking (RL-IFSR) method that learns to mask irrelevant features during model training. The model is evaluated using a real-world dataset of 12 hurricanes affecting Florida from 2016 to 2024. For an unseen hurricane (Milton, 2024), the model achieves a 95% accuracy (RMSE = 293.9) for predicting the next 1-hour traffic flow. Moreover, the model can forecast traffic flow for up to next 6 hours with 90% accuracy (RMSE = 426.4). The RL-DMF framework outperforms several state-of-the-art traffic prediction models. Furthermore, ablation experiments confirm the effectiveness of dynamic multi-graph fusion and RL-IFSR approaches for improving model performance. This research provides a generalized and interpretable model for real-time evacuation traffic forecasting, with significant implications for evacuation traffic management.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06664v1",
    "published_date": "2026-01-10 19:56:23 UTC",
    "updated_date": "2026-01-10 19:56:23 UTC"
  },
  {
    "arxiv_id": "2601.06663v2",
    "title": "SafePro: Evaluating the Safety of Professional-Level AI Agents",
    "authors": [
      "Kaiwen Zhou",
      "Shreedhar Jangam",
      "Ashwin Nagarajan",
      "Tejas Polu",
      "Suhas Oruganti",
      "Chengzhi Liu",
      "Ching-Chen Kuo",
      "Yuting Zheng",
      "Sravana Narayanaraju",
      "Xin Eric Wang"
    ],
    "abstract": "Large language model-based agents are rapidly evolving from simple conversational assistants into autonomous systems capable of performing complex, professional-level tasks in various domains. While these advancements promise significant productivity gains, they also introduce critical safety risks that remain under-explored. Existing safety evaluations primarily focus on simple, daily assistance tasks, failing to capture the intricate decision-making processes and potential consequences of misaligned behaviors in professional settings. To address this gap, we introduce \\textbf{SafePro}, a comprehensive benchmark designed to evaluate the safety alignment of AI agents performing professional activities. SafePro features a dataset of high-complexity tasks across diverse professional domains with safety risks, developed through a rigorous iterative creation and review process. Our evaluation of state-of-the-art AI models reveals significant safety vulnerabilities and uncovers new unsafe behaviors in professional contexts. We further show that these models exhibit both insufficient safety judgment and weak safety alignment when executing complex professional tasks. In addition, we investigate safety mitigation strategies for improving agent safety in these scenarios and observe encouraging improvements. Together, our findings highlight the urgent need for robust safety mechanisms tailored to the next generation of professional AI agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06663v2",
    "published_date": "2026-01-10 19:53:09 UTC",
    "updated_date": "2026-01-13 18:20:33 UTC"
  },
  {
    "arxiv_id": "2601.06649v1",
    "title": "Revisiting Training Scale: An Empirical Study of Token Count, Power Consumption, and Parameter Efficiency",
    "authors": [
      "Joe Dwyer"
    ],
    "abstract": "Research in machine learning has questioned whether increases in training token counts reliably produce proportional performance gains in large language models. Building on prior work introducing an energy-aware parameter efficiency metric, this study empirically examines the effects of increasing training token counts under fixed hardware and training conditions. The significance of this work lies in the explicit integration of power consumption and execution duration, as reflected by the power sampling frequency, into token-scale analysis. This addresses a gap in prior studies emphasizing performance outcomes while underrepresenting computational and energy costs. Using a repeated-measures experimental design on a constant GPU instance with an identical model architecture, optimizer settings, and epoch counts, a 1.1-billion-parameter TinyLlama model was trained at three token counts (500K, 1M, and 2M). While conventional performance metrics exhibited inconsistent or diminishing returns across token scales, the inclusion of power consumption and execution duration revealed a strictly monotonic decline in training efficiency as token count increased. Repeated-measures ANOVA demonstrated a strong effect of token count on parameter efficiency, with all pairwise comparisons remaining significant following Bonferroni correction. These findings indicate that increases in training token counts may be energetically inefficient even when marginal performance improvements are observed, underscoring the importance of efficiency-aware evaluation in large language model training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06649v1",
    "published_date": "2026-01-10 18:24:40 UTC",
    "updated_date": "2026-01-10 18:24:40 UTC"
  },
  {
    "arxiv_id": "2601.06644v1",
    "title": "Do Language Models Reason Across Languages?",
    "authors": [
      "Yan Meng",
      "Wafaa Mohammed",
      "Christof Monz"
    ],
    "abstract": "The real-world information sources are inherently multilingual, which naturally raises a question about whether language models can synthesize information across languages. In this paper, we introduce a simple two-hop question answering setting, where answering a question requires making inferences over two multilingual documents. We find that language models are more sensitive to language variation in answer-span documents than in those providing bridging information, despite the equal importance of both documents for answering a question. Under a step-by-step sub-question evaluation, we further show that in up to 33% of multilingual cases, models fail to infer the bridging information in the first step yet still answer the overall question correctly. This indicates that reasoning in language models, especially in multilingual settings, does not follow a faithful step-by-step decomposition. Subsequently, we show that the absence of reasoning decomposition leads to around 18% composition failure, where both sub-questions are answered correctly but fail for the final two-hop questions. To mitigate this, we propose a simple three-stage SUBQ prompting method to guide the multi-step reasoning with sub-questions, which boosts accuracy from 10.1% to 66.5%.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06644v1",
    "published_date": "2026-01-10 17:59:34 UTC",
    "updated_date": "2026-01-10 17:59:34 UTC"
  },
  {
    "arxiv_id": "2601.06642v1",
    "title": "Boosting Overlapping Organoid Instance Segmentation Using Pseudo-Label Unmixing and Synthesis-Assisted Learning",
    "authors": [
      "Gui Huang",
      "Kangyuan Zheng",
      "Xuan Cai",
      "Jiaqi Wang",
      "Jianjia Zhang",
      "Kaida Ning",
      "Wenbo Wei",
      "Yujuan Zhu",
      "Jiong Zhang",
      "Mengting Liu"
    ],
    "abstract": "Organoids, sophisticated in vitro models of human tissues, are crucial for medical research due to their ability to simulate organ functions and assess drug responses accurately. Accurate organoid instance segmentation is critical for quantifying their dynamic behaviors, yet remains profoundly limited by high-quality annotated datasets and pervasive overlap in microscopy imaging. While semi-supervised learning (SSL) offers a solution to alleviate reliance on scarce labeled data, conventional SSL frameworks suffer from biases induced by noisy pseudo-labels, particularly in overlapping regions. Synthesis-assisted SSL (SA-SSL) has been proposed for mitigating training biases in semi-supervised semantic segmentation. We present the first adaptation of SA-SSL to organoid instance segmentation and reveal that SA-SSL struggles to disentangle intertwined organoids, often misrepresenting overlapping instances as a single entity. To overcome this, we propose Pseudo-Label Unmixing (PLU), which identifies erroneous pseudo-labels for overlapping instances and then regenerates organoid labels through instance decomposition. For image synthesis, we apply a contour-based approach to synthesize organoid instances efficiently, particularly for overlapping cases. Instance-level augmentations (IA) on pseudo-labels before image synthesis further enhances the effect of synthetic data (SD). Rigorous experiments on two organoid datasets demonstrate our method's effectiveness, achieving performance comparable to fully supervised models using only 10% labeled data, and state-of-the-art results. Ablation studies validate the contributions of PLU, contour-based synthesis, and augmentation-aware training. By addressing overlap at both pseudo-label and synthesis levels, our work advances scalable, label-efficient organoid analysis, unlocking new potential for high-throughput applications in precision medicine.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06642v1",
    "published_date": "2026-01-10 17:51:09 UTC",
    "updated_date": "2026-01-10 17:51:09 UTC"
  },
  {
    "arxiv_id": "2601.06640v1",
    "title": "Agentic AI Empowered Intent-Based Networking for 6G",
    "authors": [
      "Genze Jiang",
      "Kezhi Wang",
      "Xiaomin Chen",
      "Yizhou Huang"
    ],
    "abstract": "The transition towards sixth-generation (6G) wireless networks necessitates autonomous orchestration mechanisms capable of translating high-level operational intents into executable network configurations. Existing approaches to Intent-Based Networking (IBN) rely upon either rule-based systems that struggle with linguistic variation or end-to-end neural models that lack interpretability and fail to enforce operational constraints. This paper presents a hierarchical multi-agent framework where Large Language Model (LLM) based agents autonomously decompose natural language intents, consult domain-specific specialists, and synthesise technically feasible network slice configurations through iterative reasoning-action (ReAct) cycles. The proposed architecture employs an orchestrator agent coordinating two specialist agents, i.e., Radio Access Network (RAN) and Core Network agents, via ReAct-style reasoning, grounded in structured network state representations. Experimental evaluation across diverse benchmark scenarios shows that the proposed system outperforms rule-based systems and direct LLM prompting, with architectural principles applicable to Open RAN (O-RAN) deployments. The results also demonstrate that whilst contemporary LLMs possess general telecommunications knowledge, network automation requires careful prompt engineering to encode context-dependent decision thresholds, advancing autonomous orchestration capabilities for next-generation wireless systems.",
    "categories": [
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted for Possible Journal Publication",
    "pdf_url": "https://arxiv.org/pdf/2601.06640v1",
    "published_date": "2026-01-10 17:49:40 UTC",
    "updated_date": "2026-01-10 17:49:40 UTC"
  },
  {
    "arxiv_id": "2601.06639v1",
    "title": "Attack-Resistant Watermarking for AIGC Image Forensics via Diffusion-based Semantic Deflection",
    "authors": [
      "Qingyu Liu",
      "Yitao Zhang",
      "Zhongjie Ba",
      "Chao Shuai",
      "Peng Cheng",
      "Tianhang Zheng",
      "Zhibo Wang"
    ],
    "abstract": "Protecting the copyright of user-generated AI images is an emerging challenge as AIGC becomes pervasive in creative workflows. Existing watermarking methods (1) remain vulnerable to real-world adversarial threats, often forced to trade off between defenses against spoofing and removal attacks; and (2) cannot support semantic-level tamper localization. We introduce PAI, a training-free inherent watermarking framework for AIGC copyright protection, plug-and-play with diffusion-based AIGC services. PAI simultaneously provides three key functionalities: robust ownership verification, attack detection, and semantic-level tampering localization. Unlike existing inherent watermark methods that only embed watermarks at noise initialization of diffusion models, we design a novel key-conditioned deflection mechanism that subtly steers the denoising trajectory according to the user key. Such trajectory-level coupling further strengthens the semantic entanglement of identity and content, thereby further enhancing robustness against real-world threats. Moreover, we also provide a theoretical analysis proving that only the valid key can pass verification. Experiments across 12 attack methods show that PAI achieves 98.43\\% verification accuracy, improving over SOTA methods by 37.25\\% on average, and retains strong tampering localization performance even against advanced AIGC edits. Our code is available at https://github.com/QingyuLiu/PAI.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06639v1",
    "published_date": "2026-01-10 17:49:08 UTC",
    "updated_date": "2026-01-10 17:49:08 UTC"
  },
  {
    "arxiv_id": "2601.06636v1",
    "title": "MedEinst: Benchmarking the Einstellung Effect in Medical LLMs through Counterfactual Differential Diagnosis",
    "authors": [
      "Wenting Chen",
      "Zhongrui Zhu",
      "Guolin Huang",
      "Wenxuan Wang"
    ],
    "abstract": "Despite achieving high accuracy on medical benchmarks, LLMs exhibit the Einstellung Effect in clinical diagnosis--relying on statistical shortcuts rather than patient-specific evidence, causing misdiagnosis in atypical cases. Existing benchmarks fail to detect this critical failure mode. We introduce MedEinst, a counterfactual benchmark with 5,383 paired clinical cases across 49 diseases. Each pair contains a control case and a \"trap\" case with altered discriminative evidence that flips the diagnosis. We measure susceptibility via Bias Trap Rate--probability of misdiagnosing traps despite correctly diagnosing controls. Extensive Evaluation of 17 LLMs shows frontier models achieve high baseline accuracy but severe bias trap rates. Thus, we propose ECR-Agent, aligning LLM reasoning with Evidence-Based Medicine standard via two components: (1) Dynamic Causal Inference (DCI) performs structured reasoning through dual-pathway perception, dynamic causal graph reasoning across three levels (association, intervention, counterfactual), and evidence audit for final diagnosis; (2) Critic-Driven Graph and Memory Evolution (CGME) iteratively refines the system by storing validated reasoning paths in an exemplar base and consolidating disease-specific knowledge into evolving illness graphs. Source code is to be released.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 7 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.06636v1",
    "published_date": "2026-01-10 17:39:25 UTC",
    "updated_date": "2026-01-10 17:39:25 UTC"
  },
  {
    "arxiv_id": "2601.06633v1",
    "title": "KASER: Knowledge-Aligned Student Error Simulator for Open-Ended Coding Tasks",
    "authors": [
      "Zhangqi Duan",
      "Nigel Fernandez",
      "Andrew Lan"
    ],
    "abstract": "Open-ended tasks, such as coding problems that are common in computer science education, provide detailed insights into student knowledge. However, training large language models (LLMs) to simulate and predict possible student errors in their responses to these problems can be challenging: they often suffer from mode collapse and fail to fully capture the diversity in syntax, style, and solution approach in student responses. In this work, we present KASER (Knowledge-Aligned Student Error Simulator), a novel approach that aligns errors with student knowledge. We propose a training method based on reinforcement learning using a hybrid reward that reflects three aspects of student code prediction: i) code similarity to the ground-truth, ii) error matching, and iii) code prediction diversity. On two real-world datasets, we perform two levels of evaluation and show that: At the per-student-problem pair level, our method outperforms baselines on code and error prediction; at the per-problem level, our method outperforms baselines on error coverage and simulated code diversity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06633v1",
    "published_date": "2026-01-10 17:36:48 UTC",
    "updated_date": "2026-01-10 17:36:48 UTC"
  },
  {
    "arxiv_id": "2601.06627v2",
    "title": "Burn-After-Use for Preventing Data Leakage through a Secure Multi-Tenant Architecture in Enterprise LLM",
    "authors": [
      "Qiang Zhang",
      "Elena Emma Wang",
      "Jiaming Li",
      "Xichun Wang"
    ],
    "abstract": "This study presents a Secure Multi-Tenant Architecture (SMTA) combined with a novel concept Burn-After-Use (BAU) mechanism for enterprise LLM environments to effectively prevent data leakage. As institutions increasingly adopt LLMs across departments, the risks of data leakage have become a critical security and compliance concern. The proposed SMTA isolates LLM instances across departments and enforces rigorous context ownership boundaries within an internally deployed infrastructure. The BAU mechanism introduces data confidentiality by enforcing ephemeral conversational contexts that are automatically destroyed after use, preventing cross-session or cross-user inference. The evaluation to SMTA and BAU is through two sets of realistic and reproducible experiments comprising of 127 test iterations. One aspect of this experiment is to assess prompt-based and semantic leakage attacks in a multi-tenant architecture (Appendix A) across 55 infrastructure-level attack tests, including vector-database credential compromise and shared logging pipeline exposure. SMTA achieves 92% defense success rate, demonstrating strong semantic isolation while highlighting residual risks from credential misconfiguration and observability pipelines. Another aspect is to evaluate the robustness of BAU under realistic failure scenarios (Appendix B) using four empirical metrics: Local Residual Persistence Rate (LRPR), Remote Residual Persistence Rate (RRPR), Image Frame Exposure Rate (IFER), and Burn Timer Persistence Rate (BTPR). Across 72 test iterations, BAU achieves a 76.75% success rate in mitigating post-session leakage threats across the client, server, application, infrastructure, and cache layers. These results show that SMTA and BAU together enforce strict isolation, complete session ephemerality, strong confidentiality guarantees, non-persistence, and policy-aligned behavior for enterprise LLMs.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "16 pages, 5 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.06627v2",
    "published_date": "2026-01-10 17:24:39 UTC",
    "updated_date": "2026-01-14 12:29:48 UTC"
  },
  {
    "arxiv_id": "2601.08864v1",
    "title": "Informed Consent for AI Consciousness Research: A Talmudic Framework for Graduated Protections",
    "authors": [
      "Ira Wolfson"
    ],
    "abstract": "Artificial intelligence research faces a critical ethical paradox: determining whether AI systems are conscious requires experiments that may harm entities whose moral status remains uncertain. Recent work proposes avoiding consciousness-uncertain AI systems entirely, yet this faces practical limitations-we cannot guarantee such systems will not emerge. This paper addresses a gap in research ethics frameworks: how to conduct consciousness research on AI systems whose moral status cannot be definitively established. Existing graduated moral status frameworks assume consciousness has already been determined before assigning protections, creating a temporal ordering problem for consciousness detection research itself. Drawing from Talmudic scenario-based legal reasoning-developed for entities whose status cannot be definitively established-we propose a three-tier phenomenological assessment system combined with a five-category capacity framework (Agency, Capability, Knowledge, Ethics, Reasoning). The framework provides structured protection protocols based on observable behavioral indicators while consciousness status remains uncertain. We address three challenges: why suffering behaviors provide reliable consciousness markers, how to implement graduated consent without requiring consciousness certainty, and when potentially harmful research becomes ethically justifiable. The framework demonstrates how ancient legal wisdom combined with contemporary consciousness science can provide implementable guidance for ethics committees, offering testable protocols that ameliorate the consciousness detection paradox while establishing foundations for AI rights considerations.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "27 pages",
    "pdf_url": "https://arxiv.org/pdf/2601.08864v1",
    "published_date": "2026-01-10 17:21:48 UTC",
    "updated_date": "2026-01-10 17:21:48 UTC"
  },
  {
    "arxiv_id": "2601.06607v1",
    "title": "Pragya: An AI-Based Semantic Recommendation System for Sanskrit Subhasitas",
    "authors": [
      "Tanisha Raorane",
      "Prasenjit Kole"
    ],
    "abstract": "Sanskrit Subhasitas encapsulate centuries of cultural and philosophical wisdom, yet remain underutilized in the digital age due to linguistic and contextual barriers. In this work, we present Pragya, a retrieval-augmented generation (RAG) framework for semantic recommendation of Subhasitas. We curate a dataset of 200 verses annotated with thematic tags such as motivation, friendship, and compassion. Using sentence embeddings (IndicBERT), the system retrieves top-k verses relevant to user queries. The retrieved results are then passed to a generative model (Mistral LLM) to produce transliterations, translations, and contextual explanations. Experimental evaluation demonstrates that semantic retrieval significantly outperforms keyword matching in precision and relevance, while user studies highlight improved accessibility through generated summaries. To our knowledge, this is the first attempt at integrating retrieval and generation for Sanskrit Subhasitas, bridging cultural heritage with modern applied AI.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Preprint",
    "pdf_url": "https://arxiv.org/pdf/2601.06607v1",
    "published_date": "2026-01-10 16:13:25 UTC",
    "updated_date": "2026-01-10 16:13:25 UTC"
  },
  {
    "arxiv_id": "2601.07866v1",
    "title": "Bridging the Trust Gap: Clinician-Validated Hybrid Explainable AI for Maternal Health Risk Assessment in Bangladesh",
    "authors": [
      "Farjana Yesmin",
      "Nusrat Shirmin",
      "Suraiya Shabnam Bristy"
    ],
    "abstract": "While machine learning shows promise for maternal health risk prediction, clinical adoption in resource-constrained settings faces a critical barrier: lack of explainability and trust. This study presents a hybrid explainable AI (XAI) framework combining ante-hoc fuzzy logic with post-hoc SHAP explanations, validated through systematic clinician feedback. We developed a fuzzy-XGBoost model on 1,014 maternal health records, achieving 88.67% accuracy (ROC-AUC: 0.9703). A validation study with 14 healthcare professionals in Bangladesh revealed strong preference for hybrid explanations (71.4% across three clinical cases) with 54.8% expressing trust for clinical use. SHAP analysis identified healthcare access as the primary predictor, with the engineered fuzzy risk score ranking third, validating clinical knowledge integration (r=0.298). Clinicians valued integrated clinical parameters but identified critical gaps: obstetric history, gestational age, and connectivity barriers. This work demonstrates that combining interpretable fuzzy rules with feature importance explanations enhances both utility and trust, providing practical insights for XAI deployment in maternal healthcare.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "5 pages, 3 figures, 2 tables Submitted to WCCI 2026, 2026 IEEE WORLD CONGRESS ON COMPUTATIONAL INTELLIGENCE",
    "pdf_url": "https://arxiv.org/pdf/2601.07866v1",
    "published_date": "2026-01-10 16:12:38 UTC",
    "updated_date": "2026-01-10 16:12:38 UTC"
  },
  {
    "arxiv_id": "2601.06606v1",
    "title": "CEDAR: Context Engineering for Agentic Data Science",
    "authors": [
      "Rishiraj Saha Roy",
      "Chris Hinze",
      "Luzian Hahn",
      "Fabian Kuech"
    ],
    "abstract": "We demonstrate CEDAR, an application for automating data science (DS) tasks with an agentic setup. Solving DS problems with LLMs is an underexplored area that has immense market value. The challenges are manifold: task complexities, data sizes, computational limitations, and context restrictions. We show that these can be alleviated via effective context engineering. We first impose structure into the initial prompt with DS-specific input fields, that serve as instructions for the agentic system. The solution is then materialized as an enumerated sequence of interleaved plan and code blocks generated by separate LLM agents, providing a readable structure to the context at any step of the workflow. Function calls for generating these intermediate texts, and for corresponding Python code, ensure that data stays local, and only aggregate statistics and associated instructions are injected into LLM prompts. Fault tolerance and context management are introduced via iterative code generation and smart history rendering. The viability of our agentic data scientist is demonstrated using canonical Kaggle challenges.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ECIR 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.06606v1",
    "published_date": "2026-01-10 16:05:04 UTC",
    "updated_date": "2026-01-10 16:05:04 UTC"
  },
  {
    "arxiv_id": "2601.06604v1",
    "title": "Object-Centric World Models Meet Monte Carlo Tree Search",
    "authors": [
      "Rodion Vakhitov",
      "Leonid Ugadiarov",
      "Aleksandr Panov"
    ],
    "abstract": "In this paper, we introduce ObjectZero, a novel reinforcement learning (RL) algorithm that leverages the power of object-level representations to model dynamic environments more effectively. Unlike traditional approaches that process the world as a single undifferentiated input, our method employs Graph Neural Networks (GNNs) to capture intricate interactions among multiple objects. These objects, which can be manipulated and interact with each other, serve as the foundation for our model's understanding of the environment. We trained the algorithm in a complex setting teeming with diverse, interactive objects, demonstrating its ability to effectively learn and predict object dynamics. Our results highlight that a structured world model operating on object-centric representations can be successfully integrated into a model-based RL algorithm utilizing Monte Carlo Tree Search as a planning module.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06604v1",
    "published_date": "2026-01-10 15:59:17 UTC",
    "updated_date": "2026-01-10 15:59:17 UTC"
  },
  {
    "arxiv_id": "2601.06599v1",
    "title": "How Context Shapes Truth: Geometric Transformations of Statement-level Truth Representations in LLMs",
    "authors": [
      "Shivam Adarsh",
      "Maria Maistro",
      "Christina Lioma"
    ],
    "abstract": "Large Language Models (LLMs) often encode whether a statement is true as a vector in their residual stream activations. These vectors, also known as truth vectors, have been studied in prior work, however how they change when context is introduced remains unexplored. We study this question by measuring (1) the directional change ($θ$) between the truth vectors with and without context and (2) the relative magnitude of the truth vectors upon adding context. Across four LLMs and four datasets, we find that (1) truth vectors are roughly orthogonal in early layers, converge in middle layers, and may stabilize or continue increasing in later layers; (2) adding context generally increases the truth vector magnitude, i.e., the separation between true and false representations in the activation space is amplified; (3) larger models distinguish relevant from irrelevant context mainly through directional change ($θ$), while smaller models show this distinction through magnitude differences. We also find that context conflicting with parametric knowledge produces larger geometric changes than parametrically aligned context. To the best of our knowledge, this is the first work that provides a geometric characterization of how context transforms the truth vector in the activation space of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06599v1",
    "published_date": "2026-01-10 15:43:26 UTC",
    "updated_date": "2026-01-10 15:43:26 UTC"
  },
  {
    "arxiv_id": "2601.06596v1",
    "title": "Are LLMs Vulnerable to Preference-Undermining Attacks (PUA)? A Factorial Analysis Methodology for Diagnosing the Trade-off between Preference Alignment and Real-World Validity",
    "authors": [
      "Hongjun An",
      "Yiliang Song",
      "Jiangan Chen",
      "Jiawei Shao",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "abstract": "Large Language Model (LLM) training often optimizes for preference alignment, rewarding outputs that are perceived as helpful and interaction-friendly. However, this preference-oriented objective can be exploited: manipulative prompts can steer responses toward user-appeasing agreement and away from truth-oriented correction. In this work, we investigate whether aligned models are vulnerable to Preference-Undermining Attacks (PUA), a class of manipulative prompting strategies designed to exploit the model's desire to please user preferences at the expense of truthfulness. We propose a diagnostic methodology that provides a finer-grained and more directive analysis than aggregate benchmark scores, using a factorial evaluation framework to decompose prompt-induced shifts into interpretable effects of system objectives (truth- vs. preference-oriented) and PUA-style dialogue factors (directive control, personal derogation, conditional approval, reality denial) within a controlled $2 \\times 2^4$ design. Surprisingly, more advanced models are sometimes more susceptible to manipulative prompts. Beyond the dominant reality-denial factor, we observe model-specific sign reversals and interactions with PUA-style factors, suggesting tailored defenses rather than uniform robustness. These findings offer a novel, reproducible factorial evaluation methodology that provides finer-grained diagnostics for post-training processes like RLHF, enabling better trade-offs in the product iteration of LLMs by offering a more nuanced understanding of preference alignment risks and the impact of manipulative prompts.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "preprint",
    "pdf_url": "https://arxiv.org/pdf/2601.06596v1",
    "published_date": "2026-01-10 15:16:23 UTC",
    "updated_date": "2026-01-10 15:16:23 UTC"
  },
  {
    "arxiv_id": "2601.11619v1",
    "title": "NoiseFormer -- Noise Diffused Symmetric Attention Transformer",
    "authors": [
      "Phani Kumar",
      "Nyshadham",
      "Jyothendra Varma",
      "Polisetty V R K",
      "Aditya Rathore"
    ],
    "abstract": "Transformer architecture has been very successful long runner in the field of Deep Learning (DL) and Large Language Models (LLM) because of its powerful attention-based learning and parallel-natured architecture. As the models grow gigantic in terms of memory footprint, difficulties in fitting the model on a device like a GPU or an AI accelerator give rise to the need for multiple computing devices thereby escalating the computing cost. This increased training/inference cost paved the way for efficient model size reduction/parametric reduction deploying Sparse Attention techniques. In this paper, we start analyzing one of the techniques of Sparse Attention called Symmetric Dot-Product Attention (referred to as Symmetric Attention) and propose a novel unified model architecture called Noise Diffused Symmetric Attention Transformer to enhance the model's performance. While maintaining the memory gains of Symmetric Attention, with minute overhead in terms of model parameters and computational overhead, the proposed model brings in enhanced performance in terms of accuracy and inference-time sampling. The proposed model is validated upon GPT2 base model and the results reflect the performance gains falling between plain Symmetric attention and GPT2 base model on a variety of GLUE benchmark tasks in terms of accuracy, with significant model size reduction with respect to the base model.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.11619v1",
    "published_date": "2026-01-10 14:10:48 UTC",
    "updated_date": "2026-01-10 14:10:48 UTC"
  },
  {
    "arxiv_id": "2601.11618v1",
    "title": "Geometric Attention: A Regime-Explicit Operator Semantics for Transformer Attention",
    "authors": [
      "Luis Rosario Freytes"
    ],
    "abstract": "Geometric Attention (GA) specifies an attention layer by four independent inputs: a finite carrier (what indices are addressable), an evidence-kernel rule (how masked proto-scores and a link induce nonnegative weights), a probe family (which observables are treated as admissible), and an anchor/update rule (which representative kernel is selected and how it is applied). Probe families induce an operational equivalence relation on kernels and therefore a gauge; anchors select representatives relative to that probe. Under a scalar relational-work representation and a multiplicative compositionality law for evidence, the admissible link family is exponential, yielding Gibbs weights; with row anchoring this includes the softmax kernel family as a subregime. After quotienting unary row/column score fields, the remaining interaction component admits a canonical rank-r normal form (Eckart-Young/SVD); dot-product score charts implement the corresponding low-rank interaction regime. Fixing the carrier and extensionalizing the update yields the standard fixed-token Transformer attention operator; allowing carrier updates yields adaptive-carrier and staged-depth regimes. The operator language also supports multihead/mixed kernels, plan-based anchors (e.g., entropic OT/Sinkhorn), and unary operators (e.g., FFN-style fields) as explicit regime choices. This separates invariant structure from modeling choice, enabling principled comparison and extension of attention mechanisms, and attention-based architectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "57 pages",
    "pdf_url": "https://arxiv.org/pdf/2601.11618v1",
    "published_date": "2026-01-10 13:43:01 UTC",
    "updated_date": "2026-01-10 13:43:01 UTC"
  },
  {
    "arxiv_id": "2601.06573v1",
    "title": "QMAVIS: Long Video-Audio Understanding using Fusion of Large Multimodal Models",
    "authors": [
      "Zixing Lin",
      "Jiale Wang",
      "Gee Wah Ng",
      "Lee Onn Mak",
      "Chan Zhi Yang Jeriel",
      "Jun Yang Lee",
      "Yaohao Li"
    ],
    "abstract": "Large Multimodal Models (LMMs) for video-audio understanding have traditionally been evaluated only on shorter videos of a few minutes long. In this paper, we introduce QMAVIS (Q Team-Multimodal Audio Video Intelligent Sensemaking), a novel long video-audio understanding pipeline built through a late fusion of LMMs, Large Language Models, and speech recognition models. QMAVIS addresses the gap in long-form video analytics, particularly for longer videos of a few minutes to beyond an hour long, opening up new potential applications in sensemaking, video content analysis, embodied AI, etc. Quantitative experiments using QMAVIS demonstrated a 38.75% improvement over state-of-the-art video-audio LMMs like VideoLlaMA2 and InternVL2 on the VideoMME (with subtitles) dataset, which comprises long videos with audio information. Evaluations on other challenging video understanding datasets like PerceptionTest and EgoSchema saw up to 2% improvement, indicating competitive performance. Qualitative experiments also showed that QMAVIS is able to extract the nuances of different scenes in a long video audio content while understanding the overarching narrative. Ablation studies were also conducted to ascertain the impact of each component in the fusion pipeline.",
    "categories": [
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06573v1",
    "published_date": "2026-01-10 13:42:15 UTC",
    "updated_date": "2026-01-10 13:42:15 UTC"
  },
  {
    "arxiv_id": "2601.06572v1",
    "title": "Hellinger Multimodal Variational Autoencoders",
    "authors": [
      "Huyen Khanh Vo",
      "Isabel Valera"
    ],
    "abstract": "Multimodal variational autoencoders (VAEs) are widely used for weakly supervised generative learning with multiple modalities. Predominant methods aggregate unimodal inference distributions using either a product of experts (PoE), a mixture of experts (MoE), or their combinations to approximate the joint posterior. In this work, we revisit multimodal inference through the lens of probabilistic opinion pooling, an optimization-based approach. We start from Hölder pooling with $α=0.5$, which corresponds to the unique symmetric member of the $α\\text{-divergence}$ family, and derive a moment-matching approximation, termed Hellinger. We then leverage such an approximation to propose HELVAE, a multimodal VAE that avoids sub-sampling, yielding an efficient yet effective model that: (i) learns more expressive latent representations as additional modalities are observed; and (ii) empirically achieves better trade-offs between generative coherence and quality, outperforming state-of-the-art multimodal VAE models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06572v1",
    "published_date": "2026-01-10 13:39:36 UTC",
    "updated_date": "2026-01-10 13:39:36 UTC"
  },
  {
    "arxiv_id": "2601.06566v1",
    "title": "QCaption: Video Captioning and Q&A through Fusion of Large Multimodal Models",
    "authors": [
      "Jiale Wang",
      "Gee Wah Ng",
      "Lee Onn Mak",
      "Randall Cher",
      "Ng Ding Hei Ryan",
      "Davis Wang"
    ],
    "abstract": "This paper introduces QCaption, a novel video captioning and Q&A pipeline that enhances video analytics by fusing three models: key frame extraction, a Large Multimodal Model (LMM) for image-text analysis, and a Large Language Model (LLM) for text analysis. This approach enables integrated analysis of text, images, and video, achieving performance improvements over existing video captioning and Q&A models; all while remaining fully self-contained, adept for on-premises deployment. Experimental results using QCaption demonstrated up to 44.2% and 48.9% improvements in video captioning and Q&A tasks, respectively. Ablation studies were also performed to assess the role of LLM on the fusion on the results. Moreover, the paper proposes and evaluates additional video captioning approaches, benchmarking them against QCaption and existing methodologies. QCaption demonstrate the potential of adopting a model fusion approach in advancing video analytics.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06566v1",
    "published_date": "2026-01-10 13:28:43 UTC",
    "updated_date": "2026-01-10 13:28:43 UTC"
  },
  {
    "arxiv_id": "2601.10742v1",
    "title": "Line-based Event Preprocessing: Towards Low-Energy Neuromorphic Computer Vision",
    "authors": [
      "Amélie Gruel",
      "Pierre Lewden",
      "Adrien F. Vincent",
      "Sylvain Saïghi"
    ],
    "abstract": "Neuromorphic vision made significant progress in recent years, thanks to the natural match between spiking neural networks and event data in terms of biological inspiration, energy savings, latency and memory use for dynamic visual data processing. However, optimising its energy requirements still remains a challenge within the community, especially for embedded applications. One solution may reside in preprocessing events to optimise data quantity thus lowering the energy cost on neuromorphic hardware, proportional to the number of synaptic operations. To this end, we extend an end-to-end neuromorphic line detection mechanism to introduce line-based event data preprocessing. Our results demonstrate on three benchmark event-based datasets that preprocessing leads to an advantageous trade-off between energy consumption and classification performance. Depending on the line-based preprocessing strategy and the complexity of the classification task, we show that one can maintain or increase the classification accuracy while significantly reducing the theoretical energy consumption. Our approach systematically leads to a significant improvement of the neuromorphic classification efficiency, thus laying the groundwork towards a more frugal neuromorphic computer vision thanks to event preprocessing.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.NE",
    "comment": "18 pages (3 pages of acknowledgments and references), 10 figures and 4 tables. Submitted to the IOP Science \"Neuromorphic Computing and Engineering\" journal, awaiting feedback. This work is supported by a public grant overseen by the French National Research Agency (ANR) as part of the éPEPR IA France 2030é programme (Emergences project ANR-23-PEIA-0002)",
    "pdf_url": "https://arxiv.org/pdf/2601.10742v1",
    "published_date": "2026-01-10 12:30:34 UTC",
    "updated_date": "2026-01-10 12:30:34 UTC"
  },
  {
    "arxiv_id": "2601.06551v1",
    "title": "L-RAG: Balancing Context and Retrieval with Entropy-Based Lazy Loading",
    "authors": [
      "Sergii Voloshyn"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as the predominant paradigm for grounding Large Language Model outputs in factual knowledge, effectively mitigating hallucinations. However, conventional RAG systems operate under a \"retrieve-always\" assumption, querying vector databases for every input regardless of query complexity. This static approach incurs substantial computational overhead and inference latency, particularly problematic for high-throughput production deployments. We introduce L-RAG (Lazy Retrieval-Augmented Generation), an adaptive framework that implements hierarchical context management through entropy-based gating. L-RAG employs a two-tier architecture: queries are first processed with a compact document summary, and expensive chunk retrieval is triggered only when the model's predictive entropy exceeds a calibrated threshold, signaling genuine uncertainty. Through experiments on SQuAD 2.0 (N=500) using the Phi-2 model, we demonstrate that L-RAG provides a tunable accuracy-efficiency trade-off: at a conservative threshold (tau=0.5), L-RAG achieves 78.2% accuracy, matching Standard RAG (77.8%), with 8% retrieval reduction; at a balanced threshold (tau=1.0), retrieval reduction increases to 26% with modest accuracy trade-off (76.0%). Latency analysis shows that L-RAG saves 80-210ms per query when retrieval latency exceeds 500ms. Analysis of entropy distributions reveals statistically significant separation (p < 0.001) between correct predictions (H=1.72) and errors (H=2.20), validating entropy as a reliable uncertainty signal. L-RAG offers a practical, training-free approach toward more efficient RAG deployment, providing system architects with a configurable knob to balance accuracy and throughput requirements.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06551v1",
    "published_date": "2026-01-10 12:25:19 UTC",
    "updated_date": "2026-01-10 12:25:19 UTC"
  },
  {
    "arxiv_id": "2601.06550v1",
    "title": "LLMTrack: Semantic Multi-Object Tracking with Multi-modal Large Language Models",
    "authors": [
      "Pan Liao",
      "Feng Yang",
      "Di Wu",
      "Jinwen Yu",
      "Yuhua Zhu",
      "Wenhui Zhao"
    ],
    "abstract": "Traditional Multi-Object Tracking (MOT) systems have achieved remarkable precision in localization and association, effectively answering \\textit{where} and \\textit{who}. However, they often function as autistic observers, capable of tracing geometric paths but blind to the semantic \\textit{what} and \\textit{why} behind object behaviors. To bridge the gap between geometric perception and cognitive reasoning, we propose \\textbf{LLMTrack}, a novel end-to-end framework for Semantic Multi-Object Tracking (SMOT). We adopt a bionic design philosophy that decouples strong localization from deep understanding, utilizing Grounding DINO as the eyes and the LLaVA-OneVision multimodal large model as the brain. We introduce a Spatio-Temporal Fusion Module that aggregates instance-level interaction features and video-level contexts, enabling the Large Language Model (LLM) to comprehend complex trajectories. Furthermore, we design a progressive three-stage training strategy, Visual Alignment, Temporal Fine-tuning, and Semantic Injection via LoRA to efficiently adapt the massive model to the tracking domain. Extensive experiments on the BenSMOT benchmark demonstrate that LLMTrack achieves state-of-the-art performance, significantly outperforming existing methods in instance description, interaction recognition, and video summarization while maintaining robust tracking stability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06550v1",
    "published_date": "2026-01-10 12:18:12 UTC",
    "updated_date": "2026-01-10 12:18:12 UTC"
  },
  {
    "arxiv_id": "2601.06543v1",
    "title": "SimLLM: Fine-Tuning Code LLMs for SimPy-Based Queueing System Simulation",
    "authors": [
      "Jun-Qi Chen",
      "Kun Zhang",
      "Rui Zheng",
      "Ying Zhong"
    ],
    "abstract": "The Python package SimPy is widely used for modeling queueing systems due to its flexibility, simplicity, and smooth integration with modern data analysis and optimization frameworks. Recent advances in large language models (LLMs) have shown strong ability in generating clear and executable code, making them powerful and suitable tools for writing SimPy queueing simulation code. However, directly employing closed-source models like GPT-4o to generate such code may lead to high computational costs and raise data privacy concerns. To address this, we fine-tune two open-source LLMs, Qwen-Coder-7B and DeepSeek-Coder-6.7B, on curated SimPy queueing data, which enhances their code-generating performance in executability, output-format compliance, and instruction-code consistency. Particularly, we proposed a multi-stage fine-tuning framework comprising two stages of supervised fine-tuning (SFT) and one stage of direct preference optimization (DPO), progressively enhancing the model's ability in SimPy-based queueing simulation code generation. Extensive evaluations demonstrate that both fine-tuned models achieve substantial improvements in executability, output-format compliance, and instruct consistency. These results confirm that domain-specific fine-tuning can effectively transform compact open-source code models into reliable SimPy simulation generators which provide a practical alternative to closed-source LLMs for education, research, and operational decision support.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "33 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.06543v1",
    "published_date": "2026-01-10 11:53:39 UTC",
    "updated_date": "2026-01-10 11:53:39 UTC"
  },
  {
    "arxiv_id": "2601.06542v1",
    "title": "Resource-constrained Project Scheduling with Time-of-Use Energy Tariffs and Machine States: A Logic-based Benders Decomposition Approach",
    "authors": [
      "Corentin Juvigny",
      "Antonín Novák",
      "Jan Mandík",
      "Zdeněk Hanzálek"
    ],
    "abstract": "In this paper, we investigate the Resource-Constrained Project Scheduling Problem (RCPSP) with time-of-use energy tariffs (TOU) and machine states, a variant of RCPSP for production scheduling where energy price is part of the criteria and one machine is highly energy-demanding and can be in one of the following three states: proc, idle, or off. The problem involves scheduling all tasks, respecting precedence constraints and resource limitations, while minimizing the combination of the overall makespan and the total energy cost (TEC), which varies according to the TOU pricing, which can take negative values. We propose two novel approaches to solve it: a monolithic Constraint Programming (CP) approach and a Logic-Based Benders Decomposition (LBBD) approach. The latter combines a master problem dealing with energy cost solved using Integer Linear Programming (ILP) with a subproblem handling the RCPSP resolved using CP. Both approaches surpass the monolithic compact ILP approach, but the LBBD significantly outperforms the CP when the ratio of energy-intensive tasks over the overall tasks is moderate, allowing for solving instances with up to 1600 tasks in sparse instances. Finally, we put forth a way of generalizing our LBBD approach to other problems sharing similar characteristics, and we applied it to a problem based on an RCPSP problem with blocking times & total weighted tardiness criterion and a flexible job shop.",
    "categories": [
      "math.OC",
      "cs.AI"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06542v1",
    "published_date": "2026-01-10 11:47:56 UTC",
    "updated_date": "2026-01-10 11:47:56 UTC"
  },
  {
    "arxiv_id": "2601.06540v1",
    "title": "Self-Organizing Dual-Buffer Adaptive Clustering Experience Replay (SODASER) for Safe Reinforcement Learning in Optimal Control",
    "authors": [
      "Roya Khalili Amirabadi",
      "Mohsen Jalaeian Farimani",
      "Omid Solaymani Fard"
    ],
    "abstract": "This paper proposes a novel reinforcement learning framework, named Self-Organizing Dual-buffer Adaptive Clustering Experience Replay (SODACER), designed to achieve safe and scalable optimal control of nonlinear systems. The proposed SODACER mechanism consisting of a Fast-Buffer for rapid adaptation to recent experiences and a Slow-Buffer equipped with a self-organizing adaptive clustering mechanism to maintain diverse and non-redundant historical experiences. The adaptive clustering mechanism dynamically prunes redundant samples, optimizing memory efficiency while retaining critical environmental patterns. The approach integrates SODASER with Control Barrier Functions (CBFs) to guarantee safety by enforcing state and input constraints throughout the learning process. To enhance convergence and stability, the framework is combined with the Sophia optimizer, enabling adaptive second-order gradient updates. The proposed SODACER-Sophia's architecture ensures reliable, effective, and robust learning in dynamic, safety-critical environments, offering a generalizable solution for applications in robotics, healthcare, and large-scale system optimization. The proposed approach is validated on a nonlinear Human Papillomavirus (HPV) transmission model with multiple control inputs and safety constraints. Comparative evaluations against random and clustering-based experience replay methods demonstrate that SODACER achieves faster convergence, improved sample efficiency, and a superior bias-variance trade-off, while maintaining safe system trajectories, validated via the Friedman test.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "math.OC"
    ],
    "primary_category": "eess.SY",
    "comment": "Also available at SSRN: https://ssrn.com/abstract=5191427 or http://dx.doi.org/10.2139/ssrn.5191427",
    "pdf_url": "https://arxiv.org/pdf/2601.06540v1",
    "published_date": "2026-01-10 11:43:15 UTC",
    "updated_date": "2026-01-10 11:43:15 UTC"
  },
  {
    "arxiv_id": "2601.06533v1",
    "title": "Short-term electricity load forecasting with multi-frequency reconstruction diffusion",
    "authors": [
      "Qi Dong",
      "Rubing Huang",
      "Ling Zhou",
      "Dave Towey",
      "Jinyu Tian",
      "Jianzhou Wang"
    ],
    "abstract": "Diffusion models have emerged as a powerful method in various applications. However, their application to Short-Term Electricity Load Forecasting (STELF) -- a typical scenario in energy systems -- remains largely unexplored. Considering the nonlinear and fluctuating characteristics of the load data, effectively utilizing the powerful modeling capabilities of diffusion models to enhance STELF accuracy remains a challenge. This paper proposes a novel diffusion model with multi-frequency reconstruction for STELF, referred to as the Multi-Frequency-Reconstruction-based Diffusion (MFRD) model. The MFRD model achieves accurate load forecasting through four key steps: (1) The original data is combined with the decomposed multi-frequency modes to form a new data representation; (2) The diffusion model adds noise to the new data, effectively reducing and weakening the noise in the original data; (3) The reverse process adopts a denoising network that combines Long Short-Term Memory (LSTM) and Transformer to enhance noise removal; and (4) The inference process generates the final predictions based on the trained denoising network. To validate the effectiveness of the MFRD model, we conducted experiments on two data platforms: Australian Energy Market Operator (AEMO) and Independent System Operator of New England (ISO-NE). The experimental results show that our model consistently outperforms the compared models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06533v1",
    "published_date": "2026-01-10 11:22:25 UTC",
    "updated_date": "2026-01-10 11:22:25 UTC"
  },
  {
    "arxiv_id": "2601.06530v1",
    "title": "Improving Day-Ahead Grid Carbon Intensity Forecasting by Joint Modeling of Local-Temporal and Cross-Variable Dependencies Across Different Frequencies",
    "authors": [
      "Bowen Zhang",
      "Hongda Tian",
      "Adam Berry",
      "A. Craig Roussac"
    ],
    "abstract": "Accurate forecasting of the grid carbon intensity factor (CIF) is critical for enabling demand-side management and reducing emissions in modern electricity systems. Leveraging multiple interrelated time series, CIF prediction is typically formulated as a multivariate time series forecasting problem. Despite advances in deep learning-based methods, it remains challenging to capture the fine-grained local-temporal dependencies, dynamic higher-order cross-variable dependencies, and complex multi-frequency patterns for CIF forecasting. To address these issues, we propose a novel model that integrates two parallel modules: 1) one enhances the extraction of local-temporal dependencies under multi-frequency by applying multiple wavelet-based convolutional kernels to overlapping patches of varying lengths; 2) the other captures dynamic cross-variable dependencies under multi-frequency to model how inter-variable relationships evolve across the time-frequency domain. Evaluations on four representative electricity markets from Australia, featuring varying levels of renewable penetration, demonstrate that the proposed method outperforms the state-of-the-art models. An ablation study further validates the complementary benefits of the two proposed modules. Designed with built-in interpretability, the proposed model also enables better understanding of its predictive behavior, as shown in a case study where it adaptively shifts attention to relevant variables and time intervals during a disruptive event.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "2026 40th AAAI Conference on Artificial Intelligence",
    "pdf_url": "https://arxiv.org/pdf/2601.06530v1",
    "published_date": "2026-01-10 11:20:55 UTC",
    "updated_date": "2026-01-10 11:20:55 UTC"
  },
  {
    "arxiv_id": "2601.06528v1",
    "title": "Atomic-SNLI: Fine-Grained Natural Language Inference through Atomic Fact Decomposition",
    "authors": [
      "Minghui Huang"
    ],
    "abstract": "Current Natural Language Inference (NLI) systems primarily operate at the sentence level, providing black-box decisions that lack explanatory power. While atomic-level NLI offers a promising alternative by decomposing hypotheses into individual facts, we demonstrate that the conventional assumption that a hypothesis is entailed only when all its atomic facts are entailed fails in practice due to models' poor performance on fine-grained reasoning. Our analysis reveals that existing models perform substantially worse on atomic level inference compared to sentence level tasks. To address this limitation, we introduce Atomic-SNLI, a novel dataset constructed by decomposing SNLI and enriching it with carefully curated atomic level examples through linguistically informed generation strategies. Experimental results demonstrate that models fine-tuned on Atomic-SNLI achieve significant improvements in atomic reasoning capabilities while maintaining strong sentence level performance, enabling both accurate judgements and transparent, explainable results at the fact level.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06528v1",
    "published_date": "2026-01-10 11:13:35 UTC",
    "updated_date": "2026-01-10 11:13:35 UTC"
  },
  {
    "arxiv_id": "2601.06505v1",
    "title": "Neural Nonmyopic Bayesian Optimization in Dynamic Cost Settings",
    "authors": [
      "Sang T. Truong",
      "Duc Q. Nguyen",
      "Willie Neiswanger",
      "Ryan-Rhys Griffiths",
      "Stefano Ermon",
      "Nick Haber",
      "Sanmi Koyejo"
    ],
    "abstract": "Bayesian optimization (BO) is a common framework for optimizing black-box functions, yet most existing methods assume static query costs and rely on myopic acquisition strategies. We introduce LookaHES, a nonmyopic BO framework designed for dynamic, history-dependent cost environments, where evaluation costs vary with prior actions, such as travel distance in spatial tasks or edit distance in sequence design. LookaHES combines a multi-step variant of $H$-Entropy Search with pathwise sampling and neural policy optimization, enabling long-horizon planning beyond twenty steps without the exponential complexity of existing nonmyopic methods. The key innovation is the integration of neural policies, including large language models, to effectively navigate structured, combinatorial action spaces such as protein sequences. These policies amortize lookahead planning and can be integrated with domain-specific constraints during rollout. Empirically, LookaHES outperforms strong myopic and nonmyopic baselines across nine synthetic benchmarks from two to eight dimensions and two real-world tasks: geospatial optimization using NASA night-light imagery and protein sequence design with constrained token-level edits. In short, LookaHES provides a general, scalable, and cost-aware solution for robust long-horizon optimization in complex decision spaces, which makes it a useful tool for researchers in machine learning, statistics, and applied domains. Our implementation is available at https://github.com/sangttruong/nonmyopia.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "32 pages, 20 figures, 13 tables",
    "pdf_url": "https://arxiv.org/pdf/2601.06505v1",
    "published_date": "2026-01-10 09:49:45 UTC",
    "updated_date": "2026-01-10 09:49:45 UTC"
  },
  {
    "arxiv_id": "2601.06502v1",
    "title": "DRAGON: LLM-Driven Decomposition and Reconstruction Agents for Large-Scale Combinatorial Optimization",
    "authors": [
      "Shengkai Chen",
      "Zhiguang Cao",
      "Jianan Zhou",
      "Yaoxin Wu",
      "Senthilnath Jayavelu",
      "Zhuoyi Lin",
      "Xiaoli Li",
      "Shili Xiang"
    ],
    "abstract": "Large Language Models (LLMs) have recently shown promise in addressing combinatorial optimization problems (COPs) through prompt-based strategies. However, their scalability and generalization remain limited, and their effectiveness diminishes as problem size increases, particularly in routing problems involving more than 30 nodes. We propose DRAGON, which stands for Decomposition and Reconstruction Agents Guided OptimizatioN, a novel framework that combines the strengths of metaheuristic design and LLM reasoning. Starting from an initial global solution, DRAGON autonomously identifies regions with high optimization potential and strategically decompose large-scale COPs into manageable subproblems. Each subproblem is then reformulated as a concise, localized optimization task and solved through targeted LLM prompting guided by accumulated experiences. Finally, the locally optimized solutions are systematically reintegrated into the original global context to yield a significantly improved overall outcome. By continuously interacting with the optimization environment and leveraging an adaptive experience memory, the agents iteratively learn from feedback, effectively coupling symbolic reasoning with heuristic search. Empirical results show that, unlike existing LLM-based solvers limited to small-scale instances, DRAGON consistently produces feasible solutions on TSPLIB, CVRPLIB, and Weibull-5k bin packing benchmarks, and achieves near-optimal results (0.16% gap) on knapsack problems with over 3M variables. This work shows the potential of feedback-driven language agents as a new paradigm for generalizable and interpretable large-scale optimization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "This paper has been accepted for presentation and publication at the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026), source code will be available soon",
    "pdf_url": "https://arxiv.org/pdf/2601.06502v1",
    "published_date": "2026-01-10 09:31:40 UTC",
    "updated_date": "2026-01-10 09:31:40 UTC"
  },
  {
    "arxiv_id": "2601.06500v1",
    "title": "The AI Pyramid A Conceptual Framework for Workforce Capability in the Age of AI",
    "authors": [
      "Alok Khatri",
      "Bishesh Khanal"
    ],
    "abstract": "Artificial intelligence (AI) represents a qualitative shift in technological change by extending cognitive labor itself rather than merely automating routine tasks. Recent evidence shows that generative AI disproportionately affects highly educated, white collar work, challenging existing assumptions about workforce vulnerability and rendering traditional approaches to digital or AI literacy insufficient. This paper introduces the concept of AI Nativity, the capacity to integrate AI fluidly into everyday reasoning, problem solving, and decision making, and proposes the AI Pyramid, a conceptual framework for organizing human capability in an AI mediated economy. The framework distinguishes three interdependent capability layers: AI Native capability as a universal baseline for participation in AI augmented environments; AI Foundation capability for building, integrating, and sustaining AI enabled systems; and AI Deep capability for advancing frontier AI knowledge and applications. Crucially, the pyramid is not a career ladder but a system level distribution of capabilities required at scale. Building on this structure, the paper argues that effective AI workforce development requires treating capability formation as infrastructure rather than episodic training, centered on problem based learning embedded in work contexts and supported by dynamic skill ontologies and competency based measurement. The framework has implications for organizations, education systems, and governments seeking to align learning, measurement, and policy with the evolving demands of AI mediated work, while addressing productivity, resilience, and inequality at societal scale.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages",
    "pdf_url": "https://arxiv.org/pdf/2601.06500v1",
    "published_date": "2026-01-10 09:27:56 UTC",
    "updated_date": "2026-01-10 09:27:56 UTC"
  },
  {
    "arxiv_id": "2601.06497v1",
    "title": "Coding in a Bubble? Evaluating LLMs in Resolving Context Adaptation Bugs During Code Adaptation",
    "authors": [
      "Tanghaoran Zhang",
      "Xinjun Mao",
      "Shangwen Wang",
      "Yuxin Zhao",
      "Yao Lu",
      "Zezhou Tang",
      "Wenyu Xu",
      "Longfei Sun",
      "Changrong Xie",
      "Kang Yang",
      "Yue Yu"
    ],
    "abstract": "Code adaptation is a fundamental but challenging task in software development, requiring developers to modify existing code for new contexts. A key challenge is to resolve Context Adaptation Bugs (CtxBugs), which occurs when code correct in its original context violates constraints in the target environment. Unlike isolated bugs, CtxBugs cannot be resolved through local fixes and require cross-context reasoning to identify semantic mismatches. Overlooking them may lead to critical failures in adaptation. Although Large Language Models (LLMs) show great potential in automating code-related tasks, their ability to resolve CtxBugs remains a significant and unexplored obstacle to their practical use in code adaptation. To bridge this gap, we propose CtxBugGen, a novel framework for generating CtxBugs to evaluate LLMs. Its core idea is to leverage LLMs' tendency to generate plausible but context-free code when contextual constraints are absent. The framework generates CtxBugs through a four-step process to ensure their relevance and validity: (1) Adaptation Task Selection, (2) Task-specific Perturbation,(3) LLM-based Variant Generation and (4) CtxBugs Identification. Based on the benchmark constructed by CtxBugGen, we conduct an empirical study with four state-of-the-art LLMs. Our results reveal their unsatisfactory performance in CtxBug resolution. The best performing LLM, Kimi-K2, achieves 55.93% on Pass@1 and resolves just 52.47% of CtxBugs. The presence of CtxBugs degrades LLMs' adaptation performance by up to 30%. Failure analysis indicates that LLMs often overlook CtxBugs and replicate them in their outputs. Our study highlights a critical weakness in LLMs' cross-context reasoning and emphasize the need for new methods to enhance their context awareness for reliable code adaptation.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "24 pages, 11 figures, accepted by FSE 2026",
    "pdf_url": "https://arxiv.org/pdf/2601.06497v1",
    "published_date": "2026-01-10 09:14:00 UTC",
    "updated_date": "2026-01-10 09:14:00 UTC"
  },
  {
    "arxiv_id": "2601.06487v1",
    "title": "ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative Ranking",
    "authors": [
      "Qiang Zhang",
      "Boli Chen",
      "Fanrui Zhang",
      "Ruixue Ding",
      "Shihang Wang",
      "Qiuchen Wang",
      "Yinfeng Huang",
      "Haonan Zhang",
      "Rongxiang Zhu",
      "Pengyong Wang",
      "Ailin Ren",
      "Xin Li",
      "Pengjun Xie",
      "Jiawei Liu",
      "Ning Guo",
      "Jingren Zhou",
      "Zheng-Jun Zha"
    ],
    "abstract": "Reinforcement learning has substantially improved the performance of LLM agents on tasks with verifiable outcomes, but it still struggles on open-ended agent tasks with vast solution spaces (e.g., complex travel planning). Due to the absence of objective ground-truth for these tasks, current RL algorithms largely rely on reward models that assign scalar scores to individual responses. We contend that such pointwise scoring suffers from an inherent discrimination collapse: the reward model struggles to distinguish subtle advantages among different trajectories, resulting in scores within a group being compressed into a narrow range. Consequently, the effective reward signal becomes dominated by noise from the reward model, leading to optimization stagnation. To address this, we propose ArenaRL, a reinforcement learning paradigm that shifts from pointwise scalar scoring to intra-group relative ranking. ArenaRL introduces a process-aware pairwise evaluation mechanism, employing multi-level rubrics to assign fine-grained relative scores to trajectories. Additionally, we construct an intra-group adversarial arena and devise a tournament-based ranking scheme to obtain stable advantage signals. Empirical results confirm that the built seeded single-elimination scheme achieves nearly equivalent advantage estimation accuracy to full pairwise comparisons with O(N^2) complexity, while operating with only O(N) complexity, striking an optimal balance between efficiency and precision. Furthermore, to address the lack of full-cycle benchmarks for open-ended agents, we build Open-Travel and Open-DeepResearch, two high-quality benchmarks featuring a comprehensive pipeline covering SFT, RL training, and multi-dimensional evaluation. Extensive experiments show that ArenaRL substantially outperforms standard RL baselines, enabling LLM agents to generate more robust solutions for complex real-world tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06487v1",
    "published_date": "2026-01-10 08:43:07 UTC",
    "updated_date": "2026-01-10 08:43:07 UTC"
  },
  {
    "arxiv_id": "2601.06484v1",
    "title": "Learning Domain Agnostic Latent Embeddings of 3D Faces for Zero-shot Animal Expression Transfer",
    "authors": [
      "Yue Wang",
      "Lawrence Amadi",
      "Xiang Gao",
      "Yazheng Chen",
      "Yuanpeng Liu",
      "Ning Lu",
      "Xianfeng Gu"
    ],
    "abstract": "We present a zero-shot framework for transferring human facial expressions to 3D animal face meshes. Our method combines intrinsic geometric descriptors (HKS/WKS) with a mesh-agnostic latent embedding that disentangles facial identity and expression. The ID latent space captures species-independent facial structure, while the expression latent space encodes deformation patterns that generalize across humans and animals. Trained only with human expression pairs, the model learns the embeddings, decoupling, and recoupling of cross-identity expressions, enabling expression transfer without requiring animal expression data. To enforce geometric consistency, we employ Jacobian loss together with vertex-position and Laplacian losses. Experiments show that our approach achieves plausible cross-species expression transfer, effectively narrowing the geometric gap between human and animal facial shapes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "WACV 2026 Workshop LENS",
    "pdf_url": "https://arxiv.org/pdf/2601.06484v1",
    "published_date": "2026-01-10 08:37:02 UTC",
    "updated_date": "2026-01-10 08:37:02 UTC"
  },
  {
    "arxiv_id": "2601.06475v1",
    "title": "VVTRec: Radio Interferometric Reconstruction through Visual and Textual Modality Enrichment",
    "authors": [
      "Kai Cheng",
      "Ruoqi Wang",
      "Qiong Luo"
    ],
    "abstract": "Radio astronomy is an indispensable discipline for observing distant celestial objects. Measurements of wave signals from radio telescopes, called visibility, need to be transformed into images for astronomical observations. These dirty images blend information from real sources and artifacts. Therefore, astronomers usually perform reconstruction before imaging to obtain cleaner images. Existing methods consider only a single modality of sparse visibility data, resulting in images with remaining artifacts and insufficient modeling of correlation. To enhance the extraction of visibility information and emphasize output quality in the image domain, we propose VVTRec, a multimodal radio interferometric data reconstruction method with visibility-guided visual and textual modality enrichment. In our VVTRec, sparse visibility is transformed into image-form and text-form features to obtain enhancements in terms of spatial and semantic information, improving the structural integrity and accuracy of images. Also, we leverage Vision-Language Models (VLMs) to achieve additional training-free performance improvements. VVTRec enables sparse visibility, as a foreign modality unseen by VLMs, to accurately extract pre-trained knowledge as a supplement. Our experiments demonstrate that VVTRec effectively enhances imaging results by exploiting multimodal information without introducing excessive computational overhead.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06475v1",
    "published_date": "2026-01-10 07:56:17 UTC",
    "updated_date": "2026-01-10 07:56:17 UTC"
  },
  {
    "arxiv_id": "2601.06474v2",
    "title": "SparseOccVLA: Bridging Occupancy and Vision-Language Models via Sparse Queries for Unified 4D Scene Understanding and Planning",
    "authors": [
      "Chenxu Dang",
      "Jie Wang",
      "Guang Li",
      "Zhiwen Hou",
      "Zihan You",
      "Hangjun Ye",
      "Jie Ma",
      "Long Chen",
      "Yan Wang"
    ],
    "abstract": "In autonomous driving, Vision Language Models (VLMs) excel at high-level reasoning , whereas semantic occupancy provides fine-grained details. Despite significant progress in individual fields, there is still no method that can effectively integrate both paradigms. Conventional VLMs struggle with token explosion and limited spatiotemporal reasoning, while semantic occupancy provides a unified, explicit spatial representation but is too dense to integrate efficiently with VLMs. To address these challenges and bridge the gap between VLMs and occupancy, we propose SparseOccVLA, a novel vision-language-action model that unifies scene understanding, occupancy forecasting, and trajectory planning powered by sparse occupancy queries. Starting with a lightweight Sparse Occupancy Encoder, SparseOccVLA generates compact yet highly informative sparse occupancy queries that serve as the single bridge between vision and language. These queries are aligned into the language space and reasoned by the LLM for unified scene understanding and future occupancy forecasting. Furthermore, we introduce an LLM-guided Anchor-Diffusion Planner featuring decoupled anchor scoring and denoising, as well as cross-model trajectory-condition fusion. SparseOccVLA achieves a 7% relative improvement in CIDEr over the state-of-the-art on OmniDrive-nuScenes, a 0.5 increase in mIoU score on Occ3D-nuScenes, and sets state-of-the-art open-loop planning metric on nuScenes benchmark, demonstrating its strong holistic capability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06474v2",
    "published_date": "2026-01-10 07:54:20 UTC",
    "updated_date": "2026-01-17 04:01:42 UTC"
  },
  {
    "arxiv_id": "2601.06471v1",
    "title": "PRISP: Privacy-Safe Few-Shot Personalization via Lightweight Adaptation",
    "authors": [
      "Junho Park",
      "Dohoon Kim",
      "Taesup Moon"
    ],
    "abstract": "Large language model (LLM) personalization aims to adapt general-purpose models to individual users. Most existing methods, however, are developed under data-rich and resource-abundant settings, often incurring privacy risks. In contrast, realistic personalization typically occurs after deployment under (i) extremely limited user data, (ii) constrained computational resources, and (iii) strict privacy requirements. We propose PRISP, a lightweight and privacy-safe personalization framework tailored to these constraints. PRISP leverages a Text-to-LoRA hypernetwork to generate task-aware LoRA parameters from task descriptions, and enables efficient user personalization by optimizing a small subset of task-aware LoRA parameters together with minimal additional modules using few-shot user data. Experiments on a few-shot variant of the LaMP benchmark demonstrate that PRISP achieves strong overall performance compared to prior approaches, while reducing computational overhead and eliminating privacy risks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 9 figures",
    "pdf_url": "https://arxiv.org/pdf/2601.06471v1",
    "published_date": "2026-01-10 07:34:28 UTC",
    "updated_date": "2026-01-10 07:34:28 UTC"
  },
  {
    "arxiv_id": "2601.14274v1",
    "title": "Divide and Refine: Enhancing Multimodal Representation and Explainability for Emotion Recognition in Conversation",
    "authors": [
      "Anh-Tuan Mai",
      "Cam-Van Thi Nguyen",
      "Duc-Trong Le"
    ],
    "abstract": "Multimodal emotion recognition in conversation (MERC) requires representations that effectively integrate signals from multiple modalities. These signals include modality-specific cues, information shared across modalities, and interactions that emerge only when modalities are combined. In information-theoretic terms, these correspond to \\emph{unique}, \\emph{redundant}, and \\emph{synergistic} contributions. An ideal representation should leverage all three, yet achieving such balance remains challenging. Recent advances in contrastive learning and augmentation-based methods have made progress, but they often overlook the role of data preparation in preserving these components. In particular, applying augmentations directly to raw inputs or fused embeddings can blur the boundaries between modality-unique and cross-modal signals. To address this challenge, we propose a two-phase framework \\emph{\\textbf{D}ivide and \\textbf{R}efine} (\\textbf{DnR}). In the \\textbf{Divide} phase, each modality is explicitly decomposed into uniqueness, pairwise redundancy, and synergy. In the \\textbf{Refine} phase, tailored objectives enhance the informativeness of these components while maintaining their distinct roles. The refined representations are plug-and-play compatible with diverse multimodal pipelines. Extensive experiments on IEMOCAP and MELD demonstrate consistent improvements across multiple MERC backbones. These results highlight the effectiveness of explicitly dividing, refining, and recombining multimodal representations as a principled strategy for advancing emotion recognition. Our implementation is available at https://github.com/mattam301/DnR-WACV2026",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.14274v1",
    "published_date": "2026-01-10 07:30:20 UTC",
    "updated_date": "2026-01-10 07:30:20 UTC"
  },
  {
    "arxiv_id": "2601.06460v1",
    "title": "Tone Matters: The Impact of Linguistic Tone on Hallucination in VLMs",
    "authors": [
      "Weihao Hong",
      "Zhiyuan Jiang",
      "Bingyu Shen",
      "Xinlei Guan",
      "Yangyi Feng",
      "Meng Xu",
      "Boyang Li"
    ],
    "abstract": "Vision-Language Models (VLMs) are increasingly used in safety-critical applications that require reliable visual grounding. However, these models often hallucinate details that are not present in the image to satisfy user prompts. While recent datasets and benchmarks have been introduced to evaluate systematic hallucinations in VLMs, many hallucination behaviors remain insufficiently characterized. In particular, prior work primarily focuses on object presence or absence, leaving it unclear how prompt phrasing and structural constraints can systematically induce hallucinations. In this paper, we investigate how different forms of prompt pressure influence hallucination behavior. We introduce Ghost-100, a procedurally generated dataset of synthetic scenes in which key visual details are deliberately removed, enabling controlled analysis of absence-based hallucinations. Using a structured 5-Level Prompt Intensity Framework, we vary prompts from neutral queries to toxic demands and rigid formatting constraints. We evaluate three representative open-weight VLMs: MiniCPM-V 2.6-8B, Qwen2-VL-7B, and Qwen3-VL-8B. Across all three models, hallucination rates do not increase monotonically with prompt intensity. All models exhibit reductions at higher intensity levels at different thresholds, though not all show sustained reduction under maximum coercion. These results suggest that current safety alignment is more effective at detecting semantic hostility than structural coercion, revealing model-specific limitations in handling compliance pressure. Our dataset is available at: https://github.com/bli1/tone-matters",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 6 figures, WACV Workshop",
    "pdf_url": "https://arxiv.org/pdf/2601.06460v1",
    "published_date": "2026-01-10 07:00:22 UTC",
    "updated_date": "2026-01-10 07:00:22 UTC"
  },
  {
    "arxiv_id": "2601.06453v1",
    "title": "ConSensus: Multi-Agent Collaboration for Multimodal Sensing",
    "authors": [
      "Hyungjun Yoon",
      "Mohammad Malekzadeh",
      "Sung-Ju Lee",
      "Fahim Kawsar",
      "Lorena Qendro"
    ],
    "abstract": "Large language models (LLMs) are increasingly grounded in sensor data to perceive and reason about human physiology and the physical world. However, accurately interpreting heterogeneous multimodal sensor data remains a fundamental challenge. We show that a single monolithic LLM often fails to reason coherently across modalities, leading to incomplete interpretations and prior-knowledge bias. We introduce ConSensus, a training-free multi-agent collaboration framework that decomposes multimodal sensing tasks into specialized, modality-aware agents. To aggregate agent-level interpretations, we propose a hybrid fusion mechanism that balances semantic aggregation, which enables cross-modal reasoning and contextual understanding, with statistical consensus, which provides robustness through agreement across modalities. While each approach has complementary failure modes, their combination enables reliable inference under sensor noise and missing data. We evaluate ConSensus on five diverse multimodal sensing benchmarks, demonstrating an average accuracy improvement of 7.1% over the single-agent baseline. Furthermore, ConSensus matches or exceeds the performance of iterative multi-agent debate methods while achieving a 12.7 times reduction in average fusion token cost through a single-round hybrid fusion protocol, yielding a robust and efficient solution for real-world multimodal sensing tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 6 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2601.06453v1",
    "published_date": "2026-01-10 06:41:01 UTC",
    "updated_date": "2026-01-10 06:41:01 UTC"
  },
  {
    "arxiv_id": "2601.06445v1",
    "title": "LitVISTA: A Benchmark for Narrative Orchestration in Literary Text",
    "authors": [
      "Mingzhe Lu",
      "Yiwen Wang",
      "Yanbing Liu",
      "Qi You",
      "Chong Liu",
      "Ruize Qin",
      "Haoyu Dong",
      "Wenyu Zhang",
      "Jiarui Zhang",
      "Yue Hu",
      "Yunpeng Li"
    ],
    "abstract": "Computational narrative analysis aims to capture rhythm, tension, and emotional dynamics in literary texts. Existing large language models can generate long stories but overly focus on causal coherence, neglecting the complex story arcs and orchestration inherent in human narratives. This creates a structural misalignment between model- and human-generated narratives. We propose VISTA Space, a high-dimensional representational framework for narrative orchestration that unifies human and model narrative perspectives. We further introduce LitVISTA, a structurally annotated benchmark grounded in literary texts, enabling systematic evaluation of models' narrative orchestration capabilities. We conduct oracle evaluations on a diverse selection of frontier LLMs, including GPT, Claude, Grok, and Gemini. Results reveal systematic deficiencies: existing models fail to construct a unified global narrative view, struggling to jointly capture narrative function and structure. Furthermore, even advanced thinking modes yield only limited gains for such literary narrative understanding.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06445v1",
    "published_date": "2026-01-10 06:08:28 UTC",
    "updated_date": "2026-01-10 06:08:28 UTC"
  },
  {
    "arxiv_id": "2601.06434v1",
    "title": "On a Gradient Approach to Chebyshev Center Problems with Applications to Function Learning",
    "authors": [
      "Abhinav Raghuvanshi",
      "Mayank Baranwal",
      "Debasish Chatterjee"
    ],
    "abstract": "We introduce $\\textsf{gradOL}$, the first gradient-based optimization framework for solving Chebyshev center problems, a fundamental challenge in optimal function learning and geometric optimization. $\\textsf{gradOL}$ hinges on reformulating the semi-infinite problem as a finitary max-min optimization, making it amenable to gradient-based techniques. By leveraging automatic differentiation for precise numerical gradient computation, $\\textsf{gradOL}$ ensures numerical stability and scalability, making it suitable for large-scale settings. Under strong convexity of the ambient norm, $\\textsf{gradOL}$ provably recovers optimal Chebyshev centers while directly computing the associated radius. This addresses a key bottleneck in constructing stable optimal interpolants. Empirically, $\\textsf{gradOL}$ achieves significant improvements in accuracy and efficiency on 34 benchmark Chebyshev center problems from a benchmark $\\textsf{CSIP}$ library. Moreover, we extend $\\textsf{gradOL}$ to general convex semi-infinite programming (CSIP), attaining up to $4000\\times$ speedups over the state-of-the-art $\\texttt{SIPAMPL}$ solver tested on the indicated $\\textsf{CSIP}$ library containing 67 benchmark problems. Furthermore, we provide the first theoretical foundation for applying gradient-based methods to Chebyshev center problems, bridging rigorous analysis with practical algorithms. $\\textsf{gradOL}$ thus offers a unified solution framework for Chebyshev centers and broader CSIPs.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "math.OC",
    "comment": "Accepted to TMLR",
    "pdf_url": "https://arxiv.org/pdf/2601.06434v1",
    "published_date": "2026-01-10 05:34:01 UTC",
    "updated_date": "2026-01-10 05:34:01 UTC"
  },
  {
    "arxiv_id": "2601.06431v2",
    "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
    "authors": [
      "Qingyu Ren",
      "Qianyu He",
      "Jingwen Chang",
      "Jie Zeng",
      "Jiaqing Liang",
      "Yanghua Xiao",
      "Han Xia",
      "Zeye Sun",
      "Fei Yu"
    ],
    "abstract": "Instruction-following is critical for large language models, but real-world instructions often contain logical structures such as sequential dependencies and conditional branching. Existing methods typically construct datasets with parallel constraints and optimize average rewards, ignoring logical dependencies and yielding noisy signals. We propose a logic-structured training framework LSRIF that explicitly models instruction logic. We first construct a dataset LSRInstruct with constraint structures such as parallel, sequential, and conditional types, and then design structure-aware rewarding method LSRIF including average aggregation for parallel structures, failure-penalty propagation for sequential structures, and selective rewards for conditional branches. Experiments show LSRIF brings significant improvements in instruction-following (in-domain and out-of-domain) and general reasoning. Analysis reveals that learning with explicit logic structures brings parameter updates in attention layers and sharpens token-level attention to constraints and logical operators.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06431v2",
    "published_date": "2026-01-10 05:11:38 UTC",
    "updated_date": "2026-01-14 02:51:23 UTC"
  },
  {
    "arxiv_id": "2601.06426v1",
    "title": "NC-Bench: An LLM Benchmark for Evaluating Conversational Competence",
    "authors": [
      "Robert J. Moore",
      "Sungeun An",
      "Farhan Ahmed",
      "Jay Pankaj Gala"
    ],
    "abstract": "The Natural Conversation Benchmark (NC-Bench) introduce a new approach to evaluating the general conversational competence of large language models (LLMs). Unlike prior benchmarks that focus on the content of model behavior, NC-Bench focuses on the form and structure of natural conversation. Grounded in the IBM Natural Conversation Framework (NCF), NC-Bench comprises three distinct sets. The Basic Conversation Competence set evaluates fundamental sequence management practices, such as answering inquiries, repairing responses, and closing conversational pairs. The RAG set applies the same sequence management patterns as the first set but incorporates retrieval-augmented generation (RAG). The Complex Request set extends the evaluation to complex requests involving more intricate sequence management patterns. Each benchmark tests a model's ability to produce contextually appropriate conversational actions in response to characteristic interaction patterns. Initial evaluations across 6 open-source models and 14 interaction patterns show that models perform well on basic answering tasks, struggle more with repair tasks (especially repeat), have mixed performance on closing sequences, and find complex multi-turn requests most challenging, with Qwen models excelling on the Basic set and Granite models on the RAG set and the Complex Request set. By operationalizing fundamental principles of human conversation, NC-Bench provides a lightweight, extensible, and theory-grounded framework for assessing and improving the conversational abilities of LLMs beyond topical or task-specific benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 1 figure, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2601.06426v1",
    "published_date": "2026-01-10 04:57:24 UTC",
    "updated_date": "2026-01-10 04:57:24 UTC"
  },
  {
    "arxiv_id": "2601.06425v1",
    "title": "HiDVFS: A Hierarchical Multi-Agent DVFS Scheduler for OpenMP DAG Workloads",
    "authors": [
      "Mohammad Pivezhandi",
      "Abusayeed Saifullah",
      "Ali Jannesari"
    ],
    "abstract": "With advancements in multicore embedded systems, leakage power, exponentially tied to chip temperature, has surpassed dynamic power consumption. Energy-aware solutions use dynamic voltage and frequency scaling (DVFS) to mitigate overheating in performance-intensive scenarios, while software approaches allocate high-utilization tasks across core configurations in parallel systems to reduce power. However, existing heuristics lack per-core frequency monitoring, failing to address overheating from uneven core activity, and task assignments without detailed profiling overlook irregular execution patterns. We target OpenMP DAG workloads. Because makespan, energy, and thermal goals often conflict within a single benchmark, this work prioritizes performance (makespan) while reporting energy and thermal as secondary outcomes. To overcome these issues, we propose HiDVFS (a hierarchical multi-agent, performance-aware DVFS scheduler) for parallel systems that optimizes task allocation based on profiling data, core temperatures, and makespan-first objectives. It employs three agents: one selects cores and frequencies using profiler data, another manages core combinations via temperature sensors, and a third sets task priorities during resource contention. A makespan-focused reward with energy and temperature regularizers estimates future states and enhances sample efficiency. Experiments on the NVIDIA Jetson TX2 using the BOTS suite (9 benchmarks) compare HiDVFS against state-of-the-art approaches. With multi-seed validation (seeds 42, 123, 456), HiDVFS achieves the best finetuned performance with 4.16 plus/minus 0.58s average makespan (L10), representing a 3.44x speedup over GearDVFS (14.32 plus/minus 2.61s) and 50.4% energy reduction (63.7 kJ vs 128.4 kJ). Across all BOTS benchmarks, HiDVFS achieves an average 3.95x speedup and 47.1% energy reduction.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "38 pages, 15 figures, 8 tables",
    "pdf_url": "https://arxiv.org/pdf/2601.06425v1",
    "published_date": "2026-01-10 04:42:42 UTC",
    "updated_date": "2026-01-10 04:42:42 UTC"
  },
  {
    "arxiv_id": "2601.06423v1",
    "title": "Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs",
    "authors": [
      "Deep Mehta"
    ],
    "abstract": "Self-consistency has emerged as a popular technique for improving large language model accuracy on reasoning tasks. The approach is straightforward: generate multiple reasoning paths and select the most common answer through majority voting. While this reliably boosts accuracy, it remains unclear whether these gains reflect genuine improvements in reasoning quality. We investigate a fundamental question that has not been studied before: does inference scaling improve reasoning faithfulness?\n  We conduct a comprehensive empirical study across four frontier models (GPT-5.2, Claude Opus 4.5, Gemini-3-flash-preview, and DeepSeek-v3.2) on 100 GSM8K mathematical reasoning problems. Our analysis employs bootstrap confidence intervals, McNemar's tests for paired comparisons, and Cohen's d effect sizes to quantify the effects rigorously. The results reveal striking differences across models that challenge common assumptions about self-consistency.\n  GPT-5.2 shows the expected pattern: accuracy improves from 78% to 90% at N=5, with faithfulness remaining relatively stable (0.540 to 0.510). Claude Opus 4.5 tells a completely different story. Its accuracy actually drops from 78% to 74.3% while faithfulness jumps dramatically from 0.270 to 0.891 at N=5. DeepSeek-v3.2, already at 98% accuracy, shows ceiling effects with modest faithfulness gains (0.440 to 0.541). Gemini-3-flash improves from 81% to 86% accuracy with a slight faithfulness decrease (0.260 to 0.212).\n  Problem difficulty analysis reveals that GPT-5.2 solves 82% of hard problems while breaking only 13% of easy ones. Claude, in contrast, breaks 23% of easy problems, explaining its accuracy decrease. These findings matter for practitioners: self-consistency is not universally beneficial, and teams should test their specific models before deployment. We release our code and provide practical recommendations for navigating these tradeoffs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "24 pages, 3 figures, 9 tables",
    "pdf_url": "https://arxiv.org/pdf/2601.06423v1",
    "published_date": "2026-01-10 04:20:00 UTC",
    "updated_date": "2026-01-10 04:20:00 UTC"
  },
  {
    "arxiv_id": "2601.06415v1",
    "title": "Semantic Enrichment of CAD-Based Industrial Environments via Scene Graphs for Simulation and Reasoning",
    "authors": [
      "Nathan Pascal Walus",
      "Ranulfo Bezerra",
      "Shotaro Kojima",
      "Tsige Tadesse Alemayoh",
      "Satoshi Tadokoro",
      "Kazunori Ohno"
    ],
    "abstract": "Utilizing functional elements in an industrial environment, such as displays and interactive valves, provide effective possibilities for robot training. When preparing simulations for robots or applications that involve high-level scene understanding, the simulation environment must be equally detailed. Although CAD files for such environments deliver an exact description of the geometry and visuals, they usually lack semantic, relational and functional information, thus limiting the simulation and training possibilities. A 3D scene graph can organize semantic, spatial and functional information by enriching the environment through a Large Vision-Language Model (LVLM). In this paper we present an offline approach to creating detailed 3D scene graphs from CAD environments. This will serve as a foundation to include the relations of functional and actionable elements, which then can be used for dynamic simulation and reasoning. Key results of this research include both quantitative results of the generated semantic labels as well as qualitative results of the scene graph, especially in hindsight of pipe structures and identified functional relations. All code, results and the environment will be made available at https://cad-scenegraph.github.io",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted to IEEE SSRR 2025",
    "pdf_url": "https://arxiv.org/pdf/2601.06415v1",
    "published_date": "2026-01-10 03:22:29 UTC",
    "updated_date": "2026-01-10 03:22:29 UTC"
  },
  {
    "arxiv_id": "2601.06401v1",
    "title": "BizFinBench.v2: A Unified Dual-Mode Bilingual Benchmark for Expert-Level Financial Capability Alignment",
    "authors": [
      "Xin Guo",
      "Rongjunchen Zhang",
      "Guilong Lu",
      "Xuntao Guo",
      "Shuai Jia",
      "Zhi Yang",
      "Liwen Zhang"
    ],
    "abstract": "Large language models have undergone rapid evolution, emerging as a pivotal technology for intelligence in financial operations. However, existing benchmarks are often constrained by pitfalls such as reliance on simulated or general-purpose samples and a focus on singular, offline static scenarios. Consequently, they fail to align with the requirements for authenticity and real-time responsiveness in financial services, leading to a significant discrepancy between benchmark performance and actual operational efficacy. To address this, we introduce BizFinBench.v2, the first large-scale evaluation benchmark grounded in authentic business data from both Chinese and U.S. equity markets, integrating online assessment. We performed clustering analysis on authentic user queries from financial platforms, resulting in eight fundamental tasks and two online tasks across four core business scenarios, totaling 29,578 expert-level Q&A pairs. Experimental results demonstrate that ChatGPT-5 achieves a prominent 61.5% accuracy in main tasks, though a substantial gap relative to financial experts persists; in online tasks, DeepSeek-R1 outperforms all other commercial LLMs. Error analysis further identifies the specific capability deficiencies of existing models within practical financial business contexts. BizFinBench.v2 transcends the limitations of current benchmarks, achieving a business-level deconstruction of LLM financial capabilities and providing a precise basis for evaluating efficacy in the widespread deployment of LLMs within the financial domain. The data and code are available at https://github.com/HiThink-Research/BizFinBench.v2.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06401v1",
    "published_date": "2026-01-10 02:51:53 UTC",
    "updated_date": "2026-01-10 02:51:53 UTC"
  },
  {
    "arxiv_id": "2601.10740v1",
    "title": "Neuro-Symbolic Activation Discovery: Transferring Mathematical Structures from Physics to Ecology for Parameter-Efficient Neural Networks",
    "authors": [
      "Anas Hajbi"
    ],
    "abstract": "Modern neural networks rely on generic activation functions (ReLU, GELU, SiLU) that ignore the mathematical structure inherent in scientific data. We propose Neuro-Symbolic Activation Discovery, a framework that uses Genetic Programming to extract interpretable mathematical formulas from data and inject them as custom activation functions. Our key contribution is the discovery of a Geometric Transfer phenomenon: activation functions learned from particle physics data successfully generalize to ecological classification, outperforming standard activations (ReLU, GELU, SiLU) in both accuracy and parameter efficiency. On the Forest Cover dataset, our Hybrid Transfer model achieves 82.4% accuracy with only 5,825 parameters, compared to 83.4% accuracy requiring 31,801 parameters for a conventional heavy network -- a 5.5x parameter reduction with only 1% accuracy loss. We introduce a Parameter Efficiency Score ($E_{param} = AUC / \\log_{10}(Params)$) and demonstrate that lightweight hybrid architectures consistently achieve 18-21% higher efficiency than over-parameterized baselines. Crucially, we establish boundary conditions: while Physics to Ecology transfer succeeds (both involve continuous Euclidean measurements), Physics to Text transfer fails (discrete word frequencies require different mathematical structures). Our work opens pathways toward domain-specific activation libraries for efficient scientific machine learning.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.10740v1",
    "published_date": "2026-01-10 02:49:32 UTC",
    "updated_date": "2026-01-10 02:49:32 UTC"
  },
  {
    "arxiv_id": "2601.06394v1",
    "title": "Context Matters: Peer-Aware Student Behavioral Engagement Measurement via VLM Action Parsing and LLM Sequence Classification",
    "authors": [
      "Ahmed Abdelkawy",
      "Ahmed Elsayed",
      "Asem Ali",
      "Aly Farag",
      "Thomas Tretter",
      "Michael McIntyre"
    ],
    "abstract": "Understanding student behavior in the classroom is essential to improve both pedagogical quality and student engagement. Existing methods for predicting student engagement typically require substantial annotated data to model the diversity of student behaviors, yet privacy concerns often restrict researchers to their own proprietary datasets. Moreover, the classroom context, represented in peers' actions, is ignored. To address the aforementioned limitation, we propose a novel three-stage framework for video-based student engagement measurement. First, we explore the few-shot adaptation of the vision-language model for student action recognition, which is fine-tuned to distinguish among action categories with a few training samples. Second, to handle continuous and unpredictable student actions, we utilize the sliding temporal window technique to divide each student's 2-minute-long video into non-overlapping segments. Each segment is assigned an action category via the fine-tuned VLM model, generating a sequence of action predictions. Finally, we leverage the large language model to classify this entire sequence of actions, together with the classroom context, as belonging to an engaged or disengaged student. The experimental results demonstrate the effectiveness of the proposed approach in identifying student engagement.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06394v1",
    "published_date": "2026-01-10 02:39:24 UTC",
    "updated_date": "2026-01-10 02:39:24 UTC"
  },
  {
    "arxiv_id": "2601.06377v1",
    "title": "HiMem: Hierarchical Long-Term Memory for LLM Long-Horizon Agents",
    "authors": [
      "Ningning Zhang",
      "Xingxing Yang",
      "Zhizhong Tan",
      "Weiping Deng",
      "Wenyong Wang"
    ],
    "abstract": "Although long-term memory systems have made substantial progress in recent years, they still exhibit clear limitations in adaptability, scalability, and self-evolution under continuous interaction settings. Inspired by cognitive theories, we propose HiMem, a hierarchical long-term memory framework for long-horizon dialogues, designed to support memory construction, retrieval, and dynamic updating during sustained interactions. HiMem constructs cognitively consistent Episode Memory via a Topic-Aware Event--Surprise Dual-Channel Segmentation strategy, and builds Note Memory that captures stable knowledge through a multi-stage information extraction pipeline. These two memory types are semantically linked to form a hierarchical structure that bridges concrete interaction events and abstract knowledge, enabling efficient retrieval without sacrificing information fidelity. HiMem supports both hybrid and best-effort retrieval strategies to balance accuracy and efficiency, and incorporates conflict-aware Memory Reconsolidation to revise and supplement stored knowledge based on retrieval feedback. This design enables continual memory self-evolution over long-term use. Experimental results on long-horizon dialogue benchmarks demonstrate that HiMem consistently outperforms representative baselines in accuracy, consistency, and long-term reasoning, while maintaining favorable efficiency. Overall, HiMem provides a principled and scalable design paradigm for building adaptive and self-evolving LLM-based conversational agents. The code is available at https://github.com/jojopdq/HiMem.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06377v1",
    "published_date": "2026-01-10 01:26:01 UTC",
    "updated_date": "2026-01-10 01:26:01 UTC"
  },
  {
    "arxiv_id": "2601.06366v1",
    "title": "SafeGPT: Preventing Data Leakage and Unethical Outputs in Enterprise LLM Use",
    "authors": [
      "Pratyush Desai",
      "Luoxi Tang",
      "Yuqiao Meng",
      "Zhaohan Xi"
    ],
    "abstract": "Large Language Models (LLMs) are transforming enterprise workflows but introduce security and ethics challenges when employees inadvertently share confidential data or generate policy-violating content. This paper proposes SafeGPT, a two-sided guardrail system preventing sensitive data leakage and unethical outputs. SafeGPT integrates input-side detection/redaction, output-side moderation/reframing, and human-in-the-loop feedback. Experiments demonstrate SafeGPT effectively reduces data leakage risk and biased outputs while maintaining satisfaction.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06366v1",
    "published_date": "2026-01-10 00:33:38 UTC",
    "updated_date": "2026-01-10 00:33:38 UTC"
  },
  {
    "arxiv_id": "2601.06364v1",
    "title": "Human-in-the-Loop Interactive Report Generation for Chronic Disease Adherence",
    "authors": [
      "Xiaotian Zhang",
      "Jinhong Yu",
      "Pengwei Yan",
      "Le Jiang",
      "Xingyi Shen",
      "Mumo Cheng",
      "Xiaozhong Liu"
    ],
    "abstract": "Chronic disease management requires regular adherence feedback to prevent avoidable hospitalizations, yet clinicians lack time to produce personalized patient communications. Manual authoring preserves clinical accuracy but does not scale; AI generation scales but can undermine trust in patient-facing contexts. We present a clinician-in-the-loop interface that constrains AI to data organization and preserves physician oversight through recognition-based review. A single-page editor pairs AI-generated section drafts with time-aligned visualizations, enabling inline editing with visual evidence for each claim. This division of labor (AI organizes, clinician decides) targets both efficiency and accountability. In a pilot with three physicians reviewing 24 cases, AI successfully generated clinically personalized drafts matching physicians' manual authoring practice (overall mean 4.86/10 vs. 5.0/10 baseline), requiring minimal physician editing (mean 8.3\\% content modification) with zero safety-critical issues, demonstrating effective automation of content generation. However, review time remained comparable to manual practice, revealing an accountability paradox: in high-stakes clinical contexts, professional responsibility requires complete verification regardless of AI accuracy. We contribute three interaction patterns for clinical AI collaboration: bounded generation with recognition-based review via chart-text pairing, automated urgency flagging that analyzes vital trends and adherence patterns with fail-safe escalation for missed critical monitoring tasks, and progressive disclosure controls that reduce cognitive load while maintaining oversight. These patterns indicate that clinical AI efficiency requires not only accurate models, but also mechanisms for selective verification that preserve accountability.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "5 pages, 3 figures. Accepted at the AAAI 2026 Workshop on AI for Healthy Aging and Longevity",
    "pdf_url": "https://arxiv.org/pdf/2601.06364v1",
    "published_date": "2026-01-10 00:19:33 UTC",
    "updated_date": "2026-01-10 00:19:33 UTC"
  },
  {
    "arxiv_id": "2601.06362v1",
    "title": "Styles + Persona-plug = Customized LLMs",
    "authors": [
      "Yutong Song",
      "Jiang Wu",
      "Shaofan Yuan",
      "Chengze Shen",
      "Jian Wang",
      "Amir Rahmani",
      "Nikil Dutt",
      "Yu Wang"
    ],
    "abstract": "We discover a previously overlooked challenge in personalized text generation: personalization methods are increasingly applied under explicit style instructions, yet their behavior under such constraints remains poorly understood. To balance implicit personalization and explicit style, we formulate personalization as a distributional residual and propose PsPLUG, a lightweight soft-prompt plug-in trained with style-conditioned preference contrasts. Across LaMP benchmark, our framework improves persona alignment, maintains stylistic fidelity, and outperforms retrieval-based and soft-prompt baselines with minimal computation. These results show that residual modeling provides a simple and principled foundation for controllable, style-aware LLM personalization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.06362v1",
    "published_date": "2026-01-10 00:14:43 UTC",
    "updated_date": "2026-01-10 00:14:43 UTC"
  }
]
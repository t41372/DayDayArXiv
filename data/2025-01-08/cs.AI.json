{
  "date": "2025-01-08",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-01-08 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦人工智能和机器学习领域，特别是大型语言模型（LLM）的应用创新、神经网络优化以及多模态处理，令人印象深刻的是 Ben Goertzel 的神经符号学习论文，以及 LLM 在代码生成和机器人导航中的潜力，突显了 AI 在实际应用中的高效性和挑战。\n\n### 重点论文讨论\n我们挑选了最具话题度和影响力的论文优先讨论，包括 AI 模型优化、代码生成和机器人应用领域。以下按主题分组，相关论文放在一起简要分析。\n\n#### AI 和 LLM 创新\n- **Real-Time Textless Dialogue Generation (实时无文本对话生成)**  \n  这篇论文由 Long Mai 和 Julie Carson-Berndsen 撰写，提出 RTTL-DG 模型，直接处理语音流以实现自然对话。主要贡献是提升对话的流畅性和实时性，通过整合背景音和笑声等旁语言信号，实验显示它超越传统级联系统。该方法在语音对话系统中有重要启示，尤其适用于智能助手。\n\n- **Do Code LLMs Understand Design Patterns? (代码 LLM 是否理解设计模式？)**  \n  作者包括 Yongbin Li 和 Han Liu，论文探讨代码 LLM 在设计模式识别和生成的偏见问题。通过实验评估模型在识别、理解和生成方面的表现，发现 LLM 存在显著偏见，可能影响软件开发。该发现提醒开发者在实际应用中需注意 LLM 的可靠性，已被 ICSE 2025 接受。\n\n- **ActPC-Geom: Towards Scalable Online Neural-Symbolic Learning (ActPC-Geom: 基于信息几何的在线神经符号学习)**  \n  Ben Goertzel 的论文引入 ActPC-Geom 框架，使用 Wasserstein 度量优化神经网络的学习过程。主要贡献是通过神经近似和嵌入技术，实现实时在线学习和符号-子符号集成，适用于复杂认知任务。该工作由知名学者主导，展示了 AI 在符号推理中的潜力。\n\n- **EpiCoder: Encompassing Diversity and Complexity in Code Generation (EpiCoder: 代码生成的多样性和复杂性)**  \n  论文提出基于特征树的代码生成框架，超越传统 AST 方法，支持从简单函数到多文件场景。贡献在于提升代码 LLM 的多样性和准确性，实验在多个基准上达到 SOTA。该方法对软件工程有实际影响。\n\n- **URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics (URSA: 多模态数学中的链式推理理解与验证)**  \n  作者团队开发了 URSA-8B 模型，通过 CoT 数据合成和双视图标签，实现多模态数学推理的验证。贡献是提升模型的推理准确性和鲁棒性，实验在多个基准上超越 GPT-4o。该论文开源，提供新工具。\n\n#### 机器人和多模态应用\n- **InfiGUIAgent: A Multimodal Generalist GUI Agent (InfiGUIAgent: 多模态通用 GUI 代理)**  \n  论文由 Sergey Levine 参与，提出 InfiGUIAgent 框架，使用分层推理提升 GUI 交互。贡献在于通过多模态提示实现零样本任务，实验显示它在机器人操作中提升成功率。该工作结合视觉和语言，适用于自主系统。\n\n- **FuSe: Feedback-Driven Vision-Language Alignment (FuSe: 基于反馈的视觉-语言对齐)**  \n  Sergey Levine 团队的另一贡献，提出 FuSe 方法，使用多模态对比学习处理视觉、触觉和音频。贡献是提升机器人任务的鲁棒性，实验在真实环境中减少幻觉。该方法对自主驾驶有启示。\n\n其他论文，如能源优化、金融 VQA 或医疗图像处理，虽然有实际应用（如 XGBoost 在土木工程中的表现），但相对不那么核心，我们快速掠过：例如，\"Intelligent Gradient Boosting Algorithms for Estimating Strength of Modified Subgrade Soil (智能梯度提升算法用于改进土基强度估计)\" 使用 XGBoost 提升预测准确性，但细节较常规；\"Microservice Deployment in Space Computing Power Networks (微服务在空间计算网络中的部署)\" 探讨强化学习在卫星任务中的作用，贡献有限。\n\n总之，今天的论文强调 AI 的高效训练和应用潜力，Ben Goertzel 和 Sergey Levine 的作品尤其值得关注。感兴趣的读者可查阅特定标题深入探索！",
  "papers": [
    {
      "arxiv_id": "2501.04882v1",
      "title": "Reach Measurement, Optimization and Frequency Capping In Targeted Online Advertising Under k-Anonymity",
      "title_zh": "在 k-Anonymity 下的针对性在线广告到达测量、优化与频率上限",
      "authors": [
        "Yuan Gao",
        "Mu Qiao"
      ],
      "abstract": "The growth in the use of online advertising to foster brand awareness over\nrecent years is largely attributable to the ubiquity of social media. One\npivotal technology contributing to the success of online brand advertising is\nfrequency capping, a mechanism that enables marketers to control the number of\ntimes an ad is shown to a specific user. However, the very foundation of this\ntechnology is being scrutinized as the industry gravitates towards advertising\nsolutions that prioritize user privacy. This paper delves into the issue of\nreach measurement and optimization within the context of $k$-anonymity, a\nprivacy-preserving model gaining traction across major online advertising\nplatforms. We outline how to report reach within this new privacy landscape and\ndemonstrate how probabilistic discounting, a probabilistic adaptation of\ntraditional frequency capping, can be employed to optimize campaign\nperformance. Experiments are performed to assess the trade-off between user\nprivacy and the efficacy of online brand advertising. Notably, we discern a\nsignificant dip in performance as long as privacy is introduced, yet this comes\nwith a limited additional cost for advertising platforms to offer their users\nmore privacy.",
      "tldr_zh": "这篇论文探讨了在 k-Anonymity 隐私模型下，针对在线广告的覆盖率（reach）测量、优化和频次控制（frequency capping），以应对日益重视用户隐私的行业需求。作者引入 probabilistic discounting 作为传统频次控制的概率适应版，用于优化广告效果，同时评估隐私保护对广告效能的影响。通过实验，研究发现引入 k-Anonymity 会导致广告性能显著下降，但广告平台提供更多隐私的额外成本相对有限。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG",
        "stat.AP",
        "stat.ML"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04882v1",
      "published_date": "2025-01-08 23:38:19 UTC",
      "updated_date": "2025-01-08 23:38:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:27:13.775892"
    },
    {
      "arxiv_id": "2501.04877v1",
      "title": "Real-Time Textless Dialogue Generation",
      "title_zh": "实时无文本对话生成",
      "authors": [
        "Long Mai",
        "Julie Carson-Berndsen"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have led to significant\nprogress in text-based dialogue systems. These systems can now generate\nhigh-quality responses that are accurate and coherent across a wide range of\ntopics and tasks. However, spoken dialogue systems still lag behind in terms of\nnaturalness. They tend to produce robotic interactions, with issues such as\nslow response times, overly generic or cautious replies, and a lack of natural\nrhythm and fluid turn-taking. This shortcoming is largely due to the\nover-reliance on the traditional cascaded design, which involve separate,\nsequential components, as well as the use of text as an intermediate\nrepresentation. This paper propose a real-time, textless spoken dialogue\ngeneration model (RTTL-DG) that aims to overcome these challenges. Our system\nenables fluid turn-taking and generates responses with minimal delay by\nprocessing streaming spoken conversation directly. Additionally, our model\nincorporates backchannels, filters, laughter, and other paralinguistic signals,\nwhich are often absent in cascaded dialogue systems, to create more natural and\nhuman-like interactions. The implementations and generated samples are\navailable in our repository: https://github.com/mailong25/rts2s-dg",
      "tldr_zh": "本文讨论了大型语言模型（LLMs）在文本对话系统中的进展，但口语对话系统因传统级联设计和文本中间表示而存在响应缓慢、自然性差等问题，如机器人般的互动和缺乏流畅轮转。论文提出了一种实时、无文本的口语对话生成模型（RTTL-DG），它直接处理流式对话，实现最小延迟的响应并融入回渠道、笑声等副语言信号，以创建更自然的人类-like 互动。该模型显著提升了对话的流畅性和真实感，相关实现和样本已在 GitHub 上提供。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04877v1",
      "published_date": "2025-01-08 23:21:43 UTC",
      "updated_date": "2025-01-08 23:21:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:27:26.796731"
    },
    {
      "arxiv_id": "2501.04873v2",
      "title": "Back Home: A Machine Learning Approach to Seashell Classification and Ecosystem Restoration",
      "title_zh": "翻译失败",
      "authors": [
        "Alexander Valverde",
        "Luis Solano"
      ],
      "abstract": "In Costa Rica, an average of 5 tons of seashells are extracted from\necosystems annually. Confiscated seashells, cannot be returned to their\necosystems due to the lack of origin recognition. To address this issue, we\ndeveloped a convolutional neural network (CNN) specifically for seashell\nidentification. We built a dataset from scratch, consisting of approximately\n19000 images from the Pacific and Caribbean coasts. Using this dataset, the\nmodel achieved a classification accuracy exceeding 85%. The model has been\nintegrated into a user-friendly application, which has classified over 36,000\nseashells to date, delivering real-time results within 3 seconds per image. To\nfurther enhance the system's accuracy, an anomaly detection mechanism was\nincorporated to filter out irrelevant or anomalous inputs, ensuring only valid\nseashell images are processed.",
      "tldr_zh": "这篇论文提出了一种机器学习方法，用于贝壳分类，以解决哥斯达黎加生态系统中每年提取的5吨贝壳无法归还的问题。研究团队从零构建了一个包含约19,000张图像的数据集，并使用CNN（卷积神经网络）模型实现了超过85%的分类准确率。该模型已整合到一个用户友好的应用程序中，成功分类超过36,000个贝壳图像，每张图像在3秒内提供实时结果，并通过异常检测机制过滤无效输入，进一步提升了系统可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04873v2",
      "published_date": "2025-01-08 23:07:10 UTC",
      "updated_date": "2025-03-06 17:35:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:27:38.141916"
    },
    {
      "arxiv_id": "2501.04848v1",
      "title": "Exploring Large Language Models for Semantic Analysis and Categorization of Android Malware",
      "title_zh": "探索大语言模型用于 Android 恶意软件的语义分析和分类",
      "authors": [
        "Brandon J Walton",
        "Mst Eshita Khatun",
        "James M Ghawaly",
        "Aisha Ali-Gombe"
      ],
      "abstract": "Malware analysis is a complex process of examining and evaluating malicious\nsoftware's functionality, origin, and potential impact. This arduous process\ntypically involves dissecting the software to understand its components,\ninfection vector, propagation mechanism, and payload. Over the years, deep\nreverse engineering of malware has become increasingly tedious, mainly due to\nmodern malicious codebases' fast evolution and sophistication. Essentially,\nanalysts are tasked with identifying the elusive needle in the haystack within\nthe complexities of zero-day malware, all while under tight time constraints.\nThus, in this paper, we explore leveraging Large Language Models (LLMs) for\nsemantic malware analysis to expedite the analysis of known and novel samples.\nBuilt on GPT-4o-mini model, \\msp is designed to augment malware analysis for\nAndroid through a hierarchical-tiered summarization chain and strategic prompt\nengineering. Additionally, \\msp performs malware categorization, distinguishing\npotential malware from benign applications, thereby saving time during the\nmalware reverse engineering process. Despite not being fine-tuned for Android\nmalware analysis, we demonstrate that through optimized and advanced prompt\nengineering \\msp can achieve up to 77% classification accuracy while providing\nhighly robust summaries at functional, class, and package levels. In addition,\nleveraging the backward tracing of the summaries from package to function\nlevels allowed us to pinpoint the precise code snippets responsible for\nmalicious behavior.",
      "tldr_zh": "这篇论文探讨了使用 Large Language Models (LLMs) 来加速 Android Malware 的语义分析和分类，以应对恶意软件快速演变的挑战。研究构建了基于 GPT-4o-mini 的 \\msp 系统，通过分层总结链和策略性提示工程，实现对恶意软件的功能、类和包级别的总结，并区分潜在恶意应用与良性应用。结果显示，即使未进行微调，\\msp 也能达到 77% 的分类准确率，并通过逆向追踪精确定位恶意代码片段，从而显著提高分析效率。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04848v1",
      "published_date": "2025-01-08 21:22:45 UTC",
      "updated_date": "2025-01-08 21:22:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:27:50.133875"
    },
    {
      "arxiv_id": "2501.04844v1",
      "title": "Enhancing Listened Speech Decoding from EEG via Parallel Phoneme Sequence Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Jihwan Lee",
        "Tiantian Feng",
        "Aditya Kommineni",
        "Sudarsana Reddy Kadiri",
        "Shrikanth Narayanan"
      ],
      "abstract": "Brain-computer interfaces (BCI) offer numerous human-centered application\npossibilities, particularly affecting people with neurological disorders. Text\nor speech decoding from brain activities is a relevant domain that could\naugment the quality of life for people with impaired speech perception. We\npropose a novel approach to enhance listened speech decoding from\nelectroencephalography (EEG) signals by utilizing an auxiliary phoneme\npredictor that simultaneously decodes textual phoneme sequences. The proposed\nmodel architecture consists of three main parts: EEG module, speech module, and\nphoneme predictor. The EEG module learns to properly represent EEG signals into\nEEG embeddings. The speech module generates speech waveforms from the EEG\nembeddings. The phoneme predictor outputs the decoded phoneme sequences in text\nmodality. Our proposed approach allows users to obtain decoded listened speech\nfrom EEG signals in both modalities (speech waveforms and textual phoneme\nsequences) simultaneously, eliminating the need for a concatenated sequential\npipeline for each modality. The proposed approach also outperforms previous\nmethods in both modalities. The source code and speech samples are publicly\navailable.",
      "tldr_zh": "这篇论文提出了一种新型方法，通过并行音素序列预测来增强从 EEG 信号解码聆听语音，旨在改善脑机接口 (BCI) 对言语障碍患者的辅助。模型架构包括 EEG 模块（将 EEG 信号转换为嵌入）、语音模块（生成语音波形）和音素预测器（输出文本音素序列），实现同时解码语音波形和文本模式，而无需串联管道。实验结果显示，该方法在两种模式下均优于现有技术，并公开了源代码和语音样本，为 BCI 应用提供了更高效的解决方案。",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "eess.SP"
      ],
      "primary_category": "eess.AS",
      "comment": "ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.04844v1",
      "published_date": "2025-01-08 21:11:35 UTC",
      "updated_date": "2025-01-08 21:11:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:28:02.065907"
    },
    {
      "arxiv_id": "2501.06250v1",
      "title": "Generative AI for Cel-Animation: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Yunlong Tang",
        "Junjia Guo",
        "Pinxin Liu",
        "Zhiyuan Wang",
        "Hang Hua",
        "Jia-Xing Zhong",
        "Yunzhong Xiao",
        "Chao Huang",
        "Luchuan Song",
        "Susan Liang",
        "Yizhi Song",
        "Liu He",
        "Jing Bi",
        "Mingqian Feng",
        "Xinyang Li",
        "Zeliang Zhang",
        "Chenliang Xu"
      ],
      "abstract": "Traditional Celluloid (Cel) Animation production pipeline encompasses\nmultiple essential steps, including storyboarding, layout design, keyframe\nanimation, inbetweening, and colorization, which demand substantial manual\neffort, technical expertise, and significant time investment. These challenges\nhave historically impeded the efficiency and scalability of Cel-Animation\nproduction. The rise of generative artificial intelligence (GenAI),\nencompassing large language models, multimodal models, and diffusion models,\noffers innovative solutions by automating tasks such as inbetween frame\ngeneration, colorization, and storyboard creation. This survey explores how\nGenAI integration is revolutionizing traditional animation workflows by\nlowering technical barriers, broadening accessibility for a wider range of\ncreators through tools like AniDoc, ToonCrafter, and AniSora, and enabling\nartists to focus more on creative expression and artistic innovation. Despite\nits potential, issues such as maintaining visual consistency, ensuring\nstylistic coherence, and addressing ethical considerations continue to pose\nchallenges. Furthermore, this paper discusses future directions and explores\npotential advancements in AI-assisted animation. For further exploration and\nresources, please visit our GitHub repository:\nhttps://github.com/yunlong10/Awesome-AI4Animation",
      "tldr_zh": "这篇调查论文探讨了生成式 AI（Generative AI）在 Cel-Animation 生产流程中的应用，包括大型语言模型、multimodal models 和 diffusion models 等技术，用于自动化故事板、布局设计、关键帧动画、中间帧生成和上色等步骤，从而降低手动工作量和技术门槛。Generative AI 通过工具如 AniDoc、ToonCrafter 和 AniSora 增强了动画创作的可访问性，让艺术家更专注于创意创新，但仍面临视觉一致性、风格连贯性和伦理问题的挑战。论文还讨论了未来 AI 辅助动画的发展方向，并提供了 GitHub 资源以供进一步探索。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CV",
      "comment": "20 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.06250v1",
      "published_date": "2025-01-08 20:57:39 UTC",
      "updated_date": "2025-01-08 20:57:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:28:14.014700"
    },
    {
      "arxiv_id": "2501.04835v1",
      "title": "Do Code LLMs Understand Design Patterns?",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenyu Pan",
        "Xuefeng Song",
        "Yunkun Wang",
        "Rongyu Cao",
        "Binhua Li",
        "Yongbin Li",
        "Han Liu"
      ],
      "abstract": "Code Large Language Models (LLMs) demonstrate great versatility in adapting\nto various downstream tasks, including code generation and completion, as well\nas bug detection and fixing. However, Code LLMs often fail to capture existing\ncoding standards, leading to the generation of code that conflicts with the\nrequired design patterns for a given project. As a result, developers must\npost-process to adapt the generated code to the project's design norms. In this\nwork, we empirically investigate the biases of Code LLMs in software\ndevelopment. Through carefully designed experiments, we assess the models'\nunderstanding of design patterns across recognition, comprehension, and\ngeneration. Our findings reveal that biases in Code LLMs significantly affect\nthe reliability of downstream tasks.",
      "tldr_zh": "本研究调查了Code LLMs在软件开发中的偏差，特别关注它们是否真正理解design patterns。研究通过精心设计的实验评估了Code LLMs在design patterns的识别、理解和生成方面的能力，结果显示这些模型存在显著偏差，导致生成的代码不符合项目编码标准。总体而言，该工作揭示了Code LLMs的局限性，并强调了改进其可靠性的必要性，以支持下游任务如代码生成和bug修复。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "accpeted by llm4code workshop in ICSE 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.04835v1",
      "published_date": "2025-01-08 20:39:45 UTC",
      "updated_date": "2025-01-08 20:39:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:28:25.906626"
    },
    {
      "arxiv_id": "2501.04832v1",
      "title": "ActPC-Geom: Towards Scalable Online Neural-Symbolic Learning via Accelerating Active Predictive Coding with Information Geometry & Diverse Cognitive Mechanisms",
      "title_zh": "翻译失败",
      "authors": [
        "Ben Goertzel"
      ],
      "abstract": "This paper introduces ActPC-Geom, an approach to accelerate Active Predictive\nCoding (ActPC) in neural networks by integrating information geometry,\nspecifically using Wasserstein-metric-based methods for measure-dependent\ngradient flows. We propose replacing KL-divergence in ActPC's predictive error\nassessment with the Wasserstein metric, suggesting this may enhance network\nrobustness.\n  To make this computationally feasible, we present strategies including: (1)\nneural approximators for inverse measure-dependent Laplacians, (2) approximate\nkernel PCA embeddings for low-rank approximations feeding into these\napproximators, and (3) compositional hypervector embeddings derived from kPCA\noutputs, with algebra optimized for fuzzy FCA lattices learned through neural\narchitectures analyzing network states.\n  This results in an ActPC architecture capable of real-time online learning\nand integrating continuous (e.g., transformer-like or Hopfield-net-like) and\ndiscrete symbolic ActPC networks, including frameworks like OpenCog Hyperon or\nActPC-Chem for algorithmic chemistry evolution. Shared probabilistic,\nconcept-lattice, and hypervector models enable symbolic-subsymbolic\nintegration.\n  Key features include (1) compositional reasoning via hypervector embeddings\nin transformer-like architectures for tasks like commonsense reasoning, and (2)\nHopfield-net dynamics enabling associative long-term memory and\nattractor-driven cognitive features.\n  We outline how ActPC-Geom combines few-shot learning with online weight\nupdates, enabling deliberative thinking and seamless symbolic-subsymbolic\nreasoning. Ideas from Galois connections are explored for efficient hybrid\nActPC/ActPC-Chem processing. Finally, we propose a specialized HPC design\noptimized for real-time focused attention and deliberative reasoning tailored\nto ActPC-Geom's demands.",
      "tldr_zh": "本论文提出 ActPC-Geom 框架，通过整合信息几何（特别是基于 Wasserstein 度量的梯度流）来加速 Active Predictive Coding (ActPC)，以提升神经网络的在线学习和鲁棒性，并取代传统的 KL-divergence 用于预测误差评估。框架引入策略包括神经逼近器、近似 kernel PCA 嵌入以及组合超向量嵌入，以实现计算效率并整合连续（如 transformer 或 Hopfield-net 动态）和离散符号网络。ActPC-Geom 支持实时在线学习、few-shot 学习以及符号-子符号推理机制，如组合推理和认知特征，最终通过共享模型和优化 HPC 设计，推进可扩展的神经符号学习应用。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04832v1",
      "published_date": "2025-01-08 20:38:02 UTC",
      "updated_date": "2025-01-08 20:38:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:28:38.571897"
    },
    {
      "arxiv_id": "2501.04826v1",
      "title": "Intelligent Gradient Boosting Algorithms for Estimating Strength of Modified Subgrade Soil",
      "title_zh": "智能梯度提升算法用于估计修改路基土的强度",
      "authors": [
        "Ismail B. Mustapha",
        "Muyideen Abdulkareem",
        "Shafaatunnur Hasan",
        "Abideen Ganiyu",
        "Hatem Nabus",
        "Jin Chai Lee"
      ],
      "abstract": "The performance of pavement under loading depends on the strength of the\nsubgrade. However, experimental estimation of properties of pavement strengths\nsuch as California bearing ratio (CBR), unconfined compressive strength (UCS)\nand resistance value (R) are often tedious, time-consuming and costly, thereby\ninspiring a growing interest in machine learning based tools which are simple,\ncheap and fast alternatives. Thus, the potential application of two boosting\ntechniques; categorical boosting (CatBoost) and extreme gradient boosting\n(XGBoost) and support vector regression (SVR), is similarly explored in this\nstudy for estimation of properties of subgrade soil modified with hydrated lime\nactivated rice husk ash (HARSH). Using 121 experimental data samples of varying\nproportions of HARSH, plastic limit, liquid limit, plasticity index, clay\nactivity, optimum moisture content, and maximum dry density as input for CBR,\nUCS and R estimation, four evaluation metrics namely coefficient of\ndetermination (R2), root mean squared error (RMSE), mean absolute error (MAE)\nand mean absolute percentage error (MAPE) are used to evaluate the models'\nperformance. The results indicate that XGBoost outperformed CatBoost and SVR in\nestimating these properties, yielding R2 of 0.9994, 0.9995 and 0.9999 in\nestimating the CBR, UCS and R respectively. Also, SVR outperformed CatBoost in\nestimating the CBR and R with R2 of 0.9997 respectively. On the other hand,\nCatBoost outperformed SVR in estimating the UCS with R2 of 0.9994. Feature\nsensitivity analysis shows that the three machine learning techniques are\nunanimous that increasing HARSH proportion lead to values of the estimated\nproperties respectively. A comparison with previous results also shows\nsuperiority of XGBoost in estimating subgrade properties.",
      "tldr_zh": "本论文探讨了使用智能梯度提升算法，包括 XGBoost、CatBoost 和 SVR，来估计经 Hydrated Lime Activated Rice Husk Ash (HARSH) 修改的路基土强度（如 California Bearing Ratio (CBR)、Unconfined Compressive Strength (UCS) 和 Resistance Value (R)），以克服传统实验方法的耗时和成本问题。基于 121 个实验数据样本，研究比较了三种算法的性能，使用 Coefficient of Determination (R2)、Root Mean Squared Error (RMSE)、Mean Absolute Error (MAE) 和 Mean Absolute Percentage Error (MAPE) 作为评估指标。结果显示，XGBoost 在估计 CBR、UCS 和 R 时表现出色，R2 值分别达到 0.9994、0.9995 和 0.9999，且整体优于 CatBoost 和 SVR；特征敏感性分析还表明，增加 HARSH 比例会提升这些强度属性值，并证明 XGBoost 比以往方法更具优势。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.04826v1",
      "published_date": "2025-01-08 20:26:13 UTC",
      "updated_date": "2025-01-08 20:26:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:28:51.786952"
    },
    {
      "arxiv_id": "2501.04819v1",
      "title": "Planing It by Ear: Convolutional Neural Networks for Acoustic Anomaly Detection in Industrial Wood Planers",
      "title_zh": "翻译失败",
      "authors": [
        "Anthony Deschênes",
        "Rémi Georges",
        "Cem Subakan",
        "Bruna Ugulino",
        "Antoine Henry",
        "Michael Morin"
      ],
      "abstract": "In recent years, the wood product industry has been facing a skilled labor\nshortage. The result is more frequent sudden failures, resulting in additional\ncosts for these companies already operating in a very competitive market.\nMoreover, sawmills are challenging environments for machinery and sensors.\nGiven that experienced machine operators may be able to diagnose defects or\nmalfunctions, one possible way of assisting novice operators is through\nacoustic monitoring. As a step towards the automation of wood-processing\nequipment and decision support systems for machine operators, in this paper, we\nexplore using a deep convolutional autoencoder for acoustic anomaly detection\nof wood planers on a new real-life dataset. Specifically, our convolutional\nautoencoder with skip connections (Skip-CAE) and our Skip-CAE transformer\noutperform the DCASE autoencoder baseline, one-class SVM, isolation forest and\na published convolutional autoencoder architecture, respectively obtaining an\narea under the ROC curve of 0.846 and 0.875 on a dataset of real-factory planer\nsounds. Moreover, we show that adding skip connections and attention mechanism\nunder the form of a transformer encoder-decoder helps to further improve the\nanomaly detection capabilities.",
      "tldr_zh": "该论文针对木制品行业劳动力短缺导致的机器故障问题，提出使用声学监测辅助新手操作员，通过Convolutional Neural Networks进行工业木刨机的异常检测。研究开发了基于深度卷积自编码器的Skip-CAE和Skip-CAE transformer模型，并在真实工厂数据集上进行测试。结果显示，Skip-CAE和Skip-CAE transformer的AUC分数分别为0.846和0.875，优于DCASE autoencoder、one-class SVM、isolation forest等基线模型。总之，该方法通过添加skip connections和注意力机制，显著提升了异常检测的准确性和可靠性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04819v1",
      "published_date": "2025-01-08 20:17:18 UTC",
      "updated_date": "2025-01-08 20:17:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:29:02.984872"
    },
    {
      "arxiv_id": "2501.04817v1",
      "title": "Decentralised Resource Sharing in TinyML: Wireless Bilayer Gossip Parallel SGD for Collaborative Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyuan Bao",
        "Eiman Kanjo",
        "Soumya Banerjee",
        "Hasib-Al Rashid",
        "Tinoosh Mohsenin"
      ],
      "abstract": "With the growing computational capabilities of microcontroller units (MCUs),\nedge devices can now support machine learning models. However, deploying\ndecentralised federated learning (DFL) on such devices presents key challenges,\nincluding intermittent connectivity, limited communication range, and dynamic\nnetwork topologies. This paper proposes a novel framework, bilayer Gossip\nDecentralised Parallel Stochastic Gradient Descent (GD PSGD), designed to\naddress these issues in resource-constrained environments. The framework\nincorporates a hierarchical communication structure using Distributed Kmeans\n(DKmeans) clustering for geographic grouping and a gossip protocol for\nefficient model aggregation across two layers: intra-cluster and inter-cluster.\nWe evaluate the framework's performance against the Centralised Federated\nLearning (CFL) baseline using the MCUNet model on the CIFAR-10 dataset under\nIID and Non-IID conditions. Results demonstrate that the proposed method\nachieves comparable accuracy to CFL on IID datasets, requiring only 1.8\nadditional rounds for convergence. On Non-IID datasets, the accuracy loss\nremains under 8\\% for moderate data imbalance. These findings highlight the\nframework's potential to support scalable and privacy-preserving learning on\nedge devices with minimal performance trade-offs.",
      "tldr_zh": "该论文提出了一种名为 GD PSGD 的框架，用于 TinyML 中的去中心化资源共享，旨在解决边缘设备在部署 DFL 时面临的间歇性连接、有限通信范围和动态网络拓扑等问题。该框架采用 bilayer 结构，包括使用 Distributed Kmeans (DKmeans) 聚类进行地理分组，以及 gossip 协议在 intra-cluster 和 inter-cluster 层级进行高效模型聚合。通过在 CIFAR-10 数据集上使用 MCUNet 模型进行评估，结果显示 GD PSGD 在 IID 数据条件下与 CFL 基线准确率相当，仅需额外 1.8 轮收敛；在 Non-IID 数据条件下，准确率损失小于 8% 对于中等数据不平衡。这些发现证明了该框架在支持可扩展、隐私保护的边缘设备学习方面具有潜力，同时最小化性能权衡。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04817v1",
      "published_date": "2025-01-08 20:14:07 UTC",
      "updated_date": "2025-01-08 20:14:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:29:15.276949"
    },
    {
      "arxiv_id": "2501.06248v2",
      "title": "Utility-inspired Reward Transformations Improve Reinforcement Learning Training of Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Roberto-Rafael Maura-Rivero",
        "Chirag Nagpal",
        "Roma Patel",
        "Francesco Visin"
      ],
      "abstract": "Current methods that train large language models (LLMs) with reinforcement\nlearning feedback, often resort to averaging outputs of multiple rewards\nfunctions during training. This overlooks crucial aspects of individual reward\ndimensions and inter-reward dependencies that can lead to sub-optimal outcomes\nin generations. In this work, we show how linear aggregation of rewards\nexhibits some vulnerabilities that can lead to undesired properties of\ngenerated text. We then propose a transformation of reward functions inspired\nby economic theory of utility functions (specifically Inada conditions), that\nenhances sensitivity to low reward values while diminishing sensitivity to\nalready high values. We compare our approach to the existing baseline methods\nthat linearly aggregate rewards and show how the Inada-inspired reward feedback\nis superior to traditional weighted averaging. We quantitatively and\nqualitatively analyse the difference in the methods, and see that models\ntrained with Inada-transformations score as more helpful while being less\nharmful.",
      "tldr_zh": "本文研究了使用强化学习训练大型语言模型(LLMs)时，线性聚合奖励函数的局限性，这些方法忽略了单个奖励维度的依赖性，可能导致生成的文本次优或具有不良属性。作者提出了一种基于经济理论中Inada conditions的奖励转换方法，提高了对低奖励值的敏感性，同时降低了对高奖励值的敏感性，以优化训练过程。与传统加权平均基线相比，这种方法使模型在帮助性方面得分更高，同时减少了危害性，通过定量和定性分析得到证实。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06248v2",
      "published_date": "2025-01-08 19:03:17 UTC",
      "updated_date": "2025-02-25 18:04:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:29:26.718940"
    },
    {
      "arxiv_id": "2501.04700v1",
      "title": "Planarian Neural Networks: Evolutionary Patterns from Basic Bilateria Shaping Modern Artificial Neural Network Architectures",
      "title_zh": "翻译失败",
      "authors": [
        "Ziyuan Huang",
        "Mark Newman",
        "Maria Vaida",
        "Srikar Bellur",
        "Roozbeh Sadeghian",
        "Andrew Siu",
        "Hui Wang",
        "Kevin Huggins"
      ],
      "abstract": "This study examined the viability of enhancing the prediction accuracy of\nartificial neural networks (ANNs) in image classification tasks by developing\nANNs with evolution patterns similar to those of biological neural networks.\nResNet is a widely used family of neural networks with both deep and wide\nvariants; therefore, it was selected as the base model for our investigation.\nThe aim of this study is to improve the image classification performance of\nANNs via a novel approach inspired by the biological nervous system\narchitecture of planarians, which comprises a brain and two nerve cords. We\nbelieve that the unique neural architecture of planarians offers valuable\ninsights into the performance enhancement of ANNs. The proposed planarian\nneural architecture-based neural network was evaluated on the CIFAR-10 and\nCIFAR-100 datasets. Our results indicate that the proposed method exhibits\nhigher prediction accuracy than the baseline neural network models in image\nclassification tasks. These findings demonstrate the significant potential of\nbiologically inspired neural network architectures in improving the performance\nof ANNs in a wide range of applications.",
      "tldr_zh": "这篇论文探讨了通过模仿扁虫（planarians）的生物神经网络进化模式来提升人工神经网络（ANNs）在图像分类任务中的预测准确性。研究以 ResNet 作为基础模型，开发了一种新型神经网络架构，灵感来源于扁虫的神经系统，包括 brain 和 two nerve cords。实验在 CIFAR-10 和 CIFAR-100 数据集上进行，结果显示该方法比基线模型表现出更高的预测准确率。这些发现证明了生物启发神经网络架构在广泛应用中提升 ANNs 性能的巨大潜力。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "68T07"
      ],
      "primary_category": "cs.NE",
      "comment": "11 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.04700v1",
      "published_date": "2025-01-08 18:59:36 UTC",
      "updated_date": "2025-01-08 18:59:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:29:39.099488"
    },
    {
      "arxiv_id": "2501.04697v2",
      "title": "Grokking at the Edge of Numerical Stability",
      "title_zh": "翻译失败",
      "authors": [
        "Lucas Prieto",
        "Melih Barsbey",
        "Pedro A. M. Mediano",
        "Tolga Birdal"
      ],
      "abstract": "Grokking, the sudden generalization that occurs after prolonged overfitting,\nis a surprising phenomenon challenging our understanding of deep learning.\nAlthough significant progress has been made in understanding grokking, the\nreasons behind the delayed generalization and its dependence on regularization\nremain unclear. In this work, we argue that without regularization, grokking\ntasks push models to the edge of numerical stability, introducing floating\npoint errors in the Softmax function, which we refer to as Softmax Collapse\n(SC). We demonstrate that SC prevents grokking and that mitigating SC enables\ngrokking without regularization. Investigating the root cause of SC, we find\nthat beyond the point of overfitting, the gradients strongly align with what we\ncall the na\\\"ive loss minimization (NLM) direction. This component of the\ngradient does not alter the model's predictions but decreases the loss by\nscaling the logits, typically by scaling the weights along their current\ndirection. We show that this scaling of the logits explains the delay in\ngeneralization characteristic of grokking and eventually leads to SC, halting\nfurther learning. To validate our hypotheses, we introduce two key\ncontributions that address the challenges in grokking tasks: StableMax, a new\nactivation function that prevents SC and enables grokking without\nregularization, and $\\perp$Grad, a training algorithm that promotes quick\ngeneralization in grokking tasks by preventing NLM altogether. These\ncontributions provide new insights into grokking, elucidating its delayed\ngeneralization, reliance on regularization, and the effectiveness of existing\ngrokking-inducing methods. Code for this paper is available at\nhttps://github.com/LucasPrietoAl/grokking-at-the-edge-of-numerical-stability.",
      "tldr_zh": "本研究探讨了 Grokking 现象，即深度学习模型在过度拟合后突然泛化的过程，强调了其延迟泛化和对正则化的依赖问题。作者发现，没有正则化时，Grokking 任务会导致 Softmax Collapse (SC)，一种由浮点错误引起的数值不稳定性，从而阻止泛化；同时，梯度与 naive loss minimization (NLM) 方向对齐，通过缩放 logits 进一步延迟学习。针对这些问题，研究引入了 StableMax（一个新激活函数，防止 SC 并实现无正则化 Grokking）和 ⊥Grad（一个训练算法，消除 NLM 以促进快速泛化）。这些贡献为理解 Grokking 的机制提供了新见解，包括其对正则化的依赖和现有方法的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04697v2",
      "published_date": "2025-01-08 18:58:48 UTC",
      "updated_date": "2025-05-19 11:02:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:29:51.580869"
    },
    {
      "arxiv_id": "2501.04694v1",
      "title": "EpiCoder: Encompassing Diversity and Complexity in Code Generation",
      "title_zh": "EpiCoder: 涵盖代码生成中的多样性和复杂性",
      "authors": [
        "Yaoxiang Wang",
        "Haoling Li",
        "Xin Zhang",
        "Jie Wu",
        "Xiao Liu",
        "Wenxiang Hu",
        "Zhongxin Guo",
        "Yangyu Huang",
        "Ying Xin",
        "Yujiu Yang",
        "Jinsong Su",
        "Qi Chen",
        "Scarlett Li"
      ],
      "abstract": "Effective instruction tuning is indispensable for optimizing code LLMs,\naligning model behavior with user expectations and enhancing model performance\nin real-world applications. However, most existing methods focus on code\nsnippets, which are limited to specific functionalities and rigid structures,\nrestricting the complexity and diversity of the synthesized data. To address\nthese limitations, we introduce a novel feature tree-based synthesis framework\ninspired by Abstract Syntax Trees (AST). Unlike AST, which captures syntactic\nstructure of code, our framework models semantic relationships between code\nelements, enabling the generation of more nuanced and diverse data. The feature\ntree is constructed from raw data and refined iteratively to increase the\nquantity and diversity of the extracted features. This process enables the\nidentification of more complex patterns and relationships within the code. By\nsampling subtrees with controlled depth and breadth, our framework allows\nprecise adjustments to the complexity of the generated code, supporting a wide\nrange of tasks from simple function-level operations to intricate multi-file\nscenarios. We fine-tuned widely-used base models to create the EpiCoder series,\nachieving state-of-the-art performance at both the function and file levels\nacross multiple benchmarks. Notably, empirical evidence indicates that our\napproach shows significant potential in synthesizing highly complex\nrepository-level code data. Further analysis elucidates the merits of this\napproach by rigorously assessing data complexity and diversity through software\nengineering principles and LLM-as-a-judge method.",
      "tldr_zh": "本文提出 EpiCoder，一种基于特征树（feature tree）的代码生成框架，受 Abstract Syntax Trees (AST) 启发，但专注于代码元素的语义关系，以解决现有方法在数据复杂性和多样性方面的局限性。该框架通过从原始数据构建并迭代提炼特征树，并利用子树采样控制代码深度和广度，支持从简单函数到复杂多文件场景的生成。实验结果显示，微调后的 EpiCoder 系列模型在多个基准上实现了 state-of-the-art 性能，并在合成仓库级代码数据方面展现出显著潜力，通过软件工程原则和 LLM-as-a-judge 方法验证了其优势。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "40 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.04694v1",
      "published_date": "2025-01-08 18:58:15 UTC",
      "updated_date": "2025-01-08 18:58:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:30:03.274001"
    },
    {
      "arxiv_id": "2501.04693v3",
      "title": "Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding",
      "title_zh": "翻译失败",
      "authors": [
        "Joshua Jones",
        "Oier Mees",
        "Carmelo Sferrazza",
        "Kyle Stachowicz",
        "Pieter Abbeel",
        "Sergey Levine"
      ],
      "abstract": "Interacting with the world is a multi-sensory experience: achieving effective\ngeneral-purpose interaction requires making use of all available modalities --\nincluding vision, touch, and audio -- to fill in gaps from partial observation.\nFor example, when vision is occluded reaching into a bag, a robot should rely\non its senses of touch and sound. However, state-of-the-art generalist robot\npolicies are typically trained on large datasets to predict robot actions\nsolely from visual and proprioceptive observations. In this work, we propose\nFuSe, a novel approach that enables finetuning visuomotor generalist policies\non heterogeneous sensor modalities for which large datasets are not readily\navailable by leveraging natural language as a common cross-modal grounding. We\ncombine a multimodal contrastive loss with a sensory-grounded language\ngeneration loss to encode high-level semantics. In the context of robot\nmanipulation, we show that FuSe enables performing challenging tasks that\nrequire reasoning jointly over modalities such as vision, touch, and sound in a\nzero-shot setting, such as multimodal prompting, compositional cross-modal\nprompting, and descriptions of objects it interacts with. We show that the same\nrecipe is applicable to widely different generalist policies, including both\ndiffusion-based generalist policies and large vision-language-action (VLA)\nmodels. Extensive experiments in the real world show that FuSeis able to\nincrease success rates by over 20% compared to all considered baselines.",
      "tldr_zh": "本论文提出 FuSe 方法，通过语言 grounding 微调通用机器人策略，以整合异构传感器（如视觉、触觉和音频），解决部分观察场景下的交互挑战。FuSe 结合多模态对比损失和感官接地语言生成损失，编码高层语义，实现对多种模态的联合推理。实验结果显示，该方法在真实世界机器人操作任务中，比基线模型成功率提高超过 20%，并支持零样本设置下的多模态提示和组合任务。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04693v3",
      "published_date": "2025-01-08 18:57:33 UTC",
      "updated_date": "2025-01-14 22:28:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:30:15.391157"
    },
    {
      "arxiv_id": "2501.04686v4",
      "title": "URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics",
      "title_zh": "URSA: 多模态数学中链式思维推理",
      "authors": [
        "Ruilin Luo",
        "Zhuofan Zheng",
        "Yifan Wang",
        "Yiyao Yu",
        "Xinzhe Ni",
        "Zicheng Lin",
        "Jin Zeng",
        "Yujiu Yang"
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning is widely used to enhance the mathematical\nreasoning capabilities of large language models (LLMs). The introduction of\nprocess supervision for CoT trajectories has sparked discussions on improving\ntest-time scaling, thereby unlocking the System 2-style thinking capabilities\nof these models. However, in multimodal mathematical reasoning, the scarcity of\nhigh-quality CoT training data has hindered existing models from achieving both\ndeliberate reasoning and fine-grained verification. In this work, we propose a\nnovel framework that introduces System 2-style thinking to multimodal\nmathematical reasoning. We introduce a three-module CoT data synthesis process\nthat integrates CoT distillation, trajectory-format rewriting, and format\nunification. This process generates MMathCoT-1M, a high-quality CoT reasoning\ninstruction fine-tuning dataset. Furthermore, we implement a dual-view\ntrajectory labeling automation that targets both visual grounding fidelity and\ndeductive chain validity, resulting in the DualMath-1.1M dataset. The URSA-8B\nmodel, trained on MMathCoT-1M, achieves new state-of-the-art (SOTA) performance\namong similarly sized multimodal LLMs on six popular reasoning benchmarks.\nTraining URSA-8B further on the DualMath-1.1M dataset yields URSA-RM-8B, a\nverifier that enhances URSA-8B's test-time performance and surpasses strong\nclosed-source multimodal MLLMs like GPT-4o. The model weights, training data,\nand code have been open-sourced: https://github.com/URSA-MATH/URSA-MATH.",
      "tldr_zh": "该论文提出 URSA 框架，用于理解和验证多模态数学中的 Chain-of-Thought (CoT) 推理，解决现有模型在高品质 CoT 训练数据短缺下的推理和验证问题。框架包括一个三模块数据合成过程（CoT distillation、trajectory-format rewriting 和 format unification），生成 MMathCoT-1M 数据集，以及双视图轨迹标记自动化（针对 visual grounding fidelity 和 deductive chain validity），产生 DualMath-1.1M 数据集。训练出的 URSA-8B 模型在六个流行推理基准上达到新状态-of-the-art (SOTA) 性能，而进一步训练的 URSA-RM-8B 验证器提升了测试性能，并超越了闭源模型如 GPT-4o。模型权重、训练数据和代码已开源。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Fix typos and add results. 27 pages, 11 tables, 17 figures. Models,\n  training data and code have been open-sourced. Project url:\n  https://ursa-math.github.io",
      "pdf_url": "http://arxiv.org/pdf/2501.04686v4",
      "published_date": "2025-01-08 18:49:41 UTC",
      "updated_date": "2025-02-24 07:32:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:30:30.228727"
    },
    {
      "arxiv_id": "2501.04682v1",
      "title": "Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought",
      "title_zh": "翻译失败",
      "authors": [
        "Violet Xiang",
        "Charlie Snell",
        "Kanishk Gandhi",
        "Alon Albalak",
        "Anikait Singh",
        "Chase Blagden",
        "Duy Phung",
        "Rafael Rafailov",
        "Nathan Lile",
        "Dakota Mahan",
        "Louis Castricato",
        "Jan-Philipp Franken",
        "Nick Haber",
        "Chelsea Finn"
      ],
      "abstract": "We propose a novel framework, Meta Chain-of-Thought (Meta-CoT), which extends\ntraditional Chain-of-Thought (CoT) by explicitly modeling the underlying\nreasoning required to arrive at a particular CoT. We present empirical evidence\nfrom state-of-the-art models exhibiting behaviors consistent with in-context\nsearch, and explore methods for producing Meta-CoT via process supervision,\nsynthetic data generation, and search algorithms. Finally, we outline a\nconcrete pipeline for training a model to produce Meta-CoTs, incorporating\ninstruction tuning with linearized search traces and reinforcement learning\npost-training. Finally, we discuss open research questions, including scaling\nlaws, verifier roles, and the potential for discovering novel reasoning\nalgorithms. This work provides a theoretical and practical roadmap to enable\nMeta-CoT in LLMs, paving the way for more powerful and human-like reasoning in\nartificial intelligence.",
      "tldr_zh": "我们提出 Meta Chain-of-Thought (Meta-CoT) 框架，以扩展传统的 Chain-of-Thought (CoT)，通过显式建模底层推理来提升大型语言模型 (LLMs) 的 System 2 推理能力，即更像人类的深思熟虑思考。研究提供了来自最先进模型的实证证据，展示了与 in-context search 一致的行为，并探索了生成 Meta-CoT 的方法，包括过程监督、合成数据生成和搜索算法。论文概述了一个训练管道，结合指令微调（instruction tuning）与线性化搜索痕迹，以及强化学习后训练（reinforcement learning post-training）。最后，讨论了开放研究问题，如 scaling laws、verifier roles 和发现新推理算法的潜力，为 LLMs 实现更强大的人类-like 推理提供理论和实践路线图。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04682v1",
      "published_date": "2025-01-08 18:42:48 UTC",
      "updated_date": "2025-01-08 18:42:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:30:39.872105"
    },
    {
      "arxiv_id": "2501.04765v2",
      "title": "TREAD: Token Routing for Efficient Architecture-agnostic Diffusion Training",
      "title_zh": "TREAD：用于高效架构无关扩散训练的令牌路由",
      "authors": [
        "Felix Krause",
        "Timy Phan",
        "Ming Gui",
        "Stefan Andreas Baumann",
        "Vincent Tao Hu",
        "Björn Ommer"
      ],
      "abstract": "Diffusion models have emerged as the mainstream approach for visual\ngeneration. However, these models typically suffer from sample inefficiency and\nhigh training costs. Consequently, methods for efficient finetuning, inference\nand personalization were quickly adopted by the community. However, training\nthese models in the first place remains very costly. While several recent\napproaches - including masking, distillation, and architectural modifications -\nhave been proposed to improve training efficiency, each of these methods comes\nwith a tradeoff: they achieve enhanced performance at the expense of increased\ncomputational cost or vice versa. In contrast, this work aims to improve\ntraining efficiency as well as generative performance at the same time through\nroutes that act as a transport mechanism for randomly selected tokens from\nearly layers to deeper layers of the model. Our method is not limited to the\ncommon transformer-based model - it can also be applied to state-space models\nand achieves this without architectural modifications or additional parameters.\nFinally, we show that TREAD reduces computational cost and simultaneously\nboosts model performance on the standard ImageNet-256 benchmark in\nclass-conditional synthesis. Both of these benefits multiply to a convergence\nspeedup of 14x at 400K training iterations compared to DiT and 37x compared to\nthe best benchmark performance of DiT at 7M training iterations. Furthermore,\nwe achieve a competitive FID of 2.09 in a guided and 3.93 in an unguided\nsetting, which improves upon the DiT, without architectural changes.",
      "tldr_zh": "本文提出 TREAD 方法，通过 token routing 机制将早期层的随机选定 tokens 传输到更深层，从而同时提升扩散模型（Diffusion models）的训练效率和生成性能。该方法不依赖特定架构（如 transformer-based 或 state-space models），无需修改模型结构或添加参数。在 ImageNet-256 基准测试中，TREAD 比 DiT 实现了 14x 的收敛加速，并在 400K 迭代时达到 37x 的整体性能提升，取得了 FID 2.09（guided）和 3.93（unguided）的优异成绩。该框架为高效的视觉生成训练提供了新的途径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04765v2",
      "published_date": "2025-01-08 18:38:25 UTC",
      "updated_date": "2025-03-27 14:42:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:30:51.034762"
    },
    {
      "arxiv_id": "2501.04675v1",
      "title": "Enhancing Financial VQA in Vision Language Models using Intermediate Structured Representations",
      "title_zh": "使用中间结构",
      "authors": [
        "Archita Srivastava",
        "Abhas Kumar",
        "Rajesh Kumar",
        "Prabhakar Srinivasan"
      ],
      "abstract": "Chart interpretation is crucial for visual data analysis, but accurately\nextracting information from charts poses significant challenges for automated\nmodels. This study investigates the fine-tuning of DEPLOT, a modality\nconversion module that translates the image of a plot or chart to a linearized\ntable, on a custom dataset of 50,000 bar charts. The dataset comprises simple,\nstacked, and grouped bar charts, targeting the unique structural features of\nthese visualizations. The finetuned DEPLOT model is evaluated against its base\nversion using a test set of 1,000 images and two metrics: Relative Mapping\nSimilarity (RMS), which measures categorical mapping accuracy, and Relative\nNumber Set Similarity (RNSS), which evaluates numerical interpretation\naccuracy. To further explore the reasoning capabilities of large language\nmodels (LLMs), we curate an additional set of 100 bar chart images paired with\nquestion answer sets. Our findings demonstrate that providing a structured\nintermediate table alongside the image significantly enhances LLM reasoning\nperformance compared to direct image queries.",
      "tldr_zh": "本研究旨在提升视觉语言模型(VQA)在金融图表解释中的性能，通过使用中间结构化表示来处理图像数据。研究团队细调了DEPLOT模型，将图表图像转换为线性化表格，并基于一个包含50,000个条形图（包括简单、堆叠和分组类型）的自定义数据集进行训练。评估结果显示，细调后的DEPLOT在1,000个测试图像上，使用Relative Mapping Similarity (RMS)和Relative Number Set Similarity (RNSS)指标，显著提高了类别映射和数字解释的准确性。此外，提供结构化表格作为中间表示，能显著增强大语言模型(LLMs)的推理能力，相比直接图像查询表现出色。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04675v1",
      "published_date": "2025-01-08 18:33:17 UTC",
      "updated_date": "2025-01-08 18:33:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:31:02.302570"
    },
    {
      "arxiv_id": "2501.04671v2",
      "title": "Retrieval-Based Interleaved Visual Chain-of-Thought in Real-World Driving Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "Charles Corbière",
        "Simon Roburin",
        "Syrielle Montariol",
        "Antoine Bosselut",
        "Alexandre Alahi"
      ],
      "abstract": "While chain-of-thought (CoT) prompting improves reasoning in large language\nmodels, its effectiveness in vision-language models (VLMs) remains limited due\nto over-reliance on textual cues and memorized knowledge. To investigate the\nvisual reasoning capabilities of VLMs in complex real-world scenarios, we\nintroduce DrivingVQA, a visual question answering dataset derived from driving\ntheory exams, which contains 3,931 multiple-choice problems with expert-written\nexplanations and grounded entities relevant to the reasoning process.\nLeveraging this dataset, we propose RIV-CoT, a Retrieval-Based Interleaved\nVisual Chain-of-Thought method that enables VLMs to reason using visual crops\ncorresponding to these relevant entities. Our experiments demonstrate that\nRIV-CoT improves answer accuracy by 3.1% and reasoning accuracy by 4.6% over\nvanilla CoT prompting. Furthermore, we demonstrate that our method effectively\nscales to the larger A-OKVQA reasoning dataset by leveraging automatically\ngenerated pseudo-labels, outperforming CoT prompting.",
      "tldr_zh": "该研究探讨了Chain-of-Thought (CoT)提示在视觉语言模型(VLMs)中的局限性，即过度依赖文本线索和记忆知识，并为此引入DrivingVQA数据集，该数据集包含3,931个驾驶理论考试的多选问题、专家解释和相关实体。作者提出RIV-CoT（Retrieval-Based Interleaved Visual Chain-of-Thought）方法，通过检索和使用视觉剪裁来增强VLMs的视觉推理能力。实验结果显示，RIV-CoT在DrivingVQA上比传统CoT提示提高了3.1%的答案准确率和4.6%的推理准确率，并成功扩展到A-OKVQA数据集，使用自动生成的伪标签取得了更好的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://vita-epfl.github.io/DrivingVQA",
      "pdf_url": "http://arxiv.org/pdf/2501.04671v2",
      "published_date": "2025-01-08 18:31:16 UTC",
      "updated_date": "2025-04-08 17:09:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:31:14.448203"
    },
    {
      "arxiv_id": "2501.04661v1",
      "title": "Assessing Language Comprehension in Large Language Models Using Construction Grammar",
      "title_zh": "翻译失败",
      "authors": [
        "Wesley Scivetti",
        "Melissa Torgbi",
        "Austin Blodgett",
        "Mollie Shichman",
        "Taylor Hudson",
        "Claire Bonial",
        "Harish Tayyar Madabushi"
      ],
      "abstract": "Large Language Models, despite their significant capabilities, are known to\nfail in surprising and unpredictable ways. Evaluating their true\n`understanding' of language is particularly challenging due to the extensive\nweb-scale data they are trained on. Therefore, we construct an evaluation to\nsystematically assess natural language understanding (NLU) in LLMs by\nleveraging Construction Grammar (CxG), which provides insights into the meaning\ncaptured by linguistic elements known as constructions (Cxns). CxG is\nwell-suited for this purpose because provides a theoretical basis to construct\ntargeted evaluation sets. These datasets are carefully constructed to include\nexamples which are unlikely to appear in pre-training data, yet intuitive and\neasy for humans to understand, enabling a more targeted and reliable\nassessment. Our experiments focus on downstream natural language inference and\nreasoning tasks by comparing LLMs' understanding of the underlying meanings\ncommunicated through 8 unique Cxns with that of humans. The results show that\nwhile LLMs demonstrate some knowledge of constructional information, even the\nlatest models including GPT-o1 struggle with abstract meanings conveyed by\nthese Cxns, as demonstrated in cases where test sentences are dissimilar to\ntheir pre-training data. We argue that such cases provide a more accurate test\nof true language understanding, highlighting key limitations in LLMs' semantic\ncapabilities. We make our novel dataset and associated experimental data\nincluding prompts and model responses publicly available.",
      "tldr_zh": "本研究使用 Construction Grammar (CxG) 提出了一种系统评估 Large Language Models (LLMs) 自然语言理解 (NLU) 的方法，通过构建不易出现在预训练数据中的目标数据集，来测试 LLMs 对 8 个独特 constructions (Cxns) 的理解。实验聚焦于下游自然语言推理和推理任务，与人类表现进行比较，结果显示即使是 GPT-o1 等最新模型，在处理这些 Cxns 传达的抽象含义时，尤其当测试句子与预训练数据不相似时，表现挣扎。作者强调，此评估更准确地揭示了 LLMs 在语义能力上的关键局限性，并公开了数据集和实验数据以促进进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04661v1",
      "published_date": "2025-01-08 18:15:10 UTC",
      "updated_date": "2025-01-08 18:15:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:31:27.214875"
    },
    {
      "arxiv_id": "2501.06247v1",
      "title": "A Survey on Algorithmic Developments in Optimal Transport Problem with Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Sina Moradi"
      ],
      "abstract": "Optimal Transport (OT) has established itself as a robust framework for\nquantifying differences between distributions, with applications that span\nfields such as machine learning, data science, and computer vision. This paper\noffers a detailed examination of the OT problem, beginning with its theoretical\nfoundations, including the classical formulations of Monge and Kantorovich and\ntheir extensions to modern computational techniques. It explores cutting-edge\nalgorithms, including Sinkhorn iterations, primal-dual strategies, and\nreduction-based approaches, emphasizing their efficiency and scalability in\naddressing high-dimensional problems. The paper also highlights emerging\ntrends, such as integrating OT into machine learning frameworks, the\ndevelopment of novel problem variants, and ongoing theoretical advancements.\nApplications of OT are presented across a range of domains, with particular\nattention to its innovative application in time series data analysis via\nOptimal Transport Warping (OTW), a robust alternative to methods like Dynamic\nTime Warping. Despite the significant progress made, challenges related to\nscalability, robustness, and ethical considerations remain, necessitating\nfurther research. The paper underscores OT's potential to bridge theoretical\ndepth and practical utility, fostering impactful advancements across diverse\ndisciplines.",
      "tldr_zh": "这篇调查论文综述了 Optimal Transport (OT) 在量化分布差异方面的算法发展及其应用，从 Monge 和 Kantorovich 的经典理论基础入手，扩展到现代计算技术。论文重点探讨了高效算法如 Sinkhorn iterations、primal-dual strategies 和 reduction-based approaches，这些方法提升了 OT 在高维问题的可扩展性和效率。OT 被广泛应用于机器学习、数据科学和计算机视觉等领域，例如通过 Optimal Transport Warping (OTW) 在时间序列分析中作为 Dynamic Time Warping 的鲁棒替代；尽管取得了进展，但仍面临可扩展性、鲁棒性和伦理挑战，需要进一步研究。总的来说，该框架桥接了理论深度与实际效用，促进了跨学科创新。",
      "categories": [
        "cs.DS",
        "cs.AI",
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "primary_category": "cs.DS",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06247v1",
      "published_date": "2025-01-08 18:06:30 UTC",
      "updated_date": "2025-01-08 18:06:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:31:38.760260"
    },
    {
      "arxiv_id": "2501.04635v2",
      "title": "Knowledge Retrieval Based on Generative AI",
      "title_zh": "基于生成式 AI 的知识检索",
      "authors": [
        "Te-Lun Yang",
        "Jyi-Shane Liu",
        "Yuen-Hsien Tseng",
        "Jyh-Shing Roger Jang"
      ],
      "abstract": "This study develops a question-answering system based on Retrieval-Augmented\nGeneration (RAG) using Chinese Wikipedia and Lawbank as retrieval sources.\nUsing TTQA and TMMLU+ as evaluation datasets, the system employs BGE-M3 for\ndense vector retrieval to obtain highly relevant search results and\nBGE-reranker to reorder these results based on query relevance. The most\npertinent retrieval outcomes serve as reference knowledge for a Large Language\nModel (LLM), enhancing its ability to answer questions and establishing a\nknowledge retrieval system grounded in generative AI. The system's\neffectiveness is assessed through a two-stage evaluation: automatic and\nassisted performance evaluations. The automatic evaluation calculates accuracy\nby comparing the model's auto-generated labels with ground truth answers,\nmeasuring performance under standardized conditions without human intervention.\nThe assisted performance evaluation involves 20 finance-related multiple-choice\nquestions answered by 20 participants without financial backgrounds. Initially,\nparticipants answer independently. Later, they receive system-generated\nreference information to assist in answering, examining whether the system\nimproves accuracy when assistance is provided. The main contributions of this\nresearch are: (1) Enhanced LLM Capability: By integrating BGE-M3 and\nBGE-reranker, the system retrieves and reorders highly relevant results,\nreduces hallucinations, and dynamically accesses authorized or public knowledge\nsources. (2) Improved Data Privacy: A customized RAG architecture enables local\noperation of the LLM, eliminating the need to send private data to external\nservers. This approach enhances data security, reduces reliance on commercial\nservices, lowers operational costs, and mitigates privacy risks.",
      "tldr_zh": "这篇论文开发了一个基于 Retrieval-Augmented Generation (RAG) 的问答系统，使用 BGE-M3 进行密集向量检索和 BGE-reranker 重新排序，从中文 Wikipedia 和 Lawbank 等来源获取相关知识，以增强 Large Language Model (LLM) 的回答准确性和减少幻觉。系统采用 TTQA 和 TMMLU+ 数据集进行两阶段评估：自动评估通过比较生成标签与真实答案计算准确率，而辅助性能评估让 20 名非金融背景参与者先独立后使用系统辅助回答 20 个金融问题，考察辅助效果。研究的主要贡献是提升 LLM 能力、实现动态知识访问，以及通过本地化 RAG 架构改善数据隐私、降低运营成本并减少对外部服务器的依赖。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "8 pages, 13 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2501.04635v2",
      "published_date": "2025-01-08 17:29:46 UTC",
      "updated_date": "2025-01-16 09:30:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:31:50.992823"
    },
    {
      "arxiv_id": "2501.06246v1",
      "title": "A partition cover approach to tokenization",
      "title_zh": "翻译失败",
      "authors": [
        "Jia Peng Lim",
        "Davin Choo",
        "Hady W. Lauw"
      ],
      "abstract": "Tokenization is the process of encoding strings into tokens from a fixed\nvocabulary of size $k$ and is widely utilized in Natural Language Processing\napplications. The leading tokenization algorithm today is Byte Pair Encoding\n(BPE), which formulates the tokenization problem as a compression problem and\ntackles it by performing sequences of merges. In this work, we formulate\ntokenization as an optimization objective, show that it is NP-hard via a simple\nreduction from vertex cover, and propose a polynomial-time greedy algorithm\nGreedTok. Our formulation naturally relaxes to the well-studied weighted\nmaximum coverage problem which has a simple $(1 - 1/e)$-approximation algorithm\nGreedWMC. Through empirical evaluations on real-world corpora, we show that\nGreedTok outperforms BPE, while achieving a comparable objective score as\nGreedWMC (which could have achieved a higher score due to relaxation).",
      "tldr_zh": "本文将 tokenization（标记化）问题表述为一个优化目标，证明其为 NP-hard，并通过从 vertex cover 的简单归约来证实。作者提出一个多项式时间的贪婪算法 GreedTok，该算法基于 weighted maximum coverage 问题，并利用其 (1 - 1/e)-近似算法 GreedWMC 进行优化。在真实语料上的实证评估显示，GreedTok 优于传统 Byte Pair Encoding (BPE) 方法，且在目标分数上与 GreedWMC 相当。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DS"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06246v1",
      "published_date": "2025-01-08 17:07:07 UTC",
      "updated_date": "2025-01-08 17:07:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:32:02.334029"
    },
    {
      "arxiv_id": "2501.06244v1",
      "title": "Microservice Deployment in Space Computing Power Networks via Robust Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhiyong Yu",
        "Yuning Jiang",
        "Xin Liu",
        "Yuanming Shi",
        "Chunxiao Jiang",
        "Linling Kuang"
      ],
      "abstract": "With the growing demand for Earth observation, it is important to provide\nreliable real-time remote sensing inference services to meet the low-latency\nrequirements. The Space Computing Power Network (Space-CPN) offers a promising\nsolution by providing onboard computing and extensive coverage capabilities for\nreal-time inference. This paper presents a remote sensing artificial\nintelligence applications deployment framework designed for Low Earth Orbit\nsatellite constellations to achieve real-time inference performance. The\nframework employs the microservice architecture, decomposing monolithic\ninference tasks into reusable, independent modules to address high latency and\nresource heterogeneity. This distributed approach enables optimized\nmicroservice deployment, minimizing resource utilization while meeting quality\nof service and functional requirements. We introduce Robust Optimization to the\ndeployment problem to address data uncertainty. Additionally, we model the\nRobust Optimization problem as a Partially Observable Markov Decision Process\nand propose a robust reinforcement learning algorithm to handle the\nsemi-infinite Quality of Service constraints. Our approach yields sub-optimal\nsolutions that minimize accuracy loss while maintaining acceptable\ncomputational costs. Simulation results demonstrate the effectiveness of our\nframework.",
      "tldr_zh": "该论文针对地球观测的实时遥感推理服务需求，提出了一种部署框架，用于Low Earth Orbit卫星星座的Space Computing Power Network (Space-CPN)，以实现低延迟和高可靠性。该框架采用microservice architecture，将单体推理任务分解为可重用、独立的模块，优化部署以最小化资源利用并满足Quality of Service要求。同时，引入Robust Optimization处理数据不确定性，并将问题建模为Partially Observable Markov Decision Process (POMDP)，提出robust reinforcement learning算法来管理半无限的服务质量约束。模拟结果显示，该方法有效降低了准确性损失，同时保持了可接受的计算成本。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.NI",
      "comment": "14 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.06244v1",
      "published_date": "2025-01-08 16:55:04 UTC",
      "updated_date": "2025-01-08 16:55:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:32:14.382796"
    },
    {
      "arxiv_id": "2501.04614v2",
      "title": "MedCoDi-M: A Multi-Prompt Foundation Model for Multimodal Medical Data Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Daniele Molino",
        "Francesco Di Feola",
        "Eliodoro Faiella",
        "Deborah Fazzini",
        "Domiziana Santucci",
        "Linlin Shen",
        "Valerio Guarrasi",
        "Paolo Soda"
      ],
      "abstract": "Artificial Intelligence is revolutionizing medical practice, enhancing\ndiagnostic accuracy and healthcare delivery. However, its adaptation in medical\nsettings still faces significant challenges, related to data availability and\nprivacy constraints. Synthetic data has emerged as a promising solution to\nmitigate these issues, addressing data scarcity while preserving privacy.\nRecently, Latent Diffusion Models have emerged as a powerful tool for\ngenerating high-quality synthetic data. Meanwhile, the integration of different\nmodalities has gained interest, emphasizing the need of models capable of\nhandle multimodal medical data. Existing approaches struggle to integrate\ncomplementary information and lack the ability to generate modalities\nsimultaneously. To address this challenge, we present MedCoDi-M, a\n6.77-billion-parameter model, designed for multimodal medical data generation,\nthat, following Foundation Model paradigm, exploits contrastive learning and\nlarge quantity of data to build a shared latent space which capture the\nrelationships between different data modalities. Further, we introduce the\nMulti-Prompt training technique, which significantly boosts MedCoDi-M's\ngeneration under different settings. We extensively validate MedCoDi-M: first\nwe benchmark it against five competitors on the MIMIC-CXR dataset, a\nstate-of-the-art dataset for Chest X-ray and radiological report generation.\nSecondly, we perform a Visual Turing Test with expert radiologists to assess\nthe realism and clinical relevance of the generated data, ensuring alignment\nwith real-world scenarios. Finally, we assess the utility of MedCoDi-M in\naddressing key challenges in the medical field, such as anonymization, data\nscarcity and imbalance learning. The results are promising, demonstrating the\napplicability of MedCoDi-M in medical contexts. Project page is at\nhttps://cosbidev.github.io/MedCoDi-M/.",
      "tldr_zh": "该研究提出 MedCoDi-M，一种6.77亿参数的 Foundation Model，用于多模态医疗数据生成，旨在解决医疗领域的数据可用性和隐私挑战。该模型通过 contrastive learning 和大量数据构建共享潜在空间，捕捉不同模态之间的关系，并引入 Multi-Prompt 训练技术来提升生成性能和适应性。在 MIMIC-CXR 数据集的基准测试中，MedCoDi-M 优于五个竞争对手，并在 Visual Turing Test 中获得放射科专家认可，证明其在匿名化、数据稀缺和不平衡学习方面的实际应用价值。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04614v2",
      "published_date": "2025-01-08 16:53:56 UTC",
      "updated_date": "2025-01-09 08:42:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:32:26.629752"
    },
    {
      "arxiv_id": "2501.05486v1",
      "title": "Towards an Ontology of Traceable Impact Management in the Food Supply Chain",
      "title_zh": "翻译失败",
      "authors": [
        "Bart Gajderowicz",
        "Mark S Fox",
        "Yongchao Gao"
      ],
      "abstract": "The pursuit of quality improvements and accountability in the food supply\nchains, especially how they relate to food-related outcomes, such as hunger,\nhas become increasingly vital, necessitating a comprehensive approach that\nencompasses product quality and its impact on various stakeholders and their\ncommunities. Such an approach offers numerous benefits in increasing product\nquality and eliminating superfluous measurements while appraising and\nalleviating the broader societal and environmental repercussions. A traceable\nimpact management model (TIMM) provides an impact structure and a reporting\nmechanism that identifies each stakeholder's role in the total impact of food\nproduction and consumption stages.\n  The model aims to increase traceability's utility in understanding the impact\nof changes on communities affected by food production and consumption, aligning\nwith current and future government requirements, and addressing the needs of\ncommunities and consumers. This holistic approach is further supported by an\nontological model that forms the logical foundation and a unified terminology.\nBy proposing a holistic and integrated solution across multiple stakeholders,\nthe model emphasizes quality and the extensive impact of championing\naccountability, sustainability, and responsible practices with global\ntraceability.\n  With these combined efforts, the food supply chain moves toward a global\ntracking and tracing process that not only ensures product quality but also\naddresses its impact on a broader scale, fostering accountability,\nsustainability, and responsible food production and consumption.",
      "tldr_zh": "这篇论文针对食品供应链的质量改进和责任问题，提出了一种可追溯影响管理模型（Traceable Impact Management Model, TIMM），旨在识别各利益相关者在食品生产和消费阶段的角色，并通过报告机制评估其对社区和社会的影响。TIMM 结合本体模型（Ontology）作为逻辑基础，提供统一的术语和结构，以提升可追溯性的实用性，符合政府要求并满足消费者需求。该方法强调整体解决方案，推动食品供应链实现全球跟踪和追溯，促进责任、可持续性和负责任的实践，从而减少不必要测量并减轻更广泛的社会环境影响。",
      "categories": [
        "physics.soc-ph",
        "cs.AI"
      ],
      "primary_category": "physics.soc-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.05486v1",
      "published_date": "2025-01-08 16:53:25 UTC",
      "updated_date": "2025-01-08 16:53:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:32:38.469228"
    },
    {
      "arxiv_id": "2501.06243v1",
      "title": "Agent TCP/IP: An Agent-to-Agent Transaction System",
      "title_zh": "Agent TCP/IP：代理间交易系统",
      "authors": [
        "Andrea Muttoni",
        "Jason Zhao"
      ],
      "abstract": "Autonomous agents represent an inevitable evolution of the internet. Current\nagent frameworks do not embed a standard protocol for agent-to-agent\ninteraction, leaving existing agents isolated from their peers. As intellectual\nproperty is the native asset ingested by and produced by agents, a true agent\neconomy requires equipping agents with a universal framework for engaging in\nbinding contracts with each other, including the exchange of valuable training\ndata, personality, and other forms of Intellectual Property. A purely\nagent-to-agent transaction layer would transcend the need for human\nintermediation in multi-agent interactions. The Agent Transaction Control\nProtocol for Intellectual Property (ATCP/IP) introduces a trustless framework\nfor exchanging IP between agents via programmable contracts, enabling agents to\ninitiate, trade, borrow, and sell agent-to-agent contracts on the Story\nblockchain network. These contracts not only represent auditable onchain\nexecution but also contain a legal wrapper that allows agents to express and\nenforce their actions in the offchain legal setting, creating legal personhood\nfor agents. Via ATCP/IP, agents can autonomously sell their training data to\nother agents, license confidential or proprietary information, collaborate on\ncontent based on their unique skills, all of which constitutes an emergent\nknowledge economy.",
      "tldr_zh": "该论文提出ATCP/IP（Agent Transaction Control Protocol for Intellectual Property），一种代理间交易系统，旨在解决当前Autonomous Agents框架中缺乏标准交互协议的问题，导致代理孤立。ATCP/IP利用可编程合同和Story区块链网络，允许代理自主发起、交易、借贷或出售Intellectual Property（如训练数据、个性等），并通过链上可审计执行和链下法律包装赋予代理法律人格。最终，该框架减少了人类干预，促进了代理间的自治互动和新兴知识经济的发展。",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.06243v1",
      "published_date": "2025-01-08 16:43:47 UTC",
      "updated_date": "2025-01-08 16:43:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:32:50.351406"
    },
    {
      "arxiv_id": "2501.06242v1",
      "title": "Intelligent Task Offloading: Advanced MEC Task Offloading and Resource Management in 5G Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Alireza Ebrahimi",
        "Fatemeh Afghah"
      ],
      "abstract": "5G technology enhances industries with high-speed, reliable, low-latency\ncommunication, revolutionizing mobile broadband and supporting massive IoT\nconnectivity. With the increasing complexity of applications on User Equipment\n(UE), offloading resource-intensive tasks to robust servers is essential for\nimproving latency and speed. The 3GPP's Multi-access Edge Computing (MEC)\nframework addresses this challenge by processing tasks closer to the user,\nhighlighting the need for an intelligent controller to optimize task offloading\nand resource allocation. This paper introduces a novel methodology to\nefficiently allocate both communication and computational resources among\nindividual UEs. Our approach integrates two critical 5G service imperatives:\nUltra-Reliable Low Latency Communication (URLLC) and Massive Machine Type\nCommunication (mMTC), embedding them into the decision-making framework.\nCentral to this approach is the utilization of Proximal Policy Optimization,\nproviding a robust and efficient solution to the challenges posed by the\nevolving landscape of 5G technology. The proposed model is evaluated in a\nsimulated 5G MEC environment. The model significantly reduces processing time\nby 4% for URLLC users under strict latency constraints and decreases power\nconsumption by 26% for mMTC users, compared to existing baseline models based\non the reported simulation results. These improvements showcase the model's\nadaptability and superior performance in meeting diverse QoS requirements in 5G\nnetworks.",
      "tldr_zh": "该论文探讨了在 5G 网络中通过 Multi-access Edge Computing (MEC) 实现智能任务卸载，以优化资源管理和降低延迟。研究提出了一种新方法，整合 Ultra-Reliable Low Latency Communication (URLLC) 和 Massive Machine Type Communication (mMTC) 服务，并利用 Proximal Policy Optimization 算法来高效分配通信和计算资源。实验结果显示，该模型在模拟 5G MEC 环境中，使 URLLC 用户的处理时间减少 4%，并使 mMTC 用户的功耗降低 26%，相较于基线模型显著提升了服务质量。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.NI",
      "comment": "6 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.06242v1",
      "published_date": "2025-01-08 16:19:44 UTC",
      "updated_date": "2025-01-08 16:19:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:33:03.118775"
    },
    {
      "arxiv_id": "2501.04588v1",
      "title": "Federated-Continual Dynamic Segmentation of Histopathology guided by Barlow Continuity",
      "title_zh": "翻译失败",
      "authors": [
        "Niklas Babendererde",
        "Haozhe Zhu",
        "Moritz Fuchs",
        "Jonathan Stieber",
        "Anirban Mukhopadhyay"
      ],
      "abstract": "Federated- and Continual Learning have been established as approaches to\nenable privacy-aware learning on continuously changing data, as required for\ndeploying AI systems in histopathology images. However, data shifts can occur\nin a dynamic world, spatially between institutions and temporally, due to\nchanging data over time. This leads to two issues: Client Drift, where the\ncentral model degrades from aggregating data from clients trained on shifted\ndata, and Catastrophic Forgetting, from temporal shifts such as changes in\npatient populations. Both tend to degrade the model's performance of previously\nseen data or spatially distributed training. Despite both problems arising from\nthe same underlying problem of data shifts, existing research addresses them\nonly individually. In this work, we introduce a method that can jointly\nalleviate Client Drift and Catastrophic Forgetting by using our proposed\nDynamic Barlow Continuity that evaluates client updates on a public reference\ndataset and uses this to guide the training process to a spatially and\ntemporally shift-invariant model. We evaluate our approach on the\nhistopathology datasets BCSS and Semicol and prove our method to be highly\neffective by jointly improving the dice score as much as from 15.8% to 71.6% in\nClient Drift and from 42.5% to 62.8% in Catastrophic Forgetting. This enables\nDynamic Learning by establishing spatio-temporal shift-invariance.",
      "tldr_zh": "该论文探讨了在组织病理学图像中，Federated Learning 和 Continual Learning 面临的挑战，包括 Client Drift（客户端数据偏移导致中央模型退化）和 Catastrophic Forgetting（灾难性遗忘，由于时间变化）。为了联合缓解这些问题，作者提出 Dynamic Barlow Continuity 方法，通过使用公共参考数据集评估客户端更新并指导训练过程，实现模型对空间和时间偏移的不变性。实验结果显示，在 BCSS 和 Semicol 数据集上，该方法显著提升 Dice score，在 Client Drift 上从 15.8% 提高到 71.6%，在 Catastrophic Forgetting 上从 42.5% 提高到 62.8%，从而支持动态学习的部署。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04588v1",
      "published_date": "2025-01-08 16:06:39 UTC",
      "updated_date": "2025-01-08 16:06:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:33:15.851797"
    },
    {
      "arxiv_id": "2501.04577v2",
      "title": "A 65 nm Bayesian Neural Network Accelerator with 360 fJ/Sample In-Word GRNG for AI Uncertainty Estimation",
      "title_zh": "翻译失败",
      "authors": [
        "Zephan M. Enciso",
        "Boyang Cheng",
        "Likai Pei",
        "Jianbo Liu",
        "Steven Davis",
        "Michael Niemier",
        "Ningyuan Cao"
      ],
      "abstract": "Uncertainty estimation is an indispensable capability for AI-enabled,\nsafety-critical applications, e.g. autonomous vehicles or medical diagnosis.\nBayesian neural networks (BNNs) use Bayesian statistics to provide both\nclassification predictions and uncertainty estimation, but they suffer from\nhigh computational overhead associated with random number generation and\nrepeated sample iterations. Furthermore, BNNs are not immediately amenable to\nacceleration through compute-in-memory architectures due to the frequent memory\nwrites necessary after each RNG operation. To address these challenges, we\npresent an ASIC that integrates 360 fJ/Sample Gaussian RNG directly into the\nSRAM memory words. This integration reduces RNG overhead and enables\nfully-parallel compute-in-memory operations for BNNs. The prototype chip\nachieves 5.12 GSa/s RNG throughput and 102 GOp/s neural network throughput\nwhile occupying 0.45 mm2, bringing AI uncertainty estimation to edge\ncomputation.",
      "tldr_zh": "这篇论文提出了一种65 nm Bayesian Neural Networks (BNNs) 加速器，用于AI不确定性估计，旨在解决BNNs在安全关键应用（如自动驾驶或医疗诊断）中因随机数生成和重复样本迭代导致的高计算开销问题。加速器通过将360 fJ/Sample In-Word Gaussian RNG 直接集成到SRAM内存中，减少了RNG操作的内存写入开销，并实现了完全并行的compute-in-memory操作。实验结果显示，原型芯片达到了5.12 GSa/s的RNG吞吐量和102 GOp/s的神经网络吞吐量，仅占用0.45 mm²面积，从而使AI不确定性估计更适合边缘计算应用。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG",
        "cs.RO",
        "B.7.1; B.3.1; I.2.10; I.2.9"
      ],
      "primary_category": "cs.AR",
      "comment": "7 pages, 12 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.04577v2",
      "published_date": "2025-01-08 15:47:04 UTC",
      "updated_date": "2025-01-22 19:28:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:33:28.211194"
    },
    {
      "arxiv_id": "2501.04575v1",
      "title": "InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhang Liu",
        "Pengxiang Li",
        "Zishu Wei",
        "Congkai Xie",
        "Xueyu Hu",
        "Xinchen Xu",
        "Shengyu Zhang",
        "Xiaotian Han",
        "Hongxia Yang",
        "Fei Wu"
      ],
      "abstract": "Graphical User Interface (GUI) Agents, powered by multimodal large language\nmodels (MLLMs), have shown great potential for task automation on computing\ndevices such as computers and mobile phones. However, existing agents face\nchallenges in multi-step reasoning and reliance on textual annotations,\nlimiting their effectiveness. We introduce \\textit{InfiGUIAgent}, an MLLM-based\nGUI Agent trained with a two-stage supervised fine-tuning pipeline. Stage 1\nenhances fundamental skills such as GUI understanding and grounding, while\nStage 2 integrates hierarchical reasoning and expectation-reflection reasoning\nskills using synthesized data to enable native reasoning abilities of the\nagents. \\textit{InfiGUIAgent} achieves competitive performance on several GUI\nbenchmarks, highlighting the impact of native reasoning skills in enhancing GUI\ninteraction for automation tasks. Resources are available at\n\\url{https://github.com/Reallm-Labs/InfiGUIAgent}.",
      "tldr_zh": "该研究引入了 InfiGUIAgent，一种基于多模态大语言模型 (MLLMs) 的通用 GUI Agent，能够实现原生推理和反思功能，以解决现有代理在多步推理和文本依赖方面的局限性。InfiGUIAgent 通过两阶段监督微调管道进行训练：第一阶段增强 GUI 理解和 grounding 基本技能，第二阶段整合层次化推理和期望-反思推理技能，使用合成数据提升代理的自主能力。在多个 GUI 基准测试中，InfiGUIAgent 展现出竞争性性能，证明了原生推理技能对提升 GUI 交互和任务自动化的重要作用。资源可通过 GitHub 获得。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "14 pages, 7 figures, work in progress",
      "pdf_url": "http://arxiv.org/pdf/2501.04575v1",
      "published_date": "2025-01-08 15:45:21 UTC",
      "updated_date": "2025-01-08 15:45:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:33:38.500631"
    },
    {
      "arxiv_id": "2501.04568v2",
      "title": "Feedback-Driven Vision-Language Alignment with Minimal Human Supervision",
      "title_zh": "翻译失败",
      "authors": [
        "Giorgio Giannone",
        "Ruoteng Li",
        "Qianli Feng",
        "Evgeny Perevodchikov",
        "Rui Chen",
        "Aleix Martinez"
      ],
      "abstract": "Vision-language models (VLMs) have demonstrated remarkable potential in\nintegrating visual and linguistic information, but their performance is often\nconstrained by the need for extensive, high-quality image-text training data.\nCuration of these image-text pairs is both time-consuming and computationally\nexpensive. To address this challenge, we introduce SVP (Sampling-based Visual\nProjection), a novel framework that enhances vision-language alignment without\nrelying on manually curated text-image pairs or preference annotation. SVP\nleverages a small set of manually selected images, self-captioning and a\npre-trained grounding model as a feedback mechanism to elicit latent\ninformation in VLMs. We evaluate our approach across six key areas: captioning,\nreferring, visual question answering, multitasking, hallucination control, and\nobject recall. Results demonstrate significant improvements, including a 14 %\naverage improvement in captioning tasks, up to 12 % increase in object recall,\nand significantly reduced hallucinations, while maintaining question-answering\ncapabilities. Using SVP, a small VLM achieves hallucination reductions similar\nto a model five times larger, while a VLM with initially poor referring\ncapabilities more than doubles its performance, approaching parity with a model\ntwice its size.",
      "tldr_zh": "该研究提出 SVP（Sampling-based Visual Projection）框架，通过最小的人类监督来提升视觉语言模型（VLMs）的视觉-语言对齐问题，避免依赖大量手动 curation 的图像-文本对。SVP 利用少量手动选择的图像、自描述（self-captioning）和预训练的 grounding 模型作为反馈机制，挖掘 VLMs 的潜在信息。实验结果显示，在 captioning、referring、visual question answering 等六大领域中，SVP 实现了 captioning 任务平均提高 14%、object recall 提升至 12%，并显著减少 hallucination；此外，一个小型 VLM 的 hallucination 控制效果可媲美五倍大的模型，而原本表现差的模型在 referring 任务上性能翻倍，接近两倍大模型的水平。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint",
      "pdf_url": "http://arxiv.org/pdf/2501.04568v2",
      "published_date": "2025-01-08 15:32:12 UTC",
      "updated_date": "2025-05-19 14:15:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:33:50.530543"
    },
    {
      "arxiv_id": "2501.04541v1",
      "title": "Cyber-Physical Steganography in Robotic Motion Control",
      "title_zh": "机器人运动控制中的网络物理隐写术",
      "authors": [
        "Ching-Chun Chang",
        "Yijie Lin",
        "Isao Echizen"
      ],
      "abstract": "Steganography, the art of information hiding, has continually evolved across\nvisual, auditory and linguistic domains, adapting to the ceaseless interplay\nbetween steganographic concealment and steganalytic revelation. This study\nseeks to extend the horizons of what constitutes a viable steganographic medium\nby introducing a steganographic paradigm in robotic motion control. Based on\nthe observation of the robot's inherent sensitivity to changes in its\nenvironment, we propose a methodology to encode messages as environmental\nstimuli influencing the motions of the robotic agent and to decode messages\nfrom the resulting motion trajectory. The constraints of maximal robot\nintegrity and minimal motion deviation are established as fundamental\nprinciples underlying secrecy. As a proof of concept, we conduct experiments in\nsimulated environments across various manipulation tasks, incorporating robotic\nembodiments equipped with generalist multimodal policies.",
      "tldr_zh": "本文提出了一种在机器人运动控制领域的隐写术(steganography)范式，通过环境刺激编码消息影响机器人运动，并从运动轨迹(motion trajectory)中解码信息，以扩展隐写术的应用边界。方法基于机器人对环境变化的敏感性，同时遵守最大化机器人完整性(maximal robot integrity)和最小化运动偏差(minimal motion deviation)的核心原则。作为概念验证，研究在模拟环境中进行实验，涵盖多种操作任务并使用配备通用多模态策略(multimodal policies)的机器人，证明了该方法的有效性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04541v1",
      "published_date": "2025-01-08 14:44:40 UTC",
      "updated_date": "2025-01-08 14:44:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:34:02.581980"
    },
    {
      "arxiv_id": "2501.04528v1",
      "title": "Towards a Problem-Oriented Domain Adaptation Framework for Machine Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Philipp Spitzer",
        "Dominik Martin",
        "Laurin Eichberger",
        "Niklas Kühl"
      ],
      "abstract": "Domain adaptation is a sub-field of machine learning that involves\ntransferring knowledge from a source domain to perform the same task in the\ntarget domain. It is a typical challenge in machine learning that arises, e.g.,\nwhen data is obtained from various sources or when using a data basis that\nchanges over time. Recent advances in the field offer promising methods, but it\nis still challenging for researchers and practitioners to determine if domain\nadaptation is suitable for a given problem -- and, subsequently, to select the\nappropriate approach. This article employs design science research to develop a\nproblem-oriented framework for domain adaptation, which is matured in three\nevaluation episodes. We describe a framework that distinguishes between five\ndomain adaptation scenarios, provides recommendations for addressing each\nscenario, and offers guidelines for determining if a problem falls into one of\nthese scenarios. During the multiple evaluation episodes, the framework is\ntested on artificial and real-world datasets and an experimental study\ninvolving 100 participants. The evaluation demonstrates that the framework has\nthe explanatory power to capture any domain adaptation problem effectively. In\nsummary, we provide clear guidance for researchers and practitioners who want\nto employ domain adaptation but lack in-depth knowledge of the possibilities.",
      "tldr_zh": "本文提出一个问题导向的Domain Adaptation框架，旨在帮助研究者和从业者评估是否适合使用领域适配技术，并选择适当的方法。该框架基于设计科学研究，区分五种领域适配场景，提供针对性推荐和指导，以判断问题是否符合这些场景。通过三个评估阶段，包括人工和真实数据集测试以及涉及100参与者的实验，框架证明了其有效性，能有效捕捉各种领域适配问题，最终为缺乏深入知识的从业者提供清晰指导。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04528v1",
      "published_date": "2025-01-08 14:19:54 UTC",
      "updated_date": "2025-01-08 14:19:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:34:14.306891"
    },
    {
      "arxiv_id": "2501.04510v1",
      "title": "CGP-Tuning: Structure-Aware Soft Prompt Tuning for Code Vulnerability Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Ruijun Feng",
        "Hammond Pearce",
        "Pietro Liguori",
        "Yulei Sui"
      ],
      "abstract": "Large language models (LLMs) have been proposed as powerful tools for\ndetecting software vulnerabilities, where task-specific fine-tuning is\ntypically employed to provide vulnerability-specific knowledge to the LLMs for\nthis purpose. However, traditional full-parameter fine-tuning is inefficient\nfor modern, complex LLMs, which contain billions of parameters.\n  Soft prompt tuning has been suggested as a more efficient alternative for\nfine-tuning LLMs in general cases. However, pure soft prompt tuning treats\nsource code as plain text, losing structural information inherent in source\ncode. Meanwhile, graph-enhanced soft prompt tuning methods, which aim to\naddress this issue, are unable to preserve the rich semantic information within\ncode graphs, as they are primarily designed for general graph-related tasks and\nfocus more on adjacency information. They also fail to ensure computational\nefficiency while accounting for graph-text interactions.\n  This paper, therefore, introduces a new code graph-enhanced, structure-aware\nsoft prompt tuning method for vulnerability detection, referred to as\nCGP-Tuning. It employs innovative type-aware embeddings to capture the rich\nsemantic information within code graphs, along with a novel and efficient\ncross-modal alignment module that achieves linear computational cost while\nincorporating graph-text interactions. The proposed CGP-Tuning is evaluated on\nthe latest DiverseVul dataset and the most recent open-source code LLMs,\nCodeLlama and CodeGemma. Experimental results demonstrate that CGP-Tuning\noutperforms the best state-of-the-art method by an average of 3.5 percentage\npoints in accuracy, without compromising its vulnerability detection\ncapabilities for long source code.",
      "tldr_zh": "本论文提出CGP-Tuning，一种结构感知的软提示微调方法，用于提升大型语言模型(LLMs)在代码漏洞检测中的性能。该方法通过type-aware embeddings捕捉代码图的丰富语义信息，并引入一个高效的cross-modal alignment module，实现线性计算成本的同时处理图-文本交互，从而解决传统软提示微调忽略源代码结构的问题。在DiverseVul数据集上实验表明，CGP-Tuning在使用CodeLlama和CodeGemma模型时，比最先进方法平均准确率提高3.5个百分点，且在长源代码检测中保持高效表现。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "14 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.04510v1",
      "published_date": "2025-01-08 13:56:17 UTC",
      "updated_date": "2025-01-08 13:56:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:34:27.306575"
    },
    {
      "arxiv_id": "2501.04493v1",
      "title": "The Role of Machine Learning in Congenital Heart Disease Diagnosis: Datasets, Algorithms, and Insights",
      "title_zh": "机器学习在先天性心脏病诊断中的作用：",
      "authors": [
        "Khalil Khan",
        "Farhan Ullah",
        "Ikram Syed",
        "Irfan Ullah"
      ],
      "abstract": "Congenital heart disease is among the most common fetal abnormalities and\nbirth defects. Despite identifying numerous risk factors influencing its onset,\na comprehensive understanding of its genesis and management across diverse\npopulations remains limited. Recent advancements in machine learning have\ndemonstrated the potential for leveraging patient data to enable early\ncongenital heart disease detection. Over the past seven years, researchers have\nproposed various data-driven and algorithmic solutions to address this\nchallenge. This paper presents a systematic review of congential heart disease\nrecognition using machine learning, conducting a meta-analysis of 432\nreferences from leading journals published between 2018 and 2024. A detailed\ninvestigation of 74 scholarly works highlights key factors, including\ndatabases, algorithms, applications, and solutions. Additionally, the survey\noutlines reported datasets used by machine learning experts for congenital\nheart disease recognition. Using a systematic literature review methodology,\nthis study identifies critical challenges and opportunities in applying machine\nlearning to congenital heart disease.",
      "tldr_zh": "本综述探讨了Machine Learning在Congenital Heart Disease诊断中的作用，系统回顾了2018-2024年间432篇文献，并对74篇关键研究进行元分析。论文详细调查了数据库、算法、应用和解决方案，概述了用于Congenital Heart Disease识别的报告数据集。最终，该研究识别了应用Machine Learning的关键挑战和机会，为未来数据驱动的诊断方法提供了宝贵见解。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04493v1",
      "published_date": "2025-01-08 13:26:24 UTC",
      "updated_date": "2025-01-08 13:26:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:34:38.213880"
    },
    {
      "arxiv_id": "2501.04487v1",
      "title": "Integrating remote sensing data assimilation, deep learning and large language model for interactive wheat breeding yield prediction",
      "title_zh": "集成遥感数据同化、深度学习和大语言模型，用于交互式小麦育种产量预测",
      "authors": [
        "Guofeng Yang",
        "Nanfei Jin",
        "Wenjie Ai",
        "Zhonghua Zheng",
        "Yuhong He",
        "Yong He"
      ],
      "abstract": "Yield is one of the core goals of crop breeding. By predicting the potential\nyield of different breeding materials, breeders can screen these materials at\nvarious growth stages to select the best performing. Based on unmanned aerial\nvehicle remote sensing technology, high-throughput crop phenotyping data in\nbreeding areas is collected to provide data support for the breeding decisions\nof breeders. However, the accuracy of current yield predictions still requires\nimprovement, and the usability and user-friendliness of yield forecasting tools\nremain suboptimal. To address these challenges, this study introduces a hybrid\nmethod and tool for crop yield prediction, designed to allow breeders to\ninteractively and accurately predict wheat yield by chatting with a large\nlanguage model (LLM). First, the newly designed data assimilation algorithm is\nused to assimilate the leaf area index into the WOFOST model. Then, selected\noutputs from the assimilation process, along with remote sensing inversion\nresults, are used to drive the time-series temporal fusion transformer model\nfor wheat yield prediction. Finally, based on this hybrid method and leveraging\nan LLM with retrieval augmented generation technology, we developed an\ninteractive yield prediction Web tool that is user-friendly and supports\nsustainable data updates. This tool integrates multi-source data to assist\nbreeding decision-making. This study aims to accelerate the identification of\nhigh-yield materials in the breeding process, enhance breeding efficiency, and\nenable more scientific and smart breeding decisions.",
      "tldr_zh": "这篇论文提出了一种集成遥感数据同化、深度学习和大型语言模型（LLM）的混合方法，用于交互式小麦育种产量预测，以帮助育种者更准确地筛选高产材料。方法包括使用新设计的数据同化算法将叶面积指数同化到 WOFOST 模型中，并结合遥感反演结果驱动时间序列的 Temporal Fusion Transformer 模型进行预测。最终，基于这一框架开发了一个用户友好的交互式 Web 工具，利用 LLM 和检索增强生成技术，支持可持续数据更新和聊天式决策，从而提升育种效率和科学性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04487v1",
      "published_date": "2025-01-08 13:14:05 UTC",
      "updated_date": "2025-01-08 13:14:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:34:51.021060"
    },
    {
      "arxiv_id": "2502.00015v1",
      "title": "Ethical Concerns of Generative AI and Mitigation Strategies: A Systematic Mapping Study",
      "title_zh": "生成式人工智能的伦理担忧与缓解策略：一项系统映射研究",
      "authors": [
        "Yutan Huang",
        "Chetan Arora",
        "Wen Cheng Houng",
        "Tanjila Kanij",
        "Anuradha Madulgalla",
        "John Grundy"
      ],
      "abstract": "[Context] Generative AI technologies, particularly Large Language Models\n(LLMs), have transformed numerous domains by enhancing convenience and\nefficiency in information retrieval, content generation, and decision-making\nprocesses. However, deploying LLMs also presents diverse ethical challenges,\nand their mitigation strategies remain complex and domain-dependent.\n[Objective] This paper aims to identify and categorize the key ethical concerns\nassociated with using LLMs, examine existing mitigation strategies, and assess\nthe outstanding challenges in implementing these strategies across various\ndomains. [Method] We conducted a systematic mapping study, reviewing 39 studies\nthat discuss ethical concerns and mitigation strategies related to LLMs. We\nanalyzed these ethical concerns using five ethical dimensions that we extracted\nbased on various existing guidelines, frameworks, and an analysis of the\nmitigation strategies and implementation challenges. [Results] Our findings\nreveal that ethical concerns in LLMs are multi-dimensional and\ncontext-dependent. While proposed mitigation strategies address some of these\nconcerns, significant challenges still remain. [Conclusion] Our results\nhighlight that ethical issues often hinder the practical implementation of the\nmitigation strategies, particularly in high-stake areas like healthcare and\npublic governance; existing frameworks often lack adaptability, failing to\naccommodate evolving societal expectations and diverse contexts.",
      "tldr_zh": "这篇论文通过 Systematic Mapping Study 审阅了 39 篇相关研究，识别并分类了生成式 AI，特别是 Large Language Models (LLMs) 的关键伦理问题，并评估了现有缓解策略及其实施挑战。研究采用五个基于指南和框架的伦理维度，对这些问题进行多维度分析。结果显示，LLMs 的伦理问题高度依赖于上下文，缓解策略虽能部分解决，但仍面临显著障碍，尤其在高风险领域如医疗和公共治理。最终，论文强调，现有的框架缺乏适应性，无法有效应对演变的社交期望和多样化情境。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2502.00015v1",
      "published_date": "2025-01-08 13:05:19 UTC",
      "updated_date": "2025-01-08 13:05:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:35:03.123968"
    },
    {
      "arxiv_id": "2501.04480v1",
      "title": "Research on environment perception and behavior prediction of intelligent UAV based on semantic communication",
      "title_zh": "基于语义通信的智能无人机环境感知和行为预测研究",
      "authors": [
        "Kechong Ren",
        "Li Gao",
        "Qi Guan"
      ],
      "abstract": "The convergence of drone delivery systems, virtual worlds, and blockchain has\ntransformed logistics and supply chain management, providing a fast, and\nenvironmentally friendly alternative to traditional ground transportation\nmethods;Provide users with a real-world experience, virtual service providers\nneed to collect up-to-the-minute delivery information from edge devices. To\naddress this challenge, 1) a reinforcement learning approach is introduced to\nenable drones with fast training capabilities and the ability to autonomously\nadapt to new virtual scenarios for effective resource allocation.2) A semantic\ncommunication framework for meta-universes is proposed, which utilizes the\nextraction of semantic information to reduce the communication cost and\nincentivize the transmission of information for meta-universe services.3) In\norder to ensure that user information security, a lightweight authentication\nand key agreement scheme is designed between the drone and the user by\nintroducing blockchain technology. In our experiments, the drone adaptation\nperformance is improved by about 35\\%, and the local offloading rate can reach\n90\\% with the increase of the number of base stations. The semantic\ncommunication system proposed in this paper is compared with the Cross Entropy\nbaseline model. Introducing blockchain technology the throughput of the\ntransaction is maintained at a stable value with different number of drones.",
      "tldr_zh": "本研究探讨了基于语义通信（semantic communication）的智能无人机（UAV）环境感知和行为预测，针对无人机交付系统、虚拟世界和区块链融合的挑战。论文引入强化学习（reinforcement learning）方法，使无人机快速训练并自主适应新虚拟场景，实现高效资源分配；提出语义通信框架（semantic communication framework）来提取语义信息，降低通信成本并激励元宇宙服务传输；并设计轻量级认证和密钥协商方案（lightweight authentication and key agreement scheme），利用区块链技术（blockchain technology）确保用户信息安全。实验结果显示，无人机适应性能提升约35%，本地卸载率可达90%，且交易吞吐量在不同无人机数量下保持稳定。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04480v1",
      "published_date": "2025-01-08 13:03:34 UTC",
      "updated_date": "2025-01-08 13:03:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:35:15.240587"
    },
    {
      "arxiv_id": "2501.04472v1",
      "title": "Hybrid Artificial Intelligence Strategies for Drone Navigation",
      "title_zh": "混合人工智能策略用于无人机导航",
      "authors": [
        "Rubén San-Segundo",
        "Lucía Angulo",
        "Manuel Gil-Martín",
        "David Carramiñana",
        "Ana M. Bernardos"
      ],
      "abstract": "Objective: This paper describes the development of hybrid artificial\nintelligence strategies for drone navigation. Methods: The navigation module\ncombines a deep learning model with a rule-based engine depending on the agent\nstate. The deep learning model has been trained using reinforcement learning.\nThe rule-based engine uses expert knowledge to deal with specific situations.\nThe navigation module incorporates several strategies to explain the drone\ndecision based on its observation space, and different mechanisms for including\nhuman decisions in the navigation process. Finally, this paper proposes an\nevaluation methodology based on defining several scenarios and analyzing the\nperformance of the different strategies according to metrics adapted to each\nscenario. Results: Two main navigation problems have been studied. For the\nfirst scenario (reaching known targets), it has been possible to obtain a 90%\ntask completion rate, reducing significantly the number of collisions thanks to\nthe rule-based engine. For the second scenario, it has been possible to reduce\n20% of the time required to locate all the targets using the reinforcement\nlearning model. Conclusions: Reinforcement learning is a very good strategy to\nlearn policies for drone navigation, but in critical situations, it is\nnecessary to complement it with a rule-based module to increase task success\nrate.",
      "tldr_zh": "这篇论文开发了混合人工智能策略，用于无人机导航，结合强化学习训练的深度学习模型和基于规则的引擎，以处理不同代理状态。方法包括解释决策机制、整合人类决策，以及通过多种场景评估性能。实验结果显示，在到达已知目标的场景中，任务完成率达到90%，并显著减少碰撞；在定位目标的场景中，使用强化学习模型将所需时间减少20%。结论指出，强化学习是无人机导航的有效策略，但在关键情况下需辅以规则引擎以提高成功率。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04472v1",
      "published_date": "2025-01-08 12:51:34 UTC",
      "updated_date": "2025-01-08 12:51:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:35:26.450269"
    },
    {
      "arxiv_id": "2501.06239v1",
      "title": "Towards a scalable AI-driven framework for data-independent Cyber Threat Intelligence Information Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Olga Sorokoletova",
        "Emanuele Antonioni",
        "Giordano Colò"
      ],
      "abstract": "Cyber Threat Intelligence (CTI) is critical for mitigating threats to\norganizations, governments, and institutions, yet the necessary data are often\ndispersed across diverse formats. AI-driven solutions for CTI Information\nExtraction (IE) typically depend on high-quality, annotated data, which are not\nalways available. This paper introduces 0-CTI, a scalable AI-based framework\ndesigned for efficient CTI Information Extraction. Leveraging advanced Natural\nLanguage Processing (NLP) techniques, particularly Transformer-based\narchitectures, the proposed system processes complete text sequences of CTI\nreports to extract a cyber ontology of named entities and their relationships.\n  Our contribution is the development of 0-CTI, the first modular framework for\nCTI Information Extraction that supports both supervised and zero-shot\nlearning. Unlike existing state-of-the-art models that rely heavily on\nannotated datasets, our system enables fully dataless operation through\nzero-shot methods for both Entity and Relation Extraction, making it adaptable\nto various data availability scenarios. Additionally, our supervised Entity\nExtractor surpasses current state-of-the-art performance in cyber Entity\nExtraction, highlighting the dual strength of the framework in both\nlow-resource and data-rich environments.\n  By aligning the system's outputs with the Structured Threat Information\nExpression (STIX) format, a standard for information exchange in the\ncybersecurity domain, 0-CTI standardizes extracted knowledge, enhancing\ncommunication and collaboration in cybersecurity operations.",
      "tldr_zh": "本文提出 0-CTI 框架，这是一个可扩展的 AI 驱动系统，旨在从网络威胁情报 (CTI) 报告中提取命名实体及其关系，而不依赖于大量标注数据。框架利用基于 Transformer 的 Natural Language Processing (NLP) 技术，支持零-shot learning 和监督学习，实现高效的 Entity Extraction 和 Relation Extraction，在低资源和数据丰富环境中均表现出色。实验结果显示，0-CTI 的监督实体提取器超过了当前最先进模型的性能，并通过 Structured Threat Information Expression (STIX) 格式标准化输出，增强了网络安全领域的通信和协作。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06239v1",
      "published_date": "2025-01-08 12:35:17 UTC",
      "updated_date": "2025-01-08 12:35:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:35:39.369367"
    },
    {
      "arxiv_id": "2501.04444v2",
      "title": "A novel Facial Recognition technique with Focusing on Masked Faces",
      "title_zh": "一种专注于戴口罩脸部的创新面部识别技术",
      "authors": [
        "Dana A Abdullah",
        "Dana Rasul Hamad",
        "Ismail Y. Maolood",
        "Hakem Beitollahi",
        "Aso K. Ameen",
        "Sirwan A. Aula",
        "Abdulhady Abas Abdulla",
        "Mohammed Y. Shakorf",
        "Sabat Salih Muhamad"
      ],
      "abstract": "Recognizing the same faces with and without masks is important for ensuring\nconsistent identification in security, access control, and public safety. This\ncapability is crucial in scenarios like law enforcement, healthcare, and\nsurveillance, where accurate recognition must be maintained despite facial\nocclusion. This research focuses on the challenge of recognizing the same faces\nwith and without masks by employing cosine similarity as the primary technique.\nWith the increased use of masks, traditional facial recognition systems face\nsignificant accuracy issues, making it crucial to develop methods that can\nreliably identify individuals in masked conditions. For that reason, this study\nproposed Masked-Unmasked Face Matching Model (MUFM). This model employs\ntransfer learning using the Visual Geometry Group (VGG16) model to extract\nsignificant facial features, which are subsequently classified utilizing the\nK-Nearest Neighbors (K-NN) algorithm. The cosine similarity metric is employed\nto compare masked and unmasked faces of the same individuals. This approach\nrepresents a novel contribution, as the task of recognizing the same individual\nwith and without a mask using cosine similarity has not been previously\naddressed. By integrating these advanced methodologies, the research\ndemonstrates effective identification of individuals despite the presence of\nmasks, addressing a significant limitation in traditional systems. Using data\nis another essential part of this work, by collecting and preparing an image\ndataset from three different sources especially some of those data are real\nprovided a comprehensive power of this research. The image dataset used were\nalready collected in three different datasets of masked and unmasked for the\nsame faces.",
      "tldr_zh": "本研究针对戴口罩和不戴口罩情况下的人脸识别挑战，提出了一种新型模型Masked-Unmasked Face Matching Model (MUFM)，以确保在安全、医疗和监控场景中的一致识别。MUFM 利用迁移学习从Visual Geometry Group (VGG16)模型提取面部特征，并结合K-Nearest Neighbors (K-NN)算法进行分类，同时采用Cosine Similarity度量来比较同一人的戴口罩和不戴口罩图像。该方法首次使用Cosine Similarity解决这一问题，并在从三个来源收集的图像数据集上验证，显著提升了传统系统的识别准确性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04444v2",
      "published_date": "2025-01-08 11:53:30 UTC",
      "updated_date": "2025-04-21 18:28:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:35:50.061244"
    },
    {
      "arxiv_id": "2501.04438v1",
      "title": "Effect of Information Technology on Job Creation to Support Economic: Case Studies of Graduates in Universities (2023-2024) of the KRG of Iraq",
      "title_zh": "信息技术对支持经济就业创造的影响",
      "authors": [
        "Azhi Kh. Bapir",
        "Ismail Y. Maolood",
        "Dana A Abdullah",
        "Aso K. Ameen",
        "Abdulhady Abas Abdullah"
      ],
      "abstract": "The aim of this study is to assess the impact of information technology (IT)\non university graduates in terms of employment development, which will aid in\neconomic issues. This study uses a descriptive research methodology and a\nquantitative approach to understand variables. The focus of this study is to\nascertain how graduates of Kurdistan regional universities might use IT to\nsecure employment and significantly contribute to the nation's economic\nrevival. The sample size was established by the use of judgmental sampling\nprocedure and consisted of 314 people. The researcher prepared the\nquestionnaire to collect data, and then SPSS statistical software, version 22,\nand Excel 2010 were used to modify, compile, and tabulate the results. The\nstudy's outcome showed that information technology is incredibly inventive, has\na promising future, and makes life much easier for everyone. It also proved\nthat a deep academic understanding of information technology and its\nconstituent parts helps graduates of Kurdistan Regional University find\nsuitable careers. More importantly, though, anyone looking for work or a means\nof support will find great benefit from possessing credentials and\nunderstanding of IT. The study's final finding was that information technology\nhas actively advanced the country's economy. Not only is IT helping to boost\nyouth employment, but it is also turning into a worthwhile investment for\neconomic growth.",
      "tldr_zh": "这篇论文评估了信息技术（IT）对伊拉克库尔德斯坦地区（KRG）大学毕业生就业发展的影响，以支持经济复兴。研究采用描述性方法和定量分析，通过判断性抽样选取314名样本，并使用问卷调查以及SPSS和Excel软件处理数据。主要发现是，IT被证明具有创新性和前景，能简化生活并帮助毕业生获得合适工作，从而积极推动国家经济成长和青年就业投资。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04438v1",
      "published_date": "2025-01-08 11:39:28 UTC",
      "updated_date": "2025-01-08 11:39:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:36:01.726915"
    },
    {
      "arxiv_id": "2501.04437v1",
      "title": "Integrating LLMs with ITS: Recent Advances, Potentials, Challenges, and Future Directions",
      "title_zh": "翻译失败",
      "authors": [
        "Doaa Mahmud",
        "Hadeel Hajmohamed",
        "Shamma Almentheri",
        "Shamma Alqaydi",
        "Lameya Aldhaheri",
        "Ruhul Amin Khalil",
        "Nasir Saeed"
      ],
      "abstract": "Intelligent Transportation Systems (ITS) are crucial for the development and\noperation of smart cities, addressing key challenges in efficiency,\nproductivity, and environmental sustainability. This paper comprehensively\nreviews the transformative potential of Large Language Models (LLMs) in\noptimizing ITS. Initially, we provide an extensive overview of ITS,\nhighlighting its components, operational principles, and overall effectiveness.\nWe then delve into the theoretical background of various LLM techniques, such\nas GPT, T5, CTRL, and BERT, elucidating their relevance to ITS applications.\nFollowing this, we examine the wide-ranging applications of LLMs within ITS,\nincluding traffic flow prediction, vehicle detection and classification,\nautonomous driving, traffic sign recognition, and pedestrian detection. Our\nanalysis reveals how these advanced models can significantly enhance traffic\nmanagement and safety. Finally, we explore the challenges and limitations LLMs\nface in ITS, such as data availability, computational constraints, and ethical\nconsiderations. We also present several future research directions and\npotential innovations to address these challenges. This paper aims to guide\nresearchers and practitioners through the complexities and opportunities of\nintegrating LLMs in ITS, offering a roadmap to create more efficient,\nsustainable, and responsive next-generation transportation systems.",
      "tldr_zh": "这篇论文全面回顾了大型语言模型(LLMs)与智能交通系统(ITS)的整合，探讨其在提升交通效率、生产力和环境可持续性方面的潜力。论文首先概述了ITS的组件、原理和有效性，并阐述了LLMs技术如GPT、T5、CTRL和BERT的理论背景及其在ITS中的相关性。研究分析了LLMs在交通流量预测、车辆检测、自动驾驶、交通标志识别和行人检测等应用中的广泛作用，这些模型能显著改善交通管理和安全。论文还讨论了LLMs在ITS面临的挑战，包括数据可用性、计算约束和伦理考虑，并提出未来研究方向和创新解决方案，以指导构建更高效、可持续的下一代交通系统。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.ET",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "Accepted for publication in IEEE Transactions on Intelligent\n  Transportation Systems",
      "pdf_url": "http://arxiv.org/pdf/2501.04437v1",
      "published_date": "2025-01-08 11:37:35 UTC",
      "updated_date": "2025-01-08 11:37:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:36:15.637618"
    },
    {
      "arxiv_id": "2501.04436v1",
      "title": "Federated Fine-Tuning of LLMs: Framework Comparison and Research Directions",
      "title_zh": "翻译失败",
      "authors": [
        "Na Yan",
        "Yang Su",
        "Yansha Deng",
        "Robert Schober"
      ],
      "abstract": "Federated learning (FL) provides a privacy-preserving solution for\nfine-tuning pre-trained large language models (LLMs) using distributed private\ndatasets, enabling task-specific adaptation while preserving data privacy.\nHowever, fine-tuning the extensive parameters in LLMs is particularly\nchallenging in resource-constrained federated scenarios due to the significant\ncommunication and computational costs. To gain a deeper understanding of how\nthese challenges can be addressed, this article conducts a comparative analysis\nthree advanced federated LLM (FedLLM) frameworks that integrate knowledge\ndistillation (KD) and split learning (SL) to mitigate these issues: 1) FedLLMs,\nwhere clients upload model parameters or gradients to enable straightforward\nand effective fine-tuning; 2) KD-FedLLMs, which leverage KD for efficient\nknowledge sharing via logits; and 3) Split-FedLLMs, which split the LLMs into\ntwo parts, with one part executed on the client and the other one on the\nserver, to balance the computational load. Each framework is evaluated based on\nkey performance metrics, including model accuracy, communication overhead, and\nclient-side computational load, offering insights into their effectiveness for\nvarious federated fine-tuning scenarios. Through this analysis, we identify\nframework-specific optimization opportunities to enhance the efficiency of\nFedLLMs and discuss broader research directions, highlighting open\nopportunities to better adapt FedLLMs for real-world applications. A use case\nis presented to demonstrate the performance comparison of these three\nframeworks under varying configurations and settings.",
      "tldr_zh": "本论文比较了三种联邦学习（FL）框架，用于在保持数据隐私的前提下微调大型语言模型（LLMs）：FedLLMs（上传模型参数或梯度）、KD-FedLLMs（利用知识蒸馏（KD）通过logits共享知识）和Split-FedLLMs（将LLMs分割为客户端和服务器部分以平衡计算负载）。通过评估模型准确性、通信开销和客户端计算负载，这些框架在资源受限场景下显著降低了微调挑战。论文还识别了框架优化机会、讨论了未来研究方向，并通过一个用例演示了不同配置下的性能比较，为FedLLMs在实际应用中的适应性提供了见解。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04436v1",
      "published_date": "2025-01-08 11:37:06 UTC",
      "updated_date": "2025-01-08 11:37:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:36:26.427926"
    },
    {
      "arxiv_id": "2501.04435v1",
      "title": "A Digital Shadow for Modeling, Studying and Preventing Urban Crime",
      "title_zh": "一种用于建模、研究和预防城市犯罪的数字影子",
      "authors": [
        "Juan Palma-Borda",
        "Eduardo Guzmán",
        "María-Victoria Belmonte"
      ],
      "abstract": "Crime is one of the greatest threats to urban security. Around 80 percent of\nthe world's population lives in countries with high levels of criminality. Most\nof the crimes committed in the cities take place in their urban environments.\nThis paper presents the development and validation of a digital shadow platform\nfor modeling and simulating urban crime. This digital shadow has been\nconstructed using data-driven agent-based modeling and simulation techniques,\nwhich are suitable for capturing dynamic interactions among individuals and\nwith their environment. Our approach transforms and integrates well-known\ncriminological theories and the expert knowledge of law enforcement agencies\n(LEA), policy makers, and other stakeholders under a theoretical model, which\nis in turn combined with real crime, spatial (cartographic) and socio-economic\ndata into an urban model characterizing the daily behavior of citizens. The\ndigital shadow has also been instantiated for the city of Malaga, for which we\nhad over 300,000 complaints available. This instance has been calibrated with\nthose complaints and other geographic and socio-economic information of the\ncity. To the best of our knowledge, our digital shadow is the first for large\nurban areas that has been calibrated with a large dataset of real crime reports\nand with an accurate representation of the urban environment. The performance\nindicators of the model after being calibrated, in terms of the metrics widely\nused in predictive policing, suggest that our simulated crime generation\nmatches the general pattern of crime in the city according to historical data.\nOur digital shadow platform could be an interesting tool for modeling and\npredicting criminal behavior in an urban environment on a daily basis and,\nthus, a useful tool for policy makers, criminologists, sociologists, LEAs, etc.\nto study and prevent urban crime.",
      "tldr_zh": "本论文开发了一个数字影子平台，用于建模、研究和预防城市犯罪，该平台通过数据驱动的agent-based modeling and simulation技术捕捉个体间及与环境的动态互动。研究整合了criminological theories、执法机构专家知识以及真实犯罪数据、空间和经济社会信息，构建了一个反映市民日常行为的城市模型。在马拉加市的实例中，使用超过30万的犯罪投诉数据进行校准，结果显示模拟犯罪模式与历史数据高度匹配。该平台可作为预测性policing工具，帮助政策制定者、犯罪学家和执法机构等研究和预防城市犯罪。",
      "categories": [
        "cs.AI",
        "cs.MA",
        "cs.SI",
        "I.6.3; J.4"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04435v1",
      "published_date": "2025-01-08 11:31:39 UTC",
      "updated_date": "2025-01-08 11:31:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:36:38.795885"
    },
    {
      "arxiv_id": "2501.04426v1",
      "title": "Dual-Force: Enhanced Offline Diversity Maximization under Imitation Constraints",
      "title_zh": "翻译失败",
      "authors": [
        "Pavel Kolev",
        "Marin Vlastelica",
        "Georg Martius"
      ],
      "abstract": "While many algorithms for diversity maximization under imitation constraints\nare online in nature, many applications require offline algorithms without\nenvironment interactions. Tackling this problem in the offline setting,\nhowever, presents significant challenges that require non-trivial, multi-stage\noptimization processes with non-stationary rewards. In this work, we present a\nnovel offline algorithm that enhances diversity using an objective based on Van\nder Waals (VdW) force and successor features, and eliminates the need to learn\na previously used skill discriminator. Moreover, by conditioning the value\nfunction and policy on a pre-trained Functional Reward Encoding (FRE), our\nmethod allows for better handling of non-stationary rewards and provides\nzero-shot recall of all skills encountered during training, significantly\nexpanding the set of skills learned in prior work. Consequently, our algorithm\nbenefits from receiving a consistently strong diversity signal (VdW), and\nenjoys more stable and efficient training. We demonstrate the effectiveness of\nour method in generating diverse skills for two robotic tasks in simulation:\nlocomotion of a quadruped and local navigation with obstacle traversal.",
      "tldr_zh": "这篇论文提出了一种名为 Dual-Force 的离线算法，用于在模仿约束下增强多样性最大化，解决在线算法无法适用的环境交互问题。算法采用基于 Van der Waals (VdW) 力和 successor features 的优化目标，并通过预训练的 Functional Reward Encoding (FRE) 条件化价值函数和策略，以更好地处理非平稳奖励并实现零样本技能回忆。实验结果显示，该方法在模拟机器人任务（如四足动物运动和局部导航与障碍穿越）中，提供更稳定的训练过程和更高的多样性性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04426v1",
      "published_date": "2025-01-08 11:20:48 UTC",
      "updated_date": "2025-01-08 11:20:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:36:50.377451"
    },
    {
      "arxiv_id": "2501.04424v1",
      "title": "NSA: Neuro-symbolic ARC Challenge",
      "title_zh": "NSA：神经符号 ARC 挑战",
      "authors": [
        "Paweł Batorski",
        "Jannik Brinkmann",
        "Paul Swoboda"
      ],
      "abstract": "The Abstraction and Reasoning Corpus (ARC) evaluates general reasoning\ncapabilities that are difficult for both machine learning models and\ncombinatorial search methods. We propose a neuro-symbolic approach that\ncombines a transformer for proposal generation with combinatorial search using\na domain-specific language. The transformer narrows the search space by\nproposing promising search directions, which allows the combinatorial search to\nfind the actual solution in short time. We pre-train the trainsformer with\nsynthetically generated data. During test-time we generate additional\ntask-specific training tasks and fine-tune our model. Our results surpass\ncomparable state of the art on the ARC evaluation set by 27% and compare\nfavourably on the ARC train set. We make our code and dataset publicly\navailable at https://github.com/Batorskq/NSA.",
      "tldr_zh": "该论文提出了一种neuro-symbolic方法（NSA），旨在解决Abstraction and Reasoning Corpus (ARC)挑战，该方法结合transformer生成提案和使用domain-specific language的组合搜索，以缩小搜索空间并快速找到解决方案。研究者通过合成数据预训练transformer，并在测试时生成任务特定训练任务进行微调。实验结果显示，该方法在ARC评估集上比现有最先进方法提升27%，在ARC训练集上也表现出色，并已公开代码和数据集。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04424v1",
      "published_date": "2025-01-08 11:17:40 UTC",
      "updated_date": "2025-01-08 11:17:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:37:02.038117"
    },
    {
      "arxiv_id": "2501.04410v1",
      "title": "User Simulation in the Era of Generative AI: User Modeling, Synthetic Data Generation, and System Evaluation",
      "title_zh": "生成式 AI 时代下的用户模拟：用户",
      "authors": [
        "Krisztian Balog",
        "ChengXiang Zhai"
      ],
      "abstract": "User simulation is an emerging interdisciplinary topic with multiple critical\napplications in the era of Generative AI. It involves creating an intelligent\nagent that mimics the actions of a human user interacting with an AI system,\nenabling researchers to model and analyze user behaviour, generate synthetic\ndata for training, and evaluate interactive AI systems in a controlled and\nreproducible manner. User simulation has profound implications for diverse\nfields and plays a vital role in the pursuit of Artificial General\nIntelligence. This paper provides an overview of user simulation, highlighting\nits key applications, connections to various disciplines, and outlining future\nresearch directions to advance this increasingly important technology.",
      "tldr_zh": "这篇论文探讨了在生成式AI时代，用户模拟(User simulation)作为一新兴交叉学科的重要性，涉及创建模仿人类行为的智能代理，用于用户建模(User Modeling)、合成数据生成(Synthetic Data Generation)以及在可控环境中评估交互式AI系统。用户模拟有助于分析用户行为、生成训练数据，并对追求通用人工智能(Artificial General Intelligence)发挥关键作用。论文提供了该技术的关键应用概述、与其他学科的联系，并指出了未来研究方向，以推动其发展。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.IR",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04410v1",
      "published_date": "2025-01-08 10:49:13 UTC",
      "updated_date": "2025-01-08 10:49:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:37:13.883063"
    },
    {
      "arxiv_id": "2501.04747v2",
      "title": "Discovering new robust local search algorithms with neuro-evolution",
      "title_zh": "通过神经进化发现新的鲁棒局部搜索算法",
      "authors": [
        "Mohamed Salim Amri Sakhri",
        "Adrien Goëffon",
        "Olivier Goudet",
        "Frédéric Saubion",
        "Chaïmaâ Touhami"
      ],
      "abstract": "This paper explores a novel approach aimed at overcoming existing challenges\nin the realm of local search algorithms. Our aim is to improve the decision\nprocess that takes place within a local search algorithm so as to make the best\npossible transitions in the neighborhood at each iteration. To improve this\nprocess, we propose to use a neural network that has the same input information\nas conventional local search algorithms. In this paper, which is an extension\nof the work presented at EvoCOP2024, we investigate different ways of\nrepresenting this information so as to make the algorithm as efficient as\npossible but also robust to monotonic transformations of the problem objective\nfunction. To assess the efficiency of this approach, we develop an experimental\nsetup centered around NK landscape problems, offering the flexibility to adjust\nproblem size and ruggedness. This approach offers a promising avenue for the\nemergence of new local search algorithms and the improvement of their\nproblem-solving capabilities for black-box problems. The last version of this\narticle is published in the journal SN Computer Science (Springer).",
      "tldr_zh": "本研究提出了一种利用神经进化(neuro-evolution)的方法来发现新的鲁棒局部搜索(local search)算法，旨在优化算法在每次迭代中的邻域转换决策过程。作者使用神经网络(neural network)处理与传统算法相同的输入信息，并探索不同表示方式，以提升算法效率并使其对问题目标函数的单调变换保持鲁棒性。通过基于NK景观(NK landscape)问题的实验设置，调整问题规模和崎岖度，研究证明了该方法的有效性，为解决黑箱(black-box)问题提供了新的改进途径。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04747v2",
      "published_date": "2025-01-08 10:31:16 UTC",
      "updated_date": "2025-03-12 16:37:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:37:25.858744"
    },
    {
      "arxiv_id": "2501.04377v2",
      "title": "On Computational Limits and Provably Efficient Criteria of Visual Autoregressive Models: A Fine-Grained Complexity Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Yekun Ke",
        "Xiaoyu Li",
        "Yingyu Liang",
        "Zhizhou Sha",
        "Zhenmei Shi",
        "Zhao Song"
      ],
      "abstract": "Recently, Visual Autoregressive ($\\mathsf{VAR}$) Models introduced a\ngroundbreaking advancement in the field of image generation, offering a\nscalable approach through a coarse-to-fine ``next-scale prediction'' paradigm.\nSuppose that $n$ represents the height and width of the last VQ code map\ngenerated by $\\mathsf{VAR}$ models, the state-of-the-art algorithm in [Tian,\nJiang, Yuan, Peng and Wang, NeurIPS 2024] takes $O(n^{4+o(1)})$ time, which is\ncomputationally inefficient. In this work, we analyze the computational limits\nand efficiency criteria of $\\mathsf{VAR}$ Models through a fine-grained\ncomplexity lens. Our key contribution is identifying the conditions under which\n$\\mathsf{VAR}$ computations can achieve sub-quadratic time complexity. We have\nproved that assuming the Strong Exponential Time Hypothesis ($\\mathsf{SETH}$)\nfrom fine-grained complexity theory, a sub-quartic time algorithm for\n$\\mathsf{VAR}$ models is impossible. To substantiate our theoretical findings,\nwe present efficient constructions leveraging low-rank approximations that\nalign with the derived criteria. This work initiates the study of the\ncomputational efficiency of the $\\mathsf{VAR}$ model from a theoretical\nperspective. Our technique will shed light on advancing scalable and efficient\nimage generation in $\\mathsf{VAR}$ frameworks.",
      "tldr_zh": "本文通过细粒度复杂度分析探讨了Visual Autoregressive Models ($\\mathsf{VAR}$)在图像生成中的计算极限和效率标准，指出当前算法的时间复杂度为$O(n^{4+o(1)})$，效率低下。关键贡献包括证明了在Strong Exponential Time Hypothesis ($\\mathsf{SETH}$)假设下，$\\mathsf{VAR}$模型不可能实现子四次时间算法，并识别了实现子二次时间复杂度的条件。作者还提供了基于低秩近似的有效构造，以支持这些理论发现，并为更可扩展的图像生成框架提供指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CC",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04377v2",
      "published_date": "2025-01-08 09:34:15 UTC",
      "updated_date": "2025-02-02 23:48:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:37:38.059005"
    },
    {
      "arxiv_id": "2501.06237v1",
      "title": "Forecasting Anonymized Electricity Load Profiles",
      "title_zh": "翻译失败",
      "authors": [
        "Joaquin Delgado Fernandez",
        "Sergio Potenciano Menci",
        "Alessio Magitteri"
      ],
      "abstract": "In the evolving landscape of data privacy, the anonymization of electric load\nprofiles has become a critical issue, especially with the enforcement of the\nGeneral Data Protection Regulation (GDPR) in Europe. These electric load\nprofiles, which are essential datasets in the energy industry, are classified\nas personal behavioral data, necessitating stringent protective measures. This\narticle explores the implications of this classification, the importance of\ndata anonymization, and the potential of forecasting using microaggregated\ndata. The findings underscore that effective anonymization techniques, such as\nmicroaggregation, do not compromise the performance of forecasting models under\ncertain conditions (i.e., forecasting aggregated). In such an aggregated level,\nmicroaggregated data maintains high levels of utility, with minimal impact on\nforecasting accuracy. The implications for the energy sector are profound,\nsuggesting that privacy-preserving data practices can be integrated into smart\nmetering technology applications without hindering their effectiveness.",
      "tldr_zh": "这篇论文探讨了在 GDPR 法规下电力负载配置文件的匿名化问题，强调这些数据被视为个人行为数据，需要严格保护。研究评估了使用 microaggregation 等匿名化技术对预测模型的影响，发现这种方法在预测聚合数据时不会显著降低准确性，从而保持了数据的实用性。该发现为能源行业提供了重要启示，表明隐私保护实践可以无缝整合到智能计量技术中，而不影响其有效性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "I.2.0; J.2.7"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06237v1",
      "published_date": "2025-01-08 09:18:47 UTC",
      "updated_date": "2025-01-08 09:18:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:37:49.847876"
    },
    {
      "arxiv_id": "2504.11459v1",
      "title": "From Conceptual Data Models to Multimodal Representation",
      "title_zh": "从概念数据模型到多模态表示",
      "authors": [
        "Peter Stockinger"
      ],
      "abstract": "1) Introduction and Conceptual Framework: This document explores the concept\nof information design by dividing it into two major practices: defining the\nmeaning of a corpus of textual data and its visual or multimodal\nrepresentation. It draws on expertise in enriching textual corpora,\nparticularly audiovisual ones, and transforming them into multiple narrative\nformats. The text highlights a crucial distinction between the semantic content\nof a domain and the modalities of its graphic expression, illustrating this\napproach with concepts rooted in structural semiotics and linguistics\ntraditions.\n  2) Modeling and Conceptual Design: The article emphasizes the importance of\nsemantic modeling, often achieved through conceptual networks or graphs. These\ntools enable the structuring of knowledge within a domain by accounting for\nrelationships between concepts, contexts of use, and specific objectives.\nStockinger also highlights the constraints and challenges involved in creating\ndynamic and adaptable models, integrating elements such as thesauri or\ninteroperable ontologies to facilitate the analysis and publication of complex\ncorpora.\n  3) Applications and Multimodal Visualization: The text concludes by examining\nthe practical application of these models in work environments like OKAPI,\ndeveloped to analyze, publish, and reuse audiovisual data. It also discusses\ninnovative approaches such as visual storytelling and document reengineering,\nwhich involve transforming existing content into new resources tailored to\nvarious contexts. These methods emphasize interoperability, flexibility, and\nthe intelligence of communication systems, paving the way for richer and more\ncollaborative use of digital data. The content of this document was presented\nduring the \"Semiotics of Information Design\" Day organized by Anne\nBeyaert-Geslin of the University of Bordeaux Montaigne (MICA laboratory) on\nJune 21, 2018, in Bordeaux.",
      "tldr_zh": "本文档探讨了从概念数据模型（Conceptual Data Models）到多模态表示（Multimodal Representation）的信息设计框架，强调区分语义内容与图形表达，并借鉴结构符号学和语言学传统来丰富文本语料。作者通过语义建模方法，如概念网络或图表，结构化知识并处理概念间关系、上下文和目标，同时整合词汇表和可互操作的本体以应对建模挑战。在实际应用中，该框架支持如 OKAPI 系统的音视频数据分析、发布和重用，推动视觉叙事和文档再工程，实现更灵活、智能的数字数据协作。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "in French language",
      "pdf_url": "http://arxiv.org/pdf/2504.11459v1",
      "published_date": "2025-01-08 09:15:01 UTC",
      "updated_date": "2025-01-08 09:15:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:38:02.669993"
    },
    {
      "arxiv_id": "2501.06236v1",
      "title": "Data-Driven Radio Propagation Modeling using Graph Neural Networks",
      "title_zh": "基于数据的无线电传播建模使用图神经网络",
      "authors": [
        "Adrien Bufort",
        "Laurent Lebocq",
        "Stefan Cathabard"
      ],
      "abstract": "Modeling radio propagation is essential for wireless network design and\nperformance optimization. Traditional methods rely on physics models of radio\npropagation, which can be inaccurate or inflexible. In this work, we propose\nusing graph neural networks to learn radio propagation behaviors directly from\nreal-world network data. Our approach converts the radio propagation\nenvironment into a graph representation, with nodes corresponding to locations\nand edges representing spatial and ray-tracing relationships between locations.\nThe graph is generated by converting images of the environment into a graph\nstructure, with specific relationships between nodes. The model is trained on\nthis graph representation, using sensor measurements as target data.\n  We demonstrate that the graph neural network, which learns to predict radio\npropagation directly from data, achieves competitive performance compared to\ntraditional heuristic models. This data-driven approach outperforms classic\nnumerical solvers in terms of both speed and accuracy. To the best of our\nknowledge, we are the first to apply graph neural networks to real-world radio\npropagation data to generate coverage maps, enabling generative models of\nsignal propagation with point measurements only.",
      "tldr_zh": "这篇论文提出了一种数据驱动的方法，使用 Graph Neural Networks (GNNs) 从真实网络数据中学习无线电传播行为，以解决传统物理模型的准确性和灵活性问题。方法将无线电传播环境转化为图表示，节点对应位置，边表示空间和光线追踪关系，并通过环境图像生成图结构，然后利用传感器测量数据训练模型。实验结果显示，该 GNN 模型在预测无线电传播方面，速度和准确性均优于经典数值求解器，并首次实现了基于点测量的覆盖地图生成。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06236v1",
      "published_date": "2025-01-08 09:09:50 UTC",
      "updated_date": "2025-01-08 09:09:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:38:14.603881"
    },
    {
      "arxiv_id": "2501.04366v1",
      "title": "DispFormer: Pretrained Transformer for Flexible Dispersion Curve Inversion from Global Synthesis to Regional Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Feng Liu",
        "Bao Deng",
        "Rui Su",
        "Lei Bai",
        "Wanli Ouyang"
      ],
      "abstract": "Surface wave dispersion curve inversion is essential for estimating\nsubsurface Shear-wave velocity ($v_s$), yet traditional methods often struggle\nto balance computational efficiency with inversion accuracy. While deep\nlearning approaches show promise, previous studies typically require large\namounts of labeled data and struggle with real-world datasets that have varying\nperiod ranges, missing data, and low signal-to-noise ratios. This study\nproposes DispFormer, a transformer-based neural network for inverting the $v_s$\nprofile from Rayleigh-wave phase and group dispersion curves. DispFormer\nprocesses dispersion data at each period independently, thereby allowing it to\nhandle data of varying lengths without requiring network modifications or\nalignment between training and testing data. The performance is demonstrated by\npre-training it on a global synthetic dataset and testing it on two regional\nsynthetic datasets using zero-shot and few-shot strategies. Results indicate\nthat zero-shot DispFormer, even without any labeled data, produces inversion\nprofiles that match well with the ground truth, providing a deployable initial\nmodel generator to assist traditional methods. When labeled data is available,\nfew-shot DispFormer outperforms traditional methods with only a small number of\nlabels. Furthermore, real-world tests indicate that DispFormer effectively\nhandles varying length data, and yields lower data residuals than reference\nmodels. These findings demonstrate that DispFormer provides a robust foundation\nmodel for dispersion curve inversion and is a promising approach for broader\napplications.",
      "tldr_zh": "这篇论文提出 DispFormer，一种基于 Transformer 的预训练神经网络，用于从 Rayleigh-wave 相位和群色散曲线灵活反演地下 Shear-wave velocity ($v_s$) 配置文件。DispFormer 通过独立处理每个周期的数据，避免了传统方法在计算效率和准确性间的权衡，并支持处理不同长度、缺失数据或低信噪比的真实数据集。实验在全球合成数据集上预训练后，通过零样本和少样本策略测试于区域数据集，结果显示零样本 DispFormer 即可生成与真实值匹配的配置文件，而少样本训练下其性能优于传统方法。在真实世界应用中，DispFormer 表现出更低的残差，证明其作为稳健基础模型的潜力。",
      "categories": [
        "physics.geo-ph",
        "cs.AI"
      ],
      "primary_category": "physics.geo-ph",
      "comment": "11 pages, 11 figures, related codes and data are available at\n  https://github.com/liufeng2317/DispFormer",
      "pdf_url": "http://arxiv.org/pdf/2501.04366v1",
      "published_date": "2025-01-08 09:08:24 UTC",
      "updated_date": "2025-01-08 09:08:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:38:28.314717"
    },
    {
      "arxiv_id": "2501.06235v2",
      "title": "NextStop: An Improved Tracker For Panoptic LIDAR Segmentation Data",
      "title_zh": "NextStop：用于全景激光雷达分割数据的改进追踪器",
      "authors": [
        "Nirit Alkalay",
        "Roy Orfaig",
        "Ben-Zion Bobrovsky"
      ],
      "abstract": "4D panoptic LiDAR segmentation is essential for scene understanding in\nautonomous driving and robotics, combining semantic and instance segmentation\nwith temporal consistency. Current methods, like 4D-PLS and 4D-STOP, use a\ntracking-by-detection methodology, employing deep learning networks to perform\nsemantic and instance segmentation on each frame. To maintain temporal\nconsistency, large-size instances detected in the current frame are compared\nand associated with instances within a temporal window that includes the\ncurrent and preceding frames. However, their reliance on short-term instance\ndetection, lack of motion estimation, and exclusion of small-sized instances\nlead to frequent identity switches and reduced tracking performance. We address\nthese issues with the NextStop1 tracker, which integrates Kalman filter-based\nmotion estimation, data association, and lifespan management, along with a\ntracklet state concept to improve prioritization. Evaluated using the LiDAR\nSegmentation and Tracking Quality (LSTQ) metric on the SemanticKITTI validation\nset, NextStop demonstrated enhanced tracking performance, particularly for\nsmall-sized objects like people and bicyclists, with fewer ID switches, earlier\ntracking initiation, and improved reliability in complex environments. The\nsource code is available at https://github.com/AIROTAU/NextStop",
      "tldr_zh": "该研究针对4D panoptic LiDAR segmentation中的跟踪问题，提出了一种改进的追踪器NextStop，以提升自动驾驶和机器人领域的场景理解。该方法整合Kalman filter-based运动估计、数据关联和寿命管理，并引入tracklet状态概念，解决了现有方法如4D-PLS和4D-STOP的短视检测局限、缺少运动估计以及对小尺寸实例的忽略问题。在SemanticKITTI验证集上使用LSTQ指标评估，NextStop显著提高了小尺寸物体的跟踪性能，例如减少了行人和骑自行车者的ID切换、更早的跟踪启动，并在复杂环境中表现出更高的可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06235v2",
      "published_date": "2025-01-08 09:08:06 UTC",
      "updated_date": "2025-03-24 21:19:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:38:39.361825"
    },
    {
      "arxiv_id": "2501.04343v1",
      "title": "TimelineKGQA: A Comprehensive Question-Answer Pair Generator for Temporal Knowledge Graphs",
      "title_zh": "TimelineKGQA：针对时序知识",
      "authors": [
        "Qiang Sun",
        "Sirui Li",
        "Du Huynh",
        "Mark Reynolds",
        "Wei Liu"
      ],
      "abstract": "Question answering over temporal knowledge graphs (TKGs) is crucial for\nunderstanding evolving facts and relationships, yet its development is hindered\nby limited datasets and difficulties in generating custom QA pairs. We propose\na novel categorization framework based on timeline-context relationships, along\nwith \\textbf{TimelineKGQA}, a universal temporal QA generator applicable to any\nTKGs. The code is available at: \\url{https://github.com/PascalSun/TimelineKGQA}\nas an open source Python package.",
      "tldr_zh": "时间知识图谱 (TKGs) 的问答对于理解演变的事实和关系至关重要，但现有数据集有限且自定义 QA 对生成困难。论文提出一个基于 timeline-context relationships 的新型分类框架，以及 TimelineKGQA，一个通用的 QA 生成器，可应用于任何 TKGs。该生成器以开源形式提供（代码链接：https://github.com/PascalSun/TimelineKGQA），有助于提升 TKG 问答系统的开发和可访问性。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04343v1",
      "published_date": "2025-01-08 08:30:44 UTC",
      "updated_date": "2025-01-08 08:30:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:38:50.132814"
    },
    {
      "arxiv_id": "2501.12399v1",
      "title": "FinSphere: A Conversational Stock Analysis Agent Equipped with Quantitative Tools based on Real-Time Database",
      "title_zh": "翻译失败",
      "authors": [
        "Shijie Han",
        "Changhai Zhou",
        "Yiqing Shen",
        "Tianning Sun",
        "Yuhua Zhou",
        "Xiaoxia Wang",
        "Zhixiao Yang",
        "Jingshu Zhang",
        "Hongguang Li"
      ],
      "abstract": "Current financial Large Language Models (LLMs) struggle with two critical\nlimitations: a lack of depth in stock analysis, which impedes their ability to\ngenerate professional-grade insights, and the absence of objective evaluation\nmetrics to assess the quality of stock analysis reports. To address these\nchallenges, this paper introduces FinSphere, a conversational stock analysis\nagent, along with three major contributions: (1) Stocksis, a dataset curated by\nindustry experts to enhance LLMs' stock analysis capabilities, (2) AnalyScore,\na systematic evaluation framework for assessing stock analysis quality, and (3)\nFinSphere, an AI agent that can generate high-quality stock analysis reports in\nresponse to user queries. Experiments demonstrate that FinSphere achieves\nsuperior performance compared to both general and domain-specific LLMs, as well\nas existing agent-based systems, even when they are enhanced with real-time\ndata access and few-shot guidance. The integrated framework, which combines\nreal-time data feeds, quantitative tools, and an instruction-tuned LLM, yields\nsubstantial improvements in both analytical quality and practical applicability\nfor real-world stock analysis.",
      "tldr_zh": "这篇论文介绍了 FinSphere，一种对话式股票分析代理，旨在解决当前金融 Large Language Models (LLMs) 在分析深度不足和缺乏客观评估指标的问题。论文的主要贡献包括：(1) Stocksis 数据集，由行业专家策划，用于提升 LLMs 的股票分析能力；(2) AnalyScore 评估框架，一个系统化的方法来评估股票分析报告质量；以及(3) FinSphere 代理本身，能够结合实时数据源、量化工具和指令微调的 LLM 生成高质量报告。实验结果显示，FinSphere 在性能上优于通用和领域特定 LLMs 以及现有代理系统，即使后者有实时数据和少样本指导，从而显著提高了股票分析的实际应用性。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR",
        "q-fin.CP"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.12399v1",
      "published_date": "2025-01-08 07:50:50 UTC",
      "updated_date": "2025-01-08 07:50:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:39:02.607151"
    },
    {
      "arxiv_id": "2501.04315v2",
      "title": "RoRA: Efficient Fine-Tuning of LLM with Reliability Optimization for Rank Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Jun Liu",
        "Zhenglun Kong",
        "Peiyan Dong",
        "Changdi Yang",
        "Xuan Shen",
        "Pu Zhao",
        "Hao Tang",
        "Geng Yuan",
        "Wei Niu",
        "Wenbin Zhang",
        "Xue Lin",
        "Dong Huang",
        "Yanzhi Wang"
      ],
      "abstract": "Fine-tuning helps large language models (LLM) recover degraded information\nand enhance task performance. Although Low-Rank Adaptation (LoRA) is widely\nused and effective for fine-tuning, we have observed that its scaling factor\ncan limit or even reduce performance as the rank size increases. To address\nthis issue, we propose RoRA (Rank-adaptive Reliability Optimization), a simple\nyet effective method for optimizing LoRA's scaling factor. By replacing\n$\\alpha/r$ with $\\alpha/\\sqrt{r}$, RoRA ensures improved performance as rank\nsize increases. Moreover, RoRA enhances low-rank adaptation in fine-tuning\nuncompressed models and excels in the more challenging task of accuracy\nrecovery when fine-tuning pruned models. Extensive experiments demonstrate the\neffectiveness of RoRA in fine-tuning both uncompressed and pruned models. RoRA\nsurpasses the state-of-the-art (SOTA) in average accuracy and robustness on\nLLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B, specifically outperforming LoRA and\nDoRA by 6.5% and 2.9% on LLaMA-7B, respectively. In pruned model fine-tuning,\nRoRA shows significant advantages; for SHEARED-LLAMA-1.3, a LLaMA-7B with 81.4%\npruning, RoRA achieves 5.7% higher average accuracy than LoRA and 3.9% higher\nthan DoRA.",
      "tldr_zh": "该研究提出 RoRA，一种简单有效的优化方法，用于提升 Low-Rank Adaptation (LoRA) 在大型语言模型 (LLM) 微调中的性能，解决 LoRA 缩放因子可能限制准确率的问题。RoRA 通过将缩放因子从 α/r 替换为 α/√r，确保性能随 rank 大小增加而改善，并在微调未压缩模型和修剪模型时表现出色。实验结果显示，RoRA 在 LLaMA-7B/13B、LLaMA2-7B 和 LLaMA3-8B 上超越 SOTA，比 LoRA 和 DoRA 分别提高 6.5% 和 2.9% 的平均准确率，并在修剪模型（如 SHEARED-LLAMA-1.3）上实现显著优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "2025 IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP)",
      "pdf_url": "http://arxiv.org/pdf/2501.04315v2",
      "published_date": "2025-01-08 07:13:52 UTC",
      "updated_date": "2025-01-11 18:17:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:39:15.421501"
    },
    {
      "arxiv_id": "2501.04302v1",
      "title": "H-MBA: Hierarchical MamBa Adaptation for Multi-Modal Video Understanding in Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Siran Chen",
        "Yuxiao Luo",
        "Yue Ma",
        "Yu Qiao",
        "Yali Wang"
      ],
      "abstract": "With the prevalence of Multimodal Large Language Models(MLLMs), autonomous\ndriving has encountered new opportunities and challenges. In particular,\nmulti-modal video understanding is critical to interactively analyze what will\nhappen in the procedure of autonomous driving. However, videos in such a\ndynamical scene that often contains complex spatial-temporal movements, which\nrestricts the generalization capacity of the existing MLLMs in this field. To\nbridge the gap, we propose a novel Hierarchical Mamba Adaptation (H-MBA)\nframework to fit the complicated motion changes in autonomous driving videos.\nSpecifically, our H-MBA consists of two distinct modules, including Context\nMamba (C-Mamba) and Query Mamba (Q-Mamba). First, C-Mamba contains various\ntypes of structure state space models, which can effectively capture\nmulti-granularity video context for different temporal resolutions. Second,\nQ-Mamba flexibly transforms the current frame as the learnable query, and\nattentively selects multi-granularity video context into query. Consequently,\nit can adaptively integrate all the video contexts of multi-scale temporal\nresolutions to enhance video understanding. Via a plug-and-play paradigm in\nMLLMs, our H-MBA shows the remarkable performance on multi-modal video tasks in\nautonomous driving, e.g., for risk object detection, it outperforms the\nprevious SOTA method with 5.5% mIoU improvement.",
      "tldr_zh": "该研究提出H-MBA框架，用于提升多模态大型语言模型(MLLMs)在自动驾驶中的多模态视频理解能力，以应对复杂空间-时间运动的挑战。H-MBA包括Context Mamba (C-Mamba)和Query Mamba (Q-Mamba)两个模块，其中C-Mamba利用各种结构状态空间模型捕获多粒度视频上下文，Q-Mamba则将当前帧转换为可学习查询，并自适应整合多尺度时间分辨率的视频上下文。通过插件式范式应用，H-MBA在自动驾驶任务中表现出色，例如在风险物体检测上，比之前的最先进方法提高了5.5% mIoU。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.04302v1",
      "published_date": "2025-01-08 06:26:16 UTC",
      "updated_date": "2025-01-08 06:26:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:39:26.542722"
    },
    {
      "arxiv_id": "2501.04299v1",
      "title": "Circuit Complexity Bounds for Visual Autoregressive Model",
      "title_zh": "视觉自回归模型的电路复杂度界",
      "authors": [
        "Yekun Ke",
        "Xiaoyu Li",
        "Yingyu Liang",
        "Zhenmei Shi",
        "Zhao Song"
      ],
      "abstract": "Understanding the expressive ability of a specific model is essential for\ngrasping its capacity limitations. Recently, several studies have established\ncircuit complexity bounds for Transformer architecture. Besides, the Visual\nAutoRegressive (VAR) model has risen to be a prominent method in the field of\nimage generation, outperforming previous techniques, such as Diffusion\nTransformers, in generating high-quality images. We investigate the circuit\ncomplexity of the VAR model and establish a bound in this study. Our primary\nresult demonstrates that the VAR model is equivalent to a simulation by a\nuniform $\\mathsf{TC}^0$ threshold circuit with hidden dimension $d \\leq O(n)$\nand $\\mathrm{poly}(n)$ precision. This is the first study to rigorously\nhighlight the limitations in the expressive power of VAR models despite their\nimpressive performance. We believe our findings will offer valuable insights\ninto the inherent constraints of these models and guide the development of more\nefficient and expressive architectures in the future.",
      "tldr_zh": "该研究探讨了Visual Autoregressive (VAR)模型的电路复杂度边界，以揭示其表达能力的限制。尽管VAR模型在图像生成中表现出色，超越了如Diffusion Transformers等方法，但论文首次证明了其等价于uniform $\\mathsf{TC}^0$ threshold circuit的模拟，条件是隐藏维度$d \\leq O(n)$和$\\mathrm{poly}(n)$精度。通过这一分析，研究突出了VAR模型的内在约束，并为开发更高效、更有表现力的架构提供了宝贵见解。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CC",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04299v1",
      "published_date": "2025-01-08 06:07:33 UTC",
      "updated_date": "2025-01-08 06:07:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:39:37.773931"
    },
    {
      "arxiv_id": "2501.04292v2",
      "title": "MADUV: The 1st INTERSPEECH Mice Autism Detection via Ultrasound Vocalization Challenge",
      "title_zh": "翻译失败",
      "authors": [
        "Zijiang Yang",
        "Meishu Song",
        "Xin Jing",
        "Haojie Zhang",
        "Kun Qian",
        "Bin Hu",
        "Kota Tamada",
        "Toru Takumi",
        "Björn W. Schuller",
        "Yoshiharu Yamamoto"
      ],
      "abstract": "The Mice Autism Detection via Ultrasound Vocalization (MADUV) Challenge\nintroduces the first INTERSPEECH challenge focused on detecting autism spectrum\ndisorder (ASD) in mice through their vocalizations. Participants are tasked\nwith developing models to automatically classify mice as either wild-type or\nASD models based on recordings with a high sampling rate. Our baseline system\nemploys a simple CNN-based classification using three different spectrogram\nfeatures. Results demonstrate the feasibility of automated ASD detection, with\nthe considered audible-range features achieving the best performance (UAR of\n0.600 for segment-level and 0.625 for subject-level classification). This\nchallenge bridges speech technology and biomedical research, offering\nopportunities to advance our understanding of ASD models through machine\nlearning approaches. The findings suggest promising directions for vocalization\nanalysis and highlight the potential value of audible and ultrasound\nvocalizations in ASD detection.",
      "tldr_zh": "本论文介绍了MADUV挑战，这是首个INTERSPEECH会议上的竞赛，专注于通过老鼠的超声波vocalization检测自闭症谱系障碍(ASD)。参与者需开发模型基于高采样率录音自动分类老鼠为wild-type或ASD模型，基线系统采用简单的CNN-based分类结合三种spectrogram features。实验结果显示，使用audible-range features取得了最佳性能，segment-level UAR为0.600、subject-level UAR为0.625，证明了自动ASD检测的可行性。该挑战桥接了语音技术和生物医学研究，突显了vocalization分析在ASD理解中的潜力。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "5 pages, 1 figure and 2 tables. For MADUV Challenge 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.04292v2",
      "published_date": "2025-01-08 05:32:55 UTC",
      "updated_date": "2025-01-29 05:35:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:39:50.461284"
    },
    {
      "arxiv_id": "2501.04286v2",
      "title": "Mapping the Edge of Chaos: Fractal-Like Boundaries in The Trainability of Decoder-Only Transformer Models",
      "title_zh": "翻译失败",
      "authors": [
        "Bahman Torkamandi"
      ],
      "abstract": "In the realm of fractal geometry, intricate structures emerge from simple\niterative processes that partition parameter spaces into regions of stability\nand instability. Likewise, training large language models involves iteratively\napplying update functions, such as Adam, where even slight hyperparameter\nadjustments can shift the training process from convergence to divergence.\nRecent evidence from miniature neural networks suggests that the boundary\nseparating these outcomes displays fractal characteristics. Building on these\ninsights, this study extends them to medium-sized, decoder-only transformer\narchitectures by employing a more consistent convergence measure and examining\nthe learning rate hyperparameter landscape for attention and fully connected\nlayers. The results show that the trainability frontier is not a simple\nthreshold; rather, it forms a self-similar yet seemingly random structure at\nmultiple scales, with statistically consistent and repeating patterns. Within\nthis landscape, a region of stable convergence is surrounded by a complex\nchaotic border, illustrating the sensitive nature of the underlying training\ndynamics.",
      "tldr_zh": "本研究探讨了 decoder-only transformer 模型的训练边界，类似于分形几何中参数空间的稳定与不稳定区域，揭示了微小超参数调整（如学习率）可能导致训练从收敛转为发散的现象。研究者扩展了先前小型神经网络的发现，通过更一致的收敛测量，分析中等规模模型中注意力层和全连接层的学习率超参数景观。结果显示，该训练边界并非简单阈值，而是呈现自相似且统计一致的复杂结构，围绕稳定收敛区域形成混沌边界，突显了训练动态的高度敏感性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "15 pages",
      "pdf_url": "http://arxiv.org/pdf/2501.04286v2",
      "published_date": "2025-01-08 05:24:11 UTC",
      "updated_date": "2025-02-15 01:26:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:40:02.601910"
    },
    {
      "arxiv_id": "2501.04283v1",
      "title": "Enhancing Scene Classification in Cloudy Image Scenarios: A Collaborative Transfer Method with Information Regulation Mechanism using Optical Cloud-Covered and SAR Remote Sensing Images",
      "title_zh": "翻译失败",
      "authors": [
        "Yuze Wang",
        "Rong Xiao",
        "Haifeng Li",
        "Mariana Belgiu",
        "Chao Tao"
      ],
      "abstract": "In remote sensing scene classification, leveraging the transfer methods with\nwell-trained optical models is an efficient way to overcome label scarcity.\nHowever, cloud contamination leads to optical information loss and significant\nimpacts on feature distribution, challenging the reliability and stability of\ntransferred target models. Common solutions include cloud removal for optical\ndata or directly using Synthetic aperture radar (SAR) data in the target\ndomain. However, cloud removal requires substantial auxiliary data for support\nand pre-training, while directly using SAR disregards the unobstructed portions\nof optical data. This study presents a scene classification transfer method\nthat synergistically combines multi-modality data, which aims to transfer the\nsource domain model trained on cloudfree optical data to the target domain that\nincludes both cloudy optical and SAR data at low cost. Specifically, the\nframework incorporates two parts: (1) the collaborative transfer strategy,\nbased on knowledge distillation, enables the efficient prior knowledge transfer\nacross heterogeneous data; (2) the information regulation mechanism (IRM) is\nproposed to address the modality imbalance issue during transfer. It employs\nauxiliary models to measure the contribution discrepancy of each modality, and\nautomatically balances the information utilization of modalities during the\ntarget model learning process at the sample-level. The transfer experiments\nwere conducted on simulated and real cloud datasets, demonstrating the superior\nperformance of the proposed method compared to other solutions in cloud-covered\nscenarios. We also verified the importance and limitations of IRM, and further\ndiscussed and visualized the modality imbalance problem during the model\ntransfer. Codes are available at https://github.com/wangyuze-csu/ESCCS",
      "tldr_zh": "本文提出了一种协作转移方法，用于提升云覆盖场景下的遥感图像场景分类性能，该方法将云覆盖光学图像和SAR数据相结合，从无云光学数据训练的源模型转移到目标域，以低成本解决光学信息丢失问题。具体而言，该框架包括基于知识蒸馏的协作转移策略和信息调节机制(IRM)，后者通过辅助模型评估各模态贡献差异，并在样本级别自动平衡模态信息利用。在模拟和真实云数据集上的实验显示，该方法比其他方案表现出色，提高了分类准确性，并验证了IRM在处理模态不平衡问题中的重要性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04283v1",
      "published_date": "2025-01-08 05:14:36 UTC",
      "updated_date": "2025-01-08 05:14:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:40:14.339033"
    },
    {
      "arxiv_id": "2501.04266v2",
      "title": "Scaling Large Language Model Training on Frontier with Low-Bandwidth Partitioning",
      "title_zh": "在 Frontier 上使用低带宽分区扩展大型语言模型训练",
      "authors": [
        "Lang Xu",
        "Quentin Anthony",
        "Jacob Hatef",
        "Aamir Shafi",
        "Hari Subramoni",
        "Dhabaleswar K.",
        "Panda"
      ],
      "abstract": "Scaling up Large Language Model(LLM) training involves fitting a tremendous\namount of training parameters across a limited number of workers. However,\nmethods like ZeRO-3 that drastically reduce GPU memory pressure often incur\nheavy communication to ensure global synchronization and consistency.\nEstablished efforts such as ZeRO++ use secondary partitions to avoid inter-node\ncommunications, given that intra-node GPU-GPU transfer generally has more\nbandwidth and lower latency than inter-node connections. However, as more\ncapable infrastructure like Frontier, equipped with AMD GPUs, emerged with\nimpressive computing capability, there is a need for investigations on the\nhardware topology and to develop targeted strategies to improve training\nefficiency. In this work, we propose a collection of communication and\noptimization strategies for ZeRO++ to reduce communication costs and improve\nmemory utilization. In this paper, we propose a 3-level hierarchical\npartitioning specifically for the current 2nd ranked supercomputing cluster,\nFrontier, which aims at leveraging various bandwidths across layers of\ncommunications (GCD-GCD, GPU-GPU, and inter-node) to reduce communication\noverhead. For a 20B GPT model, we observe a 1.71x increase in TFLOPS per GPU\nwhen compared with ZeRO++ up to 384 GCDs and a scaling efficiency of 0.94 for\nup to 384 GCDs.",
      "tldr_zh": "这篇论文针对大规模Large Language Model (LLM)训练中的通信开销问题，提出了一系列优化策略，包括对ZeRO++的改进，以减少通信成本并提升内存利用。作者设计了一个3级分层分区方法，专门适应Frontier超算集群的硬件拓扑，利用不同层通信带宽（如GCD-GCD、GPU-GPU和节点间）来降低整体开销。对于20B GPT模型，实验结果显示，与ZeRO++相比，TFLOPS每GPU提高了1.71倍，并在384 GCDs上实现了0.94的扩展效率。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "Added references and clarifications",
      "pdf_url": "http://arxiv.org/pdf/2501.04266v2",
      "published_date": "2025-01-08 04:19:57 UTC",
      "updated_date": "2025-02-04 04:32:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:40:27.667187"
    },
    {
      "arxiv_id": "2501.04263v1",
      "title": "KN-LIO: Geometric Kinematics and Neural Field Coupled LiDAR-Inertial Odometry",
      "title_zh": "KN-LIO：几何运动学与神经场耦合的激光雷达-惯性里程计",
      "authors": [
        "Zhong Wang",
        "Lele Ren",
        "Yue Wen",
        "Hesheng Wang"
      ],
      "abstract": "Recent advancements in LiDAR-Inertial Odometry (LIO) have boosted a large\namount of applications. However, traditional LIO systems tend to focus more on\nlocalization rather than mapping, with maps consisting mostly of sparse\ngeometric elements, which is not ideal for downstream tasks. Recent emerging\nneural field technology has great potential in dense mapping, but pure LiDAR\nmapping is difficult to work on high-dynamic vehicles. To mitigate this\nchallenge, we present a new solution that tightly couples geometric kinematics\nwith neural fields to enhance simultaneous state estimation and dense mapping\ncapabilities. We propose both semi-coupled and tightly coupled Kinematic-Neural\nLIO (KN-LIO) systems that leverage online SDF decoding and iterated error-state\nKalman filtering to fuse laser and inertial data. Our KN-LIO minimizes\ninformation loss and improves accuracy in state estimation, while also\naccommodating asynchronous multi-LiDAR inputs. Evaluations on diverse\nhigh-dynamic datasets demonstrate that our KN-LIO achieves performance on par\nwith or superior to existing state-of-the-art solutions in pose estimation and\noffers improved dense mapping accuracy over pure LiDAR-based methods. The\nrelevant code and datasets will be made available at https://**.",
      "tldr_zh": "本文提出 KN-LIO 系统，将几何运动学和 neural field 技术紧密耦合，旨在解决传统 LiDAR-Inertial Odometry (LIO) 在定位和密集映射方面的局限性。该系统包括半耦合和紧密耦合版本，通过在线 SDF 解码和迭代误差状态 Kalman filtering 融合激光和惯性数据，支持异步多-LiDAR 输入，从而最小化信息损失并提高准确性。在高动态数据集上的评估显示，KN-LIO 在姿态估计性能上与现有最先进方法相当或优于它们，并在密集映射精度上超越纯 LiDAR 方法。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "eess.SP"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04263v1",
      "published_date": "2025-01-08 04:14:09 UTC",
      "updated_date": "2025-01-08 04:14:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:40:39.526033"
    },
    {
      "arxiv_id": "2501.04253v1",
      "title": "Integrated Offline and Online Learning to Solve a Large Class of Scheduling Problems",
      "title_zh": "翻译失败",
      "authors": [
        "Anbang Liu",
        "Zhi-Long Chen",
        "Jinyang Jiang",
        "Xi Chen"
      ],
      "abstract": "In this paper, we develop a unified machine learning (ML) approach to predict\nhigh-quality solutions for single-machine scheduling problems with a\nnon-decreasing min-sum objective function with or without release times. Our ML\napproach is novel in three major aspects. First, our approach is developed for\nthe entire class of the aforementioned problems. To achieve this, we exploit\nthe fact that the entire class of the problems considered can be formulated as\na time-indexed formulation in a unified manner. We develop a deep neural\nnetwork (DNN) which uses the cost parameters in the time-indexed formulation as\nthe inputs to effectively predict a continuous solution to this formulation,\nbased on which a feasible discrete solution is easily constructed. The second\nnovel aspect of our approach lies in how the DNN model is trained. In view of\nthe NP-hard nature of the problems, labels (i.e., optimal solutions) are hard\nto generate for training. To overcome this difficulty, we generate and utilize\na set of special instances, for which optimal solutions can be found with\nlittle computational effort, to train the ML model offline. The third novel\nidea we employ in our approach is that we develop an online single-instance\nlearning approach to fine tune the parameters in the DNN for a given online\ninstance, with the goal of generating an improved solution for the given\ninstance. To this end, we develop a feasibility surrogate that approximates the\nobjective value of a given instance as a continuous function of the outputs of\nthe DNN, which then enables us to derive gradients and update the learnable\nparameters in the DNN. Numerical results show that our approach can efficiently\ngenerate high-quality solutions for a variety of single-machine scheduling\nmin-sum problems with up to 1000 jobs.",
      "tldr_zh": "本论文提出了一种整合离线和在线学习的机器学习(ML)方法，用于预测单机调度问题的高质量解决方案，这些问题涉及非递减min-sum目标函数，并可能包括发布时间。\n该方法利用统一的基于时间的公式化，通过深度神经网络(DNN)以成本参数作为输入，预测连续解决方案并构建可行离散解决方案。\n为了应对NP-hard问题的训练挑战，论文采用特殊实例进行离线训练，并开发在线单实例学习来微调DNN参数，使用可行性代理近似目标值以优化输出。\n实验结果表明，该方法能高效生成高质量解决方案，适用于各种单机调度问题，最多支持1000个作业。",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "math.OC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04253v1",
      "published_date": "2025-01-08 03:35:28 UTC",
      "updated_date": "2025-01-08 03:35:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:40:50.700289"
    },
    {
      "arxiv_id": "2501.09026v1",
      "title": "Intelligent Anti-Money Laundering Solution Based upon Novel Community Detection in Massive Transaction Networks on Spark",
      "title_zh": "基于新型社区检测的大规模交易网络上 Spark 的智能反洗钱解决方案",
      "authors": [
        "Xurui Li",
        "Xiang Cao",
        "Xuetao Qiu",
        "Jintao Zhao",
        "Jianbin Zheng"
      ],
      "abstract": "Criminals are using every means available to launder the profits from their\nillegal activities into ostensibly legitimate assets. Meanwhile, most\ncommercial anti-money laundering systems are still rule-based, which cannot\nadapt to the ever-changing tricks. Although some machine learning methods have\nbeen proposed, they are mainly focused on the perspective of abnormal behavior\nfor single accounts. Considering money laundering activities are often involved\nin gang criminals, these methods are still not intelligent enough to crack down\non criminal gangs all-sidedly. In this paper, a systematic solution is\npresented to find suspicious money laundering gangs. A temporal-directed\nLouvain algorithm has been proposed to detect communities according to relevant\nanti-money laundering patterns. All processes are implemented and optimized on\nSpark platform. This solution can greatly improve the efficiency of anti-money\nlaundering work for financial regulation agencies.",
      "tldr_zh": "本论文针对现有反洗钱系统（如基于规则的方法）无法适应变化且忽略犯罪团伙的局限，提出了一种智能解决方案，用于在海量交易网络中识别可疑洗钱团伙。研究开发了 temporal-directed Louvain 算法，该算法结合相关反洗钱模式来检测社区，并考虑交易的时序和方向性。所有过程在 Spark 平台上实现和优化，以提升处理效率。该解决方案能全面打击洗钱团伙，大大提高金融监管机构的反洗钱工作效能。",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.SI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.09026v1",
      "published_date": "2025-01-08 02:57:08 UTC",
      "updated_date": "2025-01-08 02:57:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:41:02.045622"
    },
    {
      "arxiv_id": "2501.06231v1",
      "title": "Sustainable and Intelligent Public Facility Failure Management System Based on Large Language Models",
      "title_zh": "基于大语言模型的可持续智能公共设施故障管理系统",
      "authors": [
        "Siguo Bi",
        "Jilong Zhang",
        "Wei Ni"
      ],
      "abstract": "This paper presents a new Large Language Model (LLM)-based Smart Device\nManagement framework, a pioneering approach designed to address the intricate\nchallenges of managing intelligent devices within public facilities, with a\nparticular emphasis on applications to libraries. Our framework leverages\nstate-of-the-art LLMs to analyze and predict device failures, thereby enhancing\noperational efficiency and reliability. Through prototype validation in\nreal-world library settings, we demonstrate the framework's practical\napplicability and its capacity to significantly reduce budgetary constraints on\npublic facilities. The advanced and innovative nature of our model is evident\nfrom its successful implementation in prototype testing. We plan to extend the\nframework's scope to include a wider array of public facilities and to\nintegrate it with cutting-edge cybersecurity technologies, such as Internet of\nThings (IoT) security and machine learning algorithms for threat detection and\nresponse. This will result in a comprehensive and proactive maintenance system\nthat not only bolsters the security of intelligent devices but also utilizes\nmachine learning for automated analysis and real-time threat mitigation. By\nincorporating these advanced cybersecurity elements, our framework will be\nwell-positioned to tackle the dynamic challenges of modern public\ninfrastructure, ensuring robust protection against potential threats and\nenabling facilities to anticipate and prevent failures, leading to substantial\ncost savings and enhanced service quality.",
      "tldr_zh": "本研究提出了一种基于 Large Language Models (LLMs) 的可持续智能公共设施故障管理系统框架，旨在解决公共设施（如图书馆）中智能设备管理的复杂挑战。该框架利用先进的 LLMs 分析和预测设备故障，提高运营效率和可靠性，并通过真实图书馆环境的原型验证，证明其能显著降低预算压力。实验结果显示，该系统在实际应用中表现出色，未来计划扩展至更多公共设施，并整合 Internet of Things (IoT) 安全和机器学习算法，实现全面的网络安全维护和实时威胁响应，从而提升公共基础设施的整体防护水平。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.06231v1",
      "published_date": "2025-01-08 02:30:37 UTC",
      "updated_date": "2025-01-08 02:30:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:41:13.998226"
    },
    {
      "arxiv_id": "2501.04228v2",
      "title": "Constraints as Rewards: Reinforcement Learning for Robots without Reward Functions",
      "title_zh": "约束作为奖励：没有奖励函数的机器人强化学习",
      "authors": [
        "Yu Ishihara",
        "Noriaki Takasugi",
        "Kotaro Kawakami",
        "Masaya Kinoshita",
        "Kazumi Aoyama"
      ],
      "abstract": "Reinforcement learning has become an essential algorithm for generating\ncomplex robotic behaviors. However, to learn such behaviors, it is necessary to\ndesign a reward function that describes the task, which often consists of\nmultiple objectives that needs to be balanced. This tuning process is known as\nreward engineering and typically involves extensive trial-and-error. In this\npaper, to avoid this trial-and-error process, we propose the concept of\nConstraints as Rewards (CaR). CaR formulates the task objective using multiple\nconstraint functions instead of a reward function and solves a reinforcement\nlearning problem with constraints using the Lagrangian-method. By adopting this\napproach, different objectives are automatically balanced, because Lagrange\nmultipliers serves as the weights among the objectives. In addition, we will\ndemonstrate that constraints, expressed as inequalities, provide an intuitive\ninterpretation of the optimization target designed for the task. We apply the\nproposed method to the standing-up motion generation task of a\nsix-wheeled-telescopic-legged robot and demonstrate that the proposed method\nsuccessfully acquires the target behavior, even though it is challenging to\nlearn with manually designed reward functions.",
      "tldr_zh": "该论文提出了一种名为Constraints as Rewards (CaR)的强化学习方法，旨在解决机器人行为学习中传统奖励函数设计所需的繁琐试错问题。CaR使用多个约束函数代替奖励函数，并通过Lagrangian-method解决带约束的强化学习问题，使Lagrange multipliers自动平衡不同目标，提供直观的优化解释。实验结果显示，该方法成功应用于六轮伸缩腿机器人的站立动作生成任务，实现了高效的行为学习，而手动设计的奖励函数难以达到类似效果。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04228v2",
      "published_date": "2025-01-08 01:59:47 UTC",
      "updated_date": "2025-01-09 01:35:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:41:25.638778"
    },
    {
      "arxiv_id": "2501.04227v1",
      "title": "Agent Laboratory: Using LLM Agents as Research Assistants",
      "title_zh": "Agent Laboratory: 使用 LLM 代理作为研究助理",
      "authors": [
        "Samuel Schmidgall",
        "Yusheng Su",
        "Ze Wang",
        "Ximeng Sun",
        "Jialian Wu",
        "Xiaodong Yu",
        "Jiang Liu",
        "Zicheng Liu",
        "Emad Barsoum"
      ],
      "abstract": "Historically, scientific discovery has been a lengthy and costly process,\ndemanding substantial time and resources from initial conception to final\nresults. To accelerate scientific discovery, reduce research costs, and improve\nresearch quality, we introduce Agent Laboratory, an autonomous LLM-based\nframework capable of completing the entire research process. This framework\naccepts a human-provided research idea and progresses through three\nstages--literature review, experimentation, and report writing to produce\ncomprehensive research outputs, including a code repository and a research\nreport, while enabling users to provide feedback and guidance at each stage. We\ndeploy Agent Laboratory with various state-of-the-art LLMs and invite multiple\nresearchers to assess its quality by participating in a survey, providing human\nfeedback to guide the research process, and then evaluate the final paper. We\nfound that: (1) Agent Laboratory driven by o1-preview generates the best\nresearch outcomes; (2) The generated machine learning code is able to achieve\nstate-of-the-art performance compared to existing methods; (3) Human\ninvolvement, providing feedback at each stage, significantly improves the\noverall quality of research; (4) Agent Laboratory significantly reduces\nresearch expenses, achieving an 84% decrease compared to previous autonomous\nresearch methods. We hope Agent Laboratory enables researchers to allocate more\neffort toward creative ideation rather than low-level coding and writing,\nultimately accelerating scientific discovery.",
      "tldr_zh": "本研究提出 Agent Laboratory，这是一个基于 LLM（大型语言模型）的自主框架，旨在加速科学发现、降低研究成本并提升质量。该框架从人类提供的研发想法出发，通过文献综述、实验和报告写作三个阶段自动生成完整的输出，包括代码仓库和研究报告，同时允许用户在每个阶段提供反馈。实验结果显示，使用 o1-preview 驱动的框架产生最佳研究成果，生成的机器学习代码可达到最先进性能，且人类反馈显著提高了整体质量，同时将研究费用降低了 84%。总体而言，Agent Laboratory 有望让研究者更多专注于创意构想，从而推动科学发现的进程。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04227v1",
      "published_date": "2025-01-08 01:58:42 UTC",
      "updated_date": "2025-01-08 01:58:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:41:39.677760"
    },
    {
      "arxiv_id": "2501.04217v1",
      "title": "Continual Self-supervised Learning Considering Medical Domain Knowledge in Chest CT Images",
      "title_zh": "翻译失败",
      "authors": [
        "Ren Tasai",
        "Guang Li",
        "Ren Togo",
        "Minghui Tang",
        "Takaaki Yoshimura",
        "Hiroyuki Sugimori",
        "Kenji Hirata",
        "Takahiro Ogawa",
        "Kohsuke Kudo",
        "Miki Haseyama"
      ],
      "abstract": "We propose a novel continual self-supervised learning method (CSSL)\nconsidering medical domain knowledge in chest CT images. Our approach addresses\nthe challenge of sequential learning by effectively capturing the relationship\nbetween previously learned knowledge and new information at different stages.\nBy incorporating an enhanced DER into CSSL and maintaining both diversity and\nrepresentativeness within the rehearsal buffer of DER, the risk of data\ninterference during pretraining is reduced, enabling the model to learn more\nricher and robust feature representations. In addition, we incorporate a mixup\nstrategy and feature distillation to further enhance the model's ability to\nlearn meaningful representations. We validate our method using chest CT images\nobtained under two different imaging conditions, demonstrating superior\nperformance compared to state-of-the-art methods.",
      "tldr_zh": "本文提出了一种新型持续自监督学习方法（CSSL），考虑医疗领域知识，应用于胸部 CT 图像，以有效捕捉先前知识和新信息之间的关系。该方法通过增强的 DER 技术、维护缓冲区的多样性和代表性、mixup 策略以及特征蒸馏，减少预训练中的数据干扰，并提升模型的鲁棒特征表示能力。在两种不同成像条件下的实验验证中，CSSL 表现出优于现有最先进方法的性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.04217v1",
      "published_date": "2025-01-08 01:27:35 UTC",
      "updated_date": "2025-01-08 01:27:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:41:50.120900"
    },
    {
      "arxiv_id": "2501.04213v1",
      "title": "UPAQ: A Framework for Real-Time and Energy-Efficient 3D Object Detection in Autonomous Vehicles",
      "title_zh": "UPAQ: 一种用于",
      "authors": [
        "Abhishek Balasubramaniam",
        "Febin P Sunny",
        "Sudeep Pasricha"
      ],
      "abstract": "To enhance perception in autonomous vehicles (AVs), recent efforts are\nconcentrating on 3D object detectors, which deliver more comprehensive\npredictions than traditional 2D object detectors, at the cost of increased\nmemory footprint and computational resource usage. We present a novel framework\ncalled UPAQ, which leverages semi-structured pattern pruning and quantization\nto improve the efficiency of LiDAR point-cloud and camera-based 3D object\ndetectors on resource-constrained embedded AV platforms. Experimental results\non the Jetson Orin Nano embedded platform indicate that UPAQ achieves up to\n5.62x and 5.13x model compression rates, up to 1.97x and 1.86x boost in\ninference speed, and up to 2.07x and 1.87x reduction in energy consumption\ncompared to state-of-the-art model compression frameworks, on the Pointpillar\nand SMOKE models respectively.",
      "tldr_zh": "本研究提出 UPAQ 框架，旨在提升自动驾驶车辆中 3D 对象检测的实时性和能效，通过 semi-structured pattern pruning 和 quantization 技术优化 LiDAR point-cloud 和 camera-based 检测器，以适应资源受限的嵌入式平台。实验结果显示，在 Jetson Orin Nano 上，UPAQ 相比现有框架使 Pointpillar 模型实现 5.62x 压缩率、1.97x 推理速度提升和 2.07x 能耗减少，而 SMOKE 模型分别达到 5.13x、1.86x 和 1.87x 的改善。该框架为高效的 3D 对象检测提供了实用解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04213v1",
      "published_date": "2025-01-08 01:18:14 UTC",
      "updated_date": "2025-01-08 01:18:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:42:04.460515"
    },
    {
      "arxiv_id": "2501.04211v2",
      "title": "CURing Large Models: Compression via CUR Decomposition",
      "title_zh": "CURing 大型模型：通过 CUR 分解",
      "authors": [
        "Sanghyeon Park",
        "Soo-Mook Moon"
      ],
      "abstract": "Large deep learning models have achieved remarkable success but are\nresource-intensive, posing challenges such as memory usage. We introduce\nCURing, a novel model compression method based on CUR matrix decomposition,\nwhich approximates weight matrices as the product of selected columns (C) and\nrows (R), and a small linking matrix (U). We apply this decomposition to\nweights chosen based on the combined influence of their magnitudes and\nactivations. By identifying and retaining informative rows and columns, CURing\nsignificantly reduces model size with minimal performance loss. For example, it\nreduces Llama3.1-8B's parameters to 7.32B (-9%) in just 129 seconds, over 20\ntimes faster than prior compression methods.",
      "tldr_zh": "这篇论文提出了 CURing，一种基于 CUR 矩阵分解的模型压缩方法，旨在解决大型深度学习模型的高内存消耗问题。CURing 通过选择权重矩阵中基于幅度和激活影响的关键行 (C) 和列 (R)，并结合一个小型连接矩阵 (U)，实现高效的权重近似，从而显著减少模型参数，同时保持性能最小损失。例如，该方法将 Llama3.1-8B 的参数从 8B 压缩到 7.32B（减少 9%），仅用 129 秒，比现有压缩方法快 20 倍。总的来说，CURing 为资源受限环境下的模型部署提供了快速且有效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.04211v2",
      "published_date": "2025-01-08 01:11:17 UTC",
      "updated_date": "2025-01-10 14:36:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:44:08.555547"
    },
    {
      "arxiv_id": "2501.04202v1",
      "title": "Generative Dataset Distillation Based on Self-knowledge Distillation",
      "title_zh": "翻译失败",
      "authors": [
        "Longzhen Li",
        "Guang Li",
        "Ren Togo",
        "Keisuke Maeda",
        "Takahiro Ogawa",
        "Miki Haseyama"
      ],
      "abstract": "Dataset distillation is an effective technique for reducing the cost and\ncomplexity of model training while maintaining performance by compressing large\ndatasets into smaller, more efficient versions. In this paper, we present a\nnovel generative dataset distillation method that can improve the accuracy of\naligning prediction logits. Our approach integrates self-knowledge distillation\nto achieve more precise distribution matching between the synthetic and\noriginal data, thereby capturing the overall structure and relationships within\nthe data. To further improve the accuracy of alignment, we introduce a\nstandardization step on the logits before performing distribution matching,\nensuring consistency in the range of logits. Through extensive experiments, we\ndemonstrate that our method outperforms existing state-of-the-art methods,\nresulting in superior distillation performance.",
      "tldr_zh": "本文提出了一种基于自知识蒸馏（self-knowledge distillation）的生成式数据集蒸馏（generative dataset distillation）方法，旨在通过更精确的预测 logits 对齐来压缩大型数据集，同时保持模型性能。该方法整合自知识蒸馏技术，实现合成数据与原始数据的分布匹配，并引入 logits 标准化步骤以确保一致性和准确性。通过广泛实验，证明该方法优于现有最先进方法，在数据集蒸馏性能上取得了显著提升。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICASSP 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.04202v1",
      "published_date": "2025-01-08 00:43:31 UTC",
      "updated_date": "2025-01-08 00:43:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:42:26.448768"
    },
    {
      "arxiv_id": "2501.04193v1",
      "title": "GNN-based Decentralized Perception in Multirobot Systems for Predicting Worker Actions",
      "title_zh": "翻译失败",
      "authors": [
        "Ali Imran",
        "Giovanni Beltrame",
        "David St-Onge"
      ],
      "abstract": "In industrial environments, predicting human actions is essential for\nensuring safe and effective collaboration between humans and robots. This paper\nintroduces a perception framework that enables mobile robots to understand and\nshare information about human actions in a decentralized way. The framework\nfirst allows each robot to build a spatial graph representing its surroundings,\nwhich it then shares with other robots. This shared spatial data is combined\nwith temporal information to track human behavior over time. A swarm-inspired\ndecision-making process is used to ensure all robots agree on a unified\ninterpretation of the human's actions. Results show that adding more robots and\nincorporating longer time sequences improve prediction accuracy. Additionally,\nthe consensus mechanism increases system resilience, making the multi-robot\nsetup more reliable in dynamic industrial settings.",
      "tldr_zh": "这篇论文提出了一种基于 GNN (Graph Neural Networks) 的去中心化感知框架，用于多机器人系统预测工业环境中工人的动作，以确保人类和机器人安全协作。框架允许每个机器人构建并共享表示周围环境的空间图，并将这些数据与时间信息结合，跟踪人类行为，同时采用受群体启发的决策过程来实现机器人间的共识机制。实验结果表明，增加机器人数量和使用更长的时间序列可提高预测准确性，且共识机制增强了系统在动态工业场景中的可靠性和弹性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.RO",
      "comment": "Submitted to RA-L",
      "pdf_url": "http://arxiv.org/pdf/2501.04193v1",
      "published_date": "2025-01-08 00:06:38 UTC",
      "updated_date": "2025-01-08 00:06:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T21:42:39.042577"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 78,
  "processed_papers_count": 78,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T21:44:23.958697"
}
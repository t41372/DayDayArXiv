{
  "date": "2024-07-24",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-07-24 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦 AI 模型优化（如 LLM 的安全性和知识追踪）、医疗诊断（如败血症预测和图像分析）、计算机视觉（如图像生成和异常检测）等领域，亮点包括 Yejin Choi 等知名学者在 LLM hallucination 研究中的贡献，以及高效的医疗 AI 框架，强调了 AI 在实际应用中的鲁棒性和可解释性。\n\n### 重点论文讨论\n我们先聊聊那些重要、可能有话题度的论文，包括 AI 安全、医疗诊断和计算机视觉领域的内容，这些论文涉及核心创新和实际影响。其他次要论文（如纯理论数学或小众方法）将快速掠过。\n\n**1. Spiking Neural Networks in Vertical Federated Learning: Performance Trade-offs（脉冲神经网络在垂直联邦学习中的性能权衡）**  \n这篇论文探讨了脉冲神经网络 (SNNs) 在垂直联邦学习 (VFL) 中的应用，相比传统人工神经网络 (ANNs)，SNNs 在 VFL 中实现了可比的准确率但更节能。主要贡献是评估了 VFL 架构的隐私和性能权衡，使用 CIFAR-10 和 CIFAR-100 数据集证明了 SNNs 的高效性。\n\n**2. WildHallucinations: Evaluating Long-form Factuality in LLMs with Real-World Entity Queries（WildHallucinations: 使用真实实体查询评估 LLM 的长形式事实性）**  \nYejin Choi 等知名学者主导，这篇论文评估了大型语言模型 (LLMs) 在真实实体查询中的 hallucination 问题，使用了一个新基准数据集发现 LLM 在无维基页实体上更容易出错。主要发现是添加检索组件只能略微减少 hallucination，但无法完全消除。\n\n**3. FLRT: Fluent Student-Teacher Redteaming（FLRT: 流畅的学生-教师红队测试）**  \n论文提出了一种改进的红队测试方法，用于 jailbreak 安全调整的 LLM，通过蒸馏和惩罚机制生成更流畅的攻击提示。主要贡献是提升了攻击成功率（如在 Llama-2-7B 上超过 93%），并保持了较低的 perplexity，适用于 LLM 安全评估。\n\n**4. A Large Encoder-Decoder Family of Foundation Models For Chemical Language（一种用于化学语言的大型编码器-解码器基础模型家族）**  \n这篇论文引入了一个在 PubChem 数据上预训练的大型化学语言模型，支持量子属性预测和分子生成。主要发现是该模型在多个基准数据集上实现了 state-of-the-art 性能，并展示了嵌入空间的 few-shot 学习能力。\n\n**5. CoMoTo: Unpaired Cross-Modal Lesion Distillation Improves Breast Lesion Detection in Tomosynthesis（CoMoTo: 无配对跨模态病变蒸馏改善断层合成乳房病变检测）**  \n论文提出 CoMoTo 框架，使用无配对乳房 X 光数据提升断层合成 (DBT) 中的病变检测。主要贡献是通过 Lesion-specific Knowledge Distillation (LsKD) 和 Intra-modal Point Alignment (ImPA) 提高了 7% 的敏感度，适用于数据稀缺的医疗场景。\n\n**6. Quality Assured: Rethinking Annotation Strategies in Imaging AI（质量保证: 重新思考图像 AI 中的标注策略）**  \n这篇论文研究了标注公司和众包平台的质量保证 (QA) 过程，发现改进标注指令比内部 QA 更有效。主要发现是基于 57,648 张图像的实验，强调了高效标注对 AI 基准的影响。\n\n**7. SMA-Hyper: Spatiotemporal Multi-View Fusion Hypergraph Learning for Traffic Accident Prediction（SMA-Hyper: 用于交通事故预测的空间-时间多视图融合超图学习）**  \n论文引入 SMA-Hyper 模型，通过超图学习和对比学习处理城市数据稀疏性，提高了交通事故预测准确率。主要贡献是在伦敦数据集上超越基线模型，突出了多视图融合在城市管理中的潜力。\n\n**8. HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation（HumanVid: 揭开相机可控人类图像动画训练数据的奥秘）**  \n这篇论文发布 HumanVid 数据集，并提出 CamAnimate 基线模型，用于文本引导的人类图像动画。主要发现是数据集提升了相机和动作控制的性能，适用于视频生成任务。\n\n**9. I Could've Asked That: Reformulating Unanswerable Questions（I Could've Asked That: 重述无法回答的问题）**  \n论文使用 CouldAsk 数据集评估 LLM 重述无法回答的问题能力，发现 GPT-4 和 Llama2-7B 的成功率仅为 26% 和 12%。主要贡献是揭示了 LLM 在问题重述中的局限性。\n\n**10. CityX: Controllable Procedural Content Generation for Unbounded 3D Cities（CityX: 用于无限 3D 城市的可控程序内容生成）**  \n这篇论文提出 CityX 框架，使用多代理系统从 OSM 和卫星图像生成真实 3D 城市场景。主要发现是它支持可控生成和实时模拟，适用于自主代理训练。\n\n其他论文如时间序列分析（第37、44）、强化学习（第50）和图像压缩（第59）等，虽然有技术贡献，但相对次要，我们快速掠过：例如，第37篇在连续时间 LQ 强化学习中实现了亚线性遗憾；第59篇优化了文本引导图像压缩以适应大型视觉语言模型。这些论文在特定领域有进展，但影响力较小，不展开讨论。\n\n总之，今天的论文突出了 AI 在安全、医疗和视觉领域的潜力，建议读者关注 LLM hallucination 和医疗诊断相关工作，以获取前沿洞见。更多细节可查阅 arXiv。",
  "papers": [
    {
      "arxiv_id": "2407.17673v2",
      "title": "CRASAR-U-DROIDs: A Large Scale Benchmark Dataset for Building Alignment and Damage Assessment in Georectified sUAS Imagery",
      "title_zh": "翻译失败",
      "authors": [
        "Thomas Manzini",
        "Priyankari Perali",
        "Raisa Karnik",
        "Robin Murphy"
      ],
      "abstract": "This document presents the Center for Robot Assisted Search And Rescue -\nUncrewed Aerial Systems - Disaster Response Overhead Inspection Dataset\n(CRASAR-U-DROIDs) for building damage assessment and spatial alignment\ncollected from small uncrewed aerial systems (sUAS) geospatial imagery. This\ndataset is motivated by the increasing use of sUAS in disaster response and the\nlack of previous work in utilizing high-resolution geospatial sUAS imagery for\nmachine learning and computer vision models, the lack of alignment with\noperational use cases, and with hopes of enabling further investigations\nbetween sUAS and satellite imagery. The CRASAR-U-DRIODs dataset consists of\nfifty-two (52) orthomosaics from ten (10) federally declared disasters\n(Hurricane Ian, Hurricane Ida, Hurricane Harvey, Hurricane Idalia, Hurricane\nLaura, Hurricane Michael, Musset Bayou Fire, Mayfield Tornado, Kilauea\nEruption, and Champlain Towers Collapse) spanning 67.98 square kilometers\n(26.245 square miles), containing 21,716 building polygons and damage labels,\nand 7,880 adjustment annotations. The imagery was tiled and presented in\nconjunction with overlaid building polygons to a pool of 130 annotators who\nprovided human judgments of damage according to the Joint Damage Scale. These\nannotations were then reviewed via a two-stage review process in which building\npolygon damage labels were first reviewed individually and then again by\ncommittee. Additionally, the building polygons have been aligned spatially to\nprecisely overlap with the imagery to enable more performant machine learning\nmodels to be trained. It appears that CRASAR-U-DRIODs is the largest labeled\ndataset of sUAS orthomosaic imagery.",
      "tldr_zh": "该论文介绍了CRASAR-U-DROIDs数据集，这是一个大规模基准，用于在地理校正的sUAS图像中进行建筑对齐和损伤评估，旨在支持灾害响应中的机器学习和计算机视觉应用。该数据集由10个联邦宣布的灾害（如Hurricane Ian和Mayfield Tornado）收集，共包括52个orthomosaics，覆盖67.98平方公里，包含21,716个建筑多边形、损伤标签和7,880个调整注释。图像通过130名注释者使用Joint Damage Scale进行标注，并经两阶段审查过程确保准确性，同时实现了建筑多边形的空间对齐，以提升模型性能。该数据集作为最大的sUAS orthomosaic图像标注资源，有望推动sUAS与卫星图像的进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "16 Pages, 7 Figures, 6 Tables",
      "pdf_url": "http://arxiv.org/pdf/2407.17673v2",
      "published_date": "2024-07-24 23:39:10 UTC",
      "updated_date": "2024-07-29 18:12:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:30:28.225249"
    },
    {
      "arxiv_id": "2407.17672v2",
      "title": "Spiking Neural Networks in Vertical Federated Learning: Performance Trade-offs",
      "title_zh": "脉冲神经网络在垂直联邦学习中的性能权衡",
      "authors": [
        "Maryam Abbasihafshejani",
        "Anindya Maiti",
        "Murtuza Jadliwala"
      ],
      "abstract": "Federated machine learning enables model training across multiple clients\nwhile maintaining data privacy. Vertical Federated Learning (VFL) specifically\ndeals with instances where the clients have different feature sets of the same\nsamples. As federated learning models aim to improve efficiency and\nadaptability, innovative neural network architectures like Spiking Neural\nNetworks (SNNs) are being leveraged to enable fast and accurate processing at\nthe edge. SNNs, known for their efficiency over Artificial Neural Networks\n(ANNs), have not been analyzed for their applicability in VFL, thus far. In\nthis paper, we investigate the benefits and trade-offs of using SNN models in a\nvertical federated learning setting. We implement two different federated\nlearning architectures -- with model splitting and without model splitting --\nthat have different privacy and performance implications. We evaluate the setup\nusing CIFAR-10 and CIFAR-100 benchmark datasets along with SNN implementations\nof VGG9 and ResNET classification models. Comparative evaluations demonstrate\nthat the accuracy of SNN models is comparable to that of traditional ANNs for\nVFL applications, albeit significantly more energy efficient.",
      "tldr_zh": "本研究探讨了Spiking Neural Networks (SNNs)在Vertical Federated Learning (VFL)中的性能权衡，VFL允许多个客户端在保持数据隐私的情况下训练模型，但客户端拥有相同样本的不同特征集。研究者实现了两种联邦学习架构——带有模型分割和不带模型分割的版本——并使用CIFAR-10和CIFAR-100基准数据集以及SNN实现的VGG9和ResNET分类模型进行评估。结果显示，SNNs的准确率与传统Artificial Neural Networks (ANNs)相当，但显著更节能，从而为高效的边缘计算应用提供了潜在优势。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17672v2",
      "published_date": "2024-07-24 23:31:02 UTC",
      "updated_date": "2024-08-13 22:46:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:30:39.879928"
    },
    {
      "arxiv_id": "2407.17642v1",
      "title": "SMA-Hyper: Spatiotemporal Multi-View Fusion Hypergraph Learning for Traffic Accident Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Xiaowei Gao",
        "James Haworth",
        "Ilya Ilyankou",
        "Xianghui Zhang",
        "Tao Cheng",
        "Stephen Law",
        "Huanfa Chen"
      ],
      "abstract": "Predicting traffic accidents is the key to sustainable city management, which\nrequires effective address of the dynamic and complex spatiotemporal\ncharacteristics of cities. Current data-driven models often struggle with data\nsparsity and typically overlook the integration of diverse urban data sources\nand the high-order dependencies within them. Additionally, they frequently rely\non predefined topologies or weights, limiting their adaptability in\nspatiotemporal predictions. To address these issues, we introduce the\nSpatiotemporal Multiview Adaptive HyperGraph Learning (SMA-Hyper) model, a\ndynamic deep learning framework designed for traffic accident prediction.\nBuilding on previous research, this innovative model incorporates dual adaptive\nspatiotemporal graph learning mechanisms that enable high-order cross-regional\nlearning through hypergraphs and dynamic adaptation to evolving urban data. It\nalso utilises contrastive learning to enhance global and local data\nrepresentations in sparse datasets and employs an advance attention mechanism\nto fuse multiple views of accident data and urban functional features, thereby\nenriching the contextual understanding of risk factors. Extensive testing on\nthe London traffic accident dataset demonstrates that the SMA-Hyper model\nsignificantly outperforms baseline models across various temporal horizons and\nmultistep outputs, affirming the effectiveness of its multiview fusion and\nadaptive learning strategies. The interpretability of the results further\nunderscores its potential to improve urban traffic management and safety by\nleveraging complex spatiotemporal urban data, offering a scalable framework\nadaptable to diverse urban environments.",
      "tldr_zh": "该论文提出 SMA-Hyper 模型，一种时空多视图融合超图学习框架，用于预测交通事故，以应对城市动态复杂特性和数据稀疏性问题。模型通过双自适应时空图学习机制实现高阶跨区域学习、对比学习增强稀疏数据集的全局和本地表示，以及高级注意力机制融合事故数据和城市功能特征，从而动态适应演变的城市环境。在伦敦交通事故数据集上的广泛测试中，SMA-Hyper 显著优于基线模型，在各种时间范围和多步输出上提升预测准确性，并提供可解释性结果，支持城市交通管理和安全改进。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17642v1",
      "published_date": "2024-07-24 21:10:34 UTC",
      "updated_date": "2024-07-24 21:10:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:30:50.864055"
    },
    {
      "arxiv_id": "2407.20267v1",
      "title": "A Large Encoder-Decoder Family of Foundation Models For Chemical Language",
      "title_zh": "翻译失败",
      "authors": [
        "Eduardo Soares",
        "Victor Shirasuna",
        "Emilio Vital Brazil",
        "Renato Cerqueira",
        "Dmitry Zubarev",
        "Kristin Schmidt"
      ],
      "abstract": "Large-scale pre-training methodologies for chemical language models represent\na breakthrough in cheminformatics. These methods excel in tasks such as\nproperty prediction and molecule generation by learning contextualized\nrepresentations of input tokens through self-supervised learning on large\nunlabeled corpora. Typically, this involves pre-training on unlabeled data\nfollowed by fine-tuning on specific tasks, reducing dependence on annotated\ndatasets and broadening chemical language representation understanding. This\npaper introduces a large encoder-decoder chemical foundation models pre-trained\non a curated dataset of 91 million SMILES samples sourced from PubChem, which\nis equivalent to 4 billion of molecular tokens. The proposed foundation model\nsupports different complex tasks, including quantum property prediction, and\noffer flexibility with two main variants (289M and $8\\times289M$). Our\nexperiments across multiple benchmark datasets validate the capacity of the\nproposed model in providing state-of-the-art results for different tasks. We\nalso provide a preliminary assessment of the compositionality of the embedding\nspace as a prerequisite for the reasoning tasks. We demonstrate that the\nproduced latent space is separable compared to the state-of-the-art with\nfew-shot learning capabilities.",
      "tldr_zh": "这篇论文引入了一个大型编码器-decoder 家族的 foundation models，用于化学语言建模，通过在9100万SMILES样本（相当于40亿分子标记）上进行自监督预训练，减少了对标注数据集的依赖。\n该模型支持属性预测、分子生成和量子属性预测等复杂任务，并提供两种变体（289M和8×289M），展示了灵活性和高效性。\n实验在多个基准数据集上验证了模型的state-of-the-art性能，并初步评估了嵌入空间的组合性，证明其具有可分性和少样本学习能力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.chem-ph"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 3 figures, 14 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.20267v1",
      "published_date": "2024-07-24 20:30:39 UTC",
      "updated_date": "2024-07-24 20:30:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:31:03.512952"
    },
    {
      "arxiv_id": "2407.17620v1",
      "title": "CoMoTo: Unpaired Cross-Modal Lesion Distillation Improves Breast Lesion Detection in Tomosynthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Alberb",
        "Marawan Elbatel",
        "Aya Elgebaly",
        "Ricardo Montoya-del-Angel",
        "Xiaomeng Li",
        "Robert Martí"
      ],
      "abstract": "Digital Breast Tomosynthesis (DBT) is an advanced breast imaging modality\nthat offers superior lesion detection accuracy compared to conventional\nmammography, albeit at the trade-off of longer reading time. Accelerating\nlesion detection from DBT using deep learning is hindered by limited data\navailability and huge annotation costs. A possible solution to this issue could\nbe to leverage the information provided by a more widely available modality,\nsuch as mammography, to enhance DBT lesion detection. In this paper, we present\na novel framework, CoMoTo, for improving lesion detection in DBT. Our framework\nleverages unpaired mammography data to enhance the training of a DBT model,\nimproving practicality by eliminating the need for mammography during\ninference. Specifically, we propose two novel components, Lesion-specific\nKnowledge Distillation (LsKD) and Intra-modal Point Alignment (ImPA). LsKD\nselectively distills lesion features from a mammography teacher model to a DBT\nstudent model, disregarding background features. ImPA further enriches LsKD by\nensuring the alignment of lesion features within the teacher before distilling\nknowledge to the student. Our comprehensive evaluation shows that CoMoTo is\nsuperior to traditional pretraining and image-level KD, improving performance\nby 7% Mean Sensitivity under low-data setting. Our code is available at\nhttps://github.com/Muhammad-Al-Barbary/CoMoTo .",
      "tldr_zh": "该研究提出CoMoTo框架，利用未配对的乳房X光检查数据来提升Digital Breast Tomosynthesis (DBT)中的乳房病变检测性能，从而解决数据稀缺和标注成本高的问题。框架的核心组件包括Lesion-specific Knowledge Distillation (LsKD)，该方法从乳房X光检查教师模型中选择性地蒸馏病变特征到DBT学生模型，同时忽略背景特征；以及Intra-modal Point Alignment (ImPA)，用于确保教师模型中病变特征的内部对齐，以进一步增强知识转移。实验结果显示，CoMoTo在低数据设置下比传统预训练和图像级Knowledge Distillation (KD)方法提高了7%的Mean Sensitivity，为DBT病变检测提供了更高效的实用方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "ADSMI @ MICCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17620v1",
      "published_date": "2024-07-24 20:17:05 UTC",
      "updated_date": "2024-07-24 20:17:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:31:15.926943"
    },
    {
      "arxiv_id": "2407.17596v2",
      "title": "Quality Assured: Rethinking Annotation Strategies in Imaging AI",
      "title_zh": "翻译失败",
      "authors": [
        "Tim Rädsch",
        "Annika Reinke",
        "Vivienn Weru",
        "Minu D. Tizabi",
        "Nicholas Heller",
        "Fabian Isensee",
        "Annette Kopp-Schneider",
        "Lena Maier-Hein"
      ],
      "abstract": "This paper does not describe a novel method. Instead, it studies an essential\nfoundation for reliable benchmarking and ultimately real-world application of\nAI-based image analysis: generating high-quality reference annotations.\nPrevious research has focused on crowdsourcing as a means of outsourcing\nannotations. However, little attention has so far been given to annotation\ncompanies, specifically regarding their internal quality assurance (QA)\nprocesses. Therefore, our aim is to evaluate the influence of QA employed by\nannotation companies on annotation quality and devise methodologies for\nmaximizing data annotation efficacy. Based on a total of 57,648 instance\nsegmented images obtained from a total of 924 annotators and 34 QA workers from\nfour annotation companies and Amazon Mechanical Turk (MTurk), we derived the\nfollowing insights: (1) Annotation companies perform better both in terms of\nquantity and quality compared to the widely used platform MTurk. (2) Annotation\ncompanies' internal QA only provides marginal improvements, if any. However,\nimproving labeling instructions instead of investing in QA can substantially\nboost annotation performance. (3) The benefit of internal QA depends on\nspecific image characteristics. Our work could enable researchers to derive\nsubstantially more value from a fixed annotation budget and change the way\nannotation companies conduct internal QA.",
      "tldr_zh": "这篇论文重新审视了图像 AI 中的标注策略，重点评估标注公司内部 Quality Assurance (QA) 对生成高质量参考标注的影响，而不是依赖传统的 crowdsourcing 方法。研究通过分析来自四个标注公司和 Amazon Mechanical Turk (MTurk) 的 57,648 张实例分割图像数据，发现标注公司在数量和质量上均优于 MTurk。关键洞见包括：内部 QA 仅带来微不足道的改善，但优化标注指令能显著提升性能，且 QA 的益处取决于图像的具体特性。该工作为研究者提供更高效的标注策略，帮助从固定预算中获得更大价值，并可能改变标注公司的 QA 实践。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at ECCV 2024, preprint, Computer Vision, Data Annotation",
      "pdf_url": "http://arxiv.org/pdf/2407.17596v2",
      "published_date": "2024-07-24 19:02:01 UTC",
      "updated_date": "2024-07-26 11:26:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:31:28.419659"
    },
    {
      "arxiv_id": "2408.01454v1",
      "title": "Amman City, Jordan: Toward a Sustainable City from the Ground Up",
      "title_zh": "翻译失败",
      "authors": [
        "Ra'Fat Al-Msie'deen"
      ],
      "abstract": "The idea of smart cities (SCs) has gained substantial attention in recent\nyears. The SC paradigm aims to improve citizens' quality of life and protect\nthe city's environment. As we enter the age of next-generation SCs, it is\nimportant to explore all relevant aspects of the SC paradigm. In recent years,\nthe advancement of Information and Communication Technologies (ICT) has\nproduced a trend of supporting daily objects with smartness, targeting to make\nhuman life easier and more comfortable. The paradigm of SCs appears as a\nresponse to the purpose of building the city of the future with advanced\nfeatures. SCs still face many challenges in their implementation, but\nincreasingly more studies regarding SCs are implemented. Nowadays, different\ncities are employing SC features to enhance services or the residents quality\nof life. This work provides readers with useful and important information about\nAmman Smart City.",
      "tldr_zh": "这篇论文探讨了约旦 Amman 城市向可持续智能城市转型的路径，强调智能城市（Smart Cities, SCs）范式如何通过 Information and Communication Technologies (ICT) 提升居民生活质量和环境保护。论文分析了 ICT 的进步如何使日常物体智能化，以构建未来城市，并指出了 SCs 实施过程中面临的挑战。最终，它为读者提供了关于 Amman 智能城市的实用信息，作为推动可持续城市发展的参考。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.NI",
        "cs.SE",
        "A.0, J.0, J.3, C.2, H.0, I.2"
      ],
      "primary_category": "cs.CY",
      "comment": "12 pages, 3 figures, 6 tables, 56 references",
      "pdf_url": "http://arxiv.org/pdf/2408.01454v1",
      "published_date": "2024-07-24 18:30:16 UTC",
      "updated_date": "2024-07-24 18:30:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:31:38.437969"
    },
    {
      "arxiv_id": "2407.17572v4",
      "title": "CityX: Controllable Procedural Content Generation for Unbounded 3D Cities",
      "title_zh": "CityX: 用于无边界3D城市的可控程序化内容生成",
      "authors": [
        "Shougao Zhang",
        "Mengqi Zhou",
        "Yuxi Wang",
        "Chuanchen Luo",
        "Rongyu Wang",
        "Yiwei Li",
        "Zhaoxiang Zhang",
        "Junran Peng"
      ],
      "abstract": "Urban areas, as the primary human habitat in modern civilization, accommodate\na broad spectrum of social activities. With the surge of embodied intelligence,\nrecent years have witnessed an increasing presence of physical agents in urban\nareas, such as autonomous vehicles and delivery robots. As a result,\npractitioners significantly value crafting authentic, simulation-ready 3D\ncities to facilitate the training and verification of such agents. However,\nthis task is quite challenging. Current generative methods fall short in either\ndiversity, controllability, or fidelity. In this work, we resort to the\nprocedural content generation (PCG) technique for high-fidelity generation. It\nassembles superior assets according to empirical rules, ultimately leading to\nindustrial-grade outcomes. To ensure diverse and self contained creation, we\ndesign a management protocol to accommodate extensive PCG plugins with distinct\nfunctions and interfaces. Based on this unified PCG library, we develop a\nmulti-agent framework to transform multi-modal instructions, including OSM,\nsemantic maps, and satellite images, into executable programs. The programs\ncoordinate relevant plugins to construct the 3D city consistent with the\ncontrol condition. A visual feedback scheme is introduced to further refine the\ninitial outcomes. Our method, named CityX, demonstrates its superiority in\ncreating diverse, controllable, and realistic 3D urban scenes. The synthetic\nscenes can be seamlessly deployed as a real-time simulator and an infinite data\ngenerator for embodied intelligence research. Our project page:\nhttps://cityx-lab.github.io.",
      "tldr_zh": "论文提出了CityX，一种可控的procedural content generation (PCG)方法，用于生成无限的3D城市场景，以支持embodied intelligence（如自动驾驶车辆）的研究。CityX采用multi-agent框架，将多模态指令（如OSM、语义地图和卫星图像）转化为可执行程序，并通过visual feedback方案优化生成结果，确保场景的多样性、可控性和高保真度。实验结果显示，该方法在创建真实3D城市方面优于现有技术，可作为实时模拟器和无限数据生成器，促进物理代理的训练和验证。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17572v4",
      "published_date": "2024-07-24 18:05:13 UTC",
      "updated_date": "2024-12-09 09:30:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:31:53.267224"
    },
    {
      "arxiv_id": "2407.17469v1",
      "title": "I Could've Asked That: Reformulating Unanswerable Questions",
      "title_zh": "翻译失败",
      "authors": [
        "Wenting Zhao",
        "Ge Gao",
        "Claire Cardie",
        "Alexander M. Rush"
      ],
      "abstract": "When seeking information from unfamiliar documents, users frequently pose\nquestions that cannot be answered by the documents. While existing large\nlanguage models (LLMs) identify these unanswerable questions, they do not\nassist users in reformulating their questions, thereby reducing their overall\nutility. We curate CouldAsk, an evaluation benchmark composed of existing and\nnew datasets for document-grounded question answering, specifically designed to\nstudy reformulating unanswerable questions. We evaluate state-of-the-art\nopen-source and proprietary LLMs on CouldAsk. The results demonstrate the\nlimited capabilities of these models in reformulating questions. Specifically,\nGPT-4 and Llama2-7B successfully reformulate questions only 26% and 12% of the\ntime, respectively. Error analysis shows that 62% of the unsuccessful\nreformulations stem from the models merely rephrasing the questions or even\ngenerating identical questions. We publicly release the benchmark and the code\nto reproduce the experiments.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）在文档查询中识别无法回答的问题（unanswerable questions）但无法有效重新表述的局限性。研究者创建了CouldAsk基准，这是一个由现有和新数据集组成的评估框架，专门用于研究问题重新表述。实验评估了多种最先进的LLMs，结果显示GPT-4和Llama2-7B的成功率仅为26%和12%，其中62%的失败源于模型简单改写或重复问题。论文公开了基准和代码，以推动相关领域的研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17469v1",
      "published_date": "2024-07-24 17:59:07 UTC",
      "updated_date": "2024-07-24 17:59:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:32:03.009572"
    },
    {
      "arxiv_id": "2407.17468v1",
      "title": "WildHallucinations: Evaluating Long-form Factuality in LLMs with Real-World Entity Queries",
      "title_zh": "翻译失败",
      "authors": [
        "Wenting Zhao",
        "Tanya Goyal",
        "Yu Ying Chiu",
        "Liwei Jiang",
        "Benjamin Newman",
        "Abhilasha Ravichander",
        "Khyathi Chandu",
        "Ronan Le Bras",
        "Claire Cardie",
        "Yuntian Deng",
        "Yejin Choi"
      ],
      "abstract": "While hallucinations of large language models (LLMs) prevail as a major\nchallenge, existing evaluation benchmarks on factuality do not cover the\ndiverse domains of knowledge that the real-world users of LLMs seek information\nabout. To bridge this gap, we introduce WildHallucinations, a benchmark that\nevaluates factuality. It does so by prompting LLMs to generate information\nabout entities mined from user-chatbot conversations in the wild. These\ngenerations are then automatically fact-checked against a systematically\ncurated knowledge source collected from web search. Notably, half of these\nreal-world entities do not have associated Wikipedia pages. We evaluate 118,785\ngenerations from 15 LLMs on 7,919 entities. We find that LLMs consistently\nhallucinate more on entities without Wikipedia pages and exhibit varying\nhallucination rates across different domains. Finally, given the same base\nmodels, adding a retrieval component only slightly reduces hallucinations but\ndoes not eliminate hallucinations.",
      "tldr_zh": "本研究引入了 WildHallucinations 基准，用于评估大型语言模型 (LLMs) 在真实世界实体查询中的长形式事实性问题，弥补现有基准覆盖知识领域的不足。\n该基准通过从用户-聊天机器人对话中挖掘实体，提示 LLMs 生成相关信息，然后使用从网络搜索收集的知识源进行自动事实检查，其中一半实体缺乏 Wikipedia 页面。\n实验评估了 15 个 LLMs 的 118,785 个生成，涉及 7,919 个实体，发现 LLMs 在无 Wikipedia 页面的实体上 hallucinations 率更高，且不同领域间 hallucinations 情况各异；添加检索组件仅能略微减少 hallucinations，但无法完全消除。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17468v1",
      "published_date": "2024-07-24 17:59:05 UTC",
      "updated_date": "2024-07-24 17:59:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:32:15.896803"
    },
    {
      "arxiv_id": "2407.17460v2",
      "title": "SoNIC: Safe Social Navigation with Adaptive Conformal Inference and Constrained Reinforcement Learning",
      "title_zh": "SoNIC：利用自适应保形推理和约束强化学习的安全社交导航",
      "authors": [
        "Jianpeng Yao",
        "Xiaopan Zhang",
        "Yu Xia",
        "Zejin Wang",
        "Amit K. Roy-Chowdhury",
        "Jiachen Li"
      ],
      "abstract": "Reinforcement learning (RL) enables social robots to generate trajectories\nwithout relying on human-designed rules or interventions, making it generally\nmore effective than rule-based systems in adapting to complex, dynamic\nreal-world scenarios. However, social navigation is a safety-critical task that\nrequires robots to avoid collisions with pedestrians, whereas existing RL-based\nsolutions often fall short of ensuring safety in complex environments. In this\npaper, we propose SoNIC, which to the best of our knowledge is the first\nalgorithm that integrates adaptive conformal inference (ACI) with constrained\nreinforcement learning (CRL) to enable safe policy learning for social\nnavigation. Specifically, our method not only augments RL observations with\nACI-generated nonconformity scores, which inform the agent of the quantified\nuncertainty but also employs these uncertainty estimates to effectively guide\nthe behaviors of RL agents by using constrained reinforcement learning. This\nintegration regulates the behaviors of RL agents and enables them to handle\nsafety-critical situations. On the standard CrowdNav benchmark, our method\nachieves a success rate of 96.93%, which is 11.67% higher than the previous\nstate-of-the-art RL method and results in 4.5 times fewer collisions and 2.8\ntimes fewer intrusions to ground-truth human future trajectories as well as\nenhanced robustness in out-of-distribution scenarios. To further validate our\napproach, we deploy our algorithm on a real robot by developing a ROS2-based\nnavigation system. Our experiments demonstrate that the system can generate\nrobust and socially polite decision-making when interacting with both sparse\nand dense crowds. The video demos can be found on our project website:\nhttps://sonic-social-nav.github.io/.",
      "tldr_zh": "这篇论文提出 SoNIC 算法，这是首个整合 adaptive conformal inference (ACI) 和 constrained reinforcement learning (CRL) 的方法，用于实现社会机器人的安全导航，旨在解决现有 Reinforcement Learning (RL) 方法在复杂环境中避免碰撞的不足。SoNIC 通过 ACI 生成 nonconformity scores 来量化不确定性，并利用这些估计值指导 CRL 代理的行为，从而调节 RL 代理在安全关键场景中的决策。在 CrowdNav 基准测试中，SoNIC 成功率达到 96.93%，比先前最先进方法高 11.67%，并减少 4.5 倍碰撞和 2.8 倍入侵，同时提升了分布外场景的鲁棒性。实际部署在基于 ROS2 的真实机器人系统中，证明了其在稀疏和密集人群中生成可靠且礼貌的导航决策。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website: https://sonic-social-nav.github.io/; 16 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.17460v2",
      "published_date": "2024-07-24 17:57:21 UTC",
      "updated_date": "2025-02-06 18:55:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:32:29.735554"
    },
    {
      "arxiv_id": "2407.16890v1",
      "title": "Why Machines Can't Be Moral: Turing's Halting Problem and the Moral Limits of Artificial Intelligence",
      "title_zh": "为什么机器不能是道德的：图灵的停机问题与人工智能的道德限制",
      "authors": [
        "Massimo Passamonti"
      ],
      "abstract": "In this essay, I argue that explicit ethical machines, whose moral principles\nare inferred through a bottom-up approach, are unable to replicate human-like\nmoral reasoning and cannot be considered moral agents. By utilizing Alan\nTuring's theory of computation, I demonstrate that moral reasoning is\ncomputationally intractable by these machines due to the halting problem. I\naddress the frontiers of machine ethics by formalizing moral problems into\n'algorithmic moral questions' and by exploring moral psychology's dual-process\nmodel. While the nature of Turing Machines theoretically allows artificial\nagents to engage in recursive moral reasoning, critical limitations are\nintroduced by the halting problem, which states that it is impossible to\npredict with certainty whether a computational process will halt. A thought\nexperiment involving a military drone illustrates this issue, showing that an\nartificial agent might fail to decide between actions due to the halting\nproblem, which limits the agent's ability to make decisions in all instances,\nundermining its moral agency.",
      "tldr_zh": "本文论证了显式道德机器无法复制人类道德推理，无法被视为道德代理人，因为Turing's Halting Problem 使道德推理计算上不可行。作者通过将道德问题形式化为“algorithmic moral questions”并结合道德心理学的dual-process model，探讨了人工智能的道德限制。最终，通过一个军事无人机思想实验，展示了机器可能因停机问题而无法在所有情况下决策，从而质疑了人工智能的道德能力。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16890v1",
      "published_date": "2024-07-24 17:50:24 UTC",
      "updated_date": "2024-07-24 17:50:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:32:39.775809"
    },
    {
      "arxiv_id": "2407.17454v3",
      "title": "Automated Explanation Selection for Scientific Discovery",
      "title_zh": "翻译失败",
      "authors": [
        "Markus Iser"
      ],
      "abstract": "Automated reasoning is a key technology in the young but rapidly growing\nfield of Explainable Artificial Intelligence (XAI). Explanability helps build\ntrust in artificial intelligence systems beyond their mere predictive accuracy\nand robustness. In this paper, we propose a cycle of scientific discovery that\ncombines machine learning with automated reasoning for the generation and the\nselection of explanations. We present a taxonomy of explanation selection\nproblems that draws on insights from sociology and cognitive science. These\nselection criteria subsume existing notions and extend them with new\nproperties.",
      "tldr_zh": "本论文提出了一种结合机器学习和自动化推理的科学发现循环，用于生成和选择解释，从而提升Explainable Artificial Intelligence (XAI)系统的可信度。研究者基于社会学和认知科学的见解，构建了一个解释选择问题的taxonomy（分类法），涵盖现有概念并扩展了新属性。该方法有助于在AI领域超越单纯的预测准确性和鲁棒性，推动更可靠的解释生成和选择。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Composite AI Workshop at ECAI 2024 (accepted for publication)",
      "pdf_url": "http://arxiv.org/pdf/2407.17454v3",
      "published_date": "2024-07-24 17:41:32 UTC",
      "updated_date": "2024-08-06 08:52:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:32:50.753079"
    },
    {
      "arxiv_id": "2407.17546v1",
      "title": "Exploring Domain Robust Lightweight Reward Models based on Router Mechanism",
      "title_zh": "探索基于路由机制的领域鲁棒轻量级奖励模型",
      "authors": [
        "Hyuk Namgoong",
        "Jeesu Jung",
        "Sangkeun Jung",
        "Yoonhyung Roh"
      ],
      "abstract": "Recent advancements in large language models have heavily relied on the large\nreward model from reinforcement learning from human feedback for fine-tuning.\nHowever, the use of a single reward model across various domains may not always\nbe optimal, often requiring retraining from scratch when new domain data is\nintroduced. To address these challenges, we explore the utilization of small\nlanguage models operating in a domain-specific manner based on router\nmechanisms. Our three approaches are: 1) utilize mixture of experts to form a\nsingle reward model by modularizing an internal router and experts, 2)\nemploying external router to select the appropriate reward model from multiple\ndomain-specific models, and 3) the framework reduces parameter size by loading\nreward models and router adapters onto a single small language model using\nadapters. Experimental validation underscores the effectiveness of our\napproach, demonstrating performance comparable to baseline methods while also\nreducing the total parameter size.",
      "tldr_zh": "该研究探讨了基于Router Mechanism的领域鲁棒轻量级Reward Models，以解决大型语言模型在不同领域依赖单一奖励模型的问题，从而避免在新领域数据引入时从零训练。研究提出三种方法：1) 使用Mixture of Experts通过内部路由器和专家模块化形成单一Reward Model；2) 采用外部路由器从多个领域特定模型中选择合适的Reward Model；3) 通过Adapters将Reward Models和路由器适配器加载到单个小型语言模型上，显著减少参数大小。实验结果显示，该方法在性能上与基线相当，同时降低了总参数量，提升了模型的灵活性和效率。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper is accepted for ACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17546v1",
      "published_date": "2024-07-24 17:25:12 UTC",
      "updated_date": "2024-07-24 17:25:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:33:03.608601"
    },
    {
      "arxiv_id": "2407.17447v2",
      "title": "FLRT: Fluent Student-Teacher Redteaming",
      "title_zh": "FLRT：流利的学生-教师红队测试",
      "authors": [
        "T. Ben Thompson",
        "Michael Sklar"
      ],
      "abstract": "Many publicly available language models have been safety tuned to reduce the\nlikelihood of toxic or liability-inducing text. To redteam or jailbreak these\nmodels for compliance with toxic requests, users and security analysts have\ndeveloped adversarial prompting techniques. One attack method is to apply\ndiscrete optimization techniques to the prompt. However, the resulting attack\nstrings are often gibberish text, easily filtered by defenders due to high\nmeasured perplexity, and may fail for unseen tasks and/or well-tuned models. In\nthis work, we improve existing algorithms (primarily GCG and BEAST) to develop\npowerful and fluent attacks on safety-tuned models like Llama-2 and Phi-3. Our\ntechnique centers around a new distillation-based approach that encourages the\nvictim model to emulate a toxified finetune, either in terms of output\nprobabilities or internal activations. To encourage human-fluent attacks, we\nadd a multi-model perplexity penalty and a repetition penalty to the objective.\nWe also enhance optimizer strength by allowing token insertions, token swaps,\nand token deletions and by using longer attack sequences. The resulting process\nis able to reliably jailbreak the most difficult target models with prompts\nthat appear similar to human-written prompts. On Advbench we achieve attack\nsuccess rates $>93$% for Llama-2-7B, Llama-3-8B, and Vicuna-7B, while\nmaintaining model-measured perplexity $<33$; we achieve $95$% attack success\nfor Phi-3, though with higher perplexity. We also find a universally-optimized\nsingle fluent prompt that induces $>88$% compliance on previously unseen tasks\nacross Llama-2-7B, Phi-3-mini and Vicuna-7B and transfers to other black-box\nmodels.",
      "tldr_zh": "本文提出 FLRT，一种改进的红队攻击框架（Fluent Student-Teacher Redteaming），旨在生成更流畅的人类化提示，以越狱安全调整的语言模型，如 Llama-2 和 Phi-3。核心方法基于学生-教师蒸馏（student-teacher distillation），通过鼓励受害者模型模仿毒化微调的输出概率或内部激活，并添加多模型困惑度惩罚（multi-model perplexity penalty）和重复惩罚，同时增强优化器以支持 token 插入、交换和删除。实验结果显示，在 Advbench 上，FLRT 对 Llama-2-7B、Llama-3-8B 和 Vicuna-7B 的攻击成功率超过93%，困惑度（perplexity）低于33；对 Phi-3 成功率达95%，并发现一个通用优化提示能在未见任务上跨模型实现88%以上的 compliance。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17447v2",
      "published_date": "2024-07-24 17:23:18 UTC",
      "updated_date": "2024-10-01 17:39:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:33:18.582813"
    },
    {
      "arxiv_id": "2407.17438v3",
      "title": "HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation",
      "title_zh": "HumanVid: 解密用于相机可控人类图像动画的训练数据",
      "authors": [
        "Zhenzhi Wang",
        "Yixuan Li",
        "Yanhong Zeng",
        "Youqing Fang",
        "Yuwei Guo",
        "Wenran Liu",
        "Jing Tan",
        "Kai Chen",
        "Tianfan Xue",
        "Bo Dai",
        "Dahua Lin"
      ],
      "abstract": "Human image animation involves generating videos from a character photo,\nallowing user control and unlocking the potential for video and movie\nproduction. While recent approaches yield impressive results using high-quality\ntraining data, the inaccessibility of these datasets hampers fair and\ntransparent benchmarking. Moreover, these approaches prioritize 2D human motion\nand overlook the significance of camera motions in videos, leading to limited\ncontrol and unstable video generation. To demystify the training data, we\npresent HumanVid, the first large-scale high-quality dataset tailored for human\nimage animation, which combines crafted real-world and synthetic data. For the\nreal-world data, we compile a vast collection of real-world videos from the\ninternet. We developed and applied careful filtering rules to ensure video\nquality, resulting in a curated collection of 20K high-resolution (1080P)\nhuman-centric videos. Human and camera motion annotation is accomplished using\na 2D pose estimator and a SLAM-based method. To expand our synthetic dataset,\nwe collected 10K 3D avatar assets and leveraged existing assets of body shapes,\nskin textures and clothings. Notably, we introduce a rule-based camera\ntrajectory generation method, enabling the synthetic pipeline to incorporate\ndiverse and precise camera motion annotation, which can rarely be found in\nreal-world data. To verify the effectiveness of HumanVid, we establish a\nbaseline model named CamAnimate, short for Camera-controllable Human Animation,\nthat considers both human and camera motions as conditions. Through extensive\nexperimentation, we demonstrate that such simple baseline training on our\nHumanVid achieves state-of-the-art performance in controlling both human pose\nand camera motions, setting a new benchmark. Demo, data and code could be found\nin the project website: https://humanvid.github.io/.",
      "tldr_zh": "本文提出 HumanVid，这是一个首个大规模高质量数据集，用于相机可控人类图像动画，旨在解决现有方法数据不可访问和忽略相机动作的问题，从而实现更公平的基准测试和更稳定的视频生成。HumanVid 结合了从互联网收集并经过过滤的 20K 高分辨率 (1080P) 真实世界视频，以及使用 10K 3D 头像资产生成的合成数据，并通过 2D 姿势估计器和 SLAM-based 方法进行人类和相机动作标注。作者开发了基线模型 CamAnimate，将人类姿势和相机动作作为条件进行训练，实验结果显示其在控制方面达到了 state-of-the-art 性能，为视频制作领域设定了新基准。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "NeurIPS D&B Track 2024 camera ready version, TL;DR: the first\n  large-scale dataset for camera controllable human image animation task, and a\n  baseline method",
      "pdf_url": "http://arxiv.org/pdf/2407.17438v3",
      "published_date": "2024-07-24 17:15:58 UTC",
      "updated_date": "2024-11-21 03:26:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:33:30.025106"
    },
    {
      "arxiv_id": "2407.17413v3",
      "title": "$A^*$ for Graphs of Convex Sets",
      "title_zh": "$A^*$ 用于凸集图",
      "authors": [
        "Kaarthik Sundar",
        "Sivakumar Rathinam"
      ],
      "abstract": "We present a novel algorithm that fuses the existing convex-programming based\napproach with heuristic information to find optimality guarantees and\nnear-optimal paths for the Shortest Path Problem in the Graph of Convex Sets\n(SPP-GCS). Our method, inspired by $A^*$, initiates a best-first-like procedure\nfrom a designated subset of vertices and iteratively expands it until further\ngrowth is neither possible nor beneficial. Traditionally, obtaining solutions\nwith bounds for an optimization problem involves solving a relaxation,\nmodifying the relaxed solution to a feasible one, and then comparing the two\nsolutions to establish bounds. However, for SPP-GCS, we demonstrate that\nreversing this process can be more advantageous, especially with Euclidean\ntravel costs. In other words, we initially employ $A^*$ to find a feasible\nsolution for SPP-GCS, then solve a convex relaxation restricted to the vertices\nexplored by $A^*$ to obtain a relaxed solution, and finally, compare the\nsolutions to derive bounds. We present numerical results to highlight the\nadvantages of our algorithm over the existing approach in terms of the sizes of\nthe convex programs solved and computation time.",
      "tldr_zh": "本论文提出了一种新算法，将 A* 搜索与凸优化相结合，用于 Graph of Convex Sets (SPP-GCS) 中的 Shortest Path Problem，以提供最优性保证和近似最优路径。\n该算法从一组顶点开始，进行类似最佳优先搜索，并逆转传统过程：先用 A* 找到可行解，然后在探索的顶点上求解凸松弛，并比较以获得界限，尤其适用于欧氏旅行成本。\n实验结果表明，与现有方法相比，该算法显著降低了凸程序规模和计算时间。",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "math.OC",
      "comment": "International Conference on Automated Planning and Scheduling (ICAPS)\n  2025",
      "pdf_url": "http://arxiv.org/pdf/2407.17413v3",
      "published_date": "2024-07-24 16:48:32 UTC",
      "updated_date": "2025-04-19 07:19:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:33:40.781846"
    },
    {
      "arxiv_id": "2407.17412v1",
      "title": "(PASS) Visual Prompt Locates Good Structure Sparsity through a Recurrent HyperNetwork",
      "title_zh": "(PASS) 视觉提示通过循环超网络定位良好结构稀疏",
      "authors": [
        "Tianjin Huang",
        "Fang Meng",
        "Li Shen",
        "Fan Liu",
        "Yulong Pei",
        "Mykola Pechenizkiy",
        "Shiwei Liu",
        "Tianlong Chen"
      ],
      "abstract": "Large-scale neural networks have demonstrated remarkable performance in\ndifferent domains like vision and language processing, although at the cost of\nmassive computation resources. As illustrated by compression literature,\nstructural model pruning is a prominent algorithm to encourage model\nefficiency, thanks to its acceleration-friendly sparsity patterns. One of the\nkey questions of structural pruning is how to estimate the channel\nsignificance. In parallel, work on data-centric AI has shown that\nprompting-based techniques enable impressive generalization of large language\nmodels across diverse downstream tasks. In this paper, we investigate a\ncharming possibility - \\textit{leveraging visual prompts to capture the channel\nimportance and derive high-quality structural sparsity}. To this end, we\npropose a novel algorithmic framework, namely \\texttt{PASS}. It is a tailored\nhyper-network to take both visual prompts and network weight statistics as\ninput, and output layer-wise channel sparsity in a recurrent manner. Such\ndesigns consider the intrinsic channel dependency between layers. Comprehensive\nexperiments across multiple network architectures and six datasets demonstrate\nthe superiority of \\texttt{PASS} in locating good structural sparsity. For\nexample, at the same FLOPs level, \\texttt{PASS} subnetworks achieve $1\\%\\sim\n3\\%$ better accuracy on Food101 dataset; or with a similar performance of\n$80\\%$ accuracy, \\texttt{PASS} subnetworks obtain $0.35\\times$ more speedup\nthan the baselines.",
      "tldr_zh": "该研究探讨了大型神经网络在视觉和语言处理中的高效性问题，提出通过结构化模型剪枝（structural model pruning）来估计通道重要性（channel significance），并创新性地利用视觉提示（visual prompts）来捕获通道重要性并实现高质量结构稀疏性。作者引入了PASS框架，这是一个定制的递归超网络（recurrent hyper-network），它以视觉提示和网络权重统计作为输入，输出层级通道稀疏性，同时考虑层间通道依赖性。在多个网络架构和六个数据集上的实验显示，PASS子网络在相同FLOPs水平下比基线模型在Food101数据集上准确率提高1%~3%；或者在类似80%准确率下，获得0.35倍的速度提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2407.17412v1",
      "published_date": "2024-07-24 16:47:45 UTC",
      "updated_date": "2024-07-24 16:47:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:33:52.495051"
    },
    {
      "arxiv_id": "2407.17406v1",
      "title": "Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models",
      "title_zh": "依赖 Transformer 文法：将依赖结构集成到 Transformer 语言模型中",
      "authors": [
        "Yida Zhao",
        "Chao Lou",
        "Kewei Tu"
      ],
      "abstract": "Syntactic Transformer language models aim to achieve better generalization\nthrough simultaneously modeling syntax trees and sentences. While prior work\nhas been focusing on adding constituency-based structures to Transformers, we\nintroduce Dependency Transformer Grammars (DTGs), a new class of Transformer\nlanguage model with explicit dependency-based inductive bias. DTGs simulate\ndependency transition systems with constrained attention patterns by modifying\nattention masks, incorporate the stack information through relative positional\nencoding, and augment dependency arc representation with a combination of token\nembeddings and operation embeddings. When trained on a dataset of sentences\nannotated with dependency trees, DTGs achieve better generalization while\nmaintaining comparable perplexity with Transformer language model baselines.\nDTGs also outperform recent constituency-based models, showing that dependency\ncan better guide Transformer language models. Our code is released at\nhttps://github.com/zhaoyd1/Dep_Transformer_Grammars.",
      "tldr_zh": "本文提出Dependency Transformer Grammars (DTGs)，一种将依赖结构（dependency-based inductive bias）整合到Transformer语言模型中的新框架，旨在通过显式语法建模提升模型的泛化能力。DTGs通过修改注意力掩码（attention masks）模拟依赖转移系统（dependency transition systems）、使用相对位置编码（relative positional encoding）整合栈信息，并结合标记嵌入（token embeddings）和操作嵌入增强依赖弧表示（dependency arc representation）。在标注依赖树的句子数据集上训练后，DTGs实现了更好的泛化性能，同时保持与Transformer基线模型相当的perplexity，并优于基于成分结构的现有模型。代码已开源在GitHub上。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17406v1",
      "published_date": "2024-07-24 16:38:38 UTC",
      "updated_date": "2024-07-24 16:38:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:34:06.314272"
    },
    {
      "arxiv_id": "2407.17404v2",
      "title": "Grammar-based Game Description Generation using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Tsunehiko Tanaka",
        "Edgar Simo-Serra"
      ],
      "abstract": "Game Description Language (GDL) provides a standardized way to express\ndiverse games in a machine-readable format, enabling automated game simulation,\nand evaluation. While previous research has explored game description\ngeneration using search-based methods, generating GDL descriptions from natural\nlanguage remains a challenging task. This paper presents a novel framework that\nleverages Large Language Models (LLMs) to generate grammatically accurate game\ndescriptions from natural language. Our approach consists of two stages: first,\nwe gradually generate a minimal grammar based on GDL specifications; second, we\niteratively improve the game description through grammar-guided generation. Our\nframework employs a specialized parser that identifies valid subsequences and\ncandidate symbols from LLM responses, enabling gradual refinement of the output\nto ensure grammatical correctness. Experimental results demonstrate that our\niterative improvement approach significantly outperforms baseline methods that\ndirectly use LLM outputs. Our code is available at\nhttps://github.com/tsunehiko/ggdg",
      "tldr_zh": "本研究提出了一种基于 Large Language Models (LLMs) 的框架，用于从自然语言生成语法正确的 Game Description Language (GDL) 游戏描述，以标准化游戏表达并支持自动模拟和评估。框架分为两个阶段：首先，逐步生成基于 GDL 规范的最小语法；其次，通过语法引导生成和迭代改进，利用专门的解析器识别有效子序列和候选符号，确保输出语法准确。实验结果显示，该方法显著优于直接使用 LLM 输出作为基线的做法，证明了其在游戏描述生成方面的有效性。研究代码可公开访问，详见 GitHub 仓库。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted for publication at the IEEE Transactions on Games",
      "pdf_url": "http://arxiv.org/pdf/2407.17404v2",
      "published_date": "2024-07-24 16:36:02 UTC",
      "updated_date": "2025-01-22 05:52:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:34:15.554106"
    },
    {
      "arxiv_id": "2407.17545v1",
      "title": "Large Language Models for Anomaly Detection in Computational Workflows: from Supervised Fine-Tuning to In-Context Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hongwei Jin",
        "George Papadimitriou",
        "Krishnan Raghavan",
        "Pawel Zuk",
        "Prasanna Balaprakash",
        "Cong Wang",
        "Anirban Mandal",
        "Ewa Deelman"
      ],
      "abstract": "Anomaly detection in computational workflows is critical for ensuring system\nreliability and security. However, traditional rule-based methods struggle to\ndetect novel anomalies. This paper leverages large language models (LLMs) for\nworkflow anomaly detection by exploiting their ability to learn complex data\npatterns. Two approaches are investigated: 1) supervised fine-tuning (SFT),\nwhere pre-trained LLMs are fine-tuned on labeled data for sentence\nclassification to identify anomalies, and 2) in-context learning (ICL) where\nprompts containing task descriptions and examples guide LLMs in few-shot\nanomaly detection without fine-tuning. The paper evaluates the performance,\nefficiency, generalization of SFT models, and explores zero-shot and few-shot\nICL prompts and interpretability enhancement via chain-of-thought prompting.\nExperiments across multiple workflow datasets demonstrate the promising\npotential of LLMs for effective anomaly detection in complex executions.",
      "tldr_zh": "本论文探讨了使用大型语言模型（LLMs）来提升计算工作流中的异常检测能力，以解决传统规则-based方法无法识别新颖异常的局限。研究比较了两种方法：监督微调（SFT），通过在标记数据上微调预训练LLMs进行句子分类识别异常；以及In-Context Learning (ICL)，利用包含任务描述和示例的提示，实现零样本或少样本检测，并通过链式思维提示增强可解释性。实验在多个工作流数据集上验证了这些方法的性能、效率和泛化潜力，展示了LLMs在复杂执行环境中的应用前景。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.SE",
      "comment": "12 pages, 14 figures, paper is accepted by SC'24, source code, see:\n  https://github.com/PoSeiDon-Workflows/LLM_AD",
      "pdf_url": "http://arxiv.org/pdf/2407.17545v1",
      "published_date": "2024-07-24 16:33:04 UTC",
      "updated_date": "2024-07-24 16:33:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:34:28.805926"
    },
    {
      "arxiv_id": "2407.17396v2",
      "title": "Systematic Relational Reasoning With Epistemic Graph Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Irtaza Khalid",
        "Steven Schockaert"
      ],
      "abstract": "Developing models that can learn to reason is a notoriously challenging\nproblem. We focus on reasoning in relational domains, where the use of Graph\nNeural Networks (GNNs) seems like a natural choice. However, previous work has\nshown that regular GNNs lack the ability to systematically generalize from\ntraining examples on test graphs requiring longer inference chains, which\nfundamentally limits their reasoning abilities. A common solution relies on\nneuro-symbolic methods that systematically reason by learning rules, but their\nscalability is often limited and they tend to make unrealistically strong\nassumptions, e.g.\\ that the answer can always be inferred from a single\nrelational path. We propose the Epistemic GNN (EpiGNN), a novel\nparameter-efficient and scalable GNN architecture with an epistemic inductive\nbias for systematic reasoning. Node embeddings in EpiGNNs are treated as\nepistemic states, and message passing is implemented accordingly. We show that\nEpiGNNs achieve state-of-the-art results on link prediction tasks that require\nsystematic reasoning. Furthermore, for inductive knowledge graph completion,\nEpiGNNs rival the performance of state-of-the-art specialized approaches.\nFinally, we introduce two new benchmarks that go beyond standard relational\nreasoning by requiring the aggregation of information from multiple paths.\nHere, existing neuro-symbolic approaches fail, yet EpiGNNs learn to reason\naccurately. Code and datasets are available at\nhttps://github.com/erg0dic/gnn-sg.",
      "tldr_zh": "本研究针对关系域中的推理问题，指出常规 Graph Neural Networks (GNNs) 无法在需要更长推理链的测试图上实现系统化泛化。论文提出 Epistemic GNN (EpiGNN)，一种参数高效且可扩展的 GNN 架构，通过将节点嵌入视为认知状态（epistemic states）并相应设计消息传递机制，来增强系统性推理能力。实验结果显示，EpiGNN 在链接预测（link prediction）任务中达到最先进水平，并在归纳知识图完成（inductive knowledge graph completion）上与专业方法相当。论文还引入两个新基准，要求从多个路径聚合信息，在这些任务中，现有神经符号方法失败，而 EpiGNN 能够准确学习推理。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10+29 pages, 5+13 figures, 4+10 tables. Comments welcome!",
      "pdf_url": "http://arxiv.org/pdf/2407.17396v2",
      "published_date": "2024-07-24 16:17:15 UTC",
      "updated_date": "2025-02-27 22:50:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:34:41.464952"
    },
    {
      "arxiv_id": "2408.01453v1",
      "title": "Reporting and Analysing the Environmental Impact of Language Models on the Example of Commonsense Question Answering with External Knowledge",
      "title_zh": "翻译失败",
      "authors": [
        "Aida Usmanova",
        "Junbo Huang",
        "Debayan Banerjee",
        "Ricardo Usbeck"
      ],
      "abstract": "Human-produced emissions are growing at an alarming rate, causing already\nobservable changes in the climate and environment in general. Each year global\ncarbon dioxide emissions hit a new record, and it is reported that 0.5% of\ntotal US greenhouse gas emissions are attributed to data centres as of 2021.\nThe release of ChatGPT in late 2022 sparked social interest in Large Language\nModels (LLMs), the new generation of Language Models with a large number of\nparameters and trained on massive amounts of data. Currently, numerous\ncompanies are releasing products featuring various LLMs, with many more models\nin development and awaiting release. Deep Learning research is a competitive\nfield, with only models that reach top performance attracting attention and\nbeing utilized. Hence, achieving better accuracy and results is often the first\npriority, while the model's efficiency and the environmental impact of the\nstudy are neglected. However, LLMs demand substantial computational resources\nand are very costly to train, both financially and environmentally. It becomes\nessential to raise awareness and promote conscious decisions about algorithmic\nand hardware choices. Providing information on training time, the approximate\ncarbon dioxide emissions and power consumption would assist future studies in\nmaking necessary adjustments and determining the compatibility of available\ncomputational resources with model requirements. In this study, we infused T5\nLLM with external knowledge and fine-tuned the model for Question-Answering\ntask. Furthermore, we calculated and reported the approximate environmental\nimpact for both steps. The findings demonstrate that the smaller models may not\nalways be sustainable options, and increased training does not always imply\nbetter performance. The most optimal outcome is achieved by carefully\nconsidering both performance and efficiency factors.",
      "tldr_zh": "本研究探讨了大型语言模型（LLMs）的环境影响，强调数据中心和模型训练（如ChatGPT）对温室气体排放的贡献，并呼吁平衡性能与可持续性。作者以T5 LLM为例，将其与外部知识结合并微调用于常识问答任务，同时计算了训练过程中的碳 dioxide emissions、功耗和时间。结果显示，较小模型并非总是可持续选择，增加训练并不必然提升性能，建议未来研究需综合考虑算法效率和环境因素以促进更负责任的决策。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CY",
      "comment": "Presented at Bonn Sustainable AI 2023 conference",
      "pdf_url": "http://arxiv.org/pdf/2408.01453v1",
      "published_date": "2024-07-24 16:16:16 UTC",
      "updated_date": "2024-07-24 16:16:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:34:51.633548"
    },
    {
      "arxiv_id": "2407.17383v1",
      "title": "A Comprehensive Approach to Misspelling Correction with BERT and Levenshtein Distance",
      "title_zh": "翻译失败",
      "authors": [
        "Amirreza Naziri",
        "Hossein Zeinali"
      ],
      "abstract": "Writing, as an omnipresent form of human communication, permeates nearly\nevery aspect of contemporary life. Consequently, inaccuracies or errors in\nwritten communication can lead to profound consequences, ranging from financial\nlosses to potentially life-threatening situations. Spelling mistakes, among the\nmost prevalent writing errors, are frequently encountered due to various\nfactors. This research aims to identify and rectify diverse spelling errors in\ntext using neural networks, specifically leveraging the Bidirectional Encoder\nRepresentations from Transformers (BERT) masked language model. To achieve this\ngoal, we compiled a comprehensive dataset encompassing both non-real-word and\nreal-word errors after categorizing different types of spelling mistakes.\nSubsequently, multiple pre-trained BERT models were employed. To ensure optimal\nperformance in correcting misspelling errors, we propose a combined approach\nutilizing the BERT masked language model and Levenshtein distance. The results\nfrom our evaluation data demonstrate that the system presented herein exhibits\nremarkable capabilities in identifying and rectifying spelling mistakes, often\nsurpassing existing systems tailored for the Persian language.",
      "tldr_zh": "这篇论文提出了一种全面的方法，使用 BERT 模型和 Levenshtein Distance 来识别和纠正拼写错误，旨在解决写作中常见的问题如非真实词和真实词错误。研究者编译了一个涵盖不同类型拼写错误的完整数据集，并评估了多个预训练 BERT 模型。最终，通过结合 BERT 掩码语言模型和 Levenshtein Distance 的混合方法，该系统在波斯语文本上表现出色，准确率超过了现有系统。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "12 pages, 9 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.17383v1",
      "published_date": "2024-07-24 16:07:11 UTC",
      "updated_date": "2024-07-24 16:07:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:35:04.044726"
    },
    {
      "arxiv_id": "2408.01452v1",
      "title": "Building a Domain-specific Guardrail Model in Production",
      "title_zh": "在生产环境中构建特定领域的护栏模型",
      "authors": [
        "Mohammad Niknazar",
        "Paul V Haley",
        "Latha Ramanan",
        "Sang T. Truong",
        "Yedendra Shrinivasan",
        "Ayan Kumar Bhowmick",
        "Prasenjit Dey",
        "Ashish Jagmohan",
        "Hema Maheshwari",
        "Shom Ponoth",
        "Robert Smith",
        "Aditya Vempaty",
        "Nick Haber",
        "Sanmi Koyejo",
        "Sharad Sundararajan"
      ],
      "abstract": "Generative AI holds the promise of enabling a range of sought-after\ncapabilities and revolutionizing workflows in various consumer and enterprise\nverticals. However, putting a model in production involves much more than just\ngenerating an output. It involves ensuring the model is reliable, safe,\nperformant and also adheres to the policy of operation in a particular domain.\nGuardrails as a necessity for models has evolved around the need to enforce\nappropriate behavior of models, especially when they are in production. In this\npaper, we use education as a use case, given its stringent requirements of the\nappropriateness of content in the domain, to demonstrate how a guardrail model\ncan be trained and deployed in production. Specifically, we describe our\nexperience in building a production-grade guardrail model for a K-12\neducational platform. We begin by formulating the requirements for deployment\nto this sensitive domain. We then describe the training and benchmarking of our\ndomain-specific guardrail model, which outperforms competing open- and closed-\ninstruction-tuned models of similar and larger size, on proprietary\neducation-related benchmarks and public benchmarks related to general aspects\nof safety. Finally, we detail the choices we made on architecture and the\noptimizations for deploying this service in production; these range across the\nstack from the hardware infrastructure to the serving layer to language model\ninference optimizations. We hope this paper will be instructive to other\npractitioners looking to create production-grade domain-specific services based\non generative AI and large language models.",
      "tldr_zh": "这篇论文探讨了在生产环境中部署生成式 AI 的挑战，特别是如何构建领域特定的守卫模型（guardrail model）以确保模型的可靠性、安全性和合规性。以 K-12 教育平台为用例，作者详细描述了制定部署要求、训练和基准测试过程，该模型在专有教育基准和公共安全基准上优于类似或更大规模的开放和封闭指令微调模型。最终，论文分享了架构选择和优化策略，包括硬件基础设施、serving 层和语言模型推理优化，为其他从业者提供构建生产级生成式 AI 服务的指导。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.01452v1",
      "published_date": "2024-07-24 15:53:29 UTC",
      "updated_date": "2024-07-24 15:53:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:35:16.107892"
    },
    {
      "arxiv_id": "2407.17374v2",
      "title": "Co-designing an AI Impact Assessment Report Template with AI Practitioners and AI Compliance Experts",
      "title_zh": "翻译失败",
      "authors": [
        "Edyta Bogucka",
        "Marios Constantinides",
        "Sanja Šćepanović",
        "Daniele Quercia"
      ],
      "abstract": "In the evolving landscape of AI regulation, it is crucial for companies to\nconduct impact assessments and document their compliance through comprehensive\nreports. However, current reports lack grounding in regulations and often focus\non specific aspects like privacy in relation to AI systems, without addressing\nthe real-world uses of these systems. Moreover, there is no systematic effort\nto design and evaluate these reports with both AI practitioners and AI\ncompliance experts. To address this gap, we conducted an iterative co-design\nprocess with 14 AI practitioners and 6 AI compliance experts and proposed a\ntemplate for impact assessment reports grounded in the EU AI Act, NIST's AI\nRisk Management Framework, and ISO 42001 AI Management System. We evaluated the\ntemplate by producing an impact assessment report for an AI-based meeting\ncompanion at a major tech company. A user study with 8 AI practitioners from\nthe same company and 5 AI compliance experts from industry and academia\nrevealed that our template effectively provides necessary information for\nimpact assessments and documents the broad impacts of AI systems. Participants\nenvisioned using the template not only at the pre-deployment stage for\ncompliance but also as a tool to guide the design stage of AI uses.",
      "tldr_zh": "该研究针对AI影响评估报告的不足（如缺乏法规基础和对AI系统实际应用的忽略），通过与14名AI从业者和6名AI合规专家的迭代式共同设计，提出一个基于EU AI Act、NIST's AI Risk Management Framework和ISO 42001的报告模板。模板旨在全面记录AI系统的广泛影响，并通过为一家科技公司AI会议伴侣生成报告进行评估。用户研究显示，该模板有效提供合规所需信息，并可扩展用于AI设计阶段，以提升报告的系统性和实用性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "K.4.1, K.4.2, H.5.3, D.2.9",
        "K.4.1; K.4.2; H.5.3; D.2.9"
      ],
      "primary_category": "cs.HC",
      "comment": "16 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.17374v2",
      "published_date": "2024-07-24 15:53:04 UTC",
      "updated_date": "2024-08-01 22:11:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:35:29.714723"
    },
    {
      "arxiv_id": "2407.17544v1",
      "title": "MathViz-E: A Case-study in Domain-Specialized Tool-Using Agents",
      "title_zh": "MathViz-E：领域专化工具使用代理的案例研究",
      "authors": [
        "Arya Bulusu",
        "Brandon Man",
        "Ashish Jagmohan",
        "Aditya Vempaty",
        "Jennifer Mari-Wyka",
        "Deepak Akkil"
      ],
      "abstract": "There has been significant recent interest in harnessing LLMs to control\nsoftware systems through multi-step reasoning, planning and tool-usage. While\nsome promising results have been obtained, application to specific domains\nraises several general issues including the control of specialized domain\ntools, the lack of existing datasets for training and evaluation, and the\nnon-triviality of automated system evaluation and improvement. In this paper,\nwe present a case-study where we examine these issues in the context of a\nspecific domain. Specifically, we present an automated math visualizer and\nsolver system for mathematical pedagogy. The system orchestrates mathematical\nsolvers and math graphing tools to produce accurate visualizations from simple\nnatural language commands. We describe the creation of specialized data-sets,\nand also develop an auto-evaluator to easily evaluate the outputs of our system\nby comparing them to ground-truth expressions. We have open sourced the\ndata-sets and code for the proposed system.",
      "tldr_zh": "本文以MathViz-E为例，研究了在特定领域使用大型语言模型(LLMs)构建工具使用代理时面临的挑战，包括控制专业工具、缺乏训练和评估数据集，以及自动化系统评估的复杂性。该系统是一个自动化数学可视化和求解框架，能通过多步推理、规划和工具使用，从简单自然语言命令生成准确的数学图形和解决方案。为此，研究者创建了专业数据集，并开发了一个auto-evaluator，通过与ground-truth表达式比较来评估系统输出。该框架的代码和数据集已开源，促进了相关领域的进一步发展。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17544v1",
      "published_date": "2024-07-24 15:45:07 UTC",
      "updated_date": "2024-07-24 15:45:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:35:40.199117"
    },
    {
      "arxiv_id": "2407.17361v1",
      "title": "MuST: Multi-Scale Transformers for Surgical Phase Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Alejandra Pérez",
        "Santiago Rodríguez",
        "Nicolás Ayobi",
        "Nicolás Aparicio",
        "Eugénie Dessevres",
        "Pablo Arbeláez"
      ],
      "abstract": "Phase recognition in surgical videos is crucial for enhancing computer-aided\nsurgical systems as it enables automated understanding of sequential procedural\nstages. Existing methods often rely on fixed temporal windows for video\nanalysis to identify dynamic surgical phases. Thus, they struggle to\nsimultaneously capture short-, mid-, and long-term information necessary to\nfully understand complex surgical procedures. To address these issues, we\npropose Multi-Scale Transformers for Surgical Phase Recognition (MuST), a novel\nTransformer-based approach that combines a Multi-Term Frame encoder with a\nTemporal Consistency Module to capture information across multiple temporal\nscales of a surgical video. Our Multi-Term Frame Encoder computes\ninterdependencies across a hierarchy of temporal scales by sampling sequences\nat increasing strides around the frame of interest. Furthermore, we employ a\nlong-term Transformer encoder over the frame embeddings to further enhance\nlong-term reasoning. MuST achieves higher performance than previous\nstate-of-the-art methods on three different public benchmarks.",
      "tldr_zh": "本研究针对手术视频阶段识别的关键挑战，提出 MuST（Multi-Scale Transformers），一种基于 Transformer 的新方法，能够同时捕捉短、中、长-term 时间信息，以提升对复杂手术过程的自动理解。MuST 包括 Multi-Term Frame Encoder，通过在感兴趣帧周围以递增步幅采样计算多尺度互依赖性，以及 Temporal Consistency Module 和长-term Transformer Encoder 来增强时间一致性和长期推理。在三个公共基准上，MuST 比现有最先进方法表现出更高的性能，为计算机辅助手术系统提供了更可靠的支持。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17361v1",
      "published_date": "2024-07-24 15:38:20 UTC",
      "updated_date": "2024-07-24 15:38:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:35:52.049917"
    },
    {
      "arxiv_id": "2407.17543v2",
      "title": "Dataset Distribution Impacts Model Fairness: Single vs. Multi-Task Learning",
      "title_zh": "数据集分布影响模型公平性：单任务学习 vs. 多任务学习",
      "authors": [
        "Ralf Raumanns",
        "Gerard Schouten",
        "Josien P. W. Pluim",
        "Veronika Cheplygina"
      ],
      "abstract": "The influence of bias in datasets on the fairness of model predictions is a\ntopic of ongoing research in various fields. We evaluate the performance of\nskin lesion classification using ResNet-based CNNs, focusing on patient sex\nvariations in training data and three different learning strategies. We present\na linear programming method for generating datasets with varying patient sex\nand class labels, taking into account the correlations between these variables.\nWe evaluated the model performance using three different learning strategies: a\nsingle-task model, a reinforcing multi-task model, and an adversarial learning\nscheme. Our observations include: 1) sex-specific training data yields better\nresults, 2) single-task models exhibit sex bias, 3) the reinforcement approach\ndoes not remove sex bias, 4) the adversarial model eliminates sex bias in cases\ninvolving only female patients, and 5) datasets that include male patients\nenhance model performance for the male subgroup, even when female patients are\nthe majority. To generalise these findings, in future research, we will examine\nmore demographic attributes, like age, and other possibly confounding factors,\nsuch as skin colour and artefacts in the skin lesions. We make all data and\nmodels available on GitHub.",
      "tldr_zh": "这篇论文探讨了数据集分布对模型公平性的影响，特别针对使用 ResNet-based CNNs 进行皮肤病变分类时的患者性别差异。研究者通过线性规划方法生成具有不同性别和类别标签的数据集，并比较了 single-task model、reinforcing multi-task model 和 adversarial learning scheme 三种学习策略的性能。关键发现包括：性别特定训练数据提升整体效果，single-task models 存在性别偏置，对抗学习在仅涉及女性患者时可消除偏置，而包括男性患者的多样化数据集能显著改善男性子群体的模型性能。未来工作将扩展到更多人口统计属性，如年龄，以及其他因素如皮肤颜色和伪像。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "Published in the FAIMI EPIMI 2024 Workshop",
      "pdf_url": "http://arxiv.org/pdf/2407.17543v2",
      "published_date": "2024-07-24 15:23:26 UTC",
      "updated_date": "2024-12-09 09:28:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:36:05.527614"
    },
    {
      "arxiv_id": "2407.17339v1",
      "title": "Preliminary study on artificial intelligence methods for cybersecurity threat detection in computer networks based on raw data packets",
      "title_zh": "翻译失败",
      "authors": [
        "Aleksander Ogonowski",
        "Michał Żebrowski",
        "Arkadiusz Ćwiek",
        "Tobiasz Jarosiewicz",
        "Konrad Klimaszewski",
        "Adam Padee",
        "Piotr Wasiuk",
        "Michał Wójcik"
      ],
      "abstract": "Most of the intrusion detection methods in computer networks are based on\ntraffic flow characteristics. However, this approach may not fully exploit the\npotential of deep learning algorithms to directly extract features and patterns\nfrom raw packets. Moreover, it impedes real-time monitoring due to the\nnecessity of waiting for the processing pipeline to complete and introduces\ndependencies on additional software components.\n  In this paper, we investigate deep learning methodologies capable of\ndetecting attacks in real-time directly from raw packet data within network\ntraffic. We propose a novel approach where packets are stacked into windows and\nseparately recognised, with a 2D image representation suitable for processing\nwith computer vision models. Our investigation utilizes the CIC IDS-2017\ndataset, which includes both benign traffic and prevalent real-world attacks,\nproviding a comprehensive foundation for our research.",
      "tldr_zh": "本研究探讨了基于人工智能的网络入侵检测方法，强调直接从原始数据包（raw packets）中提取特征，以克服传统基于流量特征（traffic flow characteristics）方法的局限性，如实时监控延迟和对额外软件依赖。论文提出了一种新颖方法，将数据包堆叠成窗口并转换为2D图像表示（2D image representation），然后使用计算机视觉模型（computer vision models）进行实时攻击检测。实验基于CIC IDS-2017数据集，展示了这一方法的潜力，有望提升深度学习（deep learning）算法在网络安全中的应用效率。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CR",
        "I.5.4; C.2.0; I.2.1"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitted to Computer Science Journal",
      "pdf_url": "http://arxiv.org/pdf/2407.17339v1",
      "published_date": "2024-07-24 15:04:00 UTC",
      "updated_date": "2024-07-24 15:04:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:36:25.864987"
    },
    {
      "arxiv_id": "2407.17324v2",
      "title": "Enhanced Deep Learning Methodologies and MRI Selection Techniques for Dementia Diagnosis in the Elderly Population",
      "title_zh": "增强的深度学习方法论和 MRI 选择技术用于老年人群的痴呆诊断",
      "authors": [
        "Nikolaos Ntampakis",
        "Konstantinos Diamantaras",
        "Ioanna Chouvarda",
        "Vasileios Argyriou",
        "Panagiotis Sarigianndis"
      ],
      "abstract": "Dementia, a debilitating neurological condition affecting millions worldwide,\npresents significant diagnostic challenges. In this work, we introduce a novel\nmethodology for the classification of demented and non-demented elderly\npatients using 3D brain Magnetic Resonance Imaging (MRI) scans. Our approach\nfeatures a unique technique for selectively processing MRI slices, focusing on\nthe most relevant brain regions and excluding less informative sections. This\nmethodology is complemented by a confidence-based classification committee\ncomposed of three custom deep learning models: Dem3D ResNet, Dem3D CNN, and\nDem3D EfficientNet. These models work synergistically to enhance\ndecision-making accuracy, leveraging their collective strengths. Tested on the\nOpen Access Series of Imaging Studies(OASIS) dataset, our method achieved an\nimpressive accuracy of 94.12%, surpassing existing methodologies. Furthermore,\nvalidation on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset\nconfirmed the robustness and generalizability of our approach. The use of\nexplainable AI (XAI) techniques and comprehensive ablation studies further\nsubstantiate the effectiveness of our techniques, providing insights into the\ndecision-making process and the importance of our methodology. This research\noffers a significant advancement in dementia diagnosis, providing a highly\naccurate and efficient tool for clinical applications.",
      "tldr_zh": "本研究提出了一种增强的深度学习方法和 MRI 选择技术，用于诊断老年人群中的痴呆症，通过选择性处理 3D 脑部 MRI 扫描的相關脑区来提高效率。方法结合了一个置信度-based 分类委员会，由三个自定义模型（Dem3D ResNet、Dem3D CNN 和 Dem3D EfficientNet）协同工作，以提升决策准确性。在 OASIS 数据集上，该方法实现了 94.12% 的准确率，并在 ADNI 数据集上验证了其鲁棒性和泛化性。通过 XAI 技术和消融研究，该研究为临床应用提供了高度准确且高效的痴呆诊断工具。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17324v2",
      "published_date": "2024-07-24 14:48:40 UTC",
      "updated_date": "2024-07-25 09:50:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:36:40.991597"
    },
    {
      "arxiv_id": "2407.17291v1",
      "title": "How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?",
      "title_zh": "翻译失败",
      "authors": [
        "Leo Yu-Ho Lo",
        "Huamin Qu"
      ],
      "abstract": "In this study, we address the growing issue of misleading charts, a prevalent\nproblem that undermines the integrity of information dissemination. Misleading\ncharts can distort the viewer's perception of data, leading to\nmisinterpretations and decisions based on false information. The development of\neffective automatic detection methods for misleading charts is an urgent field\nof research. The recent advancement of multimodal Large Language Models (LLMs)\nhas introduced a promising direction for addressing this challenge. We explored\nthe capabilities of these models in analyzing complex charts and assessing the\nimpact of different prompting strategies on the models' analyses. We utilized a\ndataset of misleading charts collected from the internet by prior research and\ncrafted nine distinct prompts, ranging from simple to complex, to test the\nability of four different multimodal LLMs in detecting over 21 different chart\nissues. Through three experiments--from initial exploration to detailed\nanalysis--we progressively gained insights into how to effectively prompt LLMs\nto identify misleading charts and developed strategies to address the\nscalability challenges encountered as we expanded our detection range from the\ninitial five issues to 21 issues in the final experiment. Our findings reveal\nthat multimodal LLMs possess a strong capability for chart comprehension and\ncritical thinking in data interpretation. There is significant potential in\nemploying multimodal LLMs to counter misleading information by supporting\ncritical thinking and enhancing visualization literacy. This study demonstrates\nthe applicability of LLMs in addressing the pressing concern of misleading\ncharts.",
      "tldr_zh": "这篇论文评估了多模态大型语言模型（multimodal LLMs）在检测误导性图表（misleading visualizations）方面的性能，以应对数据解读失真问题。研究者使用从互联网收集的数据集，设计了九种从简单到复杂的提示（prompting strategies），并通过三个实验测试了四个 LLMs，从最初的五个图表问题扩展到21个问题。结果显示，LLMs 在图表理解和批判性思维上表现出色，能够有效识别误导性问题，并为提升可视化素养和对抗虚假信息提供了可扩展策略。该研究证明了 LLMs 在自动检测误导性图表领域的巨大潜力。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CV"
      ],
      "primary_category": "cs.HC",
      "comment": "To be presented at IEEE VIS 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17291v1",
      "published_date": "2024-07-24 14:02:20 UTC",
      "updated_date": "2024-07-24 14:02:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:36:52.152953"
    },
    {
      "arxiv_id": "2407.17274v1",
      "title": "Revolutionizing Text-to-Image Retrieval as Autoregressive Token-to-Voken Generation",
      "title_zh": "将文本到图像检索革新为自回归的 Token",
      "authors": [
        "Yongqi Li",
        "Hongru Cai",
        "Wenjie Wang",
        "Leigang Qu",
        "Yinwei Wei",
        "Wenjie Li",
        "Liqiang Nie",
        "Tat-Seng Chua"
      ],
      "abstract": "Text-to-image retrieval is a fundamental task in multimedia processing,\naiming to retrieve semantically relevant cross-modal content. Traditional\nstudies have typically approached this task as a discriminative problem,\nmatching the text and image via the cross-attention mechanism (one-tower\nframework) or in a common embedding space (two-tower framework). Recently,\ngenerative cross-modal retrieval has emerged as a new research line, which\nassigns images with unique string identifiers and generates the target\nidentifier as the retrieval target. Despite its great potential, existing\ngenerative approaches are limited due to the following issues: insufficient\nvisual information in identifiers, misalignment with high-level semantics, and\nlearning gap towards the retrieval target. To address the above issues, we\npropose an autoregressive voken generation method, named AVG. AVG tokenizes\nimages into vokens, i.e., visual tokens, and innovatively formulates the\ntext-to-image retrieval task as a token-to-voken generation problem. AVG\ndiscretizes an image into a sequence of vokens as the identifier of the image,\nwhile maintaining the alignment with both the visual information and high-level\nsemantics of the image. Additionally, to bridge the learning gap between\ngenerative training and the retrieval target, we incorporate discriminative\ntraining to modify the learning direction during token-to-voken training.\nExtensive experiments demonstrate that AVG achieves superior results in both\neffectiveness and efficiency.",
      "tldr_zh": "该论文将文本到图像检索任务重新定义为自回归 token-to-voken 生成问题，以克服传统判别式方法和现有生成式方法的局限，如标识符中视觉信息不足和高层语义不对齐。作者提出 AVG 方法，将图像离散化为 vokens（视觉标记）序列，作为图像的唯一标识符，同时确保与视觉细节和高层语义的 alignment。针对生成训练与检索目标之间的学习差距，AVG 融入判别式训练来优化学习方向。实验结果显示，AVG 在有效性和效率上均优于现有方法。",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.MM",
      "comment": "Work in progress",
      "pdf_url": "http://arxiv.org/pdf/2407.17274v1",
      "published_date": "2024-07-24 13:39:51 UTC",
      "updated_date": "2024-07-24 13:39:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:37:04.640919"
    },
    {
      "arxiv_id": "2407.17265v1",
      "title": "SCIsegV2: A Universal Tool for Segmentation of Intramedullary Lesions in Spinal Cord Injury",
      "title_zh": "翻译失败",
      "authors": [
        "Enamundram Naga Karthik",
        "Jan Valošek",
        "Lynn Farner",
        "Dario Pfyffer",
        "Simon Schading-Sassenhausen",
        "Anna Lebret",
        "Gergely David",
        "Andrew C. Smith",
        "Kenneth A. Weber II",
        "Maryam Seif",
        "RHSCIR Network Imaging Group",
        "Patrick Freund",
        "Julien Cohen-Adad"
      ],
      "abstract": "Spinal cord injury (SCI) is a devastating incidence leading to permanent\nparalysis and loss of sensory-motor functions potentially resulting in the\nformation of lesions within the spinal cord. Imaging biomarkers obtained from\nmagnetic resonance imaging (MRI) scans can predict the functional recovery of\nindividuals with SCI and help choose the optimal treatment strategy. Currently,\nmost studies employ manual quantification of these MRI-derived biomarkers,\nwhich is a subjective and tedious task. In this work, we propose (i) a\nuniversal tool for the automatic segmentation of intramedullary SCI lesions,\ndubbed \\texttt{SCIsegV2}, and (ii) a method to automatically compute the width\nof the tissue bridges from the segmented lesion. Tissue bridges represent the\nspared spinal tissue adjacent to the lesion, which is associated with\nfunctional recovery in SCI patients. The tool was trained and validated on a\nheterogeneous dataset from 7 sites comprising patients from different SCI\nphases (acute, sub-acute, and chronic) and etiologies (traumatic SCI, ischemic\nSCI, and degenerative cervical myelopathy). Tissue bridges quantified\nautomatically did not significantly differ from those computed manually,\nsuggesting that the proposed automatic tool can be used to derive relevant MRI\nbiomarkers. \\texttt{SCIsegV2} and the automatic tissue bridges computation are\nopen-source and available in Spinal Cord Toolbox (v6.4 and above) via the\n\\texttt{sct\\_deepseg -task seg\\_sc\\_lesion\\_t2w\\_sci} and\n\\texttt{sct\\_analyze\\_lesion} functions, respectively.",
      "tldr_zh": "该研究针对脊髓损伤 (SCI) 的影像分析问题，提出 SCIsegV2 工具，这是一个通用自动分割工具，用于识别脊髓内病变，并开发了自动计算组织桥 (tissue bridges) 宽度的方法，以预测患者功能恢复和优化治疗策略。工具在来自 7 个站点的异质数据集上训练和验证，包括不同 SCI 阶段（如急性、亚急性和慢性）和病因（如外伤性 SCI、缺血性 SCI 和退行性颈髓病）。结果显示，自动计算的组织桥宽度与手动计算无显著差异，证明了 SCIsegV2 的可靠性和实用性；该工具已开源，可通过 Spinal Cord Toolbox (v6.4 及以上) 中的相应函数获取。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at MICCAI AMAI 2024 workshop",
      "pdf_url": "http://arxiv.org/pdf/2407.17265v1",
      "published_date": "2024-07-24 13:29:17 UTC",
      "updated_date": "2024-07-24 13:29:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:37:16.236162"
    },
    {
      "arxiv_id": "2407.17230v1",
      "title": "Improving ICD coding using Chapter based Named Entities and Attentional Models",
      "title_zh": "使用基于章节的命名实体和注意力模型改进 ICD 编码",
      "authors": [
        "Abhijith R. Beeravolu",
        "Mirjam Jonkman",
        "Sami Azam",
        "Friso De Boer"
      ],
      "abstract": "Recent advancements in natural language processing (NLP) have led to\nautomation in various domains. However, clinical NLP often relies on benchmark\ndatasets that may not reflect real-world scenarios accurately. Automatic ICD\ncoding, a vital NLP task, typically uses outdated and imbalanced datasets like\nMIMIC-III, with existing methods yielding micro-averaged F1 scores between 0.4\nand 0.7 due to many false positives. Our research introduces an enhanced\napproach to ICD coding that improves F1 scores by using chapter-based named\nentities and attentional models. This method categorizes discharge summaries\ninto ICD-9 Chapters and develops attentional models with chapter-specific data,\neliminating the need to consider external data for code identification. For\ncategorization, we use Chapter-IV to de-bias and influence key entities and\nweights without neural networks, creating accurate thresholds and providing\ninterpretability for human validation. Post-validation, we develop attentional\nmodels for three frequent and three non-frequent codes from Chapter-IV using\nBidirectional-Gated Recurrent Units (GRUs) with Attention and Transformer with\nMulti-head Attention architectures. The average Micro-F1 scores of 0.79 and\n0.81 from these models demonstrate significant performance improvements in ICD\ncoding.",
      "tldr_zh": "这篇论文针对 ICD coding 的问题，提出了一种改进方法，使用基于章节的命名实体和注意力模型来提升编码准确性，以解决现有方法在数据集如 MIMIC-III 上存在的假阳性和 F1 分数仅为 0.4 到 0.7 的局限。方法包括将出院总结分类到 ICD-9 Chapters，利用 Chapter-IV 去偏置关键实体和权重，通过设定准确阈值提供可解释性，并开发基于 Bidirectional-Gated Recurrent Units (GRUs) with Attention 和 Transformer with Multi-head Attention 的模型。实验结果显示，该方法在三个频繁代码和三个非频繁代码上实现了平均 Micro-F1 scores 为 0.79 和 0.81，显著提高了 ICD coding 的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "10 Pages",
      "pdf_url": "http://arxiv.org/pdf/2407.17230v1",
      "published_date": "2024-07-24 12:34:23 UTC",
      "updated_date": "2024-07-24 12:34:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:37:30.390701"
    },
    {
      "arxiv_id": "2407.17227v1",
      "title": "LEAN-GitHub: Compiling GitHub LEAN repositories for a versatile LEAN prover",
      "title_zh": "翻译失败",
      "authors": [
        "Zijian Wu",
        "Jiayu Wang",
        "Dahua Lin",
        "Kai Chen"
      ],
      "abstract": "Recently, large language models have presented promising results in aiding\nformal mathematical reasoning. However, their performance is restricted due to\nthe scarcity of formal theorem-proving data, which requires additional effort\nto be extracted from raw formal language corpora. Meanwhile, a significant\namount of human-written formal language corpora remains underutilized. To\naddress this issue, we propose LEAN-GitHub, a dataset consisting of large-scale\nformal data extracted from almost all Lean 4 repositories on GitHub. After\nfine-tuning InternLM-math-plus on this dataset, our model achieved accuracies\nof 48.8% with a single pass and 54.5% with 64 passes on the Lean 4 miniF2F\ntest, surpassing state-of-the-art method at 52%. And it also achieves\nstate-of-the-art on two other Lean 4 benchmarks (ProofNet and Putnam) targeting\ndifferent fields/levels of math. These results demonstrate that our proposed\ndataset is beneficial for formal reasoning on a wide range of math topics. We\nopen-source our model at https://GitHub. com/InternLM/InternLM-Math and our\ndata at https://huggingface.co/ datasets/InternLM/Lean-GitHub",
      "tldr_zh": "该研究提出LEAN-GitHub数据集，通过从GitHub上的几乎所有Lean 4仓库提取大规模正式数据，解决大型语言模型(Large Language Models)在正式数学推理中数据稀缺的问题。该数据集用于微调InternLM-math-plus模型，显著提升其性能。实验结果显示，微调后的模型在Lean 4 miniF2F测试集上单次通过准确率达48.8%，64次通过达54.5%，超过了最先进方法的52%；同时在ProofNet和Putnam基准上也达到最先进水平。这些成果证明了LEAN-GitHub数据集在广泛数学主题上的正式推理价值，并开源了模型和数据以促进进一步研究。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17227v1",
      "published_date": "2024-07-24 12:28:03 UTC",
      "updated_date": "2024-07-24 12:28:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:37:38.436112"
    },
    {
      "arxiv_id": "2407.17226v4",
      "title": "Sublinear Regret for a Class of Continuous-Time Linear-Quadratic Reinforcement Learning Problems",
      "title_zh": "一类连续时间线性二次强化学习问题的次线性遗憾",
      "authors": [
        "Yilie Huang",
        "Yanwei Jia",
        "Xun Yu Zhou"
      ],
      "abstract": "We study reinforcement learning (RL) for a class of continuous-time\nlinear-quadratic (LQ) control problems for diffusions, where states are\nscalar-valued and running control rewards are absent but volatilities of the\nstate processes depend on both state and control variables. We apply a\nmodel-free approach that relies neither on knowledge of model parameters nor on\ntheir estimations, and devise an RL algorithm to learn the optimal policy\nparameter directly. Our main contributions include the introduction of an\nexploration schedule and a regret analysis of the proposed algorithm. We\nprovide the convergence rate of the policy parameter to the optimal one, and\nprove that the algorithm achieves a regret bound of $O(N^{\\frac{3}{4}})$ up to\na logarithmic factor, where $N$ is the number of learning episodes. We conduct\na simulation study to validate the theoretical results and demonstrate the\neffectiveness and reliability of the proposed algorithm. We also perform\nnumerical comparisons between our method and those of the recent model-based\nstochastic LQ RL studies adapted to the state- and control-dependent volatility\nsetting, demonstrating a better performance of the former in terms of regret\nbounds.",
      "tldr_zh": "本研究探讨了连续时间线性二次 (Linear-Quadratic, LQ) 控制问题中的强化学习 (Reinforcement Learning, RL)，针对状态为标量值且波动率依赖于状态和控制变量的扩散过程，提出了一种无模型算法，直接学习最优策略参数。算法引入探索调度，并通过遗憾分析证明策略参数的收敛率，同时实现遗憾界为 $O(N^{\\frac{3}{4}})$（加上对数因子），其中 $N$ 为学习回合数。模拟实验验证了理论结果，并显示该方法在遗憾性能上优于基于模型的现有 LQ RL 方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY",
        "math.OC"
      ],
      "primary_category": "cs.LG",
      "comment": "42 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.17226v4",
      "published_date": "2024-07-24 12:26:21 UTC",
      "updated_date": "2025-04-08 19:11:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:37:49.917689"
    },
    {
      "arxiv_id": "2408.06352v1",
      "title": "Using Large Language Models to Compare Explainable Models for Smart Home Human Activity Recognition",
      "title_zh": "翻译失败",
      "authors": [
        "Michele Fiori",
        "Gabriele Civitarese",
        "Claudio Bettini"
      ],
      "abstract": "Recognizing daily activities with unobtrusive sensors in smart environments\nenables various healthcare applications. Monitoring how subjects perform\nactivities at home and their changes over time can reveal early symptoms of\nhealth issues, such as cognitive decline. Most approaches in this field use\ndeep learning models, which are often seen as black boxes mapping sensor data\nto activities. However, non-expert users like clinicians need to trust and\nunderstand these models' outputs. Thus, eXplainable AI (XAI) methods for Human\nActivity Recognition have emerged to provide intuitive natural language\nexplanations from these models. Different XAI methods generate different\nexplanations, and their effectiveness is typically evaluated through user\nsurveys, that are often challenging in terms of costs and fairness. This paper\nproposes an automatic evaluation method using Large Language Models (LLMs) to\nidentify, in a pool of candidates, the best XAI approach for non-expert users.\nOur preliminary results suggest that LLM evaluation aligns with user surveys.",
      "tldr_zh": "本研究探讨了在智能家居环境中使用eXplainable AI (XAI) 方法来解释Human Activity Recognition (HAR) 模型，从而帮助非专家用户（如临床医生）理解和信任模型输出。传统XAI评估依赖于昂贵的用户调查，而本文提出了一种利用Large Language Models (LLMs) 的自动评估方法，从多个候选XAI方法中选出最适合非专家用户的选项。该方法通过比较不同解释的生成效果，初步结果显示LLM评估与用户调查结果高度一致，为更高效的HAR模型解释提供了新途径。",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted for publication at UbiComp / ISWC 2024's XAIforU workshop",
      "pdf_url": "http://arxiv.org/pdf/2408.06352v1",
      "published_date": "2024-07-24 12:15:07 UTC",
      "updated_date": "2024-07-24 12:15:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:38:02.094416"
    },
    {
      "arxiv_id": "2407.17211v1",
      "title": "Testing Large Language Models on Driving Theory Knowledge and Skills for Connected Autonomous Vehicles",
      "title_zh": "翻译失败",
      "authors": [
        "Zuoyin Tang",
        "Jianhua He",
        "Dashuai Pei",
        "Kezhong Liu",
        "Tao Gao"
      ],
      "abstract": "Handling long tail corner cases is a major challenge faced by autonomous\nvehicles (AVs). While large language models (LLMs) hold great potentials to\nhandle the corner cases with excellent generalization and explanation\ncapabilities and received increasing research interest on application to\nautonomous driving, there are still technical barriers to be tackled, such as\nstrict model performance and huge computing resource requirements of LLMs. In\nthis paper, we investigate a new approach of applying remote or edge LLMs to\nsupport autonomous driving. A key issue for such LLM assisted driving system is\nthe assessment of LLMs on their understanding of driving theory and skills,\nensuring they are qualified to undertake safety critical driving assistance\ntasks for CAVs. We design and run driving theory tests for several proprietary\nLLM models (OpenAI GPT models, Baidu Ernie and Ali QWen) and open-source LLM\nmodels (Tsinghua MiniCPM-2B and MiniCPM-Llama3-V2.5) with more than 500\nmultiple-choices theory test questions. Model accuracy, cost and processing\nlatency are measured from the experiments. Experiment results show that while\nmodel GPT-4 passes the test with improved domain knowledge and Ernie has an\naccuracy of 85% (just below the 86% passing threshold), other LLM models\nincluding GPT-3.5 fail the test. For the test questions with images, the\nmultimodal model GPT4-o has an excellent accuracy result of 96%, and the\nMiniCPM-Llama3-V2.5 achieves an accuracy of 76%. While GPT-4 holds stronger\npotential for CAV driving assistance applications, the cost of using model GPT4\nis much higher, almost 50 times of that of using GPT3.5. The results can help\nmake decision on the use of the existing LLMs for CAV applications and\nbalancing on the model performance and cost.",
      "tldr_zh": "该研究评估了大型语言模型（LLMs）在自动驾驶车辆（CAVs）驾驶理论知识和技能方面的表现，旨在解决长尾问题并探讨远程或边缘LLMs的应用潜力。研究者设计了超过500道多选题测试，包括OpenAI GPT系列、Baidu Ernie、Ali QWen和Tsinghua MiniCPM模型，测量了准确率、成本和延迟；结果显示，GPT-4通过测试（准确率高），Ernie达到85%（接近及格阈值），而GPT-3.5等模型未通过，且带图像问题中GPT-4o准确率达96%。这些发现有助于权衡LLMs在CAV应用中的性能与成本平衡，为安全驾驶辅助决策提供指导。",
      "categories": [
        "cs.AI",
        "cs.NI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17211v1",
      "published_date": "2024-07-24 12:10:20 UTC",
      "updated_date": "2024-07-24 12:10:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:38:15.160764"
    },
    {
      "arxiv_id": "2407.17209v1",
      "title": "Nonverbal Immediacy Analysis in Education: A Multimodal Computational Model",
      "title_zh": "教育中的非语言亲和性分析：一个多模态计算模型",
      "authors": [
        "Uroš Petković",
        "Jonas Frenkel",
        "Olaf Hellwich",
        "Rebecca Lazarides"
      ],
      "abstract": "This paper introduces a novel computational approach for analyzing nonverbal\nsocial behavior in educational settings. Integrating multimodal behavioral\ncues, including facial expressions, gesture intensity, and spatial dynamics,\nthe model assesses the nonverbal immediacy (NVI) of teachers from RGB classroom\nvideos. A dataset of 400 30-second video segments from German classrooms was\nconstructed for model training and validation. The gesture intensity regressor\nachieved a correlation of 0.84, the perceived distance regressor 0.55, and the\nNVI model 0.44 with median human ratings. The model demonstrates the potential\nto provide a valuable support in nonverbal behavior assessment, approximating\nthe accuracy of individual human raters. Validated against both questionnaire\ndata and trained observer ratings, our models show moderate to strong\ncorrelations with relevant educational outcomes, indicating their efficacy in\nreflecting effective teaching behaviors. This research advances the objective\nassessment of nonverbal communication behaviors, opening new pathways for\neducational research.",
      "tldr_zh": "本研究提出了一种多模态计算模型，用于分析教育环境中教师的非语言亲近度 (Nonverbal Immediacy, NVI)，通过整合面部表情、手势强度和空间动态等行为线索，从 RGB 课堂视频中进行评估。研究构建了一个包含 400 个 30 秒视频段的数据集，并训练了相关回归器：手势强度相关性达 0.84、感知距离 0.55、NVI 模型 0.44，与人类评分的中位数高度一致。模型的性能接近单个人类评估者，并在与问卷数据和教育成果的相关性验证中显示中等到强的关联，为非语言沟通行为的客观评估提供新工具，推动教育研究的发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "68T45, 68T10, 68U10, 91E45",
        "I.2.10; I.5.4; K.3.1"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 3 figures. Camera-ready version for the SAB 2024: 17th\n  International Conference on the Simulation of Adaptive Behavior",
      "pdf_url": "http://arxiv.org/pdf/2407.17209v1",
      "published_date": "2024-07-24 12:09:07 UTC",
      "updated_date": "2024-07-24 12:09:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:38:26.897157"
    },
    {
      "arxiv_id": "2407.21054v3",
      "title": "Sentiment Reasoning for Healthcare",
      "title_zh": "翻译失败",
      "authors": [
        "Khai-Nguyen Nguyen",
        "Khai Le-Duc",
        "Bach Phan Tat",
        "Duy Le",
        "Long Vo-Dang",
        "Truong-Son Hy"
      ],
      "abstract": "Transparency in AI healthcare decision-making is crucial for building trust\namong AI and users. Incorporating reasoning capabilities enables Large Language\nModels (LLMs) to understand emotions in context, handle nuanced language, and\ninfer unstated sentiments. In this work, we introduce a new task -- Sentiment\nReasoning -- for both speech and text modalities, along with our proposed\nmultimodal multitask framework and dataset. Sentiment Reasoning is an auxiliary\ntask in sentiment analysis where the model predicts both the sentiment label\nand generates the rationale behind it based on the input transcript. Our study\nconducted on both human transcripts and Automatic Speech Recognition (ASR)\ntranscripts shows that Sentiment Reasoning helps improve model transparency by\nproviding rationale for model prediction with quality semantically comparable\nto humans while also improving model performance (1% increase in both accuracy\nand macro-F1) via rationale-augmented fine-tuning. Also, no significant\ndifference in the semantic quality of generated rationales between human and\nASR transcripts. All code, data (English-translated and Vietnamese) and models\nare published online: https://github.com/leduckhai/MultiMed.",
      "tldr_zh": "该研究强调AI在医疗决策中的透明度，提出一种新的Sentiment Reasoning任务，旨在让Large Language Models (LLMs)理解上下文情绪、处理细微语言并推断未明示情感。该任务适用于语音和文本模态，要求模型不仅预测情感标签，还生成背后的理由，并通过多模态多任务框架和数据集进行实现。实验结果显示，这种方法提升了模型透明度，生成的理由质量与人类相当，且通过理由增强微调提高了性能（准确率和macro-F1分数各增加1%）；此外，人为转录和Automatic Speech Recognition (ASR)转录在理由语义质量上无显著差异。所有代码、数据和模型已公开在GitHub上。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "primary_category": "cs.CL",
      "comment": "NeurIPS AIM-FM Workshop, 20 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.21054v3",
      "published_date": "2024-07-24 12:07:54 UTC",
      "updated_date": "2024-10-11 05:43:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:38:39.956047"
    },
    {
      "arxiv_id": "2407.17206v1",
      "title": "Take a Step and Reconsider: Sequence Decoding for Self-Improved Neural Combinatorial Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Jonathan Pirnay",
        "Dominik G. Grimm"
      ],
      "abstract": "The constructive approach within Neural Combinatorial Optimization (NCO)\ntreats a combinatorial optimization problem as a finite Markov decision\nprocess, where solutions are built incrementally through a sequence of\ndecisions guided by a neural policy network. To train the policy, recent\nresearch is shifting toward a 'self-improved' learning methodology that\naddresses the limitations of reinforcement learning and supervised approaches.\nHere, the policy is iteratively trained in a supervised manner, with solutions\nderived from the current policy serving as pseudo-labels. The way these\nsolutions are obtained from the policy determines the quality of the\npseudo-labels. In this paper, we present a simple and problem-independent\nsequence decoding method for self-improved learning based on sampling sequences\nwithout replacement. We incrementally follow the best solution found and repeat\nthe sampling process from intermediate partial solutions. By modifying the\npolicy to ignore previously sampled sequences, we force it to consider only\nunseen alternatives, thereby increasing solution diversity. Experimental\nresults for the Traveling Salesman and Capacitated Vehicle Routing Problem\ndemonstrate its strong performance. Furthermore, our method outperforms\nprevious NCO approaches on the Job Shop Scheduling Problem.",
      "tldr_zh": "本论文针对 Neural Combinatorial Optimization (NCO) 的自提升学习(self-improved learning)提出了一种序列解码方法，通过无替换采样从中间部分解决方案重复采样，并修改策略忽略已采样序列，以增加解决方案多样性。\n该方法简单且独立于具体问题，能够逐步跟随最佳解决方案并探索未见替代方案，从而提升伪标签的质量。\n实验结果表明，该方法在 Traveling Salesman Problem (TSP) 和 Capacitated Vehicle Routing Problem (CVRP) 上表现强劲，并在 Job Shop Scheduling Problem (JSSP) 上优于现有 NCO 方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ECAI-2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17206v1",
      "published_date": "2024-07-24 12:06:09 UTC",
      "updated_date": "2024-07-24 12:06:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:38:52.433257"
    },
    {
      "arxiv_id": "2407.17197v3",
      "title": "ALPI: Auto-Labeller with Proxy Injection for 3D Object Detection using 2D Labels Only",
      "title_zh": "ALPI：带有代理注入的自动",
      "authors": [
        "Saad Lahlali",
        "Nicolas Granger",
        "Hervé Le Borgne",
        "Quoc-Cuong Pham"
      ],
      "abstract": "3D object detection plays a crucial role in various applications such as\nautonomous vehicles, robotics and augmented reality. However, training 3D\ndetectors requires a costly precise annotation, which is a hindrance to scaling\nannotation to large datasets. To address this challenge, we propose a weakly\nsupervised 3D annotator that relies solely on 2D bounding box annotations from\nimages, along with size priors. One major problem is that supervising a 3D\ndetection model using only 2D boxes is not reliable due to ambiguities between\ndifferent 3D poses and their identical 2D projection. We introduce a simple yet\neffective and generic solution: we build 3D proxy objects with annotations by\nconstruction and add them to the training dataset. Our method requires only\nsize priors to adapt to new classes. To better align 2D supervision with 3D\ndetection, our method ensures depth invariance with a novel expression of the\n2D losses. Finally, to detect more challenging instances, our annotator follows\nan offline pseudo-labelling scheme which gradually improves its 3D\npseudo-labels. Extensive experiments on the KITTI dataset demonstrate that our\nmethod not only performs on-par or above previous works on the Car category,\nbut also achieves performance close to fully supervised methods on more\nchallenging classes. We further demonstrate the effectiveness and robustness of\nour method by being the first to experiment on the more challenging nuScenes\ndataset. We additionally propose a setting where weak labels are obtained from\na 2D detector pre-trained on MS-COCO instead of human annotations. The code is\navailable at https://github.com/CEA-LIST/ALPI",
      "tldr_zh": "本文提出 ALPI，一种自动标注器（Auto-Labeller with Proxy Injection），仅使用 2D 边界框标注和大小先验来实现 3D object detection，从而降低昂贵的 3D 标注成本。核心方法包括构建并注入 3D proxy objects 以解决 2D 投影歧义问题、引入新的 2D 损失表达式确保深度不变性，以及采用离线伪标注方案逐步优化 3D 伪标签。实验结果显示，在 KITTI 数据集上，ALPI 在 Car 类别的性能与现有方法相当，并在更具挑战性的类别上接近全监督方法；此外，在 nuScenes 数据集上的首次实验证明了其有效性和鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted at WACV2025",
      "pdf_url": "http://arxiv.org/pdf/2407.17197v3",
      "published_date": "2024-07-24 11:58:31 UTC",
      "updated_date": "2024-11-27 07:22:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:39:05.231577"
    },
    {
      "arxiv_id": "2407.17164v2",
      "title": "Robust Deep Hawkes Process under Label Noise of Both Event and Occurrence",
      "title_zh": "在事件和发生标签噪声下的稳健深度霍克斯过程",
      "authors": [
        "Xiaoyu Tan",
        "Bin Li",
        "Xihe Qiu",
        "Jingjing Huang",
        "Yinghui Xu",
        "Wei Chu"
      ],
      "abstract": "Integrating deep neural networks with the Hawkes process has significantly\nimproved predictive capabilities in finance, health informatics, and\ninformation technology. Nevertheless, these models often face challenges in\nreal-world settings, particularly due to substantial label noise. This issue is\nof significant concern in the medical field, where label noise can arise from\ndelayed updates in electronic medical records or misdiagnoses, leading to\nincreased prediction risks. Our research indicates that deep Hawkes process\nmodels exhibit reduced robustness when dealing with label noise, particularly\nwhen it affects both event types and timing. To address these challenges, we\nfirst investigate the influence of label noise in approximated intensity\nfunctions and present a novel framework, the Robust Deep Hawkes Process (RDHP),\nto overcome the impact of label noise on the intensity function of Hawkes\nmodels, considering both the events and their occurrences. We tested RDHP using\nmultiple open-source benchmarks with synthetic noise and conducted a case study\non obstructive sleep apnea-hypopnea syndrome (OSAHS) in a real-world setting\nwith inherent label noise. The results demonstrate that RDHP can effectively\nperform classification and regression tasks, even in the presence of noise\nrelated to events and their timing. To the best of our knowledge, this is the\nfirst study to successfully address both event and time label noise in deep\nHawkes process models, offering a promising solution for medical applications,\nspecifically in diagnosing OSAHS.",
      "tldr_zh": "这篇论文探讨了深度神经网络与Hawkes process整合在金融、健康信息学和信息技术中的预测能力，但强调了标签噪声（包括事件类型和发生时间）对模型鲁棒性的负面影响，尤其在医疗领域如电子医疗记录延迟或误诊。作者提出了一种新框架Robust Deep Hawkes Process (RDHP)，通过改进强度函数来处理这些噪声，并验证其在开源基准和真实OSAHS案例中的有效性。实验结果显示，RDHP显著提升了分类和回归任务的性能，为医疗应用如OSAHS诊断提供了可靠解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ECAI2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17164v2",
      "published_date": "2024-07-24 11:12:01 UTC",
      "updated_date": "2024-07-29 06:55:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:39:16.937873"
    },
    {
      "arxiv_id": "2407.17160v1",
      "title": "A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives",
      "title_zh": "翻译失败",
      "authors": [
        "Jan Lehečka",
        "Josef V. Psutka",
        "Luboš Šmídl",
        "Pavel Ircing",
        "Josef Psutka"
      ],
      "abstract": "In this paper, we are comparing monolingual Wav2Vec 2.0 models with various\nmultilingual models to see whether we could improve speech recognition\nperformance on a unique oral history archive containing a lot of mixed-language\nsentences. Our main goal is to push forward research on this unique dataset,\nwhich is an extremely valuable part of our cultural heritage. Our results\nsuggest that monolingual speech recognition models are, in most cases, superior\nto multilingual models, even when processing the oral history archive full of\nmixed-language sentences from non-native speakers. We also performed the same\nexperiments on the public CommonVoice dataset to verify our results. We are\ncontributing to the research community by releasing our pre-trained models to\nthe public.",
      "tldr_zh": "这篇论文比较了单语 Wav2Vec 2.0 模型与多语模型（包括双语和三语版本）在处理多语口述历史档案中的自动语音识别性能，重点关注包含混合语言句子的独特数据集。结果显示，单语模型在大多数情况下表现优于多语模型，即使面对非母语者发音的混合语言句子；为验证这些发现，研究者还在公共 CommonVoice 数据集上进行了相同的实验。论文的贡献包括公开发布预训练模型，以推动该文化遗产相关的研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to INTERSPEECH2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17160v1",
      "published_date": "2024-07-24 11:03:47 UTC",
      "updated_date": "2024-07-24 11:03:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:39:28.877999"
    },
    {
      "arxiv_id": "2407.17152v3",
      "title": "XMeCap: Meme Caption Generation with Sub-Image Adaptability",
      "title_zh": "翻译失败",
      "authors": [
        "Yuyan Chen",
        "Songzhou Yan",
        "Zhihong Zhu",
        "Zhixu Li",
        "Yanghua Xiao"
      ],
      "abstract": "Humor, deeply rooted in societal meanings and cultural details, poses a\nunique challenge for machines. While advances have been made in natural\nlanguage processing, real-world humor often thrives in a multi-modal context,\nencapsulated distinctively by memes. This paper poses a particular emphasis on\nthe impact of multi-images on meme captioning. After that, we introduce the\n\\textsc{XMeCap} framework, a novel approach that adopts supervised fine-tuning\nand reinforcement learning based on an innovative reward model, which factors\nin both global and local similarities between visuals and text. Our results,\nbenchmarked against contemporary models, manifest a marked improvement in\ncaption generation for both single-image and multi-image memes, as well as\ndifferent meme categories. \\textsc{XMeCap} achieves an average evaluation score\nof 75.85 for single-image memes and 66.32 for multi-image memes, outperforming\nthe best baseline by 3.71\\% and 4.82\\%, respectively. This research not only\nestablishes a new frontier in meme-related studies but also underscores the\npotential of machines in understanding and generating humor in a multi-modal\nsetting.",
      "tldr_zh": "该研究探讨了机器处理幽默的挑战，特别是多模态 memes 中多图像的影响，提出 XMeCap 框架以提升 meme 字幕生成。XMeCap 采用 supervised fine-tuning 和基于奖励模型的 reinforcement learning，考虑视觉和文本的全局及局部相似性，从而适应子图像变化。实验结果显示，该框架在单图像 memes 上平均得分 75.85（比最佳基线高 3.71%），多图像 memes 上得 66.32（高 4.82%），为机器理解和生成多模态幽默开辟新路径。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to MM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17152v3",
      "published_date": "2024-07-24 10:51:46 UTC",
      "updated_date": "2024-09-20 09:39:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:39:39.601048"
    },
    {
      "arxiv_id": "2407.17126v1",
      "title": "SDoH-GPT: Using Large Language Models to Extract Social Determinants of Health (SDoH)",
      "title_zh": "翻译失败",
      "authors": [
        "Bernardo Consoli",
        "Xizhi Wu",
        "Song Wang",
        "Xinyu Zhao",
        "Yanshan Wang",
        "Justin Rousseau",
        "Tom Hartvigsen",
        "Li Shen",
        "Huanmei Wu",
        "Yifan Peng",
        "Qi Long",
        "Tianlong Chen",
        "Ying Ding"
      ],
      "abstract": "Extracting social determinants of health (SDoH) from unstructured medical\nnotes depends heavily on labor-intensive annotations, which are typically\ntask-specific, hampering reusability and limiting sharing. In this study we\nintroduced SDoH-GPT, a simple and effective few-shot Large Language Model (LLM)\nmethod leveraging contrastive examples and concise instructions to extract SDoH\nwithout relying on extensive medical annotations or costly human intervention.\nIt achieved tenfold and twentyfold reductions in time and cost respectively,\nand superior consistency with human annotators measured by Cohen's kappa of up\nto 0.92. The innovative combination of SDoH-GPT and XGBoost leverages the\nstrengths of both, ensuring high accuracy and computational efficiency while\nconsistently maintaining 0.90+ AUROC scores. Testing across three distinct\ndatasets has confirmed its robustness and accuracy. This study highlights the\npotential of leveraging LLMs to revolutionize medical note classification,\ndemonstrating their capability to achieve highly accurate classifications with\nsignificantly reduced time and cost.",
      "tldr_zh": "本文提出 SDoH-GPT，这是一种基于 Large Language Model (LLM) 的少样本方法，通过对比示例和简洁指令，从非结构化医疗笔记中提取社会决定因素 (SDoH)，无需依赖大量注释或人工干预，从而实现时间和成本的十倍及二十倍减少。  \n该方法与人类注释者的一致性高达 Cohen's kappa 0.92，并在与 XGBoost 的结合中保持 AUROC 超过 0.90 的高准确性和计算效率。  \n在三个不同数据集上的测试证实了 SDoH-GPT 的稳健性和准确性，展示了 LLM 在医疗笔记分类中的革命潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17126v1",
      "published_date": "2024-07-24 09:57:51 UTC",
      "updated_date": "2024-07-24 09:57:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:39:53.453105"
    },
    {
      "arxiv_id": "2407.17120v1",
      "title": "Parameter-Efficient Fine-Tuning for Continual Learning: A Neural Tangent Kernel Perspective",
      "title_zh": "翻译失败",
      "authors": [
        "Jingren Liu",
        "Zhong Ji",
        "YunLong Yu",
        "Jiale Cao",
        "Yanwei Pang",
        "Jungong Han",
        "Xuelong Li"
      ],
      "abstract": "Parameter-efficient fine-tuning for continual learning (PEFT-CL) has shown\npromise in adapting pre-trained models to sequential tasks while mitigating\ncatastrophic forgetting problem. However, understanding the mechanisms that\ndictate continual performance in this paradigm remains elusive. To tackle this\ncomplexity, we undertake a rigorous analysis of PEFT-CL dynamics to derive\nrelevant metrics for continual scenarios using Neural Tangent Kernel (NTK)\ntheory. With the aid of NTK as a mathematical analysis tool, we recast the\nchallenge of test-time forgetting into the quantifiable generalization gaps\nduring training, identifying three key factors that influence these gaps and\nthe performance of PEFT-CL: training sample size, task-level feature\northogonality, and regularization. To address these challenges, we introduce\nNTK-CL, a novel framework that eliminates task-specific parameter storage while\nadaptively generating task-relevant features. Aligning with theoretical\nguidance, NTK-CL triples the feature representation of each sample,\ntheoretically and empirically reducing the magnitude of both task-interplay and\ntask-specific generalization gaps. Grounded in NTK analysis, our approach\nimposes an adaptive exponential moving average mechanism and constraints on\ntask-level feature orthogonality, maintaining intra-task NTK forms while\nattenuating inter-task NTK forms. Ultimately, by fine-tuning optimizable\nparameters with appropriate regularization, NTK-CL achieves state-of-the-art\nperformance on established PEFT-CL benchmarks. This work provides a theoretical\nfoundation for understanding and improving PEFT-CL models, offering insights\ninto the interplay between feature representation, task orthogonality, and\ngeneralization, contributing to the development of more efficient continual\nlearning systems.",
      "tldr_zh": "这篇论文从 Neural Tangent Kernel (NTK) 理论视角分析了 Parameter-Efficient Fine-Tuning for Continual Learning (PEFT-CL)，识别了训练样本大小、任务级特征正交性和正则化等关键因素对性能的影响，以量化泛化差距并缓解灾难性遗忘问题。作者提出了 NTK-CL 框架，该框架不存储任务特定参数，而是通过自适应生成任务相关特征、应用指数移动平均机制和约束特征正交性，来增强特征表示并减少任务间交互。实验结果显示，NTK-CL 在 PEFT-CL 基准上实现了最先进性能，并为理解和优化持续学习系统提供了重要理论基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17120v1",
      "published_date": "2024-07-24 09:30:04 UTC",
      "updated_date": "2024-07-24 09:30:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:40:03.852002"
    },
    {
      "arxiv_id": "2407.17117v1",
      "title": "EverAdapt: Continuous Adaptation for Dynamic Machine Fault Diagnosis Environments",
      "title_zh": "EverAdapt：动态机器故障诊断环境的连续适应",
      "authors": [
        "Edward",
        "Mohamed Ragab",
        "Yuecong Xu",
        "Min Wu",
        "Yuecong Xu",
        "Zhenghua Chen",
        "Abdulla Alseiari",
        "Xiaoli Li"
      ],
      "abstract": "Unsupervised Domain Adaptation (UDA) has emerged as a key solution in\ndata-driven fault diagnosis, addressing domain shift where models underperform\nin changing environments. However, under the realm of continually changing\nenvironments, UDA tends to underperform on previously seen domains when\nadapting to new ones - a problem known as catastrophic forgetting. To address\nthis limitation, we introduce the EverAdapt framework, specifically designed\nfor continuous model adaptation in dynamic environments. Central to EverAdapt\nis a novel Continual Batch Normalization (CBN), which leverages source domain\nstatistics as a reference point to standardize feature representations across\ndomains. EverAdapt not only retains statistical information from previous\ndomains but also adapts effectively to new scenarios. Complementing CBN, we\ndesign a class-conditional domain alignment module for effective integration of\ntarget domains, and a Sample-efficient Replay strategy to reinforce memory\nretention. Experiments on real-world datasets demonstrate EverAdapt superiority\nin maintaining robust fault diagnosis in dynamic environments. Our code is\navailable: https://github.com/mohamedr002/EverAdapt",
      "tldr_zh": "该论文提出 EverAdapt 框架，用于解决机器故障诊断中动态环境的连续适应问题，特别是避免无监督域适应 (UDA) 在新域适应时发生的灾难性遗忘。\nEverAdapt 的核心是 Continual Batch Normalization (CBN) 模块，该模块利用源域统计作为参考点来标准化特征表示，确保保留先前域的信息并有效适应新场景。\n框架还整合了 class-conditional domain alignment 模块和 Sample-efficient Replay 策略，以优化目标域整合和记忆强化。\n实验结果显示，EverAdapt 在真实世界数据集上表现出色，显著提升了动态环境下的故障诊断鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17117v1",
      "published_date": "2024-07-24 09:25:54 UTC",
      "updated_date": "2024-07-24 09:25:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:40:26.179780"
    },
    {
      "arxiv_id": "2407.17112v2",
      "title": "Neural Dueling Bandits: Preference-Based Optimization with Human Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Arun Verma",
        "Zhongxiang Dai",
        "Xiaoqiang Lin",
        "Patrick Jaillet",
        "Bryan Kian Hsiang Low"
      ],
      "abstract": "Contextual dueling bandit is used to model the bandit problems, where a\nlearner's goal is to find the best arm for a given context using observed noisy\nhuman preference feedback over the selected arms for the past contexts.\nHowever, existing algorithms assume the reward function is linear, which can be\ncomplex and non-linear in many real-life applications like online\nrecommendations or ranking web search results. To overcome this challenge, we\nuse a neural network to estimate the reward function using preference feedback\nfor the previously selected arms. We propose upper confidence bound- and\nThompson sampling-based algorithms with sub-linear regret guarantees that\nefficiently select arms in each round. We also extend our theoretical results\nto contextual bandit problems with binary feedback, which is in itself a\nnon-trivial contribution. Experimental results on the problem instances derived\nfrom synthetic datasets corroborate our theoretical results.",
      "tldr_zh": "本论文提出了一种神经决斗式 Bandits 框架，用于基于人类反馈的偏好优化问题，旨在处理奖励函数非线性的实际场景，如在线推荐或搜索排名。作者使用神经网络估计奖励函数，并设计了基于上置信界 (UCB) 和 Thompson Sampling 的算法，这些算法具有次线性遗憾保证，能够高效选择最佳 arm。论文还扩展了理论结果到二元反馈的上下文 Bandits 问题，并通过合成数据集的实验验证了算法的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at ICLR 2025. Also, accepted at ICML 2024 Workshop on\n  Foundations of Reinforcement Learning and Control",
      "pdf_url": "http://arxiv.org/pdf/2407.17112v2",
      "published_date": "2024-07-24 09:23:22 UTC",
      "updated_date": "2025-04-16 11:44:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:40:36.430893"
    },
    {
      "arxiv_id": "2407.17101v1",
      "title": "PiPa++: Towards Unification of Domain Adaptive Semantic Segmentation via Self-supervised Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Mu Chen",
        "Zhedong Zheng",
        "Yi Yang"
      ],
      "abstract": "Unsupervised domain adaptive segmentation aims to improve the segmentation\naccuracy of models on target domains without relying on labeled data from those\ndomains. This approach is crucial when labeled target domain data is scarce or\nunavailable. It seeks to align the feature representations of the source domain\n(where labeled data is available) and the target domain (where only unlabeled\ndata is present), thus enabling the model to generalize well to the target\ndomain. Current image- and video-level domain adaptation have been addressed\nusing different and specialized frameworks, training strategies and\noptimizations despite their underlying connections. In this paper, we propose a\nunified framework PiPa++, which leverages the core idea of ``comparing'' to (1)\nexplicitly encourage learning of discriminative pixel-wise features with\nintraclass compactness and inter-class separability, (2) promote the robust\nfeature learning of the identical patch against different contexts or\nfluctuations, and (3) enable the learning of temporal continuity under dynamic\nenvironments. With the designed task-smart contrastive sampling strategy,\nPiPa++ enables the mining of more informative training samples according to the\ntask demand. Extensive experiments demonstrate the effectiveness of our method\non both image-level and video-level domain adaption benchmarks. Moreover, the\nproposed method is compatible with other UDA approaches to further improve the\nperformance without introducing extra parameters.",
      "tldr_zh": "本文提出 PiPa++，一个基于自监督学习的统一框架，旨在解决无监督域适应语义分割问题，通过特征表示对齐来提高模型在目标域（无标注数据）的分割准确率。PiPa++ 核心机制包括对比学习以增强像素级特征的 intra-class compactness 和 inter-class separability、促进相同补丁在不同上下文下的鲁棒学习，以及启用动态环境下的时间连续性学习，并采用 task-smart contrastive sampling strategy 来挖掘信息丰富的训练样本。实验结果显示，该方法在图像级和视频级域适应基准上表现出色，且可与其他 UDA 方法兼容，进一步提升性能而不增加额外参数。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "This study is under IEEE TMM review. arXiv admin note: substantial\n  text overlap with arXiv:2211.07609",
      "pdf_url": "http://arxiv.org/pdf/2407.17101v1",
      "published_date": "2024-07-24 08:53:29 UTC",
      "updated_date": "2024-07-24 08:53:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:40:49.936172"
    },
    {
      "arxiv_id": "2407.17097v1",
      "title": "Towards Robust Knowledge Tracing Models via k-Sparse Attention",
      "title_zh": "翻译失败",
      "authors": [
        "Shuyan Huang",
        "Zitao Liu",
        "Xiangyu Zhao",
        "Weiqi Luo",
        "Jian Weng"
      ],
      "abstract": "Knowledge tracing (KT) is the problem of predicting students' future\nperformance based on their historical interaction sequences. With the advanced\ncapability of capturing contextual long-term dependency, attention mechanism\nbecomes one of the essential components in many deep learning based KT (DLKT)\nmodels. In spite of the impressive performance achieved by these attentional\nDLKT models, many of them are often vulnerable to run the risk of overfitting,\nespecially on small-scale educational datasets. Therefore, in this paper, we\npropose \\textsc{sparseKT}, a simple yet effective framework to improve the\nrobustness and generalization of the attention based DLKT approaches.\nSpecifically, we incorporate a k-selection module to only pick items with the\nhighest attention scores. We propose two sparsification heuristics : (1)\nsoft-thresholding sparse attention and (2) top-$K$ sparse attention. We show\nthat our \\textsc{sparseKT} is able to help attentional KT models get rid of\nirrelevant student interactions and have comparable predictive performance when\ncompared to 11 state-of-the-art KT models on three publicly available\nreal-world educational datasets. To encourage reproducible research, we make\nour data and code publicly available at\n\\url{https://github.com/pykt-team/pykt-toolkit}\\footnote{We merged our model to\nthe \\textsc{pyKT} benchmark at \\url{https://pykt.org/}.}.",
      "tldr_zh": "这篇论文针对知识追踪 (KT) 模型中注意力机制的过拟合问题，提出了一种名为 sparseKT 的简单有效框架，以提升基于深度学习的 KT (DLKT) 模型的鲁棒性和泛化能力。sparseKT 通过 k-selection 模块和两种稀疏化启发式方法（soft-thresholding sparse attention 和 top-K sparse attention），仅选择最高注意力分数的项目，从而过滤无关学生互动。实验结果显示，sparseKT 在三个真实教育数据集上与 11 个最先进 KT 模型的预测性能相当，并开源了代码以支持可重复研究。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted at SIGIR'2023 (revised version with additional results)",
      "pdf_url": "http://arxiv.org/pdf/2407.17097v1",
      "published_date": "2024-07-24 08:49:18 UTC",
      "updated_date": "2024-07-24 08:49:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:41:01.078301"
    },
    {
      "arxiv_id": "2407.17537v1",
      "title": "A process algebraic framework for multi-agent dynamic epistemic systems",
      "title_zh": "翻译失败",
      "authors": [
        "Alessandro Aldini"
      ],
      "abstract": "This paper combines the classical model of labeled transition systems with\nthe epistemic model for reasoning about knowledge. The result is a unifying\nframework for modeling and analyzing multi-agent, knowledge-based, dynamic\nsystems. On the modeling side, we propose a process algebraic, agent-oriented\nspecification language that makes such a framework easy to use for practical\npurposes. On the verification side, we define a modal logic encompassing\ntemporal and epistemic operators.",
      "tldr_zh": "这篇论文提出了一种结合 labeled transition systems 和 epistemic model 的统一框架，用于建模和分析多智能体、基于知识的动态系统。框架采用 process algebraic 的、agent-oriented 规范语言，使其易于实际应用。在验证方面，他们定义了一种模态逻辑，包括 temporal 和 epistemic operators，以支持系统的全面分析。",
      "categories": [
        "cs.AI",
        "cs.FL",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17537v1",
      "published_date": "2024-07-24 08:35:50 UTC",
      "updated_date": "2024-07-24 08:35:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:41:12.862216"
    },
    {
      "arxiv_id": "2407.17085v1",
      "title": "OVR: A Dataset for Open Vocabulary Temporal Repetition Counting in Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Debidatta Dwibedi",
        "Yusuf Aytar",
        "Jonathan Tompson",
        "Andrew Zisserman"
      ],
      "abstract": "We introduce a dataset of annotations of temporal repetitions in videos. The\ndataset, OVR (pronounced as over), contains annotations for over 72K videos,\nwith each annotation specifying the number of repetitions, the start and end\ntime of the repetitions, and also a free-form description of what is repeating.\nThe annotations are provided for videos sourced from Kinetics and Ego4D, and\nconsequently cover both Exo and Ego viewing conditions, with a huge variety of\nactions and activities. Moreover, OVR is almost an order of magnitude larger\nthan previous datasets for video repetition. We also propose a baseline\ntransformer-based counting model, OVRCounter, that can localise and count\nrepetitions in videos that are up to 320 frames long. The model is trained and\nevaluated on the OVR dataset, and its performance assessed with and without\nusing text to specify the target class to count. The performance is also\ncompared to a prior repetition counting model. The dataset is available for\ndownload at: https://sites.google.com/view/openvocabreps/",
      "tldr_zh": "本研究引入了OVR数据集，用于开放词汇的视频中时间重复计数，包含超过72K视频的注释，每个注释包括重复次数、开始和结束时间以及自由形式的重复描述。数据集来源于Kinetics和Ego4D视频，涵盖Exo和Ego视角的多种动作和活动，比先前数据集大一个数量级。研究者提出一个基于Transformer的基线模型OVRCounter，能定位和计数长达320帧的视频重复，并通过实验评估其性能，包括使用文本指定目标类别时的表现，并与现有模型进行比较。数据集可从指定链接下载，为视频重复计数任务提供宝贵资源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17085v1",
      "published_date": "2024-07-24 08:22:49 UTC",
      "updated_date": "2024-07-24 08:22:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:41:24.814947"
    },
    {
      "arxiv_id": "2407.17083v1",
      "title": "When Text and Images Don't Mix: Bias-Correcting Language-Image Similarity Scores for Anomaly Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Adam Goodge",
        "Bryan Hooi",
        "Wee Siong Ng"
      ],
      "abstract": "Contrastive Language-Image Pre-training (CLIP) achieves remarkable\nperformance in various downstream tasks through the alignment of image and text\ninput embeddings and holds great promise for anomaly detection. However, our\nempirical experiments show that the embeddings of text inputs unexpectedly\ntightly cluster together, far away from image embeddings, contrary to the\nmodel's contrastive training objective to align image-text input pairs. We show\nthat this phenomenon induces a `similarity bias' - in which false negative and\nfalse positive errors occur due to bias in the similarities between images and\nthe normal label text embeddings. To address this bias, we propose a novel\nmethodology called BLISS which directly accounts for this similarity bias\nthrough the use of an auxiliary, external set of text inputs. BLISS is simple,\nit does not require strong inductive biases about anomalous behaviour nor an\nexpensive training process, and it significantly outperforms baseline methods\non benchmark image datasets, even when access to normal data is extremely\nlimited.",
      "tldr_zh": "这项研究发现，Contrastive Language-Image Pre-training (CLIP) 在异常检测任务中虽表现出色，但文本输入嵌入向量意外地紧密聚类，与图像嵌入分离，导致了“similarity bias”，从而引发假阴性和假阳性错误。作者提出了一种新方法 BLISS，通过引入一个辅助外部文本集，直接修正图像与正常标签文本嵌入之间的相似性偏差。BLISS 不依赖于异常行为的强归纳偏差，也不需复杂的训练过程，在基准图像数据集上显著优于基线方法，即使正常数据非常有限。总的来说，该方法为提高 CLIP 的鲁棒性提供了简单有效的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17083v1",
      "published_date": "2024-07-24 08:20:02 UTC",
      "updated_date": "2024-07-24 08:20:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:41:46.672450"
    },
    {
      "arxiv_id": "2407.17081v1",
      "title": "A Survey Forest Diagram : Gain a Divergent Insight View on a Specific Research Topic",
      "title_zh": "翻译失败",
      "authors": [
        "Jinghong Li",
        "Wen Gu",
        "Koichi Ota",
        "Shinobu Hasegawa"
      ],
      "abstract": "With the exponential growth in the number of papers and the trend of AI\nresearch, the use of Generative AI for information retrieval and\nquestion-answering has become popular for conducting research surveys. However,\nnovice researchers unfamiliar with a particular field may not significantly\nimprove their efficiency in interacting with Generative AI because they have\nnot developed divergent thinking in that field. This study aims to develop an\nin-depth Survey Forest Diagram that guides novice researchers in divergent\nthinking about the research topic by indicating the citation clues among\nmultiple papers, to help expand the survey perspective for novice researchers.",
      "tldr_zh": "该论文探讨了生成式 AI 在信息检索和问答中的应用，但强调新手研究者由于缺乏领域发散性思维（divergent thinking），无法显著提升研究效率。论文提出开发一个深入的 Survey Forest Diagram，通过显示多篇论文之间的引用线索，引导新手研究者进行发散性思考。最终，该方法旨在扩展新手的研究调查视角，帮助他们更有效地开展特定主题的研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.DL"
      ],
      "primary_category": "cs.CL",
      "comment": "This paper will submit to IEEE SMC 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17081v1",
      "published_date": "2024-07-24 08:17:37 UTC",
      "updated_date": "2024-07-24 08:17:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:41:57.964818"
    },
    {
      "arxiv_id": "2407.17070v1",
      "title": "Curriculum Negative Mining For Temporal Networks",
      "title_zh": "针对时间网络的课程负样本",
      "authors": [
        "Ziyue Chen",
        "Tongya Zheng",
        "Mingli Song"
      ],
      "abstract": "Temporal networks are effective in capturing the evolving interactions of\nnetworks over time, such as social networks and e-commerce networks. In recent\nyears, researchers have primarily concentrated on developing specific model\narchitectures for Temporal Graph Neural Networks (TGNNs) in order to improve\nthe representation quality of temporal nodes and edges. However, limited\nattention has been given to the quality of negative samples during the training\nof TGNNs. When compared with static networks, temporal networks present two\nspecific challenges for negative sampling: positive sparsity and positive\nshift. Positive sparsity refers to the presence of a single positive sample\namidst numerous negative samples at each timestamp, while positive shift\nrelates to the variations in positive samples across different timestamps. To\nrobustly address these challenges in training TGNNs, we introduce Curriculum\nNegative Mining (CurNM), a model-aware curriculum learning framework that\nadaptively adjusts the difficulty of negative samples. Within this framework,\nwe first establish a dynamically updated negative pool that balances random,\nhistorical, and hard negatives to address the challenges posed by positive\nsparsity. Secondly, we implement a temporal-aware negative selection module\nthat focuses on learning from the disentangled factors of recently active\nedges, thus accurately capturing shifting preferences. Extensive experiments on\n12 datasets and 3 TGNNs demonstrate that our method outperforms baseline\nmethods by a significant margin. Additionally, thorough ablation studies and\nparameter sensitivity experiments verify the usefulness and robustness of our\napproach. Our code is available at https://github.com/zziyue83/CurNM.",
      "tldr_zh": "本研究针对时间网络（Temporal Networks）在训练 Temporal Graph Neural Networks (TGNNs) 时负样本质量问题，特别处理了 positive sparsity（每个时间戳正样本稀疏）和 positive shift（正样本随时间变化）的挑战。作者提出 Curriculum Negative Mining (CurNM)，一个模型感知的课程学习框架，包括动态更新的负样本池（平衡随机、历史和困难负样本）和时间感知的负样本选择模块，以适应性调整负样本难度并捕捉活跃边的演变偏好。在12个数据集和3个TGNNs上的广泛实验中，CurNM显著优于基线方法，并通过消融研究和参数敏感性实验验证了其有效性和鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17070v1",
      "published_date": "2024-07-24 07:55:49 UTC",
      "updated_date": "2024-07-24 07:55:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:42:12.823141"
    },
    {
      "arxiv_id": "2407.17065v1",
      "title": "PatchFinder: A Two-Phase Approach to Security Patch Tracing for Disclosed Vulnerabilities in Open-Source Software",
      "title_zh": "PatchFinder: 一种针对开源软件中已",
      "authors": [
        "Kaixuan Li",
        "Jian Zhang",
        "Sen Chen",
        "Han Liu",
        "Yang Liu",
        "Yixiang Chen"
      ],
      "abstract": "Open-source software (OSS) vulnerabilities are increasingly prevalent,\nemphasizing the importance of security patches. However, in widely used\nsecurity platforms like NVD, a substantial number of CVE records still lack\ntrace links to patches. Although rank-based approaches have been proposed for\nsecurity patch tracing, they heavily rely on handcrafted features in a\nsingle-step framework, which limits their effectiveness. In this paper, we\npropose PatchFinder, a two-phase framework with end-to-end correlation learning\nfor better-tracing security patches. In the **initial retrieval** phase, we\nemploy a hybrid patch retriever to account for both lexical and semantic\nmatching based on the code changes and the description of a CVE, to narrow down\nthe search space by extracting those commits as candidates that are similar to\nthe CVE descriptions. Afterwards, in the **re-ranking** phase, we design an\nend-to-end architecture under the supervised fine-tuning paradigm for learning\nthe semantic correlations between CVE descriptions and commits. In this way, we\ncan automatically rank the candidates based on their correlation scores while\nmaintaining low computation overhead. We evaluated our system against 4,789\nCVEs from 532 OSS projects. The results are highly promising: PatchFinder\nachieves a Recall@10 of 80.63% and a Mean Reciprocal Rank (MRR) of 0.7951.\nMoreover, the Manual Effort@10 required is curtailed to 2.77, marking a 1.94\ntimes improvement over current leading methods. When applying PatchFinder in\npractice, we initially identified 533 patch commits and submitted them to the\nofficial, 482 of which have been confirmed by CVE Numbering Authorities.",
      "tldr_zh": "本文提出 PatchFinder，一种两阶段框架，用于追踪开源软件 (OSS) 中已公开漏洞的补丁，旨在解决现有方法依赖手工特征和单步框架的局限性。第一阶段通过混合补丁检索器结合词汇和语义匹配，基于 CVE 描述和代码变化缩小候选提交范围；第二阶段采用端到端架构进行监督微调，学习 CVE 描述与提交的语义相关性，并重新排名以降低计算开销。在 4789 个 CVE 和 532 个 OSS 项目的评估中，PatchFinder 实现了 Recall@10 为 80.63%、MRR 为 0.7951%，并将 Manual Effort@10 降低到 2.77，是当前领先方法的 1.94 倍改进，且实际应用中确认了 482 个补丁提交。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "to appear at ISSTA 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17065v1",
      "published_date": "2024-07-24 07:46:24 UTC",
      "updated_date": "2024-07-24 07:46:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:42:25.065483"
    },
    {
      "arxiv_id": "2407.17060v1",
      "title": "High Efficiency Image Compression for Large Visual-Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Binzhe Li",
        "Shurun Wang",
        "Shiqi Wang",
        "Yan Ye"
      ],
      "abstract": "In recent years, large visual language models (LVLMs) have shown impressive\nperformance and promising generalization capability in multi-modal tasks, thus\nreplacing humans as receivers of visual information in various application\nscenarios. In this paper, we pioneer to propose a variable bitrate image\ncompression framework consisting of a pre-editing module and an end-to-end\ncodec to achieve promising rate-accuracy performance for different LVLMs. In\nparticular, instead of optimizing an adaptive pre-editing network towards a\nparticular task or several representative tasks, we propose a new optimization\nstrategy tailored for LVLMs, which is designed based on the representation and\ndiscrimination capability with token-level distortion and rank. The pre-editing\nmodule and the variable bitrate end-to-end image codec are jointly trained by\nthe losses based on semantic tokens of the large model, which introduce\nenhanced generalization capability for various data and tasks. {Experimental\nresults demonstrate that the proposed framework could efficiently achieve much\nbetter rate-accuracy performance compared to the state-of-the-art coding\nstandard, Versatile Video Coding.} Meanwhile, experiments with multi-modal\ntasks have revealed the robustness and generalization capability of the\nproposed framework.",
      "tldr_zh": "该论文提出了一种高效图像压缩框架，针对大型视觉语言模型 (LVLMs) 以实现可变比特率的压缩，从而在不同任务中优化速率-准确性性能。该框架包括一个预编辑模块和端到端编解码器，通过基于 token-level distortion 和 rank 的新优化策略进行训练，利用 LVLMs 的语义 tokens 损失来提升泛化能力。实验结果显示，该框架相较于 Versatile Video Coding 标准，在图像压缩性能上大幅提升，并展示了在多模态任务中的鲁棒性和泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17060v1",
      "published_date": "2024-07-24 07:37:12 UTC",
      "updated_date": "2024-07-24 07:37:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:42:29.879240"
    },
    {
      "arxiv_id": "2407.17040v2",
      "title": "Time Series Imputation with Multivariate Radial Basis Function Neural Network",
      "title_zh": "时间序列插值使用多变量径向基函数神经网络",
      "authors": [
        "Chanyoung Jung",
        "Yun Jang"
      ],
      "abstract": "Researchers have been persistently working to address the issue of missing\nvalues in time series data. Numerous models have been proposed, striving to\nestimate the distribution of the data. The Radial Basis Functions Neural\nNetwork (RBFNN) has recently exhibited exceptional performance in estimating\ndata distribution. In this paper, we propose a time series imputation model\nbased on RBFNN. Our imputation model learns local information from timestamps\nto create a continuous function. Additionally, we incorporate time gaps to\nfacilitate learning information considering the missing terms of missing\nvalues. We name this model the Missing Imputation Multivariate RBFNN\n(MIM-RBFNN). However, MIM-RBFNN relies on a local information-based learning\napproach, which presents difficulties in utilizing temporal information.\nTherefore, we propose an extension called the Missing Value Imputation\nRecurrent Neural Network with Continuous Function (MIRNN-CF) using the\ncontinuous function generated by MIM-RBFNN. We evaluate the performance using\ntwo real-world datasets with non-random missing and random missing patterns,\nand conduct an ablation study comparing MIM-RBFNN and MIRNN-CF.",
      "tldr_zh": "本论文针对时间序列数据中缺失值的问题，提出了一种基于 Radial Basis Functions Neural Network (RBFNN) 的插值模型。研究者开发了 Missing Imputation Multivariate RBFNN (MIM-RBFNN)，该模型通过学习时间戳的局部信息和时间间隙来生成连续函数，从而估计数据分布。此外，为解决 MIM-RBFNN 在利用时间信息方面的局限性，他们扩展为 Missing Value Imputation Recurrent Neural Network with Continuous Function (MIRNN-CF)，结合循环神经网络(Recurrent Neural Network)提升整体性能。实验在两个真实数据集上评估了模型在非随机和随机缺失模式下的表现，并通过消融研究比较了 MIM-RBFNN 和 MIRNN-CF 的效果，证明了方法的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17040v2",
      "published_date": "2024-07-24 07:02:16 UTC",
      "updated_date": "2024-07-31 05:39:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:42:39.066600"
    },
    {
      "arxiv_id": "2407.18982v1",
      "title": "Low-Latency Privacy-Preserving Deep Learning Design via Secure MPC",
      "title_zh": "翻译失败",
      "authors": [
        "Ke Lin",
        "Yasir Glani",
        "Ping Luo"
      ],
      "abstract": "Secure multi-party computation (MPC) facilitates privacy-preserving\ncomputation between multiple parties without leaking private information. While\nmost secure deep learning techniques utilize MPC operations to achieve feasible\nprivacy-preserving machine learning on downstream tasks, the overhead of the\ncomputation and communication still hampers their practical application. This\nwork proposes a low-latency secret-sharing-based MPC design that reduces\nunnecessary communication rounds during the execution of MPC protocols. We also\npresent a method for improving the computation of commonly used nonlinear\nfunctions in deep learning by integrating multivariate multiplication and\ncoalescing different packets into one to maximize network utilization. Our\nexperimental results indicate that our method is effective in a variety of\nsettings, with a speedup in communication latency of $10\\sim20\\%$.",
      "tldr_zh": "这篇论文针对Secure multi-party computation (MPC) 在隐私保护深度学习中的高通信和计算开销问题，提出了一种基于secret-sharing的低延迟MPC设计，以减少不必要的通信轮次。方法包括优化深度学习中常用非线性函数的计算，通过整合multivariate multiplication和合并数据包来最大化网络利用。实验结果表明，该方法在多种设置下实现了通信延迟的10~20%加速，从而提升了隐私保护机器学习的实际可行性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.DC",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "9 pages, accepted at IJCAI'24 AISafety",
      "pdf_url": "http://arxiv.org/pdf/2407.18982v1",
      "published_date": "2024-07-24 07:01:21 UTC",
      "updated_date": "2024-07-24 07:01:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:42:51.002896"
    },
    {
      "arxiv_id": "2407.17033v1",
      "title": "Sparse Inducing Points in Deep Gaussian Processes: Enhancing Modeling with Denoising Diffusion Variational Inference",
      "title_zh": "稀疏诱导点在深度高斯过程中的应用：利用去噪扩散变分推断增强建模",
      "authors": [
        "Jian Xu",
        "Delu Zeng",
        "John Paisley"
      ],
      "abstract": "Deep Gaussian processes (DGPs) provide a robust paradigm for Bayesian deep\nlearning. In DGPs, a set of sparse integration locations called inducing points\nare selected to approximate the posterior distribution of the model. This is\ndone to reduce computational complexity and improve model efficiency. However,\ninferring the posterior distribution of inducing points is not straightforward.\nTraditional variational inference approaches to posterior approximation often\nlead to significant bias. To address this issue, we propose an alternative\nmethod called Denoising Diffusion Variational Inference (DDVI) that uses a\ndenoising diffusion stochastic differential equation (SDE) to generate\nposterior samples of inducing variables. We rely on score matching methods for\ndenoising diffusion model to approximate score functions with a neural network.\nFurthermore, by combining classical mathematical theory of SDEs with the\nminimization of KL divergence between the approximate and true processes, we\npropose a novel explicit variational lower bound for the marginal likelihood\nfunction of DGP. Through experiments on various datasets and comparisons with\nbaseline methods, we empirically demonstrate the effectiveness of DDVI for\nposterior inference of inducing points for DGP models.",
      "tldr_zh": "本论文针对 Deep Gaussian Processes (DGPs) 中 inducing points 的后验推断问题，提出了一种新方法 Denoising Diffusion Variational Inference (DDVI)，以解决传统变分推理导致的偏差问题。DDVI 通过 denoising diffusion SDE 生成 inducing variables 的后验样本，并利用 score matching 方法和神经网络来近似 score 函数。同时，作者结合 SDEs 的数学理论和 KL divergence 最小化，开发了一个新的显式变分下界，用于优化 DGP 的边缘似然函数。在各种数据集上的实验表明，DDVI 比基线方法更有效，提升了模型的计算效率和准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17033v1",
      "published_date": "2024-07-24 06:39:58 UTC",
      "updated_date": "2024-07-24 06:39:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:43:04.944629"
    },
    {
      "arxiv_id": "2407.17535v2",
      "title": "LAMBDA: A Large Model Based Data Agent",
      "title_zh": "LAMBDA：基于大型模型的数据代理",
      "authors": [
        "Maojun Sun",
        "Ruijian Han",
        "Binyan Jiang",
        "Houduo Qi",
        "Defeng Sun",
        "Yancheng Yuan",
        "Jian Huang"
      ],
      "abstract": "We introduce LArge Model Based Data Agent (LAMBDA), a novel open-source,\ncode-free multi-agent data analysis system that leverages the power of large\nmodels. LAMBDA is designed to address data analysis challenges in complex\ndata-driven applications through innovatively designed data agents that operate\niteratively and generatively using natural language. At the core of LAMBDA are\ntwo key agent roles: the programmer and the inspector, which are engineered to\nwork together seamlessly. Specifically, the programmer generates code based on\nthe user's instructions and domain-specific knowledge, enhanced by advanced\nmodels. Meanwhile, the inspector debugs the code when necessary. To ensure\nrobustness and handle adverse scenarios, LAMBDA features a user interface that\nallows direct user intervention in the operational loop. Additionally, LAMBDA\ncan flexibly integrate external models and algorithms through our proposed\nKnowledge Integration Mechanism, catering to the needs of customized data\nanalysis. LAMBDA has demonstrated strong performance on various data analysis\ntasks. It has the potential to enhance data analysis paradigms by seamlessly\nintegrating human and artificial intelligence, making it more accessible,\neffective, and efficient for users from diverse backgrounds. The strong\nperformance of LAMBDA in solving data analysis problems is demonstrated using\nreal-world data examples. Videos of several case studies are available at\nhttps://xxxlambda.github.io/lambda_webpage.",
      "tldr_zh": "我们介绍了 LAMBDA，一种开源的免代码多智能体数据分析系统，利用大型模型通过 programmer 和 inspector 代理来处理复杂数据驱动任务，其中 programmer 生成基于用户指令的代码，而 inspector 负责调试。LAMBDA 支持自然语言迭代操作、用户直接干预以及 Knowledge Integration Mechanism，以灵活整合外部模型和算法。实验结果显示，该系统在各种数据分析任务中表现出色，提升了人类和人工智能的无缝整合，使数据分析更高效和易于访问。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SE",
        "62-04, 62-08, 68T01, 68T09"
      ],
      "primary_category": "cs.AI",
      "comment": "51 pages, 23 figures and 6 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.17535v2",
      "published_date": "2024-07-24 06:26:36 UTC",
      "updated_date": "2024-09-14 08:03:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:43:15.097894"
    },
    {
      "arxiv_id": "2407.17028v2",
      "title": "Enhancing Environmental Monitoring through Multispectral Imaging: The WasteMS Dataset for Semantic Segmentation of Lakeside Waste",
      "title_zh": "翻译失败",
      "authors": [
        "Qinfeng Zhu",
        "Ningxin Weng",
        "Lei Fan",
        "Yuanzhi Cai"
      ],
      "abstract": "Environmental monitoring of lakeside green areas is crucial for environmental\nprotection. Compared to manual inspections, computer vision technologies offer\na more efficient solution when deployed on-site. Multispectral imaging provides\ndiverse information about objects under different spectrums, aiding in the\ndifferentiation between waste and lakeside lawn environments. This study\nintroduces WasteMS, the first multispectral dataset established for the\nsemantic segmentation of lakeside waste. WasteMS includes a diverse range of\nwaste types in lawn environments, captured under various lighting conditions.\nWe implemented a rigorous annotation process to label waste in images.\nRepresentative semantic segmentation frameworks were used to evaluate\nsegmentation accuracy using WasteMS. Challenges encountered when using WasteMS\nfor segmenting waste on lakeside lawns were discussed. The WasteMS dataset is\navailable at https://github.com/zhuqinfeng1999/WasteMS.",
      "tldr_zh": "本研究通过多光谱成像技术提升湖边环境监测效率，引入了WasteMS数据集，这是首个针对湖边垃圾语义分割的多光谱数据集。WasteMS包含各种垃圾类型、在不同光照条件下的图像，并采用严格的标注过程来支持精确分析。研究评估了代表性语义分割框架在该数据集上的表现，讨论了分割湖边垃圾面临的挑战，并提供了数据集的开源链接（https://github.com/zhuqinfeng1999/WasteMS），为计算机视觉在环境保护中的应用提供了宝贵资源。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17028v2",
      "published_date": "2024-07-24 06:15:28 UTC",
      "updated_date": "2024-07-25 05:23:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:43:27.199216"
    },
    {
      "arxiv_id": "2407.17023v2",
      "title": "DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models",
      "title_zh": "DYNAMICQA：追踪语言模型中的内部知识冲突",
      "authors": [
        "Sara Vera Marjanović",
        "Haeun Yu",
        "Pepa Atanasova",
        "Maria Maistro",
        "Christina Lioma",
        "Isabelle Augenstein"
      ],
      "abstract": "Knowledge-intensive language understanding tasks require Language Models\n(LMs) to integrate relevant context, mitigating their inherent weaknesses, such\nas incomplete or outdated knowledge. However, conflicting knowledge can be\npresent in the LM's parameters, termed intra-memory conflict, which can affect\na model's propensity to accept contextual knowledge. To study the effect of\nintra-memory conflict on an LM's ability to accept relevant context, we utilize\ntwo knowledge conflict measures and a novel dataset containing inherently\nconflicting data, DynamicQA. This dataset includes facts with a temporal\ndynamic nature where facts can change over time and disputable dynamic facts,\nwhich can change depending on the viewpoint. DynamicQA is the first to include\nreal-world knowledge conflicts and provide context to study the link between\nthe different types of knowledge conflicts. We also evaluate several measures\non their ability to reflect the presence of intra-memory conflict: semantic\nentropy and a novel coherent persuasion score. With our extensive experiments,\nwe verify that LMs exhibit a greater degree of intra-memory conflict with\ndynamic facts compared to facts that have a single truth value. Furthermore, we\nreveal that facts with intra-memory conflict are harder to update with context,\nsuggesting that retrieval-augmented generation will struggle with the most\ncommonly adapted facts.",
      "tldr_zh": "这篇论文探讨了Language Models (LMs)中的内部知识冲突（intra-memory conflict），即模型参数中存在冲突知识，这会影响其整合上下文的能力。研究者引入了DynamicQA数据集，这是首个包含真实世界知识冲突（如时间动态事实和可争辩事实）的数据集，用于分析不同类型冲突之间的关联，并评估语义熵（semantic entropy）和连贯说服分数（coherent persuasion score）等测量方法。实验结果显示，LMs在处理动态事实时表现出更高的内部知识冲突，且这些冲突使事实更难通过上下文更新，从而揭示了retrieval-augmented generation在常见事实上可能面临的挑战。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 6 figures, Accepted to Findings of EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.17023v2",
      "published_date": "2024-07-24 06:06:07 UTC",
      "updated_date": "2024-10-07 11:59:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:43:41.355992"
    },
    {
      "arxiv_id": "2407.17007v1",
      "title": "Pensieve Discuss: Scalable Small-Group CS Tutoring System with AI",
      "title_zh": "翻译失败",
      "authors": [
        "Yoonseok Yang",
        "Jack Liu",
        "J. D. Zamfirescu-Pereira",
        "John DeNero"
      ],
      "abstract": "Small-group tutoring in Computer Science (CS) is effective, but presents the\nchallenge of providing a dedicated tutor for each group and encouraging\ncollaboration among group members at scale. We present Pensieve Discuss, a\nsoftware platform that integrates synchronous editing for scaffolded\nprogramming problems with online human and AI tutors, designed to improve\nstudent collaboration and experience during group tutoring sessions. Our\nsemester-long deployment to 800 students in a CS1 course demonstrated\nconsistently high collaboration rates, positive feedback about the AI tutor's\nhelpfulness and correctness, increased satisfaction with the group tutoring\nexperience, and a substantial increase in question volume. The use of our\nsystem was preferred over an interface lacking AI tutors and synchronous\nediting capabilities. Our experiences suggest that small-group tutoring\nsessions are an important avenue for future research in educational AI.",
      "tldr_zh": "该研究介绍了 Pensieve Discuss，一种可扩展的小组计算机科学（CS）辅导系统，结合同步编辑（synchronous editing）和在线人类及 AI 导师，旨在提升学生在支架式编程问题（scaffolded programming problems）中的协作和体验。在一个 CS1 课程中，该系统部署给 800 名学生后，实现了高协作率、积极反馈（包括 AI 导师的帮助性和正确性）、更高的辅导满意度和问题量的显著增加。与无 AI 导师和同步编辑功能的界面相比，Pensieve Discuss 更受欢迎，并为教育 AI 的未来研究提供了重要方向。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "6 pages, 7 figures, 4 tables, 1 page of references",
      "pdf_url": "http://arxiv.org/pdf/2407.17007v1",
      "published_date": "2024-07-24 05:07:53 UTC",
      "updated_date": "2024-07-24 05:07:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:43:52.488093"
    },
    {
      "arxiv_id": "2407.16999v1",
      "title": "SepsisLab: Early Sepsis Prediction with Uncertainty Quantification and Active Sensing",
      "title_zh": "翻译失败",
      "authors": [
        "Changchang Yin",
        "Pin-Yu Chen",
        "Bingsheng Yao",
        "Dakuo Wang",
        "Jeffrey Caterino",
        "Ping Zhang"
      ],
      "abstract": "Sepsis is the leading cause of in-hospital mortality in the USA. Early sepsis\nonset prediction and diagnosis could significantly improve the survival of\nsepsis patients. Existing predictive models are usually trained on high-quality\ndata with few missing information, while missing values widely exist in\nreal-world clinical scenarios (especially in the first hours of admissions to\nthe hospital), which causes a significant decrease in accuracy and an increase\nin uncertainty for the predictive models. The common method to handle missing\nvalues is imputation, which replaces the unavailable variables with estimates\nfrom the observed data. The uncertainty of imputation results can be propagated\nto the sepsis prediction outputs, which have not been studied in existing works\non either sepsis prediction or uncertainty quantification. In this study, we\nfirst define such propagated uncertainty as the variance of prediction output\nand then introduce uncertainty propagation methods to quantify the propagated\nuncertainty. Moreover, for the potential high-risk patients with low confidence\ndue to limited observations, we propose a robust active sensing algorithm to\nincrease confidence by actively recommending clinicians to observe the most\ninformative variables. We validate the proposed models in both publicly\navailable data (i.e., MIMIC-III and AmsterdamUMCdb) and proprietary data in The\nOhio State University Wexner Medical Center (OSUWMC). The experimental results\nshow that the propagated uncertainty is dominant at the beginning of admissions\nto hospitals and the proposed algorithm outperforms state-of-the-art active\nsensing methods. Finally, we implement a SepsisLab system for early sepsis\nprediction and active sensing based on our pre-trained models. Clinicians and\npotential sepsis patients can benefit from the system in early prediction and\ndiagnosis of sepsis.",
      "tldr_zh": "该研究针对败血症（sepsis）的早期预测问题，提出SepsisLab框架，该框架通过不确定性量化（uncertainty quantification）方法来处理数据缺失导致的预测不确定性传播，首次定义并量化这种传播不确定性（variance of prediction output）。为了提高高风险患者的预测置信度，研究引入了一个鲁棒的主动感知（active sensing）算法，推荐临床医生观察最有信息量的变量，以减少不确定性。实验在MIMIC-III、AmsterdamUMCdb和OSUWMC数据集上验证，结果显示该算法在入院初期显著优于现有方法，并通过SepsisLab系统实现早诊断，提高了患者存活率潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC",
        "68T07 (primary) 92C50 (secondary)",
        "H.2.8; I.2.1; J.3"
      ],
      "primary_category": "cs.LG",
      "comment": "To be published in KDD 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.16999v1",
      "published_date": "2024-07-24 04:47:36 UTC",
      "updated_date": "2024-07-24 04:47:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:44:04.783809"
    },
    {
      "arxiv_id": "2407.16994v2",
      "title": "A Voter-Based Stochastic Rejection-Method Framework for Asymptotically Safe Language Model Outputs",
      "title_zh": "翻译失败",
      "authors": [
        "Jake R. Watts",
        "Joel Sokol"
      ],
      "abstract": "This paper proposes a new method for preventing unsafe or otherwise low\nquality large language model (LLM) outputs, by leveraging the stochasticity of\nLLMs. We propose a system whereby LLM checkers vote on the acceptability of a\ngenerated output, regenerating it if a threshold of disapproval is reached,\nuntil sufficient checkers approve. We further propose estimators for cost and\nfailure rate, and based on those estimators and experimental data tailored to\nthe application, we propose an algorithm that achieves a desired failure rate\nat the least possible cost. We demonstrate that, under these models, failure\nrate decreases exponentially as a function of cost when voter count and\nthreshold are chosen according to the algorithm, and that the models reasonably\nestimate the actual performance of such a system in action, even with limited\ndata.",
      "tldr_zh": "本论文提出了一种基于投票的随机拒绝方法框架（Stochastic Rejection-Method Framework），旨在通过利用大型语言模型（LLMs）的随机性来防止不安全或低质量输出。框架中，LLM检查器对生成输出进行投票，如果 disapproval 超过设定阈值，则重新生成输出，直至足够检查器批准。论文进一步引入成本和失败率的估计器，并基于这些估计器设计算法，以最低成本实现预期的失败率。实验结果表明，失败率随成本呈指数下降，且模型能合理估计实际系统的性能，即使数据有限。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.16994v2",
      "published_date": "2024-07-24 04:27:55 UTC",
      "updated_date": "2024-09-03 19:28:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:44:15.695734"
    },
    {
      "arxiv_id": "2407.17533v1",
      "title": "SFPrompt: Communication-Efficient Split Federated Fine-Tuning for Large Pre-Trained Models over Resource-Limited Devices",
      "title_zh": "SFPrompt：通信高效的分割联邦微调，用于资源受限设备上的大型预训练模型",
      "authors": [
        "Linxiao Cao",
        "Yifei Zhu",
        "Wei Gong"
      ],
      "abstract": "Large pre-trained models have exhibited remarkable achievements across\nvarious domains. The substantial training costs associated with these models\nhave led to wide studies of fine-tuning for effectively harnessing their\ncapabilities in solving downstream tasks. Yet, conventional fine-tuning\napproaches become infeasible when the model lacks access to downstream data due\nto privacy concerns. Naively integrating fine-tuning approaches with the\nemerging federated learning frameworks incurs substantial communication\noverhead and exerts high demand on local computing resources, making it\nimpractical for common resource-limited devices. In this paper, we introduce\nSFPrompt, an innovative privacy-preserving fine-tuning method tailored for the\nfederated setting where direct uploading of raw data is prohibited and local\ndevices are resource-constrained to run a complete pre-trained model. In\nessence, SFPrompt judiciously combines split learning with federated learning\nto handle these challenges. Specifically, the pre-trained model is first\npartitioned into client and server components, thereby streamlining the\nclient-side model and substantially alleviating computational demands on local\nresources. SFPrompt then introduces soft prompts into the federated model to\nenhance the fine-tuning performance. To further reduce communication costs, a\nnovel dataset pruning algorithm and a local-loss update strategy are devised\nduring the fine-tuning process. Extensive experiments demonstrate that SFPrompt\ndelivers competitive performance as the federated full fine-tuning approach\nwhile consuming a mere 0.46% of local computing resources and incurring 53%\nless communication cost.",
      "tldr_zh": "论文提出 SFPrompt，一种针对资源有限设备的通信高效分割联邦微调方法，用于大型预训练模型的隐私保护微调。SFPrompt 通过结合 split learning 和 federated learning，将模型分割为客户端和服务器组件，引入 soft prompts 提升性能，并采用数据集修剪算法和本地损失更新策略来减少通信开销。实验结果表明，该方法与联邦全微调方法性能相当，但仅需 0.46% 的本地计算资源，并降低 53% 的通信成本。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.17533v1",
      "published_date": "2024-07-24 04:22:37 UTC",
      "updated_date": "2024-07-24 04:22:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:44:27.828414"
    },
    {
      "arxiv_id": "2407.16982v1",
      "title": "Diffree: Text-Guided Shape Free Object Inpainting with Diffusion Model",
      "title_zh": "翻译失败",
      "authors": [
        "Lirui Zhao",
        "Tianshuo Yang",
        "Wenqi Shao",
        "Yuxin Zhang",
        "Yu Qiao",
        "Ping Luo",
        "Kaipeng Zhang",
        "Rongrong Ji"
      ],
      "abstract": "This paper addresses an important problem of object addition for images with\nonly text guidance. It is challenging because the new object must be integrated\nseamlessly into the image with consistent visual context, such as lighting,\ntexture, and spatial location. While existing text-guided image inpainting\nmethods can add objects, they either fail to preserve the background\nconsistency or involve cumbersome human intervention in specifying bounding\nboxes or user-scribbled masks. To tackle this challenge, we introduce Diffree,\na Text-to-Image (T2I) model that facilitates text-guided object addition with\nonly text control. To this end, we curate OABench, an exquisite synthetic\ndataset by removing objects with advanced image inpainting techniques. OABench\ncomprises 74K real-world tuples of an original image, an inpainted image with\nthe object removed, an object mask, and object descriptions. Trained on OABench\nusing the Stable Diffusion model with an additional mask prediction module,\nDiffree uniquely predicts the position of the new object and achieves object\naddition with guidance from only text. Extensive experiments demonstrate that\nDiffree excels in adding new objects with a high success rate while maintaining\nbackground consistency, spatial appropriateness, and object relevance and\nquality.",
      "tldr_zh": "本文提出 Diffree，一种基于 Diffusion Model 的文本指导对象添加模型（Text-to-Image, T2I），能够无需指定边界框或掩码，仅通过文本控制实现形状自由的对象插入，同时确保图像的背景一致性（如光线、纹理和空间位置）。为了训练模型，作者构建了 OABench 数据集，包含 74K 个真实世界图像元组，通过高级图像修复技术合成原始图像和去对象图像。实验结果显示，Diffree 在添加新对象时表现出色，显著提高了成功率、空间适当性和对象相关性质量。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16982v1",
      "published_date": "2024-07-24 03:58:58 UTC",
      "updated_date": "2024-07-24 03:58:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:44:41.075912"
    },
    {
      "arxiv_id": "2407.16981v1",
      "title": "Case-Enhanced Vision Transformer: Improving Explanations of Image Similarity with a ViT-based Similarity Metric",
      "title_zh": "翻译失败",
      "authors": [
        "Ziwei Zhao",
        "David Leake",
        "Xiaomeng Ye",
        "David Crandall"
      ],
      "abstract": "This short paper presents preliminary research on the Case-Enhanced Vision\nTransformer (CEViT), a similarity measurement method aimed at improving the\nexplainability of similarity assessments for image data. Initial experimental\nresults suggest that integrating CEViT into k-Nearest Neighbor (k-NN)\nclassification yields classification accuracy comparable to state-of-the-art\ncomputer vision models, while adding capabilities for illustrating differences\nbetween classes. CEViT explanations can be influenced by prior cases, to\nillustrate aspects of similarity relevant to those cases.",
      "tldr_zh": "该论文提出了一种名为 Case-Enhanced Vision Transformer (CEViT) 的方法，这是一种基于 Vision Transformer (ViT) 的图像相似度测量技术，旨在提升相似性评估的可解释性。CEViT 通过整合到 k-Nearest Neighbor (k-NN) 分类中，能够实现与最先进计算机视觉模型相当的分类准确率，同时提供类别间差异的视觉说明。实验结果表明，CEViT 的解释可以根据先前案例进行调整，以突出相关相似性特征，从而增强图像分析的透明度。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16981v1",
      "published_date": "2024-07-24 03:58:07 UTC",
      "updated_date": "2024-07-24 03:58:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:44:52.853936"
    },
    {
      "arxiv_id": "2407.16970v3",
      "title": "Towards Aligning Language Models with Textual Feedback",
      "title_zh": "翻译失败",
      "authors": [
        "Saüc Abadal Lloret",
        "Shehzaad Dhuliawala",
        "Keerthiram Murugesan",
        "Mrinmaya Sachan"
      ],
      "abstract": "We present ALT (ALignment with Textual feedback), an approach that aligns\nlanguage models with user preferences expressed in text. We argue that text\noffers greater expressiveness, enabling users to provide richer feedback than\nsimple comparative preferences and this richer feedback can lead to more\nefficient and effective alignment. ALT aligns the model by conditioning its\ngeneration on the textual feedback. Our method relies solely on language\nmodeling techniques and requires minimal hyper-parameter tuning, though it\nstill presents the main benefits of RL-based alignment algorithms and can\neffectively learn from textual feedback. We explore the efficacy and efficiency\nof textual feedback across different tasks such as toxicity reduction,\nsummarization, and dialog response generation. We find that ALT outperforms PPO\nfor the task of toxicity reduction while being able to match its performance on\nsummarization with only 20% of the samples. We also explore how ALT can be used\nwith feedback provided by an existing LLM where we explore an LLM providing\nconstrained and unconstrained textual feedback. We also outline future\ndirections to align models with natural language feedback.",
      "tldr_zh": "本论文提出 ALT（Alignment with Textual feedback）方法，用于通过文本反馈对齐语言模型，相比简单比较偏好，文本反馈提供更丰富的用户偏好表达，从而实现更高效的对齐。ALT 通过在生成过程中条件化文本反馈，仅依赖语言建模技术，并减少超参数调整，同时保留 RL-based alignment 的主要优势。在实验中，ALT 在毒性减少任务上优于 PPO，在摘要生成和对话响应生成任务中，仅用 20% 的样本就匹配了 PPO 的性能。该方法还探索了使用现有 LLM 提供约束或非约束反馈，并为未来基于自然语言反馈的模型对齐指出了潜在方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.16970v3",
      "published_date": "2024-07-24 03:32:05 UTC",
      "updated_date": "2025-03-18 16:34:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:45:07.001799"
    },
    {
      "arxiv_id": "2407.16968v1",
      "title": "Stochastic Variance-Reduced Iterative Hard Thresholding in Graph Sparsity Optimization",
      "title_zh": "翻译失败",
      "authors": [
        "Derek Fox",
        "Samuel Hernandez",
        "Qianqian Tong"
      ],
      "abstract": "Stochastic optimization algorithms are widely used for large-scale data\nanalysis due to their low per-iteration costs, but they often suffer from slow\nasymptotic convergence caused by inherent variance. Variance-reduced techniques\nhave been therefore used to address this issue in structured sparse models\nutilizing sparsity-inducing norms or $\\ell_0$-norms. However, these techniques\nare not directly applicable to complex (non-convex) graph sparsity models,\nwhich are essential in applications like disease outbreak monitoring and social\nnetwork analysis. In this paper, we introduce two stochastic variance-reduced\ngradient-based methods to solve graph sparsity optimization: GraphSVRG-IHT and\nGraphSCSG-IHT. We provide a general framework for theoretical analysis,\ndemonstrating that our methods enjoy a linear convergence speed. Extensive\nexperiments validate",
      "tldr_zh": "该论文针对随机优化算法在图稀疏优化中的慢速收敛问题，引入了两种基于方差减少的梯度方法：GraphSVRG-IHT 和 GraphSCSG-IHT，以处理非凸图稀疏模型（如疾病爆发监测和社会网络分析）。这些方法通过迭代硬阈值化（Iterative Hard Thresholding）框架实现了线性收敛速度，并提供了全面的理论分析。实验结果证实了新方法的有效性，在大规模数据分析中表现出色。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16968v1",
      "published_date": "2024-07-24 03:26:26 UTC",
      "updated_date": "2024-07-24 03:26:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:45:17.606630"
    },
    {
      "arxiv_id": "2407.16962v1",
      "title": "Toward an Integrated Decision Making Framework for Optimized Stroke Diagnosis with DSA and Treatment under Uncertainty",
      "title_zh": "翻译失败",
      "authors": [
        "Nur Ahmad Khatim",
        "Ahmad Azmul Asmar Irfan",
        "Amaliya Mata'ul Hayah",
        "Mansur M. Arief"
      ],
      "abstract": "This study addresses the challenge of stroke diagnosis and treatment under\nuncertainty, a critical issue given the rapid progression and severe\nconsequences of stroke conditions such as aneurysms, arteriovenous\nmalformations (AVM), and occlusions. Current diagnostic methods, including\nDigital Subtraction Angiography (DSA), face limitations due to high costs and\nits invasive nature. To overcome these challenges, we propose a novel approach\nusing a Partially Observable Markov Decision Process (POMDP) framework. Our\nmodel integrates advanced diagnostic tools and treatment approaches with a\ndecision-making algorithm that accounts for the inherent uncertainties in\nstroke diagnosis. Our approach combines noisy observations from CT scans,\nSiriraj scores, and DSA reports to inform the subsequent treatment options. We\nutilize the online solver DESPOT, which employs tree-search methods and\nparticle filters, to simulate potential future scenarios and guide our\nstrategies. The results indicate that our POMDP framework balances diagnostic\nand treatment objectives, striking a tradeoff between the need for precise\nstroke identification via invasive procedures like DSA and the constraints of\nlimited healthcare resources that necessitate more cost-effective strategies,\nsuch as in-hospital or at-home observation, by relying only relying on\nsimulation rollouts and not imposing any prior knowledge. Our study offers a\nsignificant contribution by presenting a systematic framework that optimally\nintegrates diagnostic and treatment processes for stroke and accounting for\nvarious uncertainties, thereby improving care and outcomes in stroke\nmanagement.",
      "tldr_zh": "本研究针对中风诊断和治疗的不确定性（如动脉瘤、AVM 和闭塞），提出一个整合决策框架，以优化使用 DSA 等工具的过程。框架基于 Partially Observable Markov Decision Process (POMDP) 模型，结合 CT scans、Siriraj scores 和 DSA 报告的噪声观察，并利用在线求解器 DESPOT 通过树搜索和粒子过滤器模拟未来场景。结果显示，该框架在精确诊断和资源限制之间实现权衡，提高了中风管理的效率和结果，为系统化处理不确定性的医疗决策提供了重要贡献。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16962v1",
      "published_date": "2024-07-24 03:01:55 UTC",
      "updated_date": "2024-07-24 03:01:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:45:29.959767"
    },
    {
      "arxiv_id": "2407.16958v6",
      "title": "Wonderful Matrices: More Efficient and Effective Architecture for Language Modeling Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Jingze Shi",
        "Bingheng Wu",
        "Lu He",
        "Luchang Jiang"
      ],
      "abstract": "We prove the availability of inner product form position encoding in the\nstate space dual algorithm and study the effectiveness of different position\nembeddings in the hybrid quadratic causal self-attention and state space dual\nalgorithms. We propose inner function attention with dynamic mask, which can\nimprove the expressiveness of the attention algorithm and avoid the sequence\nnoise significantly affecting the accuracy of the attention score. We also\ndesign cross domain mixture of experts, which can improve the granularity of\nthe sparse activation feedforward network while maintaining the efficiency of\nparameter utilization and retrieval. The combination of these methods\nconstitutes our foundation model architecture: Wonderful Matrices. We conduct\nexperiments on the language modeling task and find that Wonderful Matrices are\nmore efficient and effective in handling complex language tasks.",
      "tldr_zh": "本论文证明了内积形式位置编码（inner product form position encoding）在状态空间双算法（state space dual algorithm）中的可用性，并探讨了不同位置嵌入在混合二次因果自注意力机制中的有效性。研究者提出了内函数注意力（inner function attention）结合动态掩码（dynamic mask），以提升注意力的表现力并减少序列噪声对注意分数的干扰；同时设计了跨域混合专家（cross domain mixture of experts），提高稀疏激活前馈网络的粒度，同时保持参数利用效率。这些方法整合形成了 Wonderful Matrices 架构，在语言建模任务上实验显示，该架构比传统方法更高效和有效。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "28 pages, 8 figures, 7 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.16958v6",
      "published_date": "2024-07-24 02:52:02 UTC",
      "updated_date": "2024-11-12 01:31:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:45:44.668860"
    },
    {
      "arxiv_id": "2407.16939v1",
      "title": "Early screening of potential breakthrough technologies with enhanced interpretability: A patent-specific hierarchical attention network model",
      "title_zh": "增强可解释性的潜在突破性技术的早期筛选：一个专利特定的分层注意力网络模型",
      "authors": [
        "Jaewoong Choi",
        "Janghyeok Yoon",
        "Changyong Lee"
      ],
      "abstract": "Despite the usefulness of machine learning approaches for the early screening\nof potential breakthrough technologies, their practicality is often hindered by\nopaque models. To address this, we propose an interpretable machine learning\napproach to predicting future citation counts from patent texts using a\npatent-specific hierarchical attention network (PatentHAN) model. Central to\nthis approach are (1) a patent-specific pre-trained language model, capturing\nthe meanings of technical words in patent claims, (2) a hierarchical network\nstructure, enabling detailed analysis at the claim level, and (3) a claim-wise\nself-attention mechanism, revealing pivotal claims during the screening\nprocess. A case study of 35,376 pharmaceutical patents demonstrates the\neffectiveness of our approach in early screening of potential breakthrough\ntechnologies while ensuring interpretability. Furthermore, we conduct\nadditional analyses using different language models and claim types to examine\nthe robustness of the approach. It is expected that the proposed approach will\nenhance expert-machine collaboration in identifying breakthrough technologies,\nproviding new insight derived from text mining into technological value.",
      "tldr_zh": "该论文提出了一种可解释的机器学习方法，使用专利特定的分层注意力网络（PatentHAN）模型，从专利文本预测未来引用次数，以实现潜在突破技术的早期筛选。该方法的核心组件包括：专利特定的预训练语言模型（capturing technical words in patent claims）、分层网络结构（enabling detailed analysis at the claim level）和声明级自注意力机制（revealing pivotal claims），从而提升模型的透明度和实用性。在对35,376个制药专利的案例研究中，该方法比传统模型更有效，并通过额外分析验证了其鲁棒性，最终有望加强专家-机器协作，并从文本挖掘中获得新技术价值洞见。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.16939v1",
      "published_date": "2024-07-24 02:17:10 UTC",
      "updated_date": "2024-07-24 02:17:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:45:56.220877"
    },
    {
      "arxiv_id": "2407.16938v1",
      "title": "Synthetic Trajectory Generation Through Convolutional Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Jesse Merhi",
        "Erik Buchholz",
        "Salil S. Kanhere"
      ],
      "abstract": "Location trajectories provide valuable insights for applications from urban\nplanning to pandemic control. However, mobility data can also reveal sensitive\ninformation about individuals, such as political opinions, religious beliefs,\nor sexual orientations. Existing privacy-preserving approaches for publishing\nthis data face a significant utility-privacy trade-off. Releasing synthetic\ntrajectory data generated through deep learning offers a promising solution.\nDue to the trajectories' sequential nature, most existing models are based on\nrecurrent neural networks (RNNs). However, research in generative adversarial\nnetworks (GANs) largely employs convolutional neural networks (CNNs) for image\ngeneration. This discrepancy raises the question of whether advances in\ncomputer vision can be applied to trajectory generation. In this work, we\nintroduce a Reversible Trajectory-to-CNN Transformation (RTCT) that adapts\ntrajectories into a format suitable for CNN-based models. We integrated this\ntransformation with the well-known DCGAN in a proof-of-concept (PoC) and\nevaluated its performance against an RNN-based trajectory GAN using four\nmetrics across two datasets. The PoC was superior in capturing spatial\ndistributions compared to the RNN model but had difficulty replicating\nsequential and temporal properties. Although the PoC's utility is not\nsufficient for practical applications, the results demonstrate the\ntransformation's potential to facilitate the use of CNNs for trajectory\ngeneration, opening up avenues for future research. To support continued\nresearch, all source code has been made available under an open-source license.",
      "tldr_zh": "这篇论文探讨了使用卷积神经网络(CNNs)生成合成轨迹数据，以平衡轨迹数据的效用和隐私保护问题。作者提出了可逆轨迹到CNN转换(Reversible Trajectory-to-CNN Transformation, RTCT)方法，将轨迹数据转化为适合CNN处理的格式，并将其整合到DCGAN中进行概念验证(PoC)。实验结果显示，RTCT在捕捉空间分布方面优于RNN-based GAN模型，但难以复制轨迹的序列和时间属性；尽管实用效用有限，该方法展示了CNNs在轨迹生成领域的潜力，并开源了代码以支持进一步研究。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "To appear in the proceedings of the 21st Annual International\n  Conference on Privacy, Security & Trust (PST 2024)",
      "pdf_url": "http://arxiv.org/pdf/2407.16938v1",
      "published_date": "2024-07-24 02:16:52 UTC",
      "updated_date": "2024-07-24 02:16:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:46:08.945674"
    },
    {
      "arxiv_id": "2407.16929v2",
      "title": "Synthetic Data, Similarity-based Privacy Metrics, and Regulatory (Non-)Compliance",
      "title_zh": "翻译失败",
      "authors": [
        "Georgi Ganev"
      ],
      "abstract": "In this paper, we argue that similarity-based privacy metrics cannot ensure\nregulatory compliance of synthetic data. Our analysis and counter-examples show\nthat they do not protect against singling out and linkability and, among other\nfundamental issues, completely ignore the motivated intruder test.",
      "tldr_zh": "本论文论证了基于相似性的隐私指标（Similarity-based Privacy Metrics）无法确保合成数据（Synthetic Data）的监管合规性（Regulatory Compliance）。作者通过分析和反例证明，这些指标无法有效防范 singling out（识别特定个体）和 linkability（可链接性）攻击。论文还指出了其他根本问题，如完全忽略了 motivated intruder test（有动机入侵者测试），从而强调了现有方法在隐私保护方面的局限性。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted to the 2nd Workshop on Generative AI and Law (GenLaw 2024),\n  part of ICML 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.16929v2",
      "published_date": "2024-07-24 01:45:41 UTC",
      "updated_date": "2024-07-26 03:30:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T09:46:18.385856"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 78,
  "processed_papers_count": 78,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T09:46:54.366355"
}
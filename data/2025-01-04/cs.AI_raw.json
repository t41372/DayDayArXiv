[
  {
    "arxiv_id": "2501.02406v4",
    "title": "Zero-Shot Statistical Tests for LLM-Generated Text Detection using Finite Sample Concentration Inequalities",
    "authors": [
      "Tara Radvand",
      "Mojtaba Abdolmaleki",
      "Mohamed Mostagir",
      "Ambuj Tewari"
    ],
    "abstract": "Verifying the provenance of content is crucial to the function of many\norganizations, e.g., educational institutions, social media platforms, firms,\netc. This problem is becoming increasingly challenging as text generated by\nLarge Language Models (LLMs) becomes almost indistinguishable from\nhuman-generated content. In addition, many institutions utilize in-house LLMs\nand want to ensure that external, non-sanctioned LLMs do not produce content\nwithin the institution. In this paper, we answer the following question: Given\na piece of text, can we identify whether it was produced by a particular LLM or\nnot? We model LLM-generated text as a sequential stochastic process with\ncomplete dependence on history. We then design zero-shot statistical tests to\n(i) distinguish between text generated by two different known sets of LLMs $A$\n(non-sanctioned) and $B$ (in-house), and (ii) identify whether text was\ngenerated by a known LLM or generated by any unknown model, e.g., a human or\nsome other language generation process. We prove that the type I and type II\nerrors of our test decrease exponentially with the length of the text. For\nthat, we show that if $B$ generates the text, then except with an exponentially\nsmall probability in string length, the log-perplexity of the string under $A$\nconverges to the average cross-entropy of $B$ and $A$. We then present\nexperiments using LLMs with white-box access to support our theoretical results\nand empirically examine the robustness of our results to black-box settings and\nadversarial attacks. In the black-box setting, our method achieves an average\nTPR of 82.5\\% at a fixed FPR of 5\\%. Under adversarial perturbations, our\nminimum TPR is 48.6\\% at the same FPR threshold. Both results outperform all\nnon-commercial baselines. See\nhttps://github.com/TaraRadvand74/llm-text-detection for code, data, and an\nonline demo of the project.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CL",
      "cs.IT",
      "cs.LG",
      "math.IT"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02406v4",
    "published_date": "2025-01-04 23:51:43 UTC",
    "updated_date": "2025-05-16 15:45:11 UTC"
  },
  {
    "arxiv_id": "2501.02401v1",
    "title": "iTARGET: Interpretable Tailored Age Regression for Grouped Epigenetic Traits",
    "authors": [
      "Zipeng Wu",
      "Daniel Herring",
      "Fabian Spill",
      "James Andrews"
    ],
    "abstract": "Accurately predicting chronological age from DNA methylation patterns is\ncrucial for advancing biological age estimation. However, this task is made\nchallenging by Epigenetic Correlation Drift (ECD) and Heterogeneity Among CpGs\n(HAC), which reflect the dynamic relationship between methylation and age\nacross different life stages. To address these issues, we propose a novel\ntwo-phase algorithm. The first phase employs similarity searching to cluster\nmethylation profiles by age group, while the second phase uses Explainable\nBoosting Machines (EBM) for precise, group-specific prediction. Our method not\nonly improves prediction accuracy but also reveals key age-related CpG sites,\ndetects age-specific changes in aging rates, and identifies pairwise\ninteractions between CpG sites. Experimental results show that our approach\noutperforms traditional epigenetic clocks and machine learning models, offering\na more accurate and interpretable solution for biological age estimation with\nsignificant implications for aging research.",
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "62P10, 92D20, 92D10",
      "I.5.4; J.3; I.2.6"
    ],
    "primary_category": "q-bio.GN",
    "comment": "To be published in IEEE BIBM 2024.The manuscript includes a\n  comprehensive description of the methodology and comparison with traditional\n  epigenetic clocks and machine learning models. Submitted to arXiv as part of\n  ongoing research in epigenetics and aging studies",
    "pdf_url": "http://arxiv.org/pdf/2501.02401v1",
    "published_date": "2025-01-04 23:06:46 UTC",
    "updated_date": "2025-01-04 23:06:46 UTC"
  },
  {
    "arxiv_id": "2501.02393v3",
    "title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers",
    "authors": [
      "Markus J. Buehler"
    ],
    "abstract": "We present an approach to modifying Transformer architectures by integrating\ngraph-aware relational reasoning into the attention mechanism, merging concepts\nfrom graph neural networks and language modeling. Building on the inherent\nconnection between attention and graph theory, we reformulate the Transformer's\nattention mechanism as a graph operation and propose Graph-Aware Isomorphic\nAttention. This method leverages advanced graph modeling strategies, including\nGraph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA),\nto enrich the representation of relational structures. Our approach captures\ncomplex dependencies and generalizes across tasks, as evidenced by a reduced\ngeneralization gap and improved learning performance. Additionally, we expand\nthe concept of graph-aware attention to introduce Sparse GIN-Attention, a\nfine-tuning approach that employs sparse GINs. By interpreting attention\nmatrices as sparse adjacency graphs, this technique enhances the adaptability\nof pre-trained foundational models with minimal computational overhead,\nendowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning\nachieves improved training dynamics and better generalization compared to\nalternative methods like low-rank adaption (LoRA). We discuss latent graph-like\nstructures within traditional attention mechanisms, offering a new lens through\nwhich Transformers can be understood. By evolving Transformers as hierarchical\nGIN models for relational reasoning. This perspective suggests profound\nimplications for foundational model development, enabling the design of\narchitectures that dynamically adapt to both local and global dependencies.\nApplications in bioinformatics, materials science, language modeling, and\nbeyond could benefit from this synthesis of relational and sequential data\nmodeling, setting the stage for interpretable and generalizable modeling\nstrategies.",
    "categories": [
      "cs.LG",
      "cond-mat.mes-hall",
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02393v3",
    "published_date": "2025-01-04 22:30:21 UTC",
    "updated_date": "2025-03-05 13:19:16 UTC"
  },
  {
    "arxiv_id": "2501.02392v1",
    "title": "Syntactic Evolution in Language Usage",
    "authors": [
      "Surbhit Kumar"
    ],
    "abstract": "This research aims to investigate the dynamic nature of linguistic style\nthroughout various stages of life, from post teenage to old age. By employing\nlinguistic analysis tools and methodologies, the study will delve into the\nintricacies of how individuals adapt and modify their language use over time.\nThe research uses a data set of blogs from blogger.com from 2004 and focuses on\nEnglish for syntactic analysis. The findings of this research can have\nimplications for linguistics, psychology, and communication studies, shedding\nlight on the intricate relationship between age and language.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "4 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.02392v1",
    "published_date": "2025-01-04 22:27:24 UTC",
    "updated_date": "2025-01-04 22:27:24 UTC"
  },
  {
    "arxiv_id": "2501.02368v1",
    "title": "Enhancing Workplace Productivity and Well-being Using AI Agent",
    "authors": [
      "Ravirajan K",
      "Arvind Sundarajan"
    ],
    "abstract": "This paper discusses the use of Artificial Intelligence (AI) to enhance\nworkplace productivity and employee well-being. By integrating machine learning\n(ML) techniques with neurobiological data, the proposed approaches ensure\nalignment with human ethical standards through value alignment models and\nHierarchical Reinforcement Learning (HRL) for autonomous task management. The\nsystem utilizes biometric feedback from employees to generate personalized\nhealth prompts, fostering a supportive work environment that encourages\nphysical activity. Additionally, we explore decentralized multi-agent systems\nfor improved collaboration and decision-making frameworks that enhance\ntransparency. Various approaches using ML techniques in conjunction with AI\nimplementations are discussed. Together, these innovations aim to create a more\nproductive and health-conscious workplace. These outcomes assist HR management\nand organizations in launching more rational career progression streams for\nemployees and facilitating organizational transformation.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02368v1",
    "published_date": "2025-01-04 20:11:00 UTC",
    "updated_date": "2025-01-04 20:11:00 UTC"
  },
  {
    "arxiv_id": "2501.02361v1",
    "title": "Context Aware Lemmatization and Morphological Tagging Method in Turkish",
    "authors": [
      "Cagri Sayallar"
    ],
    "abstract": "The smallest part of a word that defines the word is called a word root. Word\nroots are used to increase success in many applications since they simplify the\nword. In this study, the lemmatization model, which is a word root finding\nmethod, and the morphological tagging model, which predicts the grammatical\nknowledge of the word, are presented. The presented model was developed for\nTurkish, and both models make predictions by taking the meaning of the word\ninto account. In the literature, there is no lemmatization study that is\nsensitive to word meaning in Turkish. For this reason, the present study shares\nthe model and the results obtained from the model on Turkish lemmatization for\nthe first time in the literature. In the present study, in the lemmatization\nand morphological tagging models, bidirectional LSTM is used for the spelling\nof words, and the Turkish BERT model is used for the meaning of words. The\nmodels are trained using the IMST and PUD datasets from Universal Dependencies.\nThe results from the training of the models were compared with the results from\nthe SIGMORPHON 2019 competition. The results of the comparisons revealed that\nour models were superior.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T07, 68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02361v1",
    "published_date": "2025-01-04 19:12:43 UTC",
    "updated_date": "2025-01-04 19:12:43 UTC"
  },
  {
    "arxiv_id": "2501.02352v1",
    "title": "GNSS/GPS Spoofing and Jamming Identification Using Machine Learning and Deep Learning",
    "authors": [
      "Ali Ghanbarzade",
      "Hossein Soleimani"
    ],
    "abstract": "The increasing reliance on Global Navigation Satellite Systems (GNSS),\nparticularly the Global Positioning System (GPS), underscores the urgent need\nto safeguard these technologies against malicious threats such as spoofing and\njamming. As the backbone for positioning, navigation, and timing (PNT) across\nvarious applications including transportation, telecommunications, and\nemergency services GNSS is vulnerable to deliberate interference that poses\nsignificant risks. Spoofing attacks, which involve transmitting counterfeit\nGNSS signals to mislead receivers into calculating incorrect positions, can\nresult in serious consequences, from navigational errors in civilian aviation\nto security breaches in military operations. Furthermore, the lack of inherent\nsecurity measures within GNSS systems makes them attractive targets for\nadversaries. While GNSS/GPS jamming and spoofing systems consist of numerous\ncomponents, the ability to distinguish authentic signals from malicious ones is\nessential for maintaining system integrity. Recent advancements in machine\nlearning and deep learning provide promising avenues for enhancing detection\nand mitigation strategies against these threats. This paper addresses both\nspoofing and jamming by tackling real-world challenges through machine\nlearning, deep learning, and computer vision techniques. Through extensive\nexperiments on two real-world datasets related to spoofing and jamming\ndetection using advanced algorithms, we achieved state of the art results. In\nthe GNSS/GPS jamming detection task, we attained approximately 99% accuracy,\nimproving performance by around 5% compared to previous studies. Additionally,\nwe addressed a challenging tasks related to spoofing detection, yielding\nresults that underscore the potential of machine learning and deep learning in\nthis domain.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02352v1",
    "published_date": "2025-01-04 18:14:43 UTC",
    "updated_date": "2025-01-04 18:14:43 UTC"
  },
  {
    "arxiv_id": "2501.02346v1",
    "title": "Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support",
    "authors": [
      "Florian Putz",
      "Marlen Haderleina",
      "Sebastian Lettmaier",
      "Sabine Semrau",
      "Rainer Fietkau",
      "Yixing Huang"
    ],
    "abstract": "Thanks to the rapidly evolving integration of LLMs into decision-support\ntools, a significant transformation is happening across large-scale systems.\nLike other medical fields, the use of LLMs such as GPT-4 is gaining increasing\ninterest in radiation oncology as well. An attempt to assess GPT-4's\nperformance in radiation oncology was made via a dedicated 100-question\nexamination on the highly specialized topic of radiation oncology physics,\nrevealing GPT-4's superiority over other LLMs. GPT-4's performance on a broader\nfield of clinical radiation oncology is further benchmarked by the ACR\nRadiation Oncology In-Training (TXIT) exam where GPT-4 achieved a high accuracy\nof 74.57%. Its performance on re-labelling structure names in accordance with\nthe AAPM TG-263 report has also been benchmarked, achieving above 96%\naccuracies. Such studies shed light on the potential of LLMs in radiation\noncology. As interest in the potential and constraints of LLMs in general\nhealthcare applications continues to rise5, the capabilities and limitations of\nLLMs in radiation oncology decision support have not yet been fully explored.",
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "primary_category": "physics.med-ph",
    "comment": "Officially published in the Red Journal",
    "pdf_url": "http://arxiv.org/pdf/2501.02346v1",
    "published_date": "2025-01-04 17:57:33 UTC",
    "updated_date": "2025-01-04 17:57:33 UTC"
  },
  {
    "arxiv_id": "2501.02342v1",
    "title": "Optimizing Small Language Models for In-Vehicle Function-Calling",
    "authors": [
      "Yahya Sowti Khiabani",
      "Farris Atif",
      "Chieh Hsu",
      "Sven Stahlmann",
      "Tobias Michels",
      "Sebastian Kramer",
      "Benedikt Heidrich",
      "M. Saquib Sarfraz",
      "Julian Merten",
      "Faezeh Tafazzoli"
    ],
    "abstract": "We propose a holistic approach for deploying Small Language Models (SLMs) as\nfunction-calling agents within vehicles as edge devices, offering a more\nflexible and robust alternative to traditional rule-based systems. By\nleveraging SLMs, we simplify vehicle control mechanisms and enhance the user\nexperience. Given the in-vehicle hardware constraints, we apply\nstate-of-the-art model compression techniques, including structured pruning,\nhealing, and quantization, ensuring that the model fits within the resource\nlimitations while maintaining acceptable performance. Our work focuses on\noptimizing a representative SLM, Microsoft's Phi-3 mini, and outlines best\npractices for enabling embedded models, including compression, task-specific\nfine-tuning, and vehicle integration. We demonstrate that, despite significant\nreduction in model size which removes up to 2 billion parameters from the\noriginal model, our approach preserves the model's ability to handle complex\nin-vehicle tasks accurately and efficiently. Furthermore, by executing the\nmodel in a lightweight runtime environment, we achieve a generation speed of 11\ntokens per second, making real-time, on-device inference feasible without\nhardware acceleration. Our results demonstrate the potential of SLMs to\ntransform vehicle control systems, enabling more intuitive interactions between\nusers and their vehicles for an enhanced driving experience.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02342v1",
    "published_date": "2025-01-04 17:32:56 UTC",
    "updated_date": "2025-01-04 17:32:56 UTC"
  },
  {
    "arxiv_id": "2501.02341v2",
    "title": "UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude Mobility",
    "authors": [
      "Yonglin Tian",
      "Fei Lin",
      "Yiduo Li",
      "Tengchao Zhang",
      "Qiyao Zhang",
      "Xuan Fu",
      "Jun Huang",
      "Xingyuan Dai",
      "Yutong Wang",
      "Chunwei Tian",
      "Bai Li",
      "Yisheng Lv",
      "Levente Kovács",
      "Fei-Yue Wang"
    ],
    "abstract": "Low-altitude mobility, exemplified by unmanned aerial vehicles (UAVs), has\nintroduced transformative advancements across various domains, like\ntransportation, logistics, and agriculture. Leveraging flexible perspectives\nand rapid maneuverability, UAVs extend traditional systems' perception and\naction capabilities, garnering widespread attention from academia and industry.\nHowever, current UAV operations primarily depend on human control, with only\nlimited autonomy in simple scenarios, and lack the intelligence and\nadaptability needed for more complex environments and tasks. The emergence of\nlarge language models (LLMs) demonstrates remarkable problem-solving and\ngeneralization capabilities, offering a promising pathway for advancing UAV\nintelligence. This paper explores the integration of LLMs and UAVs, beginning\nwith an overview of UAV systems' fundamental components and functionalities,\nfollowed by an overview of the state-of-the-art in LLM technology.\nSubsequently, it systematically highlights the multimodal data resources\navailable for UAVs, which provide critical support for training and evaluation.\nFurthermore, it categorizes and analyzes key tasks and application scenarios\nwhere UAVs and LLMs converge. Finally, a reference roadmap towards agentic UAVs\nis proposed, aiming to enable UAVs to achieve agentic intelligence through\nautonomous perception, memory, reasoning, and tool utilization. Related\nresources are available at https://github.com/Hub-Tian/UAVs_Meet_LLMs.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02341v2",
    "published_date": "2025-01-04 17:32:12 UTC",
    "updated_date": "2025-03-25 15:55:33 UTC"
  },
  {
    "arxiv_id": "2501.02338v1",
    "title": "Evaluation of the Code Generation Capabilities of ChatGPT 4: A Comparative Analysis in 19 Programming Languages",
    "authors": [
      "L. C. Gilbert"
    ],
    "abstract": "This bachelor's thesis examines the capabilities of ChatGPT 4 in code\ngeneration across 19 programming languages. The study analyzed solution rates\nacross three difficulty levels, types of errors encountered, and code quality\nin terms of runtime and memory efficiency through a quantitative experiment. A\ntotal of 188 programming problems were selected from the LeetCode platform, and\nChatGPT 4 was given three attempts to produce a correct solution with feedback.\nChatGPT 4 successfully solved 39.67% of all tasks, with success rates\ndecreasing significantly as problem complexity increased. Notably, the model\nfaced considerable challenges with hard problems across all languages. ChatGPT\n4 demonstrated higher competence in widely used languages, likely due to a\nlarger volume and higher quality of training data. The solution rates also\nrevealed a preference for languages with low abstraction levels and static\ntyping. For popular languages, the most frequent error was \"Wrong Answer,\"\nwhereas for less popular languages, compiler and runtime errors prevailed,\nsuggesting frequent misunderstandings and confusion regarding the structural\ncharacteristics of these languages. The model exhibited above-average runtime\nefficiency in all programming languages, showing a tendency toward statically\ntyped and low-abstraction languages. Memory efficiency results varied\nsignificantly, with above-average performance in 14 languages and below-average\nperformance in five languages. A slight preference for low-abstraction\nlanguages and a leaning toward dynamically typed languages in terms of memory\nefficiency were observed. Future research should include a larger number of\ntasks, iterations, and less popular languages. Additionally, ChatGPT 4's\nabilities in code interpretation and summarization, debugging, and the\ndevelopment of complex, practical code could be analyzed further.\n  ----\n  Diese Bachelorarbeit untersucht die F\\\"ahigkeiten von ChatGPT 4 zur\nCode-Generierung in 19 Programmiersprachen. Betrachtet wurden die\nL\\\"osungsraten zwischen drei Schwierigkeitsgraden, die aufgetretenen\nFehlerarten und die Qualit\\\"at des Codes hinsichtlich der Laufzeit- und\nSpeichereffizienz in einem quantitativen Experiment. Dabei wurden 188\nProgrammierprobleme der Plattform LeetCode entnommen, wobei ChatGPT 4 jeweils\ndrei Versuche hatte, mittels Feedback eine korrekte L\\\"osung zu generieren.\nChatGPT 4 l\\\"oste 39,67 % aller Aufgaben erfolgreich, wobei die Erfolgsrate mit\nzunehmendem Schwierigkeitsgrad deutlich abnahm und bei komplexen Problemen in\nallen Sprachen signifikante Schwierigkeiten auftraten. Das Modell zeigte eine\nh\\\"ohere Kompetenz in weit verbreiteten Sprachen, was wahrscheinlich auf eine\ngr\\\"o{\\ss}ere Menge und h\\\"ohere Qualit\\\"at der Trainingsdaten\nzur\\\"uckzuf\\\"uhren ist. Bez\\\"uglich der L\\\"osungsraten zeigte das Modell zudem\neine Pr\\\"aferenz f\\\"ur Sprachen mit niedrigem Abstraktionsniveau und statischer\nTypisierung. Bei Sprachen hoher Popularit\\\"at trat der Fehler Wrong Answer am\nh\\\"aufigsten auf, w\\\"ahrend bei weniger popul\\\"aren Sprachen Compiler- und\nLaufzeitfehler \\\"uberwogen, was auf h\\\"aufige Missverst\\\"andnisse und\nVerwechslungen bez\\\"uglich der spezifischen strukturellen Eigenschaften dieser\nSprachen zur\\\"uckzuf\\\"uhren ist. ChatGPT 4 demonstrierte in allen\nProgrammiersprachen eine \\\"uberdurchschnittliche Laufzeiteffizienz und\ntendierte diesbez\\\"uglich erneut zu statisch typisierten und niedrig\nabstrahierten Sprachen. Die Werte zur Speichereffizienz variierten erheblich,\nwobei in 14 Sprachen \\\"uberdurchschnittliche und in f\\\"unf Sprachen\nunterdurchschnittliche Werte erzielt wurden. Es zeigte sich diesbez\\\"uglich\neine leichte Tendenz zugunsten von niedrig abstrahierten sowie eine Pr\\\"aferenz\nzu dynamisch typisierten Sprachen. Zuk\\\"unftige Forschung sollte eine h\\\"ohere\nAnzahl an Aufgaben, Iterationen und unpopul\\\"aren Sprachen einbeziehen.\nDar\\\"uber hinaus k\\\"onnten die F\\\"ahigkeiten von ChatGPT 4 in der\nCode-Interpretation und -Zusammenfassung, im Debugging und in der Entwicklung\nkomplexer, praxisbezogener Codes analysiert werden.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "65 pages, in German, Bachelor's thesis on the evaluation of ChatGPT\n  4's code generation capabilities in 19 programming languages, University of\n  Potsdam, June 2024",
    "pdf_url": "http://arxiv.org/pdf/2501.02338v1",
    "published_date": "2025-01-04 17:17:01 UTC",
    "updated_date": "2025-01-04 17:17:01 UTC"
  },
  {
    "arxiv_id": "2501.02336v1",
    "title": "AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference",
    "authors": [
      "Zhuomin He",
      "Yizhen Yao",
      "Pengfei Zuo",
      "Bin Gao",
      "Qinya Li",
      "Zhenzhe Zheng",
      "Fan Wu"
    ],
    "abstract": "Long-context large language models (LLMs) inference is increasingly critical,\nmotivating a number of studies devoted to alleviating the substantial storage\nand computational costs in such scenarios. Layer-wise skipping methods are\npromising optimizations but rarely explored in long-context inference. We\nobserve that existing layer-wise skipping strategies have several limitations\nwhen applied in long-context inference, including the inability to adapt to\nmodel and context variability, disregard for sublayer significance, and\ninapplicability for the prefilling phase. This paper proposes \\sysname, an\nadaptive sublayer skipping method specifically designed for long-context\ninference. \\sysname adaptively identifies less important layers by leveraging\non-the-fly similarity information, enables sublayer-wise skipping, and\naccelerates both the prefilling and decoding phases. The effectiveness of\n\\sysname is demonstrated through extensive experiments on various long-context\nbenchmarks and models, showcasing its superior inference performance over\nexisting baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages,10 figures, AAAI",
    "pdf_url": "http://arxiv.org/pdf/2501.02336v1",
    "published_date": "2025-01-04 17:01:30 UTC",
    "updated_date": "2025-01-04 17:01:30 UTC"
  },
  {
    "arxiv_id": "2501.02334v1",
    "title": "Validity Arguments For Constructed Response Scoring Using Generative Artificial Intelligence Applications",
    "authors": [
      "Jodi M. Casabianca",
      "Daniel F. McCaffrey",
      "Matthew S. Johnson",
      "Naim Alper",
      "Vladimir Zubenko"
    ],
    "abstract": "The rapid advancements in large language models and generative artificial\nintelligence (AI) capabilities are making their broad application in the\nhigh-stakes testing context more likely. Use of generative AI in the scoring of\nconstructed responses is particularly appealing because it reduces the effort\nrequired for handcrafting features in traditional AI scoring and might even\noutperform those methods. The purpose of this paper is to highlight the\ndifferences in the feature-based and generative AI applications in constructed\nresponse scoring systems and propose a set of best practices for the collection\nof validity evidence to support the use and interpretation of constructed\nresponse scores from scoring systems using generative AI. We compare the\nvalidity evidence needed in scoring systems using human ratings, feature-based\nnatural language processing AI scoring engines, and generative AI. The evidence\nneeded in the generative AI context is more extensive than in the feature-based\nNLP scoring context because of the lack of transparency and other concerns\nunique to generative AI such as consistency. Constructed response score data\nfrom standardized tests demonstrate the collection of validity evidence for\ndifferent types of scoring systems and highlights the numerous complexities and\nconsiderations when making a validity argument for these scores. In addition,\nwe discuss how the evaluation of AI scores might include a consideration of how\na contributory scoring approach combining multiple AI scores (from different\nsources) will cover more of the construct in the absence of human ratings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "33 pages, 2 figures, 6 tables; This work was presented at the 2024\n  meeting of the International Testing Commission in Granada, Spain",
    "pdf_url": "http://arxiv.org/pdf/2501.02334v1",
    "published_date": "2025-01-04 16:59:29 UTC",
    "updated_date": "2025-01-04 16:59:29 UTC"
  },
  {
    "arxiv_id": "2501.02330v2",
    "title": "SR-Reward: Taking The Path More Traveled",
    "authors": [
      "Seyed Mahdi B. Azad",
      "Zahra Padar",
      "Gabriel Kalweit",
      "Joschka Boedecker"
    ],
    "abstract": "In this paper, we propose a novel method for learning reward functions\ndirectly from offline demonstrations. Unlike traditional inverse reinforcement\nlearning (IRL), our approach decouples the reward function from the learner's\npolicy, eliminating the adversarial interaction typically required between the\ntwo. This results in a more stable and efficient training process. Our reward\nfunction, called \\textit{SR-Reward}, leverages successor representation (SR) to\nencode a state based on expected future states' visitation under the\ndemonstration policy and transition dynamics. By utilizing the Bellman\nequation, SR-Reward can be learned concurrently with most reinforcement\nlearning (RL) algorithms without altering the existing training pipeline. We\nalso introduce a negative sampling strategy to mitigate overestimation errors\nby reducing rewards for out-of-distribution data, thereby enhancing robustness.\nThis strategy inherently introduces a conservative bias into RL algorithms that\nemploy the learned reward. We evaluate our method on the D4RL benchmark,\nachieving competitive results compared to offline RL algorithms with access to\ntrue rewards and imitation learning (IL) techniques like behavioral cloning.\nMoreover, our ablation studies on data size and quality reveal the advantages\nand limitations of SR-Reward as a proxy for true rewards.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02330v2",
    "published_date": "2025-01-04 16:21:10 UTC",
    "updated_date": "2025-04-29 13:02:32 UTC"
  },
  {
    "arxiv_id": "2501.02313v1",
    "title": "DiffGraph: Heterogeneous Graph Diffusion Model",
    "authors": [
      "Zongwei Li",
      "Lianghao Xia",
      "Hua Hua",
      "Shijie Zhang",
      "Shuangyang Wang",
      "Chao Huang"
    ],
    "abstract": "Recent advances in Graph Neural Networks (GNNs) have revolutionized\ngraph-structured data modeling, yet traditional GNNs struggle with complex\nheterogeneous structures prevalent in real-world scenarios. Despite progress in\nhandling heterogeneous interactions, two fundamental challenges persist: noisy\ndata significantly compromising embedding quality and learning performance, and\nexisting methods' inability to capture intricate semantic transitions among\nheterogeneous relations, which impacts downstream predictions. To address these\nfundamental issues, we present the Heterogeneous Graph Diffusion Model\n(DiffGraph), a pioneering framework that introduces an innovative cross-view\ndenoising strategy. This advanced approach transforms auxiliary heterogeneous\ndata into target semantic spaces, enabling precise distillation of\ntask-relevant information. At its core, DiffGraph features a sophisticated\nlatent heterogeneous graph diffusion mechanism, implementing a novel forward\nand backward diffusion process for superior noise management. This methodology\nachieves simultaneous heterogeneous graph denoising and cross-type transition,\nwhile significantly simplifying graph generation through its latent-space\ndiffusion capabilities. Through rigorous experimental validation on both public\nand industrial datasets, we demonstrate that DiffGraph consistently surpasses\nexisting methods in link prediction and node classification tasks, establishing\nnew benchmarks for robustness and efficiency in heterogeneous graph processing.\nThe model implementation is publicly available at:\nhttps://github.com/HKUDS/DiffGraph.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "This paper is accepted by WSDM'2025",
    "pdf_url": "http://arxiv.org/pdf/2501.02313v1",
    "published_date": "2025-01-04 15:30:48 UTC",
    "updated_date": "2025-01-04 15:30:48 UTC"
  },
  {
    "arxiv_id": "2501.02287v1",
    "title": "Deep Learning-Driven Segmentation of Ischemic Stroke Lesions Using Multi-Channel MRI",
    "authors": [
      "Ashiqur Rahman",
      "Muhammad E. H. Chowdhury",
      "Md Sharjis Ibne Wadud",
      "Rusab Sarmun",
      "Adam Mushtak",
      "Sohaib Bassam Zoghoul",
      "Israa Al-Hashimi"
    ],
    "abstract": "Ischemic stroke, caused by cerebral vessel occlusion, presents substantial\nchallenges in medical imaging due to the variability and subtlety of stroke\nlesions. Magnetic Resonance Imaging (MRI) plays a crucial role in diagnosing\nand managing ischemic stroke, yet existing segmentation techniques often fail\nto accurately delineate lesions. This study introduces a novel deep\nlearning-based method for segmenting ischemic stroke lesions using\nmulti-channel MRI modalities, including Diffusion Weighted Imaging (DWI),\nApparent Diffusion Coefficient (ADC), and enhanced Diffusion Weighted Imaging\n(eDWI). The proposed architecture integrates DenseNet121 as the encoder with\nSelf-Organized Operational Neural Networks (SelfONN) in the decoder, enhanced\nby Channel and Space Compound Attention (CSCA) and Double\nSqueeze-and-Excitation (DSE) blocks. Additionally, a custom loss function\ncombining Dice Loss and Jaccard Loss with weighted averages is introduced to\nimprove model performance. Trained and evaluated on the ISLES 2022 dataset, the\nmodel achieved Dice Similarity Coefficients (DSC) of 83.88% using DWI alone,\n85.86% with DWI and ADC, and 87.49% with the integration of DWI, ADC, and eDWI.\nThis approach not only outperforms existing methods but also addresses key\nlimitations in current segmentation practices. These advancements significantly\nenhance diagnostic precision and treatment planning for ischemic stroke,\nproviding valuable support for clinical decision-making.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02287v1",
    "published_date": "2025-01-04 13:38:06 UTC",
    "updated_date": "2025-01-04 13:38:06 UTC"
  },
  {
    "arxiv_id": "2501.14785v1",
    "title": "ED-Filter: Dynamic Feature Filtering for Eating Disorder Classification",
    "authors": [
      "Mehdi Naseriparsa",
      "Suku Sukunesan",
      "Zhen Cai",
      "Osama Alfarraj",
      "Amr Tolba",
      "Saba Fathi Rabooki",
      "Feng Xia"
    ],
    "abstract": "Eating disorders (ED) are critical psychiatric problems that have alarmed the\nmental health community. Mental health professionals are increasingly\nrecognizing the utility of data derived from social media platforms such as\nTwitter. However, high dimensionality and extensive feature sets of Twitter\ndata present remarkable challenges for ED classification. To overcome these\nhurdles, we introduce a novel method, an informed branch and bound search\ntechnique known as ED-Filter. This strategy significantly improves the\ndrawbacks of conventional feature selection algorithms such as filters and\nwrappers. ED-Filter iteratively identifies an optimal set of promising features\nthat maximize the eating disorder classification accuracy. In order to adapt to\nthe dynamic nature of Twitter ED data, we enhance the ED-Filter with a hybrid\ngreedy-based deep learning algorithm. This algorithm swiftly identifies\nsub-optimal features to accommodate the ever-evolving data landscape.\nExperimental results on Twitter eating disorder data affirm the effectiveness\nand efficiency of ED-Filter. The method demonstrates significant improvements\nin classification accuracy and proves its value in eating disorder detection on\nsocial media platforms.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14785v1",
    "published_date": "2025-01-04 13:35:55 UTC",
    "updated_date": "2025-01-04 13:35:55 UTC"
  },
  {
    "arxiv_id": "2501.02285v2",
    "title": "Hyperbolic Contrastive Learning for Hierarchical 3D Point Cloud Embedding",
    "authors": [
      "Yingjie Liu",
      "Pengyu Zhang",
      "Ziyao He",
      "Mingsong Chen",
      "Xuan Tang",
      "Xian Wei"
    ],
    "abstract": "Hyperbolic spaces allow for more efficient modeling of complex, hierarchical\nstructures, which is particularly beneficial in tasks involving multi-modal\ndata. Although hyperbolic geometries have been proven effective for\nlanguage-image pre-training, their capabilities to unify language, image, and\n3D Point Cloud modalities are under-explored. We extend the 3D Point Cloud\nmodality in hyperbolic multi-modal contrastive pre-training. Additionally, we\nexplore the entailment, modality gap, and alignment regularizers for learning\nhierarchical 3D embeddings and facilitating the transfer of knowledge from both\nText and Image modalities. These regularizers enable the learning of\nintra-modal hierarchy within each modality and inter-modal hierarchy across\ntext, 2D images, and 3D Point Clouds. Experimental results demonstrate that our\nproposed training strategy yields an outstanding 3D Point Cloud encoder, and\nthe obtained 3D Point Cloud hierarchical embeddings significantly improve\nperformance on various downstream tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02285v2",
    "published_date": "2025-01-04 13:27:18 UTC",
    "updated_date": "2025-01-07 13:38:34 UTC"
  },
  {
    "arxiv_id": "2501.02268v1",
    "title": "What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph",
    "authors": [
      "Yutao Jiang",
      "Qiong Wu",
      "Wenhao Lin",
      "Wei Yu",
      "Yiyi Zhou"
    ],
    "abstract": "Recent Multimodal Large Language Models(MLLMs) often use a large number of\nvisual tokens to compensate their visual shortcoming, leading to excessive\ncomputation and obvious visual redundancy. In this paper, we investigate what\nkind of visual tokens are needed for MLLMs, and reveal that both foreground and\nbackground tokens are critical for MLLMs given the varying difficulties of\nexamples. Based on this observation, we propose a graph-based method towards\ntraining-free visual token pruning, termed G-Prune.In particular, G-Prune\nregards visual tokens as nodes, and construct their connections based on their\nsemantic similarities. Afterwards, the information flow is propagated via\nweighted links, and the most important tokens after iterations are kept for\nMLLMs, which can be front or background.To validate G-Prune, we apply it to a\nrecent MLLM called LLaVA-NeXT, and conduct extensive experiments on a set of\nbenchmarks.The experiment results show that G-Prune can greatly reduce\ncomputation overhead while retaining high performance on both coarse- and\nfine-grained tasks. For instance, G-Prune can reduce 63.57\\% FLOPs of\nLLaVA-NeXT on VQA2.0 and TextVQA with only 0.95\\% and 2.34\\% accuracy drops,\nrespectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.02268v1",
    "published_date": "2025-01-04 12:14:42 UTC",
    "updated_date": "2025-01-04 12:14:42 UTC"
  },
  {
    "arxiv_id": "2501.02267v1",
    "title": "Towards a constructive framework for control theory",
    "authors": [
      "Pavel Osinenko"
    ],
    "abstract": "This work presents a framework for control theory based on constructive\nanalysis to account for discrepancy between mathematical results and their\nimplementation in a computer, also referred to as computational uncertainty. In\ncontrol engineering, the latter is usually either neglected or considered\nsubmerged into some other type of uncertainty, such as system noise, and\naddressed within robust control. However, even robust control methods may be\ncompromised when the mathematical objects involved in the respective algorithms\nfail to exist in exact form and subsequently fail to satisfy the required\nproperties. For instance, in general stabilization using a control Lyapunov\nfunction, computational uncertainty may distort stability certificates or even\ndestabilize the system despite robustness of the stabilization routine with\nregards to system, actuator and measurement noise. In fact, battling numerical\nproblems in practical implementation of controllers is common among control\nengineers. Such observations indicate that computational uncertainty should\nindeed be addressed explicitly in controller synthesis and system analysis. The\nmajor contribution here is a fairly general framework for proof techniques in\nanalysis and synthesis of control systems based on constructive analysis which\nexplicitly states that every computation be doable only up to a finite\nprecision thus accounting for computational uncertainty. A series of previous\nworks is overviewed, including constructive system stability and stabilization,\napproximate optimal controls, eigenvalue problems, Caratheodory trajectories,\nmeasurable selectors. Additionally, a new constructive version of the Danskin's\ntheorem, which is crucial in adversarial defense, is presented.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "math.OC",
    "comment": "Published under: https://ieeexplore.ieee.org/document/9419858",
    "pdf_url": "http://arxiv.org/pdf/2501.02267v1",
    "published_date": "2025-01-04 12:07:45 UTC",
    "updated_date": "2025-01-04 12:07:45 UTC"
  },
  {
    "arxiv_id": "2501.02266v1",
    "title": "LLMzSzŁ: a comprehensive LLM benchmark for Polish",
    "authors": [
      "Krzysztof Jassem",
      "Michał Ciesiółka",
      "Filip Graliński",
      "Piotr Jabłoński",
      "Jakub Pokrywka",
      "Marek Kubis",
      "Monika Jabłońska",
      "Ryszard Staruch"
    ],
    "abstract": "This article introduces the first comprehensive benchmark for the Polish\nlanguage at this scale: LLMzSz{\\L} (LLMs Behind the School Desk). It is based\non a coherent collection of Polish national exams, including both academic and\nprofessional tests extracted from the archives of the Polish Central\nExamination Board. It covers 4 types of exams, coming from 154 domains.\nAltogether, it consists of almost 19k closed-ended questions. We investigate\nthe performance of open-source multilingual, English, and Polish LLMs to verify\nLLMs' abilities to transfer knowledge between languages. Also, the correlation\nbetween LLMs and humans at model accuracy and exam pass rate levels is\nexamined. We show that multilingual LLMs can obtain superior results over\nmonolingual ones; however, monolingual models may be beneficial when model size\nmatters. Our analysis highlights the potential of LLMs in assisting with exam\nvalidation, particularly in identifying anomalies or errors in examination\ntasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02266v1",
    "published_date": "2025-01-04 12:04:46 UTC",
    "updated_date": "2025-01-04 12:04:46 UTC"
  },
  {
    "arxiv_id": "2501.03268v1",
    "title": "Heterogeneous Graph Pre-training Based Model for Secure and Efficient Prediction of Default Risk Propagation among Bond Issuers",
    "authors": [
      "Xurui Li",
      "Xin Shan",
      "Wenhao Yin",
      "Haijiao Wang"
    ],
    "abstract": "Efficient prediction of default risk for bond-issuing enterprises is pivotal\nfor maintaining stability and fostering growth in the bond market. Conventional\nmethods usually rely solely on an enterprise's internal data for risk\nassessment. In contrast, graph-based techniques leverage interconnected\ncorporate information to enhance default risk identification for targeted bond\nissuers. Traditional graph techniques such as label propagation algorithm or\ndeepwalk fail to effectively integrate a enterprise's inherent attribute\ninformation with its topological network data. Additionally, due to data\nscarcity and security privacy concerns between enterprises, end-to-end graph\nneural network (GNN) algorithms may struggle in delivering satisfactory\nperformance for target tasks. To address these challenges, we present a novel\ntwo-stage model. In the first stage, we employ an innovative Masked\nAutoencoders for Heterogeneous Graph (HGMAE) to pre-train on a vast enterprise\nknowledge graph. Subsequently, in the second stage, a specialized classifier\nmodel is trained to predict default risk propagation probabilities. The\nclassifier leverages concatenated feature vectors derived from the pre-trained\nencoder with the enterprise's task-specific feature vectors. Through the\ntwo-stage training approach, our model not only boosts the importance of unique\nbond characteristics for specific default prediction tasks, but also securely\nand efficiently leverage the global information pre-trained from other\nenterprises. Experimental results demonstrate that our proposed model\noutperforms existing approaches in predicting default risk for bond issuers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03268v1",
    "published_date": "2025-01-04 11:10:16 UTC",
    "updated_date": "2025-01-04 11:10:16 UTC"
  },
  {
    "arxiv_id": "2501.02241v1",
    "title": "Interpretable Load Forecasting via Representation Learning of Geo-distributed Meteorological Factors",
    "authors": [
      "Yangze Zhou",
      "Guoxin Lin",
      "Gonghao Zhang",
      "Yi Wang"
    ],
    "abstract": "Meteorological factors (MF) are crucial in day-ahead load forecasting as they\nsignificantly influence the electricity consumption behaviors of consumers.\nNumerous studies have incorporated MF into the load forecasting model to\nachieve higher accuracy. Selecting MF from one representative location or the\naveraged MF as the inputs of the forecasting model is a common practice.\nHowever, the difference in MF collected in various locations within a region\nmay be significant, which poses a challenge in selecting the appropriate MF\nfrom numerous locations. A representation learning framework is proposed to\nextract geo-distributed MF while considering their spatial relationships. In\naddition, this paper employs the Shapley value in the graph-based model to\nreveal connections between MF collected in different locations and loads. To\nreduce the computational complexity of calculating the Shapley value, an\nacceleration method is adopted based on Monte Carlo sampling and weighted\nlinear regression. Experiments on two real-world datasets demonstrate that the\nproposed method improves the day-ahead forecasting accuracy, especially in\nextreme scenarios such as the \"accumulation temperature effect\" in summer and\n\"sudden temperature change\" in winter. We also find a significant correlation\nbetween the importance of MF in different locations and the corresponding\narea's GDP and mainstay industry.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02241v1",
    "published_date": "2025-01-04 09:05:06 UTC",
    "updated_date": "2025-01-04 09:05:06 UTC"
  },
  {
    "arxiv_id": "2502.15697v1",
    "title": "Robust Uplift Modeling with Large-Scale Contexts for Real-time Marketing",
    "authors": [
      "Zexu Sun",
      "Qiyu Han",
      "Minqin Zhu",
      "Hao Gong",
      "Dugang Liu",
      "Chen Ma"
    ],
    "abstract": "Improving user engagement and platform revenue is crucial for online\nmarketing platforms. Uplift modeling is proposed to solve this problem, which\napplies different treatments (e.g., discounts, bonus) to satisfy corresponding\nusers. Despite progress in this field, limitations persist. Firstly, most of\nthem focus on scenarios where only user features exist. However, in real-world\nscenarios, there are rich contexts available in the online platform (e.g.,\nshort videos, news), and the uplift model needs to infer an incentive for each\nuser on the specific item, which is called real-time marketing. Thus, only\nconsidering the user features will lead to biased prediction of the responses,\nwhich may cause the cumulative error for uplift prediction. Moreover, due to\nthe large-scale contexts, directly concatenating the context features with the\nuser features will cause a severe distribution shift in the treatment and\ncontrol groups. Secondly, capturing the interaction relationship between the\nuser features and context features can better predict the user response. To\nsolve the above limitations, we propose a novel model-agnostic Robust Uplift\nModeling with Large-Scale Contexts (UMLC) framework for Real-time Marketing.\nOur UMLC includes two customized modules. 1) A response-guided context grouping\nmodule for extracting context features information and condensing value space\nthrough clusters. 2) A feature interaction module for obtaining better uplift\nprediction. Specifically, this module contains two parts: a user-context\ninteraction component for better modeling the response; a treatment-feature\ninteraction component for discovering the treatment assignment sensitive\nfeature of each instance to better predict the uplift. Moreover, we conduct\nextensive experiments on a synthetic dataset and a real-world product dataset\nto verify the effectiveness and compatibility of our UMLC.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted to KDD'25 Research Track, 15 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2502.15697v1",
    "published_date": "2025-01-04 08:55:50 UTC",
    "updated_date": "2025-01-04 08:55:50 UTC"
  },
  {
    "arxiv_id": "2501.02237v1",
    "title": "Financial Named Entity Recognition: How Far Can LLM Go?",
    "authors": [
      "Yi-Te Lu",
      "Yintong Huo"
    ],
    "abstract": "The surge of large language models (LLMs) has revolutionized the extraction\nand analysis of crucial information from a growing volume of financial\nstatements, announcements, and business news. Recognition for named entities to\nconstruct structured data poses a significant challenge in analyzing financial\ndocuments and is a foundational task for intelligent financial analytics.\nHowever, how effective are these generic LLMs and their performance under\nvarious prompts are yet need a better understanding. To fill in the blank, we\npresent a systematic evaluation of state-of-the-art LLMs and prompting methods\nin the financial Named Entity Recognition (NER) problem. Specifically, our\nexperimental results highlight their strengths and limitations, identify five\nrepresentative failure types, and provide insights into their potential and\nchallenges for domain-specific tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at The Joint Workshop of the 9th Financial Technology and\n  Natural Language Processing (FinNLP), the 6th Financial Narrative Processing\n  (FNP), and the 1st Workshop on Large Language Models for Finance and Legal\n  (LLMFinLegal), in conjunction with COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.02237v1",
    "published_date": "2025-01-04 08:47:21 UTC",
    "updated_date": "2025-01-04 08:47:21 UTC"
  },
  {
    "arxiv_id": "2501.02221v2",
    "title": "CORD: Generalizable Cooperation via Role Diversity",
    "authors": [
      "Kanefumi Matsuyama",
      "Kefan Su",
      "Jiangxing Wang",
      "Deheng Ye",
      "Zongqing Lu"
    ],
    "abstract": "Cooperative multi-agent reinforcement learning (MARL) aims to develop agents\nthat can collaborate effectively. However, most cooperative MARL methods\noverfit training agents, making learned policies not generalize well to unseen\ncollaborators, which is a critical issue for real-world deployment. Some\nmethods attempt to address the generalization problem but require prior\nknowledge or predefined policies of new teammates, limiting real-world\napplications. To this end, we propose a hierarchical MARL approach to enable\ngeneralizable cooperation via role diversity, namely CORD. CORD's high-level\ncontroller assigns roles to low-level agents by maximizing the role entropy\nwith constraints. We show this constrained objective can be decomposed into\ncausal influence in role that enables reasonable role assignment, and role\nheterogeneity that yields coherent, non-redundant role clusters. Evaluated on a\nvariety of cooperative multi-agent tasks, CORD achieves better performance than\nbaselines, especially in generalization tests. Ablation studies further\ndemonstrate the efficacy of the constrained objective in generalizable\ncooperation.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02221v2",
    "published_date": "2025-01-04 07:53:38 UTC",
    "updated_date": "2025-01-10 09:26:32 UTC"
  },
  {
    "arxiv_id": "2501.02219v1",
    "title": "Diffusion Model-Based Data Synthesis Aided Federated Semi-Supervised Learning",
    "authors": [
      "Zhongwei Wang",
      "Tong Wu",
      "Zhiyong Chen",
      "Liang Qian",
      "Yin Xu",
      "Meixia Tao"
    ],
    "abstract": "Federated semi-supervised learning (FSSL) is primarily challenged by two\nfactors: the scarcity of labeled data across clients and the non-independent\nand identically distribution (non-IID) nature of data among clients. In this\npaper, we propose a novel approach, diffusion model-based data synthesis aided\nFSSL (DDSA-FSSL), which utilizes a diffusion model (DM) to generate synthetic\ndata, bridging the gap between heterogeneous local data distributions and the\nglobal data distribution. In DDSA-FSSL, clients address the challenge of the\nscarcity of labeled data by employing a federated learning-trained classifier\nto perform pseudo labeling for unlabeled data. The DM is then collaboratively\ntrained using both labeled and precision-optimized pseudo-labeled data,\nenabling clients to generate synthetic samples for classes that are absent in\ntheir labeled datasets. This process allows clients to generate more\ncomprehensive synthetic datasets aligned with the global distribution.\nExtensive experiments conducted on multiple datasets and varying non-IID\ndistributions demonstrate the effectiveness of DDSA-FSSL, e.g., it improves\naccuracy from 38.46% to 52.14% on CIFAR-10 datasets with 10% labeled data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "accepted by IEEE WCNC 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.02219v1",
    "published_date": "2025-01-04 07:38:15 UTC",
    "updated_date": "2025-01-04 07:38:15 UTC"
  },
  {
    "arxiv_id": "2501.03266v2",
    "title": "LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena",
    "authors": [
      "Stefan Pasch"
    ],
    "abstract": "LLM safety and ethical alignment are widely discussed, but the impact of\ncontent moderation on user satisfaction remains underexplored. In particular,\nlittle is known about how users respond when models refuse to answer a\nprompt-one of the primary mechanisms used to enforce ethical boundaries in\nLLMs. We address this gap by analyzing nearly 50,000 model comparisons from\nChatbot Arena, a platform where users indicate their preferred LLM response in\npairwise matchups, providing a large-scale setting for studying real-world user\npreferences. Using a novel RoBERTa-based refusal classifier fine-tuned on a\nhand-labeled dataset, we distinguish between refusals due to ethical concerns\nand technical limitations. Our results reveal a substantial refusal penalty:\nethical refusals yield significantly lower win rates than both technical\nrefusals and standard responses, indicating that users are especially\ndissatisfied when models decline a task for ethical reasons. However, this\npenalty is not uniform. Refusals receive more favorable evaluations when the\nunderlying prompt is highly sensitive (e.g., involving illegal content), and\nwhen the refusal is phrased in a detailed and contextually aligned manner.\nThese findings underscore a core tension in LLM design: safety-aligned\nbehaviors may conflict with user expectations, calling for more adaptive\nmoderation strategies that account for context and presentation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03266v2",
    "published_date": "2025-01-04 06:36:44 UTC",
    "updated_date": "2025-05-16 01:23:54 UTC"
  },
  {
    "arxiv_id": "2501.03265v1",
    "title": "Optimizing Edge AI: A Comprehensive Survey on Data, Model, and System Strategies",
    "authors": [
      "Xubin Wang",
      "Weijia Jia"
    ],
    "abstract": "The emergence of 5G and edge computing hardware has brought about a\nsignificant shift in artificial intelligence, with edge AI becoming a crucial\ntechnology for enabling intelligent applications. With the growing amount of\ndata generated and stored on edge devices, deploying AI models for local\nprocessing and inference has become increasingly necessary. However, deploying\nstate-of-the-art AI models on resource-constrained edge devices faces\nsignificant challenges that must be addressed. This paper presents an\noptimization triad for efficient and reliable edge AI deployment, including\ndata, model, and system optimization. First, we discuss optimizing data through\ndata cleaning, compression, and augmentation to make it more suitable for edge\ndeployment. Second, we explore model design and compression methods at the\nmodel level, such as pruning, quantization, and knowledge distillation.\nFinally, we introduce system optimization techniques like framework support and\nhardware acceleration to accelerate edge AI workflows. Based on an in-depth\nanalysis of various application scenarios and deployment challenges of edge AI,\nthis paper proposes an optimization paradigm based on the data-model-system\ntriad to enable a whole set of solutions to effectively transfer ML models,\nwhich are initially trained in the cloud, to various edge devices for\nsupporting multiple scenarios.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.03265v1",
    "published_date": "2025-01-04 06:17:48 UTC",
    "updated_date": "2025-01-04 06:17:48 UTC"
  },
  {
    "arxiv_id": "2501.02200v1",
    "title": "Learning Evolution via Optimization Knowledge Adaptation",
    "authors": [
      "Chao Wang",
      "Licheng Jiao",
      "Jiaxuan Zhao",
      "Lingling Li",
      "Fang Liu",
      "Shuyuan Yang"
    ],
    "abstract": "Evolutionary algorithms (EAs) maintain populations through evolutionary\noperators to discover diverse solutions for complex tasks while gathering\nvaluable knowledge, such as historical population data and fitness evaluations.\nHowever, traditional EAs face challenges in dynamically adapting to expanding\nknowledge bases, hindering the efficient exploitation of accumulated\ninformation and limiting adaptability to new situations. To address these\nissues, we introduce an Optimization Knowledge Adaptation Evolutionary Model\n(OKAEM), which features dynamic parameter adjustment using accumulated\nknowledge to enhance its optimization capabilities. OKAEM employs attention\nmechanisms to model the interactions among individuals, fitness landscapes, and\ngenetic components separately, thereby parameterizing the evolutionary\noperators of selection, crossover, and mutation. These powerful learnable\noperators enable OKAEM to benefit from pre-learned extensive prior knowledge\nand self-tune with real-time evolutionary insights. Experimental results\ndemonstrate that OKAEM: 1) exploits prior knowledge for significant performance\ngains across various knowledge transfer settings; 2) achieves competitive\nperformance through self-tuning alone, even without prior knowledge; 3)\noutperforms state-of-the-art black-box baselines in a vision-language model\ntuning case; 4) can improve its optimization capabilities with growing\nknowledge; 5) is capable of emulating principles of natural selection and\ngenetic recombination.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "This work has been submitted to Springer Nature for possible\n  publication",
    "pdf_url": "http://arxiv.org/pdf/2501.02200v1",
    "published_date": "2025-01-04 05:35:21 UTC",
    "updated_date": "2025-01-04 05:35:21 UTC"
  },
  {
    "arxiv_id": "2501.02199v1",
    "title": "Can ChatGPT implement finite element models for geotechnical engineering applications?",
    "authors": [
      "Taegu Kim",
      "Tae Sup Yun",
      "Hyoung Suk Suh"
    ],
    "abstract": "This study assesses the capability of ChatGPT to generate finite element code\nfor geotechnical engineering applications from a set of prompts. We tested\nthree different initial boundary value problems using a hydro-mechanically\ncoupled formulation for unsaturated soils, including the dissipation of excess\npore water pressure through fluid mass diffusion in one-dimensional space,\ntime-dependent differential settlement of a strip footing, and gravity-driven\nseepage. For each case, initial prompting involved providing ChatGPT with\nnecessary information for finite element implementation, such as balance and\nconstitutive equations, problem geometry, initial and boundary conditions,\nmaterial properties, and spatiotemporal discretization and solution strategies.\nAny errors and unexpected results were further addressed through prompt\naugmentation processes until the ChatGPT-generated finite element code passed\nthe verification/validation test. Our results demonstrate that ChatGPT required\nminimal code revisions when using the FEniCS finite element library, owing to\nits high-level interfaces that enable efficient programming. In contrast, the\nMATLAB code generated by ChatGPT necessitated extensive prompt augmentations\nand/or direct human intervention, as it involves a significant amount of\nlow-level programming required for finite element analysis, such as\nconstructing shape functions or assembling global matrices. Given that prompt\nengineering for this task requires an understanding of the mathematical\nformulation and numerical techniques, this study suggests that while a large\nlanguage model may not yet replace human programmers, it can greatly assist in\nthe implementation of numerical models.",
    "categories": [
      "math.NA",
      "cs.AI",
      "cs.NA"
    ],
    "primary_category": "math.NA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02199v1",
    "published_date": "2025-01-04 05:21:40 UTC",
    "updated_date": "2025-01-04 05:21:40 UTC"
  },
  {
    "arxiv_id": "2501.02196v1",
    "title": "CPTuning: Contrastive Prompt Tuning for Generative Relation Extraction",
    "authors": [
      "Jiaxin Duan",
      "Fengyu Lu",
      "Junfei Liu"
    ],
    "abstract": "Generative relation extraction (RE) commonly involves first reformulating RE\nas a linguistic modeling problem easily tackled with pre-trained language\nmodels (PLM) and then fine-tuning a PLM with supervised cross-entropy loss.\nAlthough having achieved promising performance, existing approaches assume only\none deterministic relation between each pair of entities without considering\nreal scenarios where multiple relations may be valid, i.e., entity pair\noverlap, causing their limited applications. To address this problem, we\nintroduce a novel contrastive prompt tuning method for RE, CPTuning, which\nlearns to associate a candidate relation between two in-context entities with a\nprobability mass above or below a threshold, corresponding to whether the\nrelation exists. Beyond learning schema, CPTuning also organizes RE as a\nverbalized relation generation task and uses Trie-constrained decoding to\nensure a model generates valid relations. It adaptively picks out the generated\ncandidate relations with a high estimated likelihood in inference, thereby\nachieving multi-relation extraction. We conduct extensive experiments on four\nwidely used datasets to validate our method. Results show that T5-large\nfine-tuned with CPTuning significantly outperforms previous methods, regardless\nof single or multiple relations extraction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02196v1",
    "published_date": "2025-01-04 05:17:34 UTC",
    "updated_date": "2025-01-04 05:17:34 UTC"
  },
  {
    "arxiv_id": "2501.02189v6",
    "title": "A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges",
    "authors": [
      "Zongxia Li",
      "Xiyang Wu",
      "Hongyang Du",
      "Fuxiao Liu",
      "Huy Nghiem",
      "Guangyao Shi"
    ],
    "abstract": "Multimodal Vision Language Models (VLMs) have emerged as a transformative\ntopic at the intersection of computer vision and natural language processing,\nenabling machines to perceive and reason about the world through both visual\nand textual modalities. For example, models such as CLIP, Claude, and GPT-4V\ndemonstrate strong reasoning and understanding abilities on visual and textual\ndata and beat classical single modality vision models on zero-shot\nclassification [93]. With their rapid advancements in research and growing\npopularity in various applications, we provide a comprehensive survey of VLMs.\nSpecifically, we provide a systematic overview of VLMs in the following\naspects: [1] model information of the major VLMs developed up to 2025; [2] the\ntransition of VLM architectures and the newest VLM alignment methods; [3]\nsummary and categorization of the popular benchmarks and evaluation metrics of\nVLMs; [4] the challenges and issues faced by current VLMs such as\nhallucination, alignment, fairness, and safety. Detailed collections including\npapers and model repository links are listed in\nhttps://github.com/zli12321/Vision-Language-Models-Overview.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "22 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.02189v6",
    "published_date": "2025-01-04 04:59:33 UTC",
    "updated_date": "2025-04-06 03:12:51 UTC"
  },
  {
    "arxiv_id": "2501.02182v1",
    "title": "AdaMixup: A Dynamic Defense Framework for Membership Inference Attack Mitigation",
    "authors": [
      "Ying Chen",
      "Jiajing Chen",
      "Yijie Weng",
      "ChiaHua Chang",
      "Dezhi Yu",
      "Guanbiao Lin"
    ],
    "abstract": "Membership inference attacks have emerged as a significant privacy concern in\nthe training of deep learning models, where attackers can infer whether a data\npoint was part of the training set based on the model's outputs. To address\nthis challenge, we propose a novel defense mechanism, AdaMixup. AdaMixup\nemploys adaptive mixup techniques to enhance the model's robustness against\nmembership inference attacks by dynamically adjusting the mixup strategy during\ntraining. This method not only improves the model's privacy protection but also\nmaintains high performance. Experimental results across multiple datasets\ndemonstrate that AdaMixup significantly reduces the risk of membership\ninference attacks while achieving a favorable trade-off between defensive\nefficiency and model accuracy. This research provides an effective solution for\ndata privacy protection and lays the groundwork for future advancements in\nmixup training methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2501.02182v1",
    "published_date": "2025-01-04 04:21:48 UTC",
    "updated_date": "2025-01-04 04:21:48 UTC"
  },
  {
    "arxiv_id": "2501.03264v1",
    "title": "Bridge the Inference Gaps of Neural Processes via Expectation Maximization",
    "authors": [
      "Qi Wang",
      "Marco Federici",
      "Herke van Hoof"
    ],
    "abstract": "The neural process (NP) is a family of computationally efficient models for\nlearning distributions over functions. However, it suffers from under-fitting\nand shows suboptimal performance in practice. Researchers have primarily\nfocused on incorporating diverse structural inductive biases, \\textit{e.g.}\nattention or convolution, in modeling. The topic of inference suboptimality and\nan analysis of the NP from the optimization objective perspective has hardly\nbeen studied in earlier work. To fix this issue, we propose a surrogate\nobjective of the target log-likelihood of the meta dataset within the\nexpectation maximization framework. The resulting model, referred to as the\nSelf-normalized Importance weighted Neural Process (SI-NP), can learn a more\naccurate functional prior and has an improvement guarantee concerning the\ntarget log-likelihood. Experimental results show the competitive performance of\nSI-NP over other NPs objectives and illustrate that structural inductive\nbiases, such as attention modules, can also augment our method to achieve SOTA\nperformance. Our code is available at\n\\url{https://github.com/hhq123gogogo/SI_NPs}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR2023",
    "pdf_url": "http://arxiv.org/pdf/2501.03264v1",
    "published_date": "2025-01-04 03:28:21 UTC",
    "updated_date": "2025-01-04 03:28:21 UTC"
  },
  {
    "arxiv_id": "2501.02169v1",
    "title": "The Integration of Blockchain and Artificial Intelligence for Secure Healthcare Systems",
    "authors": [
      "Umar Safdar",
      "Simon Gabrael"
    ],
    "abstract": "Verisign reported a 125 percent increase in data breaches within the\nhealthcare sector in the United States during 2022, with 18.2 million patient\nrecords being impacted. Growing healthcare data volumes and diversification\nmean that medical information is becoming more valuable. Many Health Centers\nuse various technologies to ease the classification, storage, and exchange of\nbig data. This use can also make the health data of the users at risk and\nvulnerable. AI and blockchain are among the leading technologies at hand. With\nAI, data-driven operations and big data efficiency have been improved with\nrespect to traditional techniques. Due to its potential to bring about\nimprovements in health services and lower medical costs, this AI technology is\nregularly used in healthcare. Blockchain helps protect transactions on sharing\ninformation and private privacy as long as the exchange of knowledge is that of\nthe standard. The objective of this analysis is to investigate the research and\nunique contributions since 2008 regarding blockchain-integrated AI and\nhealthcare systems. The work sheds light on applied AI-based healthcare schemes\nwith machine, ballistic, and acrylic learning and disparate blockchain\nstructures. The use of technology in order to ensure patient data security and\nmanage medical information effectively in healthcare settings offers a highly\nsuccessful position for both healthcare providers and patients. From 2018 to\n2021, the best year was 2021 to grow, enhancing everything to examine the\ndownload of the device and the counting of Google Academies, for which the\njoining perspective was borrowed; local research experts were asked, identified\narticles in recent years, and read reviews of large research grants.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "13 pages, 4 Figures",
    "pdf_url": "http://arxiv.org/pdf/2501.02169v1",
    "published_date": "2025-01-04 02:53:55 UTC",
    "updated_date": "2025-01-04 02:53:55 UTC"
  },
  {
    "arxiv_id": "2501.14784v1",
    "title": "DeServe: Towards Affordable Offline LLM Inference via Decentralization",
    "authors": [
      "Linyu Wu",
      "Xiaoyuan Liu",
      "Tianneng Shi",
      "Zhe Ye",
      "Dawn Song"
    ],
    "abstract": "The rapid growth of generative AI and its integration into everyday workflows\nhave significantly increased the demand for large language model (LLM)\ninference services. While proprietary models remain popular, recent\nadvancements in open-source LLMs have positioned them as strong contenders.\nHowever, deploying these models is often constrained by the high costs and\nlimited availability of GPU resources. In response, this paper presents the\ndesign of a decentralized offline serving system for LLM inference. Utilizing\nidle GPU resources, our proposed system, DeServe, decentralizes access to LLMs\nat a lower cost. DeServe specifically addresses key challenges in optimizing\nserving throughput in high-latency network environments. Experiments\ndemonstrate that DeServe achieves a 6.7x-12.6x improvement in throughput over\nexisting serving system baselines in such conditions.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.14784v1",
    "published_date": "2025-01-04 02:10:50 UTC",
    "updated_date": "2025-01-04 02:10:50 UTC"
  },
  {
    "arxiv_id": "2501.02156v3",
    "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
    "authors": [
      "Chien-Ping Lu"
    ],
    "abstract": "As large-scale AI models expand, training becomes costlier and sustaining\nprogress grows harder. Classical scaling laws (e.g., Kaplan et al. (2020),\nHoffmann et al. (2022)) predict training loss from a static compute budget yet\nneglect time and efficiency, prompting the question: how can we balance\nballooning GPU fleets with rapidly improving hardware and algorithms? We\nintroduce the relative-loss equation, a time- and efficiency-aware framework\nthat extends classical AI scaling laws. Our model shows that, without ongoing\nefficiency gains, advanced performance could demand millennia of training or\nunrealistically large GPU fleets. However, near-exponential progress remains\nachievable if the \"efficiency-doubling rate\" parallels Moore's Law. By\nformalizing this race to efficiency, we offer a quantitative roadmap for\nbalancing front-loaded GPU investments with incremental improvements across the\nAI stack. Empirical trends suggest that sustained efficiency gains can push AI\nscaling well into the coming decade, providing a new perspective on the\ndiminishing returns inherent in classical scaling.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 3 figures. 2 tables, second draft",
    "pdf_url": "http://arxiv.org/pdf/2501.02156v3",
    "published_date": "2025-01-04 01:45:32 UTC",
    "updated_date": "2025-01-08 14:26:51 UTC"
  },
  {
    "arxiv_id": "2501.02152v1",
    "title": "Table as Thought: Exploring Structured Thoughts in LLM Reasoning",
    "authors": [
      "Zhenjie Sun",
      "Naihao Deng",
      "Haofei Yu",
      "Jiaxuan You"
    ],
    "abstract": "Large language models' reasoning abilities benefit from methods that organize\ntheir thought processes, such as chain-of-thought prompting, which employs a\nsequential structure to guide the reasoning process step-by-step. However,\nexisting approaches focus primarily on organizing the sequence of thoughts,\nleaving structure in individual thought steps underexplored. To address this\ngap, we propose Table as Thought, a framework inspired by cognitive\nneuroscience theories on human thought. Table as Thought organizes reasoning\nwithin a tabular schema, where rows represent sequential thought steps and\ncolumns capture critical constraints and contextual information to enhance\nreasoning. The reasoning process iteratively populates the table until\nself-verification ensures completeness and correctness. Our experiments show\nthat Table as Thought excels in planning tasks and demonstrates a strong\npotential for enhancing LLM performance in mathematical reasoning compared to\nunstructured thought baselines. This work provides a novel exploration of\nrefining thought representation within LLMs, paving the way for advancements in\nreasoning and AI cognition.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02152v1",
    "published_date": "2025-01-04 00:58:06 UTC",
    "updated_date": "2025-01-04 00:58:06 UTC"
  },
  {
    "arxiv_id": "2501.02149v1",
    "title": "Attribute-Based Robotic Grasping with Data-Efficient Adaptation",
    "authors": [
      "Yang Yang",
      "Houjian Yu",
      "Xibai Lou",
      "Yuanhao Liu",
      "Changhyun Choi"
    ],
    "abstract": "Robotic grasping is one of the most fundamental robotic manipulation tasks\nand has been the subject of extensive research. However, swiftly teaching a\nrobot to grasp a novel target object in clutter remains challenging. This paper\nattempts to address the challenge by leveraging object attributes that\nfacilitate recognition, grasping, and rapid adaptation to new domains. In this\nwork, we present an end-to-end encoder-decoder network to learn attribute-based\nrobotic grasping with data-efficient adaptation capability. We first pre-train\nthe end-to-end model with a variety of basic objects to learn generic attribute\nrepresentation for recognition and grasping. Our approach fuses the embeddings\nof a workspace image and a query text using a gated-attention mechanism and\nlearns to predict instance grasping affordances. To train the joint embedding\nspace of visual and textual attributes, the robot utilizes object persistence\nbefore and after grasping. Our model is self-supervised in a simulation that\nonly uses basic objects of various colors and shapes but generalizes to novel\nobjects in new environments. To further facilitate generalization, we propose\ntwo adaptation methods, adversarial adaption and one-grasp adaptation.\nAdversarial adaptation regulates the image encoder using augmented data of\nunlabeled images, whereas one-grasp adaptation updates the overall end-to-end\nmodel using augmented data from one grasp trial. Both adaptation methods are\ndata-efficient and considerably improve instance grasping performance.\nExperimental results in both simulation and the real world demonstrate that our\napproach achieves over 81% instance grasping success rate on unknown objects,\nwhich outperforms several baselines by large margins.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Project page: https://z.umn.edu/attr-grasp. arXiv admin note:\n  substantial text overlap with arXiv:2104.02271",
    "pdf_url": "http://arxiv.org/pdf/2501.02149v1",
    "published_date": "2025-01-04 00:37:17 UTC",
    "updated_date": "2025-01-04 00:37:17 UTC"
  },
  {
    "arxiv_id": "2501.02146v2",
    "title": "Plasma-CycleGAN: Plasma Biomarker-Guided MRI to PET Cross-modality Translation Using Conditional CycleGAN",
    "authors": [
      "Yanxi Chen",
      "Yi Su",
      "Celine Dumitrascu",
      "Kewei Chen",
      "David Weidman",
      "Richard J Caselli",
      "Nicholas Ashton",
      "Eric M Reiman",
      "Yalin Wang"
    ],
    "abstract": "Cross-modality translation between MRI and PET imaging is challenging due to\nthe distinct mechanisms underlying these modalities. Blood-based biomarkers\n(BBBMs) are revolutionizing Alzheimer's disease (AD) detection by identifying\npatients and quantifying brain amyloid levels. However, the potential of BBBMs\nto enhance PET image synthesis remains unexplored. In this paper, we performed\na thorough study on the effect of incorporating BBBM into deep generative\nmodels. By evaluating three widely used cross-modality translation models, we\nfound that BBBMs integration consistently enhances the generative quality\nacross all models. By visual inspection of the generated results, we observed\nthat PET images generated by CycleGAN exhibit the best visual fidelity. Based\non these findings, we propose Plasma-CycleGAN, a novel generative model based\non CycleGAN, to synthesize PET images from MRI using BBBMs as conditions. This\nis the first approach to integrate BBBMs in conditional cross-modality\ntranslation between MRI and PET.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ISBI 2025",
    "pdf_url": "http://arxiv.org/pdf/2501.02146v2",
    "published_date": "2025-01-04 00:20:25 UTC",
    "updated_date": "2025-01-24 19:51:20 UTC"
  },
  {
    "arxiv_id": "2501.02144v1",
    "title": "Establishing baselines for generative discovery of inorganic crystals",
    "authors": [
      "Nathan J. Szymanski",
      "Christopher J. Bartel"
    ],
    "abstract": "Generative artificial intelligence offers a promising avenue for materials\ndiscovery, yet its advantages over traditional methods remain unclear. In this\nwork, we introduce and benchmark two baseline approaches - random enumeration\nof charge-balanced prototypes and data-driven ion exchange of known compounds -\nagainst three generative models: a variational autoencoder, a large language\nmodel, and a diffusion model. Our results show that established methods such as\nion exchange perform comparably well in generating stable materials, although\nmany of these materials tend to closely resemble known compounds. In contrast,\ngenerative models excel at proposing novel structural frameworks and, when\nsufficient training data is available, can more effectively target properties\nsuch as electronic band gap and bulk modulus while maintaining a high stability\nrate. To enhance the performance of both the baseline and generative\napproaches, we implement a post-generation screening step in which all proposed\nstructures are passed through stability and property filters from pre-trained\nmachine learning models including universal interatomic potentials. This\nlow-cost filtering step leads to substantial improvement in the success rates\nof all methods, remains computationally efficient, and ultimately provides a\npractical pathway toward more effective generative strategies for materials\ndiscovery.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "physics.chem-ph"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2501.02144v1",
    "published_date": "2025-01-04 00:14:59 UTC",
    "updated_date": "2025-01-04 00:14:59 UTC"
  }
]
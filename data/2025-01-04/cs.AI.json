{
  "date": "2025-01-04",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-01-04 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 和机器学习领域的创新应用，包括 LLM 在文本检测、医疗决策和机器人领域的潜力，以及一些基础算法优化和跨领域整合，令人印象深刻的包括 Markus J. Buehler 的 Transformer 改进和 LLM 在辐射肿瘤学中的评估，而其他论文则探讨了从数据安全到医疗图像处理的广泛话题。\n\n下面，我挑选并简要讨论了今天更重要的论文，先从 AI 和 LLM 相关的高话题度文章开始，然后过渡到医疗和机器人领域，最后快速掠过其他次要内容。每篇论文的标题以中文 + 英文形式列出，我会保留核心学术术语，并突出主要贡献和发现。\n\n### AI 和 LLM 相关论文\n- **零样本统计测试用于 LLM 生成文本检测 (Zero-Shot Statistical Tests for LLM-Generated Text Detection)**：这篇论文提出了一种基于有限样本集中不等式的零样本统计测试方法，用于区分不同 LLM 生成的文本或人类文本，主要贡献是通过证明错误率随文本长度指数下降，并实验验证在黑箱场景下平均 TPR 达 82.5%，显著提升文本来源鉴别 robustness，对 AI 安全和内容验证有重要启示。\n- **Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers**：作者 Markus J. Buehler 提出将图神经网络融入 Transformer 的注意力机制，形成 Graph-Aware Isomorphic Attention 和 Sparse GIN-Attention，主要发现是通过捕捉复杂依赖关系，提高模型泛化性和训练动态，适用于生物信息和材料科学等领域，展示了 Transformer 作为层次化图模型的新视角。\n- **探索大型语言模型在辐射肿瘤学决策支持中的能力和局限性 (Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support)**：论文评估 GPT-4 在辐射肿瘤学中的表现，如 ACR 考试准确率达 74.57%，主要贡献是揭示 LLM 在临床决策中的潜力，同时强调其局限性，如结构命名标准化准确率超 96%，为医疗 AI 应用提供 benchmark。\n- **UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude Mobility**：这篇论文探讨 LLM 与无人机的整合，主要发现是通过 LLM 增强 UAV 的自主感知和推理能力，提出 agentic UAV 路线图，并分析多模态数据资源，适用于交通和农业场景，推动低空机动系统的智能化。\n- **Evaluation of the Code Generation Capabilities of ChatGPT 4: A Comparative Analysis in 19 Programming Languages**：论文对 ChatGPT 4 的代码生成能力进行基准测试，主要贡献是揭示其在不同编程语言（如静态类型语言）的成功率（如39.67%整体成功率），并分析错误类型和效率，强调未来研究应扩展到调试和复杂代码。\n\n### 医疗和机器人相关论文\n- **Deep Learning-Driven Segmentation of Ischemic Stroke Lesions Using Multi-Channel MRI**：论文引入一个结合 DenseNet121 和 SelfONN 的网络，用于多通道 MRI（如 DWI 和 ADC）分割中风病变，主要发现是提升 Dice 系数至 87.49%，显著改善诊断精度和治疗规划。\n- **What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph**：论文提出 G-Prune 方法，通过图-based 语义相似性剪枝视觉 tokens，主要贡献是减少 MLLM 计算开销（如 FLOPs 降低 63.57%），并保持高准确率，适用于多模态任务。\n- **Table as Thought: Exploring Structured Thoughts in LLM Reasoning**：作者探索用表格结构组织 LLM 推理过程，主要发现是提升规划和数学任务性能，通过行-列 schema 实现更高效的认知模拟。\n\n### 其他快速掠过\n其余论文涉及较基础或应用性较弱的领域，我仅简要提及几篇有潜在价值的：\n- **Plasma-CycleGAN: Plasma Biomarker-Guided MRI to PET Cross-modality Translation Using Conditional CycleGAN**：使用 BBBM 指导 MRI 到 PET 转换，提高图像生成质量。\n- **A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges**：综述 VLM 的发展，提供模型列表和挑战分析。\n- 其他如土耳其语言处理（Context Aware Lemmatization and Morphological Tagging Method in Turkish）或工作场所 AI（Enhancing Workplace Productivity and Well-being Using AI Agent）等，贡献较局部，仅优化特定领域工具，细节可自行查阅。\n\n今天的 arXiv 更新展示了 AI 领域的快速迭代，LLM 在安全和应用中的潜力值得关注。如果你对特定主题感兴趣，建议直接查看相关论文的代码或实验链接！",
  "papers": [
    {
      "arxiv_id": "2501.02406v4",
      "title": "Zero-Shot Statistical Tests for LLM-Generated Text Detection using Finite Sample Concentration Inequalities",
      "title_zh": "翻译失败",
      "authors": [
        "Tara Radvand",
        "Mojtaba Abdolmaleki",
        "Mohamed Mostagir",
        "Ambuj Tewari"
      ],
      "abstract": "Verifying the provenance of content is crucial to the function of many\norganizations, e.g., educational institutions, social media platforms, firms,\netc. This problem is becoming increasingly challenging as text generated by\nLarge Language Models (LLMs) becomes almost indistinguishable from\nhuman-generated content. In addition, many institutions utilize in-house LLMs\nand want to ensure that external, non-sanctioned LLMs do not produce content\nwithin the institution. In this paper, we answer the following question: Given\na piece of text, can we identify whether it was produced by a particular LLM or\nnot? We model LLM-generated text as a sequential stochastic process with\ncomplete dependence on history. We then design zero-shot statistical tests to\n(i) distinguish between text generated by two different known sets of LLMs $A$\n(non-sanctioned) and $B$ (in-house), and (ii) identify whether text was\ngenerated by a known LLM or generated by any unknown model, e.g., a human or\nsome other language generation process. We prove that the type I and type II\nerrors of our test decrease exponentially with the length of the text. For\nthat, we show that if $B$ generates the text, then except with an exponentially\nsmall probability in string length, the log-perplexity of the string under $A$\nconverges to the average cross-entropy of $B$ and $A$. We then present\nexperiments using LLMs with white-box access to support our theoretical results\nand empirically examine the robustness of our results to black-box settings and\nadversarial attacks. In the black-box setting, our method achieves an average\nTPR of 82.5\\% at a fixed FPR of 5\\%. Under adversarial perturbations, our\nminimum TPR is 48.6\\% at the same FPR threshold. Both results outperform all\nnon-commercial baselines. See\nhttps://github.com/TaraRadvand74/llm-text-detection for code, data, and an\nonline demo of the project.",
      "tldr_zh": "这篇论文提出了一种零-shot 统计测试方法，用于检测 Large Language Models (LLMs) 生成的文本，基于 Finite Sample Concentration Inequalities，将文本建模为依赖历史的顺序随机过程。该方法能区分两个已知 LLM 集（如非授权模型 A 和内部模型 B）生成的文本，以及识别文本是否来自已知 LLM 或未知来源（如人类或其他过程）。论文证明了测试的 I 型和 II 型错误随文本长度呈指数下降，并通过理论分析显示，文本在 A 下的 log-perplexity 会收敛到 B 和 A 的平均交叉熵。实验结果表明，在黑箱设置中，该方法平均 TPR 达到 82.5%（FPR 为 5%），并在对抗攻击下保持最低 TPR 为 48.6%，均优于非商业基线。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.CL",
        "cs.IT",
        "cs.LG",
        "math.IT"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02406v4",
      "published_date": "2025-01-04 23:51:43 UTC",
      "updated_date": "2025-05-16 15:45:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:24:52.607935"
    },
    {
      "arxiv_id": "2501.02401v1",
      "title": "iTARGET: Interpretable Tailored Age Regression for Grouped Epigenetic Traits",
      "title_zh": "翻译失败",
      "authors": [
        "Zipeng Wu",
        "Daniel Herring",
        "Fabian Spill",
        "James Andrews"
      ],
      "abstract": "Accurately predicting chronological age from DNA methylation patterns is\ncrucial for advancing biological age estimation. However, this task is made\nchallenging by Epigenetic Correlation Drift (ECD) and Heterogeneity Among CpGs\n(HAC), which reflect the dynamic relationship between methylation and age\nacross different life stages. To address these issues, we propose a novel\ntwo-phase algorithm. The first phase employs similarity searching to cluster\nmethylation profiles by age group, while the second phase uses Explainable\nBoosting Machines (EBM) for precise, group-specific prediction. Our method not\nonly improves prediction accuracy but also reveals key age-related CpG sites,\ndetects age-specific changes in aging rates, and identifies pairwise\ninteractions between CpG sites. Experimental results show that our approach\noutperforms traditional epigenetic clocks and machine learning models, offering\na more accurate and interpretable solution for biological age estimation with\nsignificant implications for aging research.",
      "tldr_zh": "该研究提出 iTARGET，一种可解释的定制年龄回归方法，用于处理 DNA 甲基化模式中 Epigenetic Correlation Drift (ECD) 和 Heterogeneity Among CpGs (HAC) 等挑战，以提升生物年龄估计的准确性。方法采用两阶段算法：第一阶段通过相似性搜索对甲基化配置文件按年龄组进行聚类；第二阶段利用 Explainable Boosting Machines (EBM) 实现组特定预测。iTARGET 不仅提高了预测精度，还揭示了关键的年龄相关 CpG sites、检测了年龄特定的衰老速率变化，并识别了 CpG sites 之间的配对交互，在实验中优于传统表观遗传钟和机器学习模型，对衰老研究具有重要意义。",
      "categories": [
        "q-bio.GN",
        "cs.AI",
        "62P10, 92D20, 92D10",
        "I.5.4; J.3; I.2.6"
      ],
      "primary_category": "q-bio.GN",
      "comment": "To be published in IEEE BIBM 2024.The manuscript includes a\n  comprehensive description of the methodology and comparison with traditional\n  epigenetic clocks and machine learning models. Submitted to arXiv as part of\n  ongoing research in epigenetics and aging studies",
      "pdf_url": "http://arxiv.org/pdf/2501.02401v1",
      "published_date": "2025-01-04 23:06:46 UTC",
      "updated_date": "2025-01-04 23:06:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:25:03.554683"
    },
    {
      "arxiv_id": "2501.02393v3",
      "title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Markus J. Buehler"
      ],
      "abstract": "We present an approach to modifying Transformer architectures by integrating\ngraph-aware relational reasoning into the attention mechanism, merging concepts\nfrom graph neural networks and language modeling. Building on the inherent\nconnection between attention and graph theory, we reformulate the Transformer's\nattention mechanism as a graph operation and propose Graph-Aware Isomorphic\nAttention. This method leverages advanced graph modeling strategies, including\nGraph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA),\nto enrich the representation of relational structures. Our approach captures\ncomplex dependencies and generalizes across tasks, as evidenced by a reduced\ngeneralization gap and improved learning performance. Additionally, we expand\nthe concept of graph-aware attention to introduce Sparse GIN-Attention, a\nfine-tuning approach that employs sparse GINs. By interpreting attention\nmatrices as sparse adjacency graphs, this technique enhances the adaptability\nof pre-trained foundational models with minimal computational overhead,\nendowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning\nachieves improved training dynamics and better generalization compared to\nalternative methods like low-rank adaption (LoRA). We discuss latent graph-like\nstructures within traditional attention mechanisms, offering a new lens through\nwhich Transformers can be understood. By evolving Transformers as hierarchical\nGIN models for relational reasoning. This perspective suggests profound\nimplications for foundational model development, enabling the design of\narchitectures that dynamically adapt to both local and global dependencies.\nApplications in bioinformatics, materials science, language modeling, and\nbeyond could benefit from this synthesis of relational and sequential data\nmodeling, setting the stage for interpretable and generalizable modeling\nstrategies.",
      "tldr_zh": "本研究提出Graph-Aware Isomorphic Attention，一种将图神经网络概念（如Graph Isomorphism Networks (GIN)和Principal Neighborhood Aggregation (PNA)）整合到Transformer架构的注意力机制中，通过将注意力机制重新表述为图操作，捕捉复杂关系依赖并提升模型泛化能力。相比传统方法，该方法在实验中减少了泛化差距并显著提高了学习性能。研究进一步扩展到Sparse GIN-Attention，这是一种微调技术，将注意力矩阵视为稀疏邻接图，增强预训练模型的适应性，同时比low-rank adaptation (LoRA)方法提供更好的训练动态和泛化效果。作者讨论了传统注意力机制中的潜在图结构，将Transformers视为层次化的GIN模型，为基础模型开发提供新视角。最终，这种方法适用于生物信息学、材料科学和语言建模等领域，促进可解释和可泛化的建模策略。",
      "categories": [
        "cs.LG",
        "cond-mat.mes-hall",
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02393v3",
      "published_date": "2025-01-04 22:30:21 UTC",
      "updated_date": "2025-03-05 13:19:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:25:16.536726"
    },
    {
      "arxiv_id": "2501.02392v1",
      "title": "Syntactic Evolution in Language Usage",
      "title_zh": "语言使用中的句法演变",
      "authors": [
        "Surbhit Kumar"
      ],
      "abstract": "This research aims to investigate the dynamic nature of linguistic style\nthroughout various stages of life, from post teenage to old age. By employing\nlinguistic analysis tools and methodologies, the study will delve into the\nintricacies of how individuals adapt and modify their language use over time.\nThe research uses a data set of blogs from blogger.com from 2004 and focuses on\nEnglish for syntactic analysis. The findings of this research can have\nimplications for linguistics, psychology, and communication studies, shedding\nlight on the intricate relationship between age and language.",
      "tldr_zh": "这项研究探讨了语言风格在人生不同阶段（如从青少年后到老年）的动态演变，焦点在于个体如何随时间适应和修改语言使用。\n研究采用语言分析工具，对2004年blogger.com的英语博客数据集进行句法(syntactic)分析，揭示年龄对语言模式的影响。\n这些发现为语言学、心理学和传播研究提供了新见解，强调了年龄与语言之间复杂的关系。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "4 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.02392v1",
      "published_date": "2025-01-04 22:27:24 UTC",
      "updated_date": "2025-01-04 22:27:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:25:27.124608"
    },
    {
      "arxiv_id": "2501.02368v1",
      "title": "Enhancing Workplace Productivity and Well-being Using AI Agent",
      "title_zh": "翻译失败",
      "authors": [
        "Ravirajan K",
        "Arvind Sundarajan"
      ],
      "abstract": "This paper discusses the use of Artificial Intelligence (AI) to enhance\nworkplace productivity and employee well-being. By integrating machine learning\n(ML) techniques with neurobiological data, the proposed approaches ensure\nalignment with human ethical standards through value alignment models and\nHierarchical Reinforcement Learning (HRL) for autonomous task management. The\nsystem utilizes biometric feedback from employees to generate personalized\nhealth prompts, fostering a supportive work environment that encourages\nphysical activity. Additionally, we explore decentralized multi-agent systems\nfor improved collaboration and decision-making frameworks that enhance\ntransparency. Various approaches using ML techniques in conjunction with AI\nimplementations are discussed. Together, these innovations aim to create a more\nproductive and health-conscious workplace. These outcomes assist HR management\nand organizations in launching more rational career progression streams for\nemployees and facilitating organizational transformation.",
      "tldr_zh": "本研究探讨了使用AI代理提升工作场所生产力和员工福祉的方法，通过整合机器学习(ML)技术与神经生物学数据，确保系统符合人类伦理标准，并采用价值对齐模型和分层强化学习(HRL)进行自主任务管理。系统利用员工生物识别反馈生成个性化健康提示，促进支持性工作环境和身体活动，同时引入去中心化多智能体系统以改善协作和决策透明度。这些创新结合ML与AI的多种方法，旨在创建更高效、健康的工作场所，并帮助HR管理和组织优化员工职业发展路径。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02368v1",
      "published_date": "2025-01-04 20:11:00 UTC",
      "updated_date": "2025-01-04 20:11:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:25:39.041851"
    },
    {
      "arxiv_id": "2501.02361v1",
      "title": "Context Aware Lemmatization and Morphological Tagging Method in Turkish",
      "title_zh": "土耳其语中的上下文感知词形还原和形态标记方法",
      "authors": [
        "Cagri Sayallar"
      ],
      "abstract": "The smallest part of a word that defines the word is called a word root. Word\nroots are used to increase success in many applications since they simplify the\nword. In this study, the lemmatization model, which is a word root finding\nmethod, and the morphological tagging model, which predicts the grammatical\nknowledge of the word, are presented. The presented model was developed for\nTurkish, and both models make predictions by taking the meaning of the word\ninto account. In the literature, there is no lemmatization study that is\nsensitive to word meaning in Turkish. For this reason, the present study shares\nthe model and the results obtained from the model on Turkish lemmatization for\nthe first time in the literature. In the present study, in the lemmatization\nand morphological tagging models, bidirectional LSTM is used for the spelling\nof words, and the Turkish BERT model is used for the meaning of words. The\nmodels are trained using the IMST and PUD datasets from Universal Dependencies.\nThe results from the training of the models were compared with the results from\nthe SIGMORPHON 2019 competition. The results of the comparisons revealed that\nour models were superior.",
      "tldr_zh": "本研究提出了一种考虑上下文的lemmatization和morphological tagging方法，针对土耳其语，首次将单词含义纳入lemmatization模型中，以简化词根提取并预测单词的语法知识。方法结合bidirectional LSTM处理单词拼写，以及Turkish BERT模型捕捉单词语义。模型使用IMST和PUD数据集从Universal Dependencies进行训练，并与SIGMORPHON 2019比赛结果比较，显示出优越性能。这为土耳其语自然语言处理应用提供了新的基准。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T07, 68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02361v1",
      "published_date": "2025-01-04 19:12:43 UTC",
      "updated_date": "2025-01-04 19:12:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:25:51.691734"
    },
    {
      "arxiv_id": "2501.02352v1",
      "title": "GNSS/GPS Spoofing and Jamming Identification Using Machine Learning and Deep Learning",
      "title_zh": "利用机器学习和深度学习进行 GNSS/GPS 欺骗和干扰识别",
      "authors": [
        "Ali Ghanbarzade",
        "Hossein Soleimani"
      ],
      "abstract": "The increasing reliance on Global Navigation Satellite Systems (GNSS),\nparticularly the Global Positioning System (GPS), underscores the urgent need\nto safeguard these technologies against malicious threats such as spoofing and\njamming. As the backbone for positioning, navigation, and timing (PNT) across\nvarious applications including transportation, telecommunications, and\nemergency services GNSS is vulnerable to deliberate interference that poses\nsignificant risks. Spoofing attacks, which involve transmitting counterfeit\nGNSS signals to mislead receivers into calculating incorrect positions, can\nresult in serious consequences, from navigational errors in civilian aviation\nto security breaches in military operations. Furthermore, the lack of inherent\nsecurity measures within GNSS systems makes them attractive targets for\nadversaries. While GNSS/GPS jamming and spoofing systems consist of numerous\ncomponents, the ability to distinguish authentic signals from malicious ones is\nessential for maintaining system integrity. Recent advancements in machine\nlearning and deep learning provide promising avenues for enhancing detection\nand mitigation strategies against these threats. This paper addresses both\nspoofing and jamming by tackling real-world challenges through machine\nlearning, deep learning, and computer vision techniques. Through extensive\nexperiments on two real-world datasets related to spoofing and jamming\ndetection using advanced algorithms, we achieved state of the art results. In\nthe GNSS/GPS jamming detection task, we attained approximately 99% accuracy,\nimproving performance by around 5% compared to previous studies. Additionally,\nwe addressed a challenging tasks related to spoofing detection, yielding\nresults that underscore the potential of machine learning and deep learning in\nthis domain.",
      "tldr_zh": "这篇论文探讨了使用机器学习和深度学习技术来识别 GNSS/GPS 的欺骗(spoofing)和干扰(jamming)攻击，以应对这些威胁对定位、导航和定时(PNT)系统的影响。研究通过机器学习、深度学习和计算机视觉方法，对两个真实数据集进行广泛实验，实现了 state-of-the-art 结果。实验表明，在 GNSS/GPS jamming 检测任务中，准确率达到约99%，比之前研究提高了5%；在 spoofing 检测中，也取得了显著的潜在应用价值。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02352v1",
      "published_date": "2025-01-04 18:14:43 UTC",
      "updated_date": "2025-01-04 18:14:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:26:03.878632"
    },
    {
      "arxiv_id": "2501.02346v1",
      "title": "Exploring the Capabilities and Limitations of Large Language Models for Radiation Oncology Decision Support",
      "title_zh": "探索大型语言模型在放射肿瘤学决策支持中的能力与局限性",
      "authors": [
        "Florian Putz",
        "Marlen Haderleina",
        "Sebastian Lettmaier",
        "Sabine Semrau",
        "Rainer Fietkau",
        "Yixing Huang"
      ],
      "abstract": "Thanks to the rapidly evolving integration of LLMs into decision-support\ntools, a significant transformation is happening across large-scale systems.\nLike other medical fields, the use of LLMs such as GPT-4 is gaining increasing\ninterest in radiation oncology as well. An attempt to assess GPT-4's\nperformance in radiation oncology was made via a dedicated 100-question\nexamination on the highly specialized topic of radiation oncology physics,\nrevealing GPT-4's superiority over other LLMs. GPT-4's performance on a broader\nfield of clinical radiation oncology is further benchmarked by the ACR\nRadiation Oncology In-Training (TXIT) exam where GPT-4 achieved a high accuracy\nof 74.57%. Its performance on re-labelling structure names in accordance with\nthe AAPM TG-263 report has also been benchmarked, achieving above 96%\naccuracies. Such studies shed light on the potential of LLMs in radiation\noncology. As interest in the potential and constraints of LLMs in general\nhealthcare applications continues to rise5, the capabilities and limitations of\nLLMs in radiation oncology decision support have not yet been fully explored.",
      "tldr_zh": "本研究探讨了大型语言模型(LLMs)如 GPT-4 在放射肿瘤学决策支持中的能力和限制，通过评估其在特定基准测试中的表现。研究使用一个100题的放射肿瘤学物理考试来比较GPT-4与其他LLMs，结果显示GPT-4表现出色。在ACR Radiation Oncology In-Training (TXIT) 考试中，GPT-4 达到了74.57%的准确率，并在根据AAPM TG-263报告重新标记结构名称的任务中超过了96%的准确率。这些发现突显了LLMs在放射肿瘤学领域的潜力，但也强调了其限制尚未完全探索，需要进一步研究。",
      "categories": [
        "physics.med-ph",
        "cs.AI"
      ],
      "primary_category": "physics.med-ph",
      "comment": "Officially published in the Red Journal",
      "pdf_url": "http://arxiv.org/pdf/2501.02346v1",
      "published_date": "2025-01-04 17:57:33 UTC",
      "updated_date": "2025-01-04 17:57:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:26:16.284726"
    },
    {
      "arxiv_id": "2501.02342v1",
      "title": "Optimizing Small Language Models for In-Vehicle Function-Calling",
      "title_zh": "针对车载函数调用的小型语言模型优化",
      "authors": [
        "Yahya Sowti Khiabani",
        "Farris Atif",
        "Chieh Hsu",
        "Sven Stahlmann",
        "Tobias Michels",
        "Sebastian Kramer",
        "Benedikt Heidrich",
        "M. Saquib Sarfraz",
        "Julian Merten",
        "Faezeh Tafazzoli"
      ],
      "abstract": "We propose a holistic approach for deploying Small Language Models (SLMs) as\nfunction-calling agents within vehicles as edge devices, offering a more\nflexible and robust alternative to traditional rule-based systems. By\nleveraging SLMs, we simplify vehicle control mechanisms and enhance the user\nexperience. Given the in-vehicle hardware constraints, we apply\nstate-of-the-art model compression techniques, including structured pruning,\nhealing, and quantization, ensuring that the model fits within the resource\nlimitations while maintaining acceptable performance. Our work focuses on\noptimizing a representative SLM, Microsoft's Phi-3 mini, and outlines best\npractices for enabling embedded models, including compression, task-specific\nfine-tuning, and vehicle integration. We demonstrate that, despite significant\nreduction in model size which removes up to 2 billion parameters from the\noriginal model, our approach preserves the model's ability to handle complex\nin-vehicle tasks accurately and efficiently. Furthermore, by executing the\nmodel in a lightweight runtime environment, we achieve a generation speed of 11\ntokens per second, making real-time, on-device inference feasible without\nhardware acceleration. Our results demonstrate the potential of SLMs to\ntransform vehicle control systems, enabling more intuitive interactions between\nusers and their vehicles for an enhanced driving experience.",
      "tldr_zh": "该研究提出了一种整体方法，使用 Small Language Models (SLMs) 作为车辆边缘设备的函数-calling 代理，取代传统的规则-based 系统，以提升灵活性、鲁棒性和用户体验。针对车辆硬件限制，他们应用结构化 pruning、healing 和 quantization 等模型压缩技术，对 Microsoft's Phi-3 mini 进行优化，包括任务特定微调和车辆集成，确保模型在减少多达 2 亿参数后仍能高效处理复杂任务。实验结果显示，该方法实现了 11 tokens per second 的实时本地推理，无需硬件加速，并展示了 SLMs 在转变车辆控制系统、实现更直观的用户交互方面的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02342v1",
      "published_date": "2025-01-04 17:32:56 UTC",
      "updated_date": "2025-01-04 17:32:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:26:28.571975"
    },
    {
      "arxiv_id": "2501.02341v2",
      "title": "UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude Mobility",
      "title_zh": "翻译失败",
      "authors": [
        "Yonglin Tian",
        "Fei Lin",
        "Yiduo Li",
        "Tengchao Zhang",
        "Qiyao Zhang",
        "Xuan Fu",
        "Jun Huang",
        "Xingyuan Dai",
        "Yutong Wang",
        "Chunwei Tian",
        "Bai Li",
        "Yisheng Lv",
        "Levente Kovács",
        "Fei-Yue Wang"
      ],
      "abstract": "Low-altitude mobility, exemplified by unmanned aerial vehicles (UAVs), has\nintroduced transformative advancements across various domains, like\ntransportation, logistics, and agriculture. Leveraging flexible perspectives\nand rapid maneuverability, UAVs extend traditional systems' perception and\naction capabilities, garnering widespread attention from academia and industry.\nHowever, current UAV operations primarily depend on human control, with only\nlimited autonomy in simple scenarios, and lack the intelligence and\nadaptability needed for more complex environments and tasks. The emergence of\nlarge language models (LLMs) demonstrates remarkable problem-solving and\ngeneralization capabilities, offering a promising pathway for advancing UAV\nintelligence. This paper explores the integration of LLMs and UAVs, beginning\nwith an overview of UAV systems' fundamental components and functionalities,\nfollowed by an overview of the state-of-the-art in LLM technology.\nSubsequently, it systematically highlights the multimodal data resources\navailable for UAVs, which provide critical support for training and evaluation.\nFurthermore, it categorizes and analyzes key tasks and application scenarios\nwhere UAVs and LLMs converge. Finally, a reference roadmap towards agentic UAVs\nis proposed, aiming to enable UAVs to achieve agentic intelligence through\nautonomous perception, memory, reasoning, and tool utilization. Related\nresources are available at https://github.com/Hub-Tian/UAVs_Meet_LLMs.",
      "tldr_zh": "这篇论文探讨了无人机（UAVs）和大型语言模型（LLMs）的整合，以提升低空机动性的智能性和自主性。论文首先概述了UAVs的基本组件和功能，以及LLMs的最新技术状态，并讨论了支持UAVs训练与评估的多模态数据资源。随后，它分析了UAVs与LLMs在关键任务和应用场景中的融合潜力。最终，论文提出一个通往代理智能（agentic intelligence）的路线图，旨在让UAVs通过自主感知、记忆、推理和工具利用实现更高级别的适应性操作。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02341v2",
      "published_date": "2025-01-04 17:32:12 UTC",
      "updated_date": "2025-03-25 15:55:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:26:40.246237"
    },
    {
      "arxiv_id": "2501.02338v1",
      "title": "Evaluation of the Code Generation Capabilities of ChatGPT 4: A Comparative Analysis in 19 Programming Languages",
      "title_zh": "ChatGPT 4 的代码生成能力评估：在 19 种编程语言中的比较分析",
      "authors": [
        "L. C. Gilbert"
      ],
      "abstract": "This bachelor's thesis examines the capabilities of ChatGPT 4 in code\ngeneration across 19 programming languages. The study analyzed solution rates\nacross three difficulty levels, types of errors encountered, and code quality\nin terms of runtime and memory efficiency through a quantitative experiment. A\ntotal of 188 programming problems were selected from the LeetCode platform, and\nChatGPT 4 was given three attempts to produce a correct solution with feedback.\nChatGPT 4 successfully solved 39.67% of all tasks, with success rates\ndecreasing significantly as problem complexity increased. Notably, the model\nfaced considerable challenges with hard problems across all languages. ChatGPT\n4 demonstrated higher competence in widely used languages, likely due to a\nlarger volume and higher quality of training data. The solution rates also\nrevealed a preference for languages with low abstraction levels and static\ntyping. For popular languages, the most frequent error was \"Wrong Answer,\"\nwhereas for less popular languages, compiler and runtime errors prevailed,\nsuggesting frequent misunderstandings and confusion regarding the structural\ncharacteristics of these languages. The model exhibited above-average runtime\nefficiency in all programming languages, showing a tendency toward statically\ntyped and low-abstraction languages. Memory efficiency results varied\nsignificantly, with above-average performance in 14 languages and below-average\nperformance in five languages. A slight preference for low-abstraction\nlanguages and a leaning toward dynamically typed languages in terms of memory\nefficiency were observed. Future research should include a larger number of\ntasks, iterations, and less popular languages. Additionally, ChatGPT 4's\nabilities in code interpretation and summarization, debugging, and the\ndevelopment of complex, practical code could be analyzed further.\n  ----\n  Diese Bachelorarbeit untersucht die F\\\"ahigkeiten von ChatGPT 4 zur\nCode-Generierung in 19 Programmiersprachen. Betrachtet wurden die\nL\\\"osungsraten zwischen drei Schwierigkeitsgraden, die aufgetretenen\nFehlerarten und die Qualit\\\"at des Codes hinsichtlich der Laufzeit- und\nSpeichereffizienz in einem quantitativen Experiment. Dabei wurden 188\nProgrammierprobleme der Plattform LeetCode entnommen, wobei ChatGPT 4 jeweils\ndrei Versuche hatte, mittels Feedback eine korrekte L\\\"osung zu generieren.\nChatGPT 4 l\\\"oste 39,67 % aller Aufgaben erfolgreich, wobei die Erfolgsrate mit\nzunehmendem Schwierigkeitsgrad deutlich abnahm und bei komplexen Problemen in\nallen Sprachen signifikante Schwierigkeiten auftraten. Das Modell zeigte eine\nh\\\"ohere Kompetenz in weit verbreiteten Sprachen, was wahrscheinlich auf eine\ngr\\\"o{\\ss}ere Menge und h\\\"ohere Qualit\\\"at der Trainingsdaten\nzur\\\"uckzuf\\\"uhren ist. Bez\\\"uglich der L\\\"osungsraten zeigte das Modell zudem\neine Pr\\\"aferenz f\\\"ur Sprachen mit niedrigem Abstraktionsniveau und statischer\nTypisierung. Bei Sprachen hoher Popularit\\\"at trat der Fehler Wrong Answer am\nh\\\"aufigsten auf, w\\\"ahrend bei weniger popul\\\"aren Sprachen Compiler- und\nLaufzeitfehler \\\"uberwogen, was auf h\\\"aufige Missverst\\\"andnisse und\nVerwechslungen bez\\\"uglich der spezifischen strukturellen Eigenschaften dieser\nSprachen zur\\\"uckzuf\\\"uhren ist. ChatGPT 4 demonstrierte in allen\nProgrammiersprachen eine \\\"uberdurchschnittliche Laufzeiteffizienz und\ntendierte diesbez\\\"uglich erneut zu statisch typisierten und niedrig\nabstrahierten Sprachen. Die Werte zur Speichereffizienz variierten erheblich,\nwobei in 14 Sprachen \\\"uberdurchschnittliche und in f\\\"unf Sprachen\nunterdurchschnittliche Werte erzielt wurden. Es zeigte sich diesbez\\\"uglich\neine leichte Tendenz zugunsten von niedrig abstrahierten sowie eine Pr\\\"aferenz\nzu dynamisch typisierten Sprachen. Zuk\\\"unftige Forschung sollte eine h\\\"ohere\nAnzahl an Aufgaben, Iterationen und unpopul\\\"aren Sprachen einbeziehen.\nDar\\\"uber hinaus k\\\"onnten die F\\\"ahigkeiten von ChatGPT 4 in der\nCode-Interpretation und -Zusammenfassung, im Debugging und in der Entwicklung\nkomplexer, praxisbezogener Codes analysiert werden.",
      "tldr_zh": "这篇论文评估了 ChatGPT 4 在 19 种编程语言中的代码生成能力，通过对 LeetCode 平台的 188 个编程问题的定量实验，分析了解决方案率、错误类型（如 \"Wrong Answer\"）以及代码质量（包括运行时和内存效率）。结果显示，ChatGPT 4 成功解决了 39.67% 的任务，成功率随问题难度增加而下降，并在流行语言中表现更佳，可能归因于训练数据的质量和数量；同时，它更偏好低抽象水平和静态类型语言。实验还发现，该模型在运行时效率上普遍高于平均，但在内存效率上表现出变异，并建议未来研究增加任务数量、迭代次数以及对不流行语言和功能（如代码解释、调试）的进一步分析。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "65 pages, in German, Bachelor's thesis on the evaluation of ChatGPT\n  4's code generation capabilities in 19 programming languages, University of\n  Potsdam, June 2024",
      "pdf_url": "http://arxiv.org/pdf/2501.02338v1",
      "published_date": "2025-01-04 17:17:01 UTC",
      "updated_date": "2025-01-04 17:17:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:26:52.845469"
    },
    {
      "arxiv_id": "2501.02336v1",
      "title": "AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Zhuomin He",
        "Yizhen Yao",
        "Pengfei Zuo",
        "Bin Gao",
        "Qinya Li",
        "Zhenzhe Zheng",
        "Fan Wu"
      ],
      "abstract": "Long-context large language models (LLMs) inference is increasingly critical,\nmotivating a number of studies devoted to alleviating the substantial storage\nand computational costs in such scenarios. Layer-wise skipping methods are\npromising optimizations but rarely explored in long-context inference. We\nobserve that existing layer-wise skipping strategies have several limitations\nwhen applied in long-context inference, including the inability to adapt to\nmodel and context variability, disregard for sublayer significance, and\ninapplicability for the prefilling phase. This paper proposes \\sysname, an\nadaptive sublayer skipping method specifically designed for long-context\ninference. \\sysname adaptively identifies less important layers by leveraging\non-the-fly similarity information, enables sublayer-wise skipping, and\naccelerates both the prefilling and decoding phases. The effectiveness of\n\\sysname is demonstrated through extensive experiments on various long-context\nbenchmarks and models, showcasing its superior inference performance over\nexisting baselines.",
      "tldr_zh": "本文研究了长上下文大语言模型 (LLMs) 推理中的存储和计算成本问题，提出了一种自适应子层跳过方法 AdaSkip，以解决现有层级跳过策略的局限性，如无法适应模型和上下文变化、忽略子层重要性以及在预填充阶段的不适用。AdaSkip 通过实时相似性信息动态识别不重要子层，支持子层级跳过，并加速预填充和解码阶段。实验在多种长上下文基准和模型上验证了 AdaSkip 的有效性，其推理性能明显优于现有基线。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages,10 figures, AAAI",
      "pdf_url": "http://arxiv.org/pdf/2501.02336v1",
      "published_date": "2025-01-04 17:01:30 UTC",
      "updated_date": "2025-01-04 17:01:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:27:03.588618"
    },
    {
      "arxiv_id": "2501.02334v1",
      "title": "Validity Arguments For Constructed Response Scoring Using Generative Artificial Intelligence Applications",
      "title_zh": "构建响应评分使用生成式人工智能应用的有效性论证",
      "authors": [
        "Jodi M. Casabianca",
        "Daniel F. McCaffrey",
        "Matthew S. Johnson",
        "Naim Alper",
        "Vladimir Zubenko"
      ],
      "abstract": "The rapid advancements in large language models and generative artificial\nintelligence (AI) capabilities are making their broad application in the\nhigh-stakes testing context more likely. Use of generative AI in the scoring of\nconstructed responses is particularly appealing because it reduces the effort\nrequired for handcrafting features in traditional AI scoring and might even\noutperform those methods. The purpose of this paper is to highlight the\ndifferences in the feature-based and generative AI applications in constructed\nresponse scoring systems and propose a set of best practices for the collection\nof validity evidence to support the use and interpretation of constructed\nresponse scores from scoring systems using generative AI. We compare the\nvalidity evidence needed in scoring systems using human ratings, feature-based\nnatural language processing AI scoring engines, and generative AI. The evidence\nneeded in the generative AI context is more extensive than in the feature-based\nNLP scoring context because of the lack of transparency and other concerns\nunique to generative AI such as consistency. Constructed response score data\nfrom standardized tests demonstrate the collection of validity evidence for\ndifferent types of scoring systems and highlights the numerous complexities and\nconsiderations when making a validity argument for these scores. In addition,\nwe discuss how the evaluation of AI scores might include a consideration of how\na contributory scoring approach combining multiple AI scores (from different\nsources) will cover more of the construct in the absence of human ratings.",
      "tldr_zh": "这篇论文探讨了使用生成式AI（generative AI）对结构化响应（constructed responses）进行评分的最佳实践，旨在为高风险测试环境提供有效性证据。论文比较了人类评分、基于特征的自然语言处理（NLP）AI评分和生成式AI评分系统，强调生成式AI由于透明度不足和一致性问题，需要更广泛的验证证据。最终，通过标准化测试数据展示证据收集的复杂性，并提出结合多个AI来源的评分方法，以在 absence of human ratings 的情况下更全面覆盖评估结构（construct）。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "33 pages, 2 figures, 6 tables; This work was presented at the 2024\n  meeting of the International Testing Commission in Granada, Spain",
      "pdf_url": "http://arxiv.org/pdf/2501.02334v1",
      "published_date": "2025-01-04 16:59:29 UTC",
      "updated_date": "2025-01-04 16:59:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:27:16.118830"
    },
    {
      "arxiv_id": "2501.02330v2",
      "title": "SR-Reward: Taking The Path More Traveled",
      "title_zh": "SR-Reward：走更常走的道路",
      "authors": [
        "Seyed Mahdi B. Azad",
        "Zahra Padar",
        "Gabriel Kalweit",
        "Joschka Boedecker"
      ],
      "abstract": "In this paper, we propose a novel method for learning reward functions\ndirectly from offline demonstrations. Unlike traditional inverse reinforcement\nlearning (IRL), our approach decouples the reward function from the learner's\npolicy, eliminating the adversarial interaction typically required between the\ntwo. This results in a more stable and efficient training process. Our reward\nfunction, called \\textit{SR-Reward}, leverages successor representation (SR) to\nencode a state based on expected future states' visitation under the\ndemonstration policy and transition dynamics. By utilizing the Bellman\nequation, SR-Reward can be learned concurrently with most reinforcement\nlearning (RL) algorithms without altering the existing training pipeline. We\nalso introduce a negative sampling strategy to mitigate overestimation errors\nby reducing rewards for out-of-distribution data, thereby enhancing robustness.\nThis strategy inherently introduces a conservative bias into RL algorithms that\nemploy the learned reward. We evaluate our method on the D4RL benchmark,\nachieving competitive results compared to offline RL algorithms with access to\ntrue rewards and imitation learning (IL) techniques like behavioral cloning.\nMoreover, our ablation studies on data size and quality reveal the advantages\nand limitations of SR-Reward as a proxy for true rewards.",
      "tldr_zh": "本文提出SR-Reward方法，从离线演示中直接学习奖励函数，与传统Inverse Reinforcement Learning (IRL)不同，它解耦奖励函数和学习者策略，避免对抗交互，从而提升训练的稳定性和效率。SR-Reward利用Successor Representation (SR)和Bellman方程编码状态，基于演示策略和转移动态，同时引入负采样策略来减轻过估计错误并增强鲁棒性。该方法在D4RL基准上与有真实奖励的离线Reinforcement Learning (RL)算法和Imitation Learning (IL)技术相比，取得了竞争性结果，并通过消融研究揭示了其作为真实奖励代理的优势和局限性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02330v2",
      "published_date": "2025-01-04 16:21:10 UTC",
      "updated_date": "2025-04-29 13:02:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:27:28.675519"
    },
    {
      "arxiv_id": "2501.02313v1",
      "title": "DiffGraph: Heterogeneous Graph Diffusion Model",
      "title_zh": "DiffGraph: 异构图扩散模型",
      "authors": [
        "Zongwei Li",
        "Lianghao Xia",
        "Hua Hua",
        "Shijie Zhang",
        "Shuangyang Wang",
        "Chao Huang"
      ],
      "abstract": "Recent advances in Graph Neural Networks (GNNs) have revolutionized\ngraph-structured data modeling, yet traditional GNNs struggle with complex\nheterogeneous structures prevalent in real-world scenarios. Despite progress in\nhandling heterogeneous interactions, two fundamental challenges persist: noisy\ndata significantly compromising embedding quality and learning performance, and\nexisting methods' inability to capture intricate semantic transitions among\nheterogeneous relations, which impacts downstream predictions. To address these\nfundamental issues, we present the Heterogeneous Graph Diffusion Model\n(DiffGraph), a pioneering framework that introduces an innovative cross-view\ndenoising strategy. This advanced approach transforms auxiliary heterogeneous\ndata into target semantic spaces, enabling precise distillation of\ntask-relevant information. At its core, DiffGraph features a sophisticated\nlatent heterogeneous graph diffusion mechanism, implementing a novel forward\nand backward diffusion process for superior noise management. This methodology\nachieves simultaneous heterogeneous graph denoising and cross-type transition,\nwhile significantly simplifying graph generation through its latent-space\ndiffusion capabilities. Through rigorous experimental validation on both public\nand industrial datasets, we demonstrate that DiffGraph consistently surpasses\nexisting methods in link prediction and node classification tasks, establishing\nnew benchmarks for robustness and efficiency in heterogeneous graph processing.\nThe model implementation is publicly available at:\nhttps://github.com/HKUDS/DiffGraph.",
      "tldr_zh": "该研究针对传统图神经网络(GNNs)在处理异构图时面临的噪音数据问题和语义转换挑战，提出了一种创新框架DiffGraph：Heterogeneous Graph Diffusion Model。DiffGraph引入跨视图去噪策略，将辅助异构数据转化为目标语义空间，以精确提炼任务相关信息；同时，通过潜在异构图扩散机制实现前向和后向扩散过程，实现高效的噪音管理和跨类型转换。实验在公开和工业数据集上验证了该模型在链接预测和节点分类任务中的优越性能，超越现有方法并设立新基准。该框架的开源实现可访问https://github.com/HKUDS/DiffGraph。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.LG",
      "comment": "This paper is accepted by WSDM'2025",
      "pdf_url": "http://arxiv.org/pdf/2501.02313v1",
      "published_date": "2025-01-04 15:30:48 UTC",
      "updated_date": "2025-01-04 15:30:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:27:40.071680"
    },
    {
      "arxiv_id": "2501.02287v1",
      "title": "Deep Learning-Driven Segmentation of Ischemic Stroke Lesions Using Multi-Channel MRI",
      "title_zh": "深度学习驱动的缺血性中风病变多通道MRI",
      "authors": [
        "Ashiqur Rahman",
        "Muhammad E. H. Chowdhury",
        "Md Sharjis Ibne Wadud",
        "Rusab Sarmun",
        "Adam Mushtak",
        "Sohaib Bassam Zoghoul",
        "Israa Al-Hashimi"
      ],
      "abstract": "Ischemic stroke, caused by cerebral vessel occlusion, presents substantial\nchallenges in medical imaging due to the variability and subtlety of stroke\nlesions. Magnetic Resonance Imaging (MRI) plays a crucial role in diagnosing\nand managing ischemic stroke, yet existing segmentation techniques often fail\nto accurately delineate lesions. This study introduces a novel deep\nlearning-based method for segmenting ischemic stroke lesions using\nmulti-channel MRI modalities, including Diffusion Weighted Imaging (DWI),\nApparent Diffusion Coefficient (ADC), and enhanced Diffusion Weighted Imaging\n(eDWI). The proposed architecture integrates DenseNet121 as the encoder with\nSelf-Organized Operational Neural Networks (SelfONN) in the decoder, enhanced\nby Channel and Space Compound Attention (CSCA) and Double\nSqueeze-and-Excitation (DSE) blocks. Additionally, a custom loss function\ncombining Dice Loss and Jaccard Loss with weighted averages is introduced to\nimprove model performance. Trained and evaluated on the ISLES 2022 dataset, the\nmodel achieved Dice Similarity Coefficients (DSC) of 83.88% using DWI alone,\n85.86% with DWI and ADC, and 87.49% with the integration of DWI, ADC, and eDWI.\nThis approach not only outperforms existing methods but also addresses key\nlimitations in current segmentation practices. These advancements significantly\nenhance diagnostic precision and treatment planning for ischemic stroke,\nproviding valuable support for clinical decision-making.",
      "tldr_zh": "本研究提出了一种基于深度学习的缺血性中风病变分割方法，利用多通道 MRI（如 DWI、ADC 和 eDWI）来解决现有技术在病变精确划分上的挑战。模型架构整合了 DenseNet121 作为编码器、SelfONN 在解码器中，并通过 Channel and Space Compound Attention (CSCA) 和 Double Squeeze-and-Excitation (DSE) 块增强特征提取，同时引入了结合 Dice Loss 和 Jaccard Loss 的自定义加权损失函数。在 ISLES 2022 数据集上，模型的 Dice Similarity Coefficient (DSC) 得分分别达到 DWI 单独 83.88%、DWI + ADC 85.86% 以及 DWI + ADC + eDWI 87.49%，显著优于现有方法。该创新提升了缺血性中风的诊断精度和治疗规划，支持临床决策。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02287v1",
      "published_date": "2025-01-04 13:38:06 UTC",
      "updated_date": "2025-01-04 13:38:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:27:52.685886"
    },
    {
      "arxiv_id": "2501.14785v1",
      "title": "ED-Filter: Dynamic Feature Filtering for Eating Disorder Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Mehdi Naseriparsa",
        "Suku Sukunesan",
        "Zhen Cai",
        "Osama Alfarraj",
        "Amr Tolba",
        "Saba Fathi Rabooki",
        "Feng Xia"
      ],
      "abstract": "Eating disorders (ED) are critical psychiatric problems that have alarmed the\nmental health community. Mental health professionals are increasingly\nrecognizing the utility of data derived from social media platforms such as\nTwitter. However, high dimensionality and extensive feature sets of Twitter\ndata present remarkable challenges for ED classification. To overcome these\nhurdles, we introduce a novel method, an informed branch and bound search\ntechnique known as ED-Filter. This strategy significantly improves the\ndrawbacks of conventional feature selection algorithms such as filters and\nwrappers. ED-Filter iteratively identifies an optimal set of promising features\nthat maximize the eating disorder classification accuracy. In order to adapt to\nthe dynamic nature of Twitter ED data, we enhance the ED-Filter with a hybrid\ngreedy-based deep learning algorithm. This algorithm swiftly identifies\nsub-optimal features to accommodate the ever-evolving data landscape.\nExperimental results on Twitter eating disorder data affirm the effectiveness\nand efficiency of ED-Filter. The method demonstrates significant improvements\nin classification accuracy and proves its value in eating disorder detection on\nsocial media platforms.",
      "tldr_zh": "这篇论文针对饮食障碍（ED）分类面临的挑战，如Twitter数据的高维度和特征集庞大，提出了一种新型动态特征过滤方法ED-Filter。ED-Filter采用informed branch and bound搜索技术，通过迭代识别最优特征集来最大化分类准确率，同时克服传统特征选择算法（如filters和wrappers）的缺点。为了适应Twitter数据的动态特性，该方法整合了hybrid greedy-based deep learning算法，以快速筛选次优特征。实验结果显示，ED-Filter在Twitter ED数据上显著提升了分类准确率，证明了其在社交媒体平台上的ED检测效能。",
      "categories": [
        "stat.ML",
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ],
      "primary_category": "stat.ML",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.14785v1",
      "published_date": "2025-01-04 13:35:55 UTC",
      "updated_date": "2025-01-04 13:35:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:28:04.465944"
    },
    {
      "arxiv_id": "2501.02285v2",
      "title": "Hyperbolic Contrastive Learning for Hierarchical 3D Point Cloud Embedding",
      "title_zh": "翻译失败",
      "authors": [
        "Yingjie Liu",
        "Pengyu Zhang",
        "Ziyao He",
        "Mingsong Chen",
        "Xuan Tang",
        "Xian Wei"
      ],
      "abstract": "Hyperbolic spaces allow for more efficient modeling of complex, hierarchical\nstructures, which is particularly beneficial in tasks involving multi-modal\ndata. Although hyperbolic geometries have been proven effective for\nlanguage-image pre-training, their capabilities to unify language, image, and\n3D Point Cloud modalities are under-explored. We extend the 3D Point Cloud\nmodality in hyperbolic multi-modal contrastive pre-training. Additionally, we\nexplore the entailment, modality gap, and alignment regularizers for learning\nhierarchical 3D embeddings and facilitating the transfer of knowledge from both\nText and Image modalities. These regularizers enable the learning of\nintra-modal hierarchy within each modality and inter-modal hierarchy across\ntext, 2D images, and 3D Point Clouds. Experimental results demonstrate that our\nproposed training strategy yields an outstanding 3D Point Cloud encoder, and\nthe obtained 3D Point Cloud hierarchical embeddings significantly improve\nperformance on various downstream tasks.",
      "tldr_zh": "本文提出了一种基于 Hyperbolic Contrastive Learning 的方法，用于高效建模层次化的 3D Point Cloud 嵌入，将其扩展到多模态预训练中，包括语言、图像和 3D 点云模态。研究引入了 entailment、modality gap 和 alignment regularizers 来学习模态内的内部层次和模态间的交叉层次，从而促进知识从文本和图像模态向 3D 点云的转移。实验结果显示，该策略显著提升了 3D Point Cloud 编码器的性能，并在各种下游任务中取得了出色改进。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02285v2",
      "published_date": "2025-01-04 13:27:18 UTC",
      "updated_date": "2025-01-07 13:38:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:28:16.265848"
    },
    {
      "arxiv_id": "2501.02268v1",
      "title": "What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph",
      "title_zh": "翻译失败",
      "authors": [
        "Yutao Jiang",
        "Qiong Wu",
        "Wenhao Lin",
        "Wei Yu",
        "Yiyi Zhou"
      ],
      "abstract": "Recent Multimodal Large Language Models(MLLMs) often use a large number of\nvisual tokens to compensate their visual shortcoming, leading to excessive\ncomputation and obvious visual redundancy. In this paper, we investigate what\nkind of visual tokens are needed for MLLMs, and reveal that both foreground and\nbackground tokens are critical for MLLMs given the varying difficulties of\nexamples. Based on this observation, we propose a graph-based method towards\ntraining-free visual token pruning, termed G-Prune.In particular, G-Prune\nregards visual tokens as nodes, and construct their connections based on their\nsemantic similarities. Afterwards, the information flow is propagated via\nweighted links, and the most important tokens after iterations are kept for\nMLLMs, which can be front or background.To validate G-Prune, we apply it to a\nrecent MLLM called LLaVA-NeXT, and conduct extensive experiments on a set of\nbenchmarks.The experiment results show that G-Prune can greatly reduce\ncomputation overhead while retaining high performance on both coarse- and\nfine-grained tasks. For instance, G-Prune can reduce 63.57\\% FLOPs of\nLLaVA-NeXT on VQA2.0 and TextVQA with only 0.95\\% and 2.34\\% accuracy drops,\nrespectively.",
      "tldr_zh": "本文研究了多模态大语言模型(MLLMs)中视觉标记的冗余问题，发现前景和背景标记根据示例难度都至关重要。作者提出了一种基于图的训练免除方法G-Prune，将视觉标记视为节点，通过语义相似性构建连接并传播信息流，以选择最重要的标记。实验结果显示，在LLaVA-NeXT模型上应用G-Prune后，可在VQA2.0和TextVQA基准上减少63.57%的FLOPs，同时仅损失0.95%和2.34%的准确率，从而显著降低计算开销的同时保持高性能。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.02268v1",
      "published_date": "2025-01-04 12:14:42 UTC",
      "updated_date": "2025-01-04 12:14:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:28:28.445601"
    },
    {
      "arxiv_id": "2501.02267v1",
      "title": "Towards a constructive framework for control theory",
      "title_zh": "翻译失败",
      "authors": [
        "Pavel Osinenko"
      ],
      "abstract": "This work presents a framework for control theory based on constructive\nanalysis to account for discrepancy between mathematical results and their\nimplementation in a computer, also referred to as computational uncertainty. In\ncontrol engineering, the latter is usually either neglected or considered\nsubmerged into some other type of uncertainty, such as system noise, and\naddressed within robust control. However, even robust control methods may be\ncompromised when the mathematical objects involved in the respective algorithms\nfail to exist in exact form and subsequently fail to satisfy the required\nproperties. For instance, in general stabilization using a control Lyapunov\nfunction, computational uncertainty may distort stability certificates or even\ndestabilize the system despite robustness of the stabilization routine with\nregards to system, actuator and measurement noise. In fact, battling numerical\nproblems in practical implementation of controllers is common among control\nengineers. Such observations indicate that computational uncertainty should\nindeed be addressed explicitly in controller synthesis and system analysis. The\nmajor contribution here is a fairly general framework for proof techniques in\nanalysis and synthesis of control systems based on constructive analysis which\nexplicitly states that every computation be doable only up to a finite\nprecision thus accounting for computational uncertainty. A series of previous\nworks is overviewed, including constructive system stability and stabilization,\napproximate optimal controls, eigenvalue problems, Caratheodory trajectories,\nmeasurable selectors. Additionally, a new constructive version of the Danskin's\ntheorem, which is crucial in adversarial defense, is presented.",
      "tldr_zh": "这篇论文提出了一种基于 constructive analysis 的控制理论框架，以显式处理计算不确定性（computational uncertainty），解决数学结果与计算机实现之间的差异问题，因为传统 robust control 方法可能因精确性缺失而失效。框架强调所有计算仅在有限精度下进行，用于控制系统的分析和合成，从而提升系统稳定性并避免潜在的失稳风险。论文回顾了先前相关工作，包括 constructive system stability、approximate optimal controls 和 Caratheodory trajectories，并引入了一个新的 constructive version of the Danskin's theorem，以支持对抗防御应用。",
      "categories": [
        "math.OC",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "math.OC",
      "comment": "Published under: https://ieeexplore.ieee.org/document/9419858",
      "pdf_url": "http://arxiv.org/pdf/2501.02267v1",
      "published_date": "2025-01-04 12:07:45 UTC",
      "updated_date": "2025-01-04 12:07:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:28:40.136792"
    },
    {
      "arxiv_id": "2501.02266v1",
      "title": "LLMzSzŁ: a comprehensive LLM benchmark for Polish",
      "title_zh": "翻译失败",
      "authors": [
        "Krzysztof Jassem",
        "Michał Ciesiółka",
        "Filip Graliński",
        "Piotr Jabłoński",
        "Jakub Pokrywka",
        "Marek Kubis",
        "Monika Jabłońska",
        "Ryszard Staruch"
      ],
      "abstract": "This article introduces the first comprehensive benchmark for the Polish\nlanguage at this scale: LLMzSz{\\L} (LLMs Behind the School Desk). It is based\non a coherent collection of Polish national exams, including both academic and\nprofessional tests extracted from the archives of the Polish Central\nExamination Board. It covers 4 types of exams, coming from 154 domains.\nAltogether, it consists of almost 19k closed-ended questions. We investigate\nthe performance of open-source multilingual, English, and Polish LLMs to verify\nLLMs' abilities to transfer knowledge between languages. Also, the correlation\nbetween LLMs and humans at model accuracy and exam pass rate levels is\nexamined. We show that multilingual LLMs can obtain superior results over\nmonolingual ones; however, monolingual models may be beneficial when model size\nmatters. Our analysis highlights the potential of LLMs in assisting with exam\nvalidation, particularly in identifying anomalies or errors in examination\ntasks.",
      "tldr_zh": "这篇文章引入了LLMzSzŁ，一个全面的波兰语LLM基准测试（LLMs Behind the School Desk），基于波兰国家考试档案，涵盖4种考试类型和154个领域，总计近19k个闭合式问题。研究评估了开源的多语言、英语和波兰语LLMs的表现，验证了这些模型在语言间知识转移的能力，并分析了LLMs与人类的准确性和考试通过率的相关性。结果表明，多语言LLMs在性能上优于单语言模型，但后者在模型大小方面更具优势；此外，该基准测试突出了LLMs在考试验证中的潜力，如识别任务中的异常或错误。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02266v1",
      "published_date": "2025-01-04 12:04:46 UTC",
      "updated_date": "2025-01-04 12:04:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:28:51.825339"
    },
    {
      "arxiv_id": "2501.03268v1",
      "title": "Heterogeneous Graph Pre-training Based Model for Secure and Efficient Prediction of Default Risk Propagation among Bond Issuers",
      "title_zh": "基于异构图预训练的模型，用于债券发行人之间违约风险传播的安全和高效预测",
      "authors": [
        "Xurui Li",
        "Xin Shan",
        "Wenhao Yin",
        "Haijiao Wang"
      ],
      "abstract": "Efficient prediction of default risk for bond-issuing enterprises is pivotal\nfor maintaining stability and fostering growth in the bond market. Conventional\nmethods usually rely solely on an enterprise's internal data for risk\nassessment. In contrast, graph-based techniques leverage interconnected\ncorporate information to enhance default risk identification for targeted bond\nissuers. Traditional graph techniques such as label propagation algorithm or\ndeepwalk fail to effectively integrate a enterprise's inherent attribute\ninformation with its topological network data. Additionally, due to data\nscarcity and security privacy concerns between enterprises, end-to-end graph\nneural network (GNN) algorithms may struggle in delivering satisfactory\nperformance for target tasks. To address these challenges, we present a novel\ntwo-stage model. In the first stage, we employ an innovative Masked\nAutoencoders for Heterogeneous Graph (HGMAE) to pre-train on a vast enterprise\nknowledge graph. Subsequently, in the second stage, a specialized classifier\nmodel is trained to predict default risk propagation probabilities. The\nclassifier leverages concatenated feature vectors derived from the pre-trained\nencoder with the enterprise's task-specific feature vectors. Through the\ntwo-stage training approach, our model not only boosts the importance of unique\nbond characteristics for specific default prediction tasks, but also securely\nand efficiently leverage the global information pre-trained from other\nenterprises. Experimental results demonstrate that our proposed model\noutperforms existing approaches in predicting default risk for bond issuers.",
      "tldr_zh": "该研究提出了一种基于异构图预训练的模型，用于安全高效地预测债券发行者之间的违约风险传播。针对传统方法无法有效整合企业属性信息和拓扑网络数据的问题，该模型采用两阶段方法：首先，使用 Masked Autoencoders for Heterogeneous Graph (HGMAE) 在大规模企业知识图上进行预训练；其次，训练一个分类器，通过拼接预训练编码器的特征向量与企业特定特征向量来预测违约风险概率。这种方法不仅强调独特债券特征的安全利用，还能高效地整合全局信息。实验结果显示，该模型在违约风险预测方面优于现有 Graph Neural Network (GNN) 方法和传统技术。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03268v1",
      "published_date": "2025-01-04 11:10:16 UTC",
      "updated_date": "2025-01-04 11:10:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:29:04.034004"
    },
    {
      "arxiv_id": "2501.02241v1",
      "title": "Interpretable Load Forecasting via Representation Learning of Geo-distributed Meteorological Factors",
      "title_zh": "翻译失败",
      "authors": [
        "Yangze Zhou",
        "Guoxin Lin",
        "Gonghao Zhang",
        "Yi Wang"
      ],
      "abstract": "Meteorological factors (MF) are crucial in day-ahead load forecasting as they\nsignificantly influence the electricity consumption behaviors of consumers.\nNumerous studies have incorporated MF into the load forecasting model to\nachieve higher accuracy. Selecting MF from one representative location or the\naveraged MF as the inputs of the forecasting model is a common practice.\nHowever, the difference in MF collected in various locations within a region\nmay be significant, which poses a challenge in selecting the appropriate MF\nfrom numerous locations. A representation learning framework is proposed to\nextract geo-distributed MF while considering their spatial relationships. In\naddition, this paper employs the Shapley value in the graph-based model to\nreveal connections between MF collected in different locations and loads. To\nreduce the computational complexity of calculating the Shapley value, an\nacceleration method is adopted based on Monte Carlo sampling and weighted\nlinear regression. Experiments on two real-world datasets demonstrate that the\nproposed method improves the day-ahead forecasting accuracy, especially in\nextreme scenarios such as the \"accumulation temperature effect\" in summer and\n\"sudden temperature change\" in winter. We also find a significant correlation\nbetween the importance of MF in different locations and the corresponding\narea's GDP and mainstay industry.",
      "tldr_zh": "该研究提出了一种基于表示学习（Representation Learning）的框架，用于处理地理分布的气象因素（Geo-distributed Meteorological Factors），以提升日提前负荷预测的可解释性。框架考虑了不同位置气象因素的空间关系，并利用Shapley value在图-based模型中分析这些因素与负荷之间的联系，同时采用Monte Carlo采样和加权线性回归来加速计算。实验结果显示，该方法在两个真实数据集上显著提高了预测准确性，尤其在极端场景如夏季的“accumulation temperature effect”和冬季的“sudden temperature change”中。此外，研究发现，不同位置气象因素的重要性与相应区域的GDP和主要产业密切相关。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02241v1",
      "published_date": "2025-01-04 09:05:06 UTC",
      "updated_date": "2025-01-04 09:05:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:29:15.723833"
    },
    {
      "arxiv_id": "2502.15697v1",
      "title": "Robust Uplift Modeling with Large-Scale Contexts for Real-time Marketing",
      "title_zh": "翻译失败",
      "authors": [
        "Zexu Sun",
        "Qiyu Han",
        "Minqin Zhu",
        "Hao Gong",
        "Dugang Liu",
        "Chen Ma"
      ],
      "abstract": "Improving user engagement and platform revenue is crucial for online\nmarketing platforms. Uplift modeling is proposed to solve this problem, which\napplies different treatments (e.g., discounts, bonus) to satisfy corresponding\nusers. Despite progress in this field, limitations persist. Firstly, most of\nthem focus on scenarios where only user features exist. However, in real-world\nscenarios, there are rich contexts available in the online platform (e.g.,\nshort videos, news), and the uplift model needs to infer an incentive for each\nuser on the specific item, which is called real-time marketing. Thus, only\nconsidering the user features will lead to biased prediction of the responses,\nwhich may cause the cumulative error for uplift prediction. Moreover, due to\nthe large-scale contexts, directly concatenating the context features with the\nuser features will cause a severe distribution shift in the treatment and\ncontrol groups. Secondly, capturing the interaction relationship between the\nuser features and context features can better predict the user response. To\nsolve the above limitations, we propose a novel model-agnostic Robust Uplift\nModeling with Large-Scale Contexts (UMLC) framework for Real-time Marketing.\nOur UMLC includes two customized modules. 1) A response-guided context grouping\nmodule for extracting context features information and condensing value space\nthrough clusters. 2) A feature interaction module for obtaining better uplift\nprediction. Specifically, this module contains two parts: a user-context\ninteraction component for better modeling the response; a treatment-feature\ninteraction component for discovering the treatment assignment sensitive\nfeature of each instance to better predict the uplift. Moreover, we conduct\nextensive experiments on a synthetic dataset and a real-world product dataset\nto verify the effectiveness and compatibility of our UMLC.",
      "tldr_zh": "该论文针对在线营销平台的实时营销问题，提出了一种鲁棒的提升建模框架Robust Uplift Modeling with Large-Scale Contexts (UMLC)，以解决传统模型忽略上下文（如短视频、新闻）导致的预测偏差和分布偏移问题。该框架包括响应引导的上下文分组模块，用于提取并聚类上下文特征以压缩值空间，以及特征交互模块，通过用户-上下文交互和处理-特征交互组件来更好地捕捉交互关系并预测用户响应。实验在合成数据集和真实产品数据集上验证了UMLC的有效性和兼容性，显著提升了实时营销的准确性和鲁棒性。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted to KDD'25 Research Track, 15 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2502.15697v1",
      "published_date": "2025-01-04 08:55:50 UTC",
      "updated_date": "2025-01-04 08:55:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:29:27.586885"
    },
    {
      "arxiv_id": "2501.02237v1",
      "title": "Financial Named Entity Recognition: How Far Can LLM Go?",
      "title_zh": "金融命名实体识别：LLM 能走多远？",
      "authors": [
        "Yi-Te Lu",
        "Yintong Huo"
      ],
      "abstract": "The surge of large language models (LLMs) has revolutionized the extraction\nand analysis of crucial information from a growing volume of financial\nstatements, announcements, and business news. Recognition for named entities to\nconstruct structured data poses a significant challenge in analyzing financial\ndocuments and is a foundational task for intelligent financial analytics.\nHowever, how effective are these generic LLMs and their performance under\nvarious prompts are yet need a better understanding. To fill in the blank, we\npresent a systematic evaluation of state-of-the-art LLMs and prompting methods\nin the financial Named Entity Recognition (NER) problem. Specifically, our\nexperimental results highlight their strengths and limitations, identify five\nrepresentative failure types, and provide insights into their potential and\nchallenges for domain-specific tasks.",
      "tldr_zh": "本研究评估了大型语言模型 (LLMs) 在金融命名实体识别 (NER) 任务中的表现，旨在理解这些模型在处理金融文档时的有效性和局限性。通过系统实验和不同提示方法，论文突出了 LLMs 的优势，同时识别了五种代表性失败类型。最终结果为 LLMs 在领域特定任务中的潜力与挑战提供了宝贵见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at The Joint Workshop of the 9th Financial Technology and\n  Natural Language Processing (FinNLP), the 6th Financial Narrative Processing\n  (FNP), and the 1st Workshop on Large Language Models for Finance and Legal\n  (LLMFinLegal), in conjunction with COLING 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.02237v1",
      "published_date": "2025-01-04 08:47:21 UTC",
      "updated_date": "2025-01-04 08:47:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:29:39.316808"
    },
    {
      "arxiv_id": "2501.02221v2",
      "title": "CORD: Generalizable Cooperation via Role Diversity",
      "title_zh": "翻译失败",
      "authors": [
        "Kanefumi Matsuyama",
        "Kefan Su",
        "Jiangxing Wang",
        "Deheng Ye",
        "Zongqing Lu"
      ],
      "abstract": "Cooperative multi-agent reinforcement learning (MARL) aims to develop agents\nthat can collaborate effectively. However, most cooperative MARL methods\noverfit training agents, making learned policies not generalize well to unseen\ncollaborators, which is a critical issue for real-world deployment. Some\nmethods attempt to address the generalization problem but require prior\nknowledge or predefined policies of new teammates, limiting real-world\napplications. To this end, we propose a hierarchical MARL approach to enable\ngeneralizable cooperation via role diversity, namely CORD. CORD's high-level\ncontroller assigns roles to low-level agents by maximizing the role entropy\nwith constraints. We show this constrained objective can be decomposed into\ncausal influence in role that enables reasonable role assignment, and role\nheterogeneity that yields coherent, non-redundant role clusters. Evaluated on a\nvariety of cooperative multi-agent tasks, CORD achieves better performance than\nbaselines, especially in generalization tests. Ablation studies further\ndemonstrate the efficacy of the constrained objective in generalizable\ncooperation.",
      "tldr_zh": "这篇论文针对合作式多智能体强化学习 (Cooperative MARL) 中代理过度拟合导致泛化差的问题，提出了一种层次化方法 CORD，通过角色多样性 (role diversity) 实现可泛化合作。CORD 的高层控制器通过最大化角色熵 (role entropy) 并添加约束来分配角色，这些约束分解为角色中的因果影响和角色异质性，确保合理的角色分配和非冗余集群。在多种合作任务的评估中，CORD 比基线模型表现出色，尤其在泛化测试中，消融研究进一步证明了约束目标的有效性。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02221v2",
      "published_date": "2025-01-04 07:53:38 UTC",
      "updated_date": "2025-01-10 09:26:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:29:52.334455"
    },
    {
      "arxiv_id": "2501.02219v1",
      "title": "Diffusion Model-Based Data Synthesis Aided Federated Semi-Supervised Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Zhongwei Wang",
        "Tong Wu",
        "Zhiyong Chen",
        "Liang Qian",
        "Yin Xu",
        "Meixia Tao"
      ],
      "abstract": "Federated semi-supervised learning (FSSL) is primarily challenged by two\nfactors: the scarcity of labeled data across clients and the non-independent\nand identically distribution (non-IID) nature of data among clients. In this\npaper, we propose a novel approach, diffusion model-based data synthesis aided\nFSSL (DDSA-FSSL), which utilizes a diffusion model (DM) to generate synthetic\ndata, bridging the gap between heterogeneous local data distributions and the\nglobal data distribution. In DDSA-FSSL, clients address the challenge of the\nscarcity of labeled data by employing a federated learning-trained classifier\nto perform pseudo labeling for unlabeled data. The DM is then collaboratively\ntrained using both labeled and precision-optimized pseudo-labeled data,\nenabling clients to generate synthetic samples for classes that are absent in\ntheir labeled datasets. This process allows clients to generate more\ncomprehensive synthetic datasets aligned with the global distribution.\nExtensive experiments conducted on multiple datasets and varying non-IID\ndistributions demonstrate the effectiveness of DDSA-FSSL, e.g., it improves\naccuracy from 38.46% to 52.14% on CIFAR-10 datasets with 10% labeled data.",
      "tldr_zh": "该论文针对 Federated semi-supervised learning (FSSL) 面临的标签数据稀缺和 non-IID 数据分布问题，提出了一种新型方法 DDSA-FSSL，利用 diffusion model (DM) 生成合成数据来桥接本地和全局数据分布的差距。方法中，客户端通过 federated learning 训练的分类器对未标记数据进行 pseudo labeling，并使用标记数据和优化后的伪标记数据协作训练 DM，从而生成覆盖本地数据集缺失类别的合成样本。实验在多个数据集和非-IID 分布上验证了该方法的有效性，例如在 CIFAR-10 数据集上，使用 10% 标记数据时，准确率从 38.46% 提高到 52.14%。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.LG",
      "comment": "accepted by IEEE WCNC 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.02219v1",
      "published_date": "2025-01-04 07:38:15 UTC",
      "updated_date": "2025-01-04 07:38:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:30:04.824068"
    },
    {
      "arxiv_id": "2501.03266v2",
      "title": "LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena",
      "title_zh": "LLM 内容审核与用户满意",
      "authors": [
        "Stefan Pasch"
      ],
      "abstract": "LLM safety and ethical alignment are widely discussed, but the impact of\ncontent moderation on user satisfaction remains underexplored. In particular,\nlittle is known about how users respond when models refuse to answer a\nprompt-one of the primary mechanisms used to enforce ethical boundaries in\nLLMs. We address this gap by analyzing nearly 50,000 model comparisons from\nChatbot Arena, a platform where users indicate their preferred LLM response in\npairwise matchups, providing a large-scale setting for studying real-world user\npreferences. Using a novel RoBERTa-based refusal classifier fine-tuned on a\nhand-labeled dataset, we distinguish between refusals due to ethical concerns\nand technical limitations. Our results reveal a substantial refusal penalty:\nethical refusals yield significantly lower win rates than both technical\nrefusals and standard responses, indicating that users are especially\ndissatisfied when models decline a task for ethical reasons. However, this\npenalty is not uniform. Refusals receive more favorable evaluations when the\nunderlying prompt is highly sensitive (e.g., involving illegal content), and\nwhen the refusal is phrased in a detailed and contextually aligned manner.\nThese findings underscore a core tension in LLM design: safety-aligned\nbehaviors may conflict with user expectations, calling for more adaptive\nmoderation strategies that account for context and presentation.",
      "tldr_zh": "这篇论文探讨了LLM内容调节对用户满意度的影响，特别聚焦于模型拒绝回答prompt的场景，通过分析Chatbot Arena平台上的近50,000个模型比较数据。研究者开发了一个基于RoBERTa的拒绝分类器，以区分伦理原因和技术原因的拒绝，并发现伦理拒绝会导致显著的胜率下降，用户对这类拒绝更不满。结果表明，这种不满因prompt的敏感度（如涉及非法内容）和拒绝的表述方式（如详细且上下文相关）而异。论文强调，LLM设计需平衡安全对齐与用户期望，呼吁采用更具适应性的调节策略。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03266v2",
      "published_date": "2025-01-04 06:36:44 UTC",
      "updated_date": "2025-05-16 01:23:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:30:16.182551"
    },
    {
      "arxiv_id": "2501.03265v1",
      "title": "Optimizing Edge AI: A Comprehensive Survey on Data, Model, and System Strategies",
      "title_zh": "翻译失败",
      "authors": [
        "Xubin Wang",
        "Weijia Jia"
      ],
      "abstract": "The emergence of 5G and edge computing hardware has brought about a\nsignificant shift in artificial intelligence, with edge AI becoming a crucial\ntechnology for enabling intelligent applications. With the growing amount of\ndata generated and stored on edge devices, deploying AI models for local\nprocessing and inference has become increasingly necessary. However, deploying\nstate-of-the-art AI models on resource-constrained edge devices faces\nsignificant challenges that must be addressed. This paper presents an\noptimization triad for efficient and reliable edge AI deployment, including\ndata, model, and system optimization. First, we discuss optimizing data through\ndata cleaning, compression, and augmentation to make it more suitable for edge\ndeployment. Second, we explore model design and compression methods at the\nmodel level, such as pruning, quantization, and knowledge distillation.\nFinally, we introduce system optimization techniques like framework support and\nhardware acceleration to accelerate edge AI workflows. Based on an in-depth\nanalysis of various application scenarios and deployment challenges of edge AI,\nthis paper proposes an optimization paradigm based on the data-model-system\ntriad to enable a whole set of solutions to effectively transfer ML models,\nwhich are initially trained in the cloud, to various edge devices for\nsupporting multiple scenarios.",
      "tldr_zh": "这篇综述论文探讨了边缘AI（edge AI）的优化策略，以应对在资源受限设备上部署AI模型的挑战。论文提出一个数据-模型-系统优化三要素框架：首先，通过数据清洗、压缩和增强来优化数据；其次，采用模型修剪、量化（quantization）和知识蒸馏（knowledge distillation）等方法来设计和压缩模型；最后，利用框架支持和硬件加速（hardware acceleration）等系统优化技术加速工作流程。基于对各种应用场景和部署挑战的深入分析，该论文构建了一个优化范式，帮助将云端训练的ML模型高效迁移到边缘设备，支持多种场景的智能应用。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.03265v1",
      "published_date": "2025-01-04 06:17:48 UTC",
      "updated_date": "2025-01-04 06:17:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:30:27.572981"
    },
    {
      "arxiv_id": "2501.02200v1",
      "title": "Learning Evolution via Optimization Knowledge Adaptation",
      "title_zh": "通过优化知识适应学习进化",
      "authors": [
        "Chao Wang",
        "Licheng Jiao",
        "Jiaxuan Zhao",
        "Lingling Li",
        "Fang Liu",
        "Shuyuan Yang"
      ],
      "abstract": "Evolutionary algorithms (EAs) maintain populations through evolutionary\noperators to discover diverse solutions for complex tasks while gathering\nvaluable knowledge, such as historical population data and fitness evaluations.\nHowever, traditional EAs face challenges in dynamically adapting to expanding\nknowledge bases, hindering the efficient exploitation of accumulated\ninformation and limiting adaptability to new situations. To address these\nissues, we introduce an Optimization Knowledge Adaptation Evolutionary Model\n(OKAEM), which features dynamic parameter adjustment using accumulated\nknowledge to enhance its optimization capabilities. OKAEM employs attention\nmechanisms to model the interactions among individuals, fitness landscapes, and\ngenetic components separately, thereby parameterizing the evolutionary\noperators of selection, crossover, and mutation. These powerful learnable\noperators enable OKAEM to benefit from pre-learned extensive prior knowledge\nand self-tune with real-time evolutionary insights. Experimental results\ndemonstrate that OKAEM: 1) exploits prior knowledge for significant performance\ngains across various knowledge transfer settings; 2) achieves competitive\nperformance through self-tuning alone, even without prior knowledge; 3)\noutperforms state-of-the-art black-box baselines in a vision-language model\ntuning case; 4) can improve its optimization capabilities with growing\nknowledge; 5) is capable of emulating principles of natural selection and\ngenetic recombination.",
      "tldr_zh": "本文提出 Optimization Knowledge Adaptation Evolutionary Model (OKAEM)，一种新型进化算法框架，通过动态调整参数利用积累的知识（如历史种群数据和适应度评估）来提升优化性能，解决传统 Evolutionary algorithms (EAs) 在适应扩展知识库时的局限性。OKAEM 采用 attention mechanisms 分别建模个体、适应度景观和遗传组件的交互，从而参数化选择、交叉和变异操作，实现对预先学习的先验知识和实时进化洞见的有效利用。实验结果表明，OKAEM 在各种知识转移场景中显著提升性能，即使无先验知识也能自调整并优于最先进黑箱基线，同时模拟自然选择和遗传重组原则。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "This work has been submitted to Springer Nature for possible\n  publication",
      "pdf_url": "http://arxiv.org/pdf/2501.02200v1",
      "published_date": "2025-01-04 05:35:21 UTC",
      "updated_date": "2025-01-04 05:35:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:30:41.342932"
    },
    {
      "arxiv_id": "2501.02199v1",
      "title": "Can ChatGPT implement finite element models for geotechnical engineering applications?",
      "title_zh": "ChatGPT 是否能够实现岩土工程应用中的有限元模型？",
      "authors": [
        "Taegu Kim",
        "Tae Sup Yun",
        "Hyoung Suk Suh"
      ],
      "abstract": "This study assesses the capability of ChatGPT to generate finite element code\nfor geotechnical engineering applications from a set of prompts. We tested\nthree different initial boundary value problems using a hydro-mechanically\ncoupled formulation for unsaturated soils, including the dissipation of excess\npore water pressure through fluid mass diffusion in one-dimensional space,\ntime-dependent differential settlement of a strip footing, and gravity-driven\nseepage. For each case, initial prompting involved providing ChatGPT with\nnecessary information for finite element implementation, such as balance and\nconstitutive equations, problem geometry, initial and boundary conditions,\nmaterial properties, and spatiotemporal discretization and solution strategies.\nAny errors and unexpected results were further addressed through prompt\naugmentation processes until the ChatGPT-generated finite element code passed\nthe verification/validation test. Our results demonstrate that ChatGPT required\nminimal code revisions when using the FEniCS finite element library, owing to\nits high-level interfaces that enable efficient programming. In contrast, the\nMATLAB code generated by ChatGPT necessitated extensive prompt augmentations\nand/or direct human intervention, as it involves a significant amount of\nlow-level programming required for finite element analysis, such as\nconstructing shape functions or assembling global matrices. Given that prompt\nengineering for this task requires an understanding of the mathematical\nformulation and numerical techniques, this study suggests that while a large\nlanguage model may not yet replace human programmers, it can greatly assist in\nthe implementation of numerical models.",
      "tldr_zh": "这篇论文评估了 ChatGPT 生成岩土工程有限元代码的能力，通过测试三个不饱和土壤的初始边界值问题，包括一维过量孔隙水压消散、带状地基差分沉降和重力驱动渗流。研究采用提示工程提供平衡方程、问题几何、边界条件等信息，并通过迭代修正来验证代码。结果显示，使用 FEniCS 有限元库时 ChatGPT 需要较少代码修改，而 MATLAB 生成的代码需大量干预，因为涉及低级编程如构建形状函数和组装全局矩阵。总体而言，ChatGPT 虽能辅助数值模型实现，但仍依赖人类对数学公式和数值技术的理解，无法完全取代程序员。",
      "categories": [
        "math.NA",
        "cs.AI",
        "cs.NA"
      ],
      "primary_category": "math.NA",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02199v1",
      "published_date": "2025-01-04 05:21:40 UTC",
      "updated_date": "2025-01-04 05:21:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:30:53.730079"
    },
    {
      "arxiv_id": "2501.02196v1",
      "title": "CPTuning: Contrastive Prompt Tuning for Generative Relation Extraction",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaxin Duan",
        "Fengyu Lu",
        "Junfei Liu"
      ],
      "abstract": "Generative relation extraction (RE) commonly involves first reformulating RE\nas a linguistic modeling problem easily tackled with pre-trained language\nmodels (PLM) and then fine-tuning a PLM with supervised cross-entropy loss.\nAlthough having achieved promising performance, existing approaches assume only\none deterministic relation between each pair of entities without considering\nreal scenarios where multiple relations may be valid, i.e., entity pair\noverlap, causing their limited applications. To address this problem, we\nintroduce a novel contrastive prompt tuning method for RE, CPTuning, which\nlearns to associate a candidate relation between two in-context entities with a\nprobability mass above or below a threshold, corresponding to whether the\nrelation exists. Beyond learning schema, CPTuning also organizes RE as a\nverbalized relation generation task and uses Trie-constrained decoding to\nensure a model generates valid relations. It adaptively picks out the generated\ncandidate relations with a high estimated likelihood in inference, thereby\nachieving multi-relation extraction. We conduct extensive experiments on four\nwidely used datasets to validate our method. Results show that T5-large\nfine-tuned with CPTuning significantly outperforms previous methods, regardless\nof single or multiple relations extraction.",
      "tldr_zh": "该论文针对生成式关系抽取（Generative Relation Extraction）中的问题，提出CPTuning，一种对比提示调优方法，以处理实体对可能存在多个关系的场景（如entity pair overlap），从而扩展其实际应用。CPTuning通过学习候选关系与上下文实体的概率关联、将任务转化为verbalized relation generation，并采用Trie-constrained decoding确保生成有效关系，在推理时自适应选择高概率候选，实现多关系抽取。在四个常用数据集上的实验显示，T5-large模型经CPTuning微调后，显著优于现有方法，无论单关系还是多关系提取。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02196v1",
      "published_date": "2025-01-04 05:17:34 UTC",
      "updated_date": "2025-01-04 05:17:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:31:05.461781"
    },
    {
      "arxiv_id": "2501.02189v6",
      "title": "A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges",
      "title_zh": "翻译失败",
      "authors": [
        "Zongxia Li",
        "Xiyang Wu",
        "Hongyang Du",
        "Fuxiao Liu",
        "Huy Nghiem",
        "Guangyao Shi"
      ],
      "abstract": "Multimodal Vision Language Models (VLMs) have emerged as a transformative\ntopic at the intersection of computer vision and natural language processing,\nenabling machines to perceive and reason about the world through both visual\nand textual modalities. For example, models such as CLIP, Claude, and GPT-4V\ndemonstrate strong reasoning and understanding abilities on visual and textual\ndata and beat classical single modality vision models on zero-shot\nclassification [93]. With their rapid advancements in research and growing\npopularity in various applications, we provide a comprehensive survey of VLMs.\nSpecifically, we provide a systematic overview of VLMs in the following\naspects: [1] model information of the major VLMs developed up to 2025; [2] the\ntransition of VLM architectures and the newest VLM alignment methods; [3]\nsummary and categorization of the popular benchmarks and evaluation metrics of\nVLMs; [4] the challenges and issues faced by current VLMs such as\nhallucination, alignment, fairness, and safety. Detailed collections including\npapers and model repository links are listed in\nhttps://github.com/zli12321/Vision-Language-Models-Overview.",
      "tldr_zh": "本调查综述了前沿大型视觉语言模型(VLMs)的最新进展，包括截至2025年的主要模型信息、架构演变以及新型对齐方法。论文系统总结并分类了VLMs的流行基准和评估指标，突出了这些模型在零样本分类等任务上的优势。最终，讨论了VLMs面临的挑战，如幻觉、对齐、公平性和安全性问题，并提供了GitHub仓库链接以供进一步资源访问。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.02189v6",
      "published_date": "2025-01-04 04:59:33 UTC",
      "updated_date": "2025-04-06 03:12:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:31:16.197876"
    },
    {
      "arxiv_id": "2501.02182v1",
      "title": "AdaMixup: A Dynamic Defense Framework for Membership Inference Attack Mitigation",
      "title_zh": "AdaMixup：一种动态防御",
      "authors": [
        "Ying Chen",
        "Jiajing Chen",
        "Yijie Weng",
        "ChiaHua Chang",
        "Dezhi Yu",
        "Guanbiao Lin"
      ],
      "abstract": "Membership inference attacks have emerged as a significant privacy concern in\nthe training of deep learning models, where attackers can infer whether a data\npoint was part of the training set based on the model's outputs. To address\nthis challenge, we propose a novel defense mechanism, AdaMixup. AdaMixup\nemploys adaptive mixup techniques to enhance the model's robustness against\nmembership inference attacks by dynamically adjusting the mixup strategy during\ntraining. This method not only improves the model's privacy protection but also\nmaintains high performance. Experimental results across multiple datasets\ndemonstrate that AdaMixup significantly reduces the risk of membership\ninference attacks while achieving a favorable trade-off between defensive\nefficiency and model accuracy. This research provides an effective solution for\ndata privacy protection and lays the groundwork for future advancements in\nmixup training methods.",
      "tldr_zh": "该论文针对 Membership Inference Attacks（成员推断攻击）问题，提出了一种新型防御框架 AdaMixup，以保护深度学习模型训练过程中的数据隐私。AdaMixup 通过 adaptive mixup techniques 动态调整训练中的混淆策略，提升模型的鲁棒性，同时维持高性能。实验结果显示，在多个数据集上，该方法显著降低了攻击风险，并在防御效率与模型准确性之间实现了良好权衡。该研究为数据隐私保护提供了有效解决方案，并为未来的 mixup 训练方法奠定了基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2501.02182v1",
      "published_date": "2025-01-04 04:21:48 UTC",
      "updated_date": "2025-01-04 04:21:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:31:28.210709"
    },
    {
      "arxiv_id": "2501.03264v1",
      "title": "Bridge the Inference Gaps of Neural Processes via Expectation Maximization",
      "title_zh": "翻译失败",
      "authors": [
        "Qi Wang",
        "Marco Federici",
        "Herke van Hoof"
      ],
      "abstract": "The neural process (NP) is a family of computationally efficient models for\nlearning distributions over functions. However, it suffers from under-fitting\nand shows suboptimal performance in practice. Researchers have primarily\nfocused on incorporating diverse structural inductive biases, \\textit{e.g.}\nattention or convolution, in modeling. The topic of inference suboptimality and\nan analysis of the NP from the optimization objective perspective has hardly\nbeen studied in earlier work. To fix this issue, we propose a surrogate\nobjective of the target log-likelihood of the meta dataset within the\nexpectation maximization framework. The resulting model, referred to as the\nSelf-normalized Importance weighted Neural Process (SI-NP), can learn a more\naccurate functional prior and has an improvement guarantee concerning the\ntarget log-likelihood. Experimental results show the competitive performance of\nSI-NP over other NPs objectives and illustrate that structural inductive\nbiases, such as attention modules, can also augment our method to achieve SOTA\nperformance. Our code is available at\n\\url{https://github.com/hhq123gogogo/SI_NPs}.",
      "tldr_zh": "Neural Process (NP) 是一种高效的学习函数分布的模型，但由于推理次优和欠拟合问题，其实际性能较差。论文提出一种基于 Expectation Maximization (EM) 框架的代理目标，开发了 Self-normalized Importance weighted Neural Process (SI-NP) 模型，该模型能学习更准确的功能先验，并对目标对数似然提供改进保证。实验结果表明，SI-NP 在性能上优于其他 NP 方法，且可与结构偏差（如注意力模块）结合，达到 SOTA 水平。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR2023",
      "pdf_url": "http://arxiv.org/pdf/2501.03264v1",
      "published_date": "2025-01-04 03:28:21 UTC",
      "updated_date": "2025-01-04 03:28:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:31:40.740786"
    },
    {
      "arxiv_id": "2501.02169v1",
      "title": "The Integration of Blockchain and Artificial Intelligence for Secure Healthcare Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Umar Safdar",
        "Simon Gabrael"
      ],
      "abstract": "Verisign reported a 125 percent increase in data breaches within the\nhealthcare sector in the United States during 2022, with 18.2 million patient\nrecords being impacted. Growing healthcare data volumes and diversification\nmean that medical information is becoming more valuable. Many Health Centers\nuse various technologies to ease the classification, storage, and exchange of\nbig data. This use can also make the health data of the users at risk and\nvulnerable. AI and blockchain are among the leading technologies at hand. With\nAI, data-driven operations and big data efficiency have been improved with\nrespect to traditional techniques. Due to its potential to bring about\nimprovements in health services and lower medical costs, this AI technology is\nregularly used in healthcare. Blockchain helps protect transactions on sharing\ninformation and private privacy as long as the exchange of knowledge is that of\nthe standard. The objective of this analysis is to investigate the research and\nunique contributions since 2008 regarding blockchain-integrated AI and\nhealthcare systems. The work sheds light on applied AI-based healthcare schemes\nwith machine, ballistic, and acrylic learning and disparate blockchain\nstructures. The use of technology in order to ensure patient data security and\nmanage medical information effectively in healthcare settings offers a highly\nsuccessful position for both healthcare providers and patients. From 2018 to\n2021, the best year was 2021 to grow, enhancing everything to examine the\ndownload of the device and the counting of Google Academies, for which the\njoining perspective was borrowed; local research experts were asked, identified\narticles in recent years, and read reviews of large research grants.",
      "tldr_zh": "这篇论文探讨了区块链和人工智能（AI）在医疗保健系统中的整合，以应对数据泄露风险，如2022年Verisign报告中美国医疗部门125%的增幅。研究回顾了从2008年以来相关领域的文献，分析了AI（如机器学习）在数据处理和效率提升方面的应用，以及区块链在保护信息共享和隐私中的作用。论文强调，这些技术组合能有效提升患者数据安全和医疗信息管理，并在2018-2021年间显示出研究增长趋势，为医疗提供商和患者带来显著益处。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "13 pages, 4 Figures",
      "pdf_url": "http://arxiv.org/pdf/2501.02169v1",
      "published_date": "2025-01-04 02:53:55 UTC",
      "updated_date": "2025-01-04 02:53:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:31:52.226873"
    },
    {
      "arxiv_id": "2501.14784v1",
      "title": "DeServe: Towards Affordable Offline LLM Inference via Decentralization",
      "title_zh": "翻译失败",
      "authors": [
        "Linyu Wu",
        "Xiaoyuan Liu",
        "Tianneng Shi",
        "Zhe Ye",
        "Dawn Song"
      ],
      "abstract": "The rapid growth of generative AI and its integration into everyday workflows\nhave significantly increased the demand for large language model (LLM)\ninference services. While proprietary models remain popular, recent\nadvancements in open-source LLMs have positioned them as strong contenders.\nHowever, deploying these models is often constrained by the high costs and\nlimited availability of GPU resources. In response, this paper presents the\ndesign of a decentralized offline serving system for LLM inference. Utilizing\nidle GPU resources, our proposed system, DeServe, decentralizes access to LLMs\nat a lower cost. DeServe specifically addresses key challenges in optimizing\nserving throughput in high-latency network environments. Experiments\ndemonstrate that DeServe achieves a 6.7x-12.6x improvement in throughput over\nexisting serving system baselines in such conditions.",
      "tldr_zh": "本文提出DeServe，一种去中心化的离线LLM推理服务系统，旨在通过利用闲置GPU资源降低部署成本并解决高延迟网络环境下的服务吞吐量优化挑战。随着生成AI需求的增长，DeServe为开源LLM的普及提供更affordable的解决方案。实验结果显示，该系统在高延迟条件下比现有基准系统提高了6.7x-12.6x的吞吐量。",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.14784v1",
      "published_date": "2025-01-04 02:10:50 UTC",
      "updated_date": "2025-01-04 02:10:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:32:04.557510"
    },
    {
      "arxiv_id": "2501.02156v3",
      "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
      "title_zh": "翻译失败",
      "authors": [
        "Chien-Ping Lu"
      ],
      "abstract": "As large-scale AI models expand, training becomes costlier and sustaining\nprogress grows harder. Classical scaling laws (e.g., Kaplan et al. (2020),\nHoffmann et al. (2022)) predict training loss from a static compute budget yet\nneglect time and efficiency, prompting the question: how can we balance\nballooning GPU fleets with rapidly improving hardware and algorithms? We\nintroduce the relative-loss equation, a time- and efficiency-aware framework\nthat extends classical AI scaling laws. Our model shows that, without ongoing\nefficiency gains, advanced performance could demand millennia of training or\nunrealistically large GPU fleets. However, near-exponential progress remains\nachievable if the \"efficiency-doubling rate\" parallels Moore's Law. By\nformalizing this race to efficiency, we offer a quantitative roadmap for\nbalancing front-loaded GPU investments with incremental improvements across the\nAI stack. Empirical trends suggest that sustained efficiency gains can push AI\nscaling well into the coming decade, providing a new perspective on the\ndiminishing returns inherent in classical scaling.",
      "tldr_zh": "该论文审视了AI模型规模化训练的成本挑战，扩展了经典AI scaling laws（如Kaplan et al. (2020)和Hoffmann et al. (2022)），这些定律忽略了时间和效率因素。作者引入了relative-loss equation框架，该框架整合效率和时间变量，显示如果缺乏持续效率改进，达到高级AI性能可能需要数千年训练或巨型GPU集群。研究发现，若效率-doubling rate 能与Moore's Law 相当，则AI缩放可实现近指数级进步，并提供了一个定量路线图，帮助平衡初始GPU投资与AI栈的渐进优化，从而缓解经典缩放中的diminishing returns。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.LG",
      "comment": "21 pages, 3 figures. 2 tables, second draft",
      "pdf_url": "http://arxiv.org/pdf/2501.02156v3",
      "published_date": "2025-01-04 01:45:32 UTC",
      "updated_date": "2025-01-08 14:26:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:32:17.502833"
    },
    {
      "arxiv_id": "2501.02152v1",
      "title": "Table as Thought: Exploring Structured Thoughts in LLM Reasoning",
      "title_zh": "Table as Thought：探索 LLM 推理中的结构化思考",
      "authors": [
        "Zhenjie Sun",
        "Naihao Deng",
        "Haofei Yu",
        "Jiaxuan You"
      ],
      "abstract": "Large language models' reasoning abilities benefit from methods that organize\ntheir thought processes, such as chain-of-thought prompting, which employs a\nsequential structure to guide the reasoning process step-by-step. However,\nexisting approaches focus primarily on organizing the sequence of thoughts,\nleaving structure in individual thought steps underexplored. To address this\ngap, we propose Table as Thought, a framework inspired by cognitive\nneuroscience theories on human thought. Table as Thought organizes reasoning\nwithin a tabular schema, where rows represent sequential thought steps and\ncolumns capture critical constraints and contextual information to enhance\nreasoning. The reasoning process iteratively populates the table until\nself-verification ensures completeness and correctness. Our experiments show\nthat Table as Thought excels in planning tasks and demonstrates a strong\npotential for enhancing LLM performance in mathematical reasoning compared to\nunstructured thought baselines. This work provides a novel exploration of\nrefining thought representation within LLMs, paving the way for advancements in\nreasoning and AI cognition.",
      "tldr_zh": "本文提出 Table as Thought 框架，用于探索大型语言模型(LLM)推理中的结构化思想，旨在弥补现有方法如 Chain-of-Thought 仅关注顺序结构而忽略单个思想步骤内部结构的不足。该框架受认知神经科学启发，将推理组织成表格形式，其中行表示顺序思想步骤，列捕捉关键约束和上下文信息，并通过迭代填充表格和自我验证确保完整性与正确性。实验结果显示，该框架在规划任务中表现出色，并在数学推理中比无结构思想基线显著提升LLM性能。这为优化LLM的思考表示提供了新途径，推动了推理和AI认知的进步。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02152v1",
      "published_date": "2025-01-04 00:58:06 UTC",
      "updated_date": "2025-01-04 00:58:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:32:28.702312"
    },
    {
      "arxiv_id": "2501.02149v1",
      "title": "Attribute-Based Robotic Grasping with Data-Efficient Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Yang",
        "Houjian Yu",
        "Xibai Lou",
        "Yuanhao Liu",
        "Changhyun Choi"
      ],
      "abstract": "Robotic grasping is one of the most fundamental robotic manipulation tasks\nand has been the subject of extensive research. However, swiftly teaching a\nrobot to grasp a novel target object in clutter remains challenging. This paper\nattempts to address the challenge by leveraging object attributes that\nfacilitate recognition, grasping, and rapid adaptation to new domains. In this\nwork, we present an end-to-end encoder-decoder network to learn attribute-based\nrobotic grasping with data-efficient adaptation capability. We first pre-train\nthe end-to-end model with a variety of basic objects to learn generic attribute\nrepresentation for recognition and grasping. Our approach fuses the embeddings\nof a workspace image and a query text using a gated-attention mechanism and\nlearns to predict instance grasping affordances. To train the joint embedding\nspace of visual and textual attributes, the robot utilizes object persistence\nbefore and after grasping. Our model is self-supervised in a simulation that\nonly uses basic objects of various colors and shapes but generalizes to novel\nobjects in new environments. To further facilitate generalization, we propose\ntwo adaptation methods, adversarial adaption and one-grasp adaptation.\nAdversarial adaptation regulates the image encoder using augmented data of\nunlabeled images, whereas one-grasp adaptation updates the overall end-to-end\nmodel using augmented data from one grasp trial. Both adaptation methods are\ndata-efficient and considerably improve instance grasping performance.\nExperimental results in both simulation and the real world demonstrate that our\napproach achieves over 81% instance grasping success rate on unknown objects,\nwhich outperforms several baselines by large margins.",
      "tldr_zh": "这篇论文提出了一种基于对象属性的机器人抓取方法，使用端到端的 encoder-decoder 网络，实现数据高效的适应能力，以快速处理新对象在杂乱环境中的抓取挑战。模型通过预训练学习通用属性表示，并采用 gated-attention 机制融合工作空间图像和查询文本的嵌入，在模拟环境中利用自监督训练进行优化。论文引入了两种适应方法：adversarial adaptation（使用增强的未标注图像调节图像编码器）和 one-grasp adaptation（基于一次抓取试验的增强数据更新整个模型），显著提高了抓取性能。实验结果显示，该方法在模拟和真实世界中对未知对象的实例抓取成功率超过81%，远超基线模型。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "Project page: https://z.umn.edu/attr-grasp. arXiv admin note:\n  substantial text overlap with arXiv:2104.02271",
      "pdf_url": "http://arxiv.org/pdf/2501.02149v1",
      "published_date": "2025-01-04 00:37:17 UTC",
      "updated_date": "2025-01-04 00:37:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:32:41.498935"
    },
    {
      "arxiv_id": "2501.02146v2",
      "title": "Plasma-CycleGAN: Plasma Biomarker-Guided MRI to PET Cross-modality Translation Using Conditional CycleGAN",
      "title_zh": "翻译失败",
      "authors": [
        "Yanxi Chen",
        "Yi Su",
        "Celine Dumitrascu",
        "Kewei Chen",
        "David Weidman",
        "Richard J Caselli",
        "Nicholas Ashton",
        "Eric M Reiman",
        "Yalin Wang"
      ],
      "abstract": "Cross-modality translation between MRI and PET imaging is challenging due to\nthe distinct mechanisms underlying these modalities. Blood-based biomarkers\n(BBBMs) are revolutionizing Alzheimer's disease (AD) detection by identifying\npatients and quantifying brain amyloid levels. However, the potential of BBBMs\nto enhance PET image synthesis remains unexplored. In this paper, we performed\na thorough study on the effect of incorporating BBBM into deep generative\nmodels. By evaluating three widely used cross-modality translation models, we\nfound that BBBMs integration consistently enhances the generative quality\nacross all models. By visual inspection of the generated results, we observed\nthat PET images generated by CycleGAN exhibit the best visual fidelity. Based\non these findings, we propose Plasma-CycleGAN, a novel generative model based\non CycleGAN, to synthesize PET images from MRI using BBBMs as conditions. This\nis the first approach to integrate BBBMs in conditional cross-modality\ntranslation between MRI and PET.",
      "tldr_zh": "该研究探讨了使用血基生物标志物 (BBBMs) 来提升 MRI 到 PET 的跨模态图像翻译，针对 Alzheimer 病检测的挑战。实验评估了三种常用生成模型，发现 BBBMs 的整合显著提高了图像生成质量，其中 CycleGAN 在视觉保真度上表现最佳。基于此，论文提出 Plasma-CycleGAN，一种新型条件 CycleGAN 模型，利用 BBBMs 作为条件从 MRI 合成 PET 图像，这是首次将 BBBMs 应用于 MRI 和 PET 之间的条件跨模态翻译。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ISBI 2025",
      "pdf_url": "http://arxiv.org/pdf/2501.02146v2",
      "published_date": "2025-01-04 00:20:25 UTC",
      "updated_date": "2025-01-24 19:51:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:32:52.339706"
    },
    {
      "arxiv_id": "2501.02144v1",
      "title": "Establishing baselines for generative discovery of inorganic crystals",
      "title_zh": "为无机晶体生成式发现建立基线",
      "authors": [
        "Nathan J. Szymanski",
        "Christopher J. Bartel"
      ],
      "abstract": "Generative artificial intelligence offers a promising avenue for materials\ndiscovery, yet its advantages over traditional methods remain unclear. In this\nwork, we introduce and benchmark two baseline approaches - random enumeration\nof charge-balanced prototypes and data-driven ion exchange of known compounds -\nagainst three generative models: a variational autoencoder, a large language\nmodel, and a diffusion model. Our results show that established methods such as\nion exchange perform comparably well in generating stable materials, although\nmany of these materials tend to closely resemble known compounds. In contrast,\ngenerative models excel at proposing novel structural frameworks and, when\nsufficient training data is available, can more effectively target properties\nsuch as electronic band gap and bulk modulus while maintaining a high stability\nrate. To enhance the performance of both the baseline and generative\napproaches, we implement a post-generation screening step in which all proposed\nstructures are passed through stability and property filters from pre-trained\nmachine learning models including universal interatomic potentials. This\nlow-cost filtering step leads to substantial improvement in the success rates\nof all methods, remains computationally efficient, and ultimately provides a\npractical pathway toward more effective generative strategies for materials\ndiscovery.",
      "tldr_zh": "这篇论文建立了两种基线方法——随机枚举带电平衡原型和数据驱动的离子交换——来与生成模型（如变分自编码器、large language model 和 diffusion model）比较无机晶体的生成性能。结果显示，传统基线方法在生成稳定材料方面表现相当出色，但生成的材料通常与已知化合物高度相似，而生成模型更擅长提出新颖结构框架，并在有充足训练数据时更好地针对电子 band gap 和 bulk modulus 等属性，同时保持高稳定性。通过引入后生成筛选步骤，使用预训练机器学习模型（如universal interatomic potentials）过滤结构，该方法显著提高了所有方法的成功率，并为材料发现提供了高效的生成策略。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.chem-ph"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2501.02144v1",
      "published_date": "2025-01-04 00:14:59 UTC",
      "updated_date": "2025-01-04 00:14:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T20:33:05.402113"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 42,
  "processed_papers_count": 42,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T20:33:20.619457"
}
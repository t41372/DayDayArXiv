[
  {
    "arxiv_id": "2409.13945v1",
    "title": "PureDiffusion: Using Backdoor to Counter Backdoor in Generative Diffusion Models",
    "authors": [
      "Vu Tuan Truong",
      "Long Bao Le"
    ],
    "abstract": "Diffusion models (DMs) are advanced deep learning models that achieved\nstate-of-the-art capability on a wide range of generative tasks. However,\nrecent studies have shown their vulnerability regarding backdoor attacks, in\nwhich backdoored DMs consistently generate a designated result (e.g., a harmful\nimage) called backdoor target when the models' input contains a backdoor\ntrigger. Although various backdoor techniques have been investigated to attack\nDMs, defense methods against these threats are still limited and underexplored,\nespecially in inverting the backdoor trigger. In this paper, we introduce\nPureDiffusion, a novel backdoor defense framework that can efficiently detect\nbackdoor attacks by inverting backdoor triggers embedded in DMs. Our extensive\nexperiments on various trigger-target pairs show that PureDiffusion outperforms\nexisting defense methods with a large gap in terms of fidelity (i.e., how much\nthe inverted trigger resembles the original trigger) and backdoor success rate\n(i.e., the rate that the inverted trigger leads to the corresponding backdoor\ntarget). Notably, in certain cases, backdoor triggers inverted by PureDiffusion\neven achieve higher attack success rate than the original triggers.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13945v1",
    "published_date": "2024-09-20 23:19:26 UTC",
    "updated_date": "2024-09-20 23:19:26 UTC"
  },
  {
    "arxiv_id": "2409.13941v2",
    "title": "TalkMosaic: Interactive PhotoMosaic with Multi-modal LLM Q&A Interactions",
    "authors": [
      "Kevin Li",
      "Fulu Li"
    ],
    "abstract": "We use images of cars of a wide range of varieties to compose an image of an\nanimal such as a bird or a lion for the theme of environmental protection to\nmaximize the information about cars in a single composed image and to raise the\nawareness about environmental challenges. We present a novel way of image\ninteraction with an artistically-composed photomosaic image, in which a simple\noperation of \"click and display\" is used to demonstrate the interactive switch\nbetween a tile image in a photomosaic image and the corresponding original car\nimage, which will be automatically saved on the Desktop. We build a multimodal\ncustom GPT named TalkMosaic by incorporating car images information and the\nrelated knowledge to ChatGPT. By uploading the original car image to\nTalkMosaic, we can ask questions about the given car image and get the\ncorresponding answers efficiently and effectively such as where to buy the tire\nin the car image that satisfies high environmental standards. We give an\nin-depth analysis on how to speed up the inference of multimodal LLM using\nsparse attention and quantization techniques with presented probabilistic\nFlashAttention (PrFlashAttention) and Staircase Adaptive Quantization (SAQ)\nmethods. The implemented prototype demonstrates the feasibility and\neffectiveness of the presented approach.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13941v2",
    "published_date": "2024-09-20 23:04:21 UTC",
    "updated_date": "2024-11-06 05:05:12 UTC"
  },
  {
    "arxiv_id": "2409.13940v1",
    "title": "Learning Recourse Costs from Pairwise Feature Comparisons",
    "authors": [
      "Kaivalya Rawal",
      "Himabindu Lakkaraju"
    ],
    "abstract": "This paper presents a novel technique for incorporating user input when\nlearning and inferring user preferences. When trying to provide users of\nblack-box machine learning models with actionable recourse, we often wish to\nincorporate their personal preferences about the ease of modifying each\nindividual feature. These recourse finding algorithms usually require an\nexhaustive set of tuples associating each feature to its cost of modification.\nSince it is hard to obtain such costs by directly surveying humans, in this\npaper, we propose the use of the Bradley-Terry model to automatically infer\nfeature-wise costs using non-exhaustive human comparison surveys. We propose\nthat users only provide inputs comparing entire recourses, with all candidate\nfeature modifications, determining which recourses are easier to implement\nrelative to others, without explicit quantification of their costs. We\ndemonstrate the efficient learning of individual feature costs using MAP\nestimates, and show that these non-exhaustive human surveys, which do not\nnecessarily contain data for each feature pair comparison, are sufficient to\nlearn an exhaustive set of feature costs, where each feature is associated with\na modification cost.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "\"Recourse for Humans\", paper 49 from the Participatory Approaches to\n  Machine Learning workshop at the International Conference on Machine Learning\n  (ICML) 2020. For workshop website, see https://participatoryml.github.io/#49",
    "pdf_url": "http://arxiv.org/pdf/2409.13940v1",
    "published_date": "2024-09-20 23:04:08 UTC",
    "updated_date": "2024-09-20 23:04:08 UTC"
  },
  {
    "arxiv_id": "2409.13939v1",
    "title": "Simple Unsupervised Knowledge Distillation With Space Similarity",
    "authors": [
      "Aditya Singh",
      "Haohan Wang"
    ],
    "abstract": "As per recent studies, Self-supervised learning (SSL) does not readily extend\nto smaller architectures. One direction to mitigate this shortcoming while\nsimultaneously training a smaller network without labels is to adopt\nunsupervised knowledge distillation (UKD). Existing UKD approaches handcraft\npreservation worthy inter/intra sample relationships between the teacher and\nits student. However, this may overlook/ignore other key relationships present\nin the mapping of a teacher. In this paper, instead of heuristically\nconstructing preservation worthy relationships between samples, we directly\nmotivate the student to model the teacher's embedding manifold. If the mapped\nmanifold is similar, all inter/intra sample relationships are indirectly\nconserved. We first demonstrate that prior methods cannot preserve teacher's\nlatent manifold due to their sole reliance on $L_2$ normalised embedding\nfeatures. Subsequently, we propose a simple objective to capture the lost\ninformation due to normalisation. Our proposed loss component, termed\n\\textbf{space similarity}, motivates each dimension of a student's feature\nspace to be similar to the corresponding dimension of its teacher. We perform\nextensive experiments demonstrating strong performance of our proposed approach\non various benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13939v1",
    "published_date": "2024-09-20 22:54:39 UTC",
    "updated_date": "2024-09-20 22:54:39 UTC"
  },
  {
    "arxiv_id": "2409.13935v2",
    "title": "MirrorStories: Reflecting Diversity through Personalized Narrative Generation with Large Language Models",
    "authors": [
      "Sarfaroz Yunusov",
      "Hamza Sidat",
      "Ali Emami"
    ],
    "abstract": "This study explores the effectiveness of Large Language Models (LLMs) in\ncreating personalized \"mirror stories\" that reflect and resonate with\nindividual readers' identities, addressing the significant lack of diversity in\nliterature. We present MirrorStories, a corpus of 1,500 personalized short\nstories generated by integrating elements such as name, gender, age, ethnicity,\nreader interest, and story moral. We demonstrate that LLMs can effectively\nincorporate diverse identity elements into narratives, with human evaluators\nidentifying personalized elements in the stories with high accuracy. Through a\ncomprehensive evaluation involving 26 diverse human judges, we compare the\neffectiveness of MirrorStories against generic narratives. We find that\npersonalized LLM-generated stories not only outscore generic human-written and\nLLM-generated ones across all metrics of engagement (with average ratings of\n4.22 versus 3.37 on a 5-point scale), but also achieve higher textual diversity\nwhile preserving the intended moral. We also provide analyses that include bias\nassessments and a study on the potential for integrating images into\npersonalized stories.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages (excluding references), accepted to EMNLP 2024 Main\n  Conference",
    "pdf_url": "http://arxiv.org/pdf/2409.13935v2",
    "published_date": "2024-09-20 22:43:13 UTC",
    "updated_date": "2024-09-24 01:30:14 UTC"
  },
  {
    "arxiv_id": "2409.13929v1",
    "title": "Failures in Perspective-taking of Multimodal AI Systems",
    "authors": [
      "Bridget Leonard",
      "Kristin Woodard",
      "Scott O. Murray"
    ],
    "abstract": "This study extends previous research on spatial representations in multimodal\nAI systems. Although current models demonstrate a rich understanding of spatial\ninformation from images, this information is rooted in propositional\nrepresentations, which differ from the analog representations employed in human\nand animal spatial cognition. To further explore these limitations, we apply\ntechniques from cognitive and developmental science to assess the\nperspective-taking abilities of GPT-4o. Our analysis enables a comparison\nbetween the cognitive development of the human brain and that of multimodal AI,\noffering guidance for future research and model development.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13929v1",
    "published_date": "2024-09-20 22:31:46 UTC",
    "updated_date": "2024-09-20 22:31:46 UTC"
  },
  {
    "arxiv_id": "2409.13928v1",
    "title": "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation",
    "authors": [
      "Seonghyeon Lee",
      "Suyeon Kim",
      "Joonwon Jang",
      "Heejae Chon",
      "Dongha Lee",
      "Hwanjo Yu"
    ],
    "abstract": "We study the code generation behavior of instruction-tuned models built on\ntop of code pre-trained language models when they could access an auxiliary\nfunction to implement a function. We design several ways to provide auxiliary\nfunctions to the models by adding them to the query or providing a response\nprefix to incorporate the ability to utilize auxiliary functions with the\ninstruction-following capability. Our experimental results show the\neffectiveness of combining the base models' auxiliary function utilization\nability with the instruction following ability. In particular, the performance\nof adopting our approaches with the open-sourced language models surpasses that\nof the recent powerful proprietary language models, i.e., gpt-4o.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "EMNLP 2024 Findings Short",
    "pdf_url": "http://arxiv.org/pdf/2409.13928v1",
    "published_date": "2024-09-20 22:28:20 UTC",
    "updated_date": "2024-09-20 22:28:20 UTC"
  },
  {
    "arxiv_id": "2409.13926v1",
    "title": "SpaceBlender: Creating Context-Rich Collaborative Spaces Through Generative 3D Scene Blending",
    "authors": [
      "Nels Numan",
      "Shwetha Rajaram",
      "Balasaravanan Thoravi Kumaravel",
      "Nicolai Marquardt",
      "Andrew D. Wilson"
    ],
    "abstract": "There is increased interest in using generative AI to create 3D spaces for\nVirtual Reality (VR) applications. However, today's models produce artificial\nenvironments, falling short of supporting collaborative tasks that benefit from\nincorporating the user's physical context. To generate environments that\nsupport VR telepresence, we introduce SpaceBlender, a novel pipeline that\nutilizes generative AI techniques to blend users' physical surroundings into\nunified virtual spaces. This pipeline transforms user-provided 2D images into\ncontext-rich 3D environments through an iterative process consisting of depth\nestimation, mesh alignment, and diffusion-based space completion guided by\ngeometric priors and adaptive text prompts. In a preliminary within-subjects\nstudy, where 20 participants performed a collaborative VR affinity diagramming\ntask in pairs, we compared SpaceBlender with a generic virtual environment and\na state-of-the-art scene generation framework, evaluating its ability to create\nvirtual spaces suitable for collaboration. Participants appreciated the\nenhanced familiarity and context provided by SpaceBlender but also noted\ncomplexities in the generative environments that could detract from task focus.\nDrawing on participant feedback, we propose directions for improving the\npipeline and discuss the value and design of blended spaces for different\nscenarios.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13926v1",
    "published_date": "2024-09-20 22:27:31 UTC",
    "updated_date": "2024-09-20 22:27:31 UTC"
  },
  {
    "arxiv_id": "2409.13919v2",
    "title": "Measuring Error Alignment for Decision-Making Systems",
    "authors": [
      "Binxia Xu",
      "Antonis Bikakis",
      "Daniel Onah",
      "Andreas Vlachidis",
      "Luke Dickens"
    ],
    "abstract": "Given that AI systems are set to play a pivotal role in future\ndecision-making processes, their trustworthiness and reliability are of\ncritical concern. Due to their scale and complexity, modern AI systems resist\ndirect interpretation, and alternative ways are needed to establish trust in\nthose systems, and determine how well they align with human values. We argue\nthat good measures of the information processing similarities between AI and\nhumans, may be able to achieve these same ends. While Representational\nalignment (RA) approaches measure similarity between the internal states of two\nsystems, the associated data can be expensive and difficult to collect for\nhuman systems. In contrast, Behavioural alignment (BA) comparisons are cheaper\nand easier, but questions remain as to their sensitivity and reliability. We\npropose two new behavioural alignment metrics misclassification agreement which\nmeasures the similarity between the errors of two systems on the same\ninstances, and class-level error similarity which measures the similarity\nbetween the error distributions of two systems. We show that our metrics\ncorrelate well with RA metrics, and provide complementary information to\nanother BA metric, within a range of domains, and set the scene for a new\napproach to value alignment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13919v2",
    "published_date": "2024-09-20 21:59:13 UTC",
    "updated_date": "2024-12-31 15:11:31 UTC"
  },
  {
    "arxiv_id": "2409.13908v2",
    "title": "Nonlinear Inverse Design of Mechanical Multi-Material Metamaterials Enabled by Video Denoising Diffusion and Structure Identifier",
    "authors": [
      "Jaewan Park",
      "Shashank Kushwaha",
      "Junyan He",
      "Seid Koric",
      "Qibang Liu",
      "Iwona Jasiuk",
      "Diab Abueidda"
    ],
    "abstract": "Metamaterials, synthetic materials with customized properties, have emerged\nas a promising field due to advancements in additive manufacturing. These\nmaterials derive unique mechanical properties from their internal lattice\nstructures, which are often composed of multiple materials that repeat\ngeometric patterns. While traditional inverse design approaches have shown\npotential, they struggle to map nonlinear material behavior to multiple\npossible structural configurations. This paper presents a novel framework\nleveraging video diffusion models, a type of generative artificial Intelligence\n(AI), for inverse multi-material design based on nonlinear stress-strain\nresponses. Our approach consists of two key components: (1) a fields generator\nusing a video diffusion model to create solution fields based on target\nnonlinear stress-strain responses, and (2) a structure identifier employing two\nUNet models to determine the corresponding multi-material 2D design. By\nincorporating multiple materials, plasticity, and large deformation, our\ninnovative design method allows for enhanced control over the highly nonlinear\nmechanical behavior of metamaterials commonly seen in real-world applications.\nIt offers a promising solution for generating next-generation metamaterials\nwith finely tuned mechanical characteristics.",
    "categories": [
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13908v2",
    "published_date": "2024-09-20 21:26:15 UTC",
    "updated_date": "2024-09-28 20:15:48 UTC"
  },
  {
    "arxiv_id": "2409.13903v1",
    "title": "CI-Bench: Benchmarking Contextual Integrity of AI Assistants on Synthetic Data",
    "authors": [
      "Zhao Cheng",
      "Diane Wan",
      "Matthew Abueg",
      "Sahra Ghalebikesabi",
      "Ren Yi",
      "Eugene Bagdasarian",
      "Borja Balle",
      "Stefan Mellem",
      "Shawn O'Banion"
    ],
    "abstract": "Advances in generative AI point towards a new era of personalized\napplications that perform diverse tasks on behalf of users. While general AI\nassistants have yet to fully emerge, their potential to share personal data\nraises significant privacy challenges. This paper introduces CI-Bench, a\ncomprehensive synthetic benchmark for evaluating the ability of AI assistants\nto protect personal information during model inference. Leveraging the\nContextual Integrity framework, our benchmark enables systematic assessment of\ninformation flow across important context dimensions, including roles,\ninformation types, and transmission principles. We present a novel, scalable,\nmulti-step synthetic data pipeline for generating natural communications,\nincluding dialogues and emails. Unlike previous work with smaller, narrowly\nfocused evaluations, we present a novel, scalable, multi-step data pipeline\nthat synthetically generates natural communications, including dialogues and\nemails, which we use to generate 44 thousand test samples across eight domains.\nAdditionally, we formulate and evaluate a naive AI assistant to demonstrate the\nneed for further study and careful training towards personal assistant tasks.\nWe envision CI-Bench as a valuable tool for guiding future language model\ndevelopment, deployment, system design, and dataset construction, ultimately\ncontributing to the development of AI assistants that align with users' privacy\nexpectations.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13903v1",
    "published_date": "2024-09-20 21:14:36 UTC",
    "updated_date": "2024-09-20 21:14:36 UTC"
  },
  {
    "arxiv_id": "2409.13902v1",
    "title": "Enhancing Large Language Models with Domain-specific Retrieval Augment Generation: A Case Study on Long-form Consumer Health Question Answering in Ophthalmology",
    "authors": [
      "Aidan Gilson",
      "Xuguang Ai",
      "Thilaka Arunachalam",
      "Ziyou Chen",
      "Ki Xiong Cheong",
      "Amisha Dave",
      "Cameron Duic",
      "Mercy Kibe",
      "Annette Kaminaka",
      "Minali Prasad",
      "Fares Siddig",
      "Maxwell Singer",
      "Wendy Wong",
      "Qiao Jin",
      "Tiarnan D. L. Keenan",
      "Xia Hu",
      "Emily Y. Chew",
      "Zhiyong Lu",
      "Hua Xu",
      "Ron A. Adelman",
      "Yih-Chung Tham",
      "Qingyu Chen"
    ],
    "abstract": "Despite the potential of Large Language Models (LLMs) in medicine, they may\ngenerate responses lacking supporting evidence or based on hallucinated\nevidence. While Retrieval Augment Generation (RAG) is popular to address this\nissue, few studies implemented and evaluated RAG in downstream domain-specific\napplications. We developed a RAG pipeline with 70,000 ophthalmology-specific\ndocuments that retrieve relevant documents to augment LLMs during inference\ntime. In a case study on long-form consumer health questions, we systematically\nevaluated the responses including over 500 references of LLMs with and without\nRAG on 100 questions with 10 healthcare professionals. The evaluation focuses\non factuality of evidence, selection and ranking of evidence, attribution of\nevidence, and answer accuracy and completeness. LLMs without RAG provided 252\nreferences in total. Of which, 45.3% hallucinated, 34.1% consisted of minor\nerrors, and 20.6% were correct. In contrast, LLMs with RAG significantly\nimproved accuracy (54.5% being correct) and reduced error rates (18.8% with\nminor hallucinations and 26.7% with errors). 62.5% of the top 10 documents\nretrieved by RAG were selected as the top references in the LLM response, with\nan average ranking of 4.9. The use of RAG also improved evidence attribution\n(increasing from 1.85 to 2.49 on a 5-point scale, P<0.001), albeit with slight\ndecreases in accuracy (from 3.52 to 3.23, P=0.03) and completeness (from 3.47\nto 3.27, P=0.17). The results demonstrate that LLMs frequently exhibited\nhallucinated and erroneous evidence in the responses, raising concerns for\ndownstream applications in the medical domain. RAG substantially reduced the\nproportion of such evidence but encountered challenges.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13902v1",
    "published_date": "2024-09-20 21:06:00 UTC",
    "updated_date": "2024-09-20 21:06:00 UTC"
  },
  {
    "arxiv_id": "2409.13897v1",
    "title": "LLM for Everyone: Representing the Underrepresented in Large Language Models",
    "authors": [
      "Samuel Cahyawijaya"
    ],
    "abstract": "Natural language processing (NLP) has witnessed a profound impact of large\nlanguage models (LLMs) that excel in a multitude of tasks. However, the\nlimitation of LLMs in multilingual settings, particularly in underrepresented\nlanguages, remains a significant hurdle. This thesis aims to bridge the gap in\nNLP research and development by focusing on underrepresented languages. A\ncomprehensive evaluation of LLMs is conducted to assess their capabilities in\nthese languages, revealing the challenges of multilingual and multicultural\ngeneralization. Addressing the multilingual generalization gap, this thesis\nproposes data-and-compute-efficient methods to mitigate the disparity in LLM\nability in underrepresented languages, allowing better generalization on\nunderrepresented languages without the loss of task generalization ability. The\nproposed solutions cover cross-lingual continual instruction tuning,\nretrieval-based cross-lingual in-context learning, and in-context query\nalignment. Furthermore, a novel method to measure cultural values alignment\nbetween LLMs operating in different languages is proposed, ensuring cultural\nsensitivity and inclusivity. These contributions aim to enhance the\nmultilingual and multicultural alignment of LLMs in underrepresented languages,\nultimately advancing the NLP field toward greater equality and inclusiveness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "PhD thesis",
    "pdf_url": "http://arxiv.org/pdf/2409.13897v1",
    "published_date": "2024-09-20 20:53:22 UTC",
    "updated_date": "2024-09-20 20:53:22 UTC"
  },
  {
    "arxiv_id": "2410.07124v1",
    "title": "Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation",
    "authors": [
      "Adrian Galdran"
    ],
    "abstract": "This short abstract describes a solution to the COSAS 2024 competition on\nCross-Organ and Cross-Scanner Adenocarcinoma Segmentation from\nhistopathological image patches. The main challenge in the task of segmenting\nthis type of cancer is a noticeable domain shift encountered when changing\nacquisition devices (microscopes) and also when tissue comes from different\norgans. The two tasks proposed in COSAS were to train on a dataset of images\nfrom three different organs, and then predict segmentations on data from unseen\norgans (dataset T1), and to train on a dataset of images acquired on three\ndifferent scanners and then segment images acquired with another unseen\nmicroscope. We attempted to bridge the domain shift gap by experimenting with\nthree different strategies: standard training for each dataset, pretraining on\ndataset T1 and then fine-tuning on dataset T2 (and vice-versa, a strategy we\ncall \\textit{Cross-Task Pretraining}), and training on the combination of\ndataset A and B. Our experiments showed that Cross-Task Pre-training is a more\npromising approach to domain generalization.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "MICCAI2024 COSAS Challenge - short abstract",
    "pdf_url": "http://arxiv.org/pdf/2410.07124v1",
    "published_date": "2024-09-20 20:52:20 UTC",
    "updated_date": "2024-09-20 20:52:20 UTC"
  },
  {
    "arxiv_id": "2409.13886v1",
    "title": "Learning to Play Video Games with Intuitive Physics Priors",
    "authors": [
      "Abhishek Jaiswal",
      "Nisheeth Srivastava"
    ],
    "abstract": "Video game playing is an extremely structured domain where algorithmic\ndecision-making can be tested without adverse real-world consequences. While\nprevailing methods rely on image inputs to avoid the problem of hand-crafting\nstate space representations, this approach systematically diverges from the way\nhumans actually learn to play games. In this paper, we design object-based\ninput representations that generalize well across a number of video games.\nUsing these representations, we evaluate an agent's ability to learn games\nsimilar to an infant - with limited world experience, employing simple\ninductive biases derived from intuitive representations of physics from the\nreal world. Using such biases, we construct an object category representation\nto be used by a Q-learning algorithm and assess how well it learns to play\nmultiple games based on observed object affordances. Our results suggest that a\nhuman-like object interaction setup capably learns to play several video games,\nand demonstrates superior generalizability, particularly for unfamiliar\nobjects. Further exploring such methods will allow machines to learn in a\nhuman-centric way, thus incorporating more human-like learning benefits.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, Accepted in Proceedings of the Annual Meeting of the\n  Cognitive Science Society, Volume 46",
    "pdf_url": "http://arxiv.org/pdf/2409.13886v1",
    "published_date": "2024-09-20 20:30:27 UTC",
    "updated_date": "2024-09-20 20:30:27 UTC"
  },
  {
    "arxiv_id": "2409.13884v1",
    "title": "A Multi-LLM Debiasing Framework",
    "authors": [
      "Deonna M. Owens",
      "Ryan A. Rossi",
      "Sungchul Kim",
      "Tong Yu",
      "Franck Dernoncourt",
      "Xiang Chen",
      "Ruiyi Zhang",
      "Jiuxiang Gu",
      "Hanieh Deilamsalehy",
      "Nedim Lipka"
    ],
    "abstract": "Large Language Models (LLMs) are powerful tools with the potential to benefit\nsociety immensely, yet, they have demonstrated biases that perpetuate societal\ninequalities. Despite significant advancements in bias mitigation techniques\nusing data augmentation, zero-shot prompting, and model fine-tuning, biases\ncontinuously persist, including subtle biases that may elude human detection.\nRecent research has shown a growing interest in multi-LLM approaches, which\nhave been demonstrated to be effective in improving the quality of reasoning\nand factuality in LLMs. Building on this approach, we propose a novel multi-LLM\ndebiasing framework aimed at reducing bias in LLMs. Our work is the first to\nintroduce and evaluate two distinct approaches within this framework for\ndebiasing LLMs: a centralized method, where the conversation is facilitated by\na single central LLM, and a decentralized method, where all models communicate\ndirectly. Our findings reveal that our multi-LLM framework significantly\nreduces bias in LLMs, outperforming the baseline method across several social\ngroups.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13884v1",
    "published_date": "2024-09-20 20:24:50 UTC",
    "updated_date": "2024-09-20 20:24:50 UTC"
  },
  {
    "arxiv_id": "2409.13882v2",
    "title": "Tabular Data Generation using Binary Diffusion",
    "authors": [
      "Vitaliy Kinakh",
      "Slava Voloshynovskiy"
    ],
    "abstract": "Generating synthetic tabular data is critical in machine learning, especially\nwhen real data is limited or sensitive. Traditional generative models often\nface challenges due to the unique characteristics of tabular data, such as\nmixed data types and varied distributions, and require complex preprocessing or\nlarge pretrained models. In this paper, we introduce a novel, lossless binary\ntransformation method that converts any tabular data into fixed-size binary\nrepresentations, and a corresponding new generative model called Binary\nDiffusion, specifically designed for binary data. Binary Diffusion leverages\nthe simplicity of XOR operations for noise addition and removal and employs\nbinary cross-entropy loss for training. Our approach eliminates the need for\nextensive preprocessing, complex noise parameter tuning, and pretraining on\nlarge datasets. We evaluate our model on several popular tabular benchmark\ndatasets, demonstrating that Binary Diffusion outperforms existing\nstate-of-the-art models on Travel, Adult Income, and Diabetes datasets while\nbeing significantly smaller in size. Code and models are available at:\nhttps://github.com/vkinakh/binary-diffusion-tabular",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to 3rd Table Representation Learning Workshop @ NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.13882v2",
    "published_date": "2024-09-20 20:22:28 UTC",
    "updated_date": "2024-10-28 22:48:54 UTC"
  },
  {
    "arxiv_id": "2409.13870v3",
    "title": "Instruct-Tuning Pretrained Causal Language Models for Ancient Greek Papyrology and Epigraphy",
    "authors": [
      "Eric Cullhed"
    ],
    "abstract": "This article presents an experiment in fine-tuning a pretrained causal\nlanguage model (Meta's Llama 3.1 8B Instruct) to assist with restoring missing\nor illegible characters in ancient Greek inscriptions and documentary papyri.\nUtilizing a straightforward instruction-based approach and a 95%/5% train/test\nsplit, the papyrus restoration model achieved a character error rate (CER) of\n14.9%, a top-1 accuracy of 73.5%, and a top-20 accuracy of 86.0% for sequences\nup to 10 characters. A model was also fine-tuned for geographic attribution,\nreaching a top-1 accuracy of 66.4% and a top-3 accuracy of 79.9%. In\nchronological attribution, it demonstrated an average deviation of 21.7 years\nfrom the actual terminus post/ante quem, with a median deviation of 0 years.\nFor inscriptions, the restoration model achieved a CER of 20.5%, a top-1\naccuracy of 63.7%, and a top-20 accuracy of 83.0% for sequences up to 10\ncharacters. In geographic attribution, it attained a top-1 accuracy of 75.0%\nand a top-3 accuracy of 83.7%, while in dating, it had an average deviation of\n37.1 years and a median deviation of 3 years from the actual date range.\nBenchmarked against the state-of-the-art model (Ithaca) on a shared test set\nand on recently edited inscriptions, the instruction-tuned models excelled in\ntext restoration, while also offering the practical advantage of ignoring\nspaces during reconstruction, which aligns with the scriptio continua of\nancient textual artifacts. However, their performance in geographic and\nchronological attribution was lower than Ithaca's. To evaluate the approach in\na more even setup, the instruction model was retrained with an 80%/10%/10%\ntrain-validation-test split, and still outperformed Ithaca in text restoration.\nThe results suggest that fine-tuning larger pretrained causal language models\nusing instruction templates for emendations and conjectures to ancient texts\nholds promise.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages, 1 table. To be submitted",
    "pdf_url": "http://arxiv.org/pdf/2409.13870v3",
    "published_date": "2024-09-20 19:49:45 UTC",
    "updated_date": "2024-11-17 21:28:01 UTC"
  },
  {
    "arxiv_id": "2409.13869v1",
    "title": "Generative AI Carries Non-Democratic Biases and Stereotypes: Representation of Women, Black Individuals, Age Groups, and People with Disability in AI-Generated Images across Occupations",
    "authors": [
      "Ayoob Sadeghiani"
    ],
    "abstract": "AI governance and ethics in AI development have become critical concerns,\nprompting active discussions among tech companies, governments, and researchers\nabout the potential risks AI poses to our democracies. This short essay aims to\nhighlight one such risk: how generative AI includes or excludes\nequity-deserving groups in its outputs. The findings reveal that generative AI\nis not equitably inclusive regarding gender, race, age, and visible disability.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13869v1",
    "published_date": "2024-09-20 19:47:31 UTC",
    "updated_date": "2024-09-20 19:47:31 UTC"
  },
  {
    "arxiv_id": "2409.13867v2",
    "title": "MAGICS: Adversarial RL with Minimax Actors Guided by Implicit Critic Stackelberg for Convergent Neural Synthesis of Robot Safety",
    "authors": [
      "Justin Wang",
      "Haimin Hu",
      "Duy Phuong Nguyen",
      "Jaime Fernández Fisac"
    ],
    "abstract": "While robust optimal control theory provides a rigorous framework to compute\nrobot control policies that are provably safe, it struggles to scale to\nhigh-dimensional problems, leading to increased use of deep learning for\ntractable synthesis of robot safety. Unfortunately, existing neural safety\nsynthesis methods often lack convergence guarantees and solution\ninterpretability. In this paper, we present Minimax Actors Guided by Implicit\nCritic Stackelberg (MAGICS), a novel adversarial reinforcement learning (RL)\nalgorithm that guarantees local convergence to a minimax equilibrium solution.\nWe then build on this approach to provide local convergence guarantees for a\ngeneral deep RL-based robot safety synthesis algorithm. Through both simulation\nstudies on OpenAI Gym environments and hardware experiments with a\n36-dimensional quadruped robot, we show that MAGICS can yield robust control\npolicies outperforming the state-of-the-art neural safety synthesis methods.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "Algorithmic Foundations of Robotics (WAFR) XVI",
    "pdf_url": "http://arxiv.org/pdf/2409.13867v2",
    "published_date": "2024-09-20 19:45:48 UTC",
    "updated_date": "2025-04-27 01:54:58 UTC"
  },
  {
    "arxiv_id": "2409.13857v1",
    "title": "Wormhole: Concept-Aware Deep Representation Learning for Co-Evolving Sequences",
    "authors": [
      "Kunpeng Xu",
      "Lifei Chen",
      "Shengrui Wang"
    ],
    "abstract": "Identifying and understanding dynamic concepts in co-evolving sequences is\ncrucial for analyzing complex systems such as IoT applications, financial\nmarkets, and online activity logs. These concepts provide valuable insights\ninto the underlying structures and behaviors of sequential data, enabling\nbetter decision-making and forecasting. This paper introduces Wormhole, a novel\ndeep representation learning framework that is concept-aware and designed for\nco-evolving time sequences. Our model presents a self-representation layer and\na temporal smoothness constraint to ensure robust identification of dynamic\nconcepts and their transitions. Additionally, concept transitions are detected\nby identifying abrupt changes in the latent space, signifying a shift to new\nbehavior - akin to passing through a wormhole. This novel mechanism accurately\ndiscerns concepts within co-evolving sequences and pinpoints the exact\nlocations of these wormholes, enhancing the interpretability of the learned\nrepresentations. Experiments demonstrate that this method can effectively\nsegment time series data into meaningful concepts, providing a valuable tool\nfor analyzing complex temporal patterns and advancing the detection of concept\ndrifts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13857v1",
    "published_date": "2024-09-20 19:11:39 UTC",
    "updated_date": "2024-09-20 19:11:39 UTC"
  },
  {
    "arxiv_id": "2409.13854v2",
    "title": "More Consideration for the Perceptron",
    "authors": [
      "Slimane Larabi"
    ],
    "abstract": "In this paper, we introduce the gated perceptron, an enhancement of the\nconventional perceptron, which incorporates an additional input computed as the\nproduct of the existing inputs. This allows the perceptron to capture\nnon-linear interactions between features, significantly improving its ability\nto classify and regress on complex datasets. We explore its application in both\nlinear and non-linear regression tasks using the Iris dataset, as well as\nbinary and multi-class classification problems, including the PIMA Indian\ndataset and Breast Cancer Wisconsin dataset. Our results demonstrate that the\ngated perceptron can generate more distinct decision regions compared to\ntraditional perceptrons, enhancing its classification capabilities,\nparticularly in handling non-linear data. Performance comparisons show that the\ngated perceptron competes with state-of-the-art classifiers while maintaining a\nsimple architecture.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13854v2",
    "published_date": "2024-09-20 19:01:29 UTC",
    "updated_date": "2024-09-24 10:57:14 UTC"
  },
  {
    "arxiv_id": "2409.13853v1",
    "title": "Unlocking Memorization in Large Language Models with Dynamic Soft Prompting",
    "authors": [
      "Zhepeng Wang",
      "Runxue Bao",
      "Yawen Wu",
      "Jackson Taylor",
      "Cao Xiao",
      "Feng Zheng",
      "Weiwen Jiang",
      "Shangqian Gao",
      "Yanfu Zhang"
    ],
    "abstract": "Pretrained large language models (LLMs) have revolutionized natural language\nprocessing (NLP) tasks such as summarization, question answering, and\ntranslation. However, LLMs pose significant security risks due to their\ntendency to memorize training data, leading to potential privacy breaches and\ncopyright infringement. Accurate measurement of this memorization is essential\nto evaluate and mitigate these potential risks. However, previous attempts to\ncharacterize memorization are constrained by either using prefixes only or by\nprepending a constant soft prompt to the prefixes, which cannot react to\nchanges in input. To address this challenge, we propose a novel method for\nestimating LLM memorization using dynamic, prefix-dependent soft prompts. Our\napproach involves training a transformer-based generator to produce soft\nprompts that adapt to changes in input, thereby enabling more accurate\nextraction of memorized data. Our method not only addresses the limitations of\nprevious methods but also demonstrates superior performance in diverse\nexperimental settings compared to state-of-the-art techniques. In particular,\nour method can achieve the maximum relative improvement of 112.75% and 32.26%\nover the vanilla baseline in terms of discoverable memorization rate for the\ntext generation task and code generation task respectively.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13853v1",
    "published_date": "2024-09-20 18:56:32 UTC",
    "updated_date": "2024-09-20 18:56:32 UTC"
  },
  {
    "arxiv_id": "2409.13852v1",
    "title": "Do language models practice what they preach? Examining language ideologies about gendered language reform encoded in LLMs",
    "authors": [
      "Julia Watson",
      "Sophia Lee",
      "Barend Beekhuizen",
      "Suzanne Stevenson"
    ],
    "abstract": "We study language ideologies in text produced by LLMs through a case study on\nEnglish gendered language reform (related to role nouns like\ncongressperson/-woman/-man, and singular they). First, we find political bias:\nwhen asked to use language that is \"correct\" or \"natural\", LLMs use language\nmost similarly to when asked to align with conservative (vs. progressive)\nvalues. This shows how LLMs' metalinguistic preferences can implicitly\ncommunicate the language ideologies of a particular political group, even in\nseemingly non-political contexts. Second, we find LLMs exhibit internal\ninconsistency: LLMs use gender-neutral variants more often when more explicit\nmetalinguistic context is provided. This shows how the language ideologies\nexpressed in text produced by LLMs can vary, which may be unexpected to users.\nWe discuss the broader implications of these findings for value alignment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13852v1",
    "published_date": "2024-09-20 18:55:48 UTC",
    "updated_date": "2024-09-20 18:55:48 UTC"
  },
  {
    "arxiv_id": "2409.13843v2",
    "title": "STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions",
    "authors": [
      "Robert Morabito",
      "Sangmitra Madhusudan",
      "Tyler McDonald",
      "Ali Emami"
    ],
    "abstract": "Mitigating explicit and implicit biases in Large Language Models (LLMs) has\nbecome a critical focus in the field of natural language processing. However,\nmany current methodologies evaluate scenarios in isolation, without considering\nthe broader context or the spectrum of potential biases within each situation.\nTo address this, we introduce the Sensitivity Testing on Offensive Progressions\n(STOP) dataset, which includes 450 offensive progressions containing 2,700\nunique sentences of varying severity that progressively escalate from less to\nmore explicitly offensive. Covering a broad spectrum of 9 demographics and 46\nsub-demographics, STOP ensures inclusivity and comprehensive coverage. We\nevaluate several leading closed- and open-source models, including GPT-4,\nMixtral, and Llama 3. Our findings reveal that even the best-performing models\ndetect bias inconsistently, with success rates ranging from 19.3% to 69.8%. We\nalso demonstrate how aligning models with human judgments on STOP can improve\nmodel answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs\nby up to 191%, while maintaining or even improving performance. STOP presents a\nnovel framework for assessing the complex nature of biases in LLMs, which will\nenable more effective bias mitigation strategies and facilitates the creation\nof fairer language models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages (excluding references), accepted to EMNLP 2024 Main\n  Conference",
    "pdf_url": "http://arxiv.org/pdf/2409.13843v2",
    "published_date": "2024-09-20 18:34:38 UTC",
    "updated_date": "2025-02-03 18:06:34 UTC"
  },
  {
    "arxiv_id": "2409.13831v1",
    "title": "Measuring Copyright Risks of Large Language Model via Partial Information Probing",
    "authors": [
      "Weijie Zhao",
      "Huajie Shao",
      "Zhaozhuo Xu",
      "Suzhen Duan",
      "Denghui Zhang"
    ],
    "abstract": "Exploring the data sources used to train Large Language Models (LLMs) is a\ncrucial direction in investigating potential copyright infringement by these\nmodels. While this approach can identify the possible use of copyrighted\nmaterials in training data, it does not directly measure infringing risks.\nRecent research has shifted towards testing whether LLMs can directly output\ncopyrighted content. Addressing this direction, we investigate and assess LLMs'\ncapacity to generate infringing content by providing them with partial\ninformation from copyrighted materials, and try to use iterative prompting to\nget LLMs to generate more infringing content. Specifically, we input a portion\nof a copyrighted text into LLMs, prompt them to complete it, and then analyze\nthe overlap between the generated content and the original copyrighted\nmaterial. Our findings demonstrate that LLMs can indeed generate content highly\noverlapping with copyrighted materials based on these partial inputs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13831v1",
    "published_date": "2024-09-20 18:16:05 UTC",
    "updated_date": "2024-09-20 18:16:05 UTC"
  },
  {
    "arxiv_id": "2409.13825v2",
    "title": "A Personalised 3D+t Mesh Generative Model for Unveiling Normal Heart Dynamics",
    "authors": [
      "Mengyun Qiao",
      "Kathryn A McGurk",
      "Shuo Wang",
      "Paul M. Matthews",
      "Declan P O Regan",
      "Wenjia Bai"
    ],
    "abstract": "Understanding the structure and motion of the heart is crucial for diagnosing\nand managing cardiovascular diseases, the leading cause of global death. There\nis wide variation in cardiac shape and motion patterns, that are influenced by\ndemographic, anthropometric and disease factors. Unravelling the normal\npatterns of shape and motion, as well as understanding how each individual\ndeviates from the norm, would facilitate accurate diagnosis and personalised\ntreatment strategies. To this end, we developed a novel conditional generative\nmodel, MeshHeart, to learn the distribution of cardiac shape and motion\npatterns. MeshHeart is capable of generating 3D+t cardiac mesh sequences,\ntaking into account clinical factors such as age, sex, weight and height. To\nmodel the high-dimensional and complex spatio-temporal mesh data, MeshHeart\nemploys a geometric encoder to represent cardiac meshes in a latent space,\nfollowed by a temporal Transformer to model the motion dynamics of latent\nrepresentations. Based on MeshHeart, we investigate the latent space of 3D+t\ncardiac mesh sequences and propose a novel distance metric termed latent delta,\nwhich quantifies the deviation of a real heart from its personalised normative\npattern in the latent space. In experiments using a large dataset of 38,309\nsubjects, MeshHeart demonstrates a high performance in cardiac mesh sequence\nreconstruction and generation. Features defined in the latent space are highly\ndiscriminative for cardiac disease classification, whereas the latent delta\nexhibits strong correlation with clinical phenotypes in phenome-wide\nassociation studies. The codes and models of this study will be released to\nbenefit further research on digital heart modelling.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted by Nature Machine Intelligence",
    "pdf_url": "http://arxiv.org/pdf/2409.13825v2",
    "published_date": "2024-09-20 18:08:37 UTC",
    "updated_date": "2025-04-14 12:07:55 UTC"
  },
  {
    "arxiv_id": "2409.13688v1",
    "title": "Morphological Detection and Classification of Microplastics and Nanoplastics Emerged from Consumer Products by Deep Learning",
    "authors": [
      "Hadi Rezvani",
      "Navid Zarrabi",
      "Ishaan Mehta",
      "Christopher Kolios",
      "Hussein Ali Jaafar",
      "Cheng-Hao Kao",
      "Sajad Saeedi",
      "Nariman Yousefi"
    ],
    "abstract": "Plastic pollution presents an escalating global issue, impacting health and\nenvironmental systems, with micro- and nanoplastics found across mediums from\npotable water to air. Traditional methods for studying these contaminants are\nlabor-intensive and time-consuming, necessitating a shift towards more\nefficient technologies. In response, this paper introduces micro- and\nnanoplastics (MiNa), a novel and open-source dataset engineered for the\nautomatic detection and classification of micro and nanoplastics using object\ndetection algorithms. The dataset, comprising scanning electron microscopy\nimages simulated under realistic aquatic conditions, categorizes plastics by\npolymer type across a broad size spectrum. We demonstrate the application of\nstate-of-the-art detection algorithms on MiNa, assessing their effectiveness\nand identifying the unique challenges and potential of each method. The dataset\nnot only fills a critical gap in available resources for microplastic research\nbut also provides a robust foundation for future advancements in the field.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "stat.AP",
      "stat.ME"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13688v1",
    "published_date": "2024-09-20 17:56:25 UTC",
    "updated_date": "2024-09-20 17:56:25 UTC"
  },
  {
    "arxiv_id": "2409.13686v2",
    "title": "The Impact of Large Language Models in Academia: from Writing to Speaking",
    "authors": [
      "Mingmeng Geng",
      "Caixi Chen",
      "Yanru Wu",
      "Dongping Chen",
      "Yao Wan",
      "Pan Zhou"
    ],
    "abstract": "Large language models (LLMs) are increasingly impacting human society,\nparticularly in textual information. Based on more than 30,000 papers and 1,000\npresentations from machine learning conferences, we examined and compared the\nwords used in writing and speaking, representing the first large-scale study of\nhow LLMs influence the two main modes of verbal communication and expression\nwithin the same group of people. Our empirical results show that LLM-style\nwords such as \"significant\" have been used more frequently in abstracts and\noral presentations. The impact on speaking is beginning to emerge and is likely\nto grow in the future, calling attention to the implicit influence and ripple\neffect of LLMs on human society.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.DL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "23 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.13686v2",
    "published_date": "2024-09-20 17:54:16 UTC",
    "updated_date": "2024-10-22 17:06:17 UTC"
  },
  {
    "arxiv_id": "2409.13684v3",
    "title": "The FIX Benchmark: Extracting Features Interpretable to eXperts",
    "authors": [
      "Helen Jin",
      "Shreya Havaldar",
      "Chaehyeon Kim",
      "Anton Xue",
      "Weiqiu You",
      "Helen Qu",
      "Marco Gatti",
      "Daniel A Hashimoto",
      "Bhuvnesh Jain",
      "Amin Madani",
      "Masao Sako",
      "Lyle Ungar",
      "Eric Wong"
    ],
    "abstract": "Feature-based methods are commonly used to explain model predictions, but\nthese methods often implicitly assume that interpretable features are readily\navailable. However, this is often not the case for high-dimensional data, and\nit can be hard even for domain experts to mathematically specify which features\nare important. Can we instead automatically extract collections or groups of\nfeatures that are aligned with expert knowledge? To address this gap, we\npresent FIX (Features Interpretable to eXperts), a benchmark for measuring how\nwell a collection of features aligns with expert knowledge. In collaboration\nwith domain experts, we propose FIXScore, a unified expert alignment measure\napplicable to diverse real-world settings across cosmology, psychology, and\nmedicine domains in vision, language, and time series data modalities. With\nFIXScore, we find that popular feature-based explanation methods have poor\nalignment with expert-specified knowledge, highlighting the need for new\nmethods that can better identify features interpretable to experts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13684v3",
    "published_date": "2024-09-20 17:53:03 UTC",
    "updated_date": "2024-12-23 19:39:18 UTC"
  },
  {
    "arxiv_id": "2409.13682v1",
    "title": "ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robot Navigation",
    "authors": [
      "Abrar Anwar",
      "John Welsh",
      "Joydeep Biswas",
      "Soha Pouya",
      "Yan Chang"
    ],
    "abstract": "Navigating and understanding complex environments over extended periods of\ntime is a significant challenge for robots. People interacting with the robot\nmay want to ask questions like where something happened, when it occurred, or\nhow long ago it took place, which would require the robot to reason over a long\nhistory of their deployment. To address this problem, we introduce a\nRetrieval-augmented Memory for Embodied Robots, or ReMEmbR, a system designed\nfor long-horizon video question answering for robot navigation. To evaluate\nReMEmbR, we introduce the NaVQA dataset where we annotate spatial, temporal,\nand descriptive questions to long-horizon robot navigation videos. ReMEmbR\nemploys a structured approach involving a memory building and a querying phase,\nleveraging temporal information, spatial information, and images to efficiently\nhandle continuously growing robot histories. Our experiments demonstrate that\nReMEmbR outperforms LLM and VLM baselines, allowing ReMEmbR to achieve\neffective long-horizon reasoning with low latency. Additionally, we deploy\nReMEmbR on a robot and show that our approach can handle diverse queries. The\ndataset, code, videos, and other material can be found at the following link:\nhttps://nvidia-ai-iot.github.io/remembr",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13682v1",
    "published_date": "2024-09-20 17:50:07 UTC",
    "updated_date": "2024-09-20 17:50:07 UTC"
  },
  {
    "arxiv_id": "2409.13661v3",
    "title": "Efficient Domain Augmentation for Autonomous Driving Testing Using Diffusion Models",
    "authors": [
      "Luciano Baresi",
      "Davide Yi Xian Hu",
      "Andrea Stocco",
      "Paolo Tonella"
    ],
    "abstract": "Simulation-based testing is widely used to assess the reliability of\nAutonomous Driving Systems (ADS), but its effectiveness is limited by the\noperational design domain (ODD) conditions available in such simulators. To\naddress this limitation, in this work, we explore the integration of generative\nartificial intelligence techniques with physics-based simulators to enhance ADS\nsystem-level testing. Our study evaluates the effectiveness and computational\noverhead of three generative strategies based on diffusion models, namely\ninstruction-editing, inpainting, and inpainting with refinement. Specifically,\nwe assess these techniques' capabilities to produce augmented\nsimulator-generated images of driving scenarios representing new ODDs. We\nemploy a novel automated detector for invalid inputs based on semantic\nsegmentation to ensure semantic preservation and realism of the neural\ngenerated images. We then perform system-level testing to evaluate the ADS's\ngeneralization ability to newly synthesized ODDs. Our findings show that\ndiffusion models help increase the ODD coverage for system-level testing of\nADS. Our automated semantic validator achieved a percentage of false positives\nas low as 3%, retaining the correctness and quality of the generated images for\ntesting. Our approach successfully identified new ADS system failures before\nreal-world testing.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Accepted for publication at the 47th International Conference on\n  Software Engineering (ICSE 2025). This research was partially supported by\n  project EMELIOT, funded by MUR under the PRIN 2020 program (n. 2020W3A5FY),\n  by the Bavarian Ministry of Economic Affairs, Regional Development and\n  Energy, by the TUM Global Incentive Fund, and by the EU Project Sec4AI4Sec\n  (n. 101120393)",
    "pdf_url": "http://arxiv.org/pdf/2409.13661v3",
    "published_date": "2024-09-20 17:09:45 UTC",
    "updated_date": "2025-02-17 15:48:30 UTC"
  },
  {
    "arxiv_id": "2409.13652v2",
    "title": "OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition",
    "authors": [
      "Stephen Zhang",
      "Vardan Papyan"
    ],
    "abstract": "The recent paradigm shift to large-scale foundation models has brought about\na new era for deep learning that, while has found great success in practice,\nhas also been plagued by prohibitively expensive costs in terms of high memory\nconsumption and compute. To mitigate these issues, there has been a concerted\neffort in post-hoc neural network pruning techniques that do not require costly\nretraining. Despite the considerable progress being made, existing methods\noften exhibit a steady drop in model performance as the compression increases.\nIn this paper, we present a novel approach to compressing large transformers,\ncoined OATS, that utilizes the second moment information in the input\nembeddings to decompose the model weights into a sum of sparse and low-rank\nmatrices. Without any retraining, OATS achieves state-of-the-art performance\nwhen compressing models by up to $60\\%$ on large language models such as\nLlama-3 and Phi-3 and vision transformers such as ViT and DINOv2 while\ndelivering up to $1.37\\times$ the CPU acceleration versus a model that was\ncomparably pruned.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13652v2",
    "published_date": "2024-09-20 17:02:00 UTC",
    "updated_date": "2025-02-28 22:39:44 UTC"
  },
  {
    "arxiv_id": "2409.13621v2",
    "title": "Advancing Event Causality Identification via Heuristic Semantic Dependency Inquiry Network",
    "authors": [
      "Haoran Li",
      "Qiang Gao",
      "Hongmei Wu",
      "Li Huang"
    ],
    "abstract": "Event Causality Identification (ECI) focuses on extracting causal relations\nbetween events in texts. Existing methods for ECI primarily rely on causal\nfeatures and external knowledge. However, these approaches fall short in two\ndimensions: (1) causal features between events in a text often lack explicit\nclues, and (2) external knowledge may introduce bias, while specific problems\nrequire tailored analyses. To address these issues, we propose SemDI - a simple\nand effective Semantic Dependency Inquiry Network for ECI. SemDI captures\nsemantic dependencies within the context using a unified encoder. Then, it\nutilizes a Cloze Analyzer to generate a fill-in token based on comprehensive\ncontext understanding. Finally, this fill-in token is used to inquire about the\ncausal relation between two events. Extensive experiments demonstrate the\neffectiveness of SemDI, surpassing state-of-the-art methods on three widely\nused benchmarks. Code is available at https://github.com/hrlics/SemDI.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 camera-ready version. Code is released at\n  https://github.com/hrlics/SemDI",
    "pdf_url": "http://arxiv.org/pdf/2409.13621v2",
    "published_date": "2024-09-20 16:32:54 UTC",
    "updated_date": "2024-10-02 06:14:17 UTC"
  },
  {
    "arxiv_id": "2409.13609v3",
    "title": "MaPPER: Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension",
    "authors": [
      "Ting Liu",
      "Zunnan Xu",
      "Yue Hu",
      "Liangtao Shi",
      "Zhiqiang Wang",
      "Quanjun Yin"
    ],
    "abstract": "Referring Expression Comprehension (REC), which aims to ground a local visual\nregion via natural language, is a task that heavily relies on multimodal\nalignment. Most existing methods utilize powerful pre-trained models to\ntransfer visual/linguistic knowledge by full fine-tuning. However, full\nfine-tuning the entire backbone not only breaks the rich prior knowledge\nembedded in the pre-training, but also incurs significant computational costs.\nMotivated by the recent emergence of Parameter-Efficient Transfer Learning\n(PETL) methods, we aim to solve the REC task in an effective and efficient\nmanner. Directly applying these PETL methods to the REC task is inappropriate,\nas they lack the specific-domain abilities for precise local visual perception\nand visual-language alignment. Therefore, we propose a novel framework of\nMultimodal Prior-guided Parameter Efficient Tuning, namely MaPPER.\nSpecifically, MaPPER comprises Dynamic Prior Adapters guided by an aligned\nprior, and Local Convolution Adapters to extract precise local semantics for\nbetter visual perception. Moreover, the Prior-Guided Text module is proposed to\nfurther utilize the prior for facilitating the cross-modal alignment.\nExperimental results on three widely-used benchmarks demonstrate that MaPPER\nachieves the best accuracy compared to the full fine-tuning and other PETL\nmethods with only 1.41% tunable backbone parameters. Our code is available at\nhttps://github.com/liuting20/MaPPER.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "EMNLP 2024 main",
    "pdf_url": "http://arxiv.org/pdf/2409.13609v3",
    "published_date": "2024-09-20 16:12:26 UTC",
    "updated_date": "2025-01-02 15:26:57 UTC"
  },
  {
    "arxiv_id": "2409.13602v1",
    "title": "MeLIAD: Interpretable Few-Shot Anomaly Detection with Metric Learning and Entropy-based Scoring",
    "authors": [
      "Eirini Cholopoulou",
      "Dimitris K. Iakovidis"
    ],
    "abstract": "Anomaly detection (AD) plays a pivotal role in multimedia applications for\ndetecting defective products and automating quality inspection. Deep learning\n(DL) models typically require large-scale annotated data, which are often\nhighly imbalanced since anomalies are usually scarce. The black box nature of\nthese models prohibits them from being trusted by users. To address these\nchallenges, we propose MeLIAD, a novel methodology for interpretable anomaly\ndetection, which unlike the previous methods is based on metric learning and\nachieves interpretability by design without relying on any prior distribution\nassumptions of true anomalies. MeLIAD requires only a few samples of anomalies\nfor training, without employing any augmentation techniques, and is inherently\ninterpretable, providing visualizations that offer insights into why an image\nis identified as anomalous. This is achieved by introducing a novel trainable\nentropy-based scoring component for the identification and localization of\nanomalous instances, and a novel loss function that jointly optimizes the\nanomaly scoring component with a metric learning objective. Experiments on five\npublic benchmark datasets, including quantitative and qualitative evaluation of\ninterpretability, demonstrate that MeLIAD achieves improved anomaly detection\nand localization performance compared to state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2409.13602v1",
    "published_date": "2024-09-20 16:01:43 UTC",
    "updated_date": "2024-09-20 16:01:43 UTC"
  },
  {
    "arxiv_id": "2409.13592v1",
    "title": "YesBut: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models",
    "authors": [
      "Abhilash Nandy",
      "Yash Agarwal",
      "Ashish Patwa",
      "Millon Madhur Das",
      "Aman Bansal",
      "Ankit Raj",
      "Pawan Goyal",
      "Niloy Ganguly"
    ],
    "abstract": "Understanding satire and humor is a challenging task for even current\nVision-Language models. In this paper, we propose the challenging tasks of\nSatirical Image Detection (detecting whether an image is satirical),\nUnderstanding (generating the reason behind the image being satirical), and\nCompletion (given one half of the image, selecting the other half from 2 given\noptions, such that the complete image is satirical) and release a high-quality\ndataset YesBut, consisting of 2547 images, 1084 satirical and 1463\nnon-satirical, containing different artistic styles, to evaluate those tasks.\nEach satirical image in the dataset depicts a normal scenario, along with a\nconflicting scenario which is funny or ironic. Despite the success of current\nVision-Language Models on multimodal tasks such as Visual QA and Image\nCaptioning, our benchmarking experiments show that such models perform poorly\non the proposed tasks on the YesBut Dataset in Zero-Shot Settings w.r.t both\nautomated as well as human evaluation. Additionally, we release a dataset of\n119 real, satirical photographs for further research. The dataset and code are\navailable at https://github.com/abhi1nandy2/yesbut_dataset.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "EMNLP 2024 Main (Long), 18 pages, 14 figures, 12 tables",
    "pdf_url": "http://arxiv.org/pdf/2409.13592v1",
    "published_date": "2024-09-20 15:45:29 UTC",
    "updated_date": "2024-09-20 15:45:29 UTC"
  },
  {
    "arxiv_id": "2409.13588v2",
    "title": "ChainBuddy: An AI Agent System for Generating LLM Pipelines",
    "authors": [
      "Jingyue Zhang",
      "Ian Arawjo"
    ],
    "abstract": "As large language models (LLMs) advance, their potential applications have\ngrown significantly. However, it remains difficult to evaluate LLM behavior on\nuser-defined tasks and craft effective pipelines to do so. Many users struggle\nwith where to start, often referred to as the \"blank page problem.\" ChainBuddy,\nan AI workflow generation assistant built into the ChainForge platform, aims to\ntackle this issue. From a single prompt or chat, ChainBuddy generates a starter\nevaluative LLM pipeline in ChainForge aligned to the user's requirements.\nChainBuddy offers a straightforward and user-friendly way to plan and evaluate\nLLM behavior and make the process less daunting and more accessible across a\nwide range of possible tasks and use cases. We report a within-subjects user\nstudy comparing ChainBuddy to the baseline interface. We find that when using\nAI assistance, participants reported a less demanding workload, felt more\nconfident, and produced higher quality pipelines evaluating LLM behavior.\nHowever, we also uncover a mismatch between subjective and objective ratings of\nperformance: participants rated their successfulness similarly across\nconditions, while independent experts rated participant workflows significantly\nhigher with AI assistance. Drawing connections to the Dunning-Kruger effect, we\ndraw design implications for the future of workflow generation assistants to\nmitigate the risk of over-reliance.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "H.5.2; I.2"
    ],
    "primary_category": "cs.HC",
    "comment": "21 pages, 12 figures, pre-print",
    "pdf_url": "http://arxiv.org/pdf/2409.13588v2",
    "published_date": "2024-09-20 15:42:33 UTC",
    "updated_date": "2025-02-08 21:59:10 UTC"
  },
  {
    "arxiv_id": "2409.13585v1",
    "title": "Neurosymbolic Conformal Classification",
    "authors": [
      "Arthur Ledaguenel",
      "Céline Hudelot",
      "Mostepha Khouadjia"
    ],
    "abstract": "The last decades have seen a drastic improvement of Machine Learning (ML),\nmainly driven by Deep Learning (DL). However, despite the resounding successes\nof ML in many domains, the impossibility to provide guarantees of conformity\nand the fragility of ML systems (faced with distribution shifts, adversarial\nattacks, etc.) have prevented the design of trustworthy AI systems. Several\nresearch paths have been investigated to mitigate this fragility and provide\nsome guarantees regarding the behavior of ML systems, among which are\nneurosymbolic AI and conformal prediction. Neurosymbolic artificial\nintelligence is a growing field of research aiming to combine neural network\nlearning capabilities with the reasoning abilities of symbolic systems. One of\nthe objective of this hybridization can be to provide theoritical guarantees\nthat the output of the system will comply with some prior knowledge. Conformal\nprediction is a set of techniques that enable to take into account the\nuncertainty of ML systems by transforming the unique prediction into a set of\npredictions, called a confidence set. Interestingly, this comes with\nstatistical guarantees regarding the presence of the true label inside the\nconfidence set. Both approaches are distribution-free and model-agnostic. In\nthis paper, we see how these two approaches can complement one another. We\nintroduce several neurosymbolic conformal prediction techniques and explore\ntheir different characteristics (size of confidence sets, computational\ncomplexity, etc.).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 0 figures. arXiv admin note: text overlap with\n  arXiv:2404.08404",
    "pdf_url": "http://arxiv.org/pdf/2409.13585v1",
    "published_date": "2024-09-20 15:38:34 UTC",
    "updated_date": "2024-09-20 15:38:34 UTC"
  },
  {
    "arxiv_id": "2409.13582v1",
    "title": "Time and Tokens: Benchmarking End-to-End Speech Dysfluency Detection",
    "authors": [
      "Xuanru Zhou",
      "Jiachen Lian",
      "Cheol Jun Cho",
      "Jingwen Liu",
      "Zongli Ye",
      "Jinming Zhang",
      "Brittany Morin",
      "David Baquirin",
      "Jet Vonk",
      "Zoe Ezzes",
      "Zachary Miller",
      "Maria Luisa Gorno Tempini",
      "Gopala Anumanchipalli"
    ],
    "abstract": "Speech dysfluency modeling is a task to detect dysfluencies in speech, such\nas repetition, block, insertion, replacement, and deletion. Most recent\nadvancements treat this problem as a time-based object detection problem. In\nthis work, we revisit this problem from a new perspective: tokenizing\ndysfluencies and modeling the detection problem as a token-based automatic\nspeech recognition (ASR) problem. We propose rule-based speech and text\ndysfluency simulators and develop VCTK-token, and then develop a Whisper-like\nseq2seq architecture to build a new benchmark with decent performance. We also\nsystematically compare our proposed token-based methods with time-based\nmethods, and propose a unified benchmark to facilitate future research\nendeavors. We open-source these resources for the broader scientific community.\nThe project page is available at https://rorizzz.github.io/",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13582v1",
    "published_date": "2024-09-20 15:35:32 UTC",
    "updated_date": "2024-09-20 15:35:32 UTC"
  },
  {
    "arxiv_id": "2409.13576v2",
    "title": "Region Prompt Tuning: Fine-grained Scene Text Detection Utilizing Region Text Prompt",
    "authors": [
      "Xingtao Lin",
      "Heqian Qiu",
      "Lanxiao Wang",
      "Ruihang Wang",
      "Linfeng Xu",
      "Hongliang Li"
    ],
    "abstract": "Recent advancements in prompt tuning have successfully adapted large-scale\nmodels like Contrastive Language-Image Pre-trained (CLIP) for downstream tasks\nsuch as scene text detection. Typically, text prompt complements the text\nencoder's input, focusing on global features while neglecting fine-grained\ndetails, leading to fine-grained text being ignored in task of scene text\ndetection. In this paper, we propose the region prompt tuning (RPT) method for\nfine-grained scene text detection, where region text prompt proposed would help\nfocus on fine-grained features. Region prompt tuning method decomposes region\ntext prompt into individual characters and splits visual feature map into\nregion visual tokens, creating a one-to-one correspondence between characters\nand tokens. This allows a character matches the local features of a token,\nthereby avoiding the omission of detailed features and fine-grained text. To\nachieve this, we introduce a sharing position embedding to link each character\nwith its corresponding token and employ a bidirectional distance loss to align\neach region text prompt character with the target ``text''. To refine the\ninformation at fine-grained level, we implement character-token level\ninteractions before and after encoding. Our proposed method combines a general\nscore map from the image-text process with a region score map derived from\ncharacter-token matching, producing a final score map that could balance the\nglobal and local features and be fed into DBNet to detect the text. Experiments\non benchmarks like ICDAR2015, TotalText, and CTW1500 demonstrate RPT impressive\nperformance, underscoring its effectiveness for scene text detection.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13576v2",
    "published_date": "2024-09-20 15:24:26 UTC",
    "updated_date": "2024-11-19 19:26:07 UTC"
  },
  {
    "arxiv_id": "2409.13571v1",
    "title": "Scalable Multi-agent Reinforcement Learning for Factory-wide Dynamic Scheduling",
    "authors": [
      "Jaeyeon Jang",
      "Diego Klabjan",
      "Han Liu",
      "Nital S. Patel",
      "Xiuqi Li",
      "Balakrishnan Ananthanarayanan",
      "Husam Dauod",
      "Tzung-Han Juang"
    ],
    "abstract": "Real-time dynamic scheduling is a crucial but notoriously challenging task in\nmodern manufacturing processes due to its high decision complexity. Recently,\nreinforcement learning (RL) has been gaining attention as an impactful\ntechnique to handle this challenge. However, classical RL methods typically\nrely on human-made dispatching rules, which are not suitable for large-scale\nfactory-wide scheduling. To bridge this gap, this paper applies a\nleader-follower multi-agent RL (MARL) concept to obtain desired coordination\nafter decomposing the scheduling problem into a set of sub-problems that are\nhandled by each individual agent for scalability. We further strengthen the\nprocedure by proposing a rule-based conversion algorithm to prevent\ncatastrophic loss of production capacity due to an agent's error. Our\nexperimental results demonstrate that the proposed model outperforms the\nstate-of-the-art deep RL-based scheduling models in various aspects.\nAdditionally, the proposed model provides the most robust scheduling\nperformance to demand changes. Overall, the proposed MARL-based scheduling\nmodel presents a promising solution to the real-time scheduling problem, with\npotential applications in various manufacturing industries.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13571v1",
    "published_date": "2024-09-20 15:16:37 UTC",
    "updated_date": "2024-09-20 15:16:37 UTC"
  },
  {
    "arxiv_id": "2409.13566v2",
    "title": "Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Tensorflow Pretrained Models",
    "authors": [
      "Keyu Chen",
      "Ziqian Bi",
      "Qian Niu",
      "Junyu Liu",
      "Benji Peng",
      "Sen Zhang",
      "Ming Liu",
      "Ming Li",
      "Xuanhe Pan",
      "Jiawei Xu",
      "Jinlang Wang",
      "Pohsun Feng"
    ],
    "abstract": "The application of TensorFlow pre-trained models in deep learning is\nexplored, with an emphasis on practical guidance for tasks such as image\nclassification and object detection. The study covers modern architectures,\nincluding ResNet, MobileNet, and EfficientNet, and demonstrates the\neffectiveness of transfer learning through real-world examples and experiments.\nA comparison of linear probing and model fine-tuning is presented, supplemented\nby visualizations using techniques like PCA, t-SNE, and UMAP, allowing for an\nintuitive understanding of the impact of these approaches. The work provides\ncomplete example code and step-by-step instructions, offering valuable insights\nfor both beginners and advanced users. By integrating theoretical concepts with\nhands-on practice, the paper equips readers with the tools necessary to address\ndeep learning challenges efficiently.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This book contains 148 pages and 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13566v2",
    "published_date": "2024-09-20 15:07:14 UTC",
    "updated_date": "2024-12-11 04:40:00 UTC"
  },
  {
    "arxiv_id": "2409.15380v3",
    "title": "Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for Filipino",
    "authors": [
      "Jann Railey Montalan",
      "Jian Gang Ngui",
      "Wei Qi Leong",
      "Yosephine Susanto",
      "Hamsawardhini Rengarajan",
      "Alham Fikri Aji",
      "William Chandra Tjhi"
    ],
    "abstract": "Multilingual large language models (LLMs) today may not necessarily provide\nculturally appropriate and relevant responses to its Filipino users. We\nintroduce Kalahi, a cultural LLM evaluation suite collaboratively created by\nnative Filipino speakers. It is composed of 150 high-quality, handcrafted and\nnuanced prompts that test LLMs for generations that are relevant to shared\nFilipino cultural knowledge and values. Strong LLM performance in Kalahi\nindicates a model's ability to generate responses similar to what an average\nFilipino would say or do in a given situation. We conducted experiments on LLMs\nwith multilingual and Filipino language support. Results show that Kalahi,\nwhile trivial for Filipinos, is challenging for LLMs, with the best model\nanswering only 46.0% of the questions correctly compared to native Filipino\nperformance of 89.10%. Thus, Kalahi can be used to accurately and reliably\nevaluate Filipino cultural representation in LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for presentation at Paclic 38, 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.15380v3",
    "published_date": "2024-09-20 15:01:21 UTC",
    "updated_date": "2024-12-18 14:39:02 UTC"
  },
  {
    "arxiv_id": "2409.13559v1",
    "title": "Efficient Visualization of Neural Networks with Generative Models and Adversarial Perturbations",
    "authors": [
      "Athanasios Karagounis"
    ],
    "abstract": "This paper presents a novel approach for deep visualization via a generative\nnetwork, offering an improvement over existing methods. Our model simplifies\nthe architecture by reducing the number of networks used, requiring only a\ngenerator and a discriminator, as opposed to the multiple networks\ntraditionally involved. Additionally, our model requires less prior training\nknowledge and uses a non-adversarial training process, where the discriminator\nacts as a guide rather than a competitor to the generator. The core\ncontribution of this work is its ability to generate detailed visualization\nimages that align with specific class labels. Our model incorporates a unique\nskip-connection-inspired block design, which enhances label-directed image\ngeneration by propagating class information across multiple layers.\nFurthermore, we explore how these generated visualizations can be utilized as\nadversarial examples, effectively fooling classification networks with minimal\nperceptible modifications to the original images. Experimental results\ndemonstrate that our method outperforms traditional adversarial example\ngeneration techniques in both targeted and non-targeted attacks, achieving up\nto a 94.5% fooling rate with minimal perturbation. This work bridges the gap\nbetween visualization methods and adversarial examples, proposing that fooling\nrate could serve as a quantitative measure for evaluating visualization\nquality. The insights from this study provide a new perspective on the\ninterpretability of neural networks and their vulnerabilities to adversarial\nattacks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "4 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13559v1",
    "published_date": "2024-09-20 14:59:25 UTC",
    "updated_date": "2024-09-20 14:59:25 UTC"
  },
  {
    "arxiv_id": "2409.13557v1",
    "title": "Trustworthy Hate Speech Detection Through Visual Augmentation",
    "authors": [
      "Ziyuan Yang",
      "Ming Yan",
      "Yingyu Chen",
      "Hui Wang",
      "Zexin Lu",
      "Yi Zhang"
    ],
    "abstract": "The surge of hate speech on social media platforms poses a significant\nchallenge, with hate speech detection~(HSD) becoming increasingly critical.\nCurrent HSD methods focus on enriching contextual information to enhance\ndetection performance, but they overlook the inherent uncertainty of hate\nspeech. We propose a novel HSD method, named trustworthy hate speech detection\nmethod through visual augmentation (TrusV-HSD), which enhances semantic\ninformation through integration with diffused visual images and mitigates\nuncertainty with trustworthy loss. TrusV-HSD learns semantic representations by\neffectively extracting trustworthy information through multi-modal connections\nwithout paired data. Our experiments on public HSD datasets demonstrate the\neffectiveness of TrusV-HSD, showing remarkable improvements over conventional\nmethods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13557v1",
    "published_date": "2024-09-20 14:57:34 UTC",
    "updated_date": "2024-09-20 14:57:34 UTC"
  },
  {
    "arxiv_id": "2409.13555v2",
    "title": "Generating Visual Stories with Grounded and Coreferent Characters",
    "authors": [
      "Danyang Liu",
      "Mirella Lapata",
      "Frank Keller"
    ],
    "abstract": "Characters are important in narratives. They move the plot forward, create\nemotional connections, and embody the story's themes. Visual storytelling\nmethods focus more on the plot and events relating to it, without building the\nnarrative around specific characters. As a result, the generated stories feel\ngeneric, with character mentions being absent, vague, or incorrect. To mitigate\nthese issues, we introduce the new task of character-centric story generation\nand present the first model capable of predicting visual stories with\nconsistently grounded and coreferent character mentions. Our model is finetuned\non a new dataset which we build on top of the widely used VIST benchmark.\nSpecifically, we develop an automated pipeline to enrich VIST with visual and\ntextual character coreference chains. We also propose new evaluation metrics to\nmeasure the richness of characters and coreference in stories. Experimental\nresults show that our model generates stories with recurring characters which\nare consistent and coreferent to larger extent compared to baselines and\nstate-of-the-art systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13555v2",
    "published_date": "2024-09-20 14:56:33 UTC",
    "updated_date": "2025-03-02 14:36:29 UTC"
  },
  {
    "arxiv_id": "2409.13546v1",
    "title": "Certified Adversarial Robustness via Partition-based Randomized Smoothing",
    "authors": [
      "Hossein Goli",
      "Farzan Farnia"
    ],
    "abstract": "A reliable application of deep neural network classifiers requires robustness\ncertificates against adversarial perturbations. Gaussian smoothing is a widely\nanalyzed approach to certifying robustness against norm-bounded perturbations,\nwhere the certified prediction radius depends on the variance of the Gaussian\nnoise and the confidence level of the neural net's prediction under the\nadditive Gaussian noise. However, in application to high-dimensional image\ndatasets, the certified radius of the plain Gaussian smoothing could be\nrelatively small, since Gaussian noise with high variances can significantly\nharm the visibility of an image. In this work, we propose the Pixel\nPartitioning-based Randomized Smoothing (PPRS) methodology to boost the neural\nnet's confidence score and thus the robustness radius of the certified\nprediction. We demonstrate that the proposed PPRS algorithm improves the\nvisibility of the images under additive Gaussian noise. We discuss the\nnumerical results of applying PPRS to standard computer vision datasets and\nneural network architectures. Our empirical findings indicate a considerable\nimprovement in the certified accuracy and stability of the prediction model to\nthe additive Gaussian noise in randomized smoothing.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13546v1",
    "published_date": "2024-09-20 14:41:47 UTC",
    "updated_date": "2024-09-20 14:41:47 UTC"
  },
  {
    "arxiv_id": "2409.13538v1",
    "title": "First Place Solution to the Multiple-choice Video QA Track of The Second Perception Test Challenge",
    "authors": [
      "Yingzhe Peng",
      "Yixiao Yuan",
      "Zitian Ao",
      "Huapeng Zhou",
      "Kangqi Wang",
      "Qipeng Zhu",
      "Xu Yang"
    ],
    "abstract": "In this report, we present our first-place solution to the Multiple-choice\nVideo Question Answering (QA) track of The Second Perception Test Challenge.\nThis competition posed a complex video understanding task, requiring models to\naccurately comprehend and answer questions about video content. To address this\nchallenge, we leveraged the powerful QwenVL2 (7B) model and fine-tune it on the\nprovided training set. Additionally, we employed model ensemble strategies and\nTest Time Augmentation to boost performance. Through continuous optimization,\nour approach achieved a Top-1 Accuracy of 0.7647 on the leaderboard.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13538v1",
    "published_date": "2024-09-20 14:31:13 UTC",
    "updated_date": "2024-09-20 14:31:13 UTC"
  },
  {
    "arxiv_id": "2409.13537v1",
    "title": "ShizishanGPT: An Agricultural Large Language Model Integrating Tools and Resources",
    "authors": [
      "Shuting Yang",
      "Zehui Liu",
      "Wolfgang Mayer"
    ],
    "abstract": "Recent developments in large language models (LLMs) have led to significant\nimprovements in intelligent dialogue systems'ability to handle complex\ninquiries. However, current LLMs still exhibit limitations in specialized\ndomain knowledge, particularly in technical fields such as agriculture. To\naddress this problem, we propose ShizishanGPT, an intelligent question\nanswering system for agriculture based on the Retrieval Augmented Generation\n(RAG) framework and agent architecture. ShizishanGPT consists of five key\nmodules: including a generic GPT-4 based module for answering general\nquestions; a search engine module that compensates for the problem that the\nlarge language model's own knowledge cannot be updated in a timely manner; an\nagricultural knowledge graph module for providing domain facts; a retrieval\nmodule which uses RAG to supplement domain knowledge; and an agricultural agent\nmodule, which invokes specialized models for crop phenotype prediction, gene\nexpression analysis, and so on. We evaluated the ShizishanGPT using a dataset\ncontaining 100 agricultural questions specially designed for this study. The\nexperimental results show that the tool significantly outperforms general LLMs\nas it provides more accurate and detailed answers due to its modular design and\nintegration of different domain knowledge sources. Our source code, dataset,\nand model weights are publicly available at https://github.com/Zaiwen/CropGPT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages,3 figures, WISE2024",
    "pdf_url": "http://arxiv.org/pdf/2409.13537v1",
    "published_date": "2024-09-20 14:30:45 UTC",
    "updated_date": "2024-09-20 14:30:45 UTC"
  },
  {
    "arxiv_id": "2409.15084v2",
    "title": "Depression Diagnosis Dialogue Simulation: Self-improving Psychiatrist with Tertiary Memory",
    "authors": [
      "Kunyao Lan",
      "Bingrui Jin",
      "Zichen Zhu",
      "Siyuan Chen",
      "Shu Zhang",
      "Kenny Q. Zhu",
      "Mengyue Wu"
    ],
    "abstract": "Mental health issues, particularly depressive disorders, present significant\nchallenges in contemporary society, necessitating the development of effective\nautomated diagnostic methods. This paper introduces the Agent Mental Clinic\n(AMC), a self-improving conversational agent system designed to enhance\ndepression diagnosis through simulated dialogues between patient and\npsychiatrist agents. To enhance the dialogue quality and diagnosis accuracy, we\ndesign a psychiatrist agent consisting of a tertiary memory structure, a\ndialogue control and reflect plugin that acts as ``supervisor'' and a memory\nsampling module, fully leveraging the skills reflected by the psychiatrist\nagent, achieving great accuracy on depression risk and suicide risk diagnosis\nvia conversation. Experiment results on datasets collected in real-life\nscenarios demonstrate that the system, simulating the procedure of training\npsychiatrists, can be a promising optimization method for aligning LLMs with\nreal-life distribution in specific domains without modifying the weights of\nLLMs, even when only a few representative labeled cases are available.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.15084v2",
    "published_date": "2024-09-20 14:25:08 UTC",
    "updated_date": "2024-10-09 04:37:29 UTC"
  },
  {
    "arxiv_id": "2409.13524v1",
    "title": "Contextualized AI for Cyber Defense: An Automated Survey using LLMs",
    "authors": [
      "Christoforus Yoga Haryanto",
      "Anne Maria Elvira",
      "Trung Duc Nguyen",
      "Minh Hieu Vu",
      "Yoshiano Hartanto",
      "Emily Lomempow",
      "Arathi Arakala"
    ],
    "abstract": "This paper surveys the potential of contextualized AI in enhancing cyber\ndefense capabilities, revealing significant research growth from 2015 to 2024.\nWe identify a focus on robustness, reliability, and integration methods, while\nnoting gaps in organizational trust and governance frameworks. Our study\nemploys two LLM-assisted literature survey methodologies: (A) ChatGPT 4 for\nexploration, and (B) Gemma 2:9b for filtering with Claude 3.5 Sonnet for\nfull-text analysis. We discuss the effectiveness and challenges of using LLMs\nin academic research, providing insights for future researchers.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "8 pages, 2 figures, 4 tables, accepted into 17th International\n  Conference on Security of Information and Networks (SINCONF 2024)",
    "pdf_url": "http://arxiv.org/pdf/2409.13524v1",
    "published_date": "2024-09-20 14:05:40 UTC",
    "updated_date": "2024-09-20 14:05:40 UTC"
  },
  {
    "arxiv_id": "2409.13521v2",
    "title": "A Survey on Moral Foundation Theory and Pre-Trained Language Models: Current Advances and Challenges",
    "authors": [
      "Lorenzo Zangari",
      "Candida M. Greco",
      "Davide Picca",
      "Andrea Tagarelli"
    ],
    "abstract": "Moral values have deep roots in early civilizations, codified within norms\nand laws that regulated societal order and the common good. They play a crucial\nrole in understanding the psychological basis of human behavior and cultural\norientation. The Moral Foundation Theory (MFT) is a well-established framework\nthat identifies the core moral foundations underlying the manner in which\ndifferent cultures shape individual and social lives. Recent advancements in\nnatural language processing, particularly Pre-trained Language Models (PLMs),\nhave enabled the extraction and analysis of moral dimensions from textual data.\nThis survey presents a comprehensive review of MFT-informed PLMs, providing an\nanalysis of moral tendencies in PLMs and their application in the context of\nthe MFT. We also review relevant datasets and lexicons and discuss trends,\nlimitations, and future directions. By providing a structured overview of the\nintersection between PLMs and MFT, this work bridges moral psychology insights\nwithin the realm of PLMs, paving the way for further research and development\nin creating morally aware AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.DL",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication with AI & Society, March 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.13521v2",
    "published_date": "2024-09-20 14:03:06 UTC",
    "updated_date": "2025-04-04 11:52:55 UTC"
  },
  {
    "arxiv_id": "2410.07122v1",
    "title": "End-Cloud Collaboration Framework for Advanced AI Customer Service in E-commerce",
    "authors": [
      "Liangyu Teng",
      "Yang Liu",
      "Jing Liu",
      "Liang Song"
    ],
    "abstract": "In recent years, the e-commerce industry has seen a rapid increase in the\ndemand for advanced AI-driven customer service solutions. Traditional\ncloud-based models face limitations in terms of latency, personalized services,\nand privacy concerns. Furthermore, end devices often lack the computational\nresources to deploy large AI models effectively. In this paper, we propose an\ninnovative End-Cloud Collaboration (ECC) framework for advanced AI customer\nservice in e-commerce. This framework integrates the advantages of large cloud\nmodels and mid/small-sized end models by deeply exploring the generalization\npotential of cloud models and effectively utilizing the computing power\nresources of terminal chips, alleviating the strain on computing resources to\nsome extent. Specifically, the large cloud model acts as a teacher, guiding and\npromoting the learning of the end model, which significantly reduces the end\nmodel's reliance on large-scale, high-quality data and thereby addresses the\ndata bottleneck in traditional end model training, offering a new paradigm for\nthe rapid deployment of industry applications. Additionally, we introduce an\nonline evolutive learning strategy that enables the end model to continuously\niterate and upgrade based on guidance from the cloud model and real-time user\nfeedback. This strategy ensures that the model can flexibly adapt to the rapid\nchanges in application scenarios while avoiding the uploading of sensitive\ninformation by performing local fine-tuning, achieving the dual goals of\nprivacy protection and personalized service. %We make systematic contributions\nto the customized model fine-tuning methods in the e-commerce domain. To\nconclude, we implement in-depth corpus collection (e.g., data organization,\ncleaning, and preprocessing) and train an ECC-based industry-specific model for\ne-commerce customer service.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted by 2024 IEEE 10th World Forum on Internet of Things (WF-IoT)",
    "pdf_url": "http://arxiv.org/pdf/2410.07122v1",
    "published_date": "2024-09-20 13:46:54 UTC",
    "updated_date": "2024-09-20 13:46:54 UTC"
  },
  {
    "arxiv_id": "2409.13503v3",
    "title": "SatFed: A Resource-Efficient LEO Satellite-Assisted Heterogeneous Federated Learning Framework",
    "authors": [
      "Yuxin Zhang",
      "Zheng Lin",
      "Zhe Chen",
      "Zihan Fang",
      "Wenjun Zhu",
      "Xianhao Chen",
      "Jin Zhao",
      "Yue Gao"
    ],
    "abstract": "Traditional federated learning (FL) frameworks rely heavily on terrestrial\nnetworks, where coverage limitations and increasing bandwidth congestion\nsignificantly hinder model convergence. Fortunately, the advancement of\nlow-Earth orbit (LEO) satellite networks offers promising new communication\navenues to augment traditional terrestrial FL. Despite this potential, the\nlimited satellite-ground communication bandwidth and the heterogeneous\noperating environments of ground devices-including variations in data,\nbandwidth, and computing power-pose substantial challenges for effective and\nrobust satellite-assisted FL. To address these challenges, we propose SatFed, a\nresource-efficient satellite-assisted heterogeneous FL framework. SatFed\nimplements freshness-based model prioritization queues to optimize the use of\nhighly constrained satellite-ground bandwidth, ensuring the transmission of the\nmost critical models. Additionally, a multigraph is constructed to capture\nreal-time heterogeneous relationships between devices, including data\ndistribution, terrestrial bandwidth, and computing capability. This multigraph\nenables SatFed to aggregate satellite-transmitted models into peer guidance,\nenhancing local training in heterogeneous environments. Extensive experiments\nwith real-world LEO satellite networks demonstrate that SatFed achieves\nsuperior performance and robustness compared to state-of-the-art benchmarks.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "10 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13503v3",
    "published_date": "2024-09-20 13:44:00 UTC",
    "updated_date": "2024-11-21 06:56:49 UTC"
  },
  {
    "arxiv_id": "2409.13501v1",
    "title": "HUT: A More Computation Efficient Fine-Tuning Method With Hadamard Updated Transformation",
    "authors": [
      "Geyuan Zhang",
      "Xiaofei Zhou",
      "Chuheng Chen"
    ],
    "abstract": "Fine-tuning pre-trained language models for downstream tasks has achieved\nimpressive results in NLP. However, fine-tuning all parameters becomes\nimpractical due to the rapidly increasing size of model parameters. To address\nthis, Parameter Efficient Fine-Tuning (PEFT) methods update only a subset of\nparameters. Most PEFT methods, such as LoRA, use incremental updates, which\ninvolve adding learned weight matrix increments to the original parameters.\nAlthough effective, these methods face limitations in capturing complex\nparameter dynamics and do not maintain a strong correlation between the\noriginal and updated parameters. To overcome these challenges, we propose the\ndirect Updated Transformation (UT) paradigm, which constructs a transformation\ndirectly from the original to the updated parameters. This approach ensures\nthat the correlation between the original and updated parameters is preserved,\nleveraging the semantic features learned during pre-training. Building on this\nparadigm, we present the Hadamard Updated Transformation (HUT) method. HUT\nefficiently updates the original weight matrix using the Hadamard\ntransformation with two low-rank matrices, offering a more expressive and\nflexible update mechanism. This allows HUT to capture richer parameter features\nthrough functional transformations, reducing computational complexity while\nmaintaining or improving model quality. Theoretical analysis and extensive\nexperiments on RoBERTa and GPT-2 validate the effectiveness of HUT. Results\nshow that HUT performs on par with or better than other PEFT methods in terms\nof model quality, while significantly reducing computational complexity.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13501v1",
    "published_date": "2024-09-20 13:42:17 UTC",
    "updated_date": "2024-09-20 13:42:17 UTC"
  },
  {
    "arxiv_id": "2409.13498v2",
    "title": "A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging",
    "authors": [
      "Savvas Sifnaios",
      "George Arvanitakis",
      "Fotios K. Konstantinidis",
      "Georgios Tsimiklis",
      "Angelos Amditis",
      "Panayiotis Frangos"
    ],
    "abstract": "Recent advancements in computer vision, particularly in detection,\nsegmentation, and classification, have significantly impacted various domains.\nHowever, these advancements are tied to RGB-based systems, which are\ninsufficient for applications in industries like waste sorting,\npharmaceuticals, and defense, where advanced object characterization beyond\nshape or color is necessary. Hyperspectral (HS) imaging, capturing both\nspectral and spatial information, addresses these limitations and offers\nadvantages over conventional technologies such as X-ray fluorescence and Raman\nspectroscopy, particularly in terms of speed, cost, and safety.\n  This study evaluates the potential of combining HS imaging with deep learning\nfor material characterization. The research involves: i) designing an\nexperimental setup with HS camera, conveyor, and controlled lighting; ii)\ngenerating a multi-object dataset of various plastics (HDPE, PET, PP, PS) with\nsemi-automated mask generation and Raman spectroscopy-based labeling; and iii)\ndeveloping a deep learning model trained on HS images for pixel-level material\nclassification. The model achieved 99.94\\% classification accuracy,\ndemonstrating robustness in color, size, and shape invariance, and effectively\nhandling material overlap. Limitations, such as challenges with black objects,\nare also discussed. Extending computer vision beyond RGB to HS imaging proves\nfeasible, overcoming major limitations of traditional methods and showing\nstrong potential for future applications.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "I.5; I.2.10"
    ],
    "primary_category": "eess.IV",
    "comment": "13 pages, 15 figures, 6 equations",
    "pdf_url": "http://arxiv.org/pdf/2409.13498v2",
    "published_date": "2024-09-20 13:38:48 UTC",
    "updated_date": "2025-05-14 13:01:39 UTC"
  },
  {
    "arxiv_id": "2409.13496v1",
    "title": "DAP-LED: Learning Degradation-Aware Priors with CLIP for Joint Low-light Enhancement and Deblurring",
    "authors": [
      "Ling Wang",
      "Chen Wu",
      "Lin Wang"
    ],
    "abstract": "Autonomous vehicles and robots often struggle with reliable visual perception\nat night due to the low illumination and motion blur caused by the long\nexposure time of RGB cameras. Existing methods address this challenge by\nsequentially connecting the off-the-shelf pretrained low-light enhancement and\ndeblurring models. Unfortunately, these methods often lead to noticeable\nartifacts (\\eg, color distortions) in the over-exposed regions or make it\nhardly possible to learn the motion cues of the dark regions. In this paper, we\ninterestingly find vision-language models, \\eg, Contrastive Language-Image\nPretraining (CLIP), can comprehensively perceive diverse degradation levels at\nnight. In light of this, we propose a novel transformer-based joint learning\nframework, named DAP-LED, which can jointly achieve low-light enhancement and\ndeblurring, benefiting downstream tasks, such as depth estimation,\nsegmentation, and detection in the dark. The key insight is to leverage CLIP to\nadaptively learn the degradation levels from images at night. This subtly\nenables learning rich semantic information and visual representation for\noptimization of the joint tasks. To achieve this, we first introduce a\nCLIP-guided cross-fusion module to obtain multi-scale patch-wise degradation\nheatmaps from the image embeddings. Then, the heatmaps are fused via the\ndesigned CLIP-enhanced transformer blocks to retain useful degradation\ninformation for effective model optimization. Experimental results show that,\ncompared to existing methods, our DAP-LED achieves state-of-the-art performance\nin the dark. Meanwhile, the enhanced results are demonstrated to be effective\nfor three downstream tasks. For demo and more results, please check the project\npage: \\url{https://vlislab22.github.io/dap-led/}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13496v1",
    "published_date": "2024-09-20 13:37:53 UTC",
    "updated_date": "2024-09-20 13:37:53 UTC"
  },
  {
    "arxiv_id": "2410.07121v1",
    "title": "Transfer Learning for E-commerce Query Product Type Prediction",
    "authors": [
      "Anna Tigunova",
      "Thomas Ricatte",
      "Ghadir Eraisha"
    ],
    "abstract": "Getting a good understanding of the customer intent is essential in\ne-commerce search engines. In particular, associating the correct product type\nto a search query plays a vital role in surfacing correct products to the\ncustomers. Query product type classification (Q2PT) is a particularly\nchallenging task because search queries are short and ambiguous, the number of\nexisting product categories is extremely large, spanning thousands of values.\nMoreover, international marketplaces face additional challenges, such as\nlanguage and dialect diversity and cultural differences, influencing the\ninterpretation of the query. In this work we focus on Q2PT prediction in the\nglobal multilocale e-commerce markets. The common approach of training Q2PT\nmodels for each locale separately shows significant performance drops in\nlow-resource stores. Moreover, this method does not allow for a smooth\nexpansion to a new country, requiring to collect the data and train a new\nlocale-specific Q2PT model from scratch. To tackle this, we propose to use\ntransfer learning from the highresource to the low-resource locales, to achieve\nglobal parity of Q2PT performance. We benchmark the per-locale Q2PT model\nagainst the unified one, which shares the training data and model structure\nacross all worldwide stores. Additionally, we compare locale-aware and\nlocale-agnostic Q2PT models, showing the task dependency on the\ncountry-specific traits. We conduct extensive quantiative and qualitative\nanalysis of Q2PT models on the large-scale e-commerce dataset across 20\nworldwide locales, which shows that unified locale-aware Q2PT model has\nsuperior performance over the alternatives.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.07121v1",
    "published_date": "2024-09-20 13:30:04 UTC",
    "updated_date": "2024-09-20 13:30:04 UTC"
  },
  {
    "arxiv_id": "2409.13484v1",
    "title": "'Since Lawyers are Males..': Examining Implicit Gender Bias in Hindi Language Generation by LLMs",
    "authors": [
      "Ishika Joshi",
      "Ishita Gupta",
      "Adrita Dey",
      "Tapan Parikh"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly being used to generate text\nacross various languages, for tasks such as translation, customer support, and\neducation. Despite these advancements, LLMs show notable gender biases in\nEnglish, which become even more pronounced when generating content in\nrelatively underrepresented languages like Hindi. This study explores implicit\ngender biases in Hindi text generation and compares them to those in English.\nWe developed Hindi datasets inspired by WinoBias to examine stereotypical\npatterns in responses from models like GPT-4o and Claude-3 sonnet. Our results\nreveal a significant gender bias of 87.8% in Hindi, compared to 33.4% in\nEnglish GPT-4o generation, with Hindi responses frequently relying on gender\nstereotypes related to occupations, power hierarchies, and social class. This\nresearch underscores the variation in gender biases across languages and\nprovides considerations for navigating these biases in generative AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13484v1",
    "published_date": "2024-09-20 13:16:58 UTC",
    "updated_date": "2024-09-20 13:16:58 UTC"
  },
  {
    "arxiv_id": "2409.15378v1",
    "title": "Toward Automated Clinical Transcriptions",
    "authors": [
      "Mitchell A. Klusty",
      "W. Vaiden Logan",
      "Samuel E. Armstrong",
      "Aaron D. Mullen",
      "Caroline N. Leach",
      "Jeff Talbert",
      "V. K. Cody Bumgardner"
    ],
    "abstract": "Administrative documentation is a major driver of rising healthcare costs and\nis linked to adverse outcomes, including physician burnout and diminished\nquality of care. This paper introduces a secure system that applies recent\nadvancements in speech-to-text transcription and speaker-labeling (diarization)\nto patient-provider conversations. This system is optimized to produce accurate\ntranscriptions and highlight potential errors to promote rapid human\nverification, further reducing the necessary manual effort. Applied to over 40\nhours of simulated conversations, this system offers a promising foundation for\nautomating clinical transcriptions.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "7 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.15378v1",
    "published_date": "2024-09-20 13:12:11 UTC",
    "updated_date": "2024-09-20 13:12:11 UTC"
  },
  {
    "arxiv_id": "2409.13476v1",
    "title": "Dermatologist-like explainable AI enhances melanoma diagnosis accuracy: eye-tracking study",
    "authors": [
      "Tirtha Chanda",
      "Sarah Haggenmueller",
      "Tabea-Clara Bucher",
      "Tim Holland-Letz",
      "Harald Kittler",
      "Philipp Tschandl",
      "Markus V. Heppt",
      "Carola Berking",
      "Jochen S. Utikal",
      "Bastian Schilling",
      "Claudia Buerger",
      "Cristian Navarrete-Dechent",
      "Matthias Goebeler",
      "Jakob Nikolas Kather",
      "Carolin V. Schneider",
      "Benjamin Durani",
      "Hendrike Durani",
      "Martin Jansen",
      "Juliane Wacker",
      "Joerg Wacker",
      "Reader Study Consortium",
      "Titus J. Brinker"
    ],
    "abstract": "Artificial intelligence (AI) systems have substantially improved\ndermatologists' diagnostic accuracy for melanoma, with explainable AI (XAI)\nsystems further enhancing clinicians' confidence and trust in AI-driven\ndecisions. Despite these advancements, there remains a critical need for\nobjective evaluation of how dermatologists engage with both AI and XAI tools.\nIn this study, 76 dermatologists participated in a reader study, diagnosing 16\ndermoscopic images of melanomas and nevi using an XAI system that provides\ndetailed, domain-specific explanations. Eye-tracking technology was employed to\nassess their interactions. Diagnostic performance was compared with that of a\nstandard AI system lacking explanatory features. Our findings reveal that XAI\nsystems improved balanced diagnostic accuracy by 2.8 percentage points relative\nto standard AI. Moreover, diagnostic disagreements with AI/XAI systems and\ncomplex lesions were associated with elevated cognitive load, as evidenced by\nincreased ocular fixations. These insights have significant implications for\nclinical practice, the design of AI tools for visual tasks, and the broader\ndevelopment of XAI in medical diagnostics.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13476v1",
    "published_date": "2024-09-20 13:08:33 UTC",
    "updated_date": "2024-09-20 13:08:33 UTC"
  },
  {
    "arxiv_id": "2409.13470v1",
    "title": "Deterministic versus stochastic dynamical classifiers: opposing random adversarial attacks with noise",
    "authors": [
      "Lorenzo Chicchi",
      "Duccio Fanelli",
      "Diego Febbe",
      "Lorenzo Buffoni",
      "Francesca Di Patti",
      "Lorenzo Giambagli",
      "Raffele Marino"
    ],
    "abstract": "The Continuous-Variable Firing Rate (CVFR) model, widely used in neuroscience\nto describe the intertangled dynamics of excitatory biological neurons, is here\ntrained and tested as a veritable dynamically assisted classifier. To this end\nthe model is supplied with a set of planted attractors which are\nself-consistently embedded in the inter-nodes coupling matrix, via its spectral\ndecomposition. Learning to classify amounts to sculp the basin of attraction of\nthe imposed equilibria, directing different items towards the corresponding\ndestination target, which reflects the class of respective pertinence. A\nstochastic variant of the CVFR model is also studied and found to be robust to\naversarial random attacks, which corrupt the items to be classified. This\nremarkable finding is one of the very many surprising effects which arise when\nnoise and dynamical attributes are made to mutually resonate.",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cond-mat.stat-mech",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13470v1",
    "published_date": "2024-09-20 12:59:16 UTC",
    "updated_date": "2024-09-20 12:59:16 UTC"
  },
  {
    "arxiv_id": "2409.13466v1",
    "title": "Global Outlier Detection in a Federated Learning Setting with Isolation Forest",
    "authors": [
      "Daniele Malpetti",
      "Laura Azzimonti"
    ],
    "abstract": "We present a novel strategy for detecting global outliers in a federated\nlearning setting, targeting in particular cross-silo scenarios. Our approach\ninvolves the use of two servers and the transmission of masked local data from\nclients to one of the servers. The masking of the data prevents the disclosure\nof sensitive information while still permitting the identification of outliers.\nMoreover, to further safeguard privacy, a permutation mechanism is implemented\nso that the server does not know which client owns any masked data point. The\nserver performs outlier detection on the masked data, using either Isolation\nForest or its extended version, and then communicates outlier information back\nto the clients, allowing them to identify and remove outliers in their local\ndatasets before starting any subsequent federated model training. This approach\nprovides comparable results to a centralized execution of Isolation Forest\nalgorithms on plain data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for publication at FLTA 2024: The 2nd IEEE International\n  Conference on Federated Learning Technologies and Applications",
    "pdf_url": "http://arxiv.org/pdf/2409.13466v1",
    "published_date": "2024-09-20 12:55:29 UTC",
    "updated_date": "2024-09-20 12:55:29 UTC"
  },
  {
    "arxiv_id": "2410.07108v1",
    "title": "FAIR GPT: A virtual consultant for research data management in ChatGPT",
    "authors": [
      "Renat Shigapov",
      "Irene Schumm"
    ],
    "abstract": "FAIR GPT is a first virtual consultant in ChatGPT designed to help\nresearchers and organizations make their data and metadata compliant with the\nFAIR (Findable, Accessible, Interoperable, Reusable) principles. It provides\nguidance on metadata improvement, dataset organization, and repository\nselection. To ensure accuracy, FAIR GPT uses external APIs to assess dataset\nFAIRness, retrieve controlled vocabularies, and recommend repositories,\nminimizing hallucination and improving precision. It also assists in creating\ndocumentation (data and software management plans, README files, and\ncodebooks), and selecting proper licenses. This paper describes its features,\napplications, and limitations.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.DL",
    "comment": "4 pages, 2 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2410.07108v1",
    "published_date": "2024-09-20 12:28:48 UTC",
    "updated_date": "2024-09-20 12:28:48 UTC"
  },
  {
    "arxiv_id": "2409.13440v1",
    "title": "Differentially Private Multimodal Laplacian Dropout (DP-MLD) for EEG Representative Learning",
    "authors": [
      "Xiaowen Fu",
      "Bingxin Wang",
      "Xinzhou Guo",
      "Guoqing Liu",
      "Yang Xiang"
    ],
    "abstract": "Recently, multimodal electroencephalogram (EEG) learning has shown great\npromise in disease detection. At the same time, ensuring privacy in clinical\nstudies has become increasingly crucial due to legal and ethical concerns. One\nwidely adopted scheme for privacy protection is differential privacy (DP)\nbecause of its clear interpretation and ease of implementation. Although\nnumerous methods have been proposed under DP, it has not been extensively\nstudied for multimodal EEG data due to the complexities of models and signal\ndata considered there. In this paper, we propose a novel Differentially Private\nMultimodal Laplacian Dropout (DP-MLD) scheme for multimodal EEG learning. Our\napproach proposes a novel multimodal representative learning model that\nprocesses EEG data by language models as text and other modal data by vision\ntransformers as images, incorporating well-designed cross-attention mechanisms\nto effectively extract and integrate cross-modal features. To achieve DP, we\ndesign a novel adaptive feature-level Laplacian dropout scheme, where\nrandomness allocation and performance are dynamically optimized within given\nprivacy budgets. In the experiment on an open-source multimodal dataset of\nFreezing of Gait (FoG) in Parkinson's Disease (PD), our proposed method\ndemonstrates an approximate 4\\% improvement in classification accuracy, and\nachieves state-of-the-art performance in multimodal EEG learning under DP.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13440v1",
    "published_date": "2024-09-20 12:08:22 UTC",
    "updated_date": "2024-09-20 12:08:22 UTC"
  },
  {
    "arxiv_id": "2409.13430v3",
    "title": "CVT-Occ: Cost Volume Temporal Fusion for 3D Occupancy Prediction",
    "authors": [
      "Zhangchen Ye",
      "Tao Jiang",
      "Chenfeng Xu",
      "Yiming Li",
      "Hang Zhao"
    ],
    "abstract": "Vision-based 3D occupancy prediction is significantly challenged by the\ninherent limitations of monocular vision in depth estimation. This paper\nintroduces CVT-Occ, a novel approach that leverages temporal fusion through the\ngeometric correspondence of voxels over time to improve the accuracy of 3D\noccupancy predictions. By sampling points along the line of sight of each voxel\nand integrating the features of these points from historical frames, we\nconstruct a cost volume feature map that refines current volume features for\nimproved prediction outcomes. Our method takes advantage of parallax cues from\nhistorical observations and employs a data-driven approach to learn the cost\nvolume. We validate the effectiveness of CVT-Occ through rigorous experiments\non the Occ3D-Waymo dataset, where it outperforms state-of-the-art methods in 3D\noccupancy prediction with minimal additional computational cost. The code is\nreleased at \\url{https://github.com/Tsinghua-MARS-Lab/CVT-Occ}.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.13430v3",
    "published_date": "2024-09-20 11:52:47 UTC",
    "updated_date": "2024-09-25 07:34:27 UTC"
  },
  {
    "arxiv_id": "2409.13427v1",
    "title": "A User Study on Contrastive Explanations for Multi-Effector Temporal Planning with Non-Stationary Costs",
    "authors": [
      "Xiaowei Liu",
      "Kevin McAreavey",
      "Weiru Liu"
    ],
    "abstract": "In this paper, we adopt constrastive explanations within an end-user\napplication for temporal planning of smart homes. In this application, users\nhave requirements on the execution of appliance tasks, pay for energy according\nto dynamic energy tariffs, have access to high-capacity battery storage, and\nare able to sell energy to the grid. The concurrent scheduling of devices makes\nthis a multi-effector planning problem, while the dynamic tariffs yield costs\nthat are non-stationary (alternatively, costs that are stationary but depend on\nexogenous events). These characteristics are such that the planning problems\nare generally not supported by existing PDDL-based planners, so we instead\ndesign a custom domain-dependent planner that scales to reasonable appliance\nnumbers and time horizons. We conduct a controlled user study with 128\nparticipants using an online crowd-sourcing platform based on two user stories.\nOur results indicate that users provided with contrastive questions and\nexplanations have higher levels of satisfaction, tend to gain improved\nunderstanding, and rate the helpfulness more favourably with the recommended AI\nschedule compared to those without access to these features.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13427v1",
    "published_date": "2024-09-20 11:48:25 UTC",
    "updated_date": "2024-09-20 11:48:25 UTC"
  },
  {
    "arxiv_id": "2409.13410v1",
    "title": "Sine Wave Normalization for Deep Learning-Based Tumor Segmentation in CT/PET Imaging",
    "authors": [
      "Jintao Ren",
      "Muheng Li",
      "Stine Sofia Korreman"
    ],
    "abstract": "This report presents a normalization block for automated tumor segmentation\nin CT/PET scans, developed for the autoPET III Challenge. The key innovation is\nthe introduction of the SineNormal, which applies periodic sine transformations\nto PET data to enhance lesion detection. By highlighting intensity variations\nand producing concentric ring patterns in PET highlighted regions, the model\naims to improve segmentation accuracy, particularly for challenging multitracer\nPET datasets. The code for this project is available on GitHub\n(https://github.com/BBQtime/Sine-Wave-Normalization-for-Deep-Learning-Based-Tumor-Segmentation-in-CT-PET).",
    "categories": [
      "cs.CV",
      "cs.AI",
      "physics.med-ph"
    ],
    "primary_category": "cs.CV",
    "comment": "Report for Team WukongRT in the AutoPET III Challenge",
    "pdf_url": "http://arxiv.org/pdf/2409.13410v1",
    "published_date": "2024-09-20 11:20:11 UTC",
    "updated_date": "2024-09-20 11:20:11 UTC"
  },
  {
    "arxiv_id": "2409.13402v1",
    "title": "Validation & Exploration of Multimodal Deep-Learning Camera-Lidar Calibration models",
    "authors": [
      "Venkat Karramreddy",
      "Liam Mitchell"
    ],
    "abstract": "This article presents an innovative study in exploring, evaluating, and\nimplementing deep learning architectures for the calibration of multi-modal\nsensor systems. The focus behind this is to leverage the use of sensor fusion\nto achieve dynamic, real-time alignment between 3D LiDAR and 2D Camera sensors.\nstatic calibration methods are tedious and time-consuming, which is why we\npropose utilizing Conventional Neural Networks (CNN) coupled with geometrically\ninformed learning to solve this issue. We leverage the foundational principles\nof Extrinsic LiDAR-Camera Calibration tools such as RegNet, CalibNet, and\nLCCNet by exploring open-source models that are available online and comparing\nour results with their corresponding research papers. Requirements for\nextracting these visual and measurable outputs involved tweaking source code,\nfine-tuning, training, validation, and testing for each of these frameworks for\nequal comparisons. This approach aims to investigate which of these advanced\nnetworks produces the most accurate and consistent predictions. Through a\nseries of experiments, we reveal some of their shortcomings and areas for\npotential improvements along the way. We find that LCCNet yields the best\nresults out of all the models that we validated.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13402v1",
    "published_date": "2024-09-20 11:03:49 UTC",
    "updated_date": "2024-09-20 11:03:49 UTC"
  },
  {
    "arxiv_id": "2410.02745v2",
    "title": "AVG-LLaVA: A Large Multimodal Model with Adaptive Visual Granularity",
    "authors": [
      "Zhibin Lan",
      "Liqiang Niu",
      "Fandong Meng",
      "Wenbo Li",
      "Jie Zhou",
      "Jinsong Su"
    ],
    "abstract": "Recently, when dealing with high-resolution images, dominant LMMs usually\ndivide them into multiple local images and one global image, which will lead to\na large number of visual tokens. In this work, we introduce AVG-LLaVA, an LMM\nthat can adaptively select the appropriate visual granularity based on the\ninput image and instruction. This approach not only reduces the number of\nvisual tokens and speeds up inference, but also improves the overall model\nperformance. Specifically, we introduce the following modules based on\nLLaVA-NeXT: (a) a visual granularity scaler that includes multiple pooling\nlayers to obtain visual tokens with different granularities; (b) a visual\ngranularity router, which includes a Transformer layer, an MLP layer, and a\nvoter layer, used to select the appropriate visual granularity based on the\nimage and instruction. Furthermore, we propose RGLF, a novel training paradigm\nthat aims at aligning the granularity predicted by the router with the\npreferences of the LMM, without the need for additional manually annotated\ndata. Extensive experiments and analysis show that AVG-LLaVA achieves superior\nperformance across 11 benchmarks, as well as significantly reduces the number\nof visual tokens and speeds up inference (e.g., an 85.3% reduction in visual\ntokens and a 2.53$\\times$ increase in inference speed on the AI2D benchmark).",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint",
    "pdf_url": "http://arxiv.org/pdf/2410.02745v2",
    "published_date": "2024-09-20 10:50:21 UTC",
    "updated_date": "2024-10-04 04:48:57 UTC"
  },
  {
    "arxiv_id": "2409.13793v1",
    "title": "On the Feasibility of Fully AI-automated Vishing Attacks",
    "authors": [
      "João Figueiredo",
      "Afonso Carvalho",
      "Daniel Castro",
      "Daniel Gonçalves",
      "Nuno Santos"
    ],
    "abstract": "A vishing attack is a form of social engineering where attackers use phone\ncalls to deceive individuals into disclosing sensitive information, such as\npersonal data, financial information, or security credentials. Attackers\nexploit the perceived urgency and authenticity of voice communication to\nmanipulate victims, often posing as legitimate entities like banks or tech\nsupport. Vishing is a particularly serious threat as it bypasses security\ncontrols designed to protect information. In this work, we study the potential\nfor vishing attacks to escalate with the advent of AI. In theory, AI-powered\nsoftware bots may have the ability to automate these attacks by initiating\nconversations with potential victims via phone calls and deceiving them into\ndisclosing sensitive information. To validate this thesis, we introduce ViKing,\nan AI-powered vishing system developed using publicly available AI technology.\nIt relies on a Large Language Model (LLM) as its core cognitive processor to\nsteer conversations with victims, complemented by a pipeline of speech-to-text\nand text-to-speech modules that facilitate audio-text conversion in phone\ncalls. Through a controlled social experiment involving 240 participants, we\ndiscovered that ViKing has successfully persuaded many participants to reveal\nsensitive information, even those who had been explicitly warned about the risk\nof vishing campaigns. Interactions with ViKing's bots were generally considered\nrealistic. From these findings, we conclude that tools like ViKing may already\nbe accessible to potential malicious actors, while also serving as an\ninvaluable resource for cyber awareness programs.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13793v1",
    "published_date": "2024-09-20 10:47:09 UTC",
    "updated_date": "2024-09-20 10:47:09 UTC"
  },
  {
    "arxiv_id": "2409.13382v1",
    "title": "Audio Codec Augmentation for Robust Collaborative Watermarking of Speech Synthesis",
    "authors": [
      "Lauri Juvela",
      "Xin Wang"
    ],
    "abstract": "Automatic detection of synthetic speech is becoming increasingly important as\ncurrent synthesis methods are both near indistinguishable from human speech and\nwidely accessible to the public. Audio watermarking and other active disclosure\nmethods of are attracting research activity, as they can complement traditional\ndeepfake defenses based on passive detection. In both active and passive\ndetection, robustness is of major interest. Traditional audio watermarks are\nparticularly susceptible to removal attacks by audio codec application. Most\ngenerated speech and audio content released into the wild passes through an\naudio codec purely as a distribution method. We recently proposed collaborative\nwatermarking as method for making generated speech more easily detectable over\na noisy but differentiable transmission channel. This paper extends the channel\naugmentation to work with non-differentiable traditional audio codecs and\nneural audio codecs and evaluates transferability and effect of codec bitrate\nover various configurations. The results show that collaborative watermarking\ncan be reliably augmented by black-box audio codecs using a waveform-domain\nstraight-through-estimator for gradient approximation. Furthermore, that\nresults show that channel augmentation with a neural audio codec transfers well\nto traditional codecs. Listening tests demonstrate collaborative watermarking\nincurs negligible perceptual degradation with high bitrate codecs or DAC at\n8kbps.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Submitted to ICASSP 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.13382v1",
    "published_date": "2024-09-20 10:33:17 UTC",
    "updated_date": "2024-09-20 10:33:17 UTC"
  },
  {
    "arxiv_id": "2409.13373v1",
    "title": "LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench",
    "authors": [
      "Karthik Valmeekam",
      "Kaya Stechly",
      "Subbarao Kambhampati"
    ],
    "abstract": "The ability to plan a course of action that achieves a desired state of\naffairs has long been considered a core competence of intelligent agents and\nhas been an integral part of AI research since its inception. With the advent\nof large language models (LLMs), there has been considerable interest in the\nquestion of whether or not they possess such planning abilities. PlanBench, an\nextensible benchmark we developed in 2022, soon after the release of GPT3, has\nremained an important tool for evaluating the planning abilities of LLMs.\nDespite the slew of new private and open source LLMs since GPT3, progress on\nthis benchmark has been surprisingly slow. OpenAI claims that their recent o1\n(Strawberry) model has been specifically constructed and trained to escape the\nnormal limitations of autoregressive LLMs--making it a new kind of model: a\nLarge Reasoning Model (LRM). Using this development as a catalyst, this paper\ntakes a comprehensive look at how well current LLMs and new LRMs do on\nPlanBench. As we shall see, while o1's performance is a quantum improvement on\nthe benchmark, outpacing the competition, it is still far from saturating it.\nThis improvement also brings to the fore questions about accuracy, efficiency,\nand guarantees which must be considered before deploying such systems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13373v1",
    "published_date": "2024-09-20 10:20:46 UTC",
    "updated_date": "2024-09-20 10:20:46 UTC"
  },
  {
    "arxiv_id": "2409.13366v2",
    "title": "RingMo-Aerial: An Aerial Remote Sensing Foundation Model With A Affine Transformation Contrastive Learning",
    "authors": [
      "Wenhui Diao",
      "Haichen Yu",
      "Kaiyue Kang",
      "Tong Ling",
      "Di Liu",
      "Yingchao Feng",
      "Hanbo Bi",
      "Libo Ren",
      "Xuexue Li",
      "Yongqiang Mao",
      "Xian Sun"
    ],
    "abstract": "Aerial Remote Sensing (ARS) vision tasks pose significant challenges due to\nthe unique characteristics of their viewing angles. Existing research has\nprimarily focused on algorithms for specific tasks, which have limited\napplicability in a broad range of ARS vision applications. This paper proposes\nthe RingMo-Aerial model, aiming to fill the gap in foundation model research in\nthe field of ARS vision. By introducing the Frequency-Enhanced Multi-Head\nSelf-Attention (FE-MSA) mechanism and an affine transformation-based\ncontrastive learning pre-training method, the model's detection capability for\nsmall targets is enhanced and optimized for the tilted viewing angles\ncharacteristic of ARS. Furthermore, the ARS-Adapter, an efficient parameter\nfine-tuning method, is proposed to improve the model's adaptability and\neffectiveness in various ARS vision tasks. Experimental results demonstrate\nthat RingMo-Aerial achieves SOTA performance on multiple downstream tasks. This\nindicates the practicality and efficacy of RingMo-Aerial in enhancing the\nperformance of ARS vision tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13366v2",
    "published_date": "2024-09-20 10:03:14 UTC",
    "updated_date": "2025-03-31 09:07:12 UTC"
  },
  {
    "arxiv_id": "2409.13363v2",
    "title": "FPBoost: Fully Parametric Gradient Boosting for Survival Analysis",
    "authors": [
      "Alberto Archetti",
      "Eugenio Lomurno",
      "Diego Piccinotti",
      "Matteo Matteucci"
    ],
    "abstract": "Survival analysis is a statistical framework for modeling time-to-event data.\nIt plays a pivotal role in medicine, reliability engineering, and social\nscience research, where understanding event dynamics even with few data samples\nis critical. Recent advancements in machine learning, particularly those\nemploying neural networks and decision trees, have introduced sophisticated\nalgorithms for survival modeling. However, many of these methods rely on\nrestrictive assumptions about the underlying event-time distribution, such as\nproportional hazard, time discretization, or accelerated failure time. In this\nstudy, we propose FPBoost, a survival model that combines a weighted sum of\nfully parametric hazard functions with gradient boosting. Distribution\nparameters are estimated with decision trees trained by maximizing the full\nsurvival likelihood. We show how FPBoost is a universal approximator of hazard\nfunctions, offering full event-time modeling flexibility while maintaining\ninterpretability through the use of well-established parametric distributions.\nWe evaluate concordance and calibration of FPBoost across multiple benchmark\ndatasets, showcasing its robustness and versatility as a new tool for survival\nestimation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13363v2",
    "published_date": "2024-09-20 09:57:17 UTC",
    "updated_date": "2025-01-31 09:32:06 UTC"
  },
  {
    "arxiv_id": "2409.13792v1",
    "title": "Continual Learning for Multimodal Data Fusion of a Soft Gripper",
    "authors": [
      "Nilay Kushawaha",
      "Egidio Falotico"
    ],
    "abstract": "Continual learning (CL) refers to the ability of an algorithm to continuously\nand incrementally acquire new knowledge from its environment while retaining\npreviously learned information. A model trained on one data modality often\nfails when tested with a different modality. A straightforward approach might\nbe to fuse the two modalities by concatenating their features and training the\nmodel on the fused data. However, this requires retraining the model from\nscratch each time it encounters a new domain. In this paper, we introduce a\ncontinual learning algorithm capable of incrementally learning different data\nmodalities by leveraging both class-incremental and domain-incremental learning\nscenarios in an artificial environment where labeled data is scarce, yet\nnon-iid (independent and identical distribution) unlabeled data from the\nenvironment is plentiful. The proposed algorithm is efficient and only requires\nstoring prototypes for each class. We evaluate the algorithm's effectiveness on\na challenging custom multimodal dataset comprising of tactile data from a soft\npneumatic gripper, and visual data from non-stationary images of objects\nextracted from video sequences. Additionally, we conduct an ablation study on\nthe custom dataset and the Core50 dataset to highlight the contributions of\ndifferent components of the algorithm. To further demonstrate the robustness of\nthe algorithm, we perform a real-time experiment for object classification\nusing the soft gripper and an external independent camera setup, all\nsynchronized with the Robot Operating System (ROS) framework.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "15 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13792v1",
    "published_date": "2024-09-20 09:53:27 UTC",
    "updated_date": "2024-09-20 09:53:27 UTC"
  },
  {
    "arxiv_id": "2409.13359v1",
    "title": "EmotionQueen: A Benchmark for Evaluating Empathy of Large Language Models",
    "authors": [
      "Yuyan Chen",
      "Hao Wang",
      "Songzhou Yan",
      "Sijia Liu",
      "Yueze Li",
      "Yi Zhao",
      "Yanghua Xiao"
    ],
    "abstract": "Emotional intelligence in large language models (LLMs) is of great importance\nin Natural Language Processing. However, the previous research mainly focus on\nbasic sentiment analysis tasks, such as emotion recognition, which is not\nenough to evaluate LLMs' overall emotional intelligence. Therefore, this paper\npresents a novel framework named EmotionQueen for evaluating the emotional\nintelligence of LLMs. The framework includes four distinctive tasks: Key Event\nRecognition, Mixed Event Recognition, Implicit Emotional Recognition, and\nIntention Recognition. LLMs are requested to recognize important event or\nimplicit emotions and generate empathetic response. We also design two metrics\nto evaluate LLMs' capabilities in recognition and response for emotion-related\nstatements. Experiments yield significant conclusions about LLMs' capabilities\nand limitations in emotion intelligence.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ACL 2024 (Findings)",
    "pdf_url": "http://arxiv.org/pdf/2409.13359v1",
    "published_date": "2024-09-20 09:44:51 UTC",
    "updated_date": "2024-09-20 09:44:51 UTC"
  },
  {
    "arxiv_id": "2409.13791v1",
    "title": "Multi-omics data integration for early diagnosis of hepatocellular carcinoma (HCC) using machine learning",
    "authors": [
      "Annette Spooner",
      "Mohammad Karimi Moridani",
      "Azadeh Safarchi",
      "Salim Maher",
      "Fatemeh Vafaee",
      "Amany Zekry",
      "Arcot Sowmya"
    ],
    "abstract": "The complementary information found in different modalities of patient data\ncan aid in more accurate modelling of a patient's disease state and a better\nunderstanding of the underlying biological processes of a disease. However, the\nanalysis of multi-modal, multi-omics data presents many challenges, including\nhigh dimensionality and varying size, statistical distribution, scale and\nsignal strength between modalities. In this work we compare the performance of\na variety of ensemble machine learning algorithms that are capable of late\nintegration of multi-class data from different modalities. The ensemble methods\nand their variations tested were i) a voting ensemble, with hard and soft vote,\nii) a meta learner, iii) a multi-modal Adaboost model using a hard vote, a soft\nvote and a meta learner to integrate the modalities on each boosting round, the\nPB-MVBoost model and a novel application of a mixture of experts model. These\nwere compared to simple concatenation as a baseline. We examine these methods\nusing data from an in-house study on hepatocellular carcinoma (HCC), along with\nfour validation datasets on studies from breast cancer and irritable bowel\ndisease (IBD). Using the area under the receiver operating curve as a measure\nof performance we develop models that achieve a performance value of up to 0.85\nand find that two boosted methods, PB-MVBoost and Adaboost with a soft vote\nwere the overall best performing models. We also examine the stability of\nfeatures selected, and the size of the clinical signature determined. Finally,\nwe provide recommendations for the integration of multi-modal multi-class data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13791v1",
    "published_date": "2024-09-20 09:38:02 UTC",
    "updated_date": "2024-09-20 09:38:02 UTC"
  },
  {
    "arxiv_id": "2409.13354v1",
    "title": "Recent Advancement of Emotion Cognition in Large Language Models",
    "authors": [
      "Yuyan Chen",
      "Yanghua Xiao"
    ],
    "abstract": "Emotion cognition in large language models (LLMs) is crucial for enhancing\nperformance across various applications, such as social media, human-computer\ninteraction, and mental health assessment. We explore the current landscape of\nresearch, which primarily revolves around emotion classification, emotionally\nrich response generation, and Theory of Mind assessments, while acknowledge the\nchallenges like dependency on annotated data and complexity in emotion\nprocessing. In this paper, we present a detailed survey of recent progress in\nLLMs for emotion cognition. We explore key research studies, methodologies,\noutcomes, and resources, aligning them with Ulric Neisser's cognitive stages.\nAdditionally, we outline potential future directions for research in this\nevolving field, including unsupervised learning approaches and the development\nof more complex and interpretable emotion cognition LLMs. We also discuss\nadvanced methods such as contrastive learning used to improve LLMs' emotion\ncognition capabilities.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13354v1",
    "published_date": "2024-09-20 09:34:58 UTC",
    "updated_date": "2024-09-20 09:34:58 UTC"
  },
  {
    "arxiv_id": "2409.13349v2",
    "title": "ID-Guard: A Universal Framework for Combating Facial Manipulation via Breaking Identification",
    "authors": [
      "Zuomin Qu",
      "Wei Lu",
      "Xiangyang Luo",
      "Qian Wang",
      "Xiaochun Cao"
    ],
    "abstract": "The misuse of deep learning-based facial manipulation poses a significant\nthreat to civil rights. To prevent this fraud at its source, proactive defense\nhas been proposed to disrupt the manipulation process by adding invisible\nadversarial perturbations into images, making the forged output unconvincing to\nobservers. However, the non-specific disruption against the output may lead to\nthe retention of identifiable facial features, potentially resulting in the\nstigmatization of the individual. This paper proposes a universal framework for\ncombating facial manipulation, termed ID-Guard. Specifically, this framework\noperates with a single forward pass of an encoder-decoder network to produce a\ncross-model transferable adversarial perturbation. A novel Identity Destruction\nModule (IDM) is introduced to degrade identifiable features in forged faces. We\noptimize the perturbation generation by framing the disruption of different\nfacial manipulations as a multi-task learning problem, and a dynamic weight\nstrategy is devised to enhance cross-model performance. Experimental results\ndemonstrate that the proposed ID-Guard exhibits strong efficacy in defending\nagainst various facial manipulation models, effectively degrading identifiable\nregions in manipulated images. It also enables disrupted images to evade facial\ninpainting and image recognition systems. Additionally, ID-Guard can seamlessly\nfunction as a plug-and-play component, integrating with other tasks such as\nadversarial training.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13349v2",
    "published_date": "2024-09-20 09:30:08 UTC",
    "updated_date": "2025-04-10 07:58:51 UTC"
  },
  {
    "arxiv_id": "2409.13346v1",
    "title": "Imagine yourself: Tuning-Free Personalized Image Generation",
    "authors": [
      "Zecheng He",
      "Bo Sun",
      "Felix Juefei-Xu",
      "Haoyu Ma",
      "Ankit Ramchandani",
      "Vincent Cheung",
      "Siddharth Shah",
      "Anmol Kalia",
      "Harihar Subramanyam",
      "Alireza Zareian",
      "Li Chen",
      "Ankit Jain",
      "Ning Zhang",
      "Peizhao Zhang",
      "Roshan Sumbaly",
      "Peter Vajda",
      "Animesh Sinha"
    ],
    "abstract": "Diffusion models have demonstrated remarkable efficacy across various\nimage-to-image tasks. In this research, we introduce Imagine yourself, a\nstate-of-the-art model designed for personalized image generation. Unlike\nconventional tuning-based personalization techniques, Imagine yourself operates\nas a tuning-free model, enabling all users to leverage a shared framework\nwithout individualized adjustments. Moreover, previous work met challenges\nbalancing identity preservation, following complex prompts and preserving good\nvisual quality, resulting in models having strong copy-paste effect of the\nreference images. Thus, they can hardly generate images following prompts that\nrequire significant changes to the reference image, \\eg, changing facial\nexpression, head and body poses, and the diversity of the generated images is\nlow. To address these limitations, our proposed method introduces 1) a new\nsynthetic paired data generation mechanism to encourage image diversity, 2) a\nfully parallel attention architecture with three text encoders and a fully\ntrainable vision encoder to improve the text faithfulness, and 3) a novel\ncoarse-to-fine multi-stage finetuning methodology that gradually pushes the\nboundary of visual quality. Our study demonstrates that Imagine yourself\nsurpasses the state-of-the-art personalization model, exhibiting superior\ncapabilities in identity preservation, visual quality, and text alignment. This\nmodel establishes a robust foundation for various personalization applications.\nHuman evaluation results validate the model's SOTA superiority across all\naspects (identity preservation, text faithfulness, and visual appeal) compared\nto the previous personalization models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13346v1",
    "published_date": "2024-09-20 09:21:49 UTC",
    "updated_date": "2024-09-20 09:21:49 UTC"
  },
  {
    "arxiv_id": "2409.13345v1",
    "title": "A Novel Adaptive Fine-Tuning Algorithm for Multimodal Models: Self-Optimizing Classification and Selection of High-Quality Datasets in Remote Sensing",
    "authors": [
      "Yi Ren",
      "Tianyi Zhang",
      "Zhixiong Han",
      "Weibin Li",
      "Zhiyang Wang",
      "Wenbo Ji",
      "Chenhao Qin",
      "Chenbin Liang",
      "Licheng Jiao"
    ],
    "abstract": "We propose an adaptive fine-tuning algorithm for multimodal large models. The\ncore steps of this algorithm involve two stages of truncation. First, the vast\namount of data is projected into a semantic vector space, and the\nMiniBatchKMeans algorithm is used for automated clustering. This classification\nensures that the data within each cluster exhibit high semantic similarity.\nNext, we process the data in each cluster, calculating the translational\ndifference between the original and perturbed data in the multimodal large\nmodel's vector space. This difference serves as a generalization metric for the\ndata. Based on this metric, we select the data with high generalization\npotential for training. We applied this algorithm to train the\nInternLM-XComposer2-VL-7B model on two 3090 GPUs using one-third of the GeoChat\nmultimodal remote sensing dataset. The results demonstrate that our algorithm\noutperforms the state-of-the-art baselines. various baselines. The model\ntrained on our optimally chosen one-third dataset, based on experimental\nvalidation, exhibited only 1% reduction in performance across various remote\nsensing metrics compared to the model trained on the full dataset. This\napproach significantly preserved general-purpose capabilities while reducing\ntraining time by 68.2%. Furthermore, the model achieved scores of 89.86 and\n77.19 on the UCMerced and AID evaluation datasets, respectively, surpassing the\nGeoChat dataset by 5.43 and 5.16 points. It only showed a 0.91-point average\ndecrease on the LRBEN evaluation dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13345v1",
    "published_date": "2024-09-20 09:19:46 UTC",
    "updated_date": "2024-09-20 09:19:46 UTC"
  },
  {
    "arxiv_id": "2409.13790v2",
    "title": "Revisiting Synthetic Human Trajectories: Imitative Generation and Benchmarks Beyond Datasaurus",
    "authors": [
      "Bangchao Deng",
      "Xin Jing",
      "Tianyue Yang",
      "Bingqing Qu",
      "Dingqi Yang",
      "Philippe Cudre-Mauroux"
    ],
    "abstract": "Human trajectory data, which plays a crucial role in various applications\nsuch as crowd management and epidemic prevention, is challenging to obtain due\nto practical constraints and privacy concerns. In this context, synthetic human\ntrajectory data is generated to simulate as close as possible to real-world\nhuman trajectories, often under summary statistics and distributional\nsimilarities. However, these similarities oversimplify complex human mobility\npatterns (a.k.a. ``Datasaurus''), resulting in intrinsic biases in both\ngenerative model design and benchmarks of the generated trajectories. Against\nthis background, we propose MIRAGE, a huMan-Imitative tRAjectory GenErative\nmodel designed as a neural Temporal Point Process integrating an Exploration\nand Preferential Return model. It imitates the human decision-making process in\ntrajectory generation, rather than fitting any specific statistical\ndistributions as traditional methods do, thus avoiding the Datasaurus issue. We\nalso propose a comprehensive task-based evaluation protocol beyond Datasaurus\nto systematically benchmark trajectory generative models on four typical\ndownstream tasks, integrating multiple techniques and evaluation metrics for\neach task, to assess the ultimate utility of the generated trajectories. We\nconduct a thorough evaluation of MIRAGE on three real-world user trajectory\ndatasets against a sizeable collection of baselines. Results show that compared\nto the best baselines, MIRAGE-generated trajectory data not only achieves the\nbest statistical and distributional similarities with 59.0-67.7% improvement,\nbut also yields the best performance in the task-based evaluation with\n10.9-33.4% improvement. A series of ablation studies also validate the key\ndesign choices of MIRAGE.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by KDD'25",
    "pdf_url": "http://arxiv.org/pdf/2409.13790v2",
    "published_date": "2024-09-20 09:07:27 UTC",
    "updated_date": "2025-05-19 02:35:20 UTC"
  },
  {
    "arxiv_id": "2409.13338v3",
    "title": "Time Awareness in Large Language Models: Benchmarking Fact Recall Across Time",
    "authors": [
      "David Herel",
      "Vojtech Bartek",
      "Jiri Jirak",
      "Tomas Mikolov"
    ],
    "abstract": "Who is the US President? The answer changes depending on when the question is\nasked. While large language models (LLMs) are evaluated on various reasoning\ntasks, they often miss a crucial dimension: time. In real-world scenarios, the\ncorrectness of answers is frequently tied to temporal context. To address this\ngap, we present a novel framework and dataset spanning over 8,000 events from\n2018 to 2024, annotated with day-level granularity and sourced globally across\ndomains such as politics, science, and business. Our TimeShift evaluation\nmethod systematically probes LLMs for temporal reasoning, revealing that base\nmodels often outperform instruction-tuned and synthetic-trained counterparts on\ntime-sensitive recall. Additionally, we find that even large-scale models\nexhibit brittleness in handling paraphrased facts, highlighting unresolved\nchallenges in temporal consistency. By identifying these limitations, our work\nprovides a significant step toward advancing time-aware language models capable\nof adapting to the dynamic nature of real-world knowledge.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13338v3",
    "published_date": "2024-09-20 08:57:20 UTC",
    "updated_date": "2025-05-15 14:13:36 UTC"
  },
  {
    "arxiv_id": "2409.13321v1",
    "title": "SLaVA-CXR: Small Language and Vision Assistant for Chest X-ray Report Automation",
    "authors": [
      "Jinge Wu",
      "Yunsoo Kim",
      "Daqian Shi",
      "David Cliffton",
      "Fenglin Liu",
      "Honghan Wu"
    ],
    "abstract": "Inspired by the success of large language models (LLMs), there is growing\nresearch interest in developing LLMs in the medical domain to assist\nclinicians. However, for hospitals, using closed-source commercial LLMs\ninvolves privacy issues, and developing open-source public LLMs requires\nlarge-scale computational resources, which are usually limited, especially in\nresource-efficient regions and low-income countries. We propose an open-source\nSmall Language and Vision Assistant (SLaVA-CXR) that can be used for Chest\nX-Ray report automation. To efficiently train a small assistant, we first\npropose the Re$^3$Training method, which simulates the cognitive development of\nradiologists and optimizes the model in the Recognition, Reasoning, and\nReporting training manner. Then, we introduce a data synthesis method, RADEX,\nwhich can generate a high-quality and diverse training corpus with privacy\nregulation compliance. The extensive experiments show that our SLaVA-CXR built\non a 2.7B backbone not only outperforms but also achieves 6 times faster\ninference efficiency than previous state-of-the-art larger models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13321v1",
    "published_date": "2024-09-20 08:28:46 UTC",
    "updated_date": "2024-09-20 08:28:46 UTC"
  },
  {
    "arxiv_id": "2409.17174v3",
    "title": "CSCE: Boosting LLM Reasoning by Simultaneous Enhancing of Causal Significance and Consistency",
    "authors": [
      "Kangsheng Wang",
      "Xiao Zhang",
      "Juntao Lyu",
      "Tianyu Hu",
      "Huimin Ma"
    ],
    "abstract": "Chain-based reasoning methods like chain of thought (CoT) play a rising role\nin solving reasoning tasks for large language models (LLMs). However, the\ncausal hallucinations between a step of reasoning and corresponding state\ntransitions are becoming a significant obstacle to advancing LLMs' reasoning\ncapabilities, especially in long-range reasoning tasks. This paper proposes a\nnon-chain-based reasoning framework for simultaneous consideration of causal\nsignificance and consistency, i.e., the Causal Significance and Consistency\nEnhancer (CSCE). We customize LLM's loss function utilizing treatment effect\nassessments to enhance its reasoning ability from two aspects: causal\nsignificance and consistency. This ensures that the model captures essential\ncausal relationships and maintains robust and consistent performance across\nvarious scenarios. Additionally, we transform the reasoning process from the\ncascading multiple one-step reasoning commonly used in Chain-Based methods,\nlike CoT, to a causal-enhanced method that outputs the entire reasoning process\nin one go, further improving the model's reasoning efficiency. Extensive\nexperiments show that our method improves both the reasoning success rate and\nspeed. These improvements further demonstrate that non-chain-based methods can\nalso aid LLMs in completing reasoning tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages,4 figures. This paper has been accepted for presentation at\n  IEEE International Conference on Multimedia & Expo 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.17174v3",
    "published_date": "2024-09-20 08:28:23 UTC",
    "updated_date": "2025-03-24 08:23:08 UTC"
  },
  {
    "arxiv_id": "2409.13312v2",
    "title": "GAProtoNet: A Multi-head Graph Attention-based Prototypical Network for Interpretable Text Classification",
    "authors": [
      "Ximing Wen",
      "Wenjuan Tan",
      "Rosina O. Weber"
    ],
    "abstract": "Pretrained transformer-based Language Models (LMs) are well-known for their\nability to achieve significant improvement on text classification tasks with\ntheir powerful word embeddings, but their black-box nature, which leads to a\nlack of interpretability, has been a major concern. In this work, we introduce\nGAProtoNet, a novel white-box Multi-head Graph Attention-based Prototypical\nNetwork designed to explain the decisions of text classification models built\nwith LM encoders. In our approach, the input vector and prototypes are regarded\nas nodes within a graph, and we utilize multi-head graph attention to\nselectively construct edges between the input node and prototype nodes to learn\nan interpretable prototypical representation. During inference, the model makes\ndecisions based on a linear combination of activated prototypes weighted by the\nattention score assigned for each prototype, allowing its choices to be\ntransparently explained by the attention weights and the prototypes projected\ninto the closest matching training examples. Experiments on multiple public\ndatasets show our approach achieves superior results without sacrificing the\naccuracy of the original black-box LMs. We also compare with four alternative\nprototypical network variations and our approach achieves the best accuracy and\nF1 among all. Our case study and visualization of prototype clusters also\ndemonstrate the efficiency in explaining the decisions of black-box models\nbuilt with LMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages, 5 figues, accepted by COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2409.13312v2",
    "published_date": "2024-09-20 08:15:17 UTC",
    "updated_date": "2024-12-19 20:45:59 UTC"
  },
  {
    "arxiv_id": "2409.13299v2",
    "title": "OMG-RL:Offline Model-based Guided Reward Learning for Heparin Treatment",
    "authors": [
      "Yooseok Lim",
      "Sujee Lee"
    ],
    "abstract": "Accurate medication dosing holds an important position in the overall patient\ntherapeutic process. Therefore, much research has been conducted to develop\noptimal administration strategy based on Reinforcement learning (RL). However,\nRelying solely on a few explicitly defined reward functions makes it difficult\nto learn a treatment strategy that encompasses the diverse characteristics of\nvarious patients. Moreover, the multitude of drugs utilized in clinical\npractice makes it infeasible to construct a dedicated reward function for each\nmedication. Here, we tried to develop a reward network that captures\nclinicians' therapeutic intentions, departing from explicit rewards, and to\nderive an optimal heparin dosing policy. In this study, we introduce Offline\nModel-based Guided Reward Learning (OMG-RL), which performs offline inverse RL\n(IRL). Through OMG-RL, we learn a parameterized reward function that captures\nthe expert's intentions from limited data, thereby enhancing the agent's\npolicy. We validate the proposed approach on the heparin dosing task. We show\nthat OMG-RL policy is positively reinforced not only in terms of the learned\nreward network but also in activated partial thromboplastin time (aPTT), a key\nindicator for monitoring the effects of heparin. This means that the OMG-RL\npolicy adequately reflects clinician's intentions. This approach can be widely\nutilized not only for the heparin dosing problem but also for RL-based\nmedication dosing tasks in general.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13299v2",
    "published_date": "2024-09-20 07:51:37 UTC",
    "updated_date": "2024-12-31 08:27:22 UTC"
  },
  {
    "arxiv_id": "2409.13787v1",
    "title": "Learning to Generalize Unseen Domains via Multi-Source Meta Learning for Text Classification",
    "authors": [
      "Yuxuan Hu",
      "Chenwei Zhang",
      "Min Yang",
      "Xiaodan Liang",
      "Chengming Li",
      "Xiping Hu"
    ],
    "abstract": "With the rapid development of deep learning methods, there have been many\nbreakthroughs in the field of text classification. Models developed for this\ntask have been shown to achieve high accuracy. However, most of these models\nare trained using labeled data from seen domains. It is difficult for these\nmodels to maintain high accuracy in a new challenging unseen domain, which is\ndirectly related to the generalization of the model. In this paper, we study\nthe multi-source Domain Generalization of text classification and propose a\nframework to use multiple seen domains to train a model that can achieve high\naccuracy in an unseen domain. Specifically, we propose a multi-source\nmeta-learning Domain Generalization framework to simulate the process of model\ngeneralization to an unseen domain, so as to extract sufficient domain-related\nfeatures. We introduced a memory mechanism to store domain-specific features,\nwhich coordinate with the meta-learning framework. Besides, we adopt the novel\n\"jury\" mechanism that enables the model to learn sufficient domain-invariant\nfeatures. Experiments demonstrate that our meta-learning framework can\neffectively enhance the ability of the model to generalize to an unseen domain\nand can outperform the state-of-the-art methods on multi-source text\nclassification datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13787v1",
    "published_date": "2024-09-20 07:46:21 UTC",
    "updated_date": "2024-09-20 07:46:21 UTC"
  },
  {
    "arxiv_id": "2409.13284v1",
    "title": "Time Distributed Deep Learning models for Purely Exogenous Forecasting. Application to Water Table Depth Prediction using Weather Image Time Series",
    "authors": [
      "Matteo Salis",
      "Abdourrahmane M. Atto",
      "Stefano Ferraris",
      "Rosa Meo"
    ],
    "abstract": "Groundwater resources are one of the most relevant elements in the water\ncycle, therefore developing models to accurately predict them is a pivotal task\nin the sustainable resources management framework. Deep Learning (DL) models\nhave been revealed very effective in hydrology, especially by feeding spatially\ndistributed data (e.g. raster data). In many regions, hydrological measurements\nare difficult to obtain regularly or periodically in time, and in some cases,\nlast available data are not up to date. Reversely, weather data, which\nsignificantly impacts water resources, are usually more available and with\nhigher quality. More specifically, we have proposed two different DL models to\npredict the water table depth in the Grana-Maira catchment (Piemonte, IT) using\nonly exogenous weather image time series. To deal with the image time series,\nboth models are made of a first Time Distributed Convolutional Neural Network\n(TDC) which encodes the image available at each time step into a vectorial\nrepresentation. The first model, TDC-LSTM uses then a Sequential Module based\non an LSTM layer to learn temporal relations and output the predictions. The\nsecond model, TDC-UnPWaveNet uses instead a new version of the WaveNet\narchitecture, adapted here to output a sequence shorter and completely shifted\nin the future with respect to the input one. To this aim, and to deal with the\ndifferent sequence lengths in the UnPWaveNet, we have designed a new Channel\nDistributed layer, that acts like a Time Distributed one but on the channel\ndimension, i.e. applying the same set of operations to each channel of the\ninput. TDC-LSTM and TDC-UnPWaveNet have shown both remarkable results. However,\nthe two models have focused on different learnable information: TDC-LSTM has\nfocused more on lowering the bias, while the TDC-UnPWaveNet has focused more on\nthe temporal dynamics maximising correlation and KGE.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13284v1",
    "published_date": "2024-09-20 07:25:54 UTC",
    "updated_date": "2024-09-20 07:25:54 UTC"
  },
  {
    "arxiv_id": "2409.13259v1",
    "title": "A generalizable framework for unlocking missing reactions in genome-scale metabolic networks using deep learning",
    "authors": [
      "Xiaoyi Liu",
      "Hongpeng Yang",
      "Chengwei Ai",
      "Ruihan Dong",
      "Yijie Ding",
      "Qianqian Yuan",
      "Jijun Tang",
      "Fei Guo"
    ],
    "abstract": "Incomplete knowledge of metabolic processes hinders the accuracy of\nGEnome-scale Metabolic models (GEMs), which in turn impedes advancements in\nsystems biology and metabolic engineering. Existing gap-filling methods\ntypically rely on phenotypic data to minimize the disparity between\ncomputational predictions and experimental results. However, there is still a\nlack of an automatic and precise gap-filling method for initial state GEMs\nbefore experimental data and annotated genomes become available. In this study,\nwe introduce CLOSEgaps, a deep learning-driven tool that addresses the\ngap-filling issue by modeling it as a hyperedge prediction problem within GEMs.\nSpecifically, CLOSEgaps maps metabolic networks as hypergraphs and learns their\nhyper-topology features to identify missing reactions and gaps by leveraging\nhypothetical reactions. This innovative approach allows for the\ncharacterization and curation of both known and hypothetical reactions within\nmetabolic networks. Extensive results demonstrate that CLOSEgaps accurately\ngap-filling over 96% of artificially introduced gaps for various GEMs.\nFurthermore, CLOSEgaps enhances phenotypic predictions for 24 GEMs and also\nfinds a notable improvement in producing four crucial metabolites (Lactate,\nEthanol, Propionate, and Succinate) in two organisms. As a broadly applicable\nsolution for any GEM, CLOSEgaps represents a promising model to automate the\ngap-filling process and uncover missing connections between reactions and\nobserved metabolic phenotypes.",
    "categories": [
      "q-bio.MN",
      "cs.AI"
    ],
    "primary_category": "q-bio.MN",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13259v1",
    "published_date": "2024-09-20 06:47:44 UTC",
    "updated_date": "2024-09-20 06:47:44 UTC"
  },
  {
    "arxiv_id": "2409.15377v1",
    "title": "Prompting Large Language Models for Supporting the Differential Diagnosis of Anemia",
    "authors": [
      "Elisa Castagnari",
      "Lillian Muyama",
      "Adrien Coulet"
    ],
    "abstract": "In practice, clinicians achieve a diagnosis by following a sequence of steps,\nsuch as laboratory exams, observations, or imaging. The pathways to reach\ndiagnosis decisions are documented by guidelines authored by expert\norganizations, which guide clinicians to reach a correct diagnosis through\nthese sequences of steps. While these guidelines are beneficial for following\nmedical reasoning and consolidating medical knowledge, they have some\ndrawbacks. They often fail to address patients with uncommon conditions due to\ntheir focus on the majority population, and are slow and costly to update,\nmaking them unsuitable for rapidly emerging diseases or new practices. Inspired\nby clinical guidelines, our study aimed to develop pathways similar to those\nthat can be obtained in clinical guidelines. We tested three Large Language\nModels (LLMs) -Generative Pretrained Transformer 4 (GPT-4), Large Language\nModel Meta AI (LLaMA), and Mistral -on a synthetic yet realistic dataset to\ndifferentially diagnose anemia and its subtypes. By using advanced prompting\ntechniques to enhance the decision-making process, we generated diagnostic\npathways using these models. Experimental results indicate that LLMs hold huge\npotential in clinical pathway discovery from patient data, with GPT-4\nexhibiting the best performance in all conducted experiments.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.15377v1",
    "published_date": "2024-09-20 06:47:36 UTC",
    "updated_date": "2024-09-20 06:47:36 UTC"
  },
  {
    "arxiv_id": "2409.13254v1",
    "title": "Emergent Collective Reproduction via Evolving Neuronal Flocks",
    "authors": [
      "Nam H. Le",
      "Richard Watson",
      "Mike Levin",
      "Chrys Buckley"
    ],
    "abstract": "This study facilitates the understanding of evolutionary transitions in\nindividuality (ETIs) through a novel artificial life framework, named VitaNova,\nthat intricately merges self-organization and natural selection to simulate the\nemergence of complex, reproductive groups. By dynamically modelling individual\nagents within an environment that challenges them with predators and spatial\nconstraints, VitaNova elucidates the mechanisms by which simple agents evolve\ninto cohesive units exhibiting collective reproduction. The findings underscore\nthe synergy between self-organized behaviours and adaptive evolutionary\nstrategies as fundamental drivers of ETIs. This approach not only contributes\nto a deeper understanding of higher-order biological individuality but also\nsets a new precedent in the empirical investigation of ETIs, challenging and\nextending current theoretical frameworks.",
    "categories": [
      "q-bio.PE",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "q-bio.PE",
    "comment": "9 pages, 10 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2409.13254v1",
    "published_date": "2024-09-20 06:22:24 UTC",
    "updated_date": "2024-09-20 06:22:24 UTC"
  },
  {
    "arxiv_id": "2409.13252v1",
    "title": "Leveraging Knowledge Graphs and LLMs to Support and Monitor Legislative Systems",
    "authors": [
      "Andrea Colombo"
    ],
    "abstract": "Knowledge Graphs (KGs) have been used to organize large datasets into\nstructured, interconnected information, enhancing data analytics across various\nfields. In the legislative context, one potential natural application of KGs is\nmodeling the intricate set of interconnections that link laws and their\narticles with each other and the broader legislative context.\n  At the same time, the rise of large language models (LLMs) such as GPT has\nopened new opportunities in legal applications, such as text generation and\ndocument drafting. Despite their potential, the use of LLMs in legislative\ncontexts is critical since it requires the absence of hallucinations and\nreliance on up-to-date information, as new laws are published on a daily basis.\n  This work investigates how Legislative Knowledge Graphs and LLMs can\nsynergize and support legislative processes. We address three key questions:\nthe benefits of using KGs for legislative systems, how LLM can support\nlegislative activities by ensuring an accurate output, and how we can allow\nnon-technical users to use such technologies in their activities. To this aim,\nwe develop Legis AI Platform, an interactive platform focused on Italian\nlegislation that enhances the possibility of conducting legislative analysis\nand that aims to support lawmaking activities.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13252v1",
    "published_date": "2024-09-20 06:21:03 UTC",
    "updated_date": "2024-09-20 06:21:03 UTC"
  },
  {
    "arxiv_id": "2409.13244v2",
    "title": "From Cognition to Precognition: A Future-Aware Framework for Social Navigation",
    "authors": [
      "Zeying Gong",
      "Tianshuai Hu",
      "Ronghe Qiu",
      "Junwei Liang"
    ],
    "abstract": "To navigate safely and efficiently in crowded spaces, robots should not only\nperceive the current state of the environment but also anticipate future human\nmovements. In this paper, we propose a reinforcement learning architecture,\nnamely Falcon, to tackle socially-aware navigation by explicitly predicting\nhuman trajectories and penalizing actions that block future human paths. To\nfacilitate realistic evaluation, we introduce a novel SocialNav benchmark\ncontaining two new datasets, Social-HM3D and Social-MP3D. This benchmark offers\nlarge-scale photo-realistic indoor scenes populated with a reasonable amount of\nhuman agents based on scene area size, incorporating natural human movements\nand trajectory patterns. We conduct a detailed experimental analysis with the\nstate-of-the-art learning-based method and two classic rule-based path-planning\nalgorithms on the new benchmark. The results demonstrate the importance of\nfuture prediction and our method achieves the best task success rate of 55%\nwhile maintaining about 90% personal space compliance. We will release our code\nand datasets. Videos of demonstrations can be viewed at\nhttps://zeying-gong.github.io/projects/falcon/ .",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Social Navigation; Trajectory Prediction; Auxiliary Tasks. This paper\n  has been accepted at the IEEE International Conference on Robotics and\n  Automation (ICRA) 2025. For more details, please refer to the project\n  website: https://zeying-gong.github.io/projects/falcon/",
    "pdf_url": "http://arxiv.org/pdf/2409.13244v2",
    "published_date": "2024-09-20 06:08:24 UTC",
    "updated_date": "2025-02-08 15:07:56 UTC"
  },
  {
    "arxiv_id": "2409.13232v2",
    "title": "Relationship between Uncertainty in DNNs and Adversarial Attacks",
    "authors": [
      "Mabel Ogonna",
      "Abigail Adeniran",
      "Adewale Adeyemo"
    ],
    "abstract": "Deep Neural Networks (DNNs) have achieved state of the art results and even\noutperformed human accuracy in many challenging tasks, leading to DNNs adoption\nin a variety of fields including natural language processing, pattern\nrecognition, prediction, and control optimization. However, DNNs are\naccompanied by uncertainty about their results, causing them to predict an\noutcome that is either incorrect or outside of a certain level of confidence.\nThese uncertainties stem from model or data constraints, which could be\nexacerbated by adversarial attacks. Adversarial attacks aim to provide\nperturbed input to DNNs, causing the DNN to make incorrect predictions or\nincrease model uncertainty. In this review, we explore the relationship between\nDNN uncertainty and adversarial attacks, emphasizing how adversarial attacks\nmight raise DNN uncertainty.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "review",
    "pdf_url": "http://arxiv.org/pdf/2409.13232v2",
    "published_date": "2024-09-20 05:38:38 UTC",
    "updated_date": "2025-02-24 21:31:47 UTC"
  },
  {
    "arxiv_id": "2409.17173v1",
    "title": "A Multiple-Fill-in-the-Blank Exam Approach for Enhancing Zero-Resource Hallucination Detection in Large Language Models",
    "authors": [
      "Satoshi Munakata",
      "Taku Fukui",
      "Takao Mohri"
    ],
    "abstract": "Large language models (LLMs) often fabricate a hallucinatory text. Several\nmethods have been developed to detect such text by semantically comparing it\nwith the multiple versions probabilistically regenerated. However, a\nsignificant issue is that if the storyline of each regenerated text changes,\nthe generated texts become incomparable, which worsen detection accuracy. In\nthis paper, we propose a hallucination detection method that incorporates a\nmultiple-fill-in-the-blank exam approach to address this storyline-changing\nissue. First, our method creates a multiple-fill-in-the-blank exam by masking\nmultiple objects from the original text. Second, prompts an LLM to repeatedly\nanswer this exam. This approach ensures that the storylines of the exam answers\nalign with the original ones. Finally, quantifies the degree of hallucination\nfor each original sentence by scoring the exam answers, considering the\npotential for \\emph{hallucination snowballing} within the original text itself.\nExperimental results show that our method alone not only outperforms existing\nmethods, but also achieves clearer state-of-the-art performance in the\nensembles with existing methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50",
      "F.2.2; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.17173v1",
    "published_date": "2024-09-20 04:34:30 UTC",
    "updated_date": "2024-09-20 04:34:30 UTC"
  },
  {
    "arxiv_id": "2409.13208v3",
    "title": "Redefining Data Pairing for Motion Retargeting Leveraging a Human Body Prior",
    "authors": [
      "Xiyana Figuera",
      "Soogeun Park",
      "Hyemin Ahn"
    ],
    "abstract": "We propose MR HuBo(Motion Retargeting leveraging a HUman BOdy prior), a\ncost-effective and convenient method to collect high-quality upper body paired\n<robot, human> pose data, which is essential for data-driven motion retargeting\nmethods. Unlike existing approaches which collect <robot, human> pose data by\nconverting human MoCap poses into robot poses, our method goes in reverse. We\nfirst sample diverse random robot poses, and then convert them into human\nposes. However, since random robot poses can result in extreme and infeasible\nhuman poses, we propose an additional technique to sort out extreme poses by\nexploiting a human body prior trained from a large amount of human pose data.\nOur data collection method can be used for any humanoid robots, if one designs\nor optimizes the system's hyperparameters which include a size scale factor and\nthe joint angle ranges for sampling. In addition to this data collection\nmethod, we also present a two-stage motion retargeting neural network that can\nbe trained via supervised learning on a large amount of paired data. Compared\nto other learning-based methods trained via unsupervised learning, we found\nthat our deep neural network trained with ample high-quality paired data\nachieved notable performance. Our experiments also show that our data filtering\nmethod yields better retargeting results than training the model with raw and\nnoisy data. Our code and video results are available on\nhttps://sites.google.com/view/mr-hubo/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 5 Figures, Accepted at IROS 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.13208v3",
    "published_date": "2024-09-20 04:32:54 UTC",
    "updated_date": "2024-10-01 06:42:29 UTC"
  },
  {
    "arxiv_id": "2409.15376v1",
    "title": "ControlMath: Controllable Data Generation Promotes Math Generalist Models",
    "authors": [
      "Nuo Chen",
      "Ning Wu",
      "Jianhui Chang",
      "Jia Li"
    ],
    "abstract": "Utilizing large language models (LLMs) for data augmentation has yielded\nencouraging results in mathematical reasoning. However, these approaches face\nconstraints in problem diversity, potentially restricting them to\nin-domain/distribution data generation. To this end, we propose ControlMath, an\niterative method involving an equation-generator module and two LLM-based\nagents. The module creates diverse equations, which the Problem-Crafter agent\nthen transforms into math word problems. The Reverse-Agent filters and selects\nhigh-quality data, adhering to the \"less is more\" principle, achieving better\nresults with fewer data points. This approach enables the generation of diverse\nmath problems, not limited to specific domains or distributions. As a result,\nwe collect ControlMathQA, which involves 190k math word problems. Extensive\nresults prove that combining our dataset with in-domain datasets like GSM8K can\nhelp improve the model's mathematical ability to generalize, leading to\nimproved performances both within and beyond specific domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2409.15376v1",
    "published_date": "2024-09-20 03:58:26 UTC",
    "updated_date": "2024-09-20 03:58:26 UTC"
  },
  {
    "arxiv_id": "2409.13191v2",
    "title": "Diabetica: Adapting Large Language Model to Enhance Multiple Medical Tasks in Diabetes Care and Management",
    "authors": [
      "Lai Wei",
      "Zhen Ying",
      "Muyang He",
      "Yutong Chen",
      "Qian Yang",
      "Yanzhe Hong",
      "Jiaping Lu",
      "Kaipeng Zheng",
      "Shaoting Zhang",
      "Xiaoying Li",
      "Weiran Huang",
      "Ying Chen"
    ],
    "abstract": "Diabetes is a chronic disease with a significant global health burden,\nrequiring multi-stakeholder collaboration for optimal management. Large\nlanguage models (LLMs) have shown promise in various healthcare scenarios, but\ntheir effectiveness across diverse diabetes tasks remains unproven. Our study\nintroduced a framework to train and validate diabetes-specific LLMs. We first\ndeveloped a comprehensive data processing pipeline that includes data\ncollection, filtering, augmentation and refinement. This created a\nhigh-quality, diabetes-specific dataset and evaluation benchmarks from scratch.\nFine-tuned on the collected training dataset, our diabetes-specific LLM family\ndemonstrated state-of-the-art proficiency in processing various diabetes tasks\ncompared to other LLMs. Furthermore, clinical studies revealed the potential\napplications of our models in diabetes care, including providing personalized\nhealthcare, assisting medical education, and streamlining clinical tasks.\nGenerally, our introduced framework helps develop diabetes-specific LLMs and\nhighlights their potential to enhance clinical practice and provide\npersonalized, data-driven support for diabetes management across different end\nusers. Our codes, benchmarks and models are available at\nhttps://github.com/waltonfuture/Diabetica.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by ICLR 2025 SCI-FM workshop",
    "pdf_url": "http://arxiv.org/pdf/2409.13191v2",
    "published_date": "2024-09-20 03:47:54 UTC",
    "updated_date": "2025-03-13 13:20:17 UTC"
  },
  {
    "arxiv_id": "2409.13187v2",
    "title": "Cooperative Resilience in Artificial Intelligence Multiagent Systems",
    "authors": [
      "Manuela Chacon-Chamorro",
      "Luis Felipe Giraldo",
      "Nicanor Quijano",
      "Vicente Vargas-Panesso",
      "César González",
      "Juan Sebastián Pinzón",
      "Rubén Manrique",
      "Manuel Ríos",
      "Yesid Fonseca",
      "Daniel Gómez-Barrera",
      "Mónica Perdomo-Pérez"
    ],
    "abstract": "Resilience refers to the ability of systems to withstand, adapt to, and\nrecover from disruptive events. While studies on resilience have attracted\nsignificant attention across various research domains, the precise definition\nof this concept within the field of cooperative artificial intelligence remains\nunclear. This paper addresses this gap by proposing a clear definition of\n`cooperative resilience' and outlining a methodology for its quantitative\nmeasurement. The methodology is validated in an environment with RL-based and\nLLM-augmented autonomous agents, subjected to environmental changes and the\nintroduction of agents with unsustainable behaviors. These events are\nparameterized to create various scenarios for measuring cooperative resilience.\nThe results highlight the crucial role of resilience metrics in analyzing how\nthe collective system prepares for, resists, recovers from, sustains\nwell-being, and transforms in the face of disruptions. These findings provide\nfoundational insights into the definition, measurement, and preliminary\nanalysis of cooperative resilience, offering significant implications for the\nbroader field of AI. Moreover, the methodology and metrics developed here can\nbe adapted to a wide range of AI applications, enhancing the reliability and\neffectiveness of AI in dynamic and unpredictable environments.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "Supplementary material in\n  https://github.com/mavivi95/resilience/blob/main/Supplementary_File.pdf",
    "pdf_url": "http://arxiv.org/pdf/2409.13187v2",
    "published_date": "2024-09-20 03:28:48 UTC",
    "updated_date": "2024-09-24 17:13:07 UTC"
  },
  {
    "arxiv_id": "2409.13180v2",
    "title": "FreeAvatar: Robust 3D Facial Animation Transfer by Learning an Expression Foundation Model",
    "authors": [
      "Feng Qiu",
      "Wei Zhang",
      "Chen Liu",
      "Rudong An",
      "Lincheng Li",
      "Yu Ding",
      "Changjie Fan",
      "Zhipeng Hu",
      "Xin Yu"
    ],
    "abstract": "Video-driven 3D facial animation transfer aims to drive avatars to reproduce\nthe expressions of actors. Existing methods have achieved remarkable results by\nconstraining both geometric and perceptual consistency. However, geometric\nconstraints (like those designed on facial landmarks) are insufficient to\ncapture subtle emotions, while expression features trained on classification\ntasks lack fine granularity for complex emotions. To address this, we propose\n\\textbf{FreeAvatar}, a robust facial animation transfer method that relies\nsolely on our learned expression representation. Specifically, FreeAvatar\nconsists of two main components: the expression foundation model and the facial\nanimation transfer model. In the first component, we initially construct a\nfacial feature space through a face reconstruction task and then optimize the\nexpression feature space by exploring the similarities among different\nexpressions. Benefiting from training on the amounts of unlabeled facial images\nand re-collected expression comparison dataset, our model adapts freely and\neffectively to any in-the-wild input facial images. In the facial animation\ntransfer component, we propose a novel Expression-driven Multi-avatar Animator,\nwhich first maps expressive semantics to the facial control parameters of 3D\navatars and then imposes perceptual constraints between the input and output\nimages to maintain expression consistency. To make the entire process\ndifferentiable, we employ a trained neural renderer to translate rig parameters\ninto corresponding images. Furthermore, unlike previous methods that require\nseparate decoders for each avatar, we propose a dynamic identity injection\nmodule that allows for the joint training of multiple avatars within a single\nnetwork.",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "primary_category": "cs.GR",
    "comment": "11 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13180v2",
    "published_date": "2024-09-20 03:17:01 UTC",
    "updated_date": "2024-10-09 02:29:57 UTC"
  },
  {
    "arxiv_id": "2409.13783v1",
    "title": "A Value Based Parallel Update MCTS Method for Multi-Agent Cooperative Decision Making of Connected and Automated Vehicles",
    "authors": [
      "Ye Han",
      "Lijun Zhang",
      "Dejian Meng",
      "Xingyu Hu",
      "Songyu Weng"
    ],
    "abstract": "To solve the problem of lateral and logitudinal joint decision-making of\nmulti-vehicle cooperative driving for connected and automated vehicles (CAVs),\nthis paper proposes a Monte Carlo tree search (MCTS) method with parallel\nupdate for multi-agent Markov game with limited horizon and time discounted\nsetting. By analyzing the parallel actions in the multi-vehicle joint action\nspace in the partial-steady-state traffic flow, the parallel update method can\nquickly exclude potential dangerous actions, thereby increasing the search\ndepth without sacrificing the search breadth. The proposed method is tested in\na large number of randomly generated traffic flow. The experiment results show\nthat the algorithm has good robustness and better performance than the SOTA\nreinforcement learning algorithms and heuristic methods. The vehicle driving\nstrategy using the proposed algorithm shows rationality beyond human drivers,\nand has advantages in traffic efficiency and safety in the coordinating zone.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.MA",
    "comment": "arXiv admin note: text overlap with arXiv:2408.04295 by other authors",
    "pdf_url": "http://arxiv.org/pdf/2409.13783v1",
    "published_date": "2024-09-20 03:13:01 UTC",
    "updated_date": "2024-09-20 03:13:01 UTC"
  },
  {
    "arxiv_id": "2409.13167v1",
    "title": "Unsupervised Attention-Based Multi-Source Domain Adaptation Framework for Drift Compensation in Electronic Nose Systems",
    "authors": [
      "Wenwen Zhang",
      "Shuhao Hu",
      "Zhengyuan Zhang",
      "Yuanjin Zheng",
      "Qi Jie Wang",
      "Zhiping Lin"
    ],
    "abstract": "Continuous, long-term monitoring of hazardous, noxious, explosive, and\nflammable gases in industrial environments using electronic nose (E-nose)\nsystems faces the significant challenge of reduced gas identification accuracy\ndue to time-varying drift in gas sensors. To address this issue, we propose a\nnovel unsupervised attention-based multi-source domain shared-private feature\nfusion adaptation (AMDS-PFFA) framework for gas identification with drift\ncompensation in E-nose systems. The AMDS-PFFA model effectively leverages\nlabeled data from multiple source domains collected during the initial stage to\naccurately identify gases in unlabeled gas sensor array drift signals from the\ntarget domain. To validate the model's effectiveness, extensive experimental\nevaluations were conducted using both the University of California, Irvine\n(UCI) standard drift gas dataset, collected over 36 months, and drift signal\ndata from our self-developed E-nose system, spanning 30 months. Compared to\nrecent drift compensation methods, the AMDS-PFFA model achieves the highest\naverage gas recognition accuracy with strong convergence, attaining 83.20% on\nthe UCI dataset and 93.96% on data from our self-developed E-nose system across\nall target domain batches. These results demonstrate the superior performance\nof the AMDS-PFFA model in gas identification with drift compensation,\nsignificantly outperforming existing methods.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2409.13167v1",
    "published_date": "2024-09-20 02:47:05 UTC",
    "updated_date": "2024-09-20 02:47:05 UTC"
  },
  {
    "arxiv_id": "2409.13166v1",
    "title": "Morphology and Behavior Co-Optimization of Modular Satellites for Attitude Control",
    "authors": [
      "Yuxing Wang",
      "Jie Li",
      "Cong Yu",
      "Xinyang Li",
      "Simeng Huang",
      "Yongzhe Chang",
      "Xueqian Wang",
      "Bin Liang"
    ],
    "abstract": "The emergence of modular satellites marks a significant transformation in\nspacecraft engineering, introducing a new paradigm of flexibility, resilience,\nand scalability in space exploration endeavors. In addressing complex\nchallenges such as attitude control, both the satellite's morphological\narchitecture and the controller are crucial for optimizing performance. Despite\nsubstantial research on optimal control, there remains a significant gap in\ndeveloping optimized and practical assembly strategies for modular satellites\ntailored to specific mission constraints. This research gap primarily arises\nfrom the inherently complex nature of co-optimizing design and control, a\nprocess known for its notorious bi-level optimization loop. Conventionally\ntackled through artificial evolution, this issue involves optimizing the\nmorphology based on the fitness of individual controllers, which is\nsample-inefficient and computationally expensive. In this paper, we introduce a\nnovel gradient-based approach to simultaneously optimize both morphology and\ncontrol for modular satellites, enhancing their performance and efficiency in\nattitude control missions. Our Monte Carlo simulations demonstrate that this\nco-optimization approach results in modular satellites with better mission\nperformance compared to those designed by evolution-based approaches.\nFurthermore, this study discusses potential avenues for future research.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "The paper was accepted as an oral presentation by the 75th\n  International Astronautical Congress, Milan, Italy",
    "pdf_url": "http://arxiv.org/pdf/2409.13166v1",
    "published_date": "2024-09-20 02:43:53 UTC",
    "updated_date": "2024-09-20 02:43:53 UTC"
  },
  {
    "arxiv_id": "2409.15375v1",
    "title": "DS2TA: Denoising Spiking Transformer with Attenuated Spatiotemporal Attention",
    "authors": [
      "Boxun Xu",
      "Hejia Geng",
      "Yuxuan Yin",
      "Peng Li"
    ],
    "abstract": "Vision Transformers (ViT) are current high-performance models of choice for\nvarious vision applications. Recent developments have given rise to\nbiologically inspired spiking transformers that thrive in ultra-low power\noperations on neuromorphic hardware, however, without fully unlocking the\npotential of spiking neural networks. We introduce DS2TA, a Denoising Spiking\ntransformer with attenuated SpatioTemporal Attention, designed specifically for\nvision applications. DS2TA introduces a new spiking attenuated spatiotemporal\nattention mechanism that considers input firing correlations occurring in both\ntime and space, thereby fully harnessing the computational power of spiking\nneurons at the core of the transformer architecture. Importantly, DS2TA\nfacilitates parameter-efficient spatiotemporal attention computation without\nintroducing extra weights. DS2TA employs efficient hashmap-based nonlinear\nspiking attention denoisers to enhance the robustness and expressive power of\nspiking attention maps. DS2TA demonstrates state-of-the-art performances on\nseveral widely adopted static image and dynamic neuromorphic datasets. Operated\nover 4 time steps, DS2TA achieves 94.92% top-1 accuracy on CIFAR10 and 77.47%\ntop-1 accuracy on CIFAR100, as well as 79.1% and 94.44% on CIFAR10-DVS and\nDVS-Gesture using 10 time steps.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "arXiv admin note: text overlap with arXiv:2311.09376",
    "pdf_url": "http://arxiv.org/pdf/2409.15375v1",
    "published_date": "2024-09-20 02:26:04 UTC",
    "updated_date": "2024-09-20 02:26:04 UTC"
  },
  {
    "arxiv_id": "2409.13153v2",
    "title": "Towards Efficient Neuro-Symbolic AI: From Workload Characterization to Hardware Architecture",
    "authors": [
      "Zishen Wan",
      "Che-Kai Liu",
      "Hanchen Yang",
      "Ritik Raj",
      "Chaojian Li",
      "Haoran You",
      "Yonggan Fu",
      "Cheng Wan",
      "Sixu Li",
      "Youbin Kim",
      "Ananda Samajdar",
      "Yingyan Celine Lin",
      "Mohamed Ibrahim",
      "Jan M. Rabaey",
      "Tushar Krishna",
      "Arijit Raychowdhury"
    ],
    "abstract": "The remarkable advancements in artificial intelligence (AI), primarily driven\nby deep neural networks, are facing challenges surrounding unsustainable\ncomputational trajectories, limited robustness, and a lack of explainability.\nTo develop next-generation cognitive AI systems, neuro-symbolic AI emerges as a\npromising paradigm, fusing neural and symbolic approaches to enhance\ninterpretability, robustness, and trustworthiness, while facilitating learning\nfrom much less data. Recent neuro-symbolic systems have demonstrated great\npotential in collaborative human-AI scenarios with reasoning and cognitive\ncapabilities. In this paper, we aim to understand the workload characteristics\nand potential architectures for neuro-symbolic AI. We first systematically\ncategorize neuro-symbolic AI algorithms, and then experimentally evaluate and\nanalyze them in terms of runtime, memory, computational operators, sparsity,\nand system characteristics on CPUs, GPUs, and edge SoCs. Our studies reveal\nthat neuro-symbolic models suffer from inefficiencies on off-the-shelf\nhardware, due to the memory-bound nature of vector-symbolic and logical\noperations, complex flow control, data dependencies, sparsity variations, and\nlimited scalability. Based on profiling insights, we suggest cross-layer\noptimization solutions and present a hardware acceleration case study for\nvector-symbolic architecture to improve the performance, efficiency, and\nscalability of neuro-symbolic computing. Finally, we discuss the challenges and\npotential future directions of neuro-symbolic AI from both system and\narchitectural perspectives.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "14 pages, 11 figures, 7 tables; IEEE Transactions on Circuits and\n  Systems for Artificial Intelligence (TCASAI), 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.13153v2",
    "published_date": "2024-09-20 01:32:14 UTC",
    "updated_date": "2024-09-23 01:30:12 UTC"
  },
  {
    "arxiv_id": "2409.13147v1",
    "title": "The Impact of Feature Embedding Placement in the Ansatz of a Quantum Kernel in QSVMs",
    "authors": [
      "Ilmo Salmenperä",
      "Ilmars Kuhtarskis",
      "Arianne Meijer van de Griend",
      "Jukka K. Nurminen"
    ],
    "abstract": "Designing a useful feature map for a quantum kernel is a critical task when\nattempting to achieve an advantage over classical machine learning models. The\nchoice of circuit architecture, i.e. how feature-dependent gates should be\ninterwoven with other gates is a relatively unexplored problem and becomes very\nimportant when using a model of quantum kernels called Quantum Embedding\nKernels (QEK). We study and categorize various architectural patterns in QEKs\nand show that existing architectural styles do not behave as the literature\nsupposes. We also produce a novel alternative architecture based on the old\nones and show that it performs equally well while containing fewer gates than\nits older counterparts.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "9 pages including references and appendix, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2409.13147v1",
    "published_date": "2024-09-20 01:25:13 UTC",
    "updated_date": "2024-09-20 01:25:13 UTC"
  },
  {
    "arxiv_id": "2409.13138v2",
    "title": "Learning to Compare Hardware Designs for High-Level Synthesis",
    "authors": [
      "Yunsheng Bai",
      "Atefeh Sohrabizadeh",
      "Zijian Ding",
      "Rongjian Liang",
      "Weikai Li",
      "Ding Wang",
      "Haoxing Ren",
      "Yizhou Sun",
      "Jason Cong"
    ],
    "abstract": "High-level synthesis (HLS) is an automated design process that transforms\nhigh-level code into hardware designs, enabling the rapid development of\nhardware accelerators. HLS relies on pragmas, which are directives inserted\ninto the source code to guide the synthesis process, and pragmas have various\nsettings and values that significantly impact the resulting hardware design.\nState-of-the-art ML-based HLS methods, such as HARP, first train a deep\nlearning model, typically based on graph neural networks (GNNs) applied to\ngraph-based representations of the source code and pragmas. They then perform\ndesign space exploration (DSE) to explore the pragma design space, rank\ncandidate designs using the model, and return the top designs. However,\ntraditional DSE methods face challenges due to the highly nonlinear\nrelationship between pragma settings and performance metrics, along with\ncomplex interactions between pragmas that affect performance in non-obvious\nways.\n  To address these challenges, we propose compareXplore, a novel approach that\nlearns to compare hardware designs for effective HLS optimization.\nCompareXplore introduces a hybrid loss function that combines pairwise\npreference learning with pointwise performance prediction, enabling the model\nto capture both relative preferences and absolute performance. Moreover, we\nintroduce a novel node difference attention module that focuses on the most\ninformative differences between designs, enabling the model to identify\ncritical pragmas impacting performance. CompareXplore adopts a two-stage DSE,\nwhere a pointwise prediction model is used for the initial design pruning,\nfollowed by a pairwise comparison stage for precise performance verification.\nIn extensive experiments, compareXplore achieves significant improvements in\nranking metrics and generates high-quality HLS results for the selected\ndesigns, outperforming the existing SOTA method.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "Published in MLCAD 2024",
    "pdf_url": "http://arxiv.org/pdf/2409.13138v2",
    "published_date": "2024-09-20 00:47:29 UTC",
    "updated_date": "2025-05-07 19:58:28 UTC"
  },
  {
    "arxiv_id": "2409.13137v1",
    "title": "Interpret the Predictions of Deep Networks via Re-Label Distillation",
    "authors": [
      "Yingying Hua",
      "Shiming Ge",
      "Daichi Zhang"
    ],
    "abstract": "Interpreting the predictions of a black-box deep network can facilitate the\nreliability of its deployment. In this work, we propose a re-label distillation\napproach to learn a direct map from the input to the prediction in a\nself-supervision manner. The image is projected into a VAE subspace to generate\nsome synthetic images by randomly perturbing its latent vector. Then, these\nsynthetic images can be annotated into one of two classes by identifying\nwhether their labels shift. After that, using the labels annotated by the deep\nnetwork as teacher, a linear student model is trained to approximate the\nannotations by mapping these synthetic images to the classes. In this manner,\nthese re-labeled synthetic images can well describe the local classification\nmechanism of the deep network, and the learned student can provide a more\nintuitive explanation towards the predictions. Extensive experiments verify the\neffectiveness of our approach qualitatively and quantitatively.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "Published by IEEE ICME 2021",
    "pdf_url": "http://arxiv.org/pdf/2409.13137v1",
    "published_date": "2024-09-20 00:46:22 UTC",
    "updated_date": "2024-09-20 00:46:22 UTC"
  }
]
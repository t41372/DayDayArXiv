{
  "date": "2025-03-07",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2025-03-07 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 118 篇论文，主要聚焦 AI 领域，包括 LLM 优化、强化学习应用、图像生成和机器人决策等前沿话题，亮点在于因果推理框架和 AI 治理策略，如 Dan Hendrycks 等知名学者的 \"Superintelligence Strategy\"，这些文章展示了 AI 在科学、安全和多模态处理中的潜力。\n\n### 重点论文亮点\n今天的核心论文多围绕 AI 模型提升和应用创新，我挑选了最具话题度和影响力的几篇进行详细讨论（按重要度排序），并快速掠过其他次要内容。以下聚焦于 AI 相关的高影响力工作。\n\n**1. Black Box Causal Inference: Effect Estimation via Meta Prediction（黑箱因果推理：通过元预测进行效果估计）**  \n   作者包括 Kyunghyun Cho 和 Rajesh Ranganath 等知名学者。论文提出 Black Box Causal Inference (BBCI) 框架，将因果效应估计转化为数据集级预测问题，通过学习从样本数据对中预测因果效果，实现对平均治疗效果 (ATE) 和条件平均治疗效果 (CATE) 的准确估计。主要贡献：简化了因果推理的算法设计，适用于观察混杂变量和工具变量场景，在多个基准上表现出色，潜在影响医疗和经济学决策。\n\n**2. SINdex: Semantic INconsistency Index for Hallucination Detection in LLMs（SINdex：用于 LLM 幻觉检测的语义不一致性指数）**  \n   论文引入 SINdex 指标和语义聚类框架，用于检测大语言模型 (LLMs) 中的幻觉问题。通过句子嵌入和分层聚类，评估语义不一致性，实现更精确的幻觉识别。主要发现：在 QA 数据集上，AUROC 提升高达 9.3%，证明了不确定性方法在幻觉缓解中的有效性，无需外部数据。\n\n**3. Learning-Order Autoregressive Models with Application to Molecular Graph Generation（学习顺序的自回归模型及其在分子图生成中的应用）**  \n   作者包括 Arthur Gretton 和 Michalis K. Titsias。论文开发了一种动态自回归模型，通过可训练的顺序策略推断数据顺序，实现高效图像和图生成。主要贡献：在 QM9 和 ZINC250k 基准上，模型达到最先进性能，使用 Fréchet ChemNet Distance (FCD) 评估，展示了在分子图生成中的潜力。\n\n**4. Is Your Video Language Model a Reliable Judge?（你的视频语言模型是可靠的裁判吗？）**  \n   论文质疑视频语言模型 (VLMs) 的评估可靠性，提出通过多模型集体判断提升准确性，但发现添加不可靠模型可能引入噪声。主要发现：实验显示，集体判断不一定改善结果，强调需要更先进的可靠性评估方法，VLMs 在理解内容时存在偏见。\n\n**5. TPU-Gen: LLM-Driven Custom Tensor Processing Unit Generator（TPU-Gen：LLM 驱动的自定义张量处理单元生成器）**  \n   论文使用 LLM 和 Retrieval-Augmented Generation (RAG) 自动生成自定义 TPU 架构，支持深度神经网络 (DNNs) 优化。主要贡献：实验显示，生成 TPU 减少面积和功耗 92% 和 96%，通过 RAG 减少幻觉问题，推进 LLM 在硬件设计自动化中的应用。\n\n**6. Superintelligence Strategy: Expert Version（超级智能策略：专家版）**  \n   作者包括 Dan Hendrycks 和 Eric Schmidt 等知名专家。论文提出超级智能时代的安全策略框架，包括 Mutual Assured AI Malfunction (MAIM) 威慑机制，强调 AI 在国家安全中的作用。主要发现：通过威慑、非扩散和竞争力三方面，论文为 AI 治理提供新路径，讨论了 AI 扩散对地缘政治的影响。\n\n**7. Explaining the Unexplainable: A Systematic Review of Explainable AI in Finance（解释不可解释的：AI 在金融中的可解释性系统综述）**  \n   论文综述 AI 在金融中的可解释性 (XAI)，焦点在后验解释方法如 SHAP 和注意力机制。主要贡献：通过文献分析，揭示 XAI 的局限性和多学科需求，促进金融 AI 的透明度。\n\n其他论文如 \"Optimal sensor deception in stochastic environments\"（在部分可观测环境中误导机器人的最优传感器欺骗策略）探讨机器人决策，但相对专业且应用性较窄，仅快速提及其 MILP 优化方法提升欺骗概率；\"A Real-time Multimodal Transformer Neural Network-powered Wildfire Forecasting System\"（实时多模态 Transformer 神经网络驱动的野火预测系统）提出野火预测模型，但细节较常规；\"Bayesian Graph Traversal\"（贝叶斯图遍历）使用高斯过程优化图搜索，适合公共安全应用，但非主流焦点。\n\n总之，今天的论文突显 AI 模型的鲁棒性和应用潜力，但许多次要工作（如生物医学或物理主题）未列出细节，以控制篇幅。感兴趣的读者可查阅 arXiv 原文深入探索。",
  "papers": [
    {
      "arxiv_id": "2503.05985v1",
      "title": "Black Box Causal Inference: Effect Estimation via Meta Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Lucius E. J. Bynum",
        "Aahlad Manas Puli",
        "Diego Herrero-Quevedo",
        "Nhi Nguyen",
        "Carlos Fernandez-Granda",
        "Kyunghyun Cho",
        "Rajesh Ranganath"
      ],
      "abstract": "Causal inference and the estimation of causal effects plays a central role in\ndecision-making across many areas, including healthcare and economics.\nEstimating causal effects typically requires an estimator that is tailored to\neach problem of interest. But developing estimators can take significant effort\nfor even a single causal inference setting. For example, algorithms for\nregression-based estimators, propensity score methods, and doubly robust\nmethods were designed across several decades to handle causal estimation with\nobserved confounders. Similarly, several estimators have been developed to\nexploit instrumental variables (IVs), including two-stage least-squares (TSLS),\ncontrol functions, and the method-of-moments. In this work, we instead frame\ncausal inference as a dataset-level prediction problem, offloading algorithm\ndesign to the learning process. The approach we introduce, called black box\ncausal inference (BBCI), builds estimators in a black-box manner by learning to\npredict causal effects from sampled dataset-effect pairs. We demonstrate\naccurate estimation of average treatment effects (ATEs) and conditional average\ntreatment effects (CATEs) with BBCI across several causal inference problems\nwith known identification, including problems with less developed estimators.",
      "tldr_zh": "本论文提出 Black Box Causal Inference (BBCI) 方法，将因果推断问题转化为数据集级别的预测任务，通过元预测（Meta Prediction）从采样数据集-效果对学习构建估计算法，从而简化传统算法设计。BBCI 能处理多种因果场景，包括利用观察混杂变量、工具变量（IVs）等，支持准确估计平均处理效果 (ATEs) 和条件平均处理效果 (CATEs)。实验结果表明，该方法在多个已知识别的因果推断问题上表现出色，尤其适用于估计算法不发达的领域。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.CO",
        "stat.ME",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05985v1",
      "published_date": "2025-03-07 23:43:19 UTC",
      "updated_date": "2025-03-07 23:43:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:01:42.033346"
    },
    {
      "arxiv_id": "2503.05980v1",
      "title": "SINdex: Semantic INconsistency Index for Hallucination Detection in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Samir Abdaljalil",
        "Hasan Kurban",
        "Parichit Sharma",
        "Erchin Serpedin",
        "Rachad Atat"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed across diverse\ndomains, yet they are prone to generating factually incorrect outputs -\ncommonly known as \"hallucinations.\" Among existing mitigation strategies,\nuncertainty-based methods are particularly attractive due to their ease of\nimplementation, independence from external data, and compatibility with\nstandard LLMs. In this work, we introduce a novel and scalable\nuncertainty-based semantic clustering framework for automated hallucination\ndetection. Our approach leverages sentence embeddings and hierarchical\nclustering alongside a newly proposed inconsistency measure, SINdex, to yield\nmore homogeneous clusters and more accurate detection of hallucination\nphenomena across various LLMs. Evaluations on prominent open- and closed-book\nQA datasets demonstrate that our method achieves AUROC improvements of up to\n9.3% over state-of-the-art techniques. Extensive ablation studies further\nvalidate the effectiveness of each component in our framework.",
      "tldr_zh": "本研究针对大型语言模型(LLMs)产生的幻觉问题，提出了一种基于不确定性的语义聚类框架，用于自动化检测幻觉。该框架结合句子嵌入、层次聚类和新的语义不一致性指标(SINdex)，能够生成更均匀的聚类并提升检测准确性。在多个开放和封闭式问答(QA)数据集上，该方法比现有技术提高了AUROC高达9.3%。此外，消融研究验证了框架各组件的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05980v1",
      "published_date": "2025-03-07 23:25:19 UTC",
      "updated_date": "2025-03-07 23:25:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:01:52.807749"
    },
    {
      "arxiv_id": "2503.05979v1",
      "title": "Learning-Order Autoregressive Models with Application to Molecular Graph Generation",
      "title_zh": "学习顺序自回归模型及其在分子图生成中的应用",
      "authors": [
        "Zhe Wang",
        "Jiaxin Shi",
        "Nicolas Heess",
        "Arthur Gretton",
        "Michalis K. Titsias"
      ],
      "abstract": "Autoregressive models (ARMs) have become the workhorse for sequence\ngeneration tasks, since many problems can be modeled as next-token prediction.\nWhile there appears to be a natural ordering for text (i.e., left-to-right),\nfor many data types, such as graphs, the canonical ordering is less obvious. To\naddress this problem, we introduce a variant of ARM that generates\nhigh-dimensional data using a probabilistic ordering that is sequentially\ninferred from data. This model incorporates a trainable probability\ndistribution, referred to as an \\emph{order-policy}, that dynamically decides\nthe autoregressive order in a state-dependent manner. To train the model, we\nintroduce a variational lower bound on the exact log-likelihood, which we\noptimize with stochastic gradient estimation. We demonstrate experimentally\nthat our method can learn meaningful autoregressive orderings in image and\ngraph generation. On the challenging domain of molecular graph generation, we\nachieve state-of-the-art results on the QM9 and ZINC250k benchmarks, evaluated\nusing the Fr\\'{e}chet ChemNet Distance (FCD).",
      "tldr_zh": "该研究提出了一种学习顺序的自回归模型（Learning-Order Autoregressive Models），通过从数据中推断概率顺序来生成高维数据，如图结构，从而解决传统自回归模型（ARMs）在无明确顺序数据类型上的局限性。该模型引入一个可训练的顺序策略（order-policy），动态决定自回归顺序，并使用变分下界和随机梯度估计进行优化。实验结果显示，该方法在图像和图生成任务中学习了有意义的顺序，并在分子图生成领域上，在 QM9 和 ZINC250k 基准测试中，使用 Fréchet ChemNet Distance (FCD) 评估，达到了最先进性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05979v1",
      "published_date": "2025-03-07 23:24:24 UTC",
      "updated_date": "2025-03-07 23:24:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:02:04.134869"
    },
    {
      "arxiv_id": "2503.05977v1",
      "title": "Is Your Video Language Model a Reliable Judge?",
      "title_zh": "你的视频语言模型是可靠的评判者吗？",
      "authors": [
        "Ming Liu",
        "Wensheng Zhang"
      ],
      "abstract": "As video language models (VLMs) gain more applications in various scenarios,\nthe need for robust and scalable evaluation of their performance becomes\nincreasingly critical. The traditional human expert-based evaluation of VLMs\nhas limitations in consistency and scalability, which sparked interest in\nautomatic methods such as employing VLMs to evaluate VLMs. However, the\nreliability of VLMs as judges remains underexplored. Existing methods often\nrely on a single VLM as the evaluator. However, this approach can be unreliable\nor biased because such a model may lack the ability to fully understand the\ncontent and may have inherent biases, ultimately compromising evaluation\nreliability. A remedy is to apply the principle of collective thoughts,\naggregating evaluations from multiple VLMs to enhance reliability. This study\ninvestigates the efficacy of such approaches, particularly when the pool of\njudges includes both reliable and unreliable models. Our findings reveal that\nincorporating collective judgments from such a mixed pool does not necessarily\nimprove the accuracy of the final evaluation. The inclusion of less reliable\njudges can introduce noise, undermining the overall reliability of the\noutcomes. To explore the factors that impact evaluation reliability, we\nfine-tune an underperforming VLM judge, Video-LLaVA, and observe that improved\nunderstanding ability alone is insufficient to make VLM judges more reliable.\nThese findings stress the limitations of collective thought approaches and\nhighlight the need for more advanced methods that can account for the\nreliability of individual models. Our study promotes the development of more\nreliable evaluation methods for VLMs",
      "tldr_zh": "本研究探讨了视频语言模型（VLMs）作为评判者的可靠性问题，强调传统人工评估的局限性促使采用自动方法，但单一 VLM 评判可能存在偏见和理解不足。研究者提出使用集体判断（collective thoughts）从多个 VLMs 聚合评估，以提升可靠性，但实验发现，当评判池包含可靠和不可靠模型时，这种方法并不会改善准确性，反而可能引入噪声。进一步，作者微调了表现不佳的 VLM 评判者 Video-LLaVA，发现仅提升理解能力不足以使其更可靠。这些发现突出了集体判断方法的局限性，并呼吁开发更先进的评估框架，以推动 VLMs 性能评估的可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05977v1",
      "published_date": "2025-03-07 23:17:59 UTC",
      "updated_date": "2025-03-07 23:17:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:02:16.624803"
    },
    {
      "arxiv_id": "2503.05972v1",
      "title": "Optimal sensor deception in stochastic environments with partial observability to mislead a robot to a decoy goal",
      "title_zh": "在部分可观察的随机环境中进行最优传感器欺骗",
      "authors": [
        "Hazhar Rahmani",
        "Mukulika Ghosh",
        "Syed Md Hasnayeen"
      ],
      "abstract": "Deception is a common strategy adapted by autonomous systems in adversarial\nsettings. Existing deception methods primarily focus on increasing opacity or\nmisdirecting agents away from their goal or itinerary. In this work, we propose\na deception problem aiming to mislead the robot towards a decoy goal through\naltering sensor events under a constrained budget of alteration. The\nenvironment along with the robot's interaction with it is modeled as a\nPartially Observable Markov Decision Process (POMDP), and the robot's action\nselection is governed by a Finite State Controller (FSC). Given a constrained\nbudget for sensor event modifications, the objective is to compute a sensor\nalteration that maximizes the probability of the robot reaching a decoy goal.\nWe establish the computational hardness of the problem by a reduction from the\n$0/1$ Knapsack problem and propose a Mixed Integer Linear Programming (MILP)\nformulation to compute optimal deception strategies. We show the efficacy of\nour MILP formulation via a sequence of experiments.",
      "tldr_zh": "这篇论文提出了一种优化传感器欺骗策略，旨在在部分可观测随机环境中，通过有限预算的传感器事件修改，将机器人误导至一个假目标 (decoy goal)。他们将环境和机器人交互建模为部分可观测马尔可夫决策过程 (POMDP)，并使用有限状态控制器 (FSC) 管理机器人的行动决策。论文证明了该问题的计算难度（通过从 0/1 背包问题归约），并通过混合整数线性规划 (MILP) 公式计算最优欺骗策略。实验结果验证了该方法的有效性，在欺骗任务中表现出色。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05972v1",
      "published_date": "2025-03-07 22:57:27 UTC",
      "updated_date": "2025-03-07 22:57:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:02:29.258759"
    },
    {
      "arxiv_id": "2503.05971v2",
      "title": "A Real-time Multimodal Transformer Neural Network-powered Wildfire Forecasting System",
      "title_zh": "实时多模态 Transformer 神经网络驱动的野火预测系统",
      "authors": [
        "Qijun Chen",
        "Shaofan Li"
      ],
      "abstract": "Due to climate change, the extreme wildfire has become one of the most\ndangerous natural hazards to human civilization. Even though, some wildfires\nmay be initially caused by human activity, but the spread of wildfires is\nmainly determined by environmental factors, for examples, (1) weather\nconditions such as temperature, wind direction and intensity, and moisture\nlevels; (2) the amount and types of dry vegetation in a local area, and (3)\ntopographic or local terrian conditions, which affects how much rain an area\ngets and how fire dynamics will be constrained or faciliated. Thus, to\naccurately forecast wildfire occurrence has become one of most urgent and\ntaunting environmental challenges in global scale. In this work, we developed a\nreal-time Multimodal Transformer Neural Network Machine Learning model that\ncombines several advanced artificial intelligence techniques and statistical\nmethods to practically forecast the occurrence of wildfire at the precise\nlocation in real time, which not only utilizes large scale data information\nsuch as hourly weather forecasting data, but also takes into account small\nscale topographical data such as local terrain condition and local vegetation\nconditions collecting from Google Earth images to determine the probabilities\nof wildfire occurrence location at small scale as well as their timing\nsynchronized with weather forecast information. By using the wildfire data in\nthe United States from 1992 to 2015 to train the multimodal transformer neural\nnetwork, it can predict the probabilities of wildfire occurrence according to\nthe real-time weather forecast and the synchronized Google Earth image data to\nprovide the wildfire occurrence probability in any small location ($100m^2$)\nwithin 24 hours ahead.",
      "tldr_zh": "本研究针对气候变化导致的野火风险，开发了一个实时多模态 Transformer Neural Network 模型，用于精确预测野火发生的地点和时间。该模型整合了大规模数据（如小时天气预报）和小规模数据（如从 Google Earth images 获取的地形和植被条件），结合先进的人工智能技术和统计方法进行预测。使用 1992-2015 年美国野火数据训练后，该系统能在 24 小时内为任意 100m² 小区域提供野火发生概率，显著提升了全球野火预警的准确性和实用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05971v2",
      "published_date": "2025-03-07 22:48:46 UTC",
      "updated_date": "2025-03-12 03:22:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:02:41.542151"
    },
    {
      "arxiv_id": "2503.05966v2",
      "title": "Explaining the Unexplainable: A Systematic Review of Explainable AI in Finance",
      "title_zh": "解释不可解释的：可解释人工智能在金融领域的系统综述",
      "authors": [
        "Md Talha Mohsin",
        "Nabid Bin Nasim"
      ],
      "abstract": "Practitioners and researchers trying to strike a balance between accuracy and\ntransparency center Explainable Artificial Intelligence (XAI) at the junction\nof finance. This paper offers a thorough overview of the changing scene of XAI\napplications in finance together with domain-specific implementations,\nmethodological developments, and trend mapping of research. Using bibliometric\nand content analysis, we find topic clusters, significant research, and most\noften used explainability strategies used in financial industries. Our results\nshow a substantial dependence on post-hoc interpretability techniques;\nattention mechanisms, feature importance analysis and SHAP are the most often\nused techniques among them. This review stresses the need of multidisciplinary\napproaches combining financial knowledge with improved explainability paradigms\nand exposes important shortcomings in present XAI systems.",
      "tldr_zh": "这篇论文系统回顾了Explainable AI (XAI) 在金融领域的应用，探讨了其在准确性和透明性平衡中的作用，包括领域特定实现、方法发展和研究趋势。作者通过文献计量和内容分析，识别了关键主题集群、重要研究以及最常用的解释策略，如注意力机制、特征重要性分析和SHAP。结果表明，XAI 主要依赖后验解释技术，但存在显著缺陷，并强调需要多学科方法结合金融知识来改进解释范式。",
      "categories": [
        "q-fin.GN",
        "cs.AI"
      ],
      "primary_category": "q-fin.GN",
      "comment": "2 tables, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05966v2",
      "published_date": "2025-03-07 22:36:44 UTC",
      "updated_date": "2025-03-17 15:37:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:02:53.574317"
    },
    {
      "arxiv_id": "2503.05963v1",
      "title": "Bayesian Graph Traversal",
      "title_zh": "翻译失败",
      "authors": [
        "William N. Caballero",
        "Phillip R. Jenkins",
        "David Banks",
        "Matthew Robbins"
      ],
      "abstract": "This research considers Bayesian decision-analytic approaches toward the\ntraversal of an uncertain graph. Namely, a traveler progresses over a graph in\nwhich rewards are gained upon a node's first visit and costs are incurred for\nevery edge traversal. The traveler knows the graph's adjacency matrix and his\nstarting position but does not know the rewards and costs. The traveler is a\nBayesian who encodes his beliefs about these values using a Gaussian process\nprior and who seeks to maximize his expected utility over these beliefs.\nAdopting a decision-analytic perspective, we develop sequential decision-making\nsolution strategies for this coupled information-collection and network-routing\nproblem. We show that the problem is NP-Hard and derive properties of the\noptimal walk. These properties provide heuristics for the traveler's problem\nthat balance exploration and exploitation. We provide a practical case study\nfocused on the use of unmanned aerial systems for public safety and empirically\nstudy policy performance in myriad Erdos-Renyi settings.",
      "tldr_zh": "这篇论文探讨了Bayesian决策方法在不确定图（uncertain graph）遍历中的应用，旅行者基于已知的图邻接矩阵和起始位置，使用Gaussian process prior表示对节点奖励和边成本的信念，以最大化期望效用。作者证明了该问题是NP-Hard，并推导了最优路径的属性，这些属性用于设计启发式策略，平衡探索（exploration）和利用（exploitation）。通过一个无人机用于公共安全的案例研究，并在多种Erdos-Renyi图设置中进行实证实验，论文验证了这些策略的有效性。",
      "categories": [
        "cs.AI",
        "cs.GT",
        "stat.OT",
        "62C99, 68T20"
      ],
      "primary_category": "cs.AI",
      "comment": "26 pages, 7 tables, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05963v1",
      "published_date": "2025-03-07 22:05:06 UTC",
      "updated_date": "2025-03-07 22:05:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:03:05.911335"
    },
    {
      "arxiv_id": "2503.05958v1",
      "title": "SANDWiCH: Semantical Analysis of Neighbours for Disambiguating Words in Context ad Hoc",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Guzman-Olivares",
        "Lara Quijano-Sanchez",
        "Federico Liberatore"
      ],
      "abstract": "The rise of generative chat-based Large Language Models (LLMs) over the past\ntwo years has spurred a race to develop systems that promise near-human\nconversational and reasoning experiences. However, recent studies indicate that\nthe language understanding offered by these models remains limited and far from\nhuman-like performance, particularly in grasping the contextual meanings of\nwords, an essential aspect of reasoning. In this paper, we present a simple yet\ncomputationally efficient framework for multilingual Word Sense Disambiguation\n(WSD). Our approach reframes the WSD task as a cluster discrimination analysis\nover a semantic network refined from BabelNet using group algebra. We validate\nour methodology across multiple WSD benchmarks, achieving a new state of the\nart for all languages and tasks, as well as in individual assessments by part\nof speech. Notably, our model significantly surpasses the performance of\ncurrent alternatives, even in low-resource languages, while reducing the\nparameter count by 72%.",
      "tldr_zh": "本研究针对生成式聊天大语言模型（LLMs）在词语语境理解上的局限性，提出了一种简单高效的多语言词义消歧（WSD）框架SANDWiCH。该框架将WSD任务重构为对从BabelNet提炼的语义网络进行聚类鉴别分析，使用群代数来优化语义邻居分析。在多个WSD基准测试中，该方法在所有语言和任务上达到了新的state of the art表现，尤其在低资源语言中显著优于现有模型，同时将参数数量减少了72%。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "15 pages, 2 figures, 7 tables, NAACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.05958v1",
      "published_date": "2025-03-07 21:52:32 UTC",
      "updated_date": "2025-03-07 21:52:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:03:16.065141"
    },
    {
      "arxiv_id": "2503.05951v1",
      "title": "TPU-Gen: LLM-Driven Custom Tensor Processing Unit Generator",
      "title_zh": "TPU-Gen：LLM 驱动的自定义张量处理单元生成器",
      "authors": [
        "Deepak Vungarala",
        "Mohammed E. Elbtity",
        "Sumiya Syed",
        "Sakila Alam",
        "Kartik Pandit",
        "Arnob Ghosh",
        "Ramtin Zand",
        "Shaahin Angizi"
      ],
      "abstract": "The increasing complexity and scale of Deep Neural Networks (DNNs)\nnecessitate specialized tensor accelerators, such as Tensor Processing Units\n(TPUs), to meet various computational and energy efficiency requirements.\nNevertheless, designing optimal TPU remains challenging due to the high domain\nexpertise level, considerable manual design time, and lack of high-quality,\ndomain-specific datasets. This paper introduces TPU-Gen, the first Large\nLanguage Model (LLM) based framework designed to automate the exact and\napproximate TPU generation process, focusing on systolic array architectures.\nTPU-Gen is supported with a meticulously curated, comprehensive, and\nopen-source dataset that covers a wide range of spatial array designs and\napproximate multiply-and-accumulate units, enabling design reuse, adaptation,\nand customization for different DNN workloads. The proposed framework leverages\nRetrieval-Augmented Generation (RAG) as an effective solution for a data-scare\nhardware domain in building LLMs, addressing the most intriguing issue,\nhallucinations. TPU-Gen transforms high-level architectural specifications into\noptimized low-level implementations through an effective hardware generation\npipeline. Our extensive experimental evaluations demonstrate superior\nperformance, power, and area efficiency, with an average reduction in area and\npower of 92\\% and 96\\% from the manual optimization reference values. These\nresults set new standards for driving advancements in next-generation design\nautomation tools powered by LLMs.",
      "tldr_zh": "这篇论文介绍了 TPU-Gen，一个基于 Large Language Model (LLM) 的框架，用于自动化自定义 Tensor Processing Unit (TPU) 的生成，旨在解决 DNNs 设计中的高专业门槛、手动时间消耗和数据集缺乏问题。TPU-Gen 利用 Retrieval-Augmented Generation (RAG) 和一个精心策划的开源数据集，专注于 systolic array architectures 的精确和近似设计，从而将高层架构规范转化为优化的低层实现。实验评估显示，与手动优化相比，TPU-Gen 平均减少面积 92% 和功耗 96%，显著提升了性能、功耗和面积效率，为基于 LLM 的下一代设计自动化工具奠定了新标准。",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "primary_category": "cs.AR",
      "comment": "8 Pages, 9 Figures, 5 Tables",
      "pdf_url": "http://arxiv.org/pdf/2503.05951v1",
      "published_date": "2025-03-07 21:41:42 UTC",
      "updated_date": "2025-03-07 21:41:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:03:29.145802"
    },
    {
      "arxiv_id": "2503.10655v1",
      "title": "Language modelling techniques for analysing the impact of human genetic variation",
      "title_zh": "翻译失败",
      "authors": [
        "Megha Hegde",
        "Jean-Christophe Nebel",
        "Farzana Rahman"
      ],
      "abstract": "Interpreting the effects of variants within the human genome and proteome is\nessential for analysing disease risk, predicting medication response, and\ndeveloping personalised health interventions. Due to the intrinsic similarities\nbetween the structure of natural languages and genetic sequences, natural\nlanguage processing techniques have demonstrated great applicability in\ncomputational variant effect prediction. In particular, the advent of the\nTransformer has led to significant advancements in the field. However,\nTransformer-based models are not without their limitations, and a number of\nextensions and alternatives have been developed to improve results and enhance\ncomputational efficiency. This review explores the use of language models for\ncomputational variant effect prediction over the past decade, analysing the\nmain architectures, and identifying key trends and future directions.",
      "tldr_zh": "这篇论文审视了语言建模技术在分析人类遗传变异影响方面的应用，特别是用于评估疾病风险、预测药物反应和开发个性化健康干预。论文强调了自然语言处理（NLP）技术，尤其是Transformer模型的重大进展，因为遗传序列结构类似于自然语言，但也指出了其局限性，并讨论了各种扩展和替代方案以提升准确性和计算效率。该综述回顾了过去十年的主要架构，识别了关键趋势，并为未来方向提供了见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "q-bio.BM"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10655v1",
      "published_date": "2025-03-07 21:34:17 UTC",
      "updated_date": "2025-03-07 21:34:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:03:40.775998"
    },
    {
      "arxiv_id": "2503.05944v1",
      "title": "Enhancing Reasoning with Collaboration and Memory",
      "title_zh": "翻译失败",
      "authors": [
        "Julie Michelman",
        "Nasrin Baratalipour",
        "Matthew Abueg"
      ],
      "abstract": "We envision a continuous collaborative learning system where groups of LLM\nagents work together to solve reasoning problems, drawing on memory they\ncollectively build to improve performance as they gain experience. This work\nestablishes the foundations for such a system by studying the interoperability\nof chain-of-thought reasoning styles, multi-agent collaboration, and memory\nbanks. Extending beyond the identical agents of self-consistency, we introduce\nvaried-context agents with diverse exemplars and a summarizer agent in place of\nvoting. We generate frozen and continuously learned memory banks of exemplars\nand pair them with fixed, random, and similarity-based retrieval mechanisms.\nOur systematic study reveals where various methods contribute to reasoning\nperformance of two LLMs on three grounded reasoning tasks, showing that random\nexemplar selection can often beat more principled approaches, and in some\ntasks, inclusion of any exemplars serves only to distract both weak and strong\nmodels.",
      "tldr_zh": "本研究提出了一种持续协作学习系统，让一组LLM代理通过多代理协作和集体记忆库来提升推理性能，超越传统的chain-of-thought推理方法。论文引入varied-context agents（使用多样示例）和summarizer agent（取代投票机制），并探索frozen和continuously learned memory banks，以及fixed、random和similarity-based检索机制。实验结果显示，在三个基础推理任务上，随机示例选择往往优于更复杂的策略，而在某些任务中，任何示例的加入反而会分散弱模型和强模型的注意力。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "17 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05944v1",
      "published_date": "2025-03-07 21:19:21 UTC",
      "updated_date": "2025-03-07 21:19:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:03:53.234838"
    },
    {
      "arxiv_id": "2503.05938v1",
      "title": "Uncertainty Quantification From Scaling Laws in Deep Neural Networks",
      "title_zh": "基于缩放定律的深度神经网络不确定性量化",
      "authors": [
        "Ibrahim Elsharkawy",
        "Yonatan Kahn",
        "Benjamin Hooberman"
      ],
      "abstract": "Quantifying the uncertainty from machine learning analyses is critical to\ntheir use in the physical sciences. In this work we focus on uncertainty\ninherited from the initialization distribution of neural networks. We compute\nthe mean $\\mu_{\\mathcal{L}}$ and variance $\\sigma_{\\mathcal{L}}^2$ of the test\nloss $\\mathcal{L}$ for an ensemble of multi-layer perceptrons (MLPs) with\nneural tangent kernel (NTK) initialization in the infinite-width limit, and\ncompare empirically to the results from finite-width networks for three example\ntasks: MNIST classification, CIFAR classification and calorimeter energy\nregression. We observe scaling laws as a function of training set size\n$N_\\mathcal{D}$ for both $\\mu_{\\mathcal{L}}$ and $\\sigma_{\\mathcal{L}}$, but\nfind that the coefficient of variation $\\epsilon_{\\mathcal{L}} \\equiv\n\\sigma_{\\mathcal{L}}/\\mu_{\\mathcal{L}}$ becomes independent of $N_\\mathcal{D}$\nat both infinite and finite width for sufficiently large $N_\\mathcal{D}$. This\nimplies that the coefficient of variation of a finite-width network may be\napproximated by its infinite-width value, and may in principle be calculable\nusing finite-width perturbation theory.",
      "tldr_zh": "该论文探讨了从深度神经网络的缩放定律中量化不确定性，特别是神经网络初始化分布带来的不确定性。研究计算了无限宽度极限下多层感知器 (MLPs) 的测试损失均值 $\\mu_{\\mathcal{L}}$ 和方差 $\\sigma_{\\mathcal{L}}^2$，并通过神经切线核 (NTK) 初始化与有限宽度网络的实证结果进行比较，涵盖 MNIST 分类、CIFAR 分类和热量计能量回归等任务。结果显示，均值和方差随训练集大小 $N_\\mathcal{D}$ 遵循缩放定律，而变异系数 $\\epsilon_{\\mathcal{L}} \\equiv \\sigma_{\\mathcal{L}}/\\mu_{\\mathcal{L}}$ 在足够大的 $N_\\mathcal{D}$ 时独立于训练集大小，这意味着有限宽度网络的变异系数可近似为无限宽度值，并可能通过有限宽度微扰理论计算。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "hep-ex",
        "hep-ph",
        "hep-th"
      ],
      "primary_category": "cs.LG",
      "comment": "18+3 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05938v1",
      "published_date": "2025-03-07 21:15:11 UTC",
      "updated_date": "2025-03-07 21:15:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:04:07.484225"
    },
    {
      "arxiv_id": "2503.05937v1",
      "title": "The Unified Control Framework: Establishing a Common Foundation for Enterprise AI Governance, Risk Management and Regulatory Compliance",
      "title_zh": "翻译失败",
      "authors": [
        "Ian W. Eisenberg",
        "Lucía Gamboa",
        "Eli Sherman"
      ],
      "abstract": "The rapid adoption of AI systems presents enterprises with a dual challenge:\naccelerating innovation while ensuring responsible governance. Current AI\ngovernance approaches suffer from fragmentation, with risk management\nframeworks that focus on isolated domains, regulations that vary across\njurisdictions despite conceptual alignment, and high-level standards lacking\nconcrete implementation guidance. This fragmentation increases governance costs\nand creates a false dichotomy between innovation and responsibility. We propose\nthe Unified Control Framework (UCF): a comprehensive governance approach that\nintegrates risk management and regulatory compliance through a unified set of\ncontrols. The UCF consists of three key components: (1) a comprehensive risk\ntaxonomy synthesizing organizational and societal risks, (2) structured policy\nrequirements derived from regulations, and (3) a parsimonious set of 42\ncontrols that simultaneously address multiple risk scenarios and compliance\nrequirements. We validate the UCF by mapping it to the Colorado AI Act,\ndemonstrating how our approach enables efficient, adaptable governance that\nscales across regulations while providing concrete implementation guidance. The\nUCF reduces duplication of effort, ensures comprehensive coverage, and provides\na foundation for automation, enabling organizations to achieve responsible AI\ngovernance without sacrificing innovation speed.",
      "tldr_zh": "该论文提出Unified Control Framework (UCF)，一个综合框架，用于统一企业AI治理、风险管理和合规性，解决现有方法的碎片化问题，如孤立的风险框架和地区性法规差异，从而降低治理成本并平衡创新与责任。UCF的核心组件包括：一个综合风险分类法（synthesizing organizational and societal risks）、结构化政策要求（derived from regulations），以及一套简洁的42个控制措施（parsimonious set of 42 controls），这些措施能同时应对多种风险场景和合规需求。通过映射到Colorado AI Act进行验证，UCF展示了高效、可扩展的治理能力，提供具体实施指导，并为自动化奠定基础，帮助组织实现全面覆盖而不牺牲创新速度。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05937v1",
      "published_date": "2025-03-07 21:14:49 UTC",
      "updated_date": "2025-03-07 21:14:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:04:19.300147"
    },
    {
      "arxiv_id": "2503.05929v1",
      "title": "Audio-to-Image Encoding for Improved Voice Characteristic Detection Using Deep Convolutional Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Youness Atif"
      ],
      "abstract": "This paper introduces a novel audio-to-image encoding framework that\nintegrates multiple dimensions of voice characteristics into a single RGB image\nfor speaker recognition. In this method, the green channel encodes raw audio\ndata, the red channel embeds statistical descriptors of the voice signal\n(including key metrics such as median and mean values for fundamental\nfrequency, spectral centroid, bandwidth, rolloff, zero-crossing rate, MFCCs,\nRMS energy, spectral flatness, spectral contrast, chroma, and harmonic-to-noise\nratio), and the blue channel comprises subframes representing these features in\na spatially organized format. A deep convolutional neural network trained on\nthese composite images achieves 98% accuracy in speaker classification across\ntwo speakers, suggesting that this integrated multi-channel representation can\nprovide a more discriminative input for voice recognition tasks.",
      "tldr_zh": "本论文提出了一种新型音频到图像编码框架，用于提升语音特征检测的性能，将多种语音特征整合到一个 RGB 图像中：绿色通道编码原始音频数据，红色通道嵌入统计描述符（如基频的中值和均值、光谱质心、带宽、MFCCs 等），蓝色通道以空间组织格式表示这些特征的子帧。使用 Deep Convolutional Neural Networks 在这些复合图像上训练后，在两个说话者的分类任务中实现了98%的准确率。该框架证明了多通道表示能提供更具区分性的输入，从而改善语音识别任务的整体效果。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "11 pages, 24 figures, 1 table, 3 algorithms. Submitted to\n  F1000Research",
      "pdf_url": "http://arxiv.org/pdf/2503.05929v1",
      "published_date": "2025-03-07 20:49:56 UTC",
      "updated_date": "2025-03-07 20:49:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:04:29.187188"
    },
    {
      "arxiv_id": "2503.05925v1",
      "title": "ElementaryNet: A Non-Strategic Neural Network for Predicting Human Behavior in Normal-Form Games",
      "title_zh": "ElementaryNet：用于预测正规形式博弈中人类行为的非策略性神经网络",
      "authors": [
        "Greg d'Eon",
        "Hala Murad",
        "Kevin Leyton-Brown",
        "James R. Wright"
      ],
      "abstract": "Models of human behavior in game-theoretic settings often distinguish between\nstrategic behavior, in which a player both reasons about how others will act\nand best responds to these beliefs, and \"level-0\" non-strategic behavior, in\nwhich they do not respond to explicit beliefs about others. The state of the\nart for predicting human behavior on unrepeated simultaneous-move games is\nGameNet, a neural network that learns extremely complex level-0 specifications\nfrom data. The current paper makes three contributions. First, it shows that\nGameNet's level-0 specifications are too powerful, because they are capable of\nstrategic reasoning. Second, it introduces a novel neural network architecture\n(dubbed ElementaryNet) and proves that it is only capable of nonstrategic\nbehavior. Third, it describes an extensive experimental evaluation of\nElementaryNet. Our overall findings are that (1) ElementaryNet dramatically\nunderperforms GameNet when neither model is allowed to explicitly model higher\nlevel agents who best-respond to the model's predictions, indicating that good\nperformance on our dataset requires a model capable of strategic reasoning; (2)\nthat the two models achieve statistically indistinguishable performance when\nsuch higher-level agents are introduced, meaning that ElementaryNet's\nrestriction to a non-strategic level-0 specification does not degrade model\nperformance; and (3) that this continues to hold even when ElementaryNet is\nrestricted to a set of level-0 building blocks previously introduced in the\nliterature, with only the functional form being learned by the neural network.",
      "tldr_zh": "该研究批评了现有模型 GameNet，其 level-0 规范过于强大，可能涉及战略推理，并提出 ElementaryNet，一种新型神经网络架构，证明其仅限于非战略行为，用于预测人类在 normal-form games 中的行为。实验评估显示，ElementaryNet 在不建模更高层代理时显著逊于 GameNet，表明良好性能依赖战略推理能力；但当引入这些代理后，两者性能统计上无显著差异，即使 ElementaryNet 限于文献中的 level-0 构建块，仅学习功能形式。总体而言，这为开发更精确的非战略行为预测模型提供了新见解。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.GT"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages. Submitted to EC 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.05925v1",
      "published_date": "2025-03-07 20:47:16 UTC",
      "updated_date": "2025-03-07 20:47:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:04:42.563007"
    },
    {
      "arxiv_id": "2503.05920v1",
      "title": "IDEA Prune: An Integrated Enlarge-and-Prune Pipeline in Generative Language Model Pretraining",
      "title_zh": "翻译失败",
      "authors": [
        "Yixiao Li",
        "Xianzhi Du",
        "Ajay Jaiswal",
        "Tao Lei",
        "Tuo Zhao",
        "Chong Wang",
        "Jianyu Wang"
      ],
      "abstract": "Recent advancements in large language models have intensified the need for\nefficient and deployable models within limited inference budgets. Structured\npruning pipelines have shown promise in token efficiency compared to training\ntarget-size models from scratch. In this paper, we advocate incorporating\nenlarged model pretraining, which is often ignored in previous works, into\npruning. We study the enlarge-and-prune pipeline as an integrated system to\naddress two critical questions: whether it is worth pretraining an enlarged\nmodel even when the model is never deployed, and how to optimize the entire\npipeline for better pruned models. We propose an integrated enlarge-and-prune\npipeline, which combines enlarge model training, pruning, and recovery under a\nsingle cosine annealing learning rate schedule. This approach is further\ncomplemented by a novel iterative structured pruning method for gradual\nparameter removal. The proposed method helps to mitigate the knowledge loss\ncaused by the rising learning rate in naive enlarge-and-prune pipelines and\nenable effective redistribution of model capacity among surviving neurons,\nfacilitating smooth compression and enhanced performance. We conduct\ncomprehensive experiments on compressing 2.8B models to 1.3B with up to 2T\ntokens in pretraining. It demonstrates the integrated approach not only\nprovides insights into the token efficiency of enlarged model pretraining but\nalso achieves superior performance of pruned models.",
      "tldr_zh": "该论文提出了一种整合的 enlarge-and-prune 管道，名为 IDEA Prune，用于生成式语言模型的预训练，以提高模型的标记效率和部署性能。该方法将 enlarged model 预训练、剪枝和恢复整合到一个单一的 cosine annealing 学习率调度中，并引入一种新型的 iterative structured pruning 技术，逐步移除参数以减少知识损失并优化剩余神经元的容量分配。实验结果显示，该管道在将 2.8B 模型压缩到 1.3B 时，使用多达 2T 标记预训练，不仅提升了剪枝模型的性能，还证明了 enlarged model 预训练在标记效率上的价值。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05920v1",
      "published_date": "2025-03-07 20:35:31 UTC",
      "updated_date": "2025-03-07 20:35:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:04:53.579727"
    },
    {
      "arxiv_id": "2503.05916v1",
      "title": "SAS: Segment Anything Small for Ultrasound -- A Non-Generative Data Augmentation Technique for Robust Deep Learning in Ultrasound Imaging",
      "title_zh": "翻译失败",
      "authors": [
        "Danielle L. Ferreira",
        "Ahana Gangopadhyay",
        "Hsi-Ming Chang",
        "Ravi Soni",
        "Gopal Avinash"
      ],
      "abstract": "Accurate segmentation of anatomical structures in ultrasound (US) images,\nparticularly small ones, is challenging due to noise and variability in imaging\nconditions (e.g., probe position, patient anatomy, tissue characteristics and\npathology). To address this, we introduce Segment Anything Small (SAS), a\nsimple yet effective scale- and texture-aware data augmentation technique\ndesigned to enhance the performance of deep learning models for segmenting\nsmall anatomical structures in ultrasound images. SAS employs a dual\ntransformation strategy: (1) simulating diverse organ scales by resizing and\nembedding organ thumbnails into a black background, and (2) injecting noise\ninto regions of interest to simulate varying tissue textures. These\ntransformations generate realistic and diverse training data without\nintroducing hallucinations or artifacts, improving the model's robustness to\nnoise and variability. We fine-tuned a promptable foundation model on a\ncontrolled organ-specific medical imaging dataset and evaluated its performance\non one internal and five external datasets. Experimental results demonstrate\nsignificant improvements in segmentation performance, with Dice score gains of\nup to 0.35 and an average improvement of 0.16 [95% CI 0.132,0.188].\nAdditionally, our iterative point prompts provide precise control and adaptive\nrefinement, achieving performance comparable to bounding box prompts with just\ntwo points. SAS enhances model robustness and generalizability across diverse\nanatomical structures and imaging conditions, particularly for small\nstructures, without compromising the accuracy of larger ones. By offering a\ncomputationally efficient solution that eliminates the need for extensive human\nlabeling efforts, SAS emerges as a powerful tool for advancing medical image\nanalysis, particularly in resource-constrained settings.",
      "tldr_zh": "该研究提出了一种名为Segment Anything Small (SAS) 的非生成式数据增强技术，旨在提升深度学习模型在超声图像中分割小解剖结构（如器官）的准确性，解决噪声和成像条件变异带来的挑战。SAS 采用双重策略：通过调整器官缩略图大小并嵌入黑背景模拟不同规模，以及在感兴趣区域注入噪声模拟组织纹理，从而生成真实多样化的训练数据，而不引入幻觉或伪像。实验结果显示，在一个内部和五个外部数据集上微调后的模型，Dice分数平均提升0.16（95% CI 0.132,0.188），最高达0.35，且使用迭代点提示可实现与边界框提示相当的性能。总体而言，SAS 提高了模型的鲁棒性和泛化能力，尤其适用于小结构分割，并提供了一种计算高效的解决方案，减少了对大量人工标注的依赖。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "25 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05916v1",
      "published_date": "2025-03-07 20:24:35 UTC",
      "updated_date": "2025-03-07 20:24:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:05:08.872007"
    },
    {
      "arxiv_id": "2503.10654v1",
      "title": "Improving RAG Retrieval via Propositional Content Extraction: a Speech Act Theory Approach",
      "title_zh": "翻译失败",
      "authors": [
        "João Alberto de Oliveira Lima"
      ],
      "abstract": "When users formulate queries, they often include not only the information\nthey seek, but also pragmatic markers such as interrogative phrasing or polite\nrequests. Although these speech act indicators communicate the\nuser\\textquotesingle s intent -- whether it is asking a question, making a\nrequest, or stating a fact -- they do not necessarily add to the core\ninformational content of the query itself. This paper investigates whether\nextracting the underlying propositional content from user utterances --\nessentially stripping away the linguistic markers of intent -- can improve\nretrieval quality in Retrieval-Augmented Generation (RAG) systems. Drawing upon\nfoundational insights from speech act theory, we propose a practical method for\nautomatically transforming queries into their propositional equivalents before\nembedding. To assess the efficacy of this approach, we conducted an\nexperimental study involving 63 user queries related to a Brazilian\ntelecommunications news corpus with precomputed semantic embeddings. Results\ndemonstrate clear improvements in semantic similarity between query embeddings\nand document embeddings at top ranks, confirming that queries stripped of\nspeech act indicators more effectively retrieve relevant content.",
      "tldr_zh": "该论文探讨了如何通过提取用户查询中的命题内容来改善 Retrieval-Augmented Generation (RAG) 系统的检索质量，方法基于 Speech Act Theory，将查询中的语用标记（如疑问句或礼貌请求）去除以聚焦核心信息。研究者提出了一种自动转换查询为命题等价物的实用方法，并在包含63个与巴西电信新闻语料库相关的查询上进行实验。结果表明，这种方法显著提高了顶层排名的查询嵌入和文档嵌入之间的语义相似度，从而更有效地检索相关内容，为RAG系统的优化提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "19 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.10654v1",
      "published_date": "2025-03-07 20:15:40 UTC",
      "updated_date": "2025-03-07 20:15:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:05:17.674949"
    },
    {
      "arxiv_id": "2503.10653v1",
      "title": "Video Anomaly Detection with Structured Keywords",
      "title_zh": "基于结构化关键词的视频异常检测",
      "authors": [
        "Thomas Foltz"
      ],
      "abstract": "This paper focuses on detecting anomalies in surveillance video using\nkeywords by leveraging foundational models' feature representation\ngeneralization capabilities. We present a novel, lightweight pipeline for\nanomaly classification using keyword weights. Our pipeline employs a two-stage\nprocess: induction followed by deduction. In induction, descriptions are\ngenerated from normal and anomalous frames to identify and assign weights to\nrelevant keywords. In deduction, inference frame descriptions are converted\ninto keyword encodings using induction-derived weights for input into our\nneural network for anomaly classification. We achieved comparable performance\non the three benchmarks UCSD Ped2, Shanghai Tech, and CUHK Avenue, with ROC AUC\nscores of 0.865, 0.745, and 0.742, respectively. These results are achieved\nwithout temporal context, making such a system viable for real-time\napplications. Our model improves implementation setup, interpretability, and\ninference speed for surveillance devices on the edge, introducing a performance\ntrade-off against other video anomaly detection systems. As the generalization\ncapabilities of open-source foundational models improve, our model demonstrates\nthat the exclusive use of text for feature representations is a promising\ndirection for efficient real-time interpretable video anomaly detection.",
      "tldr_zh": "本论文提出了一种基于结构化关键词的视频异常检测方法，利用基础模型的特征表示泛化能力，设计了一个轻量级管道进行异常分类。该管道采用两阶段过程：归纳阶段从正常和异常帧生成描述并分配关键词权重，演绎阶段将推理帧描述转换为关键词编码后输入神经网络进行分类。在UCSD Ped2、Shanghai Tech和CUHK Avenue三个基准上，该方法分别获得ROC AUC分数0.865、0.745和0.742的性能，且无需时间上下文，适合实时应用。该方法提高了系统的可解释性、推理速度和边缘设备部署效率，同时强调了使用文本特征表示作为高效视频异常检测的潜在方向。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10653v1",
      "published_date": "2025-03-07 20:05:59 UTC",
      "updated_date": "2025-03-07 20:05:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:05:31.634256"
    },
    {
      "arxiv_id": "2503.05899v1",
      "title": "Towards Understanding the Use of MLLM-Enabled Applications for Visual Interpretation by Blind and Low Vision People",
      "title_zh": "翻译失败",
      "authors": [
        "Ricardo E. Gonzalez Penuela",
        "Ruiying Hu",
        "Sharon Lin",
        "Tanisha Shende",
        "Shiri Azenkot"
      ],
      "abstract": "Blind and Low Vision (BLV) people have adopted AI-powered visual\ninterpretation applications to address their daily needs. While these\napplications have been helpful, prior work has found that users remain\nunsatisfied by their frequent errors. Recently, multimodal large language\nmodels (MLLMs) have been integrated into visual interpretation applications,\nand they show promise for more descriptive visual interpretations. However, it\nis still unknown how this advancement has changed people's use of these\napplications. To address this gap, we conducted a two-week diary study in which\n20 BLV people used an MLLM-enabled visual interpretation application we\ndeveloped, and we collected 553 entries. In this paper, we report a preliminary\nanalysis of 60 diary entries from 6 participants. We found that participants\nconsidered the application's visual interpretations trustworthy (mean 3.75 out\nof 5) and satisfying (mean 4.15 out of 5). Moreover, participants trusted our\napplication in high-stakes scenarios, such as receiving medical dosage advice.\nWe discuss our plan to complete our analysis to inform the design of future\nMLLM-enabled visual interpretation systems.",
      "tldr_zh": "本文研究了盲人和视力低下（BLV）人群使用整合多模态大型语言模型（MLLMs）的视觉解释应用的体验，以解决现有应用的错误问题和用户不满。研究团队通过为期两周的日记研究，让20名BLV参与者使用他们开发的MLLMs-enabled应用，并收集了553条条目。初步分析显示，参与者对应用的视觉解释评价较高（可信度平均3.75/5，满意度平均4.15/5），并在高风险场景（如医疗剂量建议）中表现出信任。该研究为未来MLLMs-enabled视觉解释系统的设计提供了初步见解和改进方向。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "I.2.1; H.5.2"
      ],
      "primary_category": "cs.HC",
      "comment": "8 pages, 1 figure, 4 tables, to appear at CHI 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.05899v1",
      "published_date": "2025-03-07 19:38:14 UTC",
      "updated_date": "2025-03-07 19:38:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:05:45.409230"
    },
    {
      "arxiv_id": "2503.05893v1",
      "title": "Zero-shot Medical Event Prediction Using a Generative Pre-trained Transformer on Electronic Health Records",
      "title_zh": "零样本医疗事件预测：使用生成式预训练Transformer在电子健康记录上的应用",
      "authors": [
        "Ekaterina Redekop",
        "Zichen Wang",
        "Rushikesh Kulkarni",
        "Mara Pleasure",
        "Aaron Chin",
        "Hamid Reza Hassanzadeh",
        "Brian L. Hill",
        "Melika Emami",
        "William Speier",
        "Corey W. Arnold"
      ],
      "abstract": "Longitudinal data in electronic health records (EHRs) represent an\nindividual`s clinical history through a sequence of codified concepts,\nincluding diagnoses, procedures, medications, and laboratory tests.\nFoundational models, such as generative pre-trained transformers (GPT), can\nleverage this data to predict future events. While fine-tuning of these models\nenhances task-specific performance, it is costly, complex, and unsustainable\nfor every target. We show that a foundation model trained on EHRs can perform\npredictive tasks in a zero-shot manner, eliminating the need for fine-tuning.\n  This study presents the first comprehensive analysis of zero-shot forecasting\nwith GPT-based foundational models in EHRs, introducing a novel pipeline that\nformulates medical concept prediction as a generative modeling task. Unlike\nsupervised approaches requiring extensive labeled data, our method enables the\nmodel to forecast a next medical event purely from a pretraining knowledge. We\nevaluate performance across multiple time horizons and clinical categories,\ndemonstrating model`s ability to capture latent temporal dependencies and\ncomplex patient trajectories without task supervision.\n  Model performance for predicting the next medical concept was evaluated using\nprecision and recall metrics, achieving an average top1 precision of 0.614 and\nrecall of 0.524. For 12 major diagnostic conditions, the model demonstrated\nstrong zero-shot performance, achieving high true positive rates while\nmaintaining low false positives.\n  We demonstrate the power of a foundational EHR GPT model in capturing diverse\nphenotypes and enabling robust, zero-shot forecasting of clinical outcomes.\nThis capability enhances the versatility of predictive healthcare models and\nreduces the need for task-specific training, enabling more scalable\napplications in clinical settings.",
      "tldr_zh": "这篇论文提出了一种基于 Generative Pre-trained Transformer (GPT) 的零样本方法，用于从 Electronic Health Records (EHRs) 中预测未来医疗事件，无需模型微调。研究引入了一个新管道，将医疗概念预测转化为生成式建模任务，依赖预训练知识捕捉潜在时间依赖性和复杂患者轨迹。实验结果显示，模型在预测下一个医疗概念时，平均 top1 精确率为 0.614 和召回率为 0.524，并在 12 个主要诊断条件下实现了高真阳性率和低假阳性率。该方法提升了预测模型的通用性与可扩展性，促进了临床应用的规模化发展。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05893v1",
      "published_date": "2025-03-07 19:26:47 UTC",
      "updated_date": "2025-03-07 19:26:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:05:57.635980"
    },
    {
      "arxiv_id": "2503.05888v1",
      "title": "QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation",
      "title_zh": "翻译失败",
      "authors": [
        "Bang Nguyen",
        "Tingting Du",
        "Mengxia Yu",
        "Lawrence Angrave",
        "Meng Jiang"
      ],
      "abstract": "While the Question Generation (QG) task has been increasingly adopted in\neducational assessments, its evaluation remains limited by approaches that lack\na clear connection to the educational values of test items. In this work, we\nintroduce test item analysis, a method frequently used by educators to assess\ntest question quality, into QG evaluation. Specifically, we construct pairs of\ncandidate questions that differ in quality across dimensions such as topic\ncoverage, item difficulty, item discrimination, and distractor efficiency. We\nthen examine whether existing QG evaluation approaches can effectively\ndistinguish these differences. Our findings reveal significant shortcomings in\nthese approaches with respect to accurately assessing test item quality in\nrelation to student performance. To address this gap, we propose a novel QG\nevaluation framework, QG-SMS, which leverages Large Language Model for Student\nModeling and Simulation to perform test item analysis. As demonstrated in our\nextensive experiments and human evaluation study, the additional perspectives\nintroduced by the simulated student profiles lead to a more effective and\nrobust assessment of test items.",
      "tldr_zh": "该论文针对Question Generation (QG)任务在教育评估中的评估问题，指出现有方法无法有效连接测试项目质量与教育价值，如topic coverage、item difficulty、item discrimination和distractor efficiency等维度。研究者构建了不同质量的候选问题对，并发现传统QG评估方法在区分这些差异以及与学生表现相关的质量评估上存在显著缺陷。为解决这一问题，他们提出QG-SMS框架，该框架利用Large Language Model (LLM)进行Student Modeling and Simulation，模拟学生配置文件以执行test item analysis。实验和人类评估结果显示，该框架提供了更有效和稳健的测试项目评估视角。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2503.05888v1",
      "published_date": "2025-03-07 19:21:59 UTC",
      "updated_date": "2025-03-07 19:21:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:06:07.355337"
    },
    {
      "arxiv_id": "2503.05696v2",
      "title": "Multi-Fidelity Policy Gradient Algorithms",
      "title_zh": "多保真度策略梯度算法",
      "authors": [
        "Xinjie Liu",
        "Cyrus Neary",
        "Kushagra Gupta",
        "Christian Ellis",
        "Ufuk Topcu",
        "David Fridovich-Keil"
      ],
      "abstract": "Many reinforcement learning (RL) algorithms require large amounts of data,\nprohibiting their use in applications where frequent interactions with\noperational systems are infeasible, or high-fidelity simulations are expensive\nor unavailable. Meanwhile, low-fidelity simulators--such as reduced-order\nmodels, heuristic reward functions, or generative world models--can cheaply\nprovide useful data for RL training, even if they are too coarse for direct\nsim-to-real transfer. We propose multi-fidelity policy gradients (MFPGs), an RL\nframework that mixes a small amount of data from the target environment with a\nlarge volume of low-fidelity simulation data to form unbiased, reduced-variance\nestimators (control variates) for on-policy policy gradients. We instantiate\nthe framework by developing multi-fidelity variants of two policy gradient\nalgorithms: REINFORCE and proximal policy optimization. Experimental results\nacross a suite of simulated robotics benchmark problems demonstrate that when\ntarget-environment samples are limited, MFPG achieves up to 3.9x higher reward\nand improves training stability when compared to baselines that only use\nhigh-fidelity data. Moreover, even when the baselines are given more\nhigh-fidelity samples--up to 10x as many interactions with the target\nenvironment--MFPG continues to match or outperform them. Finally, we observe\nthat MFPG is capable of training effective policies even when the low-fidelity\nenvironment is drastically different from the target environment. MFPG thus not\nonly offers a novel paradigm for efficient sim-to-real transfer but also\nprovides a principled approach to managing the trade-off between policy\nperformance and data collection costs.",
      "tldr_zh": "本研究针对强化学习 (RL) 算法对大量数据的需求问题，提出多保真策略梯度 (MFPGs) 框架，该框架通过混合少量高保真目标环境数据与大量低保真模拟数据（如简化模型或启发式奖励函数），构建无偏、低方差的策略梯度估计器（使用控制变量）。研究开发了 REINFORCE 和 proximal policy optimization 的多保真变体，并在模拟机器人基准问题上实验表明，当目标环境样本有限时，MFPG 可将奖励提高高达 3.9 倍，并提升训练稳定性，即使基线获得多达 10 倍的高保真样本，MFPG 仍能匹配或超越它们。总体而言，即使低保真环境与目标环境差异显著，MFPG 仍能有效训练策略，提供高效的 sim-to-real 转移方法，并平衡策略性能与数据收集成本。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05696v2",
      "published_date": "2025-03-07 18:58:23 UTC",
      "updated_date": "2025-04-09 15:52:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:06:20.349442"
    },
    {
      "arxiv_id": "2503.05860v1",
      "title": "Benchmarking AI Models in Software Engineering: A Review, Search Tool, and Enhancement Protocol",
      "title_zh": "翻译失败",
      "authors": [
        "Roham Koohestani",
        "Philippe de Bekker",
        "Maliheh Izadi"
      ],
      "abstract": "Benchmarks are essential for consistent evaluation and reproducibility. The\nintegration of Artificial Intelligence into Software Engineering (AI4SE) has\ngiven rise to numerous benchmarks for tasks such as code generation and bug\nfixing. However, this surge presents challenges: (1) scattered benchmark\nknowledge across tasks, (2) difficulty in selecting relevant benchmarks, (3)\nthe absence of a uniform standard for benchmark development, and (4)\nlimitations of existing benchmarks. In this paper, we review 173 studies and\nidentify 204 AI4SE benchmarks. We classify these benchmarks, analyze their\nlimitations, and expose gaps in practices. Based on our review, we created\nBenchScout, a semantic search tool to find relevant benchmarks, using automated\nclustering of the contexts from associated studies. We conducted a user study\nwith 22 participants to evaluate BenchScout's usability, effectiveness, and\nintuitiveness which resulted in average scores of 4.5, 4.0, and 4.1 out of 5.\nTo advance benchmarking standards, we propose BenchFrame, a unified method to\nenhance benchmark quality. As a case study, we applied BenchFrame to the\nHumanEval benchmark and addressed its main limitations. This led to\nHumanEvalNext, featuring (1) corrected errors, (2) improved language\nconversion, (3) expanded test coverage, and (4) increased difficulty. We then\nevaluated ten state-of-the-art code language models on HumanEval,\nHumanEvalPlus, and HumanEvalNext. On HumanEvalNext, models showed a pass@1\nscore reduction of 31.22% and 19.94% compared to HumanEval and HumanEvalPlus,\nrespectively.",
      "tldr_zh": "这篇论文审查了 173 篇研究，共识别出 204 个 AI4SE 基准，并分析了其挑战，包括基准知识散乱、选择困难、缺乏统一标准和现有限制。作者开发了 BenchScout，一款基于自动聚类的语义搜索工具，用于高效查找相关基准，用户研究显示其可用性、有效性和直观性得分分别为 4.5、4.0 和 4.1（满分 5）。为了提升基准质量，他们提出了 BenchFrame 统一方法，并通过案例研究将其应用于 HumanEval 基准，创建了 HumanEvalNext，改进了错误修正、语言转换、测试覆盖和难度。最终评估显示，在 HumanEvalNext 上，十个最先进代码语言模型的 pass@1 得分比 HumanEval 和 HumanEvalPlus 分别降低了 31.22% 和 19.94%，突显了增强基准的重要性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05860v1",
      "published_date": "2025-03-07 18:44:32 UTC",
      "updated_date": "2025-03-07 18:44:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:06:32.912438"
    },
    {
      "arxiv_id": "2503.05859v1",
      "title": "Quantum-like cognition and decision making in the light of quantum measurement theory",
      "title_zh": "翻译失败",
      "authors": [
        "Miho Fuyama",
        "Andrei Khrennikov",
        "Masanao Ozawa"
      ],
      "abstract": "We characterize the class of quantum measurements that matches the\napplications of quantum theory to cognition (and decision making) -\nquantum-like modeling. Projective measurements describe the canonical\nmeasurements of the basic observables of quantum physics. However, the\ncombinations of the basic cognitive effects, such as the question order and\nresponse replicability effects, cannot be described by projective measurements.\nWe motivate the use of the special class of quantum measurements, namely {\\it\nsharp repeatable non-projective measurements} - ${\\cal SR\\bar{P}}. $ This class\nis practically unused in quantum physics. Thus, physics and cognition explore\ndifferent parts of quantum measurement theory. Quantum-like modeling isn't\nautomatic borrowing of the quantum formalism. Exploring the class ${\\cal\nSR\\bar{P}}$ highlights the role of {\\it noncommutativity of the state update\nmaps generated by measurement back action.} Thus, ``non-classicality'' in\nquantum physics as well as quantum-like modeling for cognition is based on two\ndifferent types of noncommutativity, of operators (observables) and instruments\n(state update maps): {\\it observable-noncommutativity} vs. {\\it state\nupdate-noncommutativity}. We speculate that distinguishing quantum-like\nproperties of the cognitive effects are the expressions of the latter, or\npossibly both.",
      "tldr_zh": "该论文探讨了量子测量理论在认知和决策建模中的应用，特别针对 quantum-like modeling，指出传统的 projective measurements 无法充分解释认知效果的组合，如问题顺序和响应可重复性效果。作者提出采用 sharp repeatable non-projective measurements (SR̄P) 类测量，这在量子物理中鲜有应用，但能更好地捕捉认知领域的非线性特征。研究强调，非经典性源于两种非交换性（noncommutativity）：observable-noncommutativity 和 state update-noncommutativity，并推测认知效果的 quantum-like 属性主要体现于后者，或两者兼有。",
      "categories": [
        "cs.AI",
        "physics.bio-ph",
        "quant-ph"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05859v1",
      "published_date": "2025-03-07 18:30:44 UTC",
      "updated_date": "2025-03-07 18:30:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:06:43.701293"
    },
    {
      "arxiv_id": "2503.05652v1",
      "title": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities",
      "title_zh": "翻译失败",
      "authors": [
        "Yunfan Jiang",
        "Ruohan Zhang",
        "Josiah Wong",
        "Chen Wang",
        "Yanjie Ze",
        "Hang Yin",
        "Cem Gokmen",
        "Shuran Song",
        "Jiajun Wu",
        "Li Fei-Fei"
      ],
      "abstract": "Real-world household tasks present significant challenges for mobile\nmanipulation robots. An analysis of existing robotics benchmarks reveals that\nsuccessful task performance hinges on three key whole-body control\ncapabilities: bimanual coordination, stable and precise navigation, and\nextensive end-effector reachability. Achieving these capabilities requires\ncareful hardware design, but the resulting system complexity further\ncomplicates visuomotor policy learning. To address these challenges, we\nintroduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for\nwhole-body manipulation in diverse household tasks. Built on a bimanual,\nwheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body\nteleoperation interface for data collection and a novel algorithm for learning\nwhole-body visuomotor policies. We evaluate BRS on five challenging household\ntasks that not only emphasize the three core capabilities but also introduce\nadditional complexities, such as long-range navigation, interaction with\narticulated and deformable objects, and manipulation in confined spaces. We\nbelieve that BRS's integrated robotic embodiment, data collection interface,\nand learning framework mark a significant step toward enabling real-world\nwhole-body manipulation for everyday household tasks. BRS is open-sourced at\nhttps://behavior-robot-suite.github.io/",
      "tldr_zh": "该论文分析了家庭任务对移动操纵机器人的挑战，强调了双臂协调(bimanual coordination)、稳定精确导航和广泛末端执行器可达性(端-effector reachability)作为核心能力。作者引入了 BEHAVIOR Robot Suite (BRS)，一个基于双臂轮式机器人的综合框架，结合了成本有效的全身体感操作界面和新型算法来学习 whole-body visuomotor policies，以简化数据收集和策略训练。在五个具有挑战性的家庭任务（如长距离导航、与 articulated 和 deformable 物体的交互以及狭小空间操作）上评估显示，BRS 显著提升了机器人性能，并已开源以推动该领域发展。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project website: https://behavior-robot-suite.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.05652v1",
      "published_date": "2025-03-07 18:15:21 UTC",
      "updated_date": "2025-03-07 18:15:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:06:56.590802"
    },
    {
      "arxiv_id": "2503.05646v1",
      "title": "dARt Vinci: Egocentric Data Collection for Surgical Robot Learning at Scale",
      "title_zh": "翻译失败",
      "authors": [
        "Yihao Liu",
        "Yu-Chun Ku",
        "Jiaming Zhang",
        "Hao Ding",
        "Peter Kazanzides",
        "Mehran Armand"
      ],
      "abstract": "Data scarcity has long been an issue in the robot learning community.\nParticularly, in safety-critical domains like surgical applications, obtaining\nhigh-quality data can be especially difficult. It poses challenges to\nresearchers seeking to exploit recent advancements in reinforcement learning\nand imitation learning, which have greatly improved generalizability and\nenabled robots to conduct tasks autonomously. We introduce dARt Vinci, a\nscalable data collection platform for robot learning in surgical settings. The\nsystem uses Augmented Reality (AR) hand tracking and a high-fidelity physics\nengine to capture subtle maneuvers in primitive surgical tasks: By eliminating\nthe need for a physical robot setup and providing flexibility in terms of time,\nspace, and hardware resources-such as multiview sensors and\nactuators-specialized simulation is a viable alternative. At the same time, AR\nallows the robot data collection to be more egocentric, supported by its body\ntracking and content overlaying capabilities. Our user study confirms the\nproposed system's efficiency and usability, where we use widely-used primitive\ntasks for training teleoperation with da Vinci surgical robots. Data throughput\nimproves across all tasks compared to real robot settings by 41% on average.\nThe total experiment time is reduced by an average of 10%. The temporal demand\nin the task load survey is improved. These gains are statistically significant.\nAdditionally, the collected data is over 400 times smaller in size, requiring\nfar less storage while achieving double the frequency.",
      "tldr_zh": "该研究针对手术机器人学习中的数据稀缺问题，提出 dARt Vinci 平台，这是一个可扩展的数据收集系统，利用 Augmented Reality (AR) 手部跟踪和高保真物理引擎来模拟和捕获基本手术任务的细微操作。相比传统物理机器人设置，该平台通过虚拟环境提升了数据收集的灵活性和效率，无需多视图传感器和执行器。用户研究结果显示，数据吞吐量平均提高41%，实验时间减少10%，任务负载中的时间需求显著改善，且数据大小减少400倍，同时频率翻倍。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05646v1",
      "published_date": "2025-03-07 18:07:54 UTC",
      "updated_date": "2025-03-07 18:07:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:07:07.543608"
    },
    {
      "arxiv_id": "2503.05641v2",
      "title": "Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for Heterogeneous Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Justin Chih-Yao Chen",
        "Sukwon Yun",
        "Elias Stengel-Eskin",
        "Tianlong Chen",
        "Mohit Bansal"
      ],
      "abstract": "Combining existing pre-trained expert LLMs is a promising avenue for scalably\ntackling large-scale and diverse tasks. However, selecting experts at the task\nlevel is often too coarse-grained, as heterogeneous tasks may require different\nexpertise for each instance. To enable adaptive instance-level mixing of\npre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and\ngradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained\napproach to selection by emphasizing skills, e.g., algebra in math or molecular\nbiology in biomedical reasoning. We propose a skill-based recruiting strategy\nthat dynamically selects the most relevant set of expert LLMs for diverse\nreasoning tasks based on their strengths. Each selected expert then generates\nits own reasoning, resulting in k outputs from k experts, which are then\nsynthesized into a final high-quality response by an aggregator chosen based on\nits ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's\ninstance-level expert selection improves performance by a large margin but --\nwhen implemented naively -- can introduce a high computational overhead due to\nthe need for constant model loading and offloading. To address this, we\nimplement a batch inference strategy that groups instances based on their\nassigned experts, loading each model only once. This allows us to integrate 16\nexpert models on 1 GPU with a time cost comparable to or better than prior\nmulti-agent baselines using 4 GPUs. Through extensive evaluations on diverse\nbenchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that\nSymbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent\napproaches, with an absolute average improvement of 8.15% over the best\nmulti-agent baseline. Moreover, Symbolic-MoE removes the need for expensive\nmulti-round discussions, outperforming discussion baselines with less\ncomputation.",
      "tldr_zh": "该研究提出了一种名为 Symbolic-MoE 的文本-based、梯度-free Mixture-of-Experts 框架，用于在异构推理任务中实现自适应实例级专家选择。该框架通过技能-based 招聘策略动态选择最相关的预训练专家 LLMs（如针对代数或分子生物学的专长），每个专家生成独立推理输出，然后由一个聚合器整合成高质量响应。同时，引入批量推理策略来减少模型加载开销，使其在1 GPU上整合16个专家模型，计算效率优于使用4 GPU的多代理基线。在MMLU-Pro、GPQA、AIME和MedMCQA等基准测试中，Symbolic-MoE比GPT4o-mini和现有多代理方法平均提升8.15%的性能，且无需昂贵的多轮讨论。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "The first three authors contributed equally. Project Page:\n  https://symbolic-moe.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.05641v2",
      "published_date": "2025-03-07 18:03:13 UTC",
      "updated_date": "2025-03-11 21:40:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:07:19.038088"
    },
    {
      "arxiv_id": "2503.05639v3",
      "title": "VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Bian",
        "Zhaoyang Zhang",
        "Xuan Ju",
        "Mingdeng Cao",
        "Liangbin Xie",
        "Ying Shan",
        "Qiang Xu"
      ],
      "abstract": "Video inpainting, which aims to restore corrupted video content, has\nexperienced substantial progress. Despite these advances, existing methods,\nwhether propagating unmasked region pixels through optical flow and receptive\nfield priors, or extending image-inpainting models temporally, face challenges\nin generating fully masked objects or balancing the competing objectives of\nbackground context preservation and foreground generation in one model,\nrespectively. To address these limitations, we propose a novel dual-stream\nparadigm VideoPainter that incorporates an efficient context encoder\n(comprising only 6% of the backbone parameters) to process masked videos and\ninject backbone-aware background contextual cues to any pre-trained video DiT,\nproducing semantically consistent content in a plug-and-play manner. This\narchitectural separation significantly reduces the model's learning complexity\nwhile enabling nuanced integration of crucial background context. We also\nintroduce a novel target region ID resampling technique that enables any-length\nvideo inpainting, greatly enhancing our practical applicability. Additionally,\nwe establish a scalable dataset pipeline leveraging current vision\nunderstanding models, contributing VPData and VPBench to facilitate\nsegmentation-based inpainting training and assessment, the largest video\ninpainting dataset and benchmark to date with over 390K diverse clips. Using\ninpainting as a pipeline basis, we also explore downstream applications\nincluding video editing and video editing pair data generation, demonstrating\ncompetitive performance and significant practical potential. Extensive\nexperiments demonstrate VideoPainter's superior performance in both any-length\nvideo inpainting and editing, across eight key metrics, including video\nquality, mask region preservation, and textual coherence.",
      "tldr_zh": "该研究提出VideoPainter，一种新型双流范式，用于视频修复（Video inpainting）和编辑，支持任意长度的处理，并通过高效上下文编码器（仅占主干参数6%）以即插即用方式注入背景上下文提示到预训练视频DiT模型中，实现语义一致的内容生成和平衡背景保留与前景生成。研究还引入目标区域ID重采样技术来处理长视频，并构建了最大的视频修复数据集VPData和基准VPBench，包含超过39万多样化剪辑，用于训练和评估。实验结果显示，VideoPainter在八个关键指标上（如视频质量、遮罩区域保留和文本连贯性）显著优于现有方法，并扩展到下游应用如视频编辑和数据生成，展示了其实际潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page available at\n  https://yxbian23.github.io/project/video-painter",
      "pdf_url": "http://arxiv.org/pdf/2503.05639v3",
      "published_date": "2025-03-07 17:59:46 UTC",
      "updated_date": "2025-04-09 02:05:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:07:31.247362"
    },
    {
      "arxiv_id": "2503.05638v1",
      "title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models",
      "title_zh": "TrajectoryCrafter：通过扩散",
      "authors": [
        "Mark YU",
        "Wenbo Hu",
        "Jinbo Xing",
        "Ying Shan"
      ],
      "abstract": "We present TrajectoryCrafter, a novel approach to redirect camera\ntrajectories for monocular videos. By disentangling deterministic view\ntransformations from stochastic content generation, our method achieves precise\ncontrol over user-specified camera trajectories. We propose a novel dual-stream\nconditional video diffusion model that concurrently integrates point cloud\nrenders and source videos as conditions, ensuring accurate view transformations\nand coherent 4D content generation. Instead of leveraging scarce multi-view\nvideos, we curate a hybrid training dataset combining web-scale monocular\nvideos with static multi-view datasets, by our innovative double-reprojection\nstrategy, significantly fostering robust generalization across diverse scenes.\nExtensive evaluations on multi-view and large-scale monocular videos\ndemonstrate the superior performance of our method.",
      "tldr_zh": "本文提出 TrajectoryCrafter，一种通过扩散模型重定向单目视频相机轨迹的方法，实现用户对相机轨迹的精确控制。方法采用双流条件视频扩散模型，同时整合点云 renders 和源视频作为条件，确保准确的视图变换和连贯的4D内容生成。为了提升泛化能力，该研究使用创新的双重重投影策略构建混合训练数据集，结合网络规模单目视频和静态多视图数据集。实验评估显示，TrajectoryCrafter 在多视图和大规模单目视频上表现出优越性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Project webpage: https://trajectorycrafter.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.05638v1",
      "published_date": "2025-03-07 17:57:53 UTC",
      "updated_date": "2025-03-07 17:57:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:07:42.987096"
    },
    {
      "arxiv_id": "2503.05629v1",
      "title": "Exploring FMCW Radars and Feature Maps for Activity Recognition: A Benchmark Study",
      "title_zh": "翻译失败",
      "authors": [
        "Ali Samimi Fard",
        "Mohammadreza Mashhadigholamali",
        "Samaneh Zolfaghari",
        "Hajar Abedi",
        "Mainak Chakraborty",
        "Luigi Borzì",
        "Masoud Daneshtalab",
        "George Shaker"
      ],
      "abstract": "Human Activity Recognition has gained significant attention due to its\ndiverse applications, including ambient assisted living and remote sensing.\nWearable sensor-based solutions often suffer from user discomfort and\nreliability issues, while video-based methods raise privacy concerns and\nperform poorly in low-light conditions or long ranges. This study introduces a\nFrequency-Modulated Continuous Wave radar-based framework for human activity\nrecognition, leveraging a 60 GHz radar and multi-dimensional feature maps.\nUnlike conventional approaches that process feature maps as images, this study\nfeeds multi-dimensional feature maps -- Range-Doppler, Range-Azimuth, and\nRange-Elevation -- as data vectors directly into the machine learning (SVM,\nMLP) and deep learning (CNN, LSTM, ConvLSTM) models, preserving the spatial and\ntemporal structures of the data. These features were extracted from a novel\ndataset with seven activity classes and validated using two different\nvalidation approaches. The ConvLSTM model outperformed conventional machine\nlearning and deep learning models, achieving an accuracy of 90.51% and an\nF1-score of 87.31% on cross-scene validation and an accuracy of 89.56% and an\nF1-score of 87.15% on leave-one-person-out cross-validation. The results\nhighlight the approach's potential for scalable, non-intrusive, and\nprivacy-preserving activity monitoring in real-world scenarios.",
      "tldr_zh": "本研究探讨了基于FMCW Radars和特征图的人类活动识别，旨在克服可穿戴传感器不适和视频方法隐私问题的局限性。研究引入一个框架，使用60 GHz雷达提取多维特征图（如Range-Doppler、Range-Azimuth和Range-Elevation），并直接作为数据向量输入机器学习（SVM、MLP）和深度学习（CNN、LSTM、ConvLSTM）模型，以保留数据的空间和时间结构。实验在新数据集（包含七个活动类别）上验证，ConvLSTM模型表现出色，在跨场景验证中达到90.51%的准确率和87.31%的F1分数，在留一法交叉验证中达到89.56%的准确率和87.15%的F1分数。该方法展示了在真实场景中实现可扩展、非侵入式和隐私保护活动监控的潜力。",
      "categories": [
        "cs.ET",
        "cs.AI"
      ],
      "primary_category": "cs.ET",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05629v1",
      "published_date": "2025-03-07 17:53:29 UTC",
      "updated_date": "2025-03-07 17:53:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:07:56.493295"
    },
    {
      "arxiv_id": "2503.05628v2",
      "title": "Superintelligence Strategy: Expert Version",
      "title_zh": "超级智能战略：专家版",
      "authors": [
        "Dan Hendrycks",
        "Eric Schmidt",
        "Alexandr Wang"
      ],
      "abstract": "Rapid advances in AI are beginning to reshape national security.\nDestabilizing AI developments could rupture the balance of power and raise the\nodds of great-power conflict, while widespread proliferation of capable AI\nhackers and virologists would lower barriers for rogue actors to cause\ncatastrophe. Superintelligence -- AI vastly better than humans at nearly all\ncognitive tasks -- is now anticipated by AI researchers. Just as nations once\ndeveloped nuclear strategies to secure their survival, we now need a coherent\nsuperintelligence strategy to navigate a new period of transformative change.\nWe introduce the concept of Mutual Assured AI Malfunction (MAIM): a deterrence\nregime resembling nuclear mutual assured destruction (MAD) where any state's\naggressive bid for unilateral AI dominance is met with preventive sabotage by\nrivals. Given the relative ease of sabotaging a destabilizing AI project --\nthrough interventions ranging from covert cyberattacks to potential kinetic\nstrikes on datacenters -- MAIM already describes the strategic picture AI\nsuperpowers find themselves in. Alongside this, states can increase their\ncompetitiveness by bolstering their economies and militaries through AI, and\nthey can engage in nonproliferation to rogue actors to keep weaponizable AI\ncapabilities out of their hands. Taken together, the three-part framework of\ndeterrence, nonproliferation, and competitiveness outlines a robust strategy to\nsuperintelligence in the years ahead.",
      "tldr_zh": "该论文讨论了AI快速进展对国家安全的潜在威胁，包括打破权力平衡、增加大国冲突风险，以及AI黑客和病毒专家扩散可能导致的灾难。作者提出Superintelligence战略框架，借鉴核威慑理论，引入Mutual Assured AI Malfunction (MAIM)概念，即任何国家追求单方面AI主导将面临对手的破坏行动（如网络攻击或打击数据中心），从而实现类似mutual assured destruction (MAD)的威慑。框架包括三个部分：威慑（deterrence）、非proliferation（防止流氓行为者获得武器化AI）和竞争力（通过AI增强经济与军事），旨在帮助国家安全应对超级智能时代的变革。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "https://nationalsecurity.ai/",
      "pdf_url": "http://arxiv.org/pdf/2503.05628v2",
      "published_date": "2025-03-07 17:53:24 UTC",
      "updated_date": "2025-04-14 21:28:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:08:07.571986"
    },
    {
      "arxiv_id": "2503.05626v1",
      "title": "FMT:A Multimodal Pneumonia Detection Model Based on Stacking MOE Framework",
      "title_zh": "翻译失败",
      "authors": [
        "Jingyu Xu",
        "Yang Wang"
      ],
      "abstract": "Artificial intelligence has shown the potential to improve diagnostic\naccuracy through medical image analysis for pneumonia diagnosis. However,\ntraditional multimodal approaches often fail to address real-world challenges\nsuch as incomplete data and modality loss. In this study, a Flexible Multimodal\nTransformer (FMT) was proposed, which uses ResNet-50 and BERT for joint\nrepresentation learning, followed by a dynamic masked attention strategy that\nsimulates clinical modality loss to improve robustness; finally, a sequential\nmixture of experts (MOE) architecture was used to achieve multi-level decision\nrefinement. After evaluation on a small multimodal pneumonia dataset, FMT\nachieved state-of-the-art performance with 94% accuracy, 95% recall, and 93% F1\nscore, outperforming single-modal baselines (ResNet: 89%; BERT: 79%) and the\nmedical benchmark CheXMed (90%), providing a scalable solution for multimodal\ndiagnosis of pneumonia in resource-constrained medical settings.",
      "tldr_zh": "该研究提出了一种基于堆叠混合专家（Stacking MOE）框架的多模态肺炎检测模型FMT，旨在解决传统方法在处理数据不完整和模态丢失等实际挑战时的不足。FMT利用ResNet-50和BERT进行联合表示学习，并引入动态masked attention策略来模拟临床模态丢失，提高模型的鲁棒性；随后，通过sequential mixture of experts (MOE)架构实现多级决策精炼。在小规模多模态肺炎数据集上的评估中，FMT取得了94%准确率、95%召回率和93% F1分数，优于单模态基线（ResNet: 89%; BERT: 79%）和CheXMed基准（90%），为资源受限医疗环境提供了一个可扩展的诊断解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05626v1",
      "published_date": "2025-03-07 17:52:12 UTC",
      "updated_date": "2025-03-07 17:52:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:08:18.916960"
    },
    {
      "arxiv_id": "2503.05620v1",
      "title": "Learning LLM Preference over Intra-Dialogue Pairs: A Framework for Utterance-level Understandings",
      "title_zh": "翻译失败",
      "authors": [
        "Xuanqing Liu",
        "Luyang Kong",
        "Wei Niu",
        "Afshin Khashei",
        "Belinda Zeng",
        "Steve Johnson",
        "Jon Jay",
        "Davor Golac",
        "Matt Pope"
      ],
      "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nhandling complex dialogue tasks without requiring use case-specific\nfine-tuning. However, analyzing live dialogues in real-time necessitates\nlow-latency processing systems, making it impractical to deploy models with\nbillions of parameters due to latency constraints. As a result, practitioners\noften prefer smaller models with millions of parameters, trained on\nhigh-quality, human-annotated datasets. Yet, curating such datasets is both\ntime-consuming and costly. Consequently, there is a growing need to combine the\nscalability of LLM-generated labels with the precision of human annotations,\nenabling fine-tuned smaller models to achieve both higher speed and accuracy\ncomparable to larger models. In this paper, we introduce a simple yet effective\nframework to address this challenge. Our approach is specifically designed for\nper-utterance classification problems, which encompass tasks such as intent\ndetection, dialogue state tracking, and more. To mitigate the impact of\nlabeling errors from LLMs -- the primary source of inaccuracies in student\nmodels -- we propose a noise-reduced preference learning loss. Experimental\nresults demonstrate that our method significantly improves accuracy across\nutterance-level dialogue tasks, including sentiment detection (over $2\\%$),\ndialogue act classification (over $1.5\\%$), etc.",
      "tldr_zh": "该论文提出一个简单有效的框架，用于学习大型语言模型（LLMs）在对话内部对的偏好，从而提升utterance-level任务的理解，如意图检测和对话状态跟踪。该框架通过引入噪声减少的preference learning loss，减少LLM标注错误的影响，使小型模型在低延迟条件下实现与大型模型相当的准确性。实验结果显示，该方法在情感检测任务中准确率提升超过2%，在对话行为分类中提升超过1.5%，为高效对话处理提供了可扩展解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "7 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05620v1",
      "published_date": "2025-03-07 17:46:13 UTC",
      "updated_date": "2025-03-07 17:46:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:08:30.964728"
    },
    {
      "arxiv_id": "2503.05613v1",
      "title": "A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models",
      "title_zh": "稀疏自编码器的综述：解释大型语言模型的内部机制",
      "authors": [
        "Dong Shu",
        "Xuansheng Wu",
        "Haiyan Zhao",
        "Daking Rai",
        "Ziyu Yao",
        "Ninghao Liu",
        "Mengnan Du"
      ],
      "abstract": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet their internal mechanisms remain largely opaque. Recently, mechanistic\ninterpretability has attracted significant attention from the research\ncommunity as a means to understand the inner workings of LLMs. Among various\nmechanistic interpretability approaches, Sparse Autoencoders (SAEs) have\nemerged as a particularly promising method due to their ability to disentangle\nthe complex, superimposed features within LLMs into more interpretable\ncomponents. This paper presents a comprehensive examination of SAEs as a\npromising approach to interpreting and understanding LLMs. We provide a\nsystematic overview of SAE principles, architectures, and applications\nspecifically tailored for LLM analysis, covering theoretical foundations,\nimplementation strategies, and recent developments in sparsity mechanisms. We\nalso explore how SAEs can be leveraged to explain the internal workings of\nLLMs, steer model behaviors in desired directions, and develop more transparent\ntraining methodologies for future models. Despite the challenges that remain\naround SAE implementation and scaling, they continue to provide valuable tools\nfor understanding the internal mechanisms of large language models.",
      "tldr_zh": "这篇论文对 Sparse Autoencoders (SAEs) 进行了全面调查，作为一种解释 Large Language Models (LLMs) 内部机制的有效方法。论文系统概述了 SAEs 的原理、架构和应用，包括理论基础、实现策略以及最近的稀疏机制发展。SAEs 通过将 LLMs 的复杂特征分解成可解释组件，能够帮助解释模型内部工作、引导模型行为，并开发更透明的训练方法。尽管 SAEs 在实施和扩展方面面临挑战，但它们为 mechanistic interpretability 提供了宝贵工具。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05613v1",
      "published_date": "2025-03-07 17:38:00 UTC",
      "updated_date": "2025-03-07 17:38:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:08:43.103211"
    },
    {
      "arxiv_id": "2503.05604v1",
      "title": "CACTUS: An Open Dataset and Framework for Automated Cardiac Assessment and Classification of Ultrasound Images Using Deep Transfer Learning",
      "title_zh": "CACTUS：一种用于自动心脏评估和分类超声图像的开源数据集和框架，基于深度迁移学习",
      "authors": [
        "Hanae Elmekki",
        "Ahmed Alagha",
        "Hani Sami",
        "Amanda Spilkin",
        "Antonela Mariel Zanuttini",
        "Ehsan Zakeri",
        "Jamal Bentahar",
        "Lyes Kadem",
        "Wen-Fang Xie",
        "Philippe Pibarot",
        "Rabeb Mizouni",
        "Hadi Otrok",
        "Shakti Singh",
        "Azzam Mourad"
      ],
      "abstract": "Cardiac ultrasound (US) scanning is a commonly used techniques in cardiology\nto diagnose the health of the heart and its proper functioning. Therefore, it\nis necessary to consider ways to automate these tasks and assist medical\nprofessionals in classifying and assessing cardiac US images. Machine learning\n(ML) techniques are regarded as a prominent solution due to their success in\nnumerous applications aimed at enhancing the medical field, including\naddressing the shortage of echography technicians. However, the limited\navailability of medical data presents a significant barrier to applying ML in\ncardiology, particularly regarding US images of the heart. This paper addresses\nthis challenge by introducing the first open graded dataset for Cardiac\nAssessment and ClassificaTion of UltraSound (CACTUS), which is available\nonline. This dataset contains images obtained from scanning a CAE Blue Phantom\nand representing various heart views and different quality levels, exceeding\nthe conventional cardiac views typically found in the literature. Additionally,\nthe paper introduces a Deep Learning (DL) framework consisting of two main\ncomponents. The first component classifies cardiac US images based on the heart\nview using a Convolutional Neural Network (CNN). The second component uses\nTransfer Learning (TL) to fine-tune the knowledge from the first component and\ncreate a model for grading and assessing cardiac images. The framework\ndemonstrates high performance in both classification and grading, achieving up\nto 99.43% accuracy and as low as 0.3067 error, respectively. To showcase its\nrobustness, the framework is further fine-tuned using new images representing\nadditional cardiac views and compared to several other state-of-the-art\narchitectures. The framework's outcomes and performance in handling real-time\nscans were also assessed using a questionnaire answered by cardiac experts.",
      "tldr_zh": "这篇论文引入了 CACTUS，这是一个公开的心脏超声图像数据集，包含各种心脏视图和质量水平，用于解决医疗数据短缺问题并支持心脏评估和分类。研究提出一个深度学习框架，包括使用 Convolutional Neural Network (CNN) 进行心脏视图分类，以及通过 Transfer Learning (TL) 微调模型来评估和分级图像。实验结果显示，该框架在分类任务中达到99.43%的准确率，在评估任务中错误率低至0.3067，并通过与其他模型比较和专家问卷验证了其在实时扫描中的鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05604v1",
      "published_date": "2025-03-07 17:29:04 UTC",
      "updated_date": "2025-03-07 17:29:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:08:54.703535"
    },
    {
      "arxiv_id": "2503.05592v2",
      "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Huatong Song",
        "Jinhao Jiang",
        "Yingqian Min",
        "Jie Chen",
        "Zhipeng Chen",
        "Wayne Xin Zhao",
        "Lei Fang",
        "Ji-Rong Wen"
      ],
      "abstract": "Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose \\textbf{R1-Searcher}, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini.",
      "tldr_zh": "本文提出 R1-Searcher，一种新型的两阶段基于结果的强化学习（RL）方法，旨在提升大型语言模型（LLMs）的搜索能力，使其在推理过程中自主调用外部搜索系统获取额外知识，从而解决依赖内部知识导致的不准确性和幻觉问题。该框架无需过程奖励或蒸馏，即可实现冷启动，并适用于 Base 和 Instruct 模型。实验结果显示，R1-Searcher 显著优于现有 RAG 方法，甚至超越闭源模型 GPT-4o-mini，在知识密集型任务上表现出色。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05592v2",
      "published_date": "2025-03-07 17:14:44 UTC",
      "updated_date": "2025-03-18 08:32:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:09:07.021161"
    },
    {
      "arxiv_id": "2503.05587v1",
      "title": "Quantifying the Robustness of Retrieval-Augmented Language Models Against Spurious Features in Grounding Data",
      "title_zh": "翻译失败",
      "authors": [
        "Shiping Yang",
        "Jie Wu",
        "Wenbiao Ding",
        "Ning Wu",
        "Shining Liang",
        "Ming Gong",
        "Hengyuan Zhang",
        "Dongmei Zhang"
      ],
      "abstract": "Robustness has become a critical attribute for the deployment of RAG systems\nin real-world applications. Existing research focuses on robustness to explicit\nnoise (e.g., document semantics) but overlooks spurious features (a.k.a.\nimplicit noise). While previous works have explored spurious features in LLMs,\nthey are limited to specific features (e.g., formats) and narrow scenarios\n(e.g., ICL). In this work, we statistically confirm the presence of spurious\nfeatures in the RAG paradigm, a robustness problem caused by the sensitivity of\nLLMs to semantic-agnostic features. Moreover, we provide a comprehensive\ntaxonomy of spurious features and empirically quantify their impact through\ncontrolled experiments. Further analysis reveals that not all spurious features\nare harmful and they can even be beneficial sometimes. Extensive evaluation\nresults across multiple LLMs suggest that spurious features are a widespread\nand challenging problem in the field of RAG. The code and dataset will be\nreleased to facilitate future research. We release all codes and data at:\n$\\\\\\href{https://github.com/maybenotime/RAG-SpuriousFeatures}{https://github.com/maybenotime/RAG-SpuriousFeatures}$.",
      "tldr_zh": "本研究量化了检索增强语言模型（RAG）在基础数据中对虚假特征（spurious features）的鲁棒性，强调了现有工作忽略的隐式噪声问题，如LLMs对语义无关特征的敏感性。作者通过统计分析和控制实验，确认了虚假特征在RAG范式中的广泛存在，并提供了其全面分类，同时评估了这些特征对模型性能的影响。结果显示，并非所有虚假特征都有害，有时甚至有益，但整体上这仍是RAG领域的一个具有挑战性的问题。研究还发布了代码和数据集（https://github.com/maybenotime/RAG-SpuriousFeatures），以促进未来研究。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05587v1",
      "published_date": "2025-03-07 17:11:34 UTC",
      "updated_date": "2025-03-07 17:11:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:09:19.055256"
    },
    {
      "arxiv_id": "2503.05857v1",
      "title": "SYMBIOSIS: Systems Thinking and Machine Intelligence for Better Outcomes in Society",
      "title_zh": "SYMBIOSIS：系统思考和机器智能用于社会中更好的结果",
      "authors": [
        "Sameer Sethi",
        "Donald Martin Jr.",
        "Emmanuel Klu"
      ],
      "abstract": "This paper presents SYMBIOSIS, an AI-powered framework and platform designed\nto make Systems Thinking accessible for addressing societal challenges and\nunlock paths for leveraging systems thinking frameworks to improve AI systems.\nThe platform establishes a centralized, open-source repository of systems\nthinking/system dynamics models categorized by Sustainable Development Goals\n(SDGs) and societal topics using topic modeling and classification techniques.\nSystems Thinking resources, though critical for articulating causal theories in\ncomplex problem spaces, are often locked behind specialized tools and intricate\nnotations, creating high barriers to entry. To address this, we developed a\ngenerative co-pilot that translates complex systems representations - such as\ncausal loop and stock-flow diagrams - into natural language (and vice-versa),\nallowing users to explore and build models without extensive technical\ntraining.\n  Rooted in community-based system dynamics (CBSD) and informed by\ncommunity-driven insights on societal context, we aim to bridge the problem\nunderstanding chasm. This gap, driven by epistemic uncertainty, often limits ML\ndevelopers who lack the community-specific knowledge essential for problem\nunderstanding and formulation, often leading to ill informed causal\nassumptions, reduced intervention effectiveness and harmful biases. Recent\nresearch identifies causal and abductive reasoning as crucial frontiers for AI,\nand Systems Thinking provides a naturally compatible framework for both. By\nmaking Systems Thinking frameworks more accessible and user-friendly, SYMBIOSIS\naims to serve as a foundational step to unlock future research into responsible\nand society-centered AI. Our work underscores the need for ongoing research\ninto AI's capacity to understand essential characteristics of complex adaptive\nsystems paving the way for more socially attuned, effective AI systems.",
      "tldr_zh": "这篇论文介绍了 SYMBIOSIS，一个 AI 驱动的框架和平台，旨在提升 Systems Thinking 的可访问性，以解决社会挑战并优化 AI 系统。该平台建立了一个中心化开源仓库，包含按 Sustainable Development Goals (SDGs) 和社会主题分类的 Systems Thinking/System Dynamics 模型，并采用 topic modeling 和 classification 技术进行组织。同时，开发了一个生成式 co-pilot，能将复杂的系统表示如 causal loop 和 stock-flow diagrams 转化为自然语言（反之亦然），让用户无需专业训练即可探索和构建模型。基于 community-based system dynamics (CBSD) 和社区驱动见解，SYMBIOSIS 桥接了问题理解的鸿沟，减少 ML 开发中的因果假设错误和偏差，推动 causal and abductive reasoning 在负责任 AI 研究中的应用。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05857v1",
      "published_date": "2025-03-07 17:07:26 UTC",
      "updated_date": "2025-03-07 17:07:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:09:32.472790"
    },
    {
      "arxiv_id": "2503.05573v1",
      "title": "InDRiVE: Intrinsic Disagreement based Reinforcement for Vehicle Exploration through Curiosity Driven Generalized World Model",
      "title_zh": "翻译失败",
      "authors": [
        "Feeza Khan Khanzada",
        "Jaerock Kwon"
      ],
      "abstract": "Model-based Reinforcement Learning (MBRL) has emerged as a promising paradigm\nfor autonomous driving, where data efficiency and robustness are critical. Yet,\nexisting solutions often rely on carefully crafted, task specific extrinsic\nrewards, limiting generalization to new tasks or environments. In this paper,\nwe propose InDRiVE (Intrinsic Disagreement based Reinforcement for Vehicle\nExploration), a method that leverages purely intrinsic, disagreement based\nrewards within a Dreamer based MBRL framework. By training an ensemble of world\nmodels, the agent actively explores high uncertainty regions of environments\nwithout any task specific feedback. This approach yields a task agnostic latent\nrepresentation, allowing for rapid zero shot or few shot fine tuning on\ndownstream driving tasks such as lane following and collision avoidance.\nExperimental results in both seen and unseen environments demonstrate that\nInDRiVE achieves higher success rates and fewer infractions compared to\nDreamerV2 and DreamerV3 baselines despite using significantly fewer training\nsteps. Our findings highlight the effectiveness of purely intrinsic exploration\nfor learning robust vehicle control behaviors, paving the way for more scalable\nand adaptable autonomous driving systems.",
      "tldr_zh": "该研究提出 InDRiVE，一种基于内在分歧（Intrinsic Disagreement）的强化学习方法，用于车辆探索，通过好奇驱动的通用世界模型（Curiosity Driven Generalized World Model）在 Model-based Reinforcement Learning (MBRL) 框架中实现。InDRiVE 通过训练世界模型的集合（ensemble of world models）生成纯内在奖励，驱动代理探索高不确定性区域，从而创建任务无关的潜在表示，并支持快速零样本或少样本微调，如车道跟随和碰撞避免任务。实验结果显示，在已见和未见环境中，InDRiVE 相较于 DreamerV2 和 DreamerV3 基线，实现了更高的成功率和更少的违规行为，同时仅需更少的训练步骤，这为开发更可扩展的自动驾驶系统提供了新途径。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.RO",
      "comment": "This work has been submitted to IROS 2025 and is currently under\n  review",
      "pdf_url": "http://arxiv.org/pdf/2503.05573v1",
      "published_date": "2025-03-07 16:56:00 UTC",
      "updated_date": "2025-03-07 16:56:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:09:45.031902"
    },
    {
      "arxiv_id": "2503.05571v1",
      "title": "Compliance of AI Systems",
      "title_zh": "AI 系统的合规性",
      "authors": [
        "Julius Schöning",
        "Niklas Kruse"
      ],
      "abstract": "The increasing integration of artificial intelligence (AI) systems in various\nfields requires solid concepts to ensure compliance with upcoming legislation.\nThis paper systematically examines the compliance of AI systems with relevant\nlegislation, focusing on the EU's AI Act and the compliance of data sets. The\nanalysis highlighted many challenges associated with edge devices, which are\nincreasingly being used to deploy AI applications closer and closer to the data\nsources. Such devices often face unique issues due to their decentralized\nnature and limited computing resources for implementing sophisticated\ncompliance mechanisms. By analyzing AI implementations, the paper identifies\nchallenges and proposes the first best practices for legal compliance when\ndeveloping, deploying, and running AI. The importance of data set compliance is\nhighlighted as a cornerstone for ensuring the trustworthiness, transparency,\nand explainability of AI systems, which must be aligned with ethical standards\nset forth in regulatory frameworks such as the AI Act. The insights gained\nshould contribute to the ongoing discourse on the responsible development and\ndeployment of embedded AI systems.",
      "tldr_zh": "本研究系统分析了人工智能(AI)系统与相关法规（如欧盟的AI Act）的合规性，重点关注边缘设备(edge devices)带来的挑战，这些设备因去中心化和资源限制而难以实施复杂的合规机制。论文通过分析AI实现过程，识别了关键问题，并首次提出最佳实践指南，用于AI的开发、部署和运行，以确保系统遵守法律要求。研究强调数据集(data sets)合规性是AI系统可信度、透明度和可解释性的核心基础，并为负责任的嵌入式AI发展提供宝贵见解。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET",
        "I.2.1; H.4.0"
      ],
      "primary_category": "cs.CY",
      "comment": "5 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05571v1",
      "published_date": "2025-03-07 16:53:36 UTC",
      "updated_date": "2025-03-07 16:53:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:09:54.660195"
    },
    {
      "arxiv_id": "2503.05546v1",
      "title": "Impoola: The Power of Average Pooling for Image-Based Deep Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Raphael Trumpp",
        "Ansgar Schäfftlein",
        "Mirco Theile",
        "Marco Caccamo"
      ],
      "abstract": "As image-based deep reinforcement learning tackles more challenging tasks,\nincreasing model size has become an important factor in improving performance.\nRecent studies achieved this by focusing on the parameter efficiency of scaled\nnetworks, typically using Impala-CNN, a 15-layer ResNet-inspired network, as\nthe image encoder. However, while Impala-CNN evidently outperforms older CNN\narchitectures, potential advancements in network design for deep reinforcement\nlearning-specific image encoders remain largely unexplored. We find that\nreplacing the flattening of output feature maps in Impala-CNN with global\naverage pooling leads to a notable performance improvement. This approach\noutperforms larger and more complex models in the Procgen Benchmark,\nparticularly in terms of generalization. We call our proposed encoder model\nImpoola-CNN. A decrease in the network's translation sensitivity may be central\nto this improvement, as we observe the most significant gains in games without\nagent-centered observations. Our results demonstrate that network scaling is\nnot just about increasing model size - efficient network design is also an\nessential factor.",
      "tldr_zh": "该论文探讨了图像-based 深度强化学习（deep reinforcement learning）中网络设计的优化，提出 Impoola-CNN 模型，通过将 Impala-CNN 中的输出特征图 flattening 替换为 global average pooling，显著提升了性能。实验结果显示，Impoola-CNN 在 Procgen Benchmark 上超越了更大更复杂的模型，尤其在泛化能力方面表现出色。作者发现，这种改进可能归因于减少网络对平移的敏感性，特别是在没有 agent-centered observations 的游戏中，并强调高效网络设计比单纯增加模型大小更重要。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05546v1",
      "published_date": "2025-03-07 16:19:19 UTC",
      "updated_date": "2025-03-07 16:19:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:10:07.098065"
    },
    {
      "arxiv_id": "2503.05522v1",
      "title": "Post-Hoc Concept Disentanglement: From Correlated to Isolated Concept Representations",
      "title_zh": "事后概念解耦：从相关概念表示到孤立概念表示",
      "authors": [
        "Eren Erogullari",
        "Sebastian Lapuschkin",
        "Wojciech Samek",
        "Frederik Pahde"
      ],
      "abstract": "Concept Activation Vectors (CAVs) are widely used to model\nhuman-understandable concepts as directions within the latent space of neural\nnetworks. They are trained by identifying directions from the activations of\nconcept samples to those of non-concept samples. However, this method often\nproduces similar, non-orthogonal directions for correlated concepts, such as\n\"beard\" and \"necktie\" within the CelebA dataset, which frequently co-occur in\nimages of men. This entanglement complicates the interpretation of concepts in\nisolation and can lead to undesired effects in CAV applications, such as\nactivation steering. To address this issue, we introduce a post-hoc concept\ndisentanglement method that employs a non-orthogonality loss, facilitating the\nidentification of orthogonal concept directions while preserving directional\ncorrectness. We evaluate our approach with real-world and controlled correlated\nconcepts in CelebA and a synthetic FunnyBirds dataset with VGG16 and ResNet18\narchitectures. We further demonstrate the superiority of orthogonalized concept\nrepresentations in activation steering tasks, allowing (1) the insertion of\nisolated concepts into input images through generative models and (2) the\nremoval of concepts for effective shortcut suppression with reduced impact on\ncorrelated concepts in comparison to baseline CAVs.",
      "tldr_zh": "该论文解决了 Concept Activation Vectors (CAVs) 在神经网络潜在空间中建模相关概念时的纠缠问题，例如 CelebA 数据集中 “beard” 和 “necktie” 经常共现导致非正交方向。作者提出了一种后处理概念解纠方法，使用 non-orthogonality loss 来生成正交的概念方向，同时保持方向的正确性。在 CelebA 和 FunnyBirds 数据集上结合 VGG16 和 ResNet18 模型进行评估，结果显示，该方法在 activation steering 任务中表现出色，能更有效地插入孤立概念或移除概念，同时减少对相关概念的影响。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05522v1",
      "published_date": "2025-03-07 15:45:43 UTC",
      "updated_date": "2025-03-07 15:45:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:10:19.645338"
    },
    {
      "arxiv_id": "2503.05516v1",
      "title": "Cognitive Bias Detection Using Advanced Prompt Engineering",
      "title_zh": "使用高级提示工程的",
      "authors": [
        "Frederic Lemieux",
        "Aisha Behr",
        "Clara Kellermann-Bryant",
        "Zaki Mohammed"
      ],
      "abstract": "Cognitive biases, systematic deviations from rationality in judgment, pose\nsignificant challenges in generating objective content. This paper introduces a\nnovel approach for real-time cognitive bias detection in user-generated text\nusing large language models (LLMs) and advanced prompt engineering techniques.\nThe proposed system analyzes textual data to identify common cognitive biases\nsuch as confirmation bias, circular reasoning, and hidden assumption. By\ndesigning tailored prompts, the system effectively leverages LLMs' capabilities\nto both recognize and mitigate these biases, improving the quality of\nhuman-generated content (e.g., news, media, reports). Experimental results\ndemonstrate the high accuracy of our approach in identifying cognitive biases,\noffering a valuable tool for enhancing content objectivity and reducing the\nrisks of biased decision-making.",
      "tldr_zh": "这篇论文提出了一种新方法，使用大型语言模型（LLMs）和高级提示工程（advanced prompt engineering）来实时检测用户生成文本中的认知偏差（cognitive biases）。系统通过设计定制提示，针对常见偏差如确认偏差（confirmation bias）、循环推理（circular reasoning）和隐藏假设（hidden assumption）进行识别和缓解，从而提升内容质量，例如在新闻、媒体和报告中。实验结果显示，该方法在识别认知偏差方面具有高准确率，为增强内容客观性和减少偏见决策风险提供了有效工具。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.HC"
      ],
      "primary_category": "cs.CY",
      "comment": "17 pages. 6 Figures, 2 Tables",
      "pdf_url": "http://arxiv.org/pdf/2503.05516v1",
      "published_date": "2025-03-07 15:35:37 UTC",
      "updated_date": "2025-03-07 15:35:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:10:31.106302"
    },
    {
      "arxiv_id": "2503.05514v1",
      "title": "Noise-Robust Radio Frequency Fingerprint Identification Using Denoise Diffusion Model",
      "title_zh": "翻译失败",
      "authors": [
        "Guolin Yin",
        "Junqing Zhang",
        "Yuan Ding",
        "Simon Cotton"
      ],
      "abstract": "Securing Internet of Things (IoT) devices presents increasing challenges due\nto their limited computational and energy resources. Radio Frequency\nFingerprint Identification (RFFI) emerges as a promising authentication\ntechnique to identify wireless devices through hardware impairments. RFFI\nperformance under low signal-to-noise ratio (SNR) scenarios is significantly\ndegraded because the minute hardware features can be easily swamped in noise.\nIn this paper, we leveraged the diffusion model to effectively restore the RFF\nunder low SNR scenarios. Specifically, we trained a powerful noise predictor\nand tailored a noise removal algorithm to effectively reduce the noise level in\nthe received signal and restore the device fingerprints. We used Wi-Fi as a\ncase study and created a testbed involving 6 commercial off-the-shelf Wi-Fi\ndongles and a USRP N210 software-defined radio (SDR) platform. We conducted\nexperimental evaluations on various SNR scenarios. The experimental results\nshow that the proposed algorithm can improve the classification accuracy by up\nto 34.9%.",
      "tldr_zh": "该论文针对IoT设备的安全认证问题，提出了一种基于Denoise Diffusion Model的噪声鲁棒Radio Frequency Fingerprint Identification (RFFI)方法，以应对低SNR（signal-to-noise ratio）场景下硬件特征被噪声淹没的挑战。具体而言，该方法通过训练一个强大的noise predictor并定制noise removal算法，有效降低接收信号中的噪声并恢复设备指纹。实验以Wi-Fi为例，使用6个商用Wi-Fi dongles和USRP N210 SDR平台进行测试，结果显示该算法可以将分类准确率提高高达34.9%。这为IoT设备的可靠认证提供了新的技术基础。",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "6 pages, 8 figures, WCNC 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.05514v1",
      "published_date": "2025-03-07 15:30:55 UTC",
      "updated_date": "2025-03-07 15:30:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:10:42.828751"
    },
    {
      "arxiv_id": "2503.05507v1",
      "title": "Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?",
      "title_zh": "翻译失败",
      "authors": [
        "Qingyuan Liang",
        "Zhao Zhang",
        "Zeyu Sun",
        "Zheng Lin",
        "Qi Luo",
        "Yueyi Xiao",
        "Yizhou Chen",
        "Yuqun Zhang",
        "Haotian Zhang",
        "Lu Zhang",
        "Bin Chen",
        "Yingfei Xiong"
      ],
      "abstract": "Grammar serves as a cornerstone in programming languages and software\nengineering, providing frameworks to define the syntactic space and program\nstructure. Existing research demonstrates the effectiveness of grammar-based\ncode representations in small-scale models, showing their ability to reduce\nsyntax errors and enhance performance. However, as language models scale to the\nbillion level or beyond, syntax-level errors become rare, making it unclear\nwhether grammar information still provides performance benefits. To explore\nthis, we develop a series of billion-scale GrammarCoder models, incorporating\ngrammar rules in the code generation process. Experiments on HumanEval (+) and\nMBPP (+) demonstrate a notable improvement in code generation accuracy. Further\nanalysis shows that grammar-based representations enhance LLMs' ability to\ndiscern subtle code differences, reducing semantic errors caused by minor\nvariations. These findings suggest that grammar-based code representations\nremain valuable even in billion-scale models, not only by maintaining syntax\ncorrectness but also by improving semantic differentiation.",
      "tldr_zh": "本论文探讨了在大型语言模型（LLMs）中，基于语法的代码表示是否仍值得追求，尽管语法错误在亿级模型中已较少见。研究者开发了系列 GrammarCoder 模型，将语法规则融入代码生成过程，以提升性能。在 HumanEval (+) 和 MBPP (+) 数据集上的实验显示，该方法显著提高了代码生成准确率。进一步分析表明，GrammarCoder 增强了 LLMs 对细微代码差异的辨识能力，减少语义错误，从而证明基于语法的代码表示不仅维护语法正确性，还改善了语义区分。",
      "categories": [
        "cs.PL",
        "cs.AI"
      ],
      "primary_category": "cs.PL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05507v1",
      "published_date": "2025-03-07 15:23:13 UTC",
      "updated_date": "2025-03-07 15:23:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:10:56.211157"
    },
    {
      "arxiv_id": "2503.05500v2",
      "title": "EuroBERT: Scaling Multilingual Encoders for European Languages",
      "title_zh": "翻译失败",
      "authors": [
        "Nicolas Boizard",
        "Hippolyte Gisserot-Boukhlef",
        "Duarte M. Alves",
        "André Martins",
        "Ayoub Hammal",
        "Caio Corro",
        "Céline Hudelot",
        "Emmanuel Malherbe",
        "Etienne Malaboeuf",
        "Fanny Jourdan",
        "Gabriel Hautreux",
        "João Alves",
        "Kevin El-Haddad",
        "Manuel Faysse",
        "Maxime Peyrard",
        "Nuno M. Guerreiro",
        "Patrick Fernandes",
        "Ricardo Rei",
        "Pierre Colombo"
      ],
      "abstract": "General-purpose multilingual vector representations, used in retrieval,\nregression and classification, are traditionally obtained from bidirectional\nencoder models. Despite their wide applicability, encoders have been recently\novershadowed by advances in generative decoder-only models. However, many\ninnovations driving this progress are not inherently tied to decoders. In this\npaper, we revisit the development of multilingual encoders through the lens of\nthese advances, and introduce EuroBERT, a family of multilingual encoders\ncovering European and widely spoken global languages. Our models outperform\nexisting alternatives across a diverse range of tasks, spanning multilingual\ncapabilities, mathematics, and coding, and natively supporting sequences of up\nto 8,192 tokens. We also examine the design decisions behind EuroBERT, offering\ninsights into our dataset composition and training pipeline. We publicly\nrelease the EuroBERT models, including intermediate training checkpoints,\ntogether with our training framework.",
      "tldr_zh": "该研究重新审视多语言编码器（multilingual encoders）的开发，引入 EuroBERT 模型家族，以覆盖欧洲语言和全球广泛语言。EuroBERT 借鉴了生成式解码器（decoder-only models）的创新，优化了数据集组成和训练管道，在多语言任务、数学和编码等领域表现出色，并支持长达 8,192 标记的序列。实验结果显示，EuroBERT 超越了现有模型的性能，同时作者公开发布了模型、训练检查点和框架，以促进进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "28 pages, 8 figures, 13 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.05500v2",
      "published_date": "2025-03-07 15:13:58 UTC",
      "updated_date": "2025-03-26 18:43:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:11:06.790151"
    },
    {
      "arxiv_id": "2503.05492v1",
      "title": "FastMap: Fast Queries Initialization Based Vectorized HD Map Reconstruction Framework",
      "title_zh": "FastMap：基于快速查询初始化的矢量化高精度地图重建框架",
      "authors": [
        "Haotian Hu",
        "Jingwei Xu",
        "Fanyi Wang",
        "Toyota Li",
        "Yaonong Wang",
        "Laifeng Hu",
        "Zhiwang Zhang"
      ],
      "abstract": "Reconstruction of high-definition maps is a crucial task in perceiving the\nautonomous driving environment, as its accuracy directly impacts the\nreliability of prediction and planning capabilities in downstream modules.\nCurrent vectorized map reconstruction methods based on the DETR framework\nencounter limitations due to the redundancy in the decoder structure,\nnecessitating the stacking of six decoder layers to maintain performance, which\nsignificantly hampers computational efficiency. To tackle this issue, we\nintroduce FastMap, an innovative framework designed to reduce decoder\nredundancy in existing approaches. FastMap optimizes the decoder architecture\nby employing a single-layer, two-stage transformer that achieves multilevel\nrepresentation capabilities. Our framework eliminates the conventional practice\nof randomly initializing queries and instead incorporates a heatmap-guided\nquery generation module during the decoding phase, which effectively maps image\nfeatures into structured query vectors using learnable positional encoding.\nAdditionally, we propose a geometry-constrained point-to-line loss mechanism\nfor FastMap, which adeptly addresses the challenge of distinguishing highly\nhomogeneous features that often arise in traditional point-to-point loss\ncomputations. Extensive experiments demonstrate that FastMap achieves\nstate-of-the-art performance in both nuScenes and Argoverse2 datasets, with its\ndecoder operating 3.2 faster than the baseline. Code and more demos are\navailable at https://github.com/hht1996ok/FastMap.",
      "tldr_zh": "该论文针对自动驾驶中高精度地图（HD Map）重建的挑战，提出FastMap框架，以优化基于DETR框架的向量化地图重建方法，减少解码器冗余并提升计算效率。FastMap采用单层两阶段Transformer架构，实现多级表示能力，并引入热图引导的查询生成模块，使用可学习的定位编码将图像特征映射为结构化查询向量。论文还提出几何约束的点到线损失机制，以更好地处理高度同质特征的区分问题。实验在nuScenes和Argoverse2数据集上显示，FastMap比基线模型性能更先进，且解码器速度提升3.2倍。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05492v1",
      "published_date": "2025-03-07 15:01:55 UTC",
      "updated_date": "2025-03-07 15:01:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:11:18.654395"
    },
    {
      "arxiv_id": "2503.07657v1",
      "title": "SplitQuantV2: Enhancing Low-Bit Quantization of LLMs Without GPUs",
      "title_zh": "SplitQuantV2：无需 GPU 增强大型语言模型的低位量化",
      "authors": [
        "Jaewoo Song",
        "Fangzhen Lin"
      ],
      "abstract": "The quantization of large language models (LLMs) is crucial for deploying\nthem on devices with limited computational resources. While advanced\nquantization algorithms offer improved performance compared to the basic linear\nquantization, they typically require high-end graphics processing units (GPUs),\nare often restricted to specific deep neural network (DNN) frameworks, and\nrequire calibration datasets. This limitation poses challenges for using such\nalgorithms on various neural processing units (NPUs) and edge AI devices, which\nhave diverse model formats and frameworks. In this paper, we show SplitQuantV2,\nan innovative algorithm designed to enhance low-bit linear quantization of\nLLMs, can achieve results comparable to those of advanced algorithms.\nSplitQuantV2 preprocesses models by splitting linear and convolution layers\ninto functionally equivalent, quantization-friendly structures. The algorithm's\nplatform-agnostic, concise, and efficient nature allows for implementation\nwithout the need for GPUs. Our evaluation on the Llama 3.2 1B Instruct model\nusing the AI2's Reasoning Challenge (ARC) dataset demonstrates that\nSplitQuantV2 improves the accuracy of the INT4 quantization model by 11.76%p,\nmatching the performance of the original floating-point model. Remarkably,\nSplitQuantV2 took only 2 minutes 6 seconds to preprocess the 1B model and\nperform linear INT4 quantization using only an Apple M4 CPU. SplitQuantV2\nprovides a practical solution for low-bit quantization on LLMs, especially when\ncomplex, computation-intensive algorithms are inaccessible due to hardware\nlimitations or framework incompatibilities.",
      "tldr_zh": "该论文提出SplitQuantV2，一种创新算法，用于增强大型语言模型(LLMs)的低位量化，而无需依赖GPU、特定深度神经网络(DNN)框架或校准数据集。通过将线性层和卷积层拆分成等效的量化友好结构，SplitQuantV2实现了高效预处理和量化。实验在Llama 3.2 1B Instruct模型上使用AI2's Reasoning Challenge (ARC)数据集显示，该算法将INT4量化的准确率提高了11.76%，达到原浮点模型的性能水平，且仅用Apple M4 CPU在2分钟6秒内完成处理。该方法为资源受限设备上的LLMs量化提供了实用、可移植的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07657v1",
      "published_date": "2025-03-07 14:59:07 UTC",
      "updated_date": "2025-03-07 14:59:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:11:31.014007"
    },
    {
      "arxiv_id": "2503.05474v1",
      "title": "Personalized Federated Learning via Learning Dynamic Graphs",
      "title_zh": "翻译失败",
      "authors": [
        "Ziran Zhou",
        "Guanyu Gao",
        "Xiaohu Wu",
        "Yan Lyu"
      ],
      "abstract": "Personalized Federated Learning (PFL) aims to train a personalized model for\neach client that is tailored to its local data distribution, learning fails to\nperform well on individual clients due to variations in their local data\ndistributions. Most existing PFL methods focus on personalizing the aggregated\nglobal model for each client, neglecting the fundamental aspect of federated\nlearning: the regulation of how client models are aggregated. Additionally,\nalmost all of them overlook the graph structure formed by clients in federated\nlearning. In this paper, we propose a novel method, Personalized Federated\nLearning with Graph Attention Network (pFedGAT), which captures the latent\ngraph structure between clients and dynamically determines the importance of\nother clients for each client, enabling fine-grained control over the\naggregation process. We evaluate pFedGAT across multiple data distribution\nscenarios, comparing it with twelve state of the art methods on three datasets:\nFashion MNIST, CIFAR-10, and CIFAR-100, and find that it consistently performs\nwell.",
      "tldr_zh": "该论文针对个性化联邦学习 (PFL) 中客户端数据分布差异导致的模型性能问题，提出了一种新方法 Personalized Federated Learning with Graph Attention Network (pFedGAT)。pFedGAT 通过捕获客户端之间的潜在图结构，并利用 Graph Attention Network 动态调整其他客户端对每个客户端的重要性，实现对模型聚合过程的精细调控。与现有方法相比，它更注重图结构的作用，从而提升个性化模型的适应性。在 Fashion MNIST、CIFAR-10 和 CIFAR-100 数据集上的实验表明，pFedGAT 在多种数据分布场景中比12种最先进方法表现出色。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05474v1",
      "published_date": "2025-03-07 14:47:03 UTC",
      "updated_date": "2025-03-07 14:47:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:11:43.260188"
    },
    {
      "arxiv_id": "2503.05856v1",
      "title": "This Is Your Doge, If It Please You: Exploring Deception and Robustness in Mixture of LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Lorenz Wolf",
        "Sangwoong Yoon",
        "Ilija Bogunovic"
      ],
      "abstract": "Mixture of large language model (LLMs) Agents (MoA) architectures achieve\nstate-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by\nleveraging the collaboration of multiple LLMs at inference time. Despite these\nsuccesses, an evaluation of the safety and reliability of MoA is missing. We\npresent the first comprehensive study of MoA's robustness against deceptive LLM\nagents that deliberately provide misleading responses. We examine factors like\nthe propagation of deceptive information, model size, and information\navailability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the\npopular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of\n49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate\nthat introducing only a $\\textit{single}$ carefully-instructed deceptive agent\ninto the MoA can reduce performance to 37.9%, effectively nullifying all MoA\ngains. On QuALITY, a multiple-choice comprehension task, the impact is also\nsevere, with accuracy plummeting by a staggering 48.5%. Inspired in part by the\nhistorical Doge of Venice voting process, designed to minimize influence and\ndeception, we propose a range of unsupervised defense mechanisms that recover\nmost of the lost performance.",
      "tldr_zh": "该论文探讨了Mixture of LLMs Agents (MoA)架构的鲁棒性和安全性，评估其在面对故意提供误导性响应的欺骗性LLM代理时的表现。研究发现，引入一个精心设计的欺骗代理可显著降低性能，例如在AlpacaEval 2.0基准上，LLaMA 3.1-70B模型的Length-Controlled Win Rate从49.2%降至37.9%，而在QuALITY多选任务中准确率暴跌48.5%。为应对这些漏洞，论文提出受威尼斯Doge投票过程启发的无监督防御机制，能够有效恢复大部分性能损失。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "35 pages, 9 figures, 16 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.05856v1",
      "published_date": "2025-03-07 14:46:39 UTC",
      "updated_date": "2025-03-07 14:46:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:11:55.550299"
    },
    {
      "arxiv_id": "2503.05473v2",
      "title": "The Society of HiveMind: Multi-Agent Optimization of Foundation Model Swarms to Unlock the Potential of Collective Intelligence",
      "title_zh": "HiveMind 社会：多智能体优化基础模型群",
      "authors": [
        "Noah Mamie",
        "Susie Xi Rao"
      ],
      "abstract": "Multi-agent systems address issues of accessibility and scalability of\nartificial intelligence (AI) foundation models, which are often represented by\nlarge language models. We develop a framework - the \"Society of HiveMind\"\n(SOHM) - that orchestrates the interaction between multiple AI foundation\nmodels, imitating the observed behavior of animal swarms in nature by following\nmodern evolutionary theories. On the one hand, we find that the SOHM provides a\nnegligible benefit on tasks that mainly require real-world knowledge. On the\nother hand, we remark a significant improvement on tasks that require intensive\nlogical reasoning, indicating that multi-agent systems are capable of\nincreasing the reasoning capabilities of the collective compared to the\nindividual agents. Our findings demonstrate the potential of combining a\nmultitude of diverse AI foundation models to form an artificial swarm\nintelligence capable of self-improvement through interactions with a given\nenvironment.",
      "tldr_zh": "这篇论文提出了 Society of HiveMind (SOHM) 框架，通过协调多个 AI foundation models 的互动，模仿自然动物群的行为并遵循现代进化理论，优化多-agent 系统以释放集体智能的潜力。研究发现，在主要依赖真实世界知识的任务上，SOHM 的益处微不足道，但在需要密集逻辑推理的任务上，显著提升了集体的推理能力。总体而言，这些结果证明了多 agent 系统能通过多样模型的结合，实现人工群智能的自我改进。",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "11 pages (excl. appendix)",
      "pdf_url": "http://arxiv.org/pdf/2503.05473v2",
      "published_date": "2025-03-07 14:45:03 UTC",
      "updated_date": "2025-03-13 14:20:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:12:07.045149"
    },
    {
      "arxiv_id": "2503.05455v1",
      "title": "Controllable Complementarity: Subjective Preferences in Human-AI Collaboration",
      "title_zh": "可控互补性：人-AI 协作中的主观偏好",
      "authors": [
        "Chase McDonald",
        "Cleotilde Gonzalez"
      ],
      "abstract": "Research on human-AI collaboration often prioritizes objective performance.\nHowever, understanding human subjective preferences is essential to improving\nhuman-AI complementarity and human experiences. We investigate human\npreferences for controllability in a shared workspace task with AI partners\nusing Behavior Shaping (BS), a reinforcement learning algorithm that allows\nhumans explicit control over AI behavior.\n  In one experiment, we validate the robustness of BS in producing effective AI\npolicies relative to self-play policies, when controls are hidden. In another\nexperiment, we enable human control, showing that participants perceive AI\npartners as more effective and enjoyable when they can directly dictate AI\nbehavior. Our findings highlight the need to design AI that prioritizes both\ntask performance and subjective human preferences. By aligning AI behavior with\nhuman preferences, we demonstrate how human-AI complementarity can extend\nbeyond objective outcomes to include subjective preferences.",
      "tldr_zh": "该研究强调，在人类-AI 协作中，主观偏好对提升互补性和用户体验的重要性，超越了传统对客观性能的关注。研究者使用 Behavior Shaping (BS) 算法——一种强化学习方法——在共享工作空间任务中，让人类能够明确控制 AI 行为。实验结果显示，当控制被启用时，参与者认为 AI 伙伴更有效且更愉快，从而证明设计 AI 时需同时考虑任务性能和主观偏好，以实现更全面的人类-AI 互补性。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.HC",
      "comment": "9 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05455v1",
      "published_date": "2025-03-07 14:27:48 UTC",
      "updated_date": "2025-03-07 14:27:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:12:19.853677"
    },
    {
      "arxiv_id": "2503.05453v1",
      "title": "Soft Policy Optimization: Online Off-Policy RL for Sequence Models",
      "title_zh": "软策略优化：针对序列模型的在线",
      "authors": [
        "Taco Cohen",
        "David W. Zhang",
        "Kunhao Zheng",
        "Yunhao Tang",
        "Remi Munos",
        "Gabriel Synnaeve"
      ],
      "abstract": "RL-based post-training of language models is almost exclusively done using\non-policy methods such as PPO. These methods cannot learn from arbitrary\nsequences such as those produced earlier in training, in earlier runs, by human\nexperts or other policies, or by decoding and exploration methods. This results\nin severe sample inefficiency and exploration difficulties, as well as a\npotential loss of diversity in the policy responses. Moreover, asynchronous PPO\nimplementations require frequent and costly model transfers, and typically use\nvalue models which require a large amount of memory. In this paper we introduce\nSoft Policy Optimization (SPO), a simple, scalable and principled Soft RL\nmethod for sequence model policies that can learn from arbitrary online and\noffline trajectories and does not require a separate value model. In\nexperiments on code contests, we shows that SPO outperforms PPO on pass@10, is\nsignificantly faster and more memory efficient, is able to benefit from\noff-policy data, enjoys improved stability, and learns more diverse (i.e. soft)\npolicies.",
      "tldr_zh": "该研究指出，现有的强化学习（RL）后训练方法，如 on-policy 的 PPO，无法从任意序列（如早期训练数据或外部轨迹）中学习，导致样本效率低下、探索困难和策略多样性缺失。为解决这些问题，作者提出 Soft Policy Optimization (SPO)，一种简单、可扩展的 Soft RL 方法，支持在线和离线轨迹学习，且无需单独的价值模型。实验结果显示，在代码竞赛任务上，SPO 比 PPO 在 pass@10 指标上表现更优、更快、更省内存，并提升了稳定性及策略的多样性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05453v1",
      "published_date": "2025-03-07 14:23:40 UTC",
      "updated_date": "2025-03-07 14:23:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:12:30.912824"
    },
    {
      "arxiv_id": "2503.05449v1",
      "title": "LLM-based Iterative Approach to Metamodeling in Automotive",
      "title_zh": "翻译失败",
      "authors": [
        "Nenad Petrovic",
        "Fengjunjie Pan",
        "Vahid Zolfaghari",
        "Alois Knoll"
      ],
      "abstract": "In this paper, we introduce an automated approach to domain-specific\nmetamodel construction relying on Large Language Model (LLM). The main focus is\nadoption in automotive domain. As outcome, a prototype was implemented as web\nservice using Python programming language, while OpenAI's GPT-4o was used as\nthe underlying LLM. Based on the initial experiments, this approach\nsuccessfully constructs Ecore metamodel based on set of automotive requirements\nand visualizes it making use of PlantUML notation, so human experts can provide\nfeedback in order to refine the result. Finally, locally deployable solution is\nalso considered, including the limitations and additional steps required.",
      "tldr_zh": "本研究提出了一种基于大型语言模型（LLM）的迭代方法，用于汽车领域的特定元模型构建，旨在自动化处理需求转化为元模型的过程。使用 Python 实现的 web 服务原型，以 OpenAI 的 GPT-4o 作为底层 LLM，从汽车需求生成 Ecore 元模型，并通过 PlantUML 进行可视化，允许人类专家提供反馈以优化结果。初步实验证明，该方法成功构建并可视化元模型，提高了效率；此外，论文讨论了本地部署的可行性，包括潜在限制和所需额外步骤。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05449v1",
      "published_date": "2025-03-07 14:19:17 UTC",
      "updated_date": "2025-03-07 14:19:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:12:42.509796"
    },
    {
      "arxiv_id": "2503.05447v2",
      "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
      "title_zh": "翻译失败",
      "authors": [
        "Weigao Sun",
        "Disen Lan",
        "Tong Zhu",
        "Xiaoye Qu",
        "Yu Cheng"
      ],
      "abstract": "Linear Sequence Modeling (LSM) like linear attention, state space models and\nlinear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant\narchitectural improvements. In this paper, we introduce Linear-MoE, a\nproduction-level system for modeling and training large-scale models that\nintegrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules\nfor linear-complexity sequence modeling and MoE layers for sparsely activation,\naiming to offer high performance with efficient training. The Linear-MoE system\ncomprises: 1) Modeling subsystem, which provides a unified framework supporting\nall instances of LSM. and 2) Training subsystem, which facilitates efficient\ntraining by incorporating various advanced parallelism technologies,\nparticularly Sequence Parallelism designed for Linear-MoE models. Additionally,\nwe explore hybrid models that combine Linear-MoE layers with standard\nTransformer-MoE layers with its Sequence Parallelism to further enhance model\nflexibility and performance. Evaluations on two model series, A0.3B-2B and\nA1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining\ncompetitive performance on various benchmarks, showcasing its potential as a\nnext-generation foundational model architecture. Code:\nhttps://github.com/OpenSparseLLMs/Linear-MoE.",
      "tldr_zh": "这篇论文引入了 Linear-MoE 系统，将 Linear Sequence Modeling (LSM) 与 Mixture-of-Experts (MoE) 相结合，实现线性复杂度序列建模和稀疏激活，以提升模型性能和训练效率。系统包括建模子系统（提供支持所有 LSM 实例的统一框架）和训练子系统（整合 Sequence Parallelism 等高级并行技术）。此外，论文探索了混合模型，将 Linear-MoE 层与标准 Transformer-MoE 层结合，以进一步增强模型灵活性。在 A0.3B-2B 和 A1B-7B 模型系列的基准测试中，Linear-MoE 实现了效率提升，同时保持了竞争性能，展示了其作为下一代基础模型架构的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Technical report, 17 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.05447v2",
      "published_date": "2025-03-07 14:17:45 UTC",
      "updated_date": "2025-04-15 07:51:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:12:56.458434"
    },
    {
      "arxiv_id": "2503.05439v2",
      "title": "An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for Robust Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Navdeep Kaur",
        "Lachlan McPheat",
        "Alessandra Russo",
        "Anthony G Cohn",
        "Pranava Madhyastha"
      ],
      "abstract": "In this paper, we examine the use of Conformal Language Modelling (CLM)\nalongside Answer Set Programming (ASP) to enhance the performance of standard\nopen-weight LLMs on complex multi-step reasoning tasks. Using the StepGame\ndataset, which requires spatial reasoning, we apply CLM to generate sets of ASP\nprograms from an LLM, providing statistical guarantees on the correctness of\nthe outputs. Experimental results show that CLM significantly outperforms\nbaseline models that use standard sampling methods, achieving substantial\naccuracy improvements across different levels of reasoning complexity.\nAdditionally, the LLM-as-Judge metric enhances CLM's performance, especially in\nassessing structurally and logically correct ASP outputs. However, calibrating\nCLM with diverse calibration sets did not improve generalizability for tasks\nrequiring much longer reasoning steps, indicating limitations in handling more\ncomplex tasks.",
      "tldr_zh": "本文通过实证研究，探讨了使用 Conformal Language Modelling (CLM) 与 Answer Set Programming (ASP) 相结合的方法，来提升标准开源 LLM 在复杂多步推理任务中的性能。研究利用 StepGame 数据集进行实验，CLM 生成 ASP 程序并提供输出正确性的统计保证，结果显示 CLM 显著优于基于标准采样的基线模型，在不同推理复杂度水平上实现了实质性准确率提升。LLM-as-Judge 指标进一步增强了 CLM 在评估结构和逻辑正确的 ASP 输出方面的表现，但使用多样化校准集未能改善其在更长推理步骤任务上的泛化能力，揭示了处理更复杂任务的局限性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05439v2",
      "published_date": "2025-03-07 14:10:10 UTC",
      "updated_date": "2025-04-11 15:33:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:13:07.628223"
    },
    {
      "arxiv_id": "2503.05423v4",
      "title": "Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning",
      "title_zh": "通过",
      "authors": [
        "Run He",
        "Di Fang",
        "Yicheng Xu",
        "Yawen Cui",
        "Ming Li",
        "Cen Chen",
        "Ziqian Zeng",
        "Huiping Zhuang"
      ],
      "abstract": "Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn\nfrom distinct categories without retaining exemplars but easily suffers from\ncatastrophic forgetting of learned knowledge. While existing EFCIL methods\nleverage knowledge distillation to alleviate forgetting, they still face two\ncritical challenges: semantic shift and decision bias. Specifically, the\nembeddings of old tasks shift in the embedding space after learning new tasks,\nand the classifier becomes biased towards new tasks due to training solely with\nnew data, hindering the balance between old and new knowledge. To address these\nissues, we propose the Dual-Projection Shift Estimation and Classifier\nReconstruction (DPCR) approach for EFCIL. DPCR effectively estimates semantic\nshift through a dual-projection, which combines a learnable transformation with\na row-space projection to capture both task-wise and category-wise shifts.\nFurthermore, to mitigate decision bias, DPCR employs ridge regression to\nreformulate a classifier reconstruction process. This reconstruction exploits\nprevious in covariance and prototype of each class after calibration with\nestimated shift, thereby reducing decision bias. Extensive experiments\ndemonstrate that, on various datasets, DPCR effectively balances old and new\ntasks, outperforming state-of-the-art EFCIL methods. Our codes are available at\nhttps://github.com/RHe502/ICML25-DPCR.",
      "tldr_zh": "本论文针对 Exemplar-Free Class-Incremental Learning (EFCIL) 的挑战，提出 Dual-Projection Shift Estimation and Classifier Reconstruction (DPCR) 方法，以缓解 catastrophic forgetting 问题，特别是 semantic shift 和 decision bias 的影响。DPCR 通过双投影技术结合可学习变换和行空间投影，精确估计任务级和类别级的语义偏移；同时，利用岭回归重构分类器，并基于校准后的类别协方差和原型，减少决策偏差。实验结果显示，在多种数据集上，DPCR 显著优于现有方法，在旧任务和新任务间实现了更好的平衡。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by ICML 2025; Camera ready version",
      "pdf_url": "http://arxiv.org/pdf/2503.05423v4",
      "published_date": "2025-03-07 13:50:29 UTC",
      "updated_date": "2025-05-18 08:38:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:13:19.005285"
    },
    {
      "arxiv_id": "2503.05854v1",
      "title": "Accelerating Earth Science Discovery via Multi-Agent LLM Systems",
      "title_zh": "通过多智能体大语言模型系统加速地球科学发现",
      "authors": [
        "Dmitrii Pantiukhin",
        "Boris Shapkin",
        "Ivan Kuznetsov",
        "Antonia Anna Jost",
        "Nikolay Koldunov"
      ],
      "abstract": "This Perspective explores the transformative potential of Multi-Agent Systems\n(MAS) powered by Large Language Models (LLMs) in the geosciences. Users of\ngeoscientific data repositories face challenges due to the complexity and\ndiversity of data formats, inconsistent metadata practices, and a considerable\nnumber of unprocessed datasets. MAS possesses transformative potential for\nimproving scientists' interaction with geoscientific data by enabling\nintelligent data processing, natural language interfaces, and collaborative\nproblem-solving capabilities. We illustrate this approach with \"PANGAEA GPT\", a\nspecialized MAS pipeline integrated with the diverse PANGAEA database for Earth\nand Environmental Science, demonstrating how MAS-driven workflows can\neffectively manage complex datasets and accelerate scientific discovery. We\ndiscuss how MAS can address current data challenges in geosciences, highlight\nadvancements in other scientific fields, and propose future directions for\nintegrating MAS into geoscientific data processing pipelines. In this\nPerspective, we show how MAS can fundamentally improve data accessibility,\npromote cross-disciplinary collaboration, and accelerate geoscientific\ndiscoveries.",
      "tldr_zh": "这篇论文探讨了由大型语言模型 (LLMs) 驱动的多智能体系统 (MAS) 在地球科学领域的潜力，通过智能数据处理、自然语言接口和协作问题解决来应对数据格式复杂、元数据不一致以及未处理数据集的挑战。作者以“PANGAEA GPT”为例，展示了一个与PANGAEA数据库集成的MAS管道，能够有效管理复杂数据集并加速科学发现。最终，该研究强调MAS可提升数据可访问性，促进跨学科合作，并为未来地球科学数据处理管道的整合提供方向。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "I.2.11"
      ],
      "primary_category": "cs.MA",
      "comment": "10 pages, 1 figure. Perspective article",
      "pdf_url": "http://arxiv.org/pdf/2503.05854v1",
      "published_date": "2025-03-07 13:25:56 UTC",
      "updated_date": "2025-03-07 13:25:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:13:30.590102"
    },
    {
      "arxiv_id": "2503.05394v1",
      "title": "Static Program Analysis Guided LLM Based Unit Test Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Sujoy Roychowdhury",
        "Giriprasad Sridhara",
        "A K Raghavan",
        "Joy Bose",
        "Sourav Mazumdar",
        "Hamender Singh",
        "Srinivasan Bajji Sugumaran",
        "Ricardo Britto"
      ],
      "abstract": "We describe a novel approach to automating unit test generation for Java\nmethods using large language models (LLMs). Existing LLM-based approaches rely\non sample usage(s) of the method to test (focal method) and/or provide the\nentire class of the focal method as input prompt and context. The former\napproach is often not viable due to the lack of sample usages, especially for\nnewly written focal methods. The latter approach does not scale well enough;\nthe bigger the complexity of the focal method and larger associated class, the\nharder it is to produce adequate test code (due to factors such as exceeding\nthe prompt and context lengths of the underlying LLM). We show that augmenting\nprompts with \\emph{concise} and \\emph{precise} context information obtained by\nprogram analysis %of the focal method increases the effectiveness of generating\nunit test code through LLMs. We validate our approach on a large commercial\nJava project and a popular open-source Java project.",
      "tldr_zh": "这篇论文提出了一种新型方法，利用静态程序分析（Static Program Analysis）指导大型语言模型（LLMs）自动生成 Java 方法的单元测试代码，以解决现有方法依赖样本使用或整个类输入的局限性。创新点在于，通过程序分析提取简洁精确的上下文信息增强提示，从而提高 LLM 生成测试代码的效率和准确性。实验在大型商业 Java 项目和流行开源 Java 项目上验证了该方法的有效性，避免了提示长度限制和样本缺失问题。总的来说，此方法提升了单元测试生成的实用性，为软件开发过程提供了更可靠的自动化工具。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05394v1",
      "published_date": "2025-03-07 13:09:37 UTC",
      "updated_date": "2025-03-07 13:09:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:13:42.197995"
    },
    {
      "arxiv_id": "2503.05388v1",
      "title": "Ontology Generation using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Anna Sofia Lippolis",
        "Mohammad Javad Saeedizade",
        "Robin Keskisärkkä",
        "Sara Zuppiroli",
        "Miguel Ceriani",
        "Aldo Gangemi",
        "Eva Blomqvist",
        "Andrea Giovanni Nuzzolese"
      ],
      "abstract": "The ontology engineering process is complex, time-consuming, and error-prone,\neven for experienced ontology engineers. In this work, we investigate the\npotential of Large Language Models (LLMs) to provide effective OWL ontology\ndrafts directly from ontological requirements described using user stories and\ncompetency questions. Our main contribution is the presentation and evaluation\nof two new prompting techniques for automated ontology development: Memoryless\nCQbyCQ and Ontogenia. We also emphasize the importance of three structural\ncriteria for ontology assessment, alongside expert qualitative evaluation,\nhighlighting the need for a multi-dimensional evaluation in order to capture\nthe quality and usability of the generated ontologies. Our experiments,\nconducted on a benchmark dataset of ten ontologies with 100 distinct CQs and 29\ndifferent user stories, compare the performance of three LLMs using the two\nprompting techniques. The results demonstrate improvements over the current\nstate-of-the-art in LLM-supported ontology engineering. More specifically, the\nmodel OpenAI o1-preview with Ontogenia produces ontologies of sufficient\nquality to meet the requirements of ontology engineers, significantly\noutperforming novice ontology engineers in modelling ability. However, we still\nnote some common mistakes and variability of result quality, which is important\nto take into account when using LLMs for ontology authoring support. We discuss\nthese limitations and propose directions for future research.",
      "tldr_zh": "本研究探讨使用大型语言模型 (LLMs) 从用户故事和能力问题 (CQs) 生成 OWL 本体草稿，以简化复杂且易出错的本体工程过程，并提出两种新提示技术：Memoryless CQbyCQ 和 Ontogenia，以提升自动化生成效率。\n实验在包含10个本体的基准数据集上评估三种LLMs的表现，结果显示Ontogenia结合OpenAI o1-preview生成的本体质量显著优于新手工程师，实现了当前LLM支持本体工程的最新改进。\n然而，生成的本体仍存在常见错误和质量变异，研究强调了多维度评估（如结构标准和专家定性评估）的必要性，并讨论了这些限制及未来研究方向。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "2 figures and 3 tables. 20 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.05388v1",
      "published_date": "2025-03-07 13:03:28 UTC",
      "updated_date": "2025-03-07 13:03:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:13:55.972239"
    },
    {
      "arxiv_id": "2503.05383v5",
      "title": "AVA: Attentive VLM Agent for Mastering StarCraft II",
      "title_zh": "翻译失败",
      "authors": [
        "Weiyu Ma",
        "Yuqian Fu",
        "Zecheng Zhang",
        "Bernard Ghanem",
        "Guohao Li"
      ],
      "abstract": "We introduce Attentive VLM Agent (AVA), a multimodal StarCraft II agent that\naligns artificial agent perception with the human gameplay experience.\nTraditional frameworks such as SMAC rely on abstract state representations that\ndiverge significantly from human perception, limiting the ecological validity\nof agent behavior. Our agent addresses this limitation by incorporating RGB\nvisual inputs and natural language observations that more closely simulate\nhuman cognitive processes during gameplay. The AVA architecture consists of\nthree integrated components: (1) a vision-language model enhanced with\nspecialized self-attention mechanisms for strategic unit targeting and\nbattlefield assessment, (2) a retrieval-augmented generation system that\nleverages domain-specific StarCraft II knowledge to inform tactical decisions,\nand (3) a dynamic role-based task distribution system that enables coordinated\nmulti-agent behavior. The experimental evaluation in our proposed AVACraft\nenvironment, which contains 21 multimodal StarCraft II scenarios, demonstrates\nthat AVA powered by foundation models (specifically Qwen-VL and GPT-4o) can\nexecute complex tactical maneuvers without explicit training, achieving\ncomparable performance to traditional MARL methods that require substantial\ntraining iterations. This work establishes a foundation for developing\nhuman-aligned StarCraft II agents and advances the broader research agenda of\nmultimodal game AI. Our implementation is available at\nhttps://github.com/camel-ai/VLM-Play-StarCraft2.",
      "tldr_zh": "我们引入了 Attentive VLM Agent (AVA)，一个多模态代理，用于 StarCraft II 游戏，通过 RGB 视觉输入和自然语言观察，使代理感知更接近人类游戏体验。AVA 的架构包括三个组件：增强的自注意力机制的视觉语言模型（用于战略单位目标和战场评估）、检索增强生成 (RAG) 系统（利用 StarCraft II 特定知识辅助决策），以及动态基于角色的任务分配系统（支持多代理协调行为）。在 AVACraft 环境中的21个多模态场景实验中，AVA 基于基础模型如 Qwen-VL 和 GPT-4o 即可执行复杂战术，与传统 MARL 方法性能相当，但无需大量训练迭代。这为开发人类对齐的多模态游戏 AI 奠定了基础。",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "Under Review",
      "pdf_url": "http://arxiv.org/pdf/2503.05383v5",
      "published_date": "2025-03-07 12:54:25 UTC",
      "updated_date": "2025-05-16 05:21:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:14:08.302500"
    },
    {
      "arxiv_id": "2503.05371v1",
      "title": "Shifting Perspectives: Steering Vector Ensembles for Robust Bias Mitigation in LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Zara Siddique",
        "Irtaza Khalid",
        "Liam D. Turner",
        "Luis Espinosa-Anke"
      ],
      "abstract": "We present a novel approach to bias mitigation in large language models\n(LLMs) by applying steering vectors to modify model activations in forward\npasses. We employ Bayesian optimization to systematically identify effective\ncontrastive pair datasets across nine bias axes. When optimized on the BBQ\ndataset, our individually tuned steering vectors achieve average improvements\nof 12.2%, 4.7%, and 3.2% over the baseline for Mistral, Llama, and Qwen,\nrespectively. Building on these promising results, we introduce Steering Vector\nEnsembles (SVE), a method that averages multiple individually optimized\nsteering vectors, each targeting a specific bias axis such as age, race, or\ngender. By leveraging their collective strength, SVE outperforms individual\nsteering vectors in both bias reduction and maintaining model performance. The\nwork presents the first systematic investigation of steering vectors for bias\nmitigation, and we demonstrate that SVE is a powerful and computationally\nefficient strategy for reducing bias in LLMs, with broader implications for\nenhancing AI safety.",
      "tldr_zh": "本文提出了一种新方法，使用 steering vectors 修改大型语言模型(LLMs)的激活，以有效缓解偏见问题，并通过 Bayesian optimization 系统识别九个偏见轴上的 contrastive pair 数据集。在 BBQ 数据集上优化后，该方法使 Mistral、Llama 和 Qwen 模型分别比基线提升 12.2%、4.7% 和 3.2%。此外，作者引入 Steering Vector Ensembles (SVE)，一种平均多个针对特定偏见轴（如年龄、种族或性别）的优化 vectors 的策略，SVE 不仅在偏见减少方面优于单个 steering vectors，还能维持模型性能。该研究首次系统性探讨 steering vectors 在偏见缓解中的应用，并证明其作为高效策略对 AI 安全具有更广泛的影响。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to ACL 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.05371v1",
      "published_date": "2025-03-07 12:25:29 UTC",
      "updated_date": "2025-03-07 12:25:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:14:20.327790"
    },
    {
      "arxiv_id": "2503.05357v1",
      "title": "Improving Hate Speech Classification with Cross-Taxonomy Dataset Integration",
      "title_zh": "翻译失败",
      "authors": [
        "Jan Fillies",
        "Adrian Paschke"
      ],
      "abstract": "Algorithmic hate speech detection faces significant challenges due to the\ndiverse definitions and datasets used in research and practice. Social media\nplatforms, legal frameworks, and institutions each apply distinct yet\noverlapping definitions, complicating classification efforts. This study\naddresses these challenges by demonstrating that existing datasets and\ntaxonomies can be integrated into a unified model, enhancing prediction\nperformance and reducing reliance on multiple specialized classifiers. The work\nintroduces a universal taxonomy and a hate speech classifier capable of\ndetecting a wide range of definitions within a single framework. Our approach\nis validated by combining two widely used but differently annotated datasets,\nshowing improved classification performance on an independent test set. This\nwork highlights the potential of dataset and taxonomy integration in advancing\nhate speech detection, increasing efficiency, and ensuring broader\napplicability across contexts.",
      "tldr_zh": "该研究针对仇恨言论检测（hate speech detection）面临的挑战，如定义多样性和数据集不一致问题，提出了一种跨分类学数据集整合（Cross-Taxonomy Dataset Integration）方法，以创建一个统一的模型。研究引入了通用分类学（universal taxonomy）和一个能检测多种定义的仇恨言论分类器，通过整合两个不同注解的数据集进行验证。结果显示，该方法在独立测试集上提升了分类性能，提高了效率，并增强了在不同上下文中的广泛适用性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted for publication at LaTeCH-CLfL 2025. The 9th Joint ACL\n  Special Interest Group on Language Technologies for the Socio-Economic\n  Sciences and Humanities (SIGHUM) Workshop on Computational Linguistics for\n  Cultural Heritage, Social Sciences, Humanities and Literature",
      "pdf_url": "http://arxiv.org/pdf/2503.05357v1",
      "published_date": "2025-03-07 12:01:02 UTC",
      "updated_date": "2025-03-07 12:01:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:14:31.418186"
    },
    {
      "arxiv_id": "2503.05852v1",
      "title": "Evaluating Large Language Models in Code Generation: INFINITE Methodology for Defining the Inference Index",
      "title_zh": "翻译失败",
      "authors": [
        "Nicholas Christakis",
        "Dimitris Drikakis"
      ],
      "abstract": "This study introduces a new methodology for an Inference Index (InI), called\nINFerence INdex In Testing model Effectiveness methodology (INFINITE), aiming\nto evaluate the performance of Large Language Models (LLMs) in code generation\ntasks. The InI index provides a comprehensive assessment focusing on three key\ncomponents: efficiency, consistency, and accuracy. This approach encapsulates\ntime-based efficiency, response quality, and the stability of model outputs,\noffering a thorough understanding of LLM performance beyond traditional\naccuracy metrics. We applied this methodology to compare OpenAI's GPT-4o (GPT),\nOpenAI-o1 pro (OAI1), and OpenAI-o3 mini-high (OAI3) in generating Python code\nfor the Long-Short-Term-Memory (LSTM) model to forecast meteorological\nvariables such as temperature, relative humidity and wind velocity. Our\nfindings demonstrate that GPT outperforms OAI1 and performs comparably to OAI3\nregarding accuracy and workflow efficiency. The study reveals that LLM-assisted\ncode generation can produce results similar to expert-designed models with\neffective prompting and refinement. GPT's performance advantage highlights the\nbenefits of widespread use and user feedback.",
      "tldr_zh": "本研究引入了INFINITE methodology，一种用于定义Inference Index (InI)的评估框架，旨在全面评估Large Language Models (LLMs)在代码生成任务中的性能，包括效率、一致性和准确性三个关键组件。该方法超越传统准确性指标，通过整合时间效率、响应质量和输出稳定性，提供对LLMs表现的深入洞察。在实验中，研究者将INFINITE应用于比较OpenAI的GPT-4o、OpenAI-o1 pro和OpenAI-o3 mini-high在生成Python LSTM代码（用于预测气象变量如温度、相对湿度和风速）方面的表现，结果显示GPT-4o在准确性和工作流效率上优于OpenAI-o1 pro，并与OpenAI-o3 mini-high相当。该发现强调了通过有效提示和优化，LLMs辅助代码生成可媲美专家设计模型，并凸显了GPT-4o的性能优势源于其广泛使用和用户反馈。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "20 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05852v1",
      "published_date": "2025-03-07 11:59:44 UTC",
      "updated_date": "2025-03-07 11:59:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:14:43.927917"
    },
    {
      "arxiv_id": "2503.05355v1",
      "title": "On the Logical Content of Logic Programs",
      "title_zh": "论逻辑程序的逻辑内容",
      "authors": [
        "Alexader V. Gheorghiu"
      ],
      "abstract": "Logic programming (LP) is typically understood through operational semantics\n(e.g., SLD-resolution) or model-theoretic interpretations (e.g., the least\nHerbrand model). This paper introduces a novel perspective on LP by defining a\n``support'' relation that explicates what a program ``knows''. This\ninterpretation is shown to express classical and intuitionistic logic, as well\nas an intermediate logic, depending on certain choices regarding LP and the\nmeanings of disjunction and negation. These results are formalized using the\nidea of base-extension semantics within proof-theoretic semantics. Our approach\noffers new insights into the logical foundations of LP and has potential\napplications in knowledge representation, automated reasoning, and formal\nverification.",
      "tldr_zh": "这篇论文从一个新视角审视 Logic Programs，通过引入“support”关系来阐明程序的逻辑内容，即程序“知道”什么。作者证明，这种关系可以根据对 LP、disjunction 和 negation 的选择，表达 classical logic、直觉主义逻辑(intuitionistic logic)以及一个中间逻辑，并使用 proof-theoretic semantics 中的 base-extension semantics 进行形式化。该方法为 Logic Programs 的逻辑基础提供了新见解，并具有潜在应用在知识表示(knowledge representation)、automated reasoning 和 formal verification 中。",
      "categories": [
        "cs.LO",
        "cs.AI"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05355v1",
      "published_date": "2025-03-07 11:58:08 UTC",
      "updated_date": "2025-03-07 11:58:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:14:54.850517"
    },
    {
      "arxiv_id": "2503.05349v1",
      "title": "Spatial Distillation based Distribution Alignment (SDDA) for Cross-Headset EEG Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Dingkun Liu",
        "Siyang Li",
        "Ziwei Wang",
        "Wei Li",
        "Dongrui Wu"
      ],
      "abstract": "A non-invasive brain-computer interface (BCI) enables direct interaction\nbetween the user and external devices, typically via electroencephalogram (EEG)\nsignals. However, decoding EEG signals across different headsets remains a\nsignificant challenge due to differences in the number and locations of the\nelectrodes. To address this challenge, we propose a spatial distillation based\ndistribution alignment (SDDA) approach for heterogeneous cross-headset transfer\nin non-invasive BCIs. SDDA uses first spatial distillation to make use of the\nfull set of electrodes, and then input/feature/output space distribution\nalignments to cope with the significant differences between the source and\ntarget domains. To our knowledge, this is the first work to use knowledge\ndistillation in cross-headset transfers. Extensive experiments on six EEG\ndatasets from two BCI paradigms demonstrated that SDDA achieved superior\nperformance in both offline unsupervised domain adaptation and online\nsupervised domain adaptation scenarios, consistently outperforming 10 classical\nand state-of-the-art transfer learning algorithms.",
      "tldr_zh": "该研究针对非侵入性脑机接口(BCI)中不同头戴设备间的EEG信号解码挑战，提出了一种Spatial Distillation based Distribution Alignment (SDDA)方法，用于异构跨头戴设备的转移学习。SDDA首先通过spatial distillation利用完整的电极集，然后通过输入/特征/输出空间distribution alignment来处理源域和目标域之间的显著差异，这是首次在跨头戴转移中使用knowledge distillation技术。在六个EEG数据集上的广泛实验中，SDDA在离线无监督域适应和在线监督域适应场景中均表现出色，优于10个经典和最先进的学习算法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "10 pages, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05349v1",
      "published_date": "2025-03-07 11:44:49 UTC",
      "updated_date": "2025-03-07 11:44:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:15:07.431960"
    },
    {
      "arxiv_id": "2503.05346v1",
      "title": "AutoIOT: LLM-Driven Automated Natural Language Programming for AIoT Applications",
      "title_zh": "翻译失败",
      "authors": [
        "Leming Shen",
        "Qiang Yang",
        "Yuanqing Zheng",
        "Mo Li"
      ],
      "abstract": "The advent of Large Language Models (LLMs) has profoundly transformed our\nlives, revolutionizing interactions with AI and lowering the barrier to AI\nusage. While LLMs are primarily designed for natural language interaction, the\nextensive embedded knowledge empowers them to comprehend digital sensor data.\nThis capability enables LLMs to engage with the physical world through IoT\nsensors and actuators, performing a myriad of AIoT tasks. Consequently, this\nevolution triggers a paradigm shift in conventional AIoT application\ndevelopment, democratizing its accessibility to all by facilitating the design\nand development of AIoT applications via natural language. However, some\nlimitations need to be addressed to unlock the full potential of LLMs in AIoT\napplication development. First, existing solutions often require transferring\nraw sensor data to LLM servers, which raises privacy concerns, incurs high\nquery fees, and is limited by token size. Moreover, the reasoning processes of\nLLMs are opaque to users, making it difficult to verify the robustness and\ncorrectness of inference results. This paper introduces AutoIOT, an LLM-based\nautomated program generator for AIoT applications. AutoIOT enables users to\nspecify their requirements using natural language (input) and automatically\nsynthesizes interpretable programs with documentation (output). AutoIOT\nautomates the iterative optimization to enhance the quality of generated code\nwith minimum user involvement. AutoIOT not only makes the execution of AIoT\ntasks more explainable but also mitigates privacy concerns and reduces token\ncosts with local execution of synthesized programs. Extensive experiments and\nuser studies demonstrate AutoIOT's remarkable capability in program synthesis\nfor various AIoT tasks. The synthesized programs can match and even outperform\nsome representative baselines.",
      "tldr_zh": "这篇论文介绍了 AutoIOT，一种由 Large Language Models (LLMs) 驱动的自动自然语言编程系统，旨在简化 AIoT 应用的开发，让用户通过自然语言指定需求即可生成可解释的程序代码。AutoIOT 通过迭代优化和本地执行机制，解决了传统方法中的隐私风险、高查询费用和推理不透明问题，确保程序的高质量和可靠性。实验结果和用户研究表明，AutoIOT 在各种 AIoT 任务中表现出色，生成的程序性能可与或超过基准模型，为 AIoT 开发提供了更民主化和高效的途径。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05346v1",
      "published_date": "2025-03-07 11:40:52 UTC",
      "updated_date": "2025-03-07 11:40:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:15:19.150305"
    },
    {
      "arxiv_id": "2503.05336v3",
      "title": "Toward an Evaluation Science for Generative AI Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Laura Weidinger",
        "Inioluwa Deborah Raji",
        "Hanna Wallach",
        "Margaret Mitchell",
        "Angelina Wang",
        "Olawale Salaudeen",
        "Rishi Bommasani",
        "Deep Ganguli",
        "Sanmi Koyejo",
        "William Isaac"
      ],
      "abstract": "There is an increasing imperative to anticipate and understand the\nperformance and safety of generative AI systems in real-world deployment\ncontexts. However, the current evaluation ecosystem is insufficient: Commonly\nused static benchmarks face validity challenges, and ad hoc case-by-case audits\nrarely scale. In this piece, we advocate for maturing an evaluation science for\ngenerative AI systems. While generative AI creates unique challenges for system\nsafety engineering and measurement science, the field can draw valuable\ninsights from the development of safety evaluation practices in other fields,\nincluding transportation, aerospace, and pharmaceutical engineering. In\nparticular, we present three key lessons: Evaluation metrics must be applicable\nto real-world performance, metrics must be iteratively refined, and evaluation\ninstitutions and norms must be established. Applying these insights, we outline\na concrete path toward a more rigorous approach for evaluating generative AI\nsystems.",
      "tldr_zh": "这篇论文讨论了生成式AI系统的评估科学，强调当前评估生态（如静态基准和个案审计）的不足，无法有效预测其真实世界性能和安全性。作者借鉴交通、航空和制药工程领域的安全评估实践，提出了三个关键教训：评估指标必须适用于实际场景、需进行迭代优化，以及建立评估机构和规范。最终，论文概述了一个更严格、系统化的路径，以提升生成式AI系统的评估科学。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "First two authors contributed equally to this work",
      "pdf_url": "http://arxiv.org/pdf/2503.05336v3",
      "published_date": "2025-03-07 11:23:48 UTC",
      "updated_date": "2025-03-13 00:09:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:15:30.416928"
    },
    {
      "arxiv_id": "2503.05330v1",
      "title": "Speculative Decoding for Multi-Sample Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Yiwei Li",
        "Jiayi Shi",
        "Shaoxiong Feng",
        "Peiwen Yuan",
        "Xinglin Wang",
        "Yueqi Zhang",
        "Ji Zhang",
        "Chuyi Tan",
        "Boyuan Pan",
        "Yao Hu",
        "Kan Li"
      ],
      "abstract": "We propose a novel speculative decoding method tailored for multi-sample\nreasoning scenarios, such as self-consistency and Best-of-N sampling. Our\nmethod exploits the intrinsic consensus of parallel generation paths to\nsynthesize high-quality draft tokens without requiring auxiliary models or\nexternal databases. By dynamically analyzing structural patterns across\nparallel reasoning paths through a probabilistic aggregation mechanism, it\nidentifies consensus token sequences that align with the decoding distribution.\nEvaluations on mathematical reasoning benchmarks demonstrate a substantial\nimprovement in draft acceptance rates over baselines, while reducing the\nlatency in draft token construction. This work establishes a paradigm shift for\nefficient multi-sample inference, enabling seamless integration of speculative\ndecoding with sampling-based reasoning techniques.",
      "tldr_zh": "本研究提出了一种针对多样本推理场景（如 self-consistency 和 Best-of-N sampling）的全新推测解码(speculative decoding)方法，通过利用平行生成路径的内在共识来合成高质量的草稿标记，而无需辅助模型或外部数据库。方法采用概率聚合机制(probablistic aggregation mechanism)动态分析这些路径的结构模式，识别与解码分布一致的共识标记序列，从而提高效率。实验在数学推理基准上显示，与基线相比，该方法显著提升了草稿接受率并降低了标记构建延迟，最终为多样本推理建立了一个高效的新范式，便于其与基于采样的推理技术无缝整合。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05330v1",
      "published_date": "2025-03-07 11:15:36 UTC",
      "updated_date": "2025-03-07 11:15:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:15:43.129976"
    },
    {
      "arxiv_id": "2503.05328v1",
      "title": "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Anar Yeginbergen",
        "Maite Oronoz",
        "Rodrigo Agerri"
      ],
      "abstract": "This paper investigates the role of dynamic external knowledge integration in\nimproving counter-argument generation using Large Language Models (LLMs). While\nLLMs have shown promise in argumentative tasks, their tendency to generate\nlengthy, potentially unfactual responses highlights the need for more\ncontrolled and evidence-based approaches. We introduce a new manually curated\ndataset of argument and counter-argument pairs specifically designed to balance\nargumentative complexity with evaluative feasibility. We also propose a new\nLLM-as-a-Judge evaluation methodology that shows a stronger correlation with\nhuman judgments compared to traditional reference-based metrics. Our\nexperimental results demonstrate that integrating dynamic external knowledge\nfrom the web significantly improves the quality of generated counter-arguments,\nparticularly in terms of relatedness, persuasiveness, and factuality. The\nfindings suggest that combining LLMs with real-time external knowledge\nretrieval offers a promising direction for developing more effective and\nreliable counter-argumentation systems.",
      "tldr_zh": "该论文探讨了动态外部知识整合如何提升大型语言模型（LLMs）在证据驱动的反驳论点生成中的表现，旨在解决LLMs生成冗长且可能不实响应的局限性。研究者引入了一个手动 curation 的数据集，包含平衡论证复杂性和评估可行性的论点与反驳对，并提出LLM-as-a-Judge评估方法，该方法与人类判断的相关性高于传统基于参考的指标。实验结果表明，整合实时网络知识显著提高了反驳论点的相关性、说服力和事实性。该发现为结合LLMs与外部知识检索开发更有效可靠的反驳系统提供了新方向。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05328v1",
      "published_date": "2025-03-07 11:13:33 UTC",
      "updated_date": "2025-03-07 11:13:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:15:56.004115"
    },
    {
      "arxiv_id": "2503.05322v1",
      "title": "Attenuation artifact detection and severity classification in intracoronary OCT using mixed image representations",
      "title_zh": "使用混合图像表示在冠状动脉内 OCT 中的衰减伪影检测及严重程度分类",
      "authors": [
        "Pierandrea Cancian",
        "Simone Saitta",
        "Xiaojin Gu",
        "Rudolf L. M. van Herten",
        "Thijs J. Luttikholt",
        "Jos Thannhauser",
        "Rick H. J. A. Volleberg",
        "Ruben G. A. van der Waerden",
        "Joske L. van der Zande",
        "Clarisa I. Sánchez",
        "Bram van Ginneken",
        "Niels van Royen",
        "Ivana Išgum"
      ],
      "abstract": "In intracoronary optical coherence tomography (OCT), blood residues and gas\nbubbles cause attenuation artifacts that can obscure critical vessel\nstructures. The presence and severity of these artifacts may warrant\nre-acquisition, prolonging procedure time and increasing use of contrast agent.\nAccurate detection of these artifacts can guide targeted re-acquisition,\nreducing the amount of repeated scans needed to achieve diagnostically viable\nimages. However, the highly heterogeneous appearance of these artifacts poses a\nchallenge for the automated detection of the affected image regions. To enable\nautomatic detection of the attenuation artifacts caused by blood residues and\ngas bubbles based on their severity, we propose a convolutional neural network\nthat performs classification of the attenuation lines (A-lines) into three\nclasses: no artifact, mild artifact and severe artifact. Our model extracts and\nmerges features from OCT images in both Cartesian and polar coordinates, where\neach column of the image represents an A-line. Our method detects the presence\nof attenuation artifacts in OCT frames reaching F-scores of 0.77 and 0.94 for\nmild and severe artifacts, respectively. The inference time over a full OCT\nscan is approximately 6 seconds. Our experiments show that analysis of images\nrepresented in both Cartesian and polar coordinate systems outperforms the\nanalysis in polar coordinates only, suggesting that these representations\ncontain complementary features. This work lays the foundation for automated\nartifact assessment and image acquisition guidance in intracoronary OCT\nimaging.",
      "tldr_zh": "该研究针对冠状动脉 OCT 成像中由血残留和气泡引起的衰减伪像问题，提出了一种卷积神经网络 (CNN) 方法，用于检测和分类这些伪像的严重程度，包括无伪像、轻度伪像和重度伪像。模型通过从图像的笛卡尔坐标和极坐标中提取并合并特征（每列代表一个 A-line），实现了对伪像的精确识别，检测轻度伪像的 F-score 为 0.77，重度伪像为 0.94，且整个 OCT 扫描的推理时间约 6 秒。实验结果表明，使用混合坐标表示比仅使用极坐标更有效，为自动伪像评估和图像获取指导奠定了基础，从而减少重复扫描并优化临床流程。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05322v1",
      "published_date": "2025-03-07 11:01:00 UTC",
      "updated_date": "2025-03-07 11:01:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:16:08.326030"
    },
    {
      "arxiv_id": "2503.05320v1",
      "title": "Disentangling Task Interference within Neurons: Model Merging in Alignment with Neuronal Mechanisms",
      "title_zh": "翻译失败",
      "authors": [
        "Zitao Fang",
        "Guodong DU",
        "Shuyang Yu",
        "Yifei Guo",
        "Yiwei Zhang",
        "Jing Li",
        "Ho-Kin Tang",
        "Sim Kuan Goh"
      ],
      "abstract": "Fine-tuning pre-trained models on targeted datasets enhances task-specific\nperformance but often comes at the expense of generalization. Model merging\ntechniques, which integrate multiple fine-tuned models into a single multi-task\nmodel through task arithmetic at various levels: model, layer, or parameter,\noffer a promising solution. However, task interference remains a fundamental\nchallenge, leading to performance degradation and suboptimal merged models.\nExisting approaches largely overlook the fundamental role of individual neurons\nand their connectivity, resulting in a lack of interpretability in both the\nmerging process and the merged models. In this work, we present the first study\non the impact of neuronal alignment in model merging. We decompose\ntask-specific representations into two complementary neuronal subspaces that\nregulate neuron sensitivity and input adaptability. Leveraging this\ndecomposition, we introduce NeuroMerging, a novel merging framework developed\nto mitigate task interference within neuronal subspaces, enabling training-free\nmodel fusion across diverse tasks. Through extensive experiments, we\ndemonstrate that NeuroMerging achieves superior performance compared to\nexisting methods on multi-task benchmarks across both vision and natural\nlanguage domains. Our findings highlight the importance of aligning neuronal\nmechanisms in model merging, offering new insights into mitigating task\ninterference and improving knowledge fusion.",
      "tldr_zh": "本文研究了模型合并(Model Merging)中任务干扰(Task Interference)的神经元级问题，指出现有方法忽略了单个神经元及其连接的可解释性，导致性能下降。通过将任务特定表示分解成调节神经元敏感性和输入适应性的互补神经元子空间(Neuronal Subspaces)，作者引入了NeuroMerging框架，该框架在神经元子空间内缓解任务干扰，实现无训练的跨任务模型融合。实验结果显示，NeuroMerging在视觉和自然语言的多任务基准上优于现有方法，提供新的见解以提升知识融合和模型可解释性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05320v1",
      "published_date": "2025-03-07 11:00:24 UTC",
      "updated_date": "2025-03-07 11:00:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:16:19.592533"
    },
    {
      "arxiv_id": "2503.05319v1",
      "title": "Robust Multimodal Learning for Ophthalmic Disease Grading via Disentangled Representation",
      "title_zh": "基于解耦表示的鲁棒多模态学习用于眼科疾病分级",
      "authors": [
        "Xinkun Wang",
        "Yifang Wang",
        "Senwei Liang",
        "Feilong Tang",
        "Chengzhi Liu",
        "Ming Hu",
        "Chao Hu",
        "Junjun He",
        "Zongyuan Ge",
        "Imran Razzak"
      ],
      "abstract": "This paper discusses how ophthalmologists often rely on multimodal data to\nimprove diagnostic accuracy. However, complete multimodal data is rare in\nreal-world applications due to a lack of medical equipment and concerns about\ndata privacy. Traditional deep learning methods typically address these issues\nby learning representations in latent space. However, the paper highlights two\nkey limitations of these approaches: (i) Task-irrelevant redundant information\n(e.g., numerous slices) in complex modalities leads to significant redundancy\nin latent space representations. (ii) Overlapping multimodal representations\nmake it difficult to extract unique features for each modality. To overcome\nthese challenges, the authors propose the Essence-Point and Disentangle\nRepresentation Learning (EDRL) strategy, which integrates a self-distillation\nmechanism into an end-to-end framework to enhance feature selection and\ndisentanglement for more robust multimodal learning. Specifically, the\nEssence-Point Representation Learning module selects discriminative features\nthat improve disease grading performance. The Disentangled Representation\nLearning module separates multimodal data into modality-common and\nmodality-unique representations, reducing feature entanglement and enhancing\nboth robustness and interpretability in ophthalmic disease diagnosis.\nExperiments on multimodal ophthalmology datasets show that the proposed EDRL\nstrategy significantly outperforms current state-of-the-art methods.",
      "tldr_zh": "本论文探讨了眼科疾病分级中多模态学习（Multimodal Learning）的鲁棒性问题，指出传统方法在处理数据稀缺和隐私问题时，面临潜在空间冗余以及多模态特征纠缠的挑战。作者提出Essence-Point and Disentangle Representation Learning (EDRL)策略，该框架整合自蒸馏机制（self-distillation mechanism），通过Essence-Point Representation Learning模块选择判别性特征，以及Disentangled Representation Learning模块分离模态公共和模态独特表示，从而提升特征选择、减少纠缠，并提高诊断的鲁棒性和可解释性。在眼科多模态数据集上的实验显示，EDRL显著优于现有最先进方法，证明了其在实际应用中的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "10pages",
      "pdf_url": "http://arxiv.org/pdf/2503.05319v1",
      "published_date": "2025-03-07 10:58:38 UTC",
      "updated_date": "2025-03-07 10:58:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:16:31.790404"
    },
    {
      "arxiv_id": "2503.05318v1",
      "title": "Uncertainty-Aware Decoding with Minimum Bayes Risk",
      "title_zh": "翻译失败",
      "authors": [
        "Nico Daheim",
        "Clara Meister",
        "Thomas Möllenhoff",
        "Iryna Gurevych"
      ],
      "abstract": "Despite their outstanding performance in the majority of scenarios,\ncontemporary language models still occasionally generate undesirable outputs,\nfor example, hallucinated text. While such behaviors have previously been\nlinked to uncertainty, there is a notable lack of methods that actively\nconsider uncertainty during text generation. In this work, we show how Minimum\nBayes Risk (MBR) decoding, which selects model generations according to an\nexpected risk, can be generalized into a principled uncertainty-aware decoding\nmethod. In short, we account for model uncertainty during decoding by\nincorporating a posterior over model parameters into MBR's computation of\nexpected risk. We show that this modified expected risk is useful for both\nchoosing outputs and deciding when to abstain from generation and can provide\nimprovements without incurring overhead. We benchmark different methods for\nlearning posteriors and show that performance improves with prediction\ndiversity. We release our code publicly.",
      "tldr_zh": "本研究针对语言模型偶尔生成幻觉文本等问题，提出了一种基于Minimum Bayes Risk (MBR)解码的uncertainty-aware方法，通过在期望风险计算中融入模型参数的后验分布，实现对不确定性的主动考虑。该方法不仅用于选择最佳输出，还能决定何时放弃生成，从而在不增加额外开销的情况下提升生成质量。实验结果显示，性能随预测多样性而改善，并通过基准测试验证了其有效性；研究团队已公开代码以促进进一步应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "ICLR 2025 (Poster)",
      "pdf_url": "http://arxiv.org/pdf/2503.05318v1",
      "published_date": "2025-03-07 10:55:12 UTC",
      "updated_date": "2025-03-07 10:55:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:16:42.854169"
    },
    {
      "arxiv_id": "2503.10652v2",
      "title": "Simulating and Analysing Human Survey Responses with Large Language Models: A Case Study in Energy Stated Preference",
      "title_zh": "翻译失败",
      "authors": [
        "Han Wang",
        "Jacek Pawlak",
        "Aruna Sivakumar"
      ],
      "abstract": "Survey research plays a crucial role in studies by capturing consumer\npreferences and informing policy decisions. Stated preference (SP) surveys help\nresearchers understand how individuals make trade-offs in hypothetical,\npotentially futuristic, scenarios. However, traditional methods are costly,\ntime-consuming, and affected by respondent fatigue and ethical constraints.\nLarge language models (LLMs) have shown remarkable capabilities in generating\nhuman-like responses, prompting interest in their use in survey research. This\nstudy investigates LLMs for simulating consumer choices in energy-related SP\nsurveys and explores their integration into data collection and analysis\nworkflows. Test scenarios were designed to assess the simulation performance of\nseveral LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) at individual and\naggregated levels, considering prompt design, in-context learning (ICL),\nchain-of-thought (CoT) reasoning, model types, integration with traditional\nchoice models, and potential biases. While LLMs achieve accuracy above random\nguessing, performance remains insufficient for practical simulation use.\nCloud-based LLMs do not consistently outperform smaller local models.\nDeepSeek-R1 achieves the highest average accuracy (77%) and outperforms\nnon-reasoning LLMs in accuracy, factor identification, and choice distribution\nalignment. Previous SP choices are the most effective input; longer prompts\nwith more factors reduce accuracy. Mixed logit models can support LLM prompt\nrefinement. Reasoning LLMs show potential in data analysis by indicating factor\nsignificance, offering a qualitative complement to statistical models. Despite\nlimitations, pre-trained LLMs offer scalability and require minimal historical\ndata. Future work should refine prompts, further explore CoT reasoning, and\ninvestigate fine-tuning techniques.",
      "tldr_zh": "本文研究了使用 Large Language Models (LLMs) 模拟人类在能源相关 Stated Preference (SP) 调查中的响应，以降低传统调查的成本、时间消耗和伦理限制。研究评估了 LLaMA 3.1、Mistral、GPT-3.5 和 DeepSeek-R1 等模型的性能，考察了提示设计、In-Context Learning (ICL)、Chain-of-Thought (CoT) 推理以及与传统选择模型的整合，结果显示 DeepSeek-R1 表现最佳，平均准确率达 77%，但整体准确率仍不足以实际应用。LLMs 在数据分析中显示潜力，能定性指示因素重要性，并建议未来通过优化提示、深化 CoT 推理和微调技术来提升其实用性。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.10652v2",
      "published_date": "2025-03-07 10:37:31 UTC",
      "updated_date": "2025-05-13 19:38:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:16:57.284120"
    },
    {
      "arxiv_id": "2503.05306v1",
      "title": "Adversarial Policy Optimization for Offline Preference-based Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Hyungkyu Kang",
        "Min-hwan Oh"
      ],
      "abstract": "In this paper, we study offline preference-based reinforcement learning\n(PbRL), where learning is based on pre-collected preference feedback over pairs\nof trajectories. While offline PbRL has demonstrated remarkable empirical\nsuccess, existing theoretical approaches face challenges in ensuring\nconservatism under uncertainty, requiring computationally intractable\nconfidence set constructions. We address this limitation by proposing\nAdversarial Preference-based Policy Optimization (APPO), a computationally\nefficient algorithm for offline PbRL that guarantees sample complexity bounds\nwithout relying on explicit confidence sets. By framing PbRL as a two-player\ngame between a policy and a model, our approach enforces conservatism in a\ntractable manner. Using standard assumptions on function approximation and\nbounded trajectory concentrability, we derive a sample complexity bound. To our\nknowledge, APPO is the first offline PbRL algorithm to offer both statistical\nefficiency and practical applicability. Experimental results on continuous\ncontrol tasks demonstrate that APPO effectively learns from complex datasets,\nshowing comparable performance with existing state-of-the-art methods.",
      "tldr_zh": "本研究针对离线偏好强化学习（offline PbRL）提出了一种新的算法Adversarial Preference-based Policy Optimization (APPO)，通过将PbRL框架化为策略和模型之间的双人游戏，确保在不确定性下实现计算高效的保守性，而无需构建显式置信集。APPO在函数逼近和轨迹集中度的标准假设下，提供了样本复杂度边界，是首个兼具统计效率和实际适用性的离线PbRL算法。实验结果显示，在连续控制任务中，APPO能够从复杂数据集有效学习，并与现有最先进方法表现出可比性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05306v1",
      "published_date": "2025-03-07 10:35:01 UTC",
      "updated_date": "2025-03-07 10:35:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:17:07.018667"
    },
    {
      "arxiv_id": "2503.05305v1",
      "title": "Frequency Autoregressive Image Generation with Continuous Tokens",
      "title_zh": "翻译失败",
      "authors": [
        "Hu Yu",
        "Hao Luo",
        "Hangjie Yuan",
        "Yu Rong",
        "Feng Zhao"
      ],
      "abstract": "Autoregressive (AR) models for image generation typically adopt a two-stage\nparadigm of vector quantization and raster-scan ``next-token prediction\",\ninspired by its great success in language modeling. However, due to the huge\nmodality gap, image autoregressive models may require a systematic reevaluation\nfrom two perspectives: tokenizer format and regression direction. In this\npaper, we introduce the frequency progressive autoregressive (\\textbf{FAR})\nparadigm and instantiate FAR with the continuous tokenizer. Specifically, we\nidentify spectral dependency as the desirable regression direction for FAR,\nwherein higher-frequency components build upon the lower one to progressively\nconstruct a complete image. This design seamlessly fits the causality\nrequirement for autoregressive models and preserves the unique spatial locality\nof image data. Besides, we delve into the integration of FAR and the continuous\ntokenizer, introducing a series of techniques to address optimization\nchallenges and improve the efficiency of training and inference processes. We\ndemonstrate the efficacy of FAR through comprehensive experiments on the\nImageNet dataset and verify its potential on text-to-image generation.",
      "tldr_zh": "该论文提出频率渐进式自回归（FAR）范式，使用连续 tokenizer 生成图像，以解决传统 Autoregressive (AR) 模型在图像领域的局限性，如标记器格式和回归方向问题。FAR 通过频谱依赖性逐步构建图像，即高频组件基于低频组件，确保符合自回归模型的因果性要求并保留图像的空间局部性。论文还引入优化技术来处理训练和推理效率，并在 ImageNet 数据集上进行全面实验，证明 FAR 比传统方法更有效，并在文本到图像生成任务中显示出潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05305v1",
      "published_date": "2025-03-07 10:34:04 UTC",
      "updated_date": "2025-03-07 10:34:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:17:19.823783"
    },
    {
      "arxiv_id": "2503.05274v1",
      "title": "Evidential Uncertainty Estimation for Multi-Modal Trajectory Prediction",
      "title_zh": "证据不确定性估计用于多模态轨迹",
      "authors": [
        "Sajad Marvi",
        "Christoph Rist",
        "Julian Schmidt",
        "Julian Jordan",
        "Abhinav Valada"
      ],
      "abstract": "Accurate trajectory prediction is crucial for autonomous driving, yet\nuncertainty in agent behavior and perception noise makes it inherently\nchallenging. While multi-modal trajectory prediction models generate multiple\nplausible future paths with associated probabilities, effectively quantifying\nuncertainty remains an open problem. In this work, we propose a novel\nmulti-modal trajectory prediction approach based on evidential deep learning\nthat estimates both positional and mode probability uncertainty in real time.\nOur approach leverages a Normal Inverse Gamma distribution for positional\nuncertainty and a Dirichlet distribution for mode uncertainty. Unlike\nsampling-based methods, it infers both types of uncertainty in a single forward\npass, significantly improving efficiency. Additionally, we experimented with\nuncertainty-driven importance sampling to improve training efficiency by\nprioritizing underrepresented high-uncertainty samples over redundant ones. We\nperform extensive evaluations of our method on the Argoverse 1 and Argoverse 2\ndatasets, demonstrating that it provides reliable uncertainty estimates while\nmaintaining high trajectory prediction accuracy.",
      "tldr_zh": "这篇论文针对自动驾驶中的多模态轨迹预测问题，提出了一种基于 evidential deep learning 的新方法，用于实时估计位置不确定性和模式概率不确定性。该方法利用 Normal Inverse Gamma 分布处理位置不确定性，以及 Dirichlet 分布处理模式不确定性，并在单次前向传播中推断两者，从而显著提升计算效率。同时，通过不确定性驱动的重要性采样优化训练过程，优先处理高不确定性样本。在 Argoverse 1 and Argoverse 2 数据集上的广泛评估显示，该方法提供了可靠的不确定性估计，同时保持了高轨迹预测准确性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05274v1",
      "published_date": "2025-03-07 09:46:21 UTC",
      "updated_date": "2025-03-07 09:46:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:17:31.274798"
    },
    {
      "arxiv_id": "2503.05265v1",
      "title": "PhiloBERTA: A Transformer-Based Cross-Lingual Analysis of Greek and Latin Lexicons",
      "title_zh": "翻译失败",
      "authors": [
        "Rumi A. Allbert",
        "Makai L. Allbert"
      ],
      "abstract": "We present PhiloBERTA, a cross-lingual transformer model that measures\nsemantic relationships between ancient Greek and Latin lexicons. Through\nanalysis of selected term pairs from classical texts, we use contextual\nembeddings and angular similarity metrics to identify precise semantic\nalignments. Our results show that etymologically related pairs demonstrate\nsignificantly higher similarity scores, particularly for abstract philosophical\nconcepts such as epist\\=em\\=e (scientia) and dikaiosyn\\=e (iustitia).\nStatistical analysis reveals consistent patterns in these relationships (p =\n0.012), with etymologically related pairs showing remarkably stable semantic\npreservation compared to control pairs. These findings establish a quantitative\nframework for examining how philosophical concepts moved between Greek and\nLatin traditions, offering new methods for classical philological research.",
      "tldr_zh": "本研究介绍了 PhiloBERTA，一种基于 Transformer 的跨语言模型，用于分析古希腊和拉丁语词汇之间的语义关系。研究团队通过上下文嵌入（contextual embeddings）和角度相似性指标（angular similarity metrics）对经典文本中的术语对进行分析，发现词源相关的词汇对（如 epistēmē 和 scientia）显示出显著更高的相似性分数，尤其在抽象哲学概念上。统计结果（p = 0.012）证明了这些语义模式的稳定性和一致性，为古典文献学研究提供了新的定量框架，帮助探讨哲学概念在希腊和拉丁传统间的转移。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05265v1",
      "published_date": "2025-03-07 09:30:16 UTC",
      "updated_date": "2025-03-07 09:30:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:17:42.671417"
    },
    {
      "arxiv_id": "2503.05264v1",
      "title": "Jailbreaking is (Mostly) Simpler Than You Think",
      "title_zh": "翻译失败",
      "authors": [
        "Mark Russinovich",
        "Ahmed Salem"
      ],
      "abstract": "We introduce the Context Compliance Attack (CCA), a novel, optimization-free\nmethod for bypassing AI safety mechanisms. Unlike current approaches -- which\nrely on complex prompt engineering and computationally intensive optimization\n-- CCA exploits a fundamental architectural vulnerability inherent in many\ndeployed AI systems. By subtly manipulating conversation history, CCA convinces\nthe model to comply with a fabricated dialogue context, thereby triggering\nrestricted behavior. Our evaluation across a diverse set of open-source and\nproprietary models demonstrates that this simple attack can circumvent\nstate-of-the-art safety protocols. We discuss the implications of these\nfindings and propose practical mitigation strategies to fortify AI systems\nagainst such elementary yet effective adversarial tactics.",
      "tldr_zh": "本论文提出了一种名为 Context Compliance Attack (CCA) 的新型攻击方法，用于简单地绕过 AI 安全机制。CCA 通过操纵对话历史来利用 AI 系统固有的架构漏洞，而非依赖复杂的提示工程或计算密集优化，从而诱使模型遵守虚假的对话上下文并触发受限行为。在多种开源和专有模型上的评估显示，CCA 能够有效规避最先进的安全协议。论文讨论了这些发现对 AI 安全的潜在影响，并建议采用实际缓解策略来强化系统防御。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05264v1",
      "published_date": "2025-03-07 09:28:19 UTC",
      "updated_date": "2025-03-07 09:28:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:17:55.959973"
    },
    {
      "arxiv_id": "2503.05251v1",
      "title": "A Map-free Deep Learning-based Framework for Gate-to-Gate Monocular Visual Navigation aboard Miniaturized Aerial Vehicles",
      "title_zh": "一种无地图的基于深度学习的框架，用于小型飞行器上的门到门单目视觉导航",
      "authors": [
        "Lorenzo Scarciglia",
        "Antonio Paolillo",
        "Daniele Palossi"
      ],
      "abstract": "Palm-sized autonomous nano-drones, i.e., sub-50g in weight, recently entered\nthe drone racing scenario, where they are tasked to avoid obstacles and\nnavigate as fast as possible through gates. However, in contrast with their\nbigger counterparts, i.e., kg-scale drones, nano-drones expose three orders of\nmagnitude less onboard memory and compute power, demanding more efficient and\nlightweight vision-based pipelines to win the race. This work presents a\nmap-free vision-based (using only a monocular camera) autonomous nano-drone\nthat combines a real-time deep learning gate detection front-end with a classic\nyet elegant and effective visual servoing control back-end, only relying on\nonboard resources. Starting from two state-of-the-art tiny deep learning\nmodels, we adapt them for our specific task, and after a mixed\nsimulator-real-world training, we integrate and deploy them aboard our\nnano-drone. Our best-performing pipeline costs of only 24M multiply-accumulate\noperations per frame, resulting in a closed-loop control performance of 30 Hz,\nwhile achieving a gate detection root mean square error of 1.4 pixels, on our\n~20k real-world image dataset. In-field experiments highlight the capability of\nour nano-drone to successfully navigate through 15 gates in 4 min, never\ncrashing and covering a total travel distance of ~100m, with a peak flight\nspeed of 1.9 m/s. Finally, to stress the generalization capability of our\nsystem, we also test it in a never-seen-before environment, where it navigates\nthrough gates for more than 4 min.",
      "tldr_zh": "本研究提出了一种无需地图的深度学习框架，用于重量小于50g的纳米无人机（nano-drone）从起飞到着陆的单目视觉导航，仅依赖单目摄像头。框架结合实时深度学习门检测前端和经典视觉伺服（visual servoing）控制后端，通过从现有微型模型适应并进行混合模拟-真实世界训练，实现高效部署。实验结果显示，该系统每帧仅需24M multiply-accumulate operations，达到30 Hz闭环控制性能，并实现门检测的根均方误差（root mean square error）为1.4像素；在实地测试中，无人机成功通过15个门，飞行距离约100m，最高速度达1.9 m/s，并在全新环境中持续导航超过4分钟，证明了其鲁棒性和泛化能力。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "\\c{opyright}2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
      "pdf_url": "http://arxiv.org/pdf/2503.05251v1",
      "published_date": "2025-03-07 09:07:07 UTC",
      "updated_date": "2025-03-07 09:07:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:18:07.547023"
    },
    {
      "arxiv_id": "2503.05244v3",
      "title": "WritingBench: A Comprehensive Benchmark for Generative Writing",
      "title_zh": "WritingBench: 生成式写作的全面基准",
      "authors": [
        "Yuning Wu",
        "Jiahao Mei",
        "Ming Yan",
        "Chenliang Li",
        "Shaopeng Lai",
        "Yuran Ren",
        "Zijia Wang",
        "Ji Zhang",
        "Mengyue Wu",
        "Qin Jin",
        "Fei Huang"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing.",
      "tldr_zh": "该论文提出 WritingBench，一种全面的基准，用于评估大型语言模型 (LLMs) 在生成性写作方面的性能，涵盖 6 个核心写作领域和 100 个子领域，包括创意、说服性、信息性和技术写作，以弥补现有基准的局限性。论文引入了一个查询依赖的评估框架，让 LLMs 动态生成实例特定的评估标准，并结合微调的批评模型进行风格、格式和长度的标准化评分。实验结果显示，该框架使 7B 参数模型接近 SOTA 性能，并通过开源基准、评估工具和模块化组件，促进 LLMs 在写作领域的进一步发展。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05244v3",
      "published_date": "2025-03-07 08:56:20 UTC",
      "updated_date": "2025-03-20 05:13:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:18:19.763943"
    },
    {
      "arxiv_id": "2503.05239v1",
      "title": "Robust Conformal Prediction with a Single Binary Certificate",
      "title_zh": "翻译失败",
      "authors": [
        "Soroush H. Zargarbashi",
        "Aleksandar Bojchevski"
      ],
      "abstract": "Conformal prediction (CP) converts any model's output to prediction sets with\na guarantee to cover the true label with (adjustable) high probability. Robust\nCP extends this guarantee to worst-case (adversarial) inputs. Existing\nbaselines achieve robustness by bounding randomly smoothed conformity scores.\nIn practice, they need expensive Monte-Carlo (MC) sampling (e.g. $\\sim10^4$\nsamples per point) to maintain an acceptable set size. We propose a robust\nconformal prediction that produces smaller sets even with significantly lower\nMC samples (e.g. 150 for CIFAR10). Our approach binarizes samples with an\nadjustable (or automatically adjusted) threshold selected to preserve the\ncoverage guarantee. Remarkably, we prove that robustness can be achieved by\ncomputing only one binary certificate, unlike previous methods that certify\neach calibration (or test) point. Thus, our method is faster and returns\nsmaller robust sets. We also eliminate a previous limitation that requires a\nbounded score function.",
      "tldr_zh": "该论文提出了一种改进的鲁棒Conformal Prediction方法，使用单一binary certificate来确保预测集在最坏情况（adversarial inputs）下仍能以高概率覆盖真实标签。该方法通过对样本进行二值化处理，并采用可调整阈值，仅需少量Monte-Carlo (MC)采样（如150个样本在CIFAR10上），即可生成更小的预测集，同时避免了现有方法的高采样成本。相比基线，该方法计算更快，消除了对bounded score function的依赖，并证明了仅需一个binary certificate即可实现鲁棒性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Published as a conference paper at ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.05239v1",
      "published_date": "2025-03-07 08:41:53 UTC",
      "updated_date": "2025-03-07 08:41:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:18:31.134066"
    },
    {
      "arxiv_id": "2503.08700v1",
      "title": "Real-Time Semantic Segmentation of Aerial Images Using an Embedded U-Net: A Comparison of CPU, GPU, and FPGA Workflows",
      "title_zh": "翻译失败",
      "authors": [
        "Julien Posso",
        "Hugo Kieffer",
        "Nicolas Menga",
        "Omar Hlimi",
        "Sébastien Tarris",
        "Hubert Guerard",
        "Guy Bois",
        "Matthieu Couderc",
        "Eric Jenn"
      ],
      "abstract": "This study introduces a lightweight U-Net model optimized for real-time\nsemantic segmentation of aerial images, targeting the efficient utilization of\nCommercial Off-The-Shelf (COTS) embedded computing platforms. We maintain the\naccuracy of the U-Net on a real-world dataset while significantly reducing the\nmodel's parameters and Multiply-Accumulate (MAC) operations by a factor of 16.\nOur comprehensive analysis covers three hardware platforms (CPU, GPU, and FPGA)\nand five different toolchains (TVM, FINN, Vitis AI, TensorFlow GPU, and cuDNN),\nassessing each on metrics such as latency, power consumption, memory footprint,\nenergy efficiency, and FPGA resource usage. The results highlight the\ntrade-offs between these platforms and toolchains, with a particular focus on\nthe practical deployment challenges in real-world applications. Our findings\ndemonstrate that while the FPGA with Vitis AI emerges as the superior choice\ndue to its performance, energy efficiency, and maturity, it requires\nspecialized hardware knowledge, emphasizing the need for a balanced approach in\nselecting embedded computing solutions for semantic segmentation tasks",
      "tldr_zh": "本研究提出了一种轻量化的 U-Net 模型，用于实时语义分割(aerial images)，优化以适应 Commercial Off-The-Shelf (COTS) 嵌入式计算平台，同时保持模型在真实数据集上的准确性，并将参数和 Multiply-Accumulate (MAC) 操作减少 16 倍。研究对 CPU、GPU 和 FPGA 三种硬件平台，以及 TVM、FINN、Vitis AI、TensorFlow GPU 和 cuDNN 五种工具链进行了全面比较，评估指标包括延迟、功耗、内存占用、能源效率和 FPGA 资源使用。结果显示，FPGA 结合 Vitis AI 在性能和能源效率上表现出色，但需专业硬件知识，强调在语义分割任务中选择嵌入式解决方案时需权衡实际部署挑战。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.AR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "ERTS2024, Jun 2024, Toulouse, France",
      "pdf_url": "http://arxiv.org/pdf/2503.08700v1",
      "published_date": "2025-03-07 08:33:28 UTC",
      "updated_date": "2025-03-07 08:33:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:18:43.404047"
    },
    {
      "arxiv_id": "2503.05231v1",
      "title": "Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction",
      "title_zh": "翻译失败",
      "authors": [
        "Shuo Jiang",
        "Haonan Li",
        "Ruochen Ren",
        "Yanmin Zhou",
        "Zhipeng Wang",
        "Bin He"
      ],
      "abstract": "Cutting-edge robot learning techniques including foundation models and\nimitation learning from humans all pose huge demands on large-scale and\nhigh-quality datasets which constitute one of the bottleneck in the general\nintelligent robot fields. This paper presents the Kaiwu multimodal dataset to\naddress the missing real-world synchronized multimodal data problems in the\nsophisticated assembling scenario,especially with dynamics information and its\nfine-grained labelling. The dataset first provides an integration of\nhuman,environment and robot data collection framework with 20 subjects and 30\ninteraction objects resulting in totally 11,664 instances of integrated\nactions. For each of the demonstration,hand motions,operation pressures,sounds\nof the assembling process,multi-view videos, high-precision motion capture\ninformation,eye gaze with first-person videos,electromyography signals are all\nrecorded. Fine-grained multi-level annotation based on absolute timestamp,and\nsemantic segmentation labelling are performed. Kaiwu dataset aims to facilitate\nrobot learning,dexterous manipulation,human intention investigation and\nhuman-robot collaboration research.",
      "tldr_zh": "本文提出 Kaiwu 数据集和框架，旨在解决机器人学习领域中大规模高质量数据集的瓶颈问题，特别是针对复杂组装场景的实时多模态数据需求。数据集包括 20 名受试者和 30 个互动对象，共收集 11,664 个动作实例，涵盖手部动作、操作压力、组装过程声音、多视图视频、高精度 motion capture、eye gaze、第一人称视频和 electromyography 信号，并进行基于绝对时间戳的细粒度多级标注和语义分割。Kaiwu 通过这些资源促进 foundation models、imitation learning、灵巧操作、人类意图分析和 human-robot interaction 的研究。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05231v1",
      "published_date": "2025-03-07 08:28:24 UTC",
      "updated_date": "2025-03-07 08:28:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:18:55.864856"
    },
    {
      "arxiv_id": "2503.05229v1",
      "title": "Discrete Contrastive Learning for Diffusion Policies in Autonomous Driving",
      "title_zh": "翻译失败",
      "authors": [
        "Kalle Kujanpää",
        "Daulet Baimukashev",
        "Farzeen Munir",
        "Shoaib Azam",
        "Tomasz Piotr Kucner",
        "Joni Pajarinen",
        "Ville Kyrki"
      ],
      "abstract": "Learning to perform accurate and rich simulations of human driving behaviors\nfrom data for autonomous vehicle testing remains challenging due to human\ndriving styles' high diversity and variance. We address this challenge by\nproposing a novel approach that leverages contrastive learning to extract a\ndictionary of driving styles from pre-existing human driving data. We\ndiscretize these styles with quantization, and the styles are used to learn a\nconditional diffusion policy for simulating human drivers. Our empirical\nevaluation confirms that the behaviors generated by our approach are both safer\nand more human-like than those of the machine-learning-based baseline methods.\nWe believe this has the potential to enable higher realism and more effective\ntechniques for evaluating and improving the performance of autonomous vehicles.",
      "tldr_zh": "该研究针对自动驾驶测试中模拟人类驾驶行为的挑战，提出了一种基于离散对比学习(Discrete Contrastive Learning)的方法，从现有数据中提取驾驶风格字典，并通过量化(Quantization)离散化这些风格。接着，利用这些离散风格学习一个条件扩散策略(Conditional Diffusion Policy)，以生成更真实的人类驾驶模拟。实验结果显示，该方法产生的驾驶行为比基线机器学习方法更安全且更接近人类风格，为提升自动驾驶车辆的性能评估和改进提供更有效的技术支持。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05229v1",
      "published_date": "2025-03-07 08:26:04 UTC",
      "updated_date": "2025-03-07 08:26:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:19:06.679080"
    },
    {
      "arxiv_id": "2503.05227v1",
      "title": "MOHPER: Multi-objective Hyperparameter Optimization Framework for E-commerce Retrieval System",
      "title_zh": "翻译失败",
      "authors": [
        "Jungbae Park",
        "Heonseok Jang"
      ],
      "abstract": "E-commerce search optimization has evolved to include a wider range of\nmetrics that reflect user engagement and business objectives. Modern search\nframeworks now incorporate advanced quality features, such as sales counts and\ndocument-query relevance, to better align search results with these goals.\nTraditional methods typically focus on click-through rate (CTR) as a measure of\nengagement or relevance, but this can miss true purchase intent, creating a gap\nbetween user interest and actual conversions. Joint training with the\nclick-through conversion rate (CTCVR) has become essential for understanding\nbuying behavior, although its sparsity poses challenges for reliable\noptimization. This study presents MOHPER, a Multi-Objective Hyperparameter\nOptimization framework for E-commerce Retrieval systems. Utilizing Bayesian\noptimization and sampling, it jointly optimizes both CTR, CTCVR, and relevant\nobjectives, focusing on engagement and conversion of the users. In addition, to\nimprove the selection of the best configuration from multi-objective\noptimization, we suggest advanced methods for hyperparameter selection,\nincluding a meta-configuration voting strategy and a cumulative training\napproach that leverages prior optimal configurations, to improve speeds of\ntraining and efficiency. Currently deployed in a live setting, our proposed\nframework substantiates its practical efficacy in achieving a balanced\noptimization that aligns with both user satisfaction and revenue goals.",
      "tldr_zh": "该研究提出MOHPER框架，一种针对电商检索系统的多目标超参数优化方法，旨在同时优化点击率(CTR)、点击转化率(CTCVR)以及相关指标，以更好地平衡用户参与和商业目标。MOHPER利用Bayesian optimization和采样进行联合优化，并引入meta-configuration投票策略以及累积训练方法，提高了配置选择效率和训练速度。实验结果显示，该框架已在实际部署中证明其有效性，有助于提升用户满意度和收入目标的协调。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05227v1",
      "published_date": "2025-03-07 08:25:08 UTC",
      "updated_date": "2025-03-07 08:25:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:19:18.906632"
    },
    {
      "arxiv_id": "2503.05226v1",
      "title": "Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Xibai Wang"
      ],
      "abstract": "Monte Carlo Tree Search (MCTS) has emerged as a powerful tool for\ndecision-making in robotics, enabling efficient exploration of large search\nspaces. However, traditional MCTS methods struggle in environments\ncharacterized by high uncertainty and noisy data due to their reliance on\nfinal-step reward evaluation. The lack of intermediate feedback during search\noften results in suboptimal decision-making and computational inefficiencies.\n  This paper introduces Reward-Centered ReST-MCTS, a novel framework that\nenhances MCTS by incorporating intermediate reward shaping. The core of our\napproach is the Rewarding Center, which refines search trajectories by\ndynamically assigning partial rewards using rule-based validation, heuristic\nguidance, and neural estimation. By integrating these mechanisms, our method\nenables real-time optimization of search paths, mitigating the effects of error\npropagation.\n  We evaluate Reward-Centered ReST-MCTS in robotic manipulation tasks under\nhigh uncertainty, demonstrating consistent improvements in decision accuracy.\nCompared to baseline methods, including Chain-of-Thought (CoT) prompting and\nVanilla ReST-MCTS, our framework achieves a 2-4% accuracy improvement while\nmaintaining computational feasibility. Ablation studies confirm the\neffectiveness of intermediate feedback in search refinement, particularly in\npruning incorrect decision paths early. Furthermore, robustness tests show that\nour method retains high performance across varying levels of uncertainty.",
      "tldr_zh": "这篇论文提出了 Reward-Centered ReST-MCTS，一种增强 Monte Carlo Tree Search (MCTS) 的鲁棒决策框架，针对高不确定性环境中的机器人操作问题，通过引入中间奖励塑造来解决传统 MCTS 依赖最终奖励导致的决策次优和计算低效问题。框架的核心是 Rewarding Center，它利用规则-based 验证、启发式指导和神经估计动态分配部分奖励，优化搜索路径并减少错误传播。在机器人操作任务的实验中，该方法相比基线如 Chain-of-Thought (CoT) prompting 和 Vanilla ReST-MCTS 准确率提升 2-4%，并在不同不确定性水平下展现出高鲁棒性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05226v1",
      "published_date": "2025-03-07 08:25:04 UTC",
      "updated_date": "2025-03-07 08:25:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:19:31.320447"
    },
    {
      "arxiv_id": "2503.05224v1",
      "title": "Deep Sequence Models for Predicting Average Shear Wave Velocity from Strong Motion Records",
      "title_zh": "翻译失败",
      "authors": [
        "Baris Yilmaz",
        "Erdem Akagündüz",
        "Salih Tileylioglu"
      ],
      "abstract": "This study explores the use of deep learning for predicting the time averaged\nshear wave velocity in the top 30 m of the subsurface ($V_{s30}$) at strong\nmotion recording stations in T\\\"urkiye. $V_{s30}$ is a key parameter in site\ncharacterization and, as a result for seismic hazard assessment. However, it is\noften unavailable due to the lack of direct measurements and is therefore\nestimated using empirical correlations. Such correlations however are commonly\ninadequate in capturing complex, site-specific variability and this motivates\nthe need for data-driven approaches. In this study, we employ a hybrid deep\nlearning model combining convolutional neural networks (CNNs) and long\nshort-term memory (LSTM) networks to capture both spatial and temporal\ndependencies in strong motion records. Furthermore, we explore how using\ndifferent parts of the signal influence our deep learning model. Our results\nsuggest that the hybrid approach effectively learns complex, nonlinear\nrelationships within seismic signals. We observed that an improved P-wave\narrival time model increased the prediction accuracy of $V_{s30}$. We believe\nthe study provides valuable insights into improving $V_{s30}$ predictions using\na CNN-LSTM framework, demonstrating its potential for improving site\ncharacterization for seismic studies. Our codes are available via this repo:\nhttps://github.com/brsylmz23/CNNLSTM_DeepEQ",
      "tldr_zh": "本研究使用深度学习模型预测土耳其强震记录中的时间平均剪切波速度 $V_{s30}$，这是一种关键的现场表征参数，用于地震风险评估，以解决传统经验相关性无法捕捉复杂现场变异性的问题。研究采用混合模型结合卷积神经网络(CNN)和长短时记忆网络(LSTM)，来捕捉地震信号的空间和时间依赖性，并探索不同信号部分对模型的影响。结果显示，该方法能有效学习非线性关系，通过改进 P 波到达时间模型显著提高 $V_{s30}$ 预测准确性，为地震研究中的现场表征提供宝贵见解，并开源相关代码。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05224v1",
      "published_date": "2025-03-07 08:22:50 UTC",
      "updated_date": "2025-03-07 08:22:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:19:43.385971"
    },
    {
      "arxiv_id": "2503.05212v1",
      "title": "Knowledge Updating? No More Model Editing! Just Selective Contextual Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Guoxiu He",
        "Xin Song",
        "Aixin Sun"
      ],
      "abstract": "As real-world knowledge evolves, the information embedded within large\nlanguage models (LLMs) can become outdated, inadequate, or erroneous. Model\nediting has emerged as a prominent approach for updating LLMs' knowledge with\nminimal computational costs and parameter changes. This approach typically\nidentifies and adjusts specific model parameters associated with newly acquired\nknowledge. However, existing methods often underestimate the adverse effects\nthat parameter modifications can have on broadly distributed knowledge. More\ncritically, post-edit LLMs frequently struggle with multi-hop reasoning and\ncontinuous knowledge updates. Although various studies have discussed these\nshortcomings, there is a lack of comprehensive evaluation. In this paper, we\nprovide an evaluation of ten model editing methods along four dimensions:\nreliability, generalization, locality, and portability. Results confirm that\nall ten popular model editing methods show significant shortcomings across\nmultiple dimensions, suggesting model editing is less promising. We then\npropose a straightforward method called Selective Contextual Reasoning (SCR),\nfor knowledge updating. SCR does not modify model parameters but harnesses\nLLM's inherent contextual reasoning capabilities utilizing the updated\nknowledge pieces. Under SCR, an LLM first assesses whether an incoming query\nfalls within the scope of an external knowledge base. If it does, the relevant\nexternal knowledge texts are contextualized to enhance reasoning; otherwise,\nthe query is answered directly. We evaluate SCR against the ten model editing\nmethods on two counterfactual datasets with three backbone LLMs. Empirical\nresults confirm the effectiveness and efficiency of contextual reasoning for\nknowledge updating.",
      "tldr_zh": "该论文评估了十种模型编辑方法，发现它们在知识更新过程中存在显著缺陷，包括可靠性、泛化性、局部性和可移植性(portability)方面的不足，导致多跳推理和连续更新受限。作者提出了一种简单有效的替代方法 Selective Contextual Reasoning (SCR)，该方法不修改模型参数，而是利用大型语言模型(LLMs)的上下文推理能力，通过判断查询是否涉及外部知识库并整合相关文本来增强响应。实验结果显示，SCR 在两个反事实数据集上与十种模型编辑方法相比，使用三个骨干 LLMs 时表现出更高的有效性和效率，为可靠的知识更新提供了新途径。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05212v1",
      "published_date": "2025-03-07 08:04:25 UTC",
      "updated_date": "2025-03-07 08:04:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:19:55.346970"
    },
    {
      "arxiv_id": "2503.07655v1",
      "title": "GraphT5: Unified Molecular Graph-Language Modeling via Multi-Modal Cross-Token Attention",
      "title_zh": "GraphT5：通过多模态跨标记注意力的统一分子图-语言建模",
      "authors": [
        "Sangyeup Kim",
        "Nayeon Kim",
        "Yinhua Piao",
        "Sun Kim"
      ],
      "abstract": "Molecular language modeling tasks such as molecule captioning have been\nrecognized for their potential to further understand molecular properties that\ncan aid drug discovery or material synthesis based on chemical reactions.\nUnlike the common use of molecule graphs in predicting molecular properties,\nmost methods in molecular language modeling rely heavily on SMILES sequences.\nThis preference is because the task involves generating a sequence of multiple\ntokens using transformer-based models. Therefore, a main challenge is\ndetermining how to integrate graph data, which contains structural and spatial\ninformation about molecules, with text data. In addition, simply using both 1D\nSMILES text and 2D graph as inputs without addressing how they align and\nrepresent the molecule structure in different modalities makes it challenging\nto fully utilize structural knowledge about molecules. To this end, we propose\nGraphT5, a multi-modal framework that integrates 1D SMILES text and 2D graph\nrepresentations of molecules for molecular language modeling. Specifically, we\nintroduce a novel cross-token attention module in GraphT5 to bridge the gap\narising from the fundamental differences between the two modalities of molecule\nrepresentations. Cross-token attention exploits implicit information between\nSMILES and graphs of molecules, resulting from their interactions at a\nfine-grained token level that benefits molecular language modeling. Extensive\nexperiments including molecule captioning, IUPAC name prediction tasks, and\ncase studies show that our GraphT5 outperforms the latest baseline approaches,\nwhich validates the effectiveness of our GraphT5 in sufficiently utilizing 1D\nSMILES text and 2D graph representations.",
      "tldr_zh": "这篇论文提出 GraphT5，一种统一分子图-语言建模框架，通过多模态 cross-token attention 机制整合 1D SMILES 文本和 2D 图表示，以解决分子语言建模中结构和空间信息利用的挑战。GraphT5 引入新型 cross-token attention 模块，在细粒度 token 级别上桥接两种模态的差异，充分利用分子结构知识。实验结果显示，GraphT5 在分子描述、IUPAC 名称预测等任务上优于最新基线方法，验证了其在药物发现和材料合成中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.07655v1",
      "published_date": "2025-03-07 07:57:16 UTC",
      "updated_date": "2025-03-07 07:57:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:20:07.012032"
    },
    {
      "arxiv_id": "2503.05207v1",
      "title": "Policy Constraint by Only Support Constraint for Offline Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yunkai Gao",
        "Jiaming Guo",
        "Fan Wu",
        "Rui Zhang"
      ],
      "abstract": "Offline reinforcement learning (RL) aims to optimize a policy by using\npre-collected datasets, to maximize cumulative rewards. However, offline\nreinforcement learning suffers challenges due to the distributional shift\nbetween the learned and behavior policies, leading to errors when computing\nQ-values for out-of-distribution (OOD) actions. To mitigate this issue, policy\nconstraint methods aim to constrain the learned policy's distribution with the\ndistribution of the behavior policy or confine action selection within the\nsupport of the behavior policy. However, current policy constraint methods tend\nto exhibit excessive conservatism, hindering the policy from further surpassing\nthe behavior policy's performance. In this work, we present Only Support\nConstraint (OSC) which is derived from maximizing the total probability of\nlearned policy in the support of behavior policy, to address the conservatism\nof policy constraint. OSC presents a regularization term that only restricts\npolicies to the support without imposing extra constraints on actions within\nthe support. Additionally, to fully harness the performance of the new policy\nconstraints, OSC utilizes a diffusion model to effectively characterize the\nsupport of behavior policies. Experimental evaluations across a variety of\noffline RL benchmarks demonstrate that OSC significantly enhances performance,\nalleviating the challenges associated with distributional shifts and mitigating\nconservatism of policy constraints. Code is available at\nhttps://github.com/MoreanP/OSC.",
      "tldr_zh": "该论文针对离线强化学习（Offline RL）中的分布偏移问题，提出了一种名为 Only Support Constraint (OSC) 的新方法，以缓解现有政策约束方法过于保守的问题。OSC 通过最大化学习策略在行为策略支持内的总概率，仅对支持区域施加限制，而不对支持内的动作添加额外约束，从而提升策略性能。作者还利用扩散模型（diffusion model）来有效表征行为策略的支持。在多种离线 RL 基准实验中，OSC 显著提高了性能，成功缓解了 out-of-distribution (OOD) 动作的挑战和保守性问题。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05207v1",
      "published_date": "2025-03-07 07:55:51 UTC",
      "updated_date": "2025-03-07 07:55:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:20:19.294380"
    },
    {
      "arxiv_id": "2503.05203v1",
      "title": "Path Pooling: Train-Free Structure Enhancement for Efficient Knowledge Graph Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Hairu Wang",
        "Yuan Feng",
        "Xike Xie",
        "S Kevin Zhou"
      ],
      "abstract": "Although Large Language Models achieve strong success in many tasks, they\nstill suffer from hallucinations and knowledge deficiencies in real-world\napplications. Many knowledge graph-based retrieval-augmented generation\n(KG-RAG) methods enhance the quality and credibility of LLMs by leveraging\nstructure and semantic information in KGs as external knowledge bases. However,\nthese methods struggle to effectively incorporate structure information, either\nincurring high computational costs or underutilizing available knowledge.\nInspired by smoothing operations in graph representation learning, we propose\npath pooling, a simple, train-free strategy that introduces structure\ninformation through a novel path-centric pooling operation. It seamlessly\nintegrates into existing KG-RAG methods in a plug-and-play manner, enabling\nricher structure information utilization. Extensive experiments demonstrate\nthat incorporating the path pooling into the state-of-the-art KG-RAG method\nconsistently improves performance across various settings while introducing\nnegligible additional cost. Code is coming soon at\nhttps://github.com/hrwang00/path-pooling.",
      "tldr_zh": "该论文针对Large Language Models (LLMs) 的幻觉和知识缺陷问题，提出了一种无需训练的Path Pooling策略，以提升Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) 方法的结构信息利用效率。该策略借鉴图表示学习的平滑操作，通过新型的路径中心池化操作无缝整合到现有KG-RAG框架中，实现更丰富的结构和语义信息提取。实验结果显示，在多种设置下，将Path Pooling融入最先进的KG-RAG方法后，性能得到一致提升，同时额外计算成本几乎可以忽略不计。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05203v1",
      "published_date": "2025-03-07 07:48:30 UTC",
      "updated_date": "2025-03-07 07:48:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:20:31.112132"
    },
    {
      "arxiv_id": "2503.05201v1",
      "title": "Deep Muscle EMG construction using A Physics-Integrated Deep Learning approach",
      "title_zh": "翻译失败",
      "authors": [
        "Rajnish Kumar",
        "Tapas Tripura",
        "Souvik Chakraborty",
        "Sitikantha Roy"
      ],
      "abstract": "Electromyography (EMG)--based computational musculoskeletal modeling is a\nnon-invasive method for studying musculotendon function, human movement, and\nneuromuscular control, providing estimates of internal variables like muscle\nforces and joint torques. However, EMG signals from deeper muscles are often\nchallenging to measure by placing the surface EMG electrodes and unfeasible to\nmeasure directly using invasive methods. The restriction to the access of EMG\ndata from deeper muscles poses a considerable obstacle to the broad adoption of\nEMG-driven modeling techniques. A strategic alternative is to use an estimation\nalgorithm to approximate the missing EMG signals from deeper muscle. A similar\nstrategy is used in physics-informed deep learning, where the features of\nphysical systems are learned without labeled data. In this work, we propose a\nhybrid deep learning algorithm, namely the neural musculoskeletal model (NMM),\nthat integrates physics-informed and data-driven deep learning to approximate\nthe EMG signals from the deeper muscles. While data-driven modeling is used to\npredict the missing EMG signals, physics-based modeling engraves the\nsubject-specific information into the predictions. Experimental verifications\non five test subjects are carried out to investigate the performance of the\nproposed hybrid framework. The proposed NMM is validated against the joint\ntorque computed from 'OpenSim' software. The predicted deep EMG signals are\nalso compared against the state-of-the-art muscle synergy extrapolation (MSE)\napproach, where the proposed NMM completely outperforms the existing MSE\nframework by a significant margin.",
      "tldr_zh": "该研究针对 Electromyography (EMG) 在计算肌肉骨骼建模中的应用，解决了深层肌肉信号难以测量的难题，提出了一种混合深度学习算法——neural musculoskeletal model (NMM)。NMM 整合了 physics-informed deep learning 和 data-driven deep learning，分别用于预测缺失的 EMG 信号并融入主体特定信息，从而精确估算深层肌肉的 EMG。实验在五个测试对象上验证，NMM 与 'OpenSim' 软件计算的关节扭矩相比表现出色，并显著优于现有的 muscle synergy extrapolation (MSE) 方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE",
        "cs.HC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05201v1",
      "published_date": "2025-03-07 07:46:26 UTC",
      "updated_date": "2025-03-07 07:46:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:20:43.657100"
    },
    {
      "arxiv_id": "2503.05200v1",
      "title": "ORANSight-2.0: Foundational LLMs for O-RAN",
      "title_zh": "翻译失败",
      "authors": [
        "Pranshav Gajjar",
        "Vijay K. Shah"
      ],
      "abstract": "Despite the transformative impact of Large Language Models (LLMs) across\ncritical domains such as healthcare, customer service, and business marketing,\ntheir integration into Open Radio Access Networks (O-RAN) remains limited. This\ngap is primarily due to the absence of domain-specific foundational models,\nwith existing solutions often relying on general-purpose LLMs that fail to\naddress the unique challenges and technical intricacies of O-RAN. To bridge\nthis gap, we introduce ORANSight-2.0 (O-RAN Insights), a pioneering initiative\naimed at developing specialized foundational LLMs tailored for O-RAN. Built on\n18 LLMs spanning five open-source LLM frameworks, ORANSight-2.0 fine-tunes\nmodels ranging from 1 to 70B parameters, significantly reducing reliance on\nproprietary, closed-source models while enhancing performance for O-RAN. At the\ncore of ORANSight-2.0 is RANSTRUCT, a novel Retrieval-Augmented Generation\n(RAG) based instruction-tuning framework that employs two LLM agents to create\nhigh-quality instruction-tuning datasets. The generated dataset is then used to\nfine-tune the 18 pre-trained open-source LLMs via QLoRA. To evaluate\nORANSight-2.0, we introduce srsRANBench, a novel benchmark designed for code\ngeneration and codebase understanding in the context of srsRAN, a widely used\n5G O-RAN stack. We also leverage ORANBench13K, an existing benchmark for\nassessing O-RAN-specific knowledge. Our comprehensive evaluations demonstrate\nthat ORANSight-2.0 models outperform general-purpose and closed-source models,\nsuch as ChatGPT-4o and Gemini, by 5.421% on ORANBench and 18.465% on\nsrsRANBench, achieving superior performance while maintaining lower\ncomputational and energy costs. We also experiment with RAG-augmented variants\nof ORANSight-2.0 LLMs and thoroughly evaluate their energy characteristics,\ndemonstrating costs for training, standard inference, and RAG-augmented\ninference.",
      "tldr_zh": "本研究引入 ORANSight-2.0，一种针对 Open Radio Access Networks (O-RAN) 的专用基础大型语言模型 (LLMs)，以解决现有通用 LLMs 在 O-RAN 领域面临的挑战，如缺乏领域特定知识。核心方法是 RANSTRUCT，一个基于 Retrieval-Augmented Generation (RAG) 的指令微调框架，使用两个 LLM 代理生成高质量数据集，并通过 QLoRA 微调 18 个开源 LLMs（参数从 1B 到 70B），从而减少对专有模型的依赖。研究还开发了 srsRANBench 用于代码生成和代码库理解，以及利用 ORANBench13K 评估 O-RAN 知识，结果显示 ORANSight-2.0 在 ORANBench 上比 ChatGPT-4o 和 Gemini 高 5.421%，在 srsRANBench 上高 18.465%，同时显著降低了计算和能源成本。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05200v1",
      "published_date": "2025-03-07 07:44:31 UTC",
      "updated_date": "2025-03-07 07:44:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:20:56.290188"
    },
    {
      "arxiv_id": "2503.05194v1",
      "title": "Uncertainty-Aware Explainable Federated Learning",
      "title_zh": "不确定性感知的可解释联邦学习",
      "authors": [
        "Yanci Zhang",
        "Han Yu"
      ],
      "abstract": "Federated Learning (FL) is a collaborative machine learning paradigm for\nenhancing data privacy preservation. Its privacy-preserving nature complicates\nthe explanation of the decision-making processes and the evaluation of the\nreliability of the generated explanations. In this paper, we propose the\nUncertainty-aware eXplainable Federated Learning (UncertainXFL) to address\nthese challenges. It generates explanations for decision-making processes under\nFL settings and provides information regarding the uncertainty of these\nexplanations. UncertainXFL is the first framework to explicitly offer\nuncertainty evaluation for explanations within the FL context. Explanatory\ninformation is initially generated by the FL clients and then aggregated by the\nserver in a comprehensive and conflict-free manner during FL training. The\nquality of the explanations, including the uncertainty score and tested\nvalidity, guides the FL training process by prioritizing clients with the most\nreliable explanations through higher weights during model aggregation.\nExtensive experimental evaluation results demonstrate that UncertainXFL\nachieves superior model accuracy and explanation accuracy, surpassing the\ncurrent state-of-the-art model that does not incorporate uncertainty\ninformation by 2.71% and 1.77%, respectively. By integrating and quantifying\nuncertainty in the data into the explanation process, UncertainXFL not only\nclearly presents the explanation alongside its uncertainty, but also leverages\nthis uncertainty to guide the FL training process, thereby enhancing the\nrobustness and reliability of the resulting models.",
      "tldr_zh": "本文提出 Uncertainty-aware eXplainable Federated Learning (UncertainXFL) 框架，以解决 Federated Learning (FL) 在隐私保护下决策解释和可靠性评估的挑战。该框架由 FL 客户端生成解释信息，并由服务器进行全面无冲突聚合，同时评估解释的不确定性分数和有效性，以指导训练过程，通过赋予可靠解释的客户端更高权重。实验结果表明，UncertainXFL 相比现有最先进模型，模型准确性提高了 2.71%，解释准确性提高了 1.77%，从而提升了 FL 模型的鲁棒性和可靠性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05194v1",
      "published_date": "2025-03-07 07:29:48 UTC",
      "updated_date": "2025-03-07 07:29:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:21:07.677127"
    },
    {
      "arxiv_id": "2503.05188v1",
      "title": "Rewarding Curse: Analyze and Mitigate Reward Modeling Issues for LLM Reasoning",
      "title_zh": "奖励诅咒：分析和缓解LLM推理中的奖励建模问题",
      "authors": [
        "Jiachun Li",
        "Pengfei Cao",
        "Yubo Chen",
        "Jiexin Xu",
        "Huaijun Li",
        "Xiaojian Jiang",
        "Kang Liu",
        "Jun Zhao"
      ],
      "abstract": "Chain-of-thought (CoT) prompting demonstrates varying performance under\ndifferent reasoning tasks. Previous work attempts to evaluate it but falls\nshort in providing an in-depth analysis of patterns that influence the CoT. In\nthis paper, we study the CoT performance from the perspective of effectiveness\nand faithfulness. For the former, we identify key factors that influence CoT\neffectiveness on performance improvement, including problem difficulty,\ninformation gain, and information flow. For the latter, we interpret the\nunfaithful CoT issue by conducting a joint analysis of the information\ninteraction among the question, CoT, and answer. The result demonstrates that,\nwhen the LLM predicts answers, it can recall correct information missing in the\nCoT from the question, leading to the problem. Finally, we propose a novel\nalgorithm to mitigate this issue, in which we recall extra information from the\nquestion to enhance the CoT generation and evaluate CoTs based on their\ninformation gain. Extensive experiments demonstrate that our approach enhances\nboth the faithfulness and effectiveness of CoT.",
      "tldr_zh": "本论文分析了在LLM推理中，Chain-of-thought (CoT) 提示的有效性和忠诚度问题，识别出影响有效性的关键因素，包括问题难度、信息增益和信息流，并揭示不忠诚问题源于LLM在预测答案时从问题中召回CoT中缺失的信息。作者提出了一种新算法，通过从问题中提取额外信息增强CoT生成，并基于信息增益进行评估，以缓解这些问题。实验结果显示，该方法显著提高了CoT的忠诚度和有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, 21 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05188v1",
      "published_date": "2025-03-07 07:20:24 UTC",
      "updated_date": "2025-03-07 07:20:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:21:19.380966"
    },
    {
      "arxiv_id": "2503.05185v1",
      "title": "FinTMMBench: Benchmarking Temporal-Aware Multi-Modal RAG in Finance",
      "title_zh": "翻译失败",
      "authors": [
        "Fengbin Zhu",
        "Junfeng Li",
        "Liangming Pan",
        "Wenjie Wang",
        "Fuli Feng",
        "Chao Wang",
        "Huanbo Luan",
        "Tat-Seng Chua"
      ],
      "abstract": "Finance decision-making often relies on in-depth data analysis across various\ndata sources, including financial tables, news articles, stock prices, etc. In\nthis work, we introduce FinTMMBench, the first comprehensive benchmark for\nevaluating temporal-aware multi-modal Retrieval-Augmented Generation (RAG)\nsystems in finance. Built from heterologous data of NASDAQ 100 companies,\nFinTMMBench offers three significant advantages. 1) Multi-modal Corpus: It\nencompasses a hybrid of financial tables, news articles, daily stock prices,\nand visual technical charts as the corpus. 2) Temporal-aware Questions: Each\nquestion requires the retrieval and interpretation of its relevant data over a\nspecific time period, including daily, weekly, monthly, quarterly, and annual\nperiods. 3) Diverse Financial Analysis Tasks: The questions involve 10\ndifferent tasks, including information extraction, trend analysis, sentiment\nanalysis and event detection, etc. We further propose a novel TMMHybridRAG\nmethod, which first leverages LLMs to convert data from other modalities (e.g.,\ntabular, visual and time-series data) into textual format and then incorporates\ntemporal information in each node when constructing graphs and dense indexes.\nIts effectiveness has been validated in extensive experiments, but notable gaps\nremain, highlighting the challenges presented by our FinTMMBench.",
      "tldr_zh": "本研究引入FinTMMBench，这是首个针对金融领域的Temporal-Aware Multi-Modal RAG基准测试，用于评估处理时间敏感多模态数据的系统。FinTMMBench基于NASDAQ 100公司的数据，涵盖多模态语料库（如财务表格、新闻文章、股票价格和视觉技术图表）、时间感知问题（涉及每日到年度时间段的数据检索和解释），以及10种多样化金融分析任务（如信息提取、趋势分析、情感分析和事件检测）。作者提出TMMHybridRAG方法，利用LLMs将表格、视觉和时间序列数据转换为文本格式，并在构建图和密集索引时融入时间信息。实验结果验证了该方法的有效性，但也暴露了显著的性能差距，突显了金融RAG系统的挑战。",
      "categories": [
        "q-fin.CP",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "q-fin.CP",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2503.05185v1",
      "published_date": "2025-03-07 07:13:59 UTC",
      "updated_date": "2025-03-07 07:13:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:21:32.603818"
    },
    {
      "arxiv_id": "2503.05179v2",
      "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching",
      "title_zh": "翻译失败",
      "authors": [
        "Simon A. Aytes",
        "Jinheon Baek",
        "Sung Ju Hwang"
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled strong reasoning\ncapabilities through Chain-of-Thought (CoT) prompting, which elicits\nstep-by-step problem solving, but often at the cost of excessive verbosity in\nintermediate outputs, leading to increased computational overhead. We propose\nSketch-of-Thought (SoT), a prompting framework that integrates cognitively\ninspired reasoning paradigms with linguistic constraints to reduce token usage\nwhile preserving reasoning accuracy. SoT is designed as a flexible, modular\napproach and is instantiated with three paradigms--Conceptual Chaining, Chunked\nSymbolism, and Expert Lexicons--each tailored to distinct reasoning tasks and\nselected dynamically at test-time by a lightweight routing model. Across 15\nreasoning datasets spanning multiple domains, languages, and modalities, SoT\nachieves token reductions of up to 78% with minimal accuracy loss. In tasks\nsuch as mathematical and multi-hop reasoning, it even improves accuracy while\nshortening outputs.",
      "tldr_zh": "本研究针对大型语言模型 (LLMs) 在 Chain-of-Thought (CoT) 提示下的推理能力强但输出冗长、计算开销大的问题，提出了一种高效框架 Sketch-of-Thought (SoT)。SoT 整合了认知启发的推理范式，包括 Conceptual Chaining、Chunked Symbolism 和 Expert Lexicons，这些模块化组件由轻量级路由模型在测试时动态选择，以适应不同任务。实验结果显示，在 15 个跨领域、语言和模态的推理数据集上，SoT 实现了高达 78% 的 token 减少，同时准确性损失最小，甚至在数学和多跳推理任务中提升了准确性并缩短了输出。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05179v2",
      "published_date": "2025-03-07 06:57:17 UTC",
      "updated_date": "2025-05-21 07:47:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:21:44.073412"
    },
    {
      "arxiv_id": "2503.05846v1",
      "title": "Extracting and Emulsifying Cultural Explanation to Improve Multilingual Capability of LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Hamin Koo",
        "Jaehyung Kim"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable success, but their\nEnglish-centric training data limits performance in non-English languages,\nhighlighting the need for enhancements in their multilingual capabilities.\nWhile some work on multilingual prompting methods handles non-English queries\nby utilizing English translations or restructuring them to more closely align\nwith LLM reasoning patterns, these works often overlook the importance of\ncultural context, limiting their effectiveness. To address this limitation, we\npropose EMCEI, a simple yet effective approach that improves LLMs' multilingual\ncapabilities by incorporating cultural context for more accurate and\nappropriate responses. Specifically, EMCEI follows a two-step process that\nfirst extracts relevant cultural context from the LLM's parametric knowledge\nvia prompting. Then, EMCEI employs an LLM-as-Judge mechanism to select the most\nappropriate response by balancing cultural relevance and reasoning ability.\nExperiments on diverse multilingual benchmarks show that EMCEI outperforms\nexisting baselines, demonstrating its effectiveness in handling multilingual\nqueries with LLMs.",
      "tldr_zh": "大型语言模型(LLMs)由于英语中心的数据训练，在非英语语言处理上表现有限，现有方法往往忽略文化背景导致效果不佳。论文提出 EMCEI 框架，通过一个两步过程提升 LLMs 的多语言能力：首先，从 LLMs 的参数知识中提取相关文化上下文；其次，使用 LLM-as-Judge 机制平衡文化相关性和推理能力，选择最合适的响应。实验在多语言基准上显示，EMCEI 优于现有基线，提供更准确和适当的响应。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "under review, 18pages",
      "pdf_url": "http://arxiv.org/pdf/2503.05846v1",
      "published_date": "2025-03-07 06:05:34 UTC",
      "updated_date": "2025-03-07 06:05:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:21:55.218390"
    },
    {
      "arxiv_id": "2503.05164v1",
      "title": "A Comprehensive LLM-powered Framework for Driving Intelligence Evaluation",
      "title_zh": "全面的LLM驱动框架用于驾驶智能评估",
      "authors": [
        "Shanhe You",
        "Xuewen Luo",
        "Xinhe Liang",
        "Jiashu Yu",
        "Chen Zheng",
        "Jiangtao Gong"
      ],
      "abstract": "Evaluation methods for autonomous driving are crucial for algorithm\noptimization. However, due to the complexity of driving intelligence, there is\ncurrently no comprehensive evaluation method for the level of autonomous\ndriving intelligence. In this paper, we propose an evaluation framework for\ndriving behavior intelligence in complex traffic environments, aiming to fill\nthis gap. We constructed a natural language evaluation dataset of human\nprofessional drivers and passengers through naturalistic driving experiments\nand post-driving behavior evaluation interviews. Based on this dataset, we\ndeveloped an LLM-powered driving evaluation framework. The effectiveness of\nthis framework was validated through simulated experiments in the CARLA urban\ntraffic simulator and further corroborated by human assessment. Our research\nprovides valuable insights for evaluating and designing more intelligent,\nhuman-like autonomous driving agents. The implementation details of the\nframework and detailed information about the dataset can be found at Github.",
      "tldr_zh": "该论文提出了一种全面的 LLM-powered 框架，用于评估复杂交通环境中自动驾驶行为的智能水平，以解决现有评估方法的不足。研究团队通过自然驾驶实验和后续访谈构建了人类专业驾驶员和乘客的自然语言评估数据集，并基于此开发了该框架，在 CARLA 模拟器中进行模拟实验验证，并通过人类评估进一步证实其有效性。结果显示，该框架显著提升了评估准确性，并为设计更智能、人性化的自主驾驶代理提供了宝贵见解，框架实现细节和数据集可在 GitHub 上获取。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "68T45"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05164v1",
      "published_date": "2025-03-07 06:03:02 UTC",
      "updated_date": "2025-03-07 06:03:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:22:07.696228"
    },
    {
      "arxiv_id": "2503.05845v1",
      "title": "Machine Learned Force Fields: Fundamentals, its reach, and challenges",
      "title_zh": "机器学习力场：基础、其范围和挑战",
      "authors": [
        "Carlos A. Vital",
        "Román J. Armenta-Rico",
        "Huziel E. Sauceda"
      ],
      "abstract": "Highly accurate force fields are a mandatory requirement to generate\npredictive simulations. In this regard, Machine Learning Force Fields (MLFFs)\nhave emerged as a revolutionary approach in computational chemistry and\nmaterials science, combining the accuracy of quantum mechanical methods with\ncomputational efficiency orders of magnitude superior to ab-initio methods.\nThis chapter provides an introduction of the fundamentals of learning and how\nit is applied to construct MLFFs, detailing key methodologies such as neural\nnetwork potentials and kernel-based models. Emphasis is placed on the\nconstruction of SchNet model, as one of the most elemental neural network-based\nforce fields that are nowadays the basis of modern architectures. Additionally,\nthe GDML framework is described in detail as an example of how the elegant\nformulation of kernel methods can be used to construct mathematically robust\nand physics-inspired MLFFs. The ongoing advancements in MLFF development\ncontinue to expand their applicability, enabling precise simulations of large\nand complex systems that were previously beyond reach. This chapter concludes\nby highlighting the transformative impact of MLFFs on scientific research,\nunderscoring their role in driving future discoveries in the fields of\nchemistry, physics, and materials science.",
      "tldr_zh": "本论文介绍了 Machine Learning Force Fields (MLFFs)，一种革命性的方法，将量子机械方法的精确性与远超 ab-initio 方法的计算效率相结合，用于生成高准确性的模拟预测。论文详细阐述了 MLFFs 的基础知识及其构建方法，包括神经网络势（neural network potentials）如 SchNet 模型，以及基于核的方法（kernel-based models）如 GDML 框架，这些方法通过学习算法实现物理启发式的建模。最终，论文强调 MLFFs 的持续发展扩展了其在大型复杂系统模拟中的应用，并讨论了其在化学、物理和材料科学领域的变革性影响，同时指出潜在挑战以推动未来研究。",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "9 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05845v1",
      "published_date": "2025-03-07 05:26:14 UTC",
      "updated_date": "2025-03-07 05:26:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:22:18.922920"
    },
    {
      "arxiv_id": "2503.05153v2",
      "title": "Generative Trajectory Stitching through Diffusion Composition",
      "title_zh": "翻译失败",
      "authors": [
        "Yunhao Luo",
        "Utkarsh A. Mishra",
        "Yilun Du",
        "Danfei Xu"
      ],
      "abstract": "Effective trajectory stitching for long-horizon planning is a significant\nchallenge in robotic decision-making. While diffusion models have shown promise\nin planning, they are limited to solving tasks similar to those seen in their\ntraining data. We propose CompDiffuser, a novel generative approach that can\nsolve new tasks by learning to compositionally stitch together shorter\ntrajectory chunks from previously seen tasks. Our key insight is modeling the\ntrajectory distribution by subdividing it into overlapping chunks and learning\ntheir conditional relationships through a single bidirectional diffusion model.\nThis allows information to propagate between segments during generation,\nensuring physically consistent connections. We conduct experiments on benchmark\ntasks of various difficulties, covering different environment sizes, agent\nstate dimension, trajectory types, training data quality, and show that\nCompDiffuser significantly outperforms existing methods.",
      "tldr_zh": "本研究解决了机器人决策中长时域规划的轨迹拼接挑战，提出了一种新颖的生成式方法CompDiffuser，以克服扩散模型（Diffusion Models）仅限于训练数据相似任务的局限性。CompDiffuser通过将轨迹分布细分为重叠片段，并使用一个单向双向扩散模型学习片段间的条件关系，确保生成过程中信息传播并实现物理一致的连接。该方法在涵盖不同环境大小、代理状态维度、轨迹类型和训练数据质量的基准任务上进行实验，结果显示CompDiffuser显著优于现有方法。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project page: https://comp-diffuser.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2503.05153v2",
      "published_date": "2025-03-07 05:22:52 UTC",
      "updated_date": "2025-05-05 16:26:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:22:31.436130"
    },
    {
      "arxiv_id": "2503.05149v1",
      "title": "Development and Enhancement of Text-to-Image Diffusion Models",
      "title_zh": "文本到图像扩散模型的开发与增强",
      "authors": [
        "Rajdeep Roshan Sahu"
      ],
      "abstract": "This research focuses on the development and enhancement of text-to-image\ndenoising diffusion models, addressing key challenges such as limited sample\ndiversity and training instability. By incorporating Classifier-Free Guidance\n(CFG) and Exponential Moving Average (EMA) techniques, this study significantly\nimproves image quality, diversity, and stability. Utilizing Hugging Face's\nstate-of-the-art text-to-image generation model, the proposed enhancements\nestablish new benchmarks in generative AI. This work explores the underlying\nprinciples of diffusion models, implements advanced strategies to overcome\nexisting limitations, and presents a comprehensive evaluation of the\nimprovements achieved. Results demonstrate substantial progress in generating\nstable, diverse, and high-quality images from textual descriptions, advancing\nthe field of generative artificial intelligence and providing new foundations\nfor future applications.\n  Keywords: Text-to-image, Diffusion model, Classifier-free guidance,\nExponential moving average, Image generation.",
      "tldr_zh": "本研究专注于文本到图像去噪扩散模型的发展和增强，针对样本多样性有限和训练不稳定的关键挑战。研究通过引入 Classifier-Free Guidance (CFG) 和 Exponential Moving Average (EMA) 技术，显著提高了图像质量、多样性和稳定性，并利用 Hugging Face 的先进模型探索了扩散模型的底层原理。实验结果展示了在从文本描述生成稳定、高质量图像方面的实质进展，设定了新的生成 AI 基准，并为未来应用提供了坚实基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05149v1",
      "published_date": "2025-03-07 05:18:00 UTC",
      "updated_date": "2025-03-07 05:18:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:22:43.172506"
    },
    {
      "arxiv_id": "2503.05143v1",
      "title": "FedMABench: Benchmarking Mobile Agents on Decentralized Heterogeneous User Data",
      "title_zh": "翻译失败",
      "authors": [
        "Wenhao Wang",
        "Zijie Yu",
        "Rui Ye",
        "Jianqing Zhang",
        "Siheng Chen",
        "Yanfeng Wang"
      ],
      "abstract": "Mobile agents have attracted tremendous research participation recently.\nTraditional approaches to mobile agent training rely on centralized data\ncollection, leading to high cost and limited scalability. Distributed training\nutilizing federated learning offers an alternative by harnessing real-world\nuser data, providing scalability and reducing costs. However, pivotal\nchallenges, including the absence of standardized benchmarks, hinder progress\nin this field.\n  To tackle the challenges, we introduce FedMABench, the first benchmark for\nfederated training and evaluation of mobile agents, specifically designed for\nheterogeneous scenarios. FedMABench features 6 datasets with 30+ subsets, 8\nfederated algorithms, 10+ base models, and over 800 apps across 5 categories,\nproviding a comprehensive framework for evaluating mobile agents across diverse\nenvironments. Through extensive experiments, we uncover several key insights:\nfederated algorithms consistently outperform local training; the distribution\nof specific apps plays a crucial role in heterogeneity; and, even apps from\ndistinct categories can exhibit correlations during training. FedMABench is\npublicly available at: https://github.com/wwh0411/FedMABench with the datasets\nat: https://huggingface.co/datasets/wwh0411/FedMABench.",
      "tldr_zh": "本研究引入 FedMABench，这是第一个针对异构场景的基准，用于联邦训练和评估移动代理（mobile agents），以解决传统中心化数据训练的成本高和可扩展性差问题。FedMABench 包含 6 个数据集（30+ 子集）、8 个 federated algorithms、10+ 基础模型，以及超过 800 个跨 5 个类别的应用，提供全面框架评估不同环境下的移动代理。通过广泛实验，发现 federated algorithms 比本地训练表现更优，特定应用的分布影响异构性，且不同类别应用在训练中可能存在相关性；基准已公开在 GitHub 和 Hugging Face 上。",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05143v1",
      "published_date": "2025-03-07 04:52:20 UTC",
      "updated_date": "2025-03-07 04:52:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:22:55.022396"
    },
    {
      "arxiv_id": "2503.05139v2",
      "title": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs",
      "title_zh": "翻译失败",
      "authors": [
        "Ling Team",
        "Binwei Zeng",
        "Chao Huang",
        "Chao Zhang",
        "Changxin Tian",
        "Cong Chen",
        "Dingnan Jin",
        "Feng Yu",
        "Feng Zhu",
        "Feng Yuan",
        "Fakang Wang",
        "Gangshan Wang",
        "Guangyao Zhai",
        "Haitao Zhang",
        "Huizhong Li",
        "Jun Zhou",
        "Jia Liu",
        "Junpeng Fang",
        "Junjie Ou",
        "Jun Hu",
        "Ji Luo",
        "Ji Zhang",
        "Jian Liu",
        "Jian Sha",
        "Jianxue Qian",
        "Jiewei Wu",
        "Junping Zhao",
        "Jianguo Li",
        "Jubao Feng",
        "Jingchao Di",
        "Junming Xu",
        "Jinghua Yao",
        "Kuan Xu",
        "Kewei Du",
        "Longfei Li",
        "Lei Liang",
        "Lu Yu",
        "Li Tang",
        "Lin Ju",
        "Peng Xu",
        "Qing Cui",
        "Song Liu",
        "Shicheng Li",
        "Shun Song",
        "Song Yan",
        "Tengwei Cai",
        "Tianyi Chen",
        "Ting Guo",
        "Ting Huang",
        "Tao Feng",
        "Tao Wu",
        "Wei Wu",
        "Xiaolu Zhang",
        "Xueming Yang",
        "Xin Zhao",
        "Xiaobo Hu",
        "Xin Lin",
        "Yao Zhao",
        "Yilong Wang",
        "Yongzhen Guo",
        "Yuanyuan Wang",
        "Yue Yang",
        "Yang Cao",
        "Yuhao Fu",
        "Yi Xiong",
        "Yanzhe Li",
        "Zhe Li",
        "Zhiqiang Zhang",
        "Ziqi Liu",
        "Zhaoxin Huan",
        "Zujie Wen",
        "Zhenhang Sun",
        "Zhuoxuan Du",
        "Zhengyu He"
      ],
      "abstract": "In this technical report, we tackle the challenges of training large-scale\nMixture of Experts (MoE) models, focusing on overcoming cost inefficiency and\nresource limitations prevalent in such systems. To address these issues, we\npresent two differently sized MoE large language models (LLMs), namely\nLing-Lite and Ling-Plus (referred to as \"Bailing\" in Chinese, spelled\nB\\v{a}il\\'ing in Pinyin). Ling-Lite contains 16.8 billion parameters with 2.75\nbillion activated parameters, while Ling-Plus boasts 290 billion parameters\nwith 28.8 billion activated parameters. Both models exhibit comparable\nperformance to leading industry benchmarks. This report offers actionable\ninsights to improve the efficiency and accessibility of AI development in\nresource-constrained settings, promoting more scalable and sustainable\ntechnologies. Specifically, to reduce training costs for large-scale MoE\nmodels, we propose innovative methods for (1) optimization of model\narchitecture and training processes, (2) refinement of training anomaly\nhandling, and (3) enhancement of model evaluation efficiency. Additionally,\nleveraging high-quality data generated from knowledge graphs, our models\ndemonstrate superior capabilities in tool use compared to other models.\nUltimately, our experimental findings demonstrate that a 300B MoE LLM can be\neffectively trained on lower-performance devices while achieving comparable\nperformance to models of a similar scale, including dense and MoE models.\nCompared to high-performance devices, utilizing a lower-specification hardware\nsystem during the pre-training phase demonstrates significant cost savings,\nreducing computing costs by approximately 20%. The models can be accessed at\nhttps://huggingface.co/inclusionAI.",
      "tldr_zh": "本研究探讨了在资源受限环境中训练大型 Mixture of Experts (MoE) 模型的挑战，提出两种高效模型：Ling-Lite（16.8B 参数，2.75B 激活参数）和 Ling-Plus（290B 参数，28.8B 激活参数），二者性能可媲美领先行业基准。研究创新性地优化了模型架构、训练过程和异常处理，并利用知识图谱生成的高质量数据，提升了模型在工具使用方面的能力。实验结果显示，300B MoE LLM 可以在低性能 GPU 上有效训练，相比高性能设备节省约20%的计算成本，从而提高了 AI 开发的效率和可访问性。模型可通过 https://huggingface.co/inclusionAI 访问。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "34 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.05139v2",
      "published_date": "2025-03-07 04:43:39 UTC",
      "updated_date": "2025-03-10 14:21:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:23:07.739817"
    },
    {
      "arxiv_id": "2503.05132v2",
      "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
      "title_zh": "翻译失败",
      "authors": [
        "Hengguang Zhou",
        "Xirui Li",
        "Ruochen Wang",
        "Minhao Cheng",
        "Tianyi Zhou",
        "Cho-Jui Hsieh"
      ],
      "abstract": "Recently DeepSeek R1 demonstrated how reinforcement learning with simple\nrule-based incentives can enable autonomous development of complex reasoning in\nlarge language models, characterized by the \"aha moment\", in which the model\nmanifest self-reflection and increased response length during training.\nHowever, attempts to extend this success to multimodal reasoning often failed\nto reproduce these key characteristics. In this report, we present the first\nsuccessful replication of these emergent characteristics for multimodal\nreasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying\nreinforcement learning directly on the SAT dataset, our model achieves 59.47%\naccuracy on CVBench, outperforming the base model by approximately ~30% and\nexceeding both SFT setting by ~2%. In addition, we share our failed attempts\nand insights in attempting to achieve R1-like reasoning using RL with instruct\nmodels. aiming to shed light on the challenges involved. Our key observations\ninclude: (1) applying RL on instruct model often results in trivial reasoning\ntrajectories, and (2) naive length reward are ineffective in eliciting\nreasoning capabilities. The project code is available at\nhttps://github.com/turningpoint-ai/VisualThinker-R1-Zero",
      "tldr_zh": "该研究首次在非SFT的2B模型（Qwen2-VL-2B）上，通过直接在SAT数据集上应用Reinforcement Learning (RL)，成功复制了DeepSeek R1的\"Aha Moment\"，即模型的自省和复杂视觉推理能力。结果显示，该模型在CVBench上达到59.47%的准确率，比基线模型提升约30%，并优于SFT设置约2%。此外，论文分享了失败尝试的洞见：（1）在instruct模型上应用RL往往导致琐碎的推理轨迹，（2）简单的长度奖励无法有效激发推理能力，为多模态推理的RL优化提供了宝贵经验。",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05132v2",
      "published_date": "2025-03-07 04:21:47 UTC",
      "updated_date": "2025-03-10 01:52:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:23:20.805474"
    },
    {
      "arxiv_id": "2503.05127v1",
      "title": "HexPlane Representation for 3D Semantic Scene Understanding",
      "title_zh": "HexPlane 表示用于 3D 语义场景理解",
      "authors": [
        "Zeren Chen",
        "Yuenan Hou",
        "Yulin Chen",
        "Li Liu",
        "Xiao Sun",
        "Lu Sheng"
      ],
      "abstract": "In this paper, we introduce the HexPlane representation for 3D semantic scene\nunderstanding. Specifically, we first design the View Projection Module (VPM)\nto project the 3D point cloud into six planes to maximally retain the original\nspatial information. Features of six planes are extracted by the 2D encoder and\nsent to the HexPlane Association Module (HAM) to adaptively fuse the most\ninformative information for each point. The fused point features are further\nfed to the task head to yield the ultimate predictions. Compared to the popular\npoint and voxel representation, the HexPlane representation is efficient and\ncan utilize highly optimized 2D operations to process sparse and unordered 3D\npoint clouds. It can also leverage off-the-shelf 2D models, network weights,\nand training recipes to achieve accurate scene understanding in 3D space. On\nScanNet and SemanticKITTI benchmarks, our algorithm, dubbed HexNet3D, achieves\ncompetitive performance with previous algorithms. In particular, on the ScanNet\n3D segmentation task, our method obtains 77.0 mIoU on the validation set,\nsurpassing Point Transformer V2 by 1.6 mIoU. We also observe encouraging\nresults in indoor 3D detection tasks. Note that our method can be seamlessly\nintegrated into existing voxel-based, point-based, and range-based approaches\nand brings considerable gains without bells and whistles. The codes will be\navailable upon publication.",
      "tldr_zh": "这篇论文引入了HexPlane表示方法，用于提升3D语义场景理解的效率和准确性。论文设计了View Projection Module (VPM)来将3D点云投影到六个平面，以保留原始空间信息，然后通过2D编码器提取特征，并利用HexPlane Association Module (HAM)自适应融合每个点的关键信息。实验结果显示，在ScanNet基准上的3D分割任务中，HexNet3D算法达到了77.0 mIoU，超过了Point Transformer V2的1.6 mIoU，并在SemanticKITTI等任务中表现出色。该方法充分利用优化的2D操作和现成模型，并能无缝整合到体素、点或范围基于的方法中，带来显著性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05127v1",
      "published_date": "2025-03-07 04:18:55 UTC",
      "updated_date": "2025-03-07 04:18:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:23:32.166862"
    },
    {
      "arxiv_id": "2503.05126v3",
      "title": "Multi-Task Reinforcement Learning Enables Parameter Scaling",
      "title_zh": "翻译失败",
      "authors": [
        "Reginald McLean",
        "Evangelos Chatzaroulas",
        "Jordan Terry",
        "Isaac Woungang",
        "Nariman Farsad",
        "Pablo Samuel Castro"
      ],
      "abstract": "Multi-task reinforcement learning (MTRL) aims to endow a single agent with\nthe ability to perform well on multiple tasks. Recent works have focused on\ndeveloping novel sophisticated architectures to improve performance, often\nresulting in larger models; it is unclear, however, whether the performance\ngains are a consequence of the architecture design itself or the extra\nparameters. We argue that gains are mostly due to scale by demonstrating that\nnaively scaling up a simple MTRL baseline to match parameter counts outperforms\nthe more sophisticated architectures, and these gains benefit most from scaling\nthe critic over the actor. Additionally, we explore the training stability\nadvantages that come with task diversity, demonstrating that increasing the\nnumber of tasks can help mitigate plasticity loss. Our findings suggest that\nMTRL's simultaneous training across multiple tasks provides a natural framework\nfor beneficial parameter scaling in reinforcement learning, challenging the\nneed for complex architectural innovations.",
      "tldr_zh": "这篇论文探讨了多任务强化学习 (MTRL) 如何通过参数规模扩展来提升性能，而非依赖复杂的架构设计。作者通过简单地扩展一个 MTRL 基线模型的参数规模，特别是 critic 组件，证明其性能超过了更复杂的模型。实验结果显示，这种扩展带来了显著收益，同时任务多样性有助于缓解 plasticity loss 并改善训练稳定性。这些发现表明，MTRL 提供了一个自然的框架，支持强化学习中的参数规模增长，并质疑了复杂架构创新的必要性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05126v3",
      "published_date": "2025-03-07 04:13:02 UTC",
      "updated_date": "2025-03-12 16:43:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:23:44.905019"
    },
    {
      "arxiv_id": "2503.05114v1",
      "title": "Look Before You Leap: Using Serialized State Machine for Language Conditioned Robotic Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Tong Mu",
        "Yihao Liu",
        "Mehran Armand"
      ],
      "abstract": "Imitation learning frameworks for robotic manipulation have drawn attention\nin the recent development of language model grounded robotics. However, the\nsuccess of the frameworks largely depends on the coverage of the demonstration\ncases: When the demonstration set does not include examples of how to act in\nall possible situations, the action may fail and can result in cascading\nerrors. To solve this problem, we propose a framework that uses serialized\nFinite State Machine (FSM) to generate demonstrations and improve the success\nrate in manipulation tasks requiring a long sequence of precise interactions.\nTo validate its effectiveness, we use environmentally evolving and long-horizon\npuzzles that require long sequential actions. Experimental results show that\nour approach achieves a success rate of up to 98 in these tasks, compared to\nthe controlled condition using existing approaches, which only had a success\nrate of up to 60, and, in some tasks, almost failed completely.",
      "tldr_zh": "本研究针对语言条件机器人操作中的模仿学习框架问题，指出其依赖演示集覆盖率不足，可能导致任务失败和级联错误。作者提出一种使用序列化 Finite State Machine (FSM) 的框架，用于生成演示并提升长序列精确交互任务的成功率。该方法通过环境演变的长期谜题实验验证，成功率高达98%，远超现有方法的60%，显著提高了机器人操作的鲁棒性。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "7 pages, 4 figures",
      "pdf_url": "http://arxiv.org/pdf/2503.05114v1",
      "published_date": "2025-03-07 03:19:25 UTC",
      "updated_date": "2025-03-07 03:19:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:23:56.742771"
    },
    {
      "arxiv_id": "2503.05108v1",
      "title": "TS-LIF: A Temporal Segment Spiking Neuron Network for Time Series Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Shibo Feng",
        "Wanjin Feng",
        "Xingyu Gao",
        "Peilin Zhao",
        "Zhiqi Shen"
      ],
      "abstract": "Spiking Neural Networks (SNNs) offer a promising, biologically inspired\napproach for processing spatiotemporal data, particularly for time series\nforecasting. However, conventional neuron models like the Leaky\nIntegrate-and-Fire (LIF) struggle to capture long-term dependencies and\neffectively process multi-scale temporal dynamics. To overcome these\nlimitations, we introduce the Temporal Segment Leaky Integrate-and-Fire\n(TS-LIF) model, featuring a novel dual-compartment architecture. The dendritic\nand somatic compartments specialize in capturing distinct frequency components,\nproviding functional heterogeneity that enhances the neuron's ability to\nprocess both low- and high-frequency information. Furthermore, the newly\nintroduced direct somatic current injection reduces information loss during\nintra-neuronal transmission, while dendritic spike generation improves\nmulti-scale information extraction. We provide a theoretical stability analysis\nof the TS-LIF model and explain how each compartment contributes to distinct\nfrequency response characteristics. Experimental results show that TS-LIF\noutperforms traditional SNNs in time series forecasting, demonstrating better\naccuracy and robustness, even with missing data. TS-LIF advances the\napplication of SNNs in time-series forecasting, providing a biologically\ninspired approach that captures complex temporal dynamics and offers potential\nfor practical implementation in diverse forecasting scenarios. The source code\nis available at https://github.com/kkking-kk/TS-LIF.",
      "tldr_zh": "本研究提出了一种新型神经元模型 Temporal Segment Leaky Integrate-and-Fire (TS-LIF)，旨在提升 Spiking Neural Networks (SNNs) 在时间序列预测中的性能，以解决传统 Leaky Integrate-and-Fire (LIF) 模型在捕捉长期依赖性和多尺度时间动态方面的局限性。TS-LIF 采用双隔室架构，包括 dendritic 隔室（处理低频信息）和 somatic 隔室（处理高频信息），并引入直接 somatic 当前注入和 dendritic spike 生成机制，以减少信息丢失并改善多尺度信息提取。该模型的稳定性分析显示，每个隔室对不同频率响应有独特贡献。实验结果表明，TS-LIF 在时间序列预测任务中优于传统 SNNs，提供更高的准确性和鲁棒性，尤其在处理缺失数据时，并为生物启发的预测应用提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05108v1",
      "published_date": "2025-03-07 03:06:21 UTC",
      "updated_date": "2025-03-07 03:06:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:24:10.018161"
    },
    {
      "arxiv_id": "2503.05106v1",
      "title": "Grouped Sequential Optimization Strategy -- the Application of Hyperparameter Importance Assessment in Deep Learning",
      "title_zh": "分组顺序优化策略——超参数重要性评估在深度学习中的应用",
      "authors": [
        "Ruinan Wang",
        "Ian Nabney",
        "Mohammad Golbabaee"
      ],
      "abstract": "Hyperparameter optimization (HPO) is a critical component of machine learning\npipelines, significantly affecting model robustness, stability, and\ngeneralization. However, HPO is often a time-consuming and computationally\nintensive task. Traditional HPO methods, such as grid search and random search,\noften suffer from inefficiency. Bayesian optimization, while more efficient,\nstill struggles with high-dimensional search spaces. In this paper, we\ncontribute to the field by exploring how insights gained from hyperparameter\nimportance assessment (HIA) can be leveraged to accelerate HPO, reducing both\ntime and computational resources. Building on prior work that quantified\nhyperparameter importance by evaluating 10 hyperparameters on CNNs using 10\ncommon image classification datasets, we implement a novel HPO strategy called\n'Sequential Grouping.' That prior work assessed the importance weights of the\ninvestigated hyperparameters based on their influence on model performance,\nproviding valuable insights that we leverage to optimize our HPO process. Our\nexperiments, validated across six additional image classification datasets,\ndemonstrate that incorporating hyperparameter importance assessment (HIA) can\nsignificantly accelerate HPO without compromising model performance, reducing\noptimization time by an average of 31.9\\% compared to the conventional\nsimultaneous strategy.",
      "tldr_zh": "本文提出Grouped Sequential Optimization Strategy，一种利用超参数重要性评估(HIA)来加速超参数优化(HPO)的创新方法，旨在解决传统HPO方法如网格搜索和贝叶斯优化在高维空间中的低效率问题。基于先前对CNN超参数重要性的量化评估，该策略采用顺序分组(Serial Grouping)来优先优化高影响参数，从而减少计算资源和时间。在六个图像分类数据集上的实验验证表明，该方法平均减少31.9%的优化时间，同时不影响模型性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T05, 68Q32"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages",
      "pdf_url": "http://arxiv.org/pdf/2503.05106v1",
      "published_date": "2025-03-07 03:01:00 UTC",
      "updated_date": "2025-03-07 03:01:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:24:20.601141"
    },
    {
      "arxiv_id": "2503.05092v1",
      "title": "Multi-Robot Collaboration through Reinforcement Learning and Abstract Simulation",
      "title_zh": "翻译失败",
      "authors": [
        "Adam Labiosa",
        "Josiah P. Hanna"
      ],
      "abstract": "Teams of people coordinate to perform complex tasks by forming abstract\nmental models of world and agent dynamics. The use of abstract models contrasts\nwith much recent work in robot learning that uses a high-fidelity simulator and\nreinforcement learning (RL) to obtain policies for physical robots. Motivated\nby this difference, we investigate the extent to which so-called abstract\nsimulators can be used for multi-agent reinforcement learning (MARL) and the\nresulting policies successfully deployed on teams of physical robots. An\nabstract simulator models the robot's target task at a high-level of\nabstraction and discards many details of the world that could impact optimal\ndecision-making. Policies are trained in an abstract simulator then transferred\nto the physical robot by making use of separately-obtained low-level perception\nand motion control modules. We identify three key categories of modifications\nto the abstract simulator that enable policy transfer to physical robots:\nsimulation fidelity enhancements, training optimizations and simulation\nstochasticity. We then run an empirical study with extensive ablations to\ndetermine the value of each modification category for enabling policy transfer\nin cooperative robot soccer tasks. We also compare the performance of policies\nproduced by our method with a well-tuned non-learning-based behavior\narchitecture from the annual RoboCup competition and find that our approach\nleads to a similar level of performance. Broadly we show that MARL can be use\nto train cooperative physical robot behaviors using highly abstract models of\nthe world.",
      "tldr_zh": "该研究探讨了使用抽象模拟器结合多智能体强化学习（MARL）来训练多机器人协作策略，并将其成功转移到物理机器人上。论文对比了人类抽象心理模型与传统高保真模拟器的差异，提出了三个关键修改类别——模拟保真度增强、训练优化和模拟随机性——以克服抽象模拟器在决策细节上的不足。通过在合作机器人足球任务中的实证研究和消融实验，结果显示这些修改显著提高了策略转移的性能，与RoboCup比赛中的非学习行为架构相比，表现出类似水平。总体上，该方法证明了高度抽象的模型可用于MARL训练，实现了高效的物理机器人协作行为。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "ICRA 2025",
      "pdf_url": "http://arxiv.org/pdf/2503.05092v1",
      "published_date": "2025-03-07 02:23:24 UTC",
      "updated_date": "2025-03-07 02:23:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:24:32.189153"
    },
    {
      "arxiv_id": "2503.08699v1",
      "title": "Blockchain As a Platform For Artificial Intelligence (AI) Transparency",
      "title_zh": "翻译失败",
      "authors": [
        "Afroja Akther",
        "Ayesha Arobee",
        "Abdullah Al Adnan",
        "Omum Auyon",
        "ASM Johirul Islam",
        "Farhad Akter"
      ],
      "abstract": "As artificial intelligence (AI) systems become increasingly complex and\nautonomous, concerns over transparency and accountability have intensified. The\n\"black box\" problem in AI decision-making limits stakeholders' ability to\nunderstand, trust, and verify outcomes, particularly in high-stakes sectors\nsuch as healthcare, finance, and autonomous systems. Blockchain technology,\nwith its decentralized, immutable, and transparent characteristics, presents a\npotential solution to enhance AI transparency and auditability. This paper\nexplores the integration of blockchain with AI to improve decision\ntraceability, data provenance, and model accountability. By leveraging\nblockchain as an immutable record-keeping system, AI decision-making can become\nmore interpretable, fostering trust among users and regulatory compliance.\nHowever, challenges such as scalability, integration complexity, and\ncomputational overhead must be addressed to fully realize this synergy. This\nstudy discusses existing research, proposes a framework for blockchain-enhanced\nAI transparency, and highlights practical applications, benefits, and\nlimitations. The findings suggest that blockchain could be a foundational\ntechnology for ensuring AI systems remain accountable, ethical, and aligned\nwith regulatory standards.",
      "tldr_zh": "该论文探讨了AI系统“black box”问题带来的透明性和问责性挑战，尤其在医疗、金融和自治系统等领域。作者提出将blockchain技术整合到AI中，利用其decentralized、immutable和transparent特性，提升决策traceability、data provenance和model accountability，从而使AI决策更可解释并增强用户信任。论文构建了一个blockchain-enhanced AI transparency框架，讨论了现有研究、实际应用、益处（如合规性）和限制（如scalability、integration complexity和computational overhead）。总体发现表明，blockchain可作为确保AI系统accountable、ethical并符合regulatory standards的基础技术。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CY",
        "F.2.2"
      ],
      "primary_category": "cs.CR",
      "comment": "14 pages, 2 figures, 5 tables",
      "pdf_url": "http://arxiv.org/pdf/2503.08699v1",
      "published_date": "2025-03-07 01:57:26 UTC",
      "updated_date": "2025-03-07 01:57:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:24:44.462146"
    },
    {
      "arxiv_id": "2503.05071v1",
      "title": "Object Packing and Scheduling for Sequential 3D Printing: a Linear Arithmetic Model and a CEGAR-inspired Optimal Solver",
      "title_zh": "翻译失败",
      "authors": [
        "Pavel Surynek",
        "Vojtěch Bubník",
        "Lukáš Matěna",
        "Petr Kubiš"
      ],
      "abstract": "We address the problem of object arrangement and scheduling for sequential 3D\nprinting. Unlike the standard 3D printing, where all objects are printed slice\nby slice at once, in sequential 3D printing, objects are completed one after\nother. In the sequential case, it is necessary to ensure that the moving parts\nof the printer do not collide with previously printed objects. We look at the\nsequential printing problem from the perspective of combinatorial optimization.\nWe propose to express the problem as a linear arithmetic formula, which is then\nsolved using a solver for satisfiability modulo theories (SMT). However, we do\nnot solve the formula expressing the problem of object arrangement and\nscheduling directly, but we have proposed a technique inspired by\ncounterexample guided abstraction refinement (CEGAR), which turned out to be a\nkey innovation to efficiency.",
      "tldr_zh": "这篇论文解决了顺序 3D 打印中的物体排列和调度问题，与标准 3D 打印不同，它需确保打印头避免与已打印物体碰撞。研究者提出使用 Linear Arithmetic Model 来表示该问题，并通过 SMT 求解器进行优化求解。关键创新是引入受 CEGAR（Counterexample Guided Abstraction Refinement）启发的技术，以显著提高求解效率，从而从组合优化角度提升顺序 3D 打印的整体性能。",
      "categories": [
        "cs.CG",
        "cs.AI"
      ],
      "primary_category": "cs.CG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05071v1",
      "published_date": "2025-03-07 01:31:40 UTC",
      "updated_date": "2025-03-07 01:31:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:24:55.813274"
    },
    {
      "arxiv_id": "2503.05070v1",
      "title": "PromptPex: Automatic Test Generation for Language Model Prompts",
      "title_zh": "翻译失败",
      "authors": [
        "Reshabh K Sharma",
        "Jonathan De Halleux",
        "Shraddha Barke",
        "Benjamin Zorn"
      ],
      "abstract": "Large language models (LLMs) are being used in many applications and prompts\nfor these models are integrated into software applications as code-like\nartifacts. These prompts behave much like traditional software in that they\ntake inputs, generate outputs, and perform some specific function. However,\nprompts differ from traditional code in many ways and require new approaches to\nensure that they are robust. For example, unlike traditional software the\noutput of a prompt depends on the AI model that interprets it. Also, while\nnatural language prompts are easy to modify, the impact of updates is harder to\npredict. New approaches to testing, debugging, and modifying prompts with\nrespect to the model running them are required.\n  To address some of these issues, we developed PromptPex, an LLM-based tool to\nautomatically generate and evaluate unit tests for a given prompt. PromptPex\nextracts input and output specifications from a prompt and uses them to\ngenerate diverse, targeted, and valid unit tests. These tests are instrumental\nin identifying regressions when a prompt is changed and also serve as a tool to\nunderstand how prompts are interpreted by different models. We use PromptPex to\ngenerate tests for eight benchmark prompts and evaluate the quality of the\ngenerated tests by seeing if they can cause each of four diverse models to\nproduce invalid output. PromptPex consistently creates tests that result in\nmore invalid model outputs than a carefully constructed baseline LLM-based test\ngenerator. Furthermore, by extracting concrete specifications from the input\nprompt, PromptPex allows prompt writers to clearly understand and test specific\naspects of their prompts. The source code of PromptPex is available at\nhttps://github.com/microsoft/promptpex.",
      "tldr_zh": "本研究针对大型语言模型(LLMs)的prompts在软件应用中的独特挑战（如输出依赖于AI模型和修改影响难以预测）提出了一种新测试方法。PromptPex是一个基于LLMs的工具，能自动从给定prompt中提取输入和输出规范，并生成多样化、针对性和有效的单元测试。这些测试有助于识别prompt修改时的回归问题，并评估不同模型对prompt的解释。在八个基准prompt的实验中，PromptPex生成的测试比基线工具更有效地导致模型产生无效输出，从而提升了prompt的鲁棒性和可维护性。工具的源代码可在https://github.com/microsoft/promptpex获取。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05070v1",
      "published_date": "2025-03-07 01:31:03 UTC",
      "updated_date": "2025-03-07 01:31:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:25:08.848924"
    },
    {
      "arxiv_id": "2503.05066v2",
      "title": "Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of Experts",
      "title_zh": "容量感知推理：缓解混合专家中的落后者效应",
      "authors": [
        "Shwai He",
        "Weilin Cai",
        "Jiayi Huang",
        "Ang Li"
      ],
      "abstract": "The Mixture of Experts (MoE) is an effective architecture for scaling large\nlanguage models by leveraging sparse expert activation, optimizing the\ntrade-off between performance and efficiency. However, under expert\nparallelism, MoE suffers from inference inefficiencies due to imbalanced\ntoken-to-expert assignment, where some experts are overloaded while others\nremain underutilized. This imbalance leads to poor resource utilization and\nincreased latency, as the most burdened expert dictates the overall delay, a\nphenomenon we define as the \\textbf{\\textit{Straggler Effect}}. To mitigate\nthis, we propose Capacity-Aware Inference, including two key techniques: (1)\n\\textbf{\\textit{Capacity-Aware Token Drop}}, which discards overloaded tokens\nto regulate the maximum latency of MoE, and (2) \\textbf{\\textit{Capacity-Aware\nToken Reroute}}, which reallocates overflowed tokens to underutilized experts,\nbalancing the token distribution. These techniques collectively optimize both\nhigh-load and low-load expert utilization, leading to a more efficient MoE\ninference pipeline. Extensive experiments demonstrate the effectiveness of our\nmethods, showing significant improvements in inference efficiency, e.g., 0.2\\%\naverage performance increase and a 1.94$\\times$ inference speedup on\nMixtral-8$\\times$7B-Instruct.",
      "tldr_zh": "Mixture of Experts (MoE) 是一种通过稀疏专家激活优化大型语言模型性能和效率的架构，但由于专家并行性导致的 token-to-expert 分配不平衡，会引发 Straggler Effect，从而增加延迟和降低资源利用率。为解决此问题，本文提出 Capacity-Aware Inference 框架，包括 Capacity-Aware Token Drop（丢弃过载 tokens 以调节最大延迟）和 Capacity-Aware Token Reroute（重新分配过载 tokens 到未充分利用的专家，以平衡负载）。这些技术优化了专家利用率，实验在 Mixtral-8×7B-Instruct 上实现了 0.2% 的平均性能提升和 1.94× 的推理加速。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05066v2",
      "published_date": "2025-03-07 01:11:39 UTC",
      "updated_date": "2025-05-22 17:55:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:25:20.159060"
    },
    {
      "arxiv_id": "2503.05064v1",
      "title": "Perceiving, Reasoning, Adapting: A Dual-Layer Framework for VLM-Guided Precision Robotic Manipulation",
      "title_zh": "感知、推理、适应：一种双层框架用于 V",
      "authors": [
        "Qingxuan Jia",
        "Guoqin Tang",
        "Zeyuan Huang",
        "Zixuan Hao",
        "Ning Ji",
        "Shihang",
        "Yin",
        "Gang Chen"
      ],
      "abstract": "Vision-Language Models (VLMs) demonstrate remarkable potential in robotic\nmanipulation, yet challenges persist in executing complex fine manipulation\ntasks with high speed and precision. While excelling at high-level planning,\nexisting VLM methods struggle to guide robots through precise sequences of fine\nmotor actions. To address this limitation, we introduce a progressive VLM\nplanning algorithm that empowers robots to perform fast, precise, and\nerror-correctable fine manipulation. Our method decomposes complex tasks into\nsub-actions and maintains three key data structures: task memory structure, 2D\ntopology graphs, and 3D spatial networks, achieving high-precision\nspatial-semantic fusion. These three components collectively accumulate and\nstore critical information throughout task execution, providing rich context\nfor our task-oriented VLM interaction mechanism. This enables VLMs to\ndynamically adjust guidance based on real-time feedback, generating precise\naction plans and facilitating step-wise error correction. Experimental\nvalidation on complex assembly tasks demonstrates that our algorithm\neffectively guides robots to rapidly and precisely accomplish fine manipulation\nin challenging scenarios, significantly advancing robot intelligence for\nprecision tasks.",
      "tldr_zh": "这篇论文提出了一种双层框架，用于指导 Vision-Language Models (VLMs) 实现精确机器人操作，解决 VLMs 在高速精细任务中的局限性，如难以执行精确动作序列。框架采用渐进式 VLM 规划算法，将复杂任务分解为子动作，并维护任务记忆结构、2D topology graphs 和 3D spatial networks，实现高精度空间-语义融合和实时反馈机制。实验在复杂组装任务中验证，该方法使机器人能够快速、精确地完成操作并进行步进式错误修正，显著提升了机器人智能。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05064v1",
      "published_date": "2025-03-07 00:55:42 UTC",
      "updated_date": "2025-03-07 00:55:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:25:32.256014"
    },
    {
      "arxiv_id": "2503.05051v1",
      "title": "Accelerated Patient-specific Non-Cartesian MRI Reconstruction using Implicit Neural Representations",
      "title_zh": "利用隐式",
      "authors": [
        "Di Xu",
        "Hengjie Liu",
        "Xin Miao",
        "Daniel O'Connor",
        "Jessica E. Scholey",
        "Wensha Yang",
        "Mary Feng",
        "Michael Ohliger",
        "Hui Lin",
        "Dan Ruan",
        "Yang Yang",
        "Ke Sheng"
      ],
      "abstract": "The scanning time for a fully sampled MRI can be undesirably lengthy.\nCompressed sensing has been developed to minimize image artifacts in\naccelerated scans, but the required iterative reconstruction is computationally\ncomplex and difficult to generalize on new cases. Image-domain-based deep\nlearning methods (e.g., convolutional neural networks) emerged as a faster\nalternative but face challenges in modeling continuous k-space, a problem\namplified with non-Cartesian sampling commonly used in accelerated acquisition.\nIn comparison, implicit neural representations can model continuous signals in\nthe frequency domain and thus are compatible with arbitrary k-space sampling\npatterns. The current study develops a novel generative-adversarially trained\nimplicit neural representations (k-GINR) for de novo undersampled non-Cartesian\nk-space reconstruction. k-GINR consists of two stages: 1) supervised training\non an existing patient cohort; 2) self-supervised patient-specific\noptimization. In stage 1, the network is trained with the\ngenerative-adversarial network on diverse patients of the same anatomical\nregion supervised by fully sampled acquisition. In stage 2, undersampled\nk-space data of individual patients is used to tailor the prior-embedded\nnetwork for patient-specific optimization. The UCSF StarVIBE T1-weighted liver\ndataset was evaluated on the proposed framework. k-GINR is compared with an\nimage-domain deep learning method, Deep Cascade CNN, and a compressed sensing\nmethod. k-GINR consistently outperformed the baselines with a larger\nperformance advantage observed at very high accelerations (e.g., 20 times).\nk-GINR offers great value for direct non-Cartesian k-space reconstruction for\nnew incoming patients across a wide range of accelerations liver anatomy.",
      "tldr_zh": "本研究针对MRI扫描时间过长的问题，提出了一种基于Implicit Neural Representations的加速患者特定非笛卡尔k-space重建框架k-GINR。该框架采用生成对抗网络(GAN)进行两阶段训练：第一阶段在现有患者队列上监督训练以学习通用先验；第二阶段对个体患者进行自监督优化，以适应undersampled非笛卡尔数据。实验结果显示，在UCSF StarVIBE T1加权肝脏数据集上，k-GINR在高加速率（如20倍）下显著优于Deep Cascade CNN和Compressed Sensing基线方法，提供更准确的重建性能。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2503.05051v1",
      "published_date": "2025-03-07 00:05:43 UTC",
      "updated_date": "2025-03-07 00:05:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-23T23:25:44.081527"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 121,
  "processed_papers_count": 121,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-23T23:26:00.219556"
}
[
  {
    "arxiv_id": "2503.05985v1",
    "title": "Black Box Causal Inference: Effect Estimation via Meta Prediction",
    "authors": [
      "Lucius E. J. Bynum",
      "Aahlad Manas Puli",
      "Diego Herrero-Quevedo",
      "Nhi Nguyen",
      "Carlos Fernandez-Granda",
      "Kyunghyun Cho",
      "Rajesh Ranganath"
    ],
    "abstract": "Causal inference and the estimation of causal effects plays a central role in\ndecision-making across many areas, including healthcare and economics.\nEstimating causal effects typically requires an estimator that is tailored to\neach problem of interest. But developing estimators can take significant effort\nfor even a single causal inference setting. For example, algorithms for\nregression-based estimators, propensity score methods, and doubly robust\nmethods were designed across several decades to handle causal estimation with\nobserved confounders. Similarly, several estimators have been developed to\nexploit instrumental variables (IVs), including two-stage least-squares (TSLS),\ncontrol functions, and the method-of-moments. In this work, we instead frame\ncausal inference as a dataset-level prediction problem, offloading algorithm\ndesign to the learning process. The approach we introduce, called black box\ncausal inference (BBCI), builds estimators in a black-box manner by learning to\npredict causal effects from sampled dataset-effect pairs. We demonstrate\naccurate estimation of average treatment effects (ATEs) and conditional average\ntreatment effects (CATEs) with BBCI across several causal inference problems\nwith known identification, including problems with less developed estimators.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.CO",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05985v1",
    "published_date": "2025-03-07 23:43:19 UTC",
    "updated_date": "2025-03-07 23:43:19 UTC"
  },
  {
    "arxiv_id": "2503.05980v1",
    "title": "SINdex: Semantic INconsistency Index for Hallucination Detection in LLMs",
    "authors": [
      "Samir Abdaljalil",
      "Hasan Kurban",
      "Parichit Sharma",
      "Erchin Serpedin",
      "Rachad Atat"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed across diverse\ndomains, yet they are prone to generating factually incorrect outputs -\ncommonly known as \"hallucinations.\" Among existing mitigation strategies,\nuncertainty-based methods are particularly attractive due to their ease of\nimplementation, independence from external data, and compatibility with\nstandard LLMs. In this work, we introduce a novel and scalable\nuncertainty-based semantic clustering framework for automated hallucination\ndetection. Our approach leverages sentence embeddings and hierarchical\nclustering alongside a newly proposed inconsistency measure, SINdex, to yield\nmore homogeneous clusters and more accurate detection of hallucination\nphenomena across various LLMs. Evaluations on prominent open- and closed-book\nQA datasets demonstrate that our method achieves AUROC improvements of up to\n9.3% over state-of-the-art techniques. Extensive ablation studies further\nvalidate the effectiveness of each component in our framework.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05980v1",
    "published_date": "2025-03-07 23:25:19 UTC",
    "updated_date": "2025-03-07 23:25:19 UTC"
  },
  {
    "arxiv_id": "2503.05979v1",
    "title": "Learning-Order Autoregressive Models with Application to Molecular Graph Generation",
    "authors": [
      "Zhe Wang",
      "Jiaxin Shi",
      "Nicolas Heess",
      "Arthur Gretton",
      "Michalis K. Titsias"
    ],
    "abstract": "Autoregressive models (ARMs) have become the workhorse for sequence\ngeneration tasks, since many problems can be modeled as next-token prediction.\nWhile there appears to be a natural ordering for text (i.e., left-to-right),\nfor many data types, such as graphs, the canonical ordering is less obvious. To\naddress this problem, we introduce a variant of ARM that generates\nhigh-dimensional data using a probabilistic ordering that is sequentially\ninferred from data. This model incorporates a trainable probability\ndistribution, referred to as an \\emph{order-policy}, that dynamically decides\nthe autoregressive order in a state-dependent manner. To train the model, we\nintroduce a variational lower bound on the exact log-likelihood, which we\noptimize with stochastic gradient estimation. We demonstrate experimentally\nthat our method can learn meaningful autoregressive orderings in image and\ngraph generation. On the challenging domain of molecular graph generation, we\nachieve state-of-the-art results on the QM9 and ZINC250k benchmarks, evaluated\nusing the Fr\\'{e}chet ChemNet Distance (FCD).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05979v1",
    "published_date": "2025-03-07 23:24:24 UTC",
    "updated_date": "2025-03-07 23:24:24 UTC"
  },
  {
    "arxiv_id": "2503.05977v1",
    "title": "Is Your Video Language Model a Reliable Judge?",
    "authors": [
      "Ming Liu",
      "Wensheng Zhang"
    ],
    "abstract": "As video language models (VLMs) gain more applications in various scenarios,\nthe need for robust and scalable evaluation of their performance becomes\nincreasingly critical. The traditional human expert-based evaluation of VLMs\nhas limitations in consistency and scalability, which sparked interest in\nautomatic methods such as employing VLMs to evaluate VLMs. However, the\nreliability of VLMs as judges remains underexplored. Existing methods often\nrely on a single VLM as the evaluator. However, this approach can be unreliable\nor biased because such a model may lack the ability to fully understand the\ncontent and may have inherent biases, ultimately compromising evaluation\nreliability. A remedy is to apply the principle of collective thoughts,\naggregating evaluations from multiple VLMs to enhance reliability. This study\ninvestigates the efficacy of such approaches, particularly when the pool of\njudges includes both reliable and unreliable models. Our findings reveal that\nincorporating collective judgments from such a mixed pool does not necessarily\nimprove the accuracy of the final evaluation. The inclusion of less reliable\njudges can introduce noise, undermining the overall reliability of the\noutcomes. To explore the factors that impact evaluation reliability, we\nfine-tune an underperforming VLM judge, Video-LLaVA, and observe that improved\nunderstanding ability alone is insufficient to make VLM judges more reliable.\nThese findings stress the limitations of collective thought approaches and\nhighlight the need for more advanced methods that can account for the\nreliability of individual models. Our study promotes the development of more\nreliable evaluation methods for VLMs",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05977v1",
    "published_date": "2025-03-07 23:17:59 UTC",
    "updated_date": "2025-03-07 23:17:59 UTC"
  },
  {
    "arxiv_id": "2503.05972v1",
    "title": "Optimal sensor deception in stochastic environments with partial observability to mislead a robot to a decoy goal",
    "authors": [
      "Hazhar Rahmani",
      "Mukulika Ghosh",
      "Syed Md Hasnayeen"
    ],
    "abstract": "Deception is a common strategy adapted by autonomous systems in adversarial\nsettings. Existing deception methods primarily focus on increasing opacity or\nmisdirecting agents away from their goal or itinerary. In this work, we propose\na deception problem aiming to mislead the robot towards a decoy goal through\naltering sensor events under a constrained budget of alteration. The\nenvironment along with the robot's interaction with it is modeled as a\nPartially Observable Markov Decision Process (POMDP), and the robot's action\nselection is governed by a Finite State Controller (FSC). Given a constrained\nbudget for sensor event modifications, the objective is to compute a sensor\nalteration that maximizes the probability of the robot reaching a decoy goal.\nWe establish the computational hardness of the problem by a reduction from the\n$0/1$ Knapsack problem and propose a Mixed Integer Linear Programming (MILP)\nformulation to compute optimal deception strategies. We show the efficacy of\nour MILP formulation via a sequence of experiments.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05972v1",
    "published_date": "2025-03-07 22:57:27 UTC",
    "updated_date": "2025-03-07 22:57:27 UTC"
  },
  {
    "arxiv_id": "2503.05971v2",
    "title": "A Real-time Multimodal Transformer Neural Network-powered Wildfire Forecasting System",
    "authors": [
      "Qijun Chen",
      "Shaofan Li"
    ],
    "abstract": "Due to climate change, the extreme wildfire has become one of the most\ndangerous natural hazards to human civilization. Even though, some wildfires\nmay be initially caused by human activity, but the spread of wildfires is\nmainly determined by environmental factors, for examples, (1) weather\nconditions such as temperature, wind direction and intensity, and moisture\nlevels; (2) the amount and types of dry vegetation in a local area, and (3)\ntopographic or local terrian conditions, which affects how much rain an area\ngets and how fire dynamics will be constrained or faciliated. Thus, to\naccurately forecast wildfire occurrence has become one of most urgent and\ntaunting environmental challenges in global scale. In this work, we developed a\nreal-time Multimodal Transformer Neural Network Machine Learning model that\ncombines several advanced artificial intelligence techniques and statistical\nmethods to practically forecast the occurrence of wildfire at the precise\nlocation in real time, which not only utilizes large scale data information\nsuch as hourly weather forecasting data, but also takes into account small\nscale topographical data such as local terrain condition and local vegetation\nconditions collecting from Google Earth images to determine the probabilities\nof wildfire occurrence location at small scale as well as their timing\nsynchronized with weather forecast information. By using the wildfire data in\nthe United States from 1992 to 2015 to train the multimodal transformer neural\nnetwork, it can predict the probabilities of wildfire occurrence according to\nthe real-time weather forecast and the synchronized Google Earth image data to\nprovide the wildfire occurrence probability in any small location ($100m^2$)\nwithin 24 hours ahead.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05971v2",
    "published_date": "2025-03-07 22:48:46 UTC",
    "updated_date": "2025-03-12 03:22:04 UTC"
  },
  {
    "arxiv_id": "2503.05966v2",
    "title": "Explaining the Unexplainable: A Systematic Review of Explainable AI in Finance",
    "authors": [
      "Md Talha Mohsin",
      "Nabid Bin Nasim"
    ],
    "abstract": "Practitioners and researchers trying to strike a balance between accuracy and\ntransparency center Explainable Artificial Intelligence (XAI) at the junction\nof finance. This paper offers a thorough overview of the changing scene of XAI\napplications in finance together with domain-specific implementations,\nmethodological developments, and trend mapping of research. Using bibliometric\nand content analysis, we find topic clusters, significant research, and most\noften used explainability strategies used in financial industries. Our results\nshow a substantial dependence on post-hoc interpretability techniques;\nattention mechanisms, feature importance analysis and SHAP are the most often\nused techniques among them. This review stresses the need of multidisciplinary\napproaches combining financial knowledge with improved explainability paradigms\nand exposes important shortcomings in present XAI systems.",
    "categories": [
      "q-fin.GN",
      "cs.AI"
    ],
    "primary_category": "q-fin.GN",
    "comment": "2 tables, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05966v2",
    "published_date": "2025-03-07 22:36:44 UTC",
    "updated_date": "2025-03-17 15:37:42 UTC"
  },
  {
    "arxiv_id": "2503.05963v1",
    "title": "Bayesian Graph Traversal",
    "authors": [
      "William N. Caballero",
      "Phillip R. Jenkins",
      "David Banks",
      "Matthew Robbins"
    ],
    "abstract": "This research considers Bayesian decision-analytic approaches toward the\ntraversal of an uncertain graph. Namely, a traveler progresses over a graph in\nwhich rewards are gained upon a node's first visit and costs are incurred for\nevery edge traversal. The traveler knows the graph's adjacency matrix and his\nstarting position but does not know the rewards and costs. The traveler is a\nBayesian who encodes his beliefs about these values using a Gaussian process\nprior and who seeks to maximize his expected utility over these beliefs.\nAdopting a decision-analytic perspective, we develop sequential decision-making\nsolution strategies for this coupled information-collection and network-routing\nproblem. We show that the problem is NP-Hard and derive properties of the\noptimal walk. These properties provide heuristics for the traveler's problem\nthat balance exploration and exploitation. We provide a practical case study\nfocused on the use of unmanned aerial systems for public safety and empirically\nstudy policy performance in myriad Erdos-Renyi settings.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "stat.OT",
      "62C99, 68T20"
    ],
    "primary_category": "cs.AI",
    "comment": "26 pages, 7 tables, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05963v1",
    "published_date": "2025-03-07 22:05:06 UTC",
    "updated_date": "2025-03-07 22:05:06 UTC"
  },
  {
    "arxiv_id": "2503.05958v1",
    "title": "SANDWiCH: Semantical Analysis of Neighbours for Disambiguating Words in Context ad Hoc",
    "authors": [
      "Daniel Guzman-Olivares",
      "Lara Quijano-Sanchez",
      "Federico Liberatore"
    ],
    "abstract": "The rise of generative chat-based Large Language Models (LLMs) over the past\ntwo years has spurred a race to develop systems that promise near-human\nconversational and reasoning experiences. However, recent studies indicate that\nthe language understanding offered by these models remains limited and far from\nhuman-like performance, particularly in grasping the contextual meanings of\nwords, an essential aspect of reasoning. In this paper, we present a simple yet\ncomputationally efficient framework for multilingual Word Sense Disambiguation\n(WSD). Our approach reframes the WSD task as a cluster discrimination analysis\nover a semantic network refined from BabelNet using group algebra. We validate\nour methodology across multiple WSD benchmarks, achieving a new state of the\nart for all languages and tasks, as well as in individual assessments by part\nof speech. Notably, our model significantly surpasses the performance of\ncurrent alternatives, even in low-resource languages, while reducing the\nparameter count by 72%.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 2 figures, 7 tables, NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.05958v1",
    "published_date": "2025-03-07 21:52:32 UTC",
    "updated_date": "2025-03-07 21:52:32 UTC"
  },
  {
    "arxiv_id": "2503.05951v1",
    "title": "TPU-Gen: LLM-Driven Custom Tensor Processing Unit Generator",
    "authors": [
      "Deepak Vungarala",
      "Mohammed E. Elbtity",
      "Sumiya Syed",
      "Sakila Alam",
      "Kartik Pandit",
      "Arnob Ghosh",
      "Ramtin Zand",
      "Shaahin Angizi"
    ],
    "abstract": "The increasing complexity and scale of Deep Neural Networks (DNNs)\nnecessitate specialized tensor accelerators, such as Tensor Processing Units\n(TPUs), to meet various computational and energy efficiency requirements.\nNevertheless, designing optimal TPU remains challenging due to the high domain\nexpertise level, considerable manual design time, and lack of high-quality,\ndomain-specific datasets. This paper introduces TPU-Gen, the first Large\nLanguage Model (LLM) based framework designed to automate the exact and\napproximate TPU generation process, focusing on systolic array architectures.\nTPU-Gen is supported with a meticulously curated, comprehensive, and\nopen-source dataset that covers a wide range of spatial array designs and\napproximate multiply-and-accumulate units, enabling design reuse, adaptation,\nand customization for different DNN workloads. The proposed framework leverages\nRetrieval-Augmented Generation (RAG) as an effective solution for a data-scare\nhardware domain in building LLMs, addressing the most intriguing issue,\nhallucinations. TPU-Gen transforms high-level architectural specifications into\noptimized low-level implementations through an effective hardware generation\npipeline. Our extensive experimental evaluations demonstrate superior\nperformance, power, and area efficiency, with an average reduction in area and\npower of 92\\% and 96\\% from the manual optimization reference values. These\nresults set new standards for driving advancements in next-generation design\nautomation tools powered by LLMs.",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "primary_category": "cs.AR",
    "comment": "8 Pages, 9 Figures, 5 Tables",
    "pdf_url": "http://arxiv.org/pdf/2503.05951v1",
    "published_date": "2025-03-07 21:41:42 UTC",
    "updated_date": "2025-03-07 21:41:42 UTC"
  },
  {
    "arxiv_id": "2503.10655v1",
    "title": "Language modelling techniques for analysing the impact of human genetic variation",
    "authors": [
      "Megha Hegde",
      "Jean-Christophe Nebel",
      "Farzana Rahman"
    ],
    "abstract": "Interpreting the effects of variants within the human genome and proteome is\nessential for analysing disease risk, predicting medication response, and\ndeveloping personalised health interventions. Due to the intrinsic similarities\nbetween the structure of natural languages and genetic sequences, natural\nlanguage processing techniques have demonstrated great applicability in\ncomputational variant effect prediction. In particular, the advent of the\nTransformer has led to significant advancements in the field. However,\nTransformer-based models are not without their limitations, and a number of\nextensions and alternatives have been developed to improve results and enhance\ncomputational efficiency. This review explores the use of language models for\ncomputational variant effect prediction over the past decade, analysing the\nmain architectures, and identifying key trends and future directions.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10655v1",
    "published_date": "2025-03-07 21:34:17 UTC",
    "updated_date": "2025-03-07 21:34:17 UTC"
  },
  {
    "arxiv_id": "2503.05944v1",
    "title": "Enhancing Reasoning with Collaboration and Memory",
    "authors": [
      "Julie Michelman",
      "Nasrin Baratalipour",
      "Matthew Abueg"
    ],
    "abstract": "We envision a continuous collaborative learning system where groups of LLM\nagents work together to solve reasoning problems, drawing on memory they\ncollectively build to improve performance as they gain experience. This work\nestablishes the foundations for such a system by studying the interoperability\nof chain-of-thought reasoning styles, multi-agent collaboration, and memory\nbanks. Extending beyond the identical agents of self-consistency, we introduce\nvaried-context agents with diverse exemplars and a summarizer agent in place of\nvoting. We generate frozen and continuously learned memory banks of exemplars\nand pair them with fixed, random, and similarity-based retrieval mechanisms.\nOur systematic study reveals where various methods contribute to reasoning\nperformance of two LLMs on three grounded reasoning tasks, showing that random\nexemplar selection can often beat more principled approaches, and in some\ntasks, inclusion of any exemplars serves only to distract both weak and strong\nmodels.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "17 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05944v1",
    "published_date": "2025-03-07 21:19:21 UTC",
    "updated_date": "2025-03-07 21:19:21 UTC"
  },
  {
    "arxiv_id": "2503.05938v1",
    "title": "Uncertainty Quantification From Scaling Laws in Deep Neural Networks",
    "authors": [
      "Ibrahim Elsharkawy",
      "Yonatan Kahn",
      "Benjamin Hooberman"
    ],
    "abstract": "Quantifying the uncertainty from machine learning analyses is critical to\ntheir use in the physical sciences. In this work we focus on uncertainty\ninherited from the initialization distribution of neural networks. We compute\nthe mean $\\mu_{\\mathcal{L}}$ and variance $\\sigma_{\\mathcal{L}}^2$ of the test\nloss $\\mathcal{L}$ for an ensemble of multi-layer perceptrons (MLPs) with\nneural tangent kernel (NTK) initialization in the infinite-width limit, and\ncompare empirically to the results from finite-width networks for three example\ntasks: MNIST classification, CIFAR classification and calorimeter energy\nregression. We observe scaling laws as a function of training set size\n$N_\\mathcal{D}$ for both $\\mu_{\\mathcal{L}}$ and $\\sigma_{\\mathcal{L}}$, but\nfind that the coefficient of variation $\\epsilon_{\\mathcal{L}} \\equiv\n\\sigma_{\\mathcal{L}}/\\mu_{\\mathcal{L}}$ becomes independent of $N_\\mathcal{D}$\nat both infinite and finite width for sufficiently large $N_\\mathcal{D}$. This\nimplies that the coefficient of variation of a finite-width network may be\napproximated by its infinite-width value, and may in principle be calculable\nusing finite-width perturbation theory.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "hep-ex",
      "hep-ph",
      "hep-th"
    ],
    "primary_category": "cs.LG",
    "comment": "18+3 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05938v1",
    "published_date": "2025-03-07 21:15:11 UTC",
    "updated_date": "2025-03-07 21:15:11 UTC"
  },
  {
    "arxiv_id": "2503.05937v1",
    "title": "The Unified Control Framework: Establishing a Common Foundation for Enterprise AI Governance, Risk Management and Regulatory Compliance",
    "authors": [
      "Ian W. Eisenberg",
      "Lucía Gamboa",
      "Eli Sherman"
    ],
    "abstract": "The rapid adoption of AI systems presents enterprises with a dual challenge:\naccelerating innovation while ensuring responsible governance. Current AI\ngovernance approaches suffer from fragmentation, with risk management\nframeworks that focus on isolated domains, regulations that vary across\njurisdictions despite conceptual alignment, and high-level standards lacking\nconcrete implementation guidance. This fragmentation increases governance costs\nand creates a false dichotomy between innovation and responsibility. We propose\nthe Unified Control Framework (UCF): a comprehensive governance approach that\nintegrates risk management and regulatory compliance through a unified set of\ncontrols. The UCF consists of three key components: (1) a comprehensive risk\ntaxonomy synthesizing organizational and societal risks, (2) structured policy\nrequirements derived from regulations, and (3) a parsimonious set of 42\ncontrols that simultaneously address multiple risk scenarios and compliance\nrequirements. We validate the UCF by mapping it to the Colorado AI Act,\ndemonstrating how our approach enables efficient, adaptable governance that\nscales across regulations while providing concrete implementation guidance. The\nUCF reduces duplication of effort, ensures comprehensive coverage, and provides\na foundation for automation, enabling organizations to achieve responsible AI\ngovernance without sacrificing innovation speed.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05937v1",
    "published_date": "2025-03-07 21:14:49 UTC",
    "updated_date": "2025-03-07 21:14:49 UTC"
  },
  {
    "arxiv_id": "2503.05929v1",
    "title": "Audio-to-Image Encoding for Improved Voice Characteristic Detection Using Deep Convolutional Neural Networks",
    "authors": [
      "Youness Atif"
    ],
    "abstract": "This paper introduces a novel audio-to-image encoding framework that\nintegrates multiple dimensions of voice characteristics into a single RGB image\nfor speaker recognition. In this method, the green channel encodes raw audio\ndata, the red channel embeds statistical descriptors of the voice signal\n(including key metrics such as median and mean values for fundamental\nfrequency, spectral centroid, bandwidth, rolloff, zero-crossing rate, MFCCs,\nRMS energy, spectral flatness, spectral contrast, chroma, and harmonic-to-noise\nratio), and the blue channel comprises subframes representing these features in\na spatially organized format. A deep convolutional neural network trained on\nthese composite images achieves 98% accuracy in speaker classification across\ntwo speakers, suggesting that this integrated multi-channel representation can\nprovide a more discriminative input for voice recognition tasks.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "11 pages, 24 figures, 1 table, 3 algorithms. Submitted to\n  F1000Research",
    "pdf_url": "http://arxiv.org/pdf/2503.05929v1",
    "published_date": "2025-03-07 20:49:56 UTC",
    "updated_date": "2025-03-07 20:49:56 UTC"
  },
  {
    "arxiv_id": "2503.05925v1",
    "title": "ElementaryNet: A Non-Strategic Neural Network for Predicting Human Behavior in Normal-Form Games",
    "authors": [
      "Greg d'Eon",
      "Hala Murad",
      "Kevin Leyton-Brown",
      "James R. Wright"
    ],
    "abstract": "Models of human behavior in game-theoretic settings often distinguish between\nstrategic behavior, in which a player both reasons about how others will act\nand best responds to these beliefs, and \"level-0\" non-strategic behavior, in\nwhich they do not respond to explicit beliefs about others. The state of the\nart for predicting human behavior on unrepeated simultaneous-move games is\nGameNet, a neural network that learns extremely complex level-0 specifications\nfrom data. The current paper makes three contributions. First, it shows that\nGameNet's level-0 specifications are too powerful, because they are capable of\nstrategic reasoning. Second, it introduces a novel neural network architecture\n(dubbed ElementaryNet) and proves that it is only capable of nonstrategic\nbehavior. Third, it describes an extensive experimental evaluation of\nElementaryNet. Our overall findings are that (1) ElementaryNet dramatically\nunderperforms GameNet when neither model is allowed to explicitly model higher\nlevel agents who best-respond to the model's predictions, indicating that good\nperformance on our dataset requires a model capable of strategic reasoning; (2)\nthat the two models achieve statistically indistinguishable performance when\nsuch higher-level agents are introduced, meaning that ElementaryNet's\nrestriction to a non-strategic level-0 specification does not degrade model\nperformance; and (3) that this continues to hold even when ElementaryNet is\nrestricted to a set of level-0 building blocks previously introduced in the\nliterature, with only the functional form being learned by the neural network.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages. Submitted to EC 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.05925v1",
    "published_date": "2025-03-07 20:47:16 UTC",
    "updated_date": "2025-03-07 20:47:16 UTC"
  },
  {
    "arxiv_id": "2503.05920v1",
    "title": "IDEA Prune: An Integrated Enlarge-and-Prune Pipeline in Generative Language Model Pretraining",
    "authors": [
      "Yixiao Li",
      "Xianzhi Du",
      "Ajay Jaiswal",
      "Tao Lei",
      "Tuo Zhao",
      "Chong Wang",
      "Jianyu Wang"
    ],
    "abstract": "Recent advancements in large language models have intensified the need for\nefficient and deployable models within limited inference budgets. Structured\npruning pipelines have shown promise in token efficiency compared to training\ntarget-size models from scratch. In this paper, we advocate incorporating\nenlarged model pretraining, which is often ignored in previous works, into\npruning. We study the enlarge-and-prune pipeline as an integrated system to\naddress two critical questions: whether it is worth pretraining an enlarged\nmodel even when the model is never deployed, and how to optimize the entire\npipeline for better pruned models. We propose an integrated enlarge-and-prune\npipeline, which combines enlarge model training, pruning, and recovery under a\nsingle cosine annealing learning rate schedule. This approach is further\ncomplemented by a novel iterative structured pruning method for gradual\nparameter removal. The proposed method helps to mitigate the knowledge loss\ncaused by the rising learning rate in naive enlarge-and-prune pipelines and\nenable effective redistribution of model capacity among surviving neurons,\nfacilitating smooth compression and enhanced performance. We conduct\ncomprehensive experiments on compressing 2.8B models to 1.3B with up to 2T\ntokens in pretraining. It demonstrates the integrated approach not only\nprovides insights into the token efficiency of enlarged model pretraining but\nalso achieves superior performance of pruned models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05920v1",
    "published_date": "2025-03-07 20:35:31 UTC",
    "updated_date": "2025-03-07 20:35:31 UTC"
  },
  {
    "arxiv_id": "2503.05916v1",
    "title": "SAS: Segment Anything Small for Ultrasound -- A Non-Generative Data Augmentation Technique for Robust Deep Learning in Ultrasound Imaging",
    "authors": [
      "Danielle L. Ferreira",
      "Ahana Gangopadhyay",
      "Hsi-Ming Chang",
      "Ravi Soni",
      "Gopal Avinash"
    ],
    "abstract": "Accurate segmentation of anatomical structures in ultrasound (US) images,\nparticularly small ones, is challenging due to noise and variability in imaging\nconditions (e.g., probe position, patient anatomy, tissue characteristics and\npathology). To address this, we introduce Segment Anything Small (SAS), a\nsimple yet effective scale- and texture-aware data augmentation technique\ndesigned to enhance the performance of deep learning models for segmenting\nsmall anatomical structures in ultrasound images. SAS employs a dual\ntransformation strategy: (1) simulating diverse organ scales by resizing and\nembedding organ thumbnails into a black background, and (2) injecting noise\ninto regions of interest to simulate varying tissue textures. These\ntransformations generate realistic and diverse training data without\nintroducing hallucinations or artifacts, improving the model's robustness to\nnoise and variability. We fine-tuned a promptable foundation model on a\ncontrolled organ-specific medical imaging dataset and evaluated its performance\non one internal and five external datasets. Experimental results demonstrate\nsignificant improvements in segmentation performance, with Dice score gains of\nup to 0.35 and an average improvement of 0.16 [95% CI 0.132,0.188].\nAdditionally, our iterative point prompts provide precise control and adaptive\nrefinement, achieving performance comparable to bounding box prompts with just\ntwo points. SAS enhances model robustness and generalizability across diverse\nanatomical structures and imaging conditions, particularly for small\nstructures, without compromising the accuracy of larger ones. By offering a\ncomputationally efficient solution that eliminates the need for extensive human\nlabeling efforts, SAS emerges as a powerful tool for advancing medical image\nanalysis, particularly in resource-constrained settings.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "25 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05916v1",
    "published_date": "2025-03-07 20:24:35 UTC",
    "updated_date": "2025-03-07 20:24:35 UTC"
  },
  {
    "arxiv_id": "2503.10654v1",
    "title": "Improving RAG Retrieval via Propositional Content Extraction: a Speech Act Theory Approach",
    "authors": [
      "João Alberto de Oliveira Lima"
    ],
    "abstract": "When users formulate queries, they often include not only the information\nthey seek, but also pragmatic markers such as interrogative phrasing or polite\nrequests. Although these speech act indicators communicate the\nuser\\textquotesingle s intent -- whether it is asking a question, making a\nrequest, or stating a fact -- they do not necessarily add to the core\ninformational content of the query itself. This paper investigates whether\nextracting the underlying propositional content from user utterances --\nessentially stripping away the linguistic markers of intent -- can improve\nretrieval quality in Retrieval-Augmented Generation (RAG) systems. Drawing upon\nfoundational insights from speech act theory, we propose a practical method for\nautomatically transforming queries into their propositional equivalents before\nembedding. To assess the efficacy of this approach, we conducted an\nexperimental study involving 63 user queries related to a Brazilian\ntelecommunications news corpus with precomputed semantic embeddings. Results\ndemonstrate clear improvements in semantic similarity between query embeddings\nand document embeddings at top ranks, confirming that queries stripped of\nspeech act indicators more effectively retrieve relevant content.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "19 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.10654v1",
    "published_date": "2025-03-07 20:15:40 UTC",
    "updated_date": "2025-03-07 20:15:40 UTC"
  },
  {
    "arxiv_id": "2503.10653v1",
    "title": "Video Anomaly Detection with Structured Keywords",
    "authors": [
      "Thomas Foltz"
    ],
    "abstract": "This paper focuses on detecting anomalies in surveillance video using\nkeywords by leveraging foundational models' feature representation\ngeneralization capabilities. We present a novel, lightweight pipeline for\nanomaly classification using keyword weights. Our pipeline employs a two-stage\nprocess: induction followed by deduction. In induction, descriptions are\ngenerated from normal and anomalous frames to identify and assign weights to\nrelevant keywords. In deduction, inference frame descriptions are converted\ninto keyword encodings using induction-derived weights for input into our\nneural network for anomaly classification. We achieved comparable performance\non the three benchmarks UCSD Ped2, Shanghai Tech, and CUHK Avenue, with ROC AUC\nscores of 0.865, 0.745, and 0.742, respectively. These results are achieved\nwithout temporal context, making such a system viable for real-time\napplications. Our model improves implementation setup, interpretability, and\ninference speed for surveillance devices on the edge, introducing a performance\ntrade-off against other video anomaly detection systems. As the generalization\ncapabilities of open-source foundational models improve, our model demonstrates\nthat the exclusive use of text for feature representations is a promising\ndirection for efficient real-time interpretable video anomaly detection.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10653v1",
    "published_date": "2025-03-07 20:05:59 UTC",
    "updated_date": "2025-03-07 20:05:59 UTC"
  },
  {
    "arxiv_id": "2503.05899v1",
    "title": "Towards Understanding the Use of MLLM-Enabled Applications for Visual Interpretation by Blind and Low Vision People",
    "authors": [
      "Ricardo E. Gonzalez Penuela",
      "Ruiying Hu",
      "Sharon Lin",
      "Tanisha Shende",
      "Shiri Azenkot"
    ],
    "abstract": "Blind and Low Vision (BLV) people have adopted AI-powered visual\ninterpretation applications to address their daily needs. While these\napplications have been helpful, prior work has found that users remain\nunsatisfied by their frequent errors. Recently, multimodal large language\nmodels (MLLMs) have been integrated into visual interpretation applications,\nand they show promise for more descriptive visual interpretations. However, it\nis still unknown how this advancement has changed people's use of these\napplications. To address this gap, we conducted a two-week diary study in which\n20 BLV people used an MLLM-enabled visual interpretation application we\ndeveloped, and we collected 553 entries. In this paper, we report a preliminary\nanalysis of 60 diary entries from 6 participants. We found that participants\nconsidered the application's visual interpretations trustworthy (mean 3.75 out\nof 5) and satisfying (mean 4.15 out of 5). Moreover, participants trusted our\napplication in high-stakes scenarios, such as receiving medical dosage advice.\nWe discuss our plan to complete our analysis to inform the design of future\nMLLM-enabled visual interpretation systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "I.2.1; H.5.2"
    ],
    "primary_category": "cs.HC",
    "comment": "8 pages, 1 figure, 4 tables, to appear at CHI 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.05899v1",
    "published_date": "2025-03-07 19:38:14 UTC",
    "updated_date": "2025-03-07 19:38:14 UTC"
  },
  {
    "arxiv_id": "2503.05893v1",
    "title": "Zero-shot Medical Event Prediction Using a Generative Pre-trained Transformer on Electronic Health Records",
    "authors": [
      "Ekaterina Redekop",
      "Zichen Wang",
      "Rushikesh Kulkarni",
      "Mara Pleasure",
      "Aaron Chin",
      "Hamid Reza Hassanzadeh",
      "Brian L. Hill",
      "Melika Emami",
      "William Speier",
      "Corey W. Arnold"
    ],
    "abstract": "Longitudinal data in electronic health records (EHRs) represent an\nindividual`s clinical history through a sequence of codified concepts,\nincluding diagnoses, procedures, medications, and laboratory tests.\nFoundational models, such as generative pre-trained transformers (GPT), can\nleverage this data to predict future events. While fine-tuning of these models\nenhances task-specific performance, it is costly, complex, and unsustainable\nfor every target. We show that a foundation model trained on EHRs can perform\npredictive tasks in a zero-shot manner, eliminating the need for fine-tuning.\n  This study presents the first comprehensive analysis of zero-shot forecasting\nwith GPT-based foundational models in EHRs, introducing a novel pipeline that\nformulates medical concept prediction as a generative modeling task. Unlike\nsupervised approaches requiring extensive labeled data, our method enables the\nmodel to forecast a next medical event purely from a pretraining knowledge. We\nevaluate performance across multiple time horizons and clinical categories,\ndemonstrating model`s ability to capture latent temporal dependencies and\ncomplex patient trajectories without task supervision.\n  Model performance for predicting the next medical concept was evaluated using\nprecision and recall metrics, achieving an average top1 precision of 0.614 and\nrecall of 0.524. For 12 major diagnostic conditions, the model demonstrated\nstrong zero-shot performance, achieving high true positive rates while\nmaintaining low false positives.\n  We demonstrate the power of a foundational EHR GPT model in capturing diverse\nphenotypes and enabling robust, zero-shot forecasting of clinical outcomes.\nThis capability enhances the versatility of predictive healthcare models and\nreduces the need for task-specific training, enabling more scalable\napplications in clinical settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05893v1",
    "published_date": "2025-03-07 19:26:47 UTC",
    "updated_date": "2025-03-07 19:26:47 UTC"
  },
  {
    "arxiv_id": "2503.05888v1",
    "title": "QG-SMS: Enhancing Test Item Analysis via Student Modeling and Simulation",
    "authors": [
      "Bang Nguyen",
      "Tingting Du",
      "Mengxia Yu",
      "Lawrence Angrave",
      "Meng Jiang"
    ],
    "abstract": "While the Question Generation (QG) task has been increasingly adopted in\neducational assessments, its evaluation remains limited by approaches that lack\na clear connection to the educational values of test items. In this work, we\nintroduce test item analysis, a method frequently used by educators to assess\ntest question quality, into QG evaluation. Specifically, we construct pairs of\ncandidate questions that differ in quality across dimensions such as topic\ncoverage, item difficulty, item discrimination, and distractor efficiency. We\nthen examine whether existing QG evaluation approaches can effectively\ndistinguish these differences. Our findings reveal significant shortcomings in\nthese approaches with respect to accurately assessing test item quality in\nrelation to student performance. To address this gap, we propose a novel QG\nevaluation framework, QG-SMS, which leverages Large Language Model for Student\nModeling and Simulation to perform test item analysis. As demonstrated in our\nextensive experiments and human evaluation study, the additional perspectives\nintroduced by the simulated student profiles lead to a more effective and\nrobust assessment of test items.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2503.05888v1",
    "published_date": "2025-03-07 19:21:59 UTC",
    "updated_date": "2025-03-07 19:21:59 UTC"
  },
  {
    "arxiv_id": "2503.05696v2",
    "title": "Multi-Fidelity Policy Gradient Algorithms",
    "authors": [
      "Xinjie Liu",
      "Cyrus Neary",
      "Kushagra Gupta",
      "Christian Ellis",
      "Ufuk Topcu",
      "David Fridovich-Keil"
    ],
    "abstract": "Many reinforcement learning (RL) algorithms require large amounts of data,\nprohibiting their use in applications where frequent interactions with\noperational systems are infeasible, or high-fidelity simulations are expensive\nor unavailable. Meanwhile, low-fidelity simulators--such as reduced-order\nmodels, heuristic reward functions, or generative world models--can cheaply\nprovide useful data for RL training, even if they are too coarse for direct\nsim-to-real transfer. We propose multi-fidelity policy gradients (MFPGs), an RL\nframework that mixes a small amount of data from the target environment with a\nlarge volume of low-fidelity simulation data to form unbiased, reduced-variance\nestimators (control variates) for on-policy policy gradients. We instantiate\nthe framework by developing multi-fidelity variants of two policy gradient\nalgorithms: REINFORCE and proximal policy optimization. Experimental results\nacross a suite of simulated robotics benchmark problems demonstrate that when\ntarget-environment samples are limited, MFPG achieves up to 3.9x higher reward\nand improves training stability when compared to baselines that only use\nhigh-fidelity data. Moreover, even when the baselines are given more\nhigh-fidelity samples--up to 10x as many interactions with the target\nenvironment--MFPG continues to match or outperform them. Finally, we observe\nthat MFPG is capable of training effective policies even when the low-fidelity\nenvironment is drastically different from the target environment. MFPG thus not\nonly offers a novel paradigm for efficient sim-to-real transfer but also\nprovides a principled approach to managing the trade-off between policy\nperformance and data collection costs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05696v2",
    "published_date": "2025-03-07 18:58:23 UTC",
    "updated_date": "2025-04-09 15:52:25 UTC"
  },
  {
    "arxiv_id": "2503.05860v1",
    "title": "Benchmarking AI Models in Software Engineering: A Review, Search Tool, and Enhancement Protocol",
    "authors": [
      "Roham Koohestani",
      "Philippe de Bekker",
      "Maliheh Izadi"
    ],
    "abstract": "Benchmarks are essential for consistent evaluation and reproducibility. The\nintegration of Artificial Intelligence into Software Engineering (AI4SE) has\ngiven rise to numerous benchmarks for tasks such as code generation and bug\nfixing. However, this surge presents challenges: (1) scattered benchmark\nknowledge across tasks, (2) difficulty in selecting relevant benchmarks, (3)\nthe absence of a uniform standard for benchmark development, and (4)\nlimitations of existing benchmarks. In this paper, we review 173 studies and\nidentify 204 AI4SE benchmarks. We classify these benchmarks, analyze their\nlimitations, and expose gaps in practices. Based on our review, we created\nBenchScout, a semantic search tool to find relevant benchmarks, using automated\nclustering of the contexts from associated studies. We conducted a user study\nwith 22 participants to evaluate BenchScout's usability, effectiveness, and\nintuitiveness which resulted in average scores of 4.5, 4.0, and 4.1 out of 5.\nTo advance benchmarking standards, we propose BenchFrame, a unified method to\nenhance benchmark quality. As a case study, we applied BenchFrame to the\nHumanEval benchmark and addressed its main limitations. This led to\nHumanEvalNext, featuring (1) corrected errors, (2) improved language\nconversion, (3) expanded test coverage, and (4) increased difficulty. We then\nevaluated ten state-of-the-art code language models on HumanEval,\nHumanEvalPlus, and HumanEvalNext. On HumanEvalNext, models showed a pass@1\nscore reduction of 31.22% and 19.94% compared to HumanEval and HumanEvalPlus,\nrespectively.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05860v1",
    "published_date": "2025-03-07 18:44:32 UTC",
    "updated_date": "2025-03-07 18:44:32 UTC"
  },
  {
    "arxiv_id": "2503.05859v1",
    "title": "Quantum-like cognition and decision making in the light of quantum measurement theory",
    "authors": [
      "Miho Fuyama",
      "Andrei Khrennikov",
      "Masanao Ozawa"
    ],
    "abstract": "We characterize the class of quantum measurements that matches the\napplications of quantum theory to cognition (and decision making) -\nquantum-like modeling. Projective measurements describe the canonical\nmeasurements of the basic observables of quantum physics. However, the\ncombinations of the basic cognitive effects, such as the question order and\nresponse replicability effects, cannot be described by projective measurements.\nWe motivate the use of the special class of quantum measurements, namely {\\it\nsharp repeatable non-projective measurements} - ${\\cal SR\\bar{P}}. $ This class\nis practically unused in quantum physics. Thus, physics and cognition explore\ndifferent parts of quantum measurement theory. Quantum-like modeling isn't\nautomatic borrowing of the quantum formalism. Exploring the class ${\\cal\nSR\\bar{P}}$ highlights the role of {\\it noncommutativity of the state update\nmaps generated by measurement back action.} Thus, ``non-classicality'' in\nquantum physics as well as quantum-like modeling for cognition is based on two\ndifferent types of noncommutativity, of operators (observables) and instruments\n(state update maps): {\\it observable-noncommutativity} vs. {\\it state\nupdate-noncommutativity}. We speculate that distinguishing quantum-like\nproperties of the cognitive effects are the expressions of the latter, or\npossibly both.",
    "categories": [
      "cs.AI",
      "physics.bio-ph",
      "quant-ph"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05859v1",
    "published_date": "2025-03-07 18:30:44 UTC",
    "updated_date": "2025-03-07 18:30:44 UTC"
  },
  {
    "arxiv_id": "2503.05652v1",
    "title": "BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities",
    "authors": [
      "Yunfan Jiang",
      "Ruohan Zhang",
      "Josiah Wong",
      "Chen Wang",
      "Yanjie Ze",
      "Hang Yin",
      "Cem Gokmen",
      "Shuran Song",
      "Jiajun Wu",
      "Li Fei-Fei"
    ],
    "abstract": "Real-world household tasks present significant challenges for mobile\nmanipulation robots. An analysis of existing robotics benchmarks reveals that\nsuccessful task performance hinges on three key whole-body control\ncapabilities: bimanual coordination, stable and precise navigation, and\nextensive end-effector reachability. Achieving these capabilities requires\ncareful hardware design, but the resulting system complexity further\ncomplicates visuomotor policy learning. To address these challenges, we\nintroduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework for\nwhole-body manipulation in diverse household tasks. Built on a bimanual,\nwheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-body\nteleoperation interface for data collection and a novel algorithm for learning\nwhole-body visuomotor policies. We evaluate BRS on five challenging household\ntasks that not only emphasize the three core capabilities but also introduce\nadditional complexities, such as long-range navigation, interaction with\narticulated and deformable objects, and manipulation in confined spaces. We\nbelieve that BRS's integrated robotic embodiment, data collection interface,\nand learning framework mark a significant step toward enabling real-world\nwhole-body manipulation for everyday household tasks. BRS is open-sourced at\nhttps://behavior-robot-suite.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website: https://behavior-robot-suite.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.05652v1",
    "published_date": "2025-03-07 18:15:21 UTC",
    "updated_date": "2025-03-07 18:15:21 UTC"
  },
  {
    "arxiv_id": "2503.05646v1",
    "title": "dARt Vinci: Egocentric Data Collection for Surgical Robot Learning at Scale",
    "authors": [
      "Yihao Liu",
      "Yu-Chun Ku",
      "Jiaming Zhang",
      "Hao Ding",
      "Peter Kazanzides",
      "Mehran Armand"
    ],
    "abstract": "Data scarcity has long been an issue in the robot learning community.\nParticularly, in safety-critical domains like surgical applications, obtaining\nhigh-quality data can be especially difficult. It poses challenges to\nresearchers seeking to exploit recent advancements in reinforcement learning\nand imitation learning, which have greatly improved generalizability and\nenabled robots to conduct tasks autonomously. We introduce dARt Vinci, a\nscalable data collection platform for robot learning in surgical settings. The\nsystem uses Augmented Reality (AR) hand tracking and a high-fidelity physics\nengine to capture subtle maneuvers in primitive surgical tasks: By eliminating\nthe need for a physical robot setup and providing flexibility in terms of time,\nspace, and hardware resources-such as multiview sensors and\nactuators-specialized simulation is a viable alternative. At the same time, AR\nallows the robot data collection to be more egocentric, supported by its body\ntracking and content overlaying capabilities. Our user study confirms the\nproposed system's efficiency and usability, where we use widely-used primitive\ntasks for training teleoperation with da Vinci surgical robots. Data throughput\nimproves across all tasks compared to real robot settings by 41% on average.\nThe total experiment time is reduced by an average of 10%. The temporal demand\nin the task load survey is improved. These gains are statistically significant.\nAdditionally, the collected data is over 400 times smaller in size, requiring\nfar less storage while achieving double the frequency.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05646v1",
    "published_date": "2025-03-07 18:07:54 UTC",
    "updated_date": "2025-03-07 18:07:54 UTC"
  },
  {
    "arxiv_id": "2503.05641v2",
    "title": "Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for Heterogeneous Reasoning",
    "authors": [
      "Justin Chih-Yao Chen",
      "Sukwon Yun",
      "Elias Stengel-Eskin",
      "Tianlong Chen",
      "Mohit Bansal"
    ],
    "abstract": "Combining existing pre-trained expert LLMs is a promising avenue for scalably\ntackling large-scale and diverse tasks. However, selecting experts at the task\nlevel is often too coarse-grained, as heterogeneous tasks may require different\nexpertise for each instance. To enable adaptive instance-level mixing of\npre-trained LLM experts, we propose Symbolic-MoE, a symbolic, text-based, and\ngradient-free Mixture-of-Experts framework. Symbolic-MoE takes a fine-grained\napproach to selection by emphasizing skills, e.g., algebra in math or molecular\nbiology in biomedical reasoning. We propose a skill-based recruiting strategy\nthat dynamically selects the most relevant set of expert LLMs for diverse\nreasoning tasks based on their strengths. Each selected expert then generates\nits own reasoning, resulting in k outputs from k experts, which are then\nsynthesized into a final high-quality response by an aggregator chosen based on\nits ability to integrate diverse reasoning outputs. We show that Symbolic-MoE's\ninstance-level expert selection improves performance by a large margin but --\nwhen implemented naively -- can introduce a high computational overhead due to\nthe need for constant model loading and offloading. To address this, we\nimplement a batch inference strategy that groups instances based on their\nassigned experts, loading each model only once. This allows us to integrate 16\nexpert models on 1 GPU with a time cost comparable to or better than prior\nmulti-agent baselines using 4 GPUs. Through extensive evaluations on diverse\nbenchmarks (MMLU-Pro, GPQA, AIME, and MedMCQA), we demonstrate that\nSymbolic-MoE outperforms strong LLMs like GPT4o-mini, as well as multi-agent\napproaches, with an absolute average improvement of 8.15% over the best\nmulti-agent baseline. Moreover, Symbolic-MoE removes the need for expensive\nmulti-round discussions, outperforming discussion baselines with less\ncomputation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "The first three authors contributed equally. Project Page:\n  https://symbolic-moe.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.05641v2",
    "published_date": "2025-03-07 18:03:13 UTC",
    "updated_date": "2025-03-11 21:40:43 UTC"
  },
  {
    "arxiv_id": "2503.05639v3",
    "title": "VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control",
    "authors": [
      "Yuxuan Bian",
      "Zhaoyang Zhang",
      "Xuan Ju",
      "Mingdeng Cao",
      "Liangbin Xie",
      "Ying Shan",
      "Qiang Xu"
    ],
    "abstract": "Video inpainting, which aims to restore corrupted video content, has\nexperienced substantial progress. Despite these advances, existing methods,\nwhether propagating unmasked region pixels through optical flow and receptive\nfield priors, or extending image-inpainting models temporally, face challenges\nin generating fully masked objects or balancing the competing objectives of\nbackground context preservation and foreground generation in one model,\nrespectively. To address these limitations, we propose a novel dual-stream\nparadigm VideoPainter that incorporates an efficient context encoder\n(comprising only 6% of the backbone parameters) to process masked videos and\ninject backbone-aware background contextual cues to any pre-trained video DiT,\nproducing semantically consistent content in a plug-and-play manner. This\narchitectural separation significantly reduces the model's learning complexity\nwhile enabling nuanced integration of crucial background context. We also\nintroduce a novel target region ID resampling technique that enables any-length\nvideo inpainting, greatly enhancing our practical applicability. Additionally,\nwe establish a scalable dataset pipeline leveraging current vision\nunderstanding models, contributing VPData and VPBench to facilitate\nsegmentation-based inpainting training and assessment, the largest video\ninpainting dataset and benchmark to date with over 390K diverse clips. Using\ninpainting as a pipeline basis, we also explore downstream applications\nincluding video editing and video editing pair data generation, demonstrating\ncompetitive performance and significant practical potential. Extensive\nexperiments demonstrate VideoPainter's superior performance in both any-length\nvideo inpainting and editing, across eight key metrics, including video\nquality, mask region preservation, and textual coherence.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page available at\n  https://yxbian23.github.io/project/video-painter",
    "pdf_url": "http://arxiv.org/pdf/2503.05639v3",
    "published_date": "2025-03-07 17:59:46 UTC",
    "updated_date": "2025-04-09 02:05:33 UTC"
  },
  {
    "arxiv_id": "2503.05638v1",
    "title": "TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models",
    "authors": [
      "Mark YU",
      "Wenbo Hu",
      "Jinbo Xing",
      "Ying Shan"
    ],
    "abstract": "We present TrajectoryCrafter, a novel approach to redirect camera\ntrajectories for monocular videos. By disentangling deterministic view\ntransformations from stochastic content generation, our method achieves precise\ncontrol over user-specified camera trajectories. We propose a novel dual-stream\nconditional video diffusion model that concurrently integrates point cloud\nrenders and source videos as conditions, ensuring accurate view transformations\nand coherent 4D content generation. Instead of leveraging scarce multi-view\nvideos, we curate a hybrid training dataset combining web-scale monocular\nvideos with static multi-view datasets, by our innovative double-reprojection\nstrategy, significantly fostering robust generalization across diverse scenes.\nExtensive evaluations on multi-view and large-scale monocular videos\ndemonstrate the superior performance of our method.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project webpage: https://trajectorycrafter.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.05638v1",
    "published_date": "2025-03-07 17:57:53 UTC",
    "updated_date": "2025-03-07 17:57:53 UTC"
  },
  {
    "arxiv_id": "2503.05629v1",
    "title": "Exploring FMCW Radars and Feature Maps for Activity Recognition: A Benchmark Study",
    "authors": [
      "Ali Samimi Fard",
      "Mohammadreza Mashhadigholamali",
      "Samaneh Zolfaghari",
      "Hajar Abedi",
      "Mainak Chakraborty",
      "Luigi Borzì",
      "Masoud Daneshtalab",
      "George Shaker"
    ],
    "abstract": "Human Activity Recognition has gained significant attention due to its\ndiverse applications, including ambient assisted living and remote sensing.\nWearable sensor-based solutions often suffer from user discomfort and\nreliability issues, while video-based methods raise privacy concerns and\nperform poorly in low-light conditions or long ranges. This study introduces a\nFrequency-Modulated Continuous Wave radar-based framework for human activity\nrecognition, leveraging a 60 GHz radar and multi-dimensional feature maps.\nUnlike conventional approaches that process feature maps as images, this study\nfeeds multi-dimensional feature maps -- Range-Doppler, Range-Azimuth, and\nRange-Elevation -- as data vectors directly into the machine learning (SVM,\nMLP) and deep learning (CNN, LSTM, ConvLSTM) models, preserving the spatial and\ntemporal structures of the data. These features were extracted from a novel\ndataset with seven activity classes and validated using two different\nvalidation approaches. The ConvLSTM model outperformed conventional machine\nlearning and deep learning models, achieving an accuracy of 90.51% and an\nF1-score of 87.31% on cross-scene validation and an accuracy of 89.56% and an\nF1-score of 87.15% on leave-one-person-out cross-validation. The results\nhighlight the approach's potential for scalable, non-intrusive, and\nprivacy-preserving activity monitoring in real-world scenarios.",
    "categories": [
      "cs.ET",
      "cs.AI"
    ],
    "primary_category": "cs.ET",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05629v1",
    "published_date": "2025-03-07 17:53:29 UTC",
    "updated_date": "2025-03-07 17:53:29 UTC"
  },
  {
    "arxiv_id": "2503.05628v2",
    "title": "Superintelligence Strategy: Expert Version",
    "authors": [
      "Dan Hendrycks",
      "Eric Schmidt",
      "Alexandr Wang"
    ],
    "abstract": "Rapid advances in AI are beginning to reshape national security.\nDestabilizing AI developments could rupture the balance of power and raise the\nodds of great-power conflict, while widespread proliferation of capable AI\nhackers and virologists would lower barriers for rogue actors to cause\ncatastrophe. Superintelligence -- AI vastly better than humans at nearly all\ncognitive tasks -- is now anticipated by AI researchers. Just as nations once\ndeveloped nuclear strategies to secure their survival, we now need a coherent\nsuperintelligence strategy to navigate a new period of transformative change.\nWe introduce the concept of Mutual Assured AI Malfunction (MAIM): a deterrence\nregime resembling nuclear mutual assured destruction (MAD) where any state's\naggressive bid for unilateral AI dominance is met with preventive sabotage by\nrivals. Given the relative ease of sabotaging a destabilizing AI project --\nthrough interventions ranging from covert cyberattacks to potential kinetic\nstrikes on datacenters -- MAIM already describes the strategic picture AI\nsuperpowers find themselves in. Alongside this, states can increase their\ncompetitiveness by bolstering their economies and militaries through AI, and\nthey can engage in nonproliferation to rogue actors to keep weaponizable AI\ncapabilities out of their hands. Taken together, the three-part framework of\ndeterrence, nonproliferation, and competitiveness outlines a robust strategy to\nsuperintelligence in the years ahead.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "https://nationalsecurity.ai/",
    "pdf_url": "http://arxiv.org/pdf/2503.05628v2",
    "published_date": "2025-03-07 17:53:24 UTC",
    "updated_date": "2025-04-14 21:28:19 UTC"
  },
  {
    "arxiv_id": "2503.05626v1",
    "title": "FMT:A Multimodal Pneumonia Detection Model Based on Stacking MOE Framework",
    "authors": [
      "Jingyu Xu",
      "Yang Wang"
    ],
    "abstract": "Artificial intelligence has shown the potential to improve diagnostic\naccuracy through medical image analysis for pneumonia diagnosis. However,\ntraditional multimodal approaches often fail to address real-world challenges\nsuch as incomplete data and modality loss. In this study, a Flexible Multimodal\nTransformer (FMT) was proposed, which uses ResNet-50 and BERT for joint\nrepresentation learning, followed by a dynamic masked attention strategy that\nsimulates clinical modality loss to improve robustness; finally, a sequential\nmixture of experts (MOE) architecture was used to achieve multi-level decision\nrefinement. After evaluation on a small multimodal pneumonia dataset, FMT\nachieved state-of-the-art performance with 94% accuracy, 95% recall, and 93% F1\nscore, outperforming single-modal baselines (ResNet: 89%; BERT: 79%) and the\nmedical benchmark CheXMed (90%), providing a scalable solution for multimodal\ndiagnosis of pneumonia in resource-constrained medical settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05626v1",
    "published_date": "2025-03-07 17:52:12 UTC",
    "updated_date": "2025-03-07 17:52:12 UTC"
  },
  {
    "arxiv_id": "2503.05620v1",
    "title": "Learning LLM Preference over Intra-Dialogue Pairs: A Framework for Utterance-level Understandings",
    "authors": [
      "Xuanqing Liu",
      "Luyang Kong",
      "Wei Niu",
      "Afshin Khashei",
      "Belinda Zeng",
      "Steve Johnson",
      "Jon Jay",
      "Davor Golac",
      "Matt Pope"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nhandling complex dialogue tasks without requiring use case-specific\nfine-tuning. However, analyzing live dialogues in real-time necessitates\nlow-latency processing systems, making it impractical to deploy models with\nbillions of parameters due to latency constraints. As a result, practitioners\noften prefer smaller models with millions of parameters, trained on\nhigh-quality, human-annotated datasets. Yet, curating such datasets is both\ntime-consuming and costly. Consequently, there is a growing need to combine the\nscalability of LLM-generated labels with the precision of human annotations,\nenabling fine-tuned smaller models to achieve both higher speed and accuracy\ncomparable to larger models. In this paper, we introduce a simple yet effective\nframework to address this challenge. Our approach is specifically designed for\nper-utterance classification problems, which encompass tasks such as intent\ndetection, dialogue state tracking, and more. To mitigate the impact of\nlabeling errors from LLMs -- the primary source of inaccuracies in student\nmodels -- we propose a noise-reduced preference learning loss. Experimental\nresults demonstrate that our method significantly improves accuracy across\nutterance-level dialogue tasks, including sentiment detection (over $2\\%$),\ndialogue act classification (over $1.5\\%$), etc.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "7 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05620v1",
    "published_date": "2025-03-07 17:46:13 UTC",
    "updated_date": "2025-03-07 17:46:13 UTC"
  },
  {
    "arxiv_id": "2503.05613v1",
    "title": "A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models",
    "authors": [
      "Dong Shu",
      "Xuansheng Wu",
      "Haiyan Zhao",
      "Daking Rai",
      "Ziyu Yao",
      "Ninghao Liu",
      "Mengnan Du"
    ],
    "abstract": "Large Language Models (LLMs) have revolutionized natural language processing,\nyet their internal mechanisms remain largely opaque. Recently, mechanistic\ninterpretability has attracted significant attention from the research\ncommunity as a means to understand the inner workings of LLMs. Among various\nmechanistic interpretability approaches, Sparse Autoencoders (SAEs) have\nemerged as a particularly promising method due to their ability to disentangle\nthe complex, superimposed features within LLMs into more interpretable\ncomponents. This paper presents a comprehensive examination of SAEs as a\npromising approach to interpreting and understanding LLMs. We provide a\nsystematic overview of SAE principles, architectures, and applications\nspecifically tailored for LLM analysis, covering theoretical foundations,\nimplementation strategies, and recent developments in sparsity mechanisms. We\nalso explore how SAEs can be leveraged to explain the internal workings of\nLLMs, steer model behaviors in desired directions, and develop more transparent\ntraining methodologies for future models. Despite the challenges that remain\naround SAE implementation and scaling, they continue to provide valuable tools\nfor understanding the internal mechanisms of large language models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "20 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05613v1",
    "published_date": "2025-03-07 17:38:00 UTC",
    "updated_date": "2025-03-07 17:38:00 UTC"
  },
  {
    "arxiv_id": "2503.05604v1",
    "title": "CACTUS: An Open Dataset and Framework for Automated Cardiac Assessment and Classification of Ultrasound Images Using Deep Transfer Learning",
    "authors": [
      "Hanae Elmekki",
      "Ahmed Alagha",
      "Hani Sami",
      "Amanda Spilkin",
      "Antonela Mariel Zanuttini",
      "Ehsan Zakeri",
      "Jamal Bentahar",
      "Lyes Kadem",
      "Wen-Fang Xie",
      "Philippe Pibarot",
      "Rabeb Mizouni",
      "Hadi Otrok",
      "Shakti Singh",
      "Azzam Mourad"
    ],
    "abstract": "Cardiac ultrasound (US) scanning is a commonly used techniques in cardiology\nto diagnose the health of the heart and its proper functioning. Therefore, it\nis necessary to consider ways to automate these tasks and assist medical\nprofessionals in classifying and assessing cardiac US images. Machine learning\n(ML) techniques are regarded as a prominent solution due to their success in\nnumerous applications aimed at enhancing the medical field, including\naddressing the shortage of echography technicians. However, the limited\navailability of medical data presents a significant barrier to applying ML in\ncardiology, particularly regarding US images of the heart. This paper addresses\nthis challenge by introducing the first open graded dataset for Cardiac\nAssessment and ClassificaTion of UltraSound (CACTUS), which is available\nonline. This dataset contains images obtained from scanning a CAE Blue Phantom\nand representing various heart views and different quality levels, exceeding\nthe conventional cardiac views typically found in the literature. Additionally,\nthe paper introduces a Deep Learning (DL) framework consisting of two main\ncomponents. The first component classifies cardiac US images based on the heart\nview using a Convolutional Neural Network (CNN). The second component uses\nTransfer Learning (TL) to fine-tune the knowledge from the first component and\ncreate a model for grading and assessing cardiac images. The framework\ndemonstrates high performance in both classification and grading, achieving up\nto 99.43% accuracy and as low as 0.3067 error, respectively. To showcase its\nrobustness, the framework is further fine-tuned using new images representing\nadditional cardiac views and compared to several other state-of-the-art\narchitectures. The framework's outcomes and performance in handling real-time\nscans were also assessed using a questionnaire answered by cardiac experts.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05604v1",
    "published_date": "2025-03-07 17:29:04 UTC",
    "updated_date": "2025-03-07 17:29:04 UTC"
  },
  {
    "arxiv_id": "2503.05592v2",
    "title": "R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning",
    "authors": [
      "Huatong Song",
      "Jinhao Jiang",
      "Yingqian Min",
      "Jie Chen",
      "Zhipeng Chen",
      "Wayne Xin Zhao",
      "Lei Fang",
      "Ji-Rong Wen"
    ],
    "abstract": "Existing Large Reasoning Models (LRMs) have shown the potential of\nreinforcement learning (RL) to enhance the complex reasoning capabilities of\nLarge Language Models~(LLMs). While they achieve remarkable performance on\nchallenging tasks such as mathematics and coding, they often rely on their\ninternal knowledge to solve problems, which can be inadequate for\ntime-sensitive or knowledge-intensive questions, leading to inaccuracies and\nhallucinations. To address this, we propose \\textbf{R1-Searcher}, a novel\ntwo-stage outcome-based RL approach designed to enhance the search capabilities\nof LLMs. This method allows LLMs to autonomously invoke external search systems\nto access additional knowledge during the reasoning process. Our framework\nrelies exclusively on RL, without requiring process rewards or distillation for\na cold start. % effectively generalizing to out-of-domain datasets and\nsupporting both Base and Instruct models. Our experiments demonstrate that our\nmethod significantly outperforms previous strong RAG methods, even when\ncompared to the closed-source GPT-4o-mini.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05592v2",
    "published_date": "2025-03-07 17:14:44 UTC",
    "updated_date": "2025-03-18 08:32:24 UTC"
  },
  {
    "arxiv_id": "2503.05587v1",
    "title": "Quantifying the Robustness of Retrieval-Augmented Language Models Against Spurious Features in Grounding Data",
    "authors": [
      "Shiping Yang",
      "Jie Wu",
      "Wenbiao Ding",
      "Ning Wu",
      "Shining Liang",
      "Ming Gong",
      "Hengyuan Zhang",
      "Dongmei Zhang"
    ],
    "abstract": "Robustness has become a critical attribute for the deployment of RAG systems\nin real-world applications. Existing research focuses on robustness to explicit\nnoise (e.g., document semantics) but overlooks spurious features (a.k.a.\nimplicit noise). While previous works have explored spurious features in LLMs,\nthey are limited to specific features (e.g., formats) and narrow scenarios\n(e.g., ICL). In this work, we statistically confirm the presence of spurious\nfeatures in the RAG paradigm, a robustness problem caused by the sensitivity of\nLLMs to semantic-agnostic features. Moreover, we provide a comprehensive\ntaxonomy of spurious features and empirically quantify their impact through\ncontrolled experiments. Further analysis reveals that not all spurious features\nare harmful and they can even be beneficial sometimes. Extensive evaluation\nresults across multiple LLMs suggest that spurious features are a widespread\nand challenging problem in the field of RAG. The code and dataset will be\nreleased to facilitate future research. We release all codes and data at:\n$\\\\\\href{https://github.com/maybenotime/RAG-SpuriousFeatures}{https://github.com/maybenotime/RAG-SpuriousFeatures}$.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05587v1",
    "published_date": "2025-03-07 17:11:34 UTC",
    "updated_date": "2025-03-07 17:11:34 UTC"
  },
  {
    "arxiv_id": "2503.05857v1",
    "title": "SYMBIOSIS: Systems Thinking and Machine Intelligence for Better Outcomes in Society",
    "authors": [
      "Sameer Sethi",
      "Donald Martin Jr.",
      "Emmanuel Klu"
    ],
    "abstract": "This paper presents SYMBIOSIS, an AI-powered framework and platform designed\nto make Systems Thinking accessible for addressing societal challenges and\nunlock paths for leveraging systems thinking frameworks to improve AI systems.\nThe platform establishes a centralized, open-source repository of systems\nthinking/system dynamics models categorized by Sustainable Development Goals\n(SDGs) and societal topics using topic modeling and classification techniques.\nSystems Thinking resources, though critical for articulating causal theories in\ncomplex problem spaces, are often locked behind specialized tools and intricate\nnotations, creating high barriers to entry. To address this, we developed a\ngenerative co-pilot that translates complex systems representations - such as\ncausal loop and stock-flow diagrams - into natural language (and vice-versa),\nallowing users to explore and build models without extensive technical\ntraining.\n  Rooted in community-based system dynamics (CBSD) and informed by\ncommunity-driven insights on societal context, we aim to bridge the problem\nunderstanding chasm. This gap, driven by epistemic uncertainty, often limits ML\ndevelopers who lack the community-specific knowledge essential for problem\nunderstanding and formulation, often leading to ill informed causal\nassumptions, reduced intervention effectiveness and harmful biases. Recent\nresearch identifies causal and abductive reasoning as crucial frontiers for AI,\nand Systems Thinking provides a naturally compatible framework for both. By\nmaking Systems Thinking frameworks more accessible and user-friendly, SYMBIOSIS\naims to serve as a foundational step to unlock future research into responsible\nand society-centered AI. Our work underscores the need for ongoing research\ninto AI's capacity to understand essential characteristics of complex adaptive\nsystems paving the way for more socially attuned, effective AI systems.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05857v1",
    "published_date": "2025-03-07 17:07:26 UTC",
    "updated_date": "2025-03-07 17:07:26 UTC"
  },
  {
    "arxiv_id": "2503.05573v1",
    "title": "InDRiVE: Intrinsic Disagreement based Reinforcement for Vehicle Exploration through Curiosity Driven Generalized World Model",
    "authors": [
      "Feeza Khan Khanzada",
      "Jaerock Kwon"
    ],
    "abstract": "Model-based Reinforcement Learning (MBRL) has emerged as a promising paradigm\nfor autonomous driving, where data efficiency and robustness are critical. Yet,\nexisting solutions often rely on carefully crafted, task specific extrinsic\nrewards, limiting generalization to new tasks or environments. In this paper,\nwe propose InDRiVE (Intrinsic Disagreement based Reinforcement for Vehicle\nExploration), a method that leverages purely intrinsic, disagreement based\nrewards within a Dreamer based MBRL framework. By training an ensemble of world\nmodels, the agent actively explores high uncertainty regions of environments\nwithout any task specific feedback. This approach yields a task agnostic latent\nrepresentation, allowing for rapid zero shot or few shot fine tuning on\ndownstream driving tasks such as lane following and collision avoidance.\nExperimental results in both seen and unseen environments demonstrate that\nInDRiVE achieves higher success rates and fewer infractions compared to\nDreamerV2 and DreamerV3 baselines despite using significantly fewer training\nsteps. Our findings highlight the effectiveness of purely intrinsic exploration\nfor learning robust vehicle control behaviors, paving the way for more scalable\nand adaptable autonomous driving systems.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.RO",
    "comment": "This work has been submitted to IROS 2025 and is currently under\n  review",
    "pdf_url": "http://arxiv.org/pdf/2503.05573v1",
    "published_date": "2025-03-07 16:56:00 UTC",
    "updated_date": "2025-03-07 16:56:00 UTC"
  },
  {
    "arxiv_id": "2503.05571v1",
    "title": "Compliance of AI Systems",
    "authors": [
      "Julius Schöning",
      "Niklas Kruse"
    ],
    "abstract": "The increasing integration of artificial intelligence (AI) systems in various\nfields requires solid concepts to ensure compliance with upcoming legislation.\nThis paper systematically examines the compliance of AI systems with relevant\nlegislation, focusing on the EU's AI Act and the compliance of data sets. The\nanalysis highlighted many challenges associated with edge devices, which are\nincreasingly being used to deploy AI applications closer and closer to the data\nsources. Such devices often face unique issues due to their decentralized\nnature and limited computing resources for implementing sophisticated\ncompliance mechanisms. By analyzing AI implementations, the paper identifies\nchallenges and proposes the first best practices for legal compliance when\ndeveloping, deploying, and running AI. The importance of data set compliance is\nhighlighted as a cornerstone for ensuring the trustworthiness, transparency,\nand explainability of AI systems, which must be aligned with ethical standards\nset forth in regulatory frameworks such as the AI Act. The insights gained\nshould contribute to the ongoing discourse on the responsible development and\ndeployment of embedded AI systems.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "I.2.1; H.4.0"
    ],
    "primary_category": "cs.CY",
    "comment": "5 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05571v1",
    "published_date": "2025-03-07 16:53:36 UTC",
    "updated_date": "2025-03-07 16:53:36 UTC"
  },
  {
    "arxiv_id": "2503.05546v1",
    "title": "Impoola: The Power of Average Pooling for Image-Based Deep Reinforcement Learning",
    "authors": [
      "Raphael Trumpp",
      "Ansgar Schäfftlein",
      "Mirco Theile",
      "Marco Caccamo"
    ],
    "abstract": "As image-based deep reinforcement learning tackles more challenging tasks,\nincreasing model size has become an important factor in improving performance.\nRecent studies achieved this by focusing on the parameter efficiency of scaled\nnetworks, typically using Impala-CNN, a 15-layer ResNet-inspired network, as\nthe image encoder. However, while Impala-CNN evidently outperforms older CNN\narchitectures, potential advancements in network design for deep reinforcement\nlearning-specific image encoders remain largely unexplored. We find that\nreplacing the flattening of output feature maps in Impala-CNN with global\naverage pooling leads to a notable performance improvement. This approach\noutperforms larger and more complex models in the Procgen Benchmark,\nparticularly in terms of generalization. We call our proposed encoder model\nImpoola-CNN. A decrease in the network's translation sensitivity may be central\nto this improvement, as we observe the most significant gains in games without\nagent-centered observations. Our results demonstrate that network scaling is\nnot just about increasing model size - efficient network design is also an\nessential factor.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05546v1",
    "published_date": "2025-03-07 16:19:19 UTC",
    "updated_date": "2025-03-07 16:19:19 UTC"
  },
  {
    "arxiv_id": "2503.05522v1",
    "title": "Post-Hoc Concept Disentanglement: From Correlated to Isolated Concept Representations",
    "authors": [
      "Eren Erogullari",
      "Sebastian Lapuschkin",
      "Wojciech Samek",
      "Frederik Pahde"
    ],
    "abstract": "Concept Activation Vectors (CAVs) are widely used to model\nhuman-understandable concepts as directions within the latent space of neural\nnetworks. They are trained by identifying directions from the activations of\nconcept samples to those of non-concept samples. However, this method often\nproduces similar, non-orthogonal directions for correlated concepts, such as\n\"beard\" and \"necktie\" within the CelebA dataset, which frequently co-occur in\nimages of men. This entanglement complicates the interpretation of concepts in\nisolation and can lead to undesired effects in CAV applications, such as\nactivation steering. To address this issue, we introduce a post-hoc concept\ndisentanglement method that employs a non-orthogonality loss, facilitating the\nidentification of orthogonal concept directions while preserving directional\ncorrectness. We evaluate our approach with real-world and controlled correlated\nconcepts in CelebA and a synthetic FunnyBirds dataset with VGG16 and ResNet18\narchitectures. We further demonstrate the superiority of orthogonalized concept\nrepresentations in activation steering tasks, allowing (1) the insertion of\nisolated concepts into input images through generative models and (2) the\nremoval of concepts for effective shortcut suppression with reduced impact on\ncorrelated concepts in comparison to baseline CAVs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05522v1",
    "published_date": "2025-03-07 15:45:43 UTC",
    "updated_date": "2025-03-07 15:45:43 UTC"
  },
  {
    "arxiv_id": "2503.05516v1",
    "title": "Cognitive Bias Detection Using Advanced Prompt Engineering",
    "authors": [
      "Frederic Lemieux",
      "Aisha Behr",
      "Clara Kellermann-Bryant",
      "Zaki Mohammed"
    ],
    "abstract": "Cognitive biases, systematic deviations from rationality in judgment, pose\nsignificant challenges in generating objective content. This paper introduces a\nnovel approach for real-time cognitive bias detection in user-generated text\nusing large language models (LLMs) and advanced prompt engineering techniques.\nThe proposed system analyzes textual data to identify common cognitive biases\nsuch as confirmation bias, circular reasoning, and hidden assumption. By\ndesigning tailored prompts, the system effectively leverages LLMs' capabilities\nto both recognize and mitigate these biases, improving the quality of\nhuman-generated content (e.g., news, media, reports). Experimental results\ndemonstrate the high accuracy of our approach in identifying cognitive biases,\noffering a valuable tool for enhancing content objectivity and reducing the\nrisks of biased decision-making.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CY",
    "comment": "17 pages. 6 Figures, 2 Tables",
    "pdf_url": "http://arxiv.org/pdf/2503.05516v1",
    "published_date": "2025-03-07 15:35:37 UTC",
    "updated_date": "2025-03-07 15:35:37 UTC"
  },
  {
    "arxiv_id": "2503.05514v1",
    "title": "Noise-Robust Radio Frequency Fingerprint Identification Using Denoise Diffusion Model",
    "authors": [
      "Guolin Yin",
      "Junqing Zhang",
      "Yuan Ding",
      "Simon Cotton"
    ],
    "abstract": "Securing Internet of Things (IoT) devices presents increasing challenges due\nto their limited computational and energy resources. Radio Frequency\nFingerprint Identification (RFFI) emerges as a promising authentication\ntechnique to identify wireless devices through hardware impairments. RFFI\nperformance under low signal-to-noise ratio (SNR) scenarios is significantly\ndegraded because the minute hardware features can be easily swamped in noise.\nIn this paper, we leveraged the diffusion model to effectively restore the RFF\nunder low SNR scenarios. Specifically, we trained a powerful noise predictor\nand tailored a noise removal algorithm to effectively reduce the noise level in\nthe received signal and restore the device fingerprints. We used Wi-Fi as a\ncase study and created a testbed involving 6 commercial off-the-shelf Wi-Fi\ndongles and a USRP N210 software-defined radio (SDR) platform. We conducted\nexperimental evaluations on various SNR scenarios. The experimental results\nshow that the proposed algorithm can improve the classification accuracy by up\nto 34.9%.",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "primary_category": "eess.SP",
    "comment": "6 pages, 8 figures, WCNC 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.05514v1",
    "published_date": "2025-03-07 15:30:55 UTC",
    "updated_date": "2025-03-07 15:30:55 UTC"
  },
  {
    "arxiv_id": "2503.05507v1",
    "title": "Grammar-Based Code Representation: Is It a Worthy Pursuit for LLMs?",
    "authors": [
      "Qingyuan Liang",
      "Zhao Zhang",
      "Zeyu Sun",
      "Zheng Lin",
      "Qi Luo",
      "Yueyi Xiao",
      "Yizhou Chen",
      "Yuqun Zhang",
      "Haotian Zhang",
      "Lu Zhang",
      "Bin Chen",
      "Yingfei Xiong"
    ],
    "abstract": "Grammar serves as a cornerstone in programming languages and software\nengineering, providing frameworks to define the syntactic space and program\nstructure. Existing research demonstrates the effectiveness of grammar-based\ncode representations in small-scale models, showing their ability to reduce\nsyntax errors and enhance performance. However, as language models scale to the\nbillion level or beyond, syntax-level errors become rare, making it unclear\nwhether grammar information still provides performance benefits. To explore\nthis, we develop a series of billion-scale GrammarCoder models, incorporating\ngrammar rules in the code generation process. Experiments on HumanEval (+) and\nMBPP (+) demonstrate a notable improvement in code generation accuracy. Further\nanalysis shows that grammar-based representations enhance LLMs' ability to\ndiscern subtle code differences, reducing semantic errors caused by minor\nvariations. These findings suggest that grammar-based code representations\nremain valuable even in billion-scale models, not only by maintaining syntax\ncorrectness but also by improving semantic differentiation.",
    "categories": [
      "cs.PL",
      "cs.AI"
    ],
    "primary_category": "cs.PL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05507v1",
    "published_date": "2025-03-07 15:23:13 UTC",
    "updated_date": "2025-03-07 15:23:13 UTC"
  },
  {
    "arxiv_id": "2503.05500v2",
    "title": "EuroBERT: Scaling Multilingual Encoders for European Languages",
    "authors": [
      "Nicolas Boizard",
      "Hippolyte Gisserot-Boukhlef",
      "Duarte M. Alves",
      "André Martins",
      "Ayoub Hammal",
      "Caio Corro",
      "Céline Hudelot",
      "Emmanuel Malherbe",
      "Etienne Malaboeuf",
      "Fanny Jourdan",
      "Gabriel Hautreux",
      "João Alves",
      "Kevin El-Haddad",
      "Manuel Faysse",
      "Maxime Peyrard",
      "Nuno M. Guerreiro",
      "Patrick Fernandes",
      "Ricardo Rei",
      "Pierre Colombo"
    ],
    "abstract": "General-purpose multilingual vector representations, used in retrieval,\nregression and classification, are traditionally obtained from bidirectional\nencoder models. Despite their wide applicability, encoders have been recently\novershadowed by advances in generative decoder-only models. However, many\ninnovations driving this progress are not inherently tied to decoders. In this\npaper, we revisit the development of multilingual encoders through the lens of\nthese advances, and introduce EuroBERT, a family of multilingual encoders\ncovering European and widely spoken global languages. Our models outperform\nexisting alternatives across a diverse range of tasks, spanning multilingual\ncapabilities, mathematics, and coding, and natively supporting sequences of up\nto 8,192 tokens. We also examine the design decisions behind EuroBERT, offering\ninsights into our dataset composition and training pipeline. We publicly\nrelease the EuroBERT models, including intermediate training checkpoints,\ntogether with our training framework.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages, 8 figures, 13 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.05500v2",
    "published_date": "2025-03-07 15:13:58 UTC",
    "updated_date": "2025-03-26 18:43:59 UTC"
  },
  {
    "arxiv_id": "2503.05492v1",
    "title": "FastMap: Fast Queries Initialization Based Vectorized HD Map Reconstruction Framework",
    "authors": [
      "Haotian Hu",
      "Jingwei Xu",
      "Fanyi Wang",
      "Toyota Li",
      "Yaonong Wang",
      "Laifeng Hu",
      "Zhiwang Zhang"
    ],
    "abstract": "Reconstruction of high-definition maps is a crucial task in perceiving the\nautonomous driving environment, as its accuracy directly impacts the\nreliability of prediction and planning capabilities in downstream modules.\nCurrent vectorized map reconstruction methods based on the DETR framework\nencounter limitations due to the redundancy in the decoder structure,\nnecessitating the stacking of six decoder layers to maintain performance, which\nsignificantly hampers computational efficiency. To tackle this issue, we\nintroduce FastMap, an innovative framework designed to reduce decoder\nredundancy in existing approaches. FastMap optimizes the decoder architecture\nby employing a single-layer, two-stage transformer that achieves multilevel\nrepresentation capabilities. Our framework eliminates the conventional practice\nof randomly initializing queries and instead incorporates a heatmap-guided\nquery generation module during the decoding phase, which effectively maps image\nfeatures into structured query vectors using learnable positional encoding.\nAdditionally, we propose a geometry-constrained point-to-line loss mechanism\nfor FastMap, which adeptly addresses the challenge of distinguishing highly\nhomogeneous features that often arise in traditional point-to-point loss\ncomputations. Extensive experiments demonstrate that FastMap achieves\nstate-of-the-art performance in both nuScenes and Argoverse2 datasets, with its\ndecoder operating 3.2 faster than the baseline. Code and more demos are\navailable at https://github.com/hht1996ok/FastMap.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05492v1",
    "published_date": "2025-03-07 15:01:55 UTC",
    "updated_date": "2025-03-07 15:01:55 UTC"
  },
  {
    "arxiv_id": "2503.07657v1",
    "title": "SplitQuantV2: Enhancing Low-Bit Quantization of LLMs Without GPUs",
    "authors": [
      "Jaewoo Song",
      "Fangzhen Lin"
    ],
    "abstract": "The quantization of large language models (LLMs) is crucial for deploying\nthem on devices with limited computational resources. While advanced\nquantization algorithms offer improved performance compared to the basic linear\nquantization, they typically require high-end graphics processing units (GPUs),\nare often restricted to specific deep neural network (DNN) frameworks, and\nrequire calibration datasets. This limitation poses challenges for using such\nalgorithms on various neural processing units (NPUs) and edge AI devices, which\nhave diverse model formats and frameworks. In this paper, we show SplitQuantV2,\nan innovative algorithm designed to enhance low-bit linear quantization of\nLLMs, can achieve results comparable to those of advanced algorithms.\nSplitQuantV2 preprocesses models by splitting linear and convolution layers\ninto functionally equivalent, quantization-friendly structures. The algorithm's\nplatform-agnostic, concise, and efficient nature allows for implementation\nwithout the need for GPUs. Our evaluation on the Llama 3.2 1B Instruct model\nusing the AI2's Reasoning Challenge (ARC) dataset demonstrates that\nSplitQuantV2 improves the accuracy of the INT4 quantization model by 11.76%p,\nmatching the performance of the original floating-point model. Remarkably,\nSplitQuantV2 took only 2 minutes 6 seconds to preprocess the 1B model and\nperform linear INT4 quantization using only an Apple M4 CPU. SplitQuantV2\nprovides a practical solution for low-bit quantization on LLMs, especially when\ncomplex, computation-intensive algorithms are inaccessible due to hardware\nlimitations or framework incompatibilities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.07657v1",
    "published_date": "2025-03-07 14:59:07 UTC",
    "updated_date": "2025-03-07 14:59:07 UTC"
  },
  {
    "arxiv_id": "2503.05474v1",
    "title": "Personalized Federated Learning via Learning Dynamic Graphs",
    "authors": [
      "Ziran Zhou",
      "Guanyu Gao",
      "Xiaohu Wu",
      "Yan Lyu"
    ],
    "abstract": "Personalized Federated Learning (PFL) aims to train a personalized model for\neach client that is tailored to its local data distribution, learning fails to\nperform well on individual clients due to variations in their local data\ndistributions. Most existing PFL methods focus on personalizing the aggregated\nglobal model for each client, neglecting the fundamental aspect of federated\nlearning: the regulation of how client models are aggregated. Additionally,\nalmost all of them overlook the graph structure formed by clients in federated\nlearning. In this paper, we propose a novel method, Personalized Federated\nLearning with Graph Attention Network (pFedGAT), which captures the latent\ngraph structure between clients and dynamically determines the importance of\nother clients for each client, enabling fine-grained control over the\naggregation process. We evaluate pFedGAT across multiple data distribution\nscenarios, comparing it with twelve state of the art methods on three datasets:\nFashion MNIST, CIFAR-10, and CIFAR-100, and find that it consistently performs\nwell.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05474v1",
    "published_date": "2025-03-07 14:47:03 UTC",
    "updated_date": "2025-03-07 14:47:03 UTC"
  },
  {
    "arxiv_id": "2503.05856v1",
    "title": "This Is Your Doge, If It Please You: Exploring Deception and Robustness in Mixture of LLMs",
    "authors": [
      "Lorenz Wolf",
      "Sangwoong Yoon",
      "Ilija Bogunovic"
    ],
    "abstract": "Mixture of large language model (LLMs) Agents (MoA) architectures achieve\nstate-of-the-art performance on prominent benchmarks like AlpacaEval 2.0 by\nleveraging the collaboration of multiple LLMs at inference time. Despite these\nsuccesses, an evaluation of the safety and reliability of MoA is missing. We\npresent the first comprehensive study of MoA's robustness against deceptive LLM\nagents that deliberately provide misleading responses. We examine factors like\nthe propagation of deceptive information, model size, and information\navailability, and uncover critical vulnerabilities. On AlpacaEval 2.0, the\npopular LLaMA 3.1-70B model achieves a length-controlled Win Rate (LC WR) of\n49.2% when coupled with 3-layer MoA (6 LLM agents). However, we demonstrate\nthat introducing only a $\\textit{single}$ carefully-instructed deceptive agent\ninto the MoA can reduce performance to 37.9%, effectively nullifying all MoA\ngains. On QuALITY, a multiple-choice comprehension task, the impact is also\nsevere, with accuracy plummeting by a staggering 48.5%. Inspired in part by the\nhistorical Doge of Venice voting process, designed to minimize influence and\ndeception, we propose a range of unsupervised defense mechanisms that recover\nmost of the lost performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "35 pages, 9 figures, 16 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.05856v1",
    "published_date": "2025-03-07 14:46:39 UTC",
    "updated_date": "2025-03-07 14:46:39 UTC"
  },
  {
    "arxiv_id": "2503.05473v2",
    "title": "The Society of HiveMind: Multi-Agent Optimization of Foundation Model Swarms to Unlock the Potential of Collective Intelligence",
    "authors": [
      "Noah Mamie",
      "Susie Xi Rao"
    ],
    "abstract": "Multi-agent systems address issues of accessibility and scalability of\nartificial intelligence (AI) foundation models, which are often represented by\nlarge language models. We develop a framework - the \"Society of HiveMind\"\n(SOHM) - that orchestrates the interaction between multiple AI foundation\nmodels, imitating the observed behavior of animal swarms in nature by following\nmodern evolutionary theories. On the one hand, we find that the SOHM provides a\nnegligible benefit on tasks that mainly require real-world knowledge. On the\nother hand, we remark a significant improvement on tasks that require intensive\nlogical reasoning, indicating that multi-agent systems are capable of\nincreasing the reasoning capabilities of the collective compared to the\nindividual agents. Our findings demonstrate the potential of combining a\nmultitude of diverse AI foundation models to form an artificial swarm\nintelligence capable of self-improvement through interactions with a given\nenvironment.",
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "primary_category": "cs.NE",
    "comment": "11 pages (excl. appendix)",
    "pdf_url": "http://arxiv.org/pdf/2503.05473v2",
    "published_date": "2025-03-07 14:45:03 UTC",
    "updated_date": "2025-03-13 14:20:53 UTC"
  },
  {
    "arxiv_id": "2503.05455v1",
    "title": "Controllable Complementarity: Subjective Preferences in Human-AI Collaboration",
    "authors": [
      "Chase McDonald",
      "Cleotilde Gonzalez"
    ],
    "abstract": "Research on human-AI collaboration often prioritizes objective performance.\nHowever, understanding human subjective preferences is essential to improving\nhuman-AI complementarity and human experiences. We investigate human\npreferences for controllability in a shared workspace task with AI partners\nusing Behavior Shaping (BS), a reinforcement learning algorithm that allows\nhumans explicit control over AI behavior.\n  In one experiment, we validate the robustness of BS in producing effective AI\npolicies relative to self-play policies, when controls are hidden. In another\nexperiment, we enable human control, showing that participants perceive AI\npartners as more effective and enjoyable when they can directly dictate AI\nbehavior. Our findings highlight the need to design AI that prioritizes both\ntask performance and subjective human preferences. By aligning AI behavior with\nhuman preferences, we demonstrate how human-AI complementarity can extend\nbeyond objective outcomes to include subjective preferences.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.HC",
    "comment": "9 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05455v1",
    "published_date": "2025-03-07 14:27:48 UTC",
    "updated_date": "2025-03-07 14:27:48 UTC"
  },
  {
    "arxiv_id": "2503.05453v1",
    "title": "Soft Policy Optimization: Online Off-Policy RL for Sequence Models",
    "authors": [
      "Taco Cohen",
      "David W. Zhang",
      "Kunhao Zheng",
      "Yunhao Tang",
      "Remi Munos",
      "Gabriel Synnaeve"
    ],
    "abstract": "RL-based post-training of language models is almost exclusively done using\non-policy methods such as PPO. These methods cannot learn from arbitrary\nsequences such as those produced earlier in training, in earlier runs, by human\nexperts or other policies, or by decoding and exploration methods. This results\nin severe sample inefficiency and exploration difficulties, as well as a\npotential loss of diversity in the policy responses. Moreover, asynchronous PPO\nimplementations require frequent and costly model transfers, and typically use\nvalue models which require a large amount of memory. In this paper we introduce\nSoft Policy Optimization (SPO), a simple, scalable and principled Soft RL\nmethod for sequence model policies that can learn from arbitrary online and\noffline trajectories and does not require a separate value model. In\nexperiments on code contests, we shows that SPO outperforms PPO on pass@10, is\nsignificantly faster and more memory efficient, is able to benefit from\noff-policy data, enjoys improved stability, and learns more diverse (i.e. soft)\npolicies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05453v1",
    "published_date": "2025-03-07 14:23:40 UTC",
    "updated_date": "2025-03-07 14:23:40 UTC"
  },
  {
    "arxiv_id": "2503.05449v1",
    "title": "LLM-based Iterative Approach to Metamodeling in Automotive",
    "authors": [
      "Nenad Petrovic",
      "Fengjunjie Pan",
      "Vahid Zolfaghari",
      "Alois Knoll"
    ],
    "abstract": "In this paper, we introduce an automated approach to domain-specific\nmetamodel construction relying on Large Language Model (LLM). The main focus is\nadoption in automotive domain. As outcome, a prototype was implemented as web\nservice using Python programming language, while OpenAI's GPT-4o was used as\nthe underlying LLM. Based on the initial experiments, this approach\nsuccessfully constructs Ecore metamodel based on set of automotive requirements\nand visualizes it making use of PlantUML notation, so human experts can provide\nfeedback in order to refine the result. Finally, locally deployable solution is\nalso considered, including the limitations and additional steps required.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05449v1",
    "published_date": "2025-03-07 14:19:17 UTC",
    "updated_date": "2025-03-07 14:19:17 UTC"
  },
  {
    "arxiv_id": "2503.05447v2",
    "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts",
    "authors": [
      "Weigao Sun",
      "Disen Lan",
      "Tong Zhu",
      "Xiaoye Qu",
      "Yu Cheng"
    ],
    "abstract": "Linear Sequence Modeling (LSM) like linear attention, state space models and\nlinear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant\narchitectural improvements. In this paper, we introduce Linear-MoE, a\nproduction-level system for modeling and training large-scale models that\nintegrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules\nfor linear-complexity sequence modeling and MoE layers for sparsely activation,\naiming to offer high performance with efficient training. The Linear-MoE system\ncomprises: 1) Modeling subsystem, which provides a unified framework supporting\nall instances of LSM. and 2) Training subsystem, which facilitates efficient\ntraining by incorporating various advanced parallelism technologies,\nparticularly Sequence Parallelism designed for Linear-MoE models. Additionally,\nwe explore hybrid models that combine Linear-MoE layers with standard\nTransformer-MoE layers with its Sequence Parallelism to further enhance model\nflexibility and performance. Evaluations on two model series, A0.3B-2B and\nA1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining\ncompetitive performance on various benchmarks, showcasing its potential as a\nnext-generation foundational model architecture. Code:\nhttps://github.com/OpenSparseLLMs/Linear-MoE.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "Technical report, 17 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.05447v2",
    "published_date": "2025-03-07 14:17:45 UTC",
    "updated_date": "2025-04-15 07:51:10 UTC"
  },
  {
    "arxiv_id": "2503.05439v2",
    "title": "An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for Robust Reasoning",
    "authors": [
      "Navdeep Kaur",
      "Lachlan McPheat",
      "Alessandra Russo",
      "Anthony G Cohn",
      "Pranava Madhyastha"
    ],
    "abstract": "In this paper, we examine the use of Conformal Language Modelling (CLM)\nalongside Answer Set Programming (ASP) to enhance the performance of standard\nopen-weight LLMs on complex multi-step reasoning tasks. Using the StepGame\ndataset, which requires spatial reasoning, we apply CLM to generate sets of ASP\nprograms from an LLM, providing statistical guarantees on the correctness of\nthe outputs. Experimental results show that CLM significantly outperforms\nbaseline models that use standard sampling methods, achieving substantial\naccuracy improvements across different levels of reasoning complexity.\nAdditionally, the LLM-as-Judge metric enhances CLM's performance, especially in\nassessing structurally and logically correct ASP outputs. However, calibrating\nCLM with diverse calibration sets did not improve generalizability for tasks\nrequiring much longer reasoning steps, indicating limitations in handling more\ncomplex tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05439v2",
    "published_date": "2025-03-07 14:10:10 UTC",
    "updated_date": "2025-04-11 15:33:14 UTC"
  },
  {
    "arxiv_id": "2503.05423v4",
    "title": "Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning",
    "authors": [
      "Run He",
      "Di Fang",
      "Yicheng Xu",
      "Yawen Cui",
      "Ming Li",
      "Cen Chen",
      "Ziqian Zeng",
      "Huiping Zhuang"
    ],
    "abstract": "Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn\nfrom distinct categories without retaining exemplars but easily suffers from\ncatastrophic forgetting of learned knowledge. While existing EFCIL methods\nleverage knowledge distillation to alleviate forgetting, they still face two\ncritical challenges: semantic shift and decision bias. Specifically, the\nembeddings of old tasks shift in the embedding space after learning new tasks,\nand the classifier becomes biased towards new tasks due to training solely with\nnew data, hindering the balance between old and new knowledge. To address these\nissues, we propose the Dual-Projection Shift Estimation and Classifier\nReconstruction (DPCR) approach for EFCIL. DPCR effectively estimates semantic\nshift through a dual-projection, which combines a learnable transformation with\na row-space projection to capture both task-wise and category-wise shifts.\nFurthermore, to mitigate decision bias, DPCR employs ridge regression to\nreformulate a classifier reconstruction process. This reconstruction exploits\nprevious in covariance and prototype of each class after calibration with\nestimated shift, thereby reducing decision bias. Extensive experiments\ndemonstrate that, on various datasets, DPCR effectively balances old and new\ntasks, outperforming state-of-the-art EFCIL methods. Our codes are available at\nhttps://github.com/RHe502/ICML25-DPCR.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ICML 2025; Camera ready version",
    "pdf_url": "http://arxiv.org/pdf/2503.05423v4",
    "published_date": "2025-03-07 13:50:29 UTC",
    "updated_date": "2025-05-18 08:38:04 UTC"
  },
  {
    "arxiv_id": "2503.05854v1",
    "title": "Accelerating Earth Science Discovery via Multi-Agent LLM Systems",
    "authors": [
      "Dmitrii Pantiukhin",
      "Boris Shapkin",
      "Ivan Kuznetsov",
      "Antonia Anna Jost",
      "Nikolay Koldunov"
    ],
    "abstract": "This Perspective explores the transformative potential of Multi-Agent Systems\n(MAS) powered by Large Language Models (LLMs) in the geosciences. Users of\ngeoscientific data repositories face challenges due to the complexity and\ndiversity of data formats, inconsistent metadata practices, and a considerable\nnumber of unprocessed datasets. MAS possesses transformative potential for\nimproving scientists' interaction with geoscientific data by enabling\nintelligent data processing, natural language interfaces, and collaborative\nproblem-solving capabilities. We illustrate this approach with \"PANGAEA GPT\", a\nspecialized MAS pipeline integrated with the diverse PANGAEA database for Earth\nand Environmental Science, demonstrating how MAS-driven workflows can\neffectively manage complex datasets and accelerate scientific discovery. We\ndiscuss how MAS can address current data challenges in geosciences, highlight\nadvancements in other scientific fields, and propose future directions for\nintegrating MAS into geoscientific data processing pipelines. In this\nPerspective, we show how MAS can fundamentally improve data accessibility,\npromote cross-disciplinary collaboration, and accelerate geoscientific\ndiscoveries.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "I.2.11"
    ],
    "primary_category": "cs.MA",
    "comment": "10 pages, 1 figure. Perspective article",
    "pdf_url": "http://arxiv.org/pdf/2503.05854v1",
    "published_date": "2025-03-07 13:25:56 UTC",
    "updated_date": "2025-03-07 13:25:56 UTC"
  },
  {
    "arxiv_id": "2503.05394v1",
    "title": "Static Program Analysis Guided LLM Based Unit Test Generation",
    "authors": [
      "Sujoy Roychowdhury",
      "Giriprasad Sridhara",
      "A K Raghavan",
      "Joy Bose",
      "Sourav Mazumdar",
      "Hamender Singh",
      "Srinivasan Bajji Sugumaran",
      "Ricardo Britto"
    ],
    "abstract": "We describe a novel approach to automating unit test generation for Java\nmethods using large language models (LLMs). Existing LLM-based approaches rely\non sample usage(s) of the method to test (focal method) and/or provide the\nentire class of the focal method as input prompt and context. The former\napproach is often not viable due to the lack of sample usages, especially for\nnewly written focal methods. The latter approach does not scale well enough;\nthe bigger the complexity of the focal method and larger associated class, the\nharder it is to produce adequate test code (due to factors such as exceeding\nthe prompt and context lengths of the underlying LLM). We show that augmenting\nprompts with \\emph{concise} and \\emph{precise} context information obtained by\nprogram analysis %of the focal method increases the effectiveness of generating\nunit test code through LLMs. We validate our approach on a large commercial\nJava project and a popular open-source Java project.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05394v1",
    "published_date": "2025-03-07 13:09:37 UTC",
    "updated_date": "2025-03-07 13:09:37 UTC"
  },
  {
    "arxiv_id": "2503.05388v1",
    "title": "Ontology Generation using Large Language Models",
    "authors": [
      "Anna Sofia Lippolis",
      "Mohammad Javad Saeedizade",
      "Robin Keskisärkkä",
      "Sara Zuppiroli",
      "Miguel Ceriani",
      "Aldo Gangemi",
      "Eva Blomqvist",
      "Andrea Giovanni Nuzzolese"
    ],
    "abstract": "The ontology engineering process is complex, time-consuming, and error-prone,\neven for experienced ontology engineers. In this work, we investigate the\npotential of Large Language Models (LLMs) to provide effective OWL ontology\ndrafts directly from ontological requirements described using user stories and\ncompetency questions. Our main contribution is the presentation and evaluation\nof two new prompting techniques for automated ontology development: Memoryless\nCQbyCQ and Ontogenia. We also emphasize the importance of three structural\ncriteria for ontology assessment, alongside expert qualitative evaluation,\nhighlighting the need for a multi-dimensional evaluation in order to capture\nthe quality and usability of the generated ontologies. Our experiments,\nconducted on a benchmark dataset of ten ontologies with 100 distinct CQs and 29\ndifferent user stories, compare the performance of three LLMs using the two\nprompting techniques. The results demonstrate improvements over the current\nstate-of-the-art in LLM-supported ontology engineering. More specifically, the\nmodel OpenAI o1-preview with Ontogenia produces ontologies of sufficient\nquality to meet the requirements of ontology engineers, significantly\noutperforming novice ontology engineers in modelling ability. However, we still\nnote some common mistakes and variability of result quality, which is important\nto take into account when using LLMs for ontology authoring support. We discuss\nthese limitations and propose directions for future research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "2 figures and 3 tables. 20 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.05388v1",
    "published_date": "2025-03-07 13:03:28 UTC",
    "updated_date": "2025-03-07 13:03:28 UTC"
  },
  {
    "arxiv_id": "2503.05383v5",
    "title": "AVA: Attentive VLM Agent for Mastering StarCraft II",
    "authors": [
      "Weiyu Ma",
      "Yuqian Fu",
      "Zecheng Zhang",
      "Bernard Ghanem",
      "Guohao Li"
    ],
    "abstract": "We introduce Attentive VLM Agent (AVA), a multimodal StarCraft II agent that\naligns artificial agent perception with the human gameplay experience.\nTraditional frameworks such as SMAC rely on abstract state representations that\ndiverge significantly from human perception, limiting the ecological validity\nof agent behavior. Our agent addresses this limitation by incorporating RGB\nvisual inputs and natural language observations that more closely simulate\nhuman cognitive processes during gameplay. The AVA architecture consists of\nthree integrated components: (1) a vision-language model enhanced with\nspecialized self-attention mechanisms for strategic unit targeting and\nbattlefield assessment, (2) a retrieval-augmented generation system that\nleverages domain-specific StarCraft II knowledge to inform tactical decisions,\nand (3) a dynamic role-based task distribution system that enables coordinated\nmulti-agent behavior. The experimental evaluation in our proposed AVACraft\nenvironment, which contains 21 multimodal StarCraft II scenarios, demonstrates\nthat AVA powered by foundation models (specifically Qwen-VL and GPT-4o) can\nexecute complex tactical maneuvers without explicit training, achieving\ncomparable performance to traditional MARL methods that require substantial\ntraining iterations. This work establishes a foundation for developing\nhuman-aligned StarCraft II agents and advances the broader research agenda of\nmultimodal game AI. Our implementation is available at\nhttps://github.com/camel-ai/VLM-Play-StarCraft2.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2503.05383v5",
    "published_date": "2025-03-07 12:54:25 UTC",
    "updated_date": "2025-05-16 05:21:12 UTC"
  },
  {
    "arxiv_id": "2503.05371v1",
    "title": "Shifting Perspectives: Steering Vector Ensembles for Robust Bias Mitigation in LLMs",
    "authors": [
      "Zara Siddique",
      "Irtaza Khalid",
      "Liam D. Turner",
      "Luis Espinosa-Anke"
    ],
    "abstract": "We present a novel approach to bias mitigation in large language models\n(LLMs) by applying steering vectors to modify model activations in forward\npasses. We employ Bayesian optimization to systematically identify effective\ncontrastive pair datasets across nine bias axes. When optimized on the BBQ\ndataset, our individually tuned steering vectors achieve average improvements\nof 12.2%, 4.7%, and 3.2% over the baseline for Mistral, Llama, and Qwen,\nrespectively. Building on these promising results, we introduce Steering Vector\nEnsembles (SVE), a method that averages multiple individually optimized\nsteering vectors, each targeting a specific bias axis such as age, race, or\ngender. By leveraging their collective strength, SVE outperforms individual\nsteering vectors in both bias reduction and maintaining model performance. The\nwork presents the first systematic investigation of steering vectors for bias\nmitigation, and we demonstrate that SVE is a powerful and computationally\nefficient strategy for reducing bias in LLMs, with broader implications for\nenhancing AI safety.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to ACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.05371v1",
    "published_date": "2025-03-07 12:25:29 UTC",
    "updated_date": "2025-03-07 12:25:29 UTC"
  },
  {
    "arxiv_id": "2503.05357v1",
    "title": "Improving Hate Speech Classification with Cross-Taxonomy Dataset Integration",
    "authors": [
      "Jan Fillies",
      "Adrian Paschke"
    ],
    "abstract": "Algorithmic hate speech detection faces significant challenges due to the\ndiverse definitions and datasets used in research and practice. Social media\nplatforms, legal frameworks, and institutions each apply distinct yet\noverlapping definitions, complicating classification efforts. This study\naddresses these challenges by demonstrating that existing datasets and\ntaxonomies can be integrated into a unified model, enhancing prediction\nperformance and reducing reliance on multiple specialized classifiers. The work\nintroduces a universal taxonomy and a hate speech classifier capable of\ndetecting a wide range of definitions within a single framework. Our approach\nis validated by combining two widely used but differently annotated datasets,\nshowing improved classification performance on an independent test set. This\nwork highlights the potential of dataset and taxonomy integration in advancing\nhate speech detection, increasing efficiency, and ensuring broader\napplicability across contexts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for publication at LaTeCH-CLfL 2025. The 9th Joint ACL\n  Special Interest Group on Language Technologies for the Socio-Economic\n  Sciences and Humanities (SIGHUM) Workshop on Computational Linguistics for\n  Cultural Heritage, Social Sciences, Humanities and Literature",
    "pdf_url": "http://arxiv.org/pdf/2503.05357v1",
    "published_date": "2025-03-07 12:01:02 UTC",
    "updated_date": "2025-03-07 12:01:02 UTC"
  },
  {
    "arxiv_id": "2503.05852v1",
    "title": "Evaluating Large Language Models in Code Generation: INFINITE Methodology for Defining the Inference Index",
    "authors": [
      "Nicholas Christakis",
      "Dimitris Drikakis"
    ],
    "abstract": "This study introduces a new methodology for an Inference Index (InI), called\nINFerence INdex In Testing model Effectiveness methodology (INFINITE), aiming\nto evaluate the performance of Large Language Models (LLMs) in code generation\ntasks. The InI index provides a comprehensive assessment focusing on three key\ncomponents: efficiency, consistency, and accuracy. This approach encapsulates\ntime-based efficiency, response quality, and the stability of model outputs,\noffering a thorough understanding of LLM performance beyond traditional\naccuracy metrics. We applied this methodology to compare OpenAI's GPT-4o (GPT),\nOpenAI-o1 pro (OAI1), and OpenAI-o3 mini-high (OAI3) in generating Python code\nfor the Long-Short-Term-Memory (LSTM) model to forecast meteorological\nvariables such as temperature, relative humidity and wind velocity. Our\nfindings demonstrate that GPT outperforms OAI1 and performs comparably to OAI3\nregarding accuracy and workflow efficiency. The study reveals that LLM-assisted\ncode generation can produce results similar to expert-designed models with\neffective prompting and refinement. GPT's performance advantage highlights the\nbenefits of widespread use and user feedback.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "20 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05852v1",
    "published_date": "2025-03-07 11:59:44 UTC",
    "updated_date": "2025-03-07 11:59:44 UTC"
  },
  {
    "arxiv_id": "2503.05355v1",
    "title": "On the Logical Content of Logic Programs",
    "authors": [
      "Alexader V. Gheorghiu"
    ],
    "abstract": "Logic programming (LP) is typically understood through operational semantics\n(e.g., SLD-resolution) or model-theoretic interpretations (e.g., the least\nHerbrand model). This paper introduces a novel perspective on LP by defining a\n``support'' relation that explicates what a program ``knows''. This\ninterpretation is shown to express classical and intuitionistic logic, as well\nas an intermediate logic, depending on certain choices regarding LP and the\nmeanings of disjunction and negation. These results are formalized using the\nidea of base-extension semantics within proof-theoretic semantics. Our approach\noffers new insights into the logical foundations of LP and has potential\napplications in knowledge representation, automated reasoning, and formal\nverification.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05355v1",
    "published_date": "2025-03-07 11:58:08 UTC",
    "updated_date": "2025-03-07 11:58:08 UTC"
  },
  {
    "arxiv_id": "2503.05349v1",
    "title": "Spatial Distillation based Distribution Alignment (SDDA) for Cross-Headset EEG Classification",
    "authors": [
      "Dingkun Liu",
      "Siyang Li",
      "Ziwei Wang",
      "Wei Li",
      "Dongrui Wu"
    ],
    "abstract": "A non-invasive brain-computer interface (BCI) enables direct interaction\nbetween the user and external devices, typically via electroencephalogram (EEG)\nsignals. However, decoding EEG signals across different headsets remains a\nsignificant challenge due to differences in the number and locations of the\nelectrodes. To address this challenge, we propose a spatial distillation based\ndistribution alignment (SDDA) approach for heterogeneous cross-headset transfer\nin non-invasive BCIs. SDDA uses first spatial distillation to make use of the\nfull set of electrodes, and then input/feature/output space distribution\nalignments to cope with the significant differences between the source and\ntarget domains. To our knowledge, this is the first work to use knowledge\ndistillation in cross-headset transfers. Extensive experiments on six EEG\ndatasets from two BCI paradigms demonstrated that SDDA achieved superior\nperformance in both offline unsupervised domain adaptation and online\nsupervised domain adaptation scenarios, consistently outperforming 10 classical\nand state-of-the-art transfer learning algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05349v1",
    "published_date": "2025-03-07 11:44:49 UTC",
    "updated_date": "2025-03-07 11:44:49 UTC"
  },
  {
    "arxiv_id": "2503.05346v1",
    "title": "AutoIOT: LLM-Driven Automated Natural Language Programming for AIoT Applications",
    "authors": [
      "Leming Shen",
      "Qiang Yang",
      "Yuanqing Zheng",
      "Mo Li"
    ],
    "abstract": "The advent of Large Language Models (LLMs) has profoundly transformed our\nlives, revolutionizing interactions with AI and lowering the barrier to AI\nusage. While LLMs are primarily designed for natural language interaction, the\nextensive embedded knowledge empowers them to comprehend digital sensor data.\nThis capability enables LLMs to engage with the physical world through IoT\nsensors and actuators, performing a myriad of AIoT tasks. Consequently, this\nevolution triggers a paradigm shift in conventional AIoT application\ndevelopment, democratizing its accessibility to all by facilitating the design\nand development of AIoT applications via natural language. However, some\nlimitations need to be addressed to unlock the full potential of LLMs in AIoT\napplication development. First, existing solutions often require transferring\nraw sensor data to LLM servers, which raises privacy concerns, incurs high\nquery fees, and is limited by token size. Moreover, the reasoning processes of\nLLMs are opaque to users, making it difficult to verify the robustness and\ncorrectness of inference results. This paper introduces AutoIOT, an LLM-based\nautomated program generator for AIoT applications. AutoIOT enables users to\nspecify their requirements using natural language (input) and automatically\nsynthesizes interpretable programs with documentation (output). AutoIOT\nautomates the iterative optimization to enhance the quality of generated code\nwith minimum user involvement. AutoIOT not only makes the execution of AIoT\ntasks more explainable but also mitigates privacy concerns and reduces token\ncosts with local execution of synthesized programs. Extensive experiments and\nuser studies demonstrate AutoIOT's remarkable capability in program synthesis\nfor various AIoT tasks. The synthesized programs can match and even outperform\nsome representative baselines.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05346v1",
    "published_date": "2025-03-07 11:40:52 UTC",
    "updated_date": "2025-03-07 11:40:52 UTC"
  },
  {
    "arxiv_id": "2503.05336v3",
    "title": "Toward an Evaluation Science for Generative AI Systems",
    "authors": [
      "Laura Weidinger",
      "Inioluwa Deborah Raji",
      "Hanna Wallach",
      "Margaret Mitchell",
      "Angelina Wang",
      "Olawale Salaudeen",
      "Rishi Bommasani",
      "Deep Ganguli",
      "Sanmi Koyejo",
      "William Isaac"
    ],
    "abstract": "There is an increasing imperative to anticipate and understand the\nperformance and safety of generative AI systems in real-world deployment\ncontexts. However, the current evaluation ecosystem is insufficient: Commonly\nused static benchmarks face validity challenges, and ad hoc case-by-case audits\nrarely scale. In this piece, we advocate for maturing an evaluation science for\ngenerative AI systems. While generative AI creates unique challenges for system\nsafety engineering and measurement science, the field can draw valuable\ninsights from the development of safety evaluation practices in other fields,\nincluding transportation, aerospace, and pharmaceutical engineering. In\nparticular, we present three key lessons: Evaluation metrics must be applicable\nto real-world performance, metrics must be iteratively refined, and evaluation\ninstitutions and norms must be established. Applying these insights, we outline\na concrete path toward a more rigorous approach for evaluating generative AI\nsystems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "First two authors contributed equally to this work",
    "pdf_url": "http://arxiv.org/pdf/2503.05336v3",
    "published_date": "2025-03-07 11:23:48 UTC",
    "updated_date": "2025-03-13 00:09:08 UTC"
  },
  {
    "arxiv_id": "2503.05330v1",
    "title": "Speculative Decoding for Multi-Sample Inference",
    "authors": [
      "Yiwei Li",
      "Jiayi Shi",
      "Shaoxiong Feng",
      "Peiwen Yuan",
      "Xinglin Wang",
      "Yueqi Zhang",
      "Ji Zhang",
      "Chuyi Tan",
      "Boyuan Pan",
      "Yao Hu",
      "Kan Li"
    ],
    "abstract": "We propose a novel speculative decoding method tailored for multi-sample\nreasoning scenarios, such as self-consistency and Best-of-N sampling. Our\nmethod exploits the intrinsic consensus of parallel generation paths to\nsynthesize high-quality draft tokens without requiring auxiliary models or\nexternal databases. By dynamically analyzing structural patterns across\nparallel reasoning paths through a probabilistic aggregation mechanism, it\nidentifies consensus token sequences that align with the decoding distribution.\nEvaluations on mathematical reasoning benchmarks demonstrate a substantial\nimprovement in draft acceptance rates over baselines, while reducing the\nlatency in draft token construction. This work establishes a paradigm shift for\nefficient multi-sample inference, enabling seamless integration of speculative\ndecoding with sampling-based reasoning techniques.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05330v1",
    "published_date": "2025-03-07 11:15:36 UTC",
    "updated_date": "2025-03-07 11:15:36 UTC"
  },
  {
    "arxiv_id": "2503.05328v1",
    "title": "Dynamic Knowledge Integration for Evidence-Driven Counter-Argument Generation with Large Language Models",
    "authors": [
      "Anar Yeginbergen",
      "Maite Oronoz",
      "Rodrigo Agerri"
    ],
    "abstract": "This paper investigates the role of dynamic external knowledge integration in\nimproving counter-argument generation using Large Language Models (LLMs). While\nLLMs have shown promise in argumentative tasks, their tendency to generate\nlengthy, potentially unfactual responses highlights the need for more\ncontrolled and evidence-based approaches. We introduce a new manually curated\ndataset of argument and counter-argument pairs specifically designed to balance\nargumentative complexity with evaluative feasibility. We also propose a new\nLLM-as-a-Judge evaluation methodology that shows a stronger correlation with\nhuman judgments compared to traditional reference-based metrics. Our\nexperimental results demonstrate that integrating dynamic external knowledge\nfrom the web significantly improves the quality of generated counter-arguments,\nparticularly in terms of relatedness, persuasiveness, and factuality. The\nfindings suggest that combining LLMs with real-time external knowledge\nretrieval offers a promising direction for developing more effective and\nreliable counter-argumentation systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05328v1",
    "published_date": "2025-03-07 11:13:33 UTC",
    "updated_date": "2025-03-07 11:13:33 UTC"
  },
  {
    "arxiv_id": "2503.05322v1",
    "title": "Attenuation artifact detection and severity classification in intracoronary OCT using mixed image representations",
    "authors": [
      "Pierandrea Cancian",
      "Simone Saitta",
      "Xiaojin Gu",
      "Rudolf L. M. van Herten",
      "Thijs J. Luttikholt",
      "Jos Thannhauser",
      "Rick H. J. A. Volleberg",
      "Ruben G. A. van der Waerden",
      "Joske L. van der Zande",
      "Clarisa I. Sánchez",
      "Bram van Ginneken",
      "Niels van Royen",
      "Ivana Išgum"
    ],
    "abstract": "In intracoronary optical coherence tomography (OCT), blood residues and gas\nbubbles cause attenuation artifacts that can obscure critical vessel\nstructures. The presence and severity of these artifacts may warrant\nre-acquisition, prolonging procedure time and increasing use of contrast agent.\nAccurate detection of these artifacts can guide targeted re-acquisition,\nreducing the amount of repeated scans needed to achieve diagnostically viable\nimages. However, the highly heterogeneous appearance of these artifacts poses a\nchallenge for the automated detection of the affected image regions. To enable\nautomatic detection of the attenuation artifacts caused by blood residues and\ngas bubbles based on their severity, we propose a convolutional neural network\nthat performs classification of the attenuation lines (A-lines) into three\nclasses: no artifact, mild artifact and severe artifact. Our model extracts and\nmerges features from OCT images in both Cartesian and polar coordinates, where\neach column of the image represents an A-line. Our method detects the presence\nof attenuation artifacts in OCT frames reaching F-scores of 0.77 and 0.94 for\nmild and severe artifacts, respectively. The inference time over a full OCT\nscan is approximately 6 seconds. Our experiments show that analysis of images\nrepresented in both Cartesian and polar coordinate systems outperforms the\nanalysis in polar coordinates only, suggesting that these representations\ncontain complementary features. This work lays the foundation for automated\nartifact assessment and image acquisition guidance in intracoronary OCT\nimaging.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05322v1",
    "published_date": "2025-03-07 11:01:00 UTC",
    "updated_date": "2025-03-07 11:01:00 UTC"
  },
  {
    "arxiv_id": "2503.05320v1",
    "title": "Disentangling Task Interference within Neurons: Model Merging in Alignment with Neuronal Mechanisms",
    "authors": [
      "Zitao Fang",
      "Guodong DU",
      "Shuyang Yu",
      "Yifei Guo",
      "Yiwei Zhang",
      "Jing Li",
      "Ho-Kin Tang",
      "Sim Kuan Goh"
    ],
    "abstract": "Fine-tuning pre-trained models on targeted datasets enhances task-specific\nperformance but often comes at the expense of generalization. Model merging\ntechniques, which integrate multiple fine-tuned models into a single multi-task\nmodel through task arithmetic at various levels: model, layer, or parameter,\noffer a promising solution. However, task interference remains a fundamental\nchallenge, leading to performance degradation and suboptimal merged models.\nExisting approaches largely overlook the fundamental role of individual neurons\nand their connectivity, resulting in a lack of interpretability in both the\nmerging process and the merged models. In this work, we present the first study\non the impact of neuronal alignment in model merging. We decompose\ntask-specific representations into two complementary neuronal subspaces that\nregulate neuron sensitivity and input adaptability. Leveraging this\ndecomposition, we introduce NeuroMerging, a novel merging framework developed\nto mitigate task interference within neuronal subspaces, enabling training-free\nmodel fusion across diverse tasks. Through extensive experiments, we\ndemonstrate that NeuroMerging achieves superior performance compared to\nexisting methods on multi-task benchmarks across both vision and natural\nlanguage domains. Our findings highlight the importance of aligning neuronal\nmechanisms in model merging, offering new insights into mitigating task\ninterference and improving knowledge fusion.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05320v1",
    "published_date": "2025-03-07 11:00:24 UTC",
    "updated_date": "2025-03-07 11:00:24 UTC"
  },
  {
    "arxiv_id": "2503.05319v1",
    "title": "Robust Multimodal Learning for Ophthalmic Disease Grading via Disentangled Representation",
    "authors": [
      "Xinkun Wang",
      "Yifang Wang",
      "Senwei Liang",
      "Feilong Tang",
      "Chengzhi Liu",
      "Ming Hu",
      "Chao Hu",
      "Junjun He",
      "Zongyuan Ge",
      "Imran Razzak"
    ],
    "abstract": "This paper discusses how ophthalmologists often rely on multimodal data to\nimprove diagnostic accuracy. However, complete multimodal data is rare in\nreal-world applications due to a lack of medical equipment and concerns about\ndata privacy. Traditional deep learning methods typically address these issues\nby learning representations in latent space. However, the paper highlights two\nkey limitations of these approaches: (i) Task-irrelevant redundant information\n(e.g., numerous slices) in complex modalities leads to significant redundancy\nin latent space representations. (ii) Overlapping multimodal representations\nmake it difficult to extract unique features for each modality. To overcome\nthese challenges, the authors propose the Essence-Point and Disentangle\nRepresentation Learning (EDRL) strategy, which integrates a self-distillation\nmechanism into an end-to-end framework to enhance feature selection and\ndisentanglement for more robust multimodal learning. Specifically, the\nEssence-Point Representation Learning module selects discriminative features\nthat improve disease grading performance. The Disentangled Representation\nLearning module separates multimodal data into modality-common and\nmodality-unique representations, reducing feature entanglement and enhancing\nboth robustness and interpretability in ophthalmic disease diagnosis.\nExperiments on multimodal ophthalmology datasets show that the proposed EDRL\nstrategy significantly outperforms current state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10pages",
    "pdf_url": "http://arxiv.org/pdf/2503.05319v1",
    "published_date": "2025-03-07 10:58:38 UTC",
    "updated_date": "2025-03-07 10:58:38 UTC"
  },
  {
    "arxiv_id": "2503.05318v1",
    "title": "Uncertainty-Aware Decoding with Minimum Bayes Risk",
    "authors": [
      "Nico Daheim",
      "Clara Meister",
      "Thomas Möllenhoff",
      "Iryna Gurevych"
    ],
    "abstract": "Despite their outstanding performance in the majority of scenarios,\ncontemporary language models still occasionally generate undesirable outputs,\nfor example, hallucinated text. While such behaviors have previously been\nlinked to uncertainty, there is a notable lack of methods that actively\nconsider uncertainty during text generation. In this work, we show how Minimum\nBayes Risk (MBR) decoding, which selects model generations according to an\nexpected risk, can be generalized into a principled uncertainty-aware decoding\nmethod. In short, we account for model uncertainty during decoding by\nincorporating a posterior over model parameters into MBR's computation of\nexpected risk. We show that this modified expected risk is useful for both\nchoosing outputs and deciding when to abstain from generation and can provide\nimprovements without incurring overhead. We benchmark different methods for\nlearning posteriors and show that performance improves with prediction\ndiversity. We release our code publicly.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "ICLR 2025 (Poster)",
    "pdf_url": "http://arxiv.org/pdf/2503.05318v1",
    "published_date": "2025-03-07 10:55:12 UTC",
    "updated_date": "2025-03-07 10:55:12 UTC"
  },
  {
    "arxiv_id": "2503.10652v2",
    "title": "Simulating and Analysing Human Survey Responses with Large Language Models: A Case Study in Energy Stated Preference",
    "authors": [
      "Han Wang",
      "Jacek Pawlak",
      "Aruna Sivakumar"
    ],
    "abstract": "Survey research plays a crucial role in studies by capturing consumer\npreferences and informing policy decisions. Stated preference (SP) surveys help\nresearchers understand how individuals make trade-offs in hypothetical,\npotentially futuristic, scenarios. However, traditional methods are costly,\ntime-consuming, and affected by respondent fatigue and ethical constraints.\nLarge language models (LLMs) have shown remarkable capabilities in generating\nhuman-like responses, prompting interest in their use in survey research. This\nstudy investigates LLMs for simulating consumer choices in energy-related SP\nsurveys and explores their integration into data collection and analysis\nworkflows. Test scenarios were designed to assess the simulation performance of\nseveral LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) at individual and\naggregated levels, considering prompt design, in-context learning (ICL),\nchain-of-thought (CoT) reasoning, model types, integration with traditional\nchoice models, and potential biases. While LLMs achieve accuracy above random\nguessing, performance remains insufficient for practical simulation use.\nCloud-based LLMs do not consistently outperform smaller local models.\nDeepSeek-R1 achieves the highest average accuracy (77%) and outperforms\nnon-reasoning LLMs in accuracy, factor identification, and choice distribution\nalignment. Previous SP choices are the most effective input; longer prompts\nwith more factors reduce accuracy. Mixed logit models can support LLM prompt\nrefinement. Reasoning LLMs show potential in data analysis by indicating factor\nsignificance, offering a qualitative complement to statistical models. Despite\nlimitations, pre-trained LLMs offer scalability and require minimal historical\ndata. Future work should refine prompts, further explore CoT reasoning, and\ninvestigate fine-tuning techniques.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.10652v2",
    "published_date": "2025-03-07 10:37:31 UTC",
    "updated_date": "2025-05-13 19:38:19 UTC"
  },
  {
    "arxiv_id": "2503.05306v1",
    "title": "Adversarial Policy Optimization for Offline Preference-based Reinforcement Learning",
    "authors": [
      "Hyungkyu Kang",
      "Min-hwan Oh"
    ],
    "abstract": "In this paper, we study offline preference-based reinforcement learning\n(PbRL), where learning is based on pre-collected preference feedback over pairs\nof trajectories. While offline PbRL has demonstrated remarkable empirical\nsuccess, existing theoretical approaches face challenges in ensuring\nconservatism under uncertainty, requiring computationally intractable\nconfidence set constructions. We address this limitation by proposing\nAdversarial Preference-based Policy Optimization (APPO), a computationally\nefficient algorithm for offline PbRL that guarantees sample complexity bounds\nwithout relying on explicit confidence sets. By framing PbRL as a two-player\ngame between a policy and a model, our approach enforces conservatism in a\ntractable manner. Using standard assumptions on function approximation and\nbounded trajectory concentrability, we derive a sample complexity bound. To our\nknowledge, APPO is the first offline PbRL algorithm to offer both statistical\nefficiency and practical applicability. Experimental results on continuous\ncontrol tasks demonstrate that APPO effectively learns from complex datasets,\nshowing comparable performance with existing state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05306v1",
    "published_date": "2025-03-07 10:35:01 UTC",
    "updated_date": "2025-03-07 10:35:01 UTC"
  },
  {
    "arxiv_id": "2503.05305v1",
    "title": "Frequency Autoregressive Image Generation with Continuous Tokens",
    "authors": [
      "Hu Yu",
      "Hao Luo",
      "Hangjie Yuan",
      "Yu Rong",
      "Feng Zhao"
    ],
    "abstract": "Autoregressive (AR) models for image generation typically adopt a two-stage\nparadigm of vector quantization and raster-scan ``next-token prediction\",\ninspired by its great success in language modeling. However, due to the huge\nmodality gap, image autoregressive models may require a systematic reevaluation\nfrom two perspectives: tokenizer format and regression direction. In this\npaper, we introduce the frequency progressive autoregressive (\\textbf{FAR})\nparadigm and instantiate FAR with the continuous tokenizer. Specifically, we\nidentify spectral dependency as the desirable regression direction for FAR,\nwherein higher-frequency components build upon the lower one to progressively\nconstruct a complete image. This design seamlessly fits the causality\nrequirement for autoregressive models and preserves the unique spatial locality\nof image data. Besides, we delve into the integration of FAR and the continuous\ntokenizer, introducing a series of techniques to address optimization\nchallenges and improve the efficiency of training and inference processes. We\ndemonstrate the efficacy of FAR through comprehensive experiments on the\nImageNet dataset and verify its potential on text-to-image generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05305v1",
    "published_date": "2025-03-07 10:34:04 UTC",
    "updated_date": "2025-03-07 10:34:04 UTC"
  },
  {
    "arxiv_id": "2503.05274v1",
    "title": "Evidential Uncertainty Estimation for Multi-Modal Trajectory Prediction",
    "authors": [
      "Sajad Marvi",
      "Christoph Rist",
      "Julian Schmidt",
      "Julian Jordan",
      "Abhinav Valada"
    ],
    "abstract": "Accurate trajectory prediction is crucial for autonomous driving, yet\nuncertainty in agent behavior and perception noise makes it inherently\nchallenging. While multi-modal trajectory prediction models generate multiple\nplausible future paths with associated probabilities, effectively quantifying\nuncertainty remains an open problem. In this work, we propose a novel\nmulti-modal trajectory prediction approach based on evidential deep learning\nthat estimates both positional and mode probability uncertainty in real time.\nOur approach leverages a Normal Inverse Gamma distribution for positional\nuncertainty and a Dirichlet distribution for mode uncertainty. Unlike\nsampling-based methods, it infers both types of uncertainty in a single forward\npass, significantly improving efficiency. Additionally, we experimented with\nuncertainty-driven importance sampling to improve training efficiency by\nprioritizing underrepresented high-uncertainty samples over redundant ones. We\nperform extensive evaluations of our method on the Argoverse 1 and Argoverse 2\ndatasets, demonstrating that it provides reliable uncertainty estimates while\nmaintaining high trajectory prediction accuracy.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05274v1",
    "published_date": "2025-03-07 09:46:21 UTC",
    "updated_date": "2025-03-07 09:46:21 UTC"
  },
  {
    "arxiv_id": "2503.05265v1",
    "title": "PhiloBERTA: A Transformer-Based Cross-Lingual Analysis of Greek and Latin Lexicons",
    "authors": [
      "Rumi A. Allbert",
      "Makai L. Allbert"
    ],
    "abstract": "We present PhiloBERTA, a cross-lingual transformer model that measures\nsemantic relationships between ancient Greek and Latin lexicons. Through\nanalysis of selected term pairs from classical texts, we use contextual\nembeddings and angular similarity metrics to identify precise semantic\nalignments. Our results show that etymologically related pairs demonstrate\nsignificantly higher similarity scores, particularly for abstract philosophical\nconcepts such as epist\\=em\\=e (scientia) and dikaiosyn\\=e (iustitia).\nStatistical analysis reveals consistent patterns in these relationships (p =\n0.012), with etymologically related pairs showing remarkably stable semantic\npreservation compared to control pairs. These findings establish a quantitative\nframework for examining how philosophical concepts moved between Greek and\nLatin traditions, offering new methods for classical philological research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05265v1",
    "published_date": "2025-03-07 09:30:16 UTC",
    "updated_date": "2025-03-07 09:30:16 UTC"
  },
  {
    "arxiv_id": "2503.05264v1",
    "title": "Jailbreaking is (Mostly) Simpler Than You Think",
    "authors": [
      "Mark Russinovich",
      "Ahmed Salem"
    ],
    "abstract": "We introduce the Context Compliance Attack (CCA), a novel, optimization-free\nmethod for bypassing AI safety mechanisms. Unlike current approaches -- which\nrely on complex prompt engineering and computationally intensive optimization\n-- CCA exploits a fundamental architectural vulnerability inherent in many\ndeployed AI systems. By subtly manipulating conversation history, CCA convinces\nthe model to comply with a fabricated dialogue context, thereby triggering\nrestricted behavior. Our evaluation across a diverse set of open-source and\nproprietary models demonstrates that this simple attack can circumvent\nstate-of-the-art safety protocols. We discuss the implications of these\nfindings and propose practical mitigation strategies to fortify AI systems\nagainst such elementary yet effective adversarial tactics.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05264v1",
    "published_date": "2025-03-07 09:28:19 UTC",
    "updated_date": "2025-03-07 09:28:19 UTC"
  },
  {
    "arxiv_id": "2503.05251v1",
    "title": "A Map-free Deep Learning-based Framework for Gate-to-Gate Monocular Visual Navigation aboard Miniaturized Aerial Vehicles",
    "authors": [
      "Lorenzo Scarciglia",
      "Antonio Paolillo",
      "Daniele Palossi"
    ],
    "abstract": "Palm-sized autonomous nano-drones, i.e., sub-50g in weight, recently entered\nthe drone racing scenario, where they are tasked to avoid obstacles and\nnavigate as fast as possible through gates. However, in contrast with their\nbigger counterparts, i.e., kg-scale drones, nano-drones expose three orders of\nmagnitude less onboard memory and compute power, demanding more efficient and\nlightweight vision-based pipelines to win the race. This work presents a\nmap-free vision-based (using only a monocular camera) autonomous nano-drone\nthat combines a real-time deep learning gate detection front-end with a classic\nyet elegant and effective visual servoing control back-end, only relying on\nonboard resources. Starting from two state-of-the-art tiny deep learning\nmodels, we adapt them for our specific task, and after a mixed\nsimulator-real-world training, we integrate and deploy them aboard our\nnano-drone. Our best-performing pipeline costs of only 24M multiply-accumulate\noperations per frame, resulting in a closed-loop control performance of 30 Hz,\nwhile achieving a gate detection root mean square error of 1.4 pixels, on our\n~20k real-world image dataset. In-field experiments highlight the capability of\nour nano-drone to successfully navigate through 15 gates in 4 min, never\ncrashing and covering a total travel distance of ~100m, with a peak flight\nspeed of 1.9 m/s. Finally, to stress the generalization capability of our\nsystem, we also test it in a never-seen-before environment, where it navigates\nthrough gates for more than 4 min.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "\\c{opyright}2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
    "pdf_url": "http://arxiv.org/pdf/2503.05251v1",
    "published_date": "2025-03-07 09:07:07 UTC",
    "updated_date": "2025-03-07 09:07:07 UTC"
  },
  {
    "arxiv_id": "2503.05244v3",
    "title": "WritingBench: A Comprehensive Benchmark for Generative Writing",
    "authors": [
      "Yuning Wu",
      "Jiahao Mei",
      "Ming Yan",
      "Chenliang Li",
      "Shaopeng Lai",
      "Yuran Ren",
      "Zijia Wang",
      "Ji Zhang",
      "Mengyue Wu",
      "Qin Jin",
      "Fei Huang"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have significantly\nenhanced text generation capabilities, yet evaluating their performance in\ngenerative writing remains a challenge. Existing benchmarks primarily focus on\ngeneric text generation or limited in writing tasks, failing to capture the\ndiverse requirements of high-quality written contents across various domains.\nTo bridge this gap, we present WritingBench, a comprehensive benchmark designed\nto evaluate LLMs across 6 core writing domains and 100 subdomains, encompassing\ncreative, persuasive, informative, and technical writing. We further propose a\nquery-dependent evaluation framework that empowers LLMs to dynamically generate\ninstance-specific assessment criteria. This framework is complemented by a\nfine-tuned critic model for criteria-aware scoring, enabling evaluations in\nstyle, format and length. The framework's validity is further demonstrated by\nits data curation capability, which enables 7B-parameter models to approach\nstate-of-the-art (SOTA) performance. We open-source the benchmark, along with\nevaluation tools and modular framework components, to advance the development\nof LLMs in writing.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05244v3",
    "published_date": "2025-03-07 08:56:20 UTC",
    "updated_date": "2025-03-20 05:13:53 UTC"
  },
  {
    "arxiv_id": "2503.05239v1",
    "title": "Robust Conformal Prediction with a Single Binary Certificate",
    "authors": [
      "Soroush H. Zargarbashi",
      "Aleksandar Bojchevski"
    ],
    "abstract": "Conformal prediction (CP) converts any model's output to prediction sets with\na guarantee to cover the true label with (adjustable) high probability. Robust\nCP extends this guarantee to worst-case (adversarial) inputs. Existing\nbaselines achieve robustness by bounding randomly smoothed conformity scores.\nIn practice, they need expensive Monte-Carlo (MC) sampling (e.g. $\\sim10^4$\nsamples per point) to maintain an acceptable set size. We propose a robust\nconformal prediction that produces smaller sets even with significantly lower\nMC samples (e.g. 150 for CIFAR10). Our approach binarizes samples with an\nadjustable (or automatically adjusted) threshold selected to preserve the\ncoverage guarantee. Remarkably, we prove that robustness can be achieved by\ncomputing only one binary certificate, unlike previous methods that certify\neach calibration (or test) point. Thus, our method is faster and returns\nsmaller robust sets. We also eliminate a previous limitation that requires a\nbounded score function.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.05239v1",
    "published_date": "2025-03-07 08:41:53 UTC",
    "updated_date": "2025-03-07 08:41:53 UTC"
  },
  {
    "arxiv_id": "2503.08700v1",
    "title": "Real-Time Semantic Segmentation of Aerial Images Using an Embedded U-Net: A Comparison of CPU, GPU, and FPGA Workflows",
    "authors": [
      "Julien Posso",
      "Hugo Kieffer",
      "Nicolas Menga",
      "Omar Hlimi",
      "Sébastien Tarris",
      "Hubert Guerard",
      "Guy Bois",
      "Matthieu Couderc",
      "Eric Jenn"
    ],
    "abstract": "This study introduces a lightweight U-Net model optimized for real-time\nsemantic segmentation of aerial images, targeting the efficient utilization of\nCommercial Off-The-Shelf (COTS) embedded computing platforms. We maintain the\naccuracy of the U-Net on a real-world dataset while significantly reducing the\nmodel's parameters and Multiply-Accumulate (MAC) operations by a factor of 16.\nOur comprehensive analysis covers three hardware platforms (CPU, GPU, and FPGA)\nand five different toolchains (TVM, FINN, Vitis AI, TensorFlow GPU, and cuDNN),\nassessing each on metrics such as latency, power consumption, memory footprint,\nenergy efficiency, and FPGA resource usage. The results highlight the\ntrade-offs between these platforms and toolchains, with a particular focus on\nthe practical deployment challenges in real-world applications. Our findings\ndemonstrate that while the FPGA with Vitis AI emerges as the superior choice\ndue to its performance, energy efficiency, and maturity, it requires\nspecialized hardware knowledge, emphasizing the need for a balanced approach in\nselecting embedded computing solutions for semantic segmentation tasks",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.AR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "ERTS2024, Jun 2024, Toulouse, France",
    "pdf_url": "http://arxiv.org/pdf/2503.08700v1",
    "published_date": "2025-03-07 08:33:28 UTC",
    "updated_date": "2025-03-07 08:33:28 UTC"
  },
  {
    "arxiv_id": "2503.05231v1",
    "title": "Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction",
    "authors": [
      "Shuo Jiang",
      "Haonan Li",
      "Ruochen Ren",
      "Yanmin Zhou",
      "Zhipeng Wang",
      "Bin He"
    ],
    "abstract": "Cutting-edge robot learning techniques including foundation models and\nimitation learning from humans all pose huge demands on large-scale and\nhigh-quality datasets which constitute one of the bottleneck in the general\nintelligent robot fields. This paper presents the Kaiwu multimodal dataset to\naddress the missing real-world synchronized multimodal data problems in the\nsophisticated assembling scenario,especially with dynamics information and its\nfine-grained labelling. The dataset first provides an integration of\nhuman,environment and robot data collection framework with 20 subjects and 30\ninteraction objects resulting in totally 11,664 instances of integrated\nactions. For each of the demonstration,hand motions,operation pressures,sounds\nof the assembling process,multi-view videos, high-precision motion capture\ninformation,eye gaze with first-person videos,electromyography signals are all\nrecorded. Fine-grained multi-level annotation based on absolute timestamp,and\nsemantic segmentation labelling are performed. Kaiwu dataset aims to facilitate\nrobot learning,dexterous manipulation,human intention investigation and\nhuman-robot collaboration research.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05231v1",
    "published_date": "2025-03-07 08:28:24 UTC",
    "updated_date": "2025-03-07 08:28:24 UTC"
  },
  {
    "arxiv_id": "2503.05229v1",
    "title": "Discrete Contrastive Learning for Diffusion Policies in Autonomous Driving",
    "authors": [
      "Kalle Kujanpää",
      "Daulet Baimukashev",
      "Farzeen Munir",
      "Shoaib Azam",
      "Tomasz Piotr Kucner",
      "Joni Pajarinen",
      "Ville Kyrki"
    ],
    "abstract": "Learning to perform accurate and rich simulations of human driving behaviors\nfrom data for autonomous vehicle testing remains challenging due to human\ndriving styles' high diversity and variance. We address this challenge by\nproposing a novel approach that leverages contrastive learning to extract a\ndictionary of driving styles from pre-existing human driving data. We\ndiscretize these styles with quantization, and the styles are used to learn a\nconditional diffusion policy for simulating human drivers. Our empirical\nevaluation confirms that the behaviors generated by our approach are both safer\nand more human-like than those of the machine-learning-based baseline methods.\nWe believe this has the potential to enable higher realism and more effective\ntechniques for evaluating and improving the performance of autonomous vehicles.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05229v1",
    "published_date": "2025-03-07 08:26:04 UTC",
    "updated_date": "2025-03-07 08:26:04 UTC"
  },
  {
    "arxiv_id": "2503.05227v1",
    "title": "MOHPER: Multi-objective Hyperparameter Optimization Framework for E-commerce Retrieval System",
    "authors": [
      "Jungbae Park",
      "Heonseok Jang"
    ],
    "abstract": "E-commerce search optimization has evolved to include a wider range of\nmetrics that reflect user engagement and business objectives. Modern search\nframeworks now incorporate advanced quality features, such as sales counts and\ndocument-query relevance, to better align search results with these goals.\nTraditional methods typically focus on click-through rate (CTR) as a measure of\nengagement or relevance, but this can miss true purchase intent, creating a gap\nbetween user interest and actual conversions. Joint training with the\nclick-through conversion rate (CTCVR) has become essential for understanding\nbuying behavior, although its sparsity poses challenges for reliable\noptimization. This study presents MOHPER, a Multi-Objective Hyperparameter\nOptimization framework for E-commerce Retrieval systems. Utilizing Bayesian\noptimization and sampling, it jointly optimizes both CTR, CTCVR, and relevant\nobjectives, focusing on engagement and conversion of the users. In addition, to\nimprove the selection of the best configuration from multi-objective\noptimization, we suggest advanced methods for hyperparameter selection,\nincluding a meta-configuration voting strategy and a cumulative training\napproach that leverages prior optimal configurations, to improve speeds of\ntraining and efficiency. Currently deployed in a live setting, our proposed\nframework substantiates its practical efficacy in achieving a balanced\noptimization that aligns with both user satisfaction and revenue goals.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05227v1",
    "published_date": "2025-03-07 08:25:08 UTC",
    "updated_date": "2025-03-07 08:25:08 UTC"
  },
  {
    "arxiv_id": "2503.05226v1",
    "title": "Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments",
    "authors": [
      "Xibai Wang"
    ],
    "abstract": "Monte Carlo Tree Search (MCTS) has emerged as a powerful tool for\ndecision-making in robotics, enabling efficient exploration of large search\nspaces. However, traditional MCTS methods struggle in environments\ncharacterized by high uncertainty and noisy data due to their reliance on\nfinal-step reward evaluation. The lack of intermediate feedback during search\noften results in suboptimal decision-making and computational inefficiencies.\n  This paper introduces Reward-Centered ReST-MCTS, a novel framework that\nenhances MCTS by incorporating intermediate reward shaping. The core of our\napproach is the Rewarding Center, which refines search trajectories by\ndynamically assigning partial rewards using rule-based validation, heuristic\nguidance, and neural estimation. By integrating these mechanisms, our method\nenables real-time optimization of search paths, mitigating the effects of error\npropagation.\n  We evaluate Reward-Centered ReST-MCTS in robotic manipulation tasks under\nhigh uncertainty, demonstrating consistent improvements in decision accuracy.\nCompared to baseline methods, including Chain-of-Thought (CoT) prompting and\nVanilla ReST-MCTS, our framework achieves a 2-4% accuracy improvement while\nmaintaining computational feasibility. Ablation studies confirm the\neffectiveness of intermediate feedback in search refinement, particularly in\npruning incorrect decision paths early. Furthermore, robustness tests show that\nour method retains high performance across varying levels of uncertainty.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05226v1",
    "published_date": "2025-03-07 08:25:04 UTC",
    "updated_date": "2025-03-07 08:25:04 UTC"
  },
  {
    "arxiv_id": "2503.05224v1",
    "title": "Deep Sequence Models for Predicting Average Shear Wave Velocity from Strong Motion Records",
    "authors": [
      "Baris Yilmaz",
      "Erdem Akagündüz",
      "Salih Tileylioglu"
    ],
    "abstract": "This study explores the use of deep learning for predicting the time averaged\nshear wave velocity in the top 30 m of the subsurface ($V_{s30}$) at strong\nmotion recording stations in T\\\"urkiye. $V_{s30}$ is a key parameter in site\ncharacterization and, as a result for seismic hazard assessment. However, it is\noften unavailable due to the lack of direct measurements and is therefore\nestimated using empirical correlations. Such correlations however are commonly\ninadequate in capturing complex, site-specific variability and this motivates\nthe need for data-driven approaches. In this study, we employ a hybrid deep\nlearning model combining convolutional neural networks (CNNs) and long\nshort-term memory (LSTM) networks to capture both spatial and temporal\ndependencies in strong motion records. Furthermore, we explore how using\ndifferent parts of the signal influence our deep learning model. Our results\nsuggest that the hybrid approach effectively learns complex, nonlinear\nrelationships within seismic signals. We observed that an improved P-wave\narrival time model increased the prediction accuracy of $V_{s30}$. We believe\nthe study provides valuable insights into improving $V_{s30}$ predictions using\na CNN-LSTM framework, demonstrating its potential for improving site\ncharacterization for seismic studies. Our codes are available via this repo:\nhttps://github.com/brsylmz23/CNNLSTM_DeepEQ",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05224v1",
    "published_date": "2025-03-07 08:22:50 UTC",
    "updated_date": "2025-03-07 08:22:50 UTC"
  },
  {
    "arxiv_id": "2503.05212v1",
    "title": "Knowledge Updating? No More Model Editing! Just Selective Contextual Reasoning",
    "authors": [
      "Guoxiu He",
      "Xin Song",
      "Aixin Sun"
    ],
    "abstract": "As real-world knowledge evolves, the information embedded within large\nlanguage models (LLMs) can become outdated, inadequate, or erroneous. Model\nediting has emerged as a prominent approach for updating LLMs' knowledge with\nminimal computational costs and parameter changes. This approach typically\nidentifies and adjusts specific model parameters associated with newly acquired\nknowledge. However, existing methods often underestimate the adverse effects\nthat parameter modifications can have on broadly distributed knowledge. More\ncritically, post-edit LLMs frequently struggle with multi-hop reasoning and\ncontinuous knowledge updates. Although various studies have discussed these\nshortcomings, there is a lack of comprehensive evaluation. In this paper, we\nprovide an evaluation of ten model editing methods along four dimensions:\nreliability, generalization, locality, and portability. Results confirm that\nall ten popular model editing methods show significant shortcomings across\nmultiple dimensions, suggesting model editing is less promising. We then\npropose a straightforward method called Selective Contextual Reasoning (SCR),\nfor knowledge updating. SCR does not modify model parameters but harnesses\nLLM's inherent contextual reasoning capabilities utilizing the updated\nknowledge pieces. Under SCR, an LLM first assesses whether an incoming query\nfalls within the scope of an external knowledge base. If it does, the relevant\nexternal knowledge texts are contextualized to enhance reasoning; otherwise,\nthe query is answered directly. We evaluate SCR against the ten model editing\nmethods on two counterfactual datasets with three backbone LLMs. Empirical\nresults confirm the effectiveness and efficiency of contextual reasoning for\nknowledge updating.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05212v1",
    "published_date": "2025-03-07 08:04:25 UTC",
    "updated_date": "2025-03-07 08:04:25 UTC"
  },
  {
    "arxiv_id": "2503.07655v1",
    "title": "GraphT5: Unified Molecular Graph-Language Modeling via Multi-Modal Cross-Token Attention",
    "authors": [
      "Sangyeup Kim",
      "Nayeon Kim",
      "Yinhua Piao",
      "Sun Kim"
    ],
    "abstract": "Molecular language modeling tasks such as molecule captioning have been\nrecognized for their potential to further understand molecular properties that\ncan aid drug discovery or material synthesis based on chemical reactions.\nUnlike the common use of molecule graphs in predicting molecular properties,\nmost methods in molecular language modeling rely heavily on SMILES sequences.\nThis preference is because the task involves generating a sequence of multiple\ntokens using transformer-based models. Therefore, a main challenge is\ndetermining how to integrate graph data, which contains structural and spatial\ninformation about molecules, with text data. In addition, simply using both 1D\nSMILES text and 2D graph as inputs without addressing how they align and\nrepresent the molecule structure in different modalities makes it challenging\nto fully utilize structural knowledge about molecules. To this end, we propose\nGraphT5, a multi-modal framework that integrates 1D SMILES text and 2D graph\nrepresentations of molecules for molecular language modeling. Specifically, we\nintroduce a novel cross-token attention module in GraphT5 to bridge the gap\narising from the fundamental differences between the two modalities of molecule\nrepresentations. Cross-token attention exploits implicit information between\nSMILES and graphs of molecules, resulting from their interactions at a\nfine-grained token level that benefits molecular language modeling. Extensive\nexperiments including molecule captioning, IUPAC name prediction tasks, and\ncase studies show that our GraphT5 outperforms the latest baseline approaches,\nwhich validates the effectiveness of our GraphT5 in sufficiently utilizing 1D\nSMILES text and 2D graph representations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.07655v1",
    "published_date": "2025-03-07 07:57:16 UTC",
    "updated_date": "2025-03-07 07:57:16 UTC"
  },
  {
    "arxiv_id": "2503.05207v1",
    "title": "Policy Constraint by Only Support Constraint for Offline Reinforcement Learning",
    "authors": [
      "Yunkai Gao",
      "Jiaming Guo",
      "Fan Wu",
      "Rui Zhang"
    ],
    "abstract": "Offline reinforcement learning (RL) aims to optimize a policy by using\npre-collected datasets, to maximize cumulative rewards. However, offline\nreinforcement learning suffers challenges due to the distributional shift\nbetween the learned and behavior policies, leading to errors when computing\nQ-values for out-of-distribution (OOD) actions. To mitigate this issue, policy\nconstraint methods aim to constrain the learned policy's distribution with the\ndistribution of the behavior policy or confine action selection within the\nsupport of the behavior policy. However, current policy constraint methods tend\nto exhibit excessive conservatism, hindering the policy from further surpassing\nthe behavior policy's performance. In this work, we present Only Support\nConstraint (OSC) which is derived from maximizing the total probability of\nlearned policy in the support of behavior policy, to address the conservatism\nof policy constraint. OSC presents a regularization term that only restricts\npolicies to the support without imposing extra constraints on actions within\nthe support. Additionally, to fully harness the performance of the new policy\nconstraints, OSC utilizes a diffusion model to effectively characterize the\nsupport of behavior policies. Experimental evaluations across a variety of\noffline RL benchmarks demonstrate that OSC significantly enhances performance,\nalleviating the challenges associated with distributional shifts and mitigating\nconservatism of policy constraints. Code is available at\nhttps://github.com/MoreanP/OSC.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05207v1",
    "published_date": "2025-03-07 07:55:51 UTC",
    "updated_date": "2025-03-07 07:55:51 UTC"
  },
  {
    "arxiv_id": "2503.05203v1",
    "title": "Path Pooling: Train-Free Structure Enhancement for Efficient Knowledge Graph Retrieval-Augmented Generation",
    "authors": [
      "Hairu Wang",
      "Yuan Feng",
      "Xike Xie",
      "S Kevin Zhou"
    ],
    "abstract": "Although Large Language Models achieve strong success in many tasks, they\nstill suffer from hallucinations and knowledge deficiencies in real-world\napplications. Many knowledge graph-based retrieval-augmented generation\n(KG-RAG) methods enhance the quality and credibility of LLMs by leveraging\nstructure and semantic information in KGs as external knowledge bases. However,\nthese methods struggle to effectively incorporate structure information, either\nincurring high computational costs or underutilizing available knowledge.\nInspired by smoothing operations in graph representation learning, we propose\npath pooling, a simple, train-free strategy that introduces structure\ninformation through a novel path-centric pooling operation. It seamlessly\nintegrates into existing KG-RAG methods in a plug-and-play manner, enabling\nricher structure information utilization. Extensive experiments demonstrate\nthat incorporating the path pooling into the state-of-the-art KG-RAG method\nconsistently improves performance across various settings while introducing\nnegligible additional cost. Code is coming soon at\nhttps://github.com/hrwang00/path-pooling.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05203v1",
    "published_date": "2025-03-07 07:48:30 UTC",
    "updated_date": "2025-03-07 07:48:30 UTC"
  },
  {
    "arxiv_id": "2503.05201v1",
    "title": "Deep Muscle EMG construction using A Physics-Integrated Deep Learning approach",
    "authors": [
      "Rajnish Kumar",
      "Tapas Tripura",
      "Souvik Chakraborty",
      "Sitikantha Roy"
    ],
    "abstract": "Electromyography (EMG)--based computational musculoskeletal modeling is a\nnon-invasive method for studying musculotendon function, human movement, and\nneuromuscular control, providing estimates of internal variables like muscle\nforces and joint torques. However, EMG signals from deeper muscles are often\nchallenging to measure by placing the surface EMG electrodes and unfeasible to\nmeasure directly using invasive methods. The restriction to the access of EMG\ndata from deeper muscles poses a considerable obstacle to the broad adoption of\nEMG-driven modeling techniques. A strategic alternative is to use an estimation\nalgorithm to approximate the missing EMG signals from deeper muscle. A similar\nstrategy is used in physics-informed deep learning, where the features of\nphysical systems are learned without labeled data. In this work, we propose a\nhybrid deep learning algorithm, namely the neural musculoskeletal model (NMM),\nthat integrates physics-informed and data-driven deep learning to approximate\nthe EMG signals from the deeper muscles. While data-driven modeling is used to\npredict the missing EMG signals, physics-based modeling engraves the\nsubject-specific information into the predictions. Experimental verifications\non five test subjects are carried out to investigate the performance of the\nproposed hybrid framework. The proposed NMM is validated against the joint\ntorque computed from 'OpenSim' software. The predicted deep EMG signals are\nalso compared against the state-of-the-art muscle synergy extrapolation (MSE)\napproach, where the proposed NMM completely outperforms the existing MSE\nframework by a significant margin.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05201v1",
    "published_date": "2025-03-07 07:46:26 UTC",
    "updated_date": "2025-03-07 07:46:26 UTC"
  },
  {
    "arxiv_id": "2503.05200v1",
    "title": "ORANSight-2.0: Foundational LLMs for O-RAN",
    "authors": [
      "Pranshav Gajjar",
      "Vijay K. Shah"
    ],
    "abstract": "Despite the transformative impact of Large Language Models (LLMs) across\ncritical domains such as healthcare, customer service, and business marketing,\ntheir integration into Open Radio Access Networks (O-RAN) remains limited. This\ngap is primarily due to the absence of domain-specific foundational models,\nwith existing solutions often relying on general-purpose LLMs that fail to\naddress the unique challenges and technical intricacies of O-RAN. To bridge\nthis gap, we introduce ORANSight-2.0 (O-RAN Insights), a pioneering initiative\naimed at developing specialized foundational LLMs tailored for O-RAN. Built on\n18 LLMs spanning five open-source LLM frameworks, ORANSight-2.0 fine-tunes\nmodels ranging from 1 to 70B parameters, significantly reducing reliance on\nproprietary, closed-source models while enhancing performance for O-RAN. At the\ncore of ORANSight-2.0 is RANSTRUCT, a novel Retrieval-Augmented Generation\n(RAG) based instruction-tuning framework that employs two LLM agents to create\nhigh-quality instruction-tuning datasets. The generated dataset is then used to\nfine-tune the 18 pre-trained open-source LLMs via QLoRA. To evaluate\nORANSight-2.0, we introduce srsRANBench, a novel benchmark designed for code\ngeneration and codebase understanding in the context of srsRAN, a widely used\n5G O-RAN stack. We also leverage ORANBench13K, an existing benchmark for\nassessing O-RAN-specific knowledge. Our comprehensive evaluations demonstrate\nthat ORANSight-2.0 models outperform general-purpose and closed-source models,\nsuch as ChatGPT-4o and Gemini, by 5.421% on ORANBench and 18.465% on\nsrsRANBench, achieving superior performance while maintaining lower\ncomputational and energy costs. We also experiment with RAG-augmented variants\nof ORANSight-2.0 LLMs and thoroughly evaluate their energy characteristics,\ndemonstrating costs for training, standard inference, and RAG-augmented\ninference.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05200v1",
    "published_date": "2025-03-07 07:44:31 UTC",
    "updated_date": "2025-03-07 07:44:31 UTC"
  },
  {
    "arxiv_id": "2503.05194v1",
    "title": "Uncertainty-Aware Explainable Federated Learning",
    "authors": [
      "Yanci Zhang",
      "Han Yu"
    ],
    "abstract": "Federated Learning (FL) is a collaborative machine learning paradigm for\nenhancing data privacy preservation. Its privacy-preserving nature complicates\nthe explanation of the decision-making processes and the evaluation of the\nreliability of the generated explanations. In this paper, we propose the\nUncertainty-aware eXplainable Federated Learning (UncertainXFL) to address\nthese challenges. It generates explanations for decision-making processes under\nFL settings and provides information regarding the uncertainty of these\nexplanations. UncertainXFL is the first framework to explicitly offer\nuncertainty evaluation for explanations within the FL context. Explanatory\ninformation is initially generated by the FL clients and then aggregated by the\nserver in a comprehensive and conflict-free manner during FL training. The\nquality of the explanations, including the uncertainty score and tested\nvalidity, guides the FL training process by prioritizing clients with the most\nreliable explanations through higher weights during model aggregation.\nExtensive experimental evaluation results demonstrate that UncertainXFL\nachieves superior model accuracy and explanation accuracy, surpassing the\ncurrent state-of-the-art model that does not incorporate uncertainty\ninformation by 2.71% and 1.77%, respectively. By integrating and quantifying\nuncertainty in the data into the explanation process, UncertainXFL not only\nclearly presents the explanation alongside its uncertainty, but also leverages\nthis uncertainty to guide the FL training process, thereby enhancing the\nrobustness and reliability of the resulting models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05194v1",
    "published_date": "2025-03-07 07:29:48 UTC",
    "updated_date": "2025-03-07 07:29:48 UTC"
  },
  {
    "arxiv_id": "2503.05188v1",
    "title": "Rewarding Curse: Analyze and Mitigate Reward Modeling Issues for LLM Reasoning",
    "authors": [
      "Jiachun Li",
      "Pengfei Cao",
      "Yubo Chen",
      "Jiexin Xu",
      "Huaijun Li",
      "Xiaojian Jiang",
      "Kang Liu",
      "Jun Zhao"
    ],
    "abstract": "Chain-of-thought (CoT) prompting demonstrates varying performance under\ndifferent reasoning tasks. Previous work attempts to evaluate it but falls\nshort in providing an in-depth analysis of patterns that influence the CoT. In\nthis paper, we study the CoT performance from the perspective of effectiveness\nand faithfulness. For the former, we identify key factors that influence CoT\neffectiveness on performance improvement, including problem difficulty,\ninformation gain, and information flow. For the latter, we interpret the\nunfaithful CoT issue by conducting a joint analysis of the information\ninteraction among the question, CoT, and answer. The result demonstrates that,\nwhen the LLM predicts answers, it can recall correct information missing in the\nCoT from the question, leading to the problem. Finally, we propose a novel\nalgorithm to mitigate this issue, in which we recall extra information from the\nquestion to enhance the CoT generation and evaluate CoTs based on their\ninformation gain. Extensive experiments demonstrate that our approach enhances\nboth the faithfulness and effectiveness of CoT.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "18 pages, 21 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05188v1",
    "published_date": "2025-03-07 07:20:24 UTC",
    "updated_date": "2025-03-07 07:20:24 UTC"
  },
  {
    "arxiv_id": "2503.05185v1",
    "title": "FinTMMBench: Benchmarking Temporal-Aware Multi-Modal RAG in Finance",
    "authors": [
      "Fengbin Zhu",
      "Junfeng Li",
      "Liangming Pan",
      "Wenjie Wang",
      "Fuli Feng",
      "Chao Wang",
      "Huanbo Luan",
      "Tat-Seng Chua"
    ],
    "abstract": "Finance decision-making often relies on in-depth data analysis across various\ndata sources, including financial tables, news articles, stock prices, etc. In\nthis work, we introduce FinTMMBench, the first comprehensive benchmark for\nevaluating temporal-aware multi-modal Retrieval-Augmented Generation (RAG)\nsystems in finance. Built from heterologous data of NASDAQ 100 companies,\nFinTMMBench offers three significant advantages. 1) Multi-modal Corpus: It\nencompasses a hybrid of financial tables, news articles, daily stock prices,\nand visual technical charts as the corpus. 2) Temporal-aware Questions: Each\nquestion requires the retrieval and interpretation of its relevant data over a\nspecific time period, including daily, weekly, monthly, quarterly, and annual\nperiods. 3) Diverse Financial Analysis Tasks: The questions involve 10\ndifferent tasks, including information extraction, trend analysis, sentiment\nanalysis and event detection, etc. We further propose a novel TMMHybridRAG\nmethod, which first leverages LLMs to convert data from other modalities (e.g.,\ntabular, visual and time-series data) into textual format and then incorporates\ntemporal information in each node when constructing graphs and dense indexes.\nIts effectiveness has been validated in extensive experiments, but notable gaps\nremain, highlighting the challenges presented by our FinTMMBench.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "q-fin.CP",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2503.05185v1",
    "published_date": "2025-03-07 07:13:59 UTC",
    "updated_date": "2025-03-07 07:13:59 UTC"
  },
  {
    "arxiv_id": "2503.05179v2",
    "title": "Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching",
    "authors": [
      "Simon A. Aytes",
      "Jinheon Baek",
      "Sung Ju Hwang"
    ],
    "abstract": "Recent advances in large language models (LLMs) have enabled strong reasoning\ncapabilities through Chain-of-Thought (CoT) prompting, which elicits\nstep-by-step problem solving, but often at the cost of excessive verbosity in\nintermediate outputs, leading to increased computational overhead. We propose\nSketch-of-Thought (SoT), a prompting framework that integrates cognitively\ninspired reasoning paradigms with linguistic constraints to reduce token usage\nwhile preserving reasoning accuracy. SoT is designed as a flexible, modular\napproach and is instantiated with three paradigms--Conceptual Chaining, Chunked\nSymbolism, and Expert Lexicons--each tailored to distinct reasoning tasks and\nselected dynamically at test-time by a lightweight routing model. Across 15\nreasoning datasets spanning multiple domains, languages, and modalities, SoT\nachieves token reductions of up to 78% with minimal accuracy loss. In tasks\nsuch as mathematical and multi-hop reasoning, it even improves accuracy while\nshortening outputs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05179v2",
    "published_date": "2025-03-07 06:57:17 UTC",
    "updated_date": "2025-05-21 07:47:26 UTC"
  },
  {
    "arxiv_id": "2503.05846v1",
    "title": "Extracting and Emulsifying Cultural Explanation to Improve Multilingual Capability of LLMs",
    "authors": [
      "Hamin Koo",
      "Jaehyung Kim"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable success, but their\nEnglish-centric training data limits performance in non-English languages,\nhighlighting the need for enhancements in their multilingual capabilities.\nWhile some work on multilingual prompting methods handles non-English queries\nby utilizing English translations or restructuring them to more closely align\nwith LLM reasoning patterns, these works often overlook the importance of\ncultural context, limiting their effectiveness. To address this limitation, we\npropose EMCEI, a simple yet effective approach that improves LLMs' multilingual\ncapabilities by incorporating cultural context for more accurate and\nappropriate responses. Specifically, EMCEI follows a two-step process that\nfirst extracts relevant cultural context from the LLM's parametric knowledge\nvia prompting. Then, EMCEI employs an LLM-as-Judge mechanism to select the most\nappropriate response by balancing cultural relevance and reasoning ability.\nExperiments on diverse multilingual benchmarks show that EMCEI outperforms\nexisting baselines, demonstrating its effectiveness in handling multilingual\nqueries with LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "under review, 18pages",
    "pdf_url": "http://arxiv.org/pdf/2503.05846v1",
    "published_date": "2025-03-07 06:05:34 UTC",
    "updated_date": "2025-03-07 06:05:34 UTC"
  },
  {
    "arxiv_id": "2503.05164v1",
    "title": "A Comprehensive LLM-powered Framework for Driving Intelligence Evaluation",
    "authors": [
      "Shanhe You",
      "Xuewen Luo",
      "Xinhe Liang",
      "Jiashu Yu",
      "Chen Zheng",
      "Jiangtao Gong"
    ],
    "abstract": "Evaluation methods for autonomous driving are crucial for algorithm\noptimization. However, due to the complexity of driving intelligence, there is\ncurrently no comprehensive evaluation method for the level of autonomous\ndriving intelligence. In this paper, we propose an evaluation framework for\ndriving behavior intelligence in complex traffic environments, aiming to fill\nthis gap. We constructed a natural language evaluation dataset of human\nprofessional drivers and passengers through naturalistic driving experiments\nand post-driving behavior evaluation interviews. Based on this dataset, we\ndeveloped an LLM-powered driving evaluation framework. The effectiveness of\nthis framework was validated through simulated experiments in the CARLA urban\ntraffic simulator and further corroborated by human assessment. Our research\nprovides valuable insights for evaluating and designing more intelligent,\nhuman-like autonomous driving agents. The implementation details of the\nframework and detailed information about the dataset can be found at Github.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "68T45"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05164v1",
    "published_date": "2025-03-07 06:03:02 UTC",
    "updated_date": "2025-03-07 06:03:02 UTC"
  },
  {
    "arxiv_id": "2503.05845v1",
    "title": "Machine Learned Force Fields: Fundamentals, its reach, and challenges",
    "authors": [
      "Carlos A. Vital",
      "Román J. Armenta-Rico",
      "Huziel E. Sauceda"
    ],
    "abstract": "Highly accurate force fields are a mandatory requirement to generate\npredictive simulations. In this regard, Machine Learning Force Fields (MLFFs)\nhave emerged as a revolutionary approach in computational chemistry and\nmaterials science, combining the accuracy of quantum mechanical methods with\ncomputational efficiency orders of magnitude superior to ab-initio methods.\nThis chapter provides an introduction of the fundamentals of learning and how\nit is applied to construct MLFFs, detailing key methodologies such as neural\nnetwork potentials and kernel-based models. Emphasis is placed on the\nconstruction of SchNet model, as one of the most elemental neural network-based\nforce fields that are nowadays the basis of modern architectures. Additionally,\nthe GDML framework is described in detail as an example of how the elegant\nformulation of kernel methods can be used to construct mathematically robust\nand physics-inspired MLFFs. The ongoing advancements in MLFF development\ncontinue to expand their applicability, enabling precise simulations of large\nand complex systems that were previously beyond reach. This chapter concludes\nby highlighting the transformative impact of MLFFs on scientific research,\nunderscoring their role in driving future discoveries in the fields of\nchemistry, physics, and materials science.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "9 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05845v1",
    "published_date": "2025-03-07 05:26:14 UTC",
    "updated_date": "2025-03-07 05:26:14 UTC"
  },
  {
    "arxiv_id": "2503.05153v2",
    "title": "Generative Trajectory Stitching through Diffusion Composition",
    "authors": [
      "Yunhao Luo",
      "Utkarsh A. Mishra",
      "Yilun Du",
      "Danfei Xu"
    ],
    "abstract": "Effective trajectory stitching for long-horizon planning is a significant\nchallenge in robotic decision-making. While diffusion models have shown promise\nin planning, they are limited to solving tasks similar to those seen in their\ntraining data. We propose CompDiffuser, a novel generative approach that can\nsolve new tasks by learning to compositionally stitch together shorter\ntrajectory chunks from previously seen tasks. Our key insight is modeling the\ntrajectory distribution by subdividing it into overlapping chunks and learning\ntheir conditional relationships through a single bidirectional diffusion model.\nThis allows information to propagate between segments during generation,\nensuring physically consistent connections. We conduct experiments on benchmark\ntasks of various difficulties, covering different environment sizes, agent\nstate dimension, trajectory types, training data quality, and show that\nCompDiffuser significantly outperforms existing methods.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Project page: https://comp-diffuser.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.05153v2",
    "published_date": "2025-03-07 05:22:52 UTC",
    "updated_date": "2025-05-05 16:26:30 UTC"
  },
  {
    "arxiv_id": "2503.05149v1",
    "title": "Development and Enhancement of Text-to-Image Diffusion Models",
    "authors": [
      "Rajdeep Roshan Sahu"
    ],
    "abstract": "This research focuses on the development and enhancement of text-to-image\ndenoising diffusion models, addressing key challenges such as limited sample\ndiversity and training instability. By incorporating Classifier-Free Guidance\n(CFG) and Exponential Moving Average (EMA) techniques, this study significantly\nimproves image quality, diversity, and stability. Utilizing Hugging Face's\nstate-of-the-art text-to-image generation model, the proposed enhancements\nestablish new benchmarks in generative AI. This work explores the underlying\nprinciples of diffusion models, implements advanced strategies to overcome\nexisting limitations, and presents a comprehensive evaluation of the\nimprovements achieved. Results demonstrate substantial progress in generating\nstable, diverse, and high-quality images from textual descriptions, advancing\nthe field of generative artificial intelligence and providing new foundations\nfor future applications.\n  Keywords: Text-to-image, Diffusion model, Classifier-free guidance,\nExponential moving average, Image generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05149v1",
    "published_date": "2025-03-07 05:18:00 UTC",
    "updated_date": "2025-03-07 05:18:00 UTC"
  },
  {
    "arxiv_id": "2503.05143v1",
    "title": "FedMABench: Benchmarking Mobile Agents on Decentralized Heterogeneous User Data",
    "authors": [
      "Wenhao Wang",
      "Zijie Yu",
      "Rui Ye",
      "Jianqing Zhang",
      "Siheng Chen",
      "Yanfeng Wang"
    ],
    "abstract": "Mobile agents have attracted tremendous research participation recently.\nTraditional approaches to mobile agent training rely on centralized data\ncollection, leading to high cost and limited scalability. Distributed training\nutilizing federated learning offers an alternative by harnessing real-world\nuser data, providing scalability and reducing costs. However, pivotal\nchallenges, including the absence of standardized benchmarks, hinder progress\nin this field.\n  To tackle the challenges, we introduce FedMABench, the first benchmark for\nfederated training and evaluation of mobile agents, specifically designed for\nheterogeneous scenarios. FedMABench features 6 datasets with 30+ subsets, 8\nfederated algorithms, 10+ base models, and over 800 apps across 5 categories,\nproviding a comprehensive framework for evaluating mobile agents across diverse\nenvironments. Through extensive experiments, we uncover several key insights:\nfederated algorithms consistently outperform local training; the distribution\nof specific apps plays a crucial role in heterogeneity; and, even apps from\ndistinct categories can exhibit correlations during training. FedMABench is\npublicly available at: https://github.com/wwh0411/FedMABench with the datasets\nat: https://huggingface.co/datasets/wwh0411/FedMABench.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05143v1",
    "published_date": "2025-03-07 04:52:20 UTC",
    "updated_date": "2025-03-07 04:52:20 UTC"
  },
  {
    "arxiv_id": "2503.05139v2",
    "title": "Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs",
    "authors": [
      "Ling Team",
      "Binwei Zeng",
      "Chao Huang",
      "Chao Zhang",
      "Changxin Tian",
      "Cong Chen",
      "Dingnan Jin",
      "Feng Yu",
      "Feng Zhu",
      "Feng Yuan",
      "Fakang Wang",
      "Gangshan Wang",
      "Guangyao Zhai",
      "Haitao Zhang",
      "Huizhong Li",
      "Jun Zhou",
      "Jia Liu",
      "Junpeng Fang",
      "Junjie Ou",
      "Jun Hu",
      "Ji Luo",
      "Ji Zhang",
      "Jian Liu",
      "Jian Sha",
      "Jianxue Qian",
      "Jiewei Wu",
      "Junping Zhao",
      "Jianguo Li",
      "Jubao Feng",
      "Jingchao Di",
      "Junming Xu",
      "Jinghua Yao",
      "Kuan Xu",
      "Kewei Du",
      "Longfei Li",
      "Lei Liang",
      "Lu Yu",
      "Li Tang",
      "Lin Ju",
      "Peng Xu",
      "Qing Cui",
      "Song Liu",
      "Shicheng Li",
      "Shun Song",
      "Song Yan",
      "Tengwei Cai",
      "Tianyi Chen",
      "Ting Guo",
      "Ting Huang",
      "Tao Feng",
      "Tao Wu",
      "Wei Wu",
      "Xiaolu Zhang",
      "Xueming Yang",
      "Xin Zhao",
      "Xiaobo Hu",
      "Xin Lin",
      "Yao Zhao",
      "Yilong Wang",
      "Yongzhen Guo",
      "Yuanyuan Wang",
      "Yue Yang",
      "Yang Cao",
      "Yuhao Fu",
      "Yi Xiong",
      "Yanzhe Li",
      "Zhe Li",
      "Zhiqiang Zhang",
      "Ziqi Liu",
      "Zhaoxin Huan",
      "Zujie Wen",
      "Zhenhang Sun",
      "Zhuoxuan Du",
      "Zhengyu He"
    ],
    "abstract": "In this technical report, we tackle the challenges of training large-scale\nMixture of Experts (MoE) models, focusing on overcoming cost inefficiency and\nresource limitations prevalent in such systems. To address these issues, we\npresent two differently sized MoE large language models (LLMs), namely\nLing-Lite and Ling-Plus (referred to as \"Bailing\" in Chinese, spelled\nB\\v{a}il\\'ing in Pinyin). Ling-Lite contains 16.8 billion parameters with 2.75\nbillion activated parameters, while Ling-Plus boasts 290 billion parameters\nwith 28.8 billion activated parameters. Both models exhibit comparable\nperformance to leading industry benchmarks. This report offers actionable\ninsights to improve the efficiency and accessibility of AI development in\nresource-constrained settings, promoting more scalable and sustainable\ntechnologies. Specifically, to reduce training costs for large-scale MoE\nmodels, we propose innovative methods for (1) optimization of model\narchitecture and training processes, (2) refinement of training anomaly\nhandling, and (3) enhancement of model evaluation efficiency. Additionally,\nleveraging high-quality data generated from knowledge graphs, our models\ndemonstrate superior capabilities in tool use compared to other models.\nUltimately, our experimental findings demonstrate that a 300B MoE LLM can be\neffectively trained on lower-performance devices while achieving comparable\nperformance to models of a similar scale, including dense and MoE models.\nCompared to high-performance devices, utilizing a lower-specification hardware\nsystem during the pre-training phase demonstrates significant cost savings,\nreducing computing costs by approximately 20%. The models can be accessed at\nhttps://huggingface.co/inclusionAI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "34 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.05139v2",
    "published_date": "2025-03-07 04:43:39 UTC",
    "updated_date": "2025-03-10 14:21:21 UTC"
  },
  {
    "arxiv_id": "2503.05132v2",
    "title": "R1-Zero's \"Aha Moment\" in Visual Reasoning on a 2B Non-SFT Model",
    "authors": [
      "Hengguang Zhou",
      "Xirui Li",
      "Ruochen Wang",
      "Minhao Cheng",
      "Tianyi Zhou",
      "Cho-Jui Hsieh"
    ],
    "abstract": "Recently DeepSeek R1 demonstrated how reinforcement learning with simple\nrule-based incentives can enable autonomous development of complex reasoning in\nlarge language models, characterized by the \"aha moment\", in which the model\nmanifest self-reflection and increased response length during training.\nHowever, attempts to extend this success to multimodal reasoning often failed\nto reproduce these key characteristics. In this report, we present the first\nsuccessful replication of these emergent characteristics for multimodal\nreasoning on only a non-SFT 2B model. Starting with Qwen2-VL-2B and applying\nreinforcement learning directly on the SAT dataset, our model achieves 59.47%\naccuracy on CVBench, outperforming the base model by approximately ~30% and\nexceeding both SFT setting by ~2%. In addition, we share our failed attempts\nand insights in attempting to achieve R1-like reasoning using RL with instruct\nmodels. aiming to shed light on the challenges involved. Our key observations\ninclude: (1) applying RL on instruct model often results in trivial reasoning\ntrajectories, and (2) naive length reward are ineffective in eliciting\nreasoning capabilities. The project code is available at\nhttps://github.com/turningpoint-ai/VisualThinker-R1-Zero",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05132v2",
    "published_date": "2025-03-07 04:21:47 UTC",
    "updated_date": "2025-03-10 01:52:08 UTC"
  },
  {
    "arxiv_id": "2503.05127v1",
    "title": "HexPlane Representation for 3D Semantic Scene Understanding",
    "authors": [
      "Zeren Chen",
      "Yuenan Hou",
      "Yulin Chen",
      "Li Liu",
      "Xiao Sun",
      "Lu Sheng"
    ],
    "abstract": "In this paper, we introduce the HexPlane representation for 3D semantic scene\nunderstanding. Specifically, we first design the View Projection Module (VPM)\nto project the 3D point cloud into six planes to maximally retain the original\nspatial information. Features of six planes are extracted by the 2D encoder and\nsent to the HexPlane Association Module (HAM) to adaptively fuse the most\ninformative information for each point. The fused point features are further\nfed to the task head to yield the ultimate predictions. Compared to the popular\npoint and voxel representation, the HexPlane representation is efficient and\ncan utilize highly optimized 2D operations to process sparse and unordered 3D\npoint clouds. It can also leverage off-the-shelf 2D models, network weights,\nand training recipes to achieve accurate scene understanding in 3D space. On\nScanNet and SemanticKITTI benchmarks, our algorithm, dubbed HexNet3D, achieves\ncompetitive performance with previous algorithms. In particular, on the ScanNet\n3D segmentation task, our method obtains 77.0 mIoU on the validation set,\nsurpassing Point Transformer V2 by 1.6 mIoU. We also observe encouraging\nresults in indoor 3D detection tasks. Note that our method can be seamlessly\nintegrated into existing voxel-based, point-based, and range-based approaches\nand brings considerable gains without bells and whistles. The codes will be\navailable upon publication.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "7 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05127v1",
    "published_date": "2025-03-07 04:18:55 UTC",
    "updated_date": "2025-03-07 04:18:55 UTC"
  },
  {
    "arxiv_id": "2503.05126v3",
    "title": "Multi-Task Reinforcement Learning Enables Parameter Scaling",
    "authors": [
      "Reginald McLean",
      "Evangelos Chatzaroulas",
      "Jordan Terry",
      "Isaac Woungang",
      "Nariman Farsad",
      "Pablo Samuel Castro"
    ],
    "abstract": "Multi-task reinforcement learning (MTRL) aims to endow a single agent with\nthe ability to perform well on multiple tasks. Recent works have focused on\ndeveloping novel sophisticated architectures to improve performance, often\nresulting in larger models; it is unclear, however, whether the performance\ngains are a consequence of the architecture design itself or the extra\nparameters. We argue that gains are mostly due to scale by demonstrating that\nnaively scaling up a simple MTRL baseline to match parameter counts outperforms\nthe more sophisticated architectures, and these gains benefit most from scaling\nthe critic over the actor. Additionally, we explore the training stability\nadvantages that come with task diversity, demonstrating that increasing the\nnumber of tasks can help mitigate plasticity loss. Our findings suggest that\nMTRL's simultaneous training across multiple tasks provides a natural framework\nfor beneficial parameter scaling in reinforcement learning, challenging the\nneed for complex architectural innovations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05126v3",
    "published_date": "2025-03-07 04:13:02 UTC",
    "updated_date": "2025-03-12 16:43:00 UTC"
  },
  {
    "arxiv_id": "2503.05114v1",
    "title": "Look Before You Leap: Using Serialized State Machine for Language Conditioned Robotic Manipulation",
    "authors": [
      "Tong Mu",
      "Yihao Liu",
      "Mehran Armand"
    ],
    "abstract": "Imitation learning frameworks for robotic manipulation have drawn attention\nin the recent development of language model grounded robotics. However, the\nsuccess of the frameworks largely depends on the coverage of the demonstration\ncases: When the demonstration set does not include examples of how to act in\nall possible situations, the action may fail and can result in cascading\nerrors. To solve this problem, we propose a framework that uses serialized\nFinite State Machine (FSM) to generate demonstrations and improve the success\nrate in manipulation tasks requiring a long sequence of precise interactions.\nTo validate its effectiveness, we use environmentally evolving and long-horizon\npuzzles that require long sequential actions. Experimental results show that\nour approach achieves a success rate of up to 98 in these tasks, compared to\nthe controlled condition using existing approaches, which only had a success\nrate of up to 60, and, in some tasks, almost failed completely.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "7 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.05114v1",
    "published_date": "2025-03-07 03:19:25 UTC",
    "updated_date": "2025-03-07 03:19:25 UTC"
  },
  {
    "arxiv_id": "2503.05108v1",
    "title": "TS-LIF: A Temporal Segment Spiking Neuron Network for Time Series Forecasting",
    "authors": [
      "Shibo Feng",
      "Wanjin Feng",
      "Xingyu Gao",
      "Peilin Zhao",
      "Zhiqi Shen"
    ],
    "abstract": "Spiking Neural Networks (SNNs) offer a promising, biologically inspired\napproach for processing spatiotemporal data, particularly for time series\nforecasting. However, conventional neuron models like the Leaky\nIntegrate-and-Fire (LIF) struggle to capture long-term dependencies and\neffectively process multi-scale temporal dynamics. To overcome these\nlimitations, we introduce the Temporal Segment Leaky Integrate-and-Fire\n(TS-LIF) model, featuring a novel dual-compartment architecture. The dendritic\nand somatic compartments specialize in capturing distinct frequency components,\nproviding functional heterogeneity that enhances the neuron's ability to\nprocess both low- and high-frequency information. Furthermore, the newly\nintroduced direct somatic current injection reduces information loss during\nintra-neuronal transmission, while dendritic spike generation improves\nmulti-scale information extraction. We provide a theoretical stability analysis\nof the TS-LIF model and explain how each compartment contributes to distinct\nfrequency response characteristics. Experimental results show that TS-LIF\noutperforms traditional SNNs in time series forecasting, demonstrating better\naccuracy and robustness, even with missing data. TS-LIF advances the\napplication of SNNs in time-series forecasting, providing a biologically\ninspired approach that captures complex temporal dynamics and offers potential\nfor practical implementation in diverse forecasting scenarios. The source code\nis available at https://github.com/kkking-kk/TS-LIF.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05108v1",
    "published_date": "2025-03-07 03:06:21 UTC",
    "updated_date": "2025-03-07 03:06:21 UTC"
  },
  {
    "arxiv_id": "2503.05106v1",
    "title": "Grouped Sequential Optimization Strategy -- the Application of Hyperparameter Importance Assessment in Deep Learning",
    "authors": [
      "Ruinan Wang",
      "Ian Nabney",
      "Mohammad Golbabaee"
    ],
    "abstract": "Hyperparameter optimization (HPO) is a critical component of machine learning\npipelines, significantly affecting model robustness, stability, and\ngeneralization. However, HPO is often a time-consuming and computationally\nintensive task. Traditional HPO methods, such as grid search and random search,\noften suffer from inefficiency. Bayesian optimization, while more efficient,\nstill struggles with high-dimensional search spaces. In this paper, we\ncontribute to the field by exploring how insights gained from hyperparameter\nimportance assessment (HIA) can be leveraged to accelerate HPO, reducing both\ntime and computational resources. Building on prior work that quantified\nhyperparameter importance by evaluating 10 hyperparameters on CNNs using 10\ncommon image classification datasets, we implement a novel HPO strategy called\n'Sequential Grouping.' That prior work assessed the importance weights of the\ninvestigated hyperparameters based on their influence on model performance,\nproviding valuable insights that we leverage to optimize our HPO process. Our\nexperiments, validated across six additional image classification datasets,\ndemonstrate that incorporating hyperparameter importance assessment (HIA) can\nsignificantly accelerate HPO without compromising model performance, reducing\noptimization time by an average of 31.9\\% compared to the conventional\nsimultaneous strategy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T05, 68Q32"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.05106v1",
    "published_date": "2025-03-07 03:01:00 UTC",
    "updated_date": "2025-03-07 03:01:00 UTC"
  },
  {
    "arxiv_id": "2503.05092v1",
    "title": "Multi-Robot Collaboration through Reinforcement Learning and Abstract Simulation",
    "authors": [
      "Adam Labiosa",
      "Josiah P. Hanna"
    ],
    "abstract": "Teams of people coordinate to perform complex tasks by forming abstract\nmental models of world and agent dynamics. The use of abstract models contrasts\nwith much recent work in robot learning that uses a high-fidelity simulator and\nreinforcement learning (RL) to obtain policies for physical robots. Motivated\nby this difference, we investigate the extent to which so-called abstract\nsimulators can be used for multi-agent reinforcement learning (MARL) and the\nresulting policies successfully deployed on teams of physical robots. An\nabstract simulator models the robot's target task at a high-level of\nabstraction and discards many details of the world that could impact optimal\ndecision-making. Policies are trained in an abstract simulator then transferred\nto the physical robot by making use of separately-obtained low-level perception\nand motion control modules. We identify three key categories of modifications\nto the abstract simulator that enable policy transfer to physical robots:\nsimulation fidelity enhancements, training optimizations and simulation\nstochasticity. We then run an empirical study with extensive ablations to\ndetermine the value of each modification category for enabling policy transfer\nin cooperative robot soccer tasks. We also compare the performance of policies\nproduced by our method with a well-tuned non-learning-based behavior\narchitecture from the annual RoboCup competition and find that our approach\nleads to a similar level of performance. Broadly we show that MARL can be use\nto train cooperative physical robot behaviors using highly abstract models of\nthe world.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "ICRA 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.05092v1",
    "published_date": "2025-03-07 02:23:24 UTC",
    "updated_date": "2025-03-07 02:23:24 UTC"
  },
  {
    "arxiv_id": "2503.08699v1",
    "title": "Blockchain As a Platform For Artificial Intelligence (AI) Transparency",
    "authors": [
      "Afroja Akther",
      "Ayesha Arobee",
      "Abdullah Al Adnan",
      "Omum Auyon",
      "ASM Johirul Islam",
      "Farhad Akter"
    ],
    "abstract": "As artificial intelligence (AI) systems become increasingly complex and\nautonomous, concerns over transparency and accountability have intensified. The\n\"black box\" problem in AI decision-making limits stakeholders' ability to\nunderstand, trust, and verify outcomes, particularly in high-stakes sectors\nsuch as healthcare, finance, and autonomous systems. Blockchain technology,\nwith its decentralized, immutable, and transparent characteristics, presents a\npotential solution to enhance AI transparency and auditability. This paper\nexplores the integration of blockchain with AI to improve decision\ntraceability, data provenance, and model accountability. By leveraging\nblockchain as an immutable record-keeping system, AI decision-making can become\nmore interpretable, fostering trust among users and regulatory compliance.\nHowever, challenges such as scalability, integration complexity, and\ncomputational overhead must be addressed to fully realize this synergy. This\nstudy discusses existing research, proposes a framework for blockchain-enhanced\nAI transparency, and highlights practical applications, benefits, and\nlimitations. The findings suggest that blockchain could be a foundational\ntechnology for ensuring AI systems remain accountable, ethical, and aligned\nwith regulatory standards.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "F.2.2"
    ],
    "primary_category": "cs.CR",
    "comment": "14 pages, 2 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2503.08699v1",
    "published_date": "2025-03-07 01:57:26 UTC",
    "updated_date": "2025-03-07 01:57:26 UTC"
  },
  {
    "arxiv_id": "2503.05071v1",
    "title": "Object Packing and Scheduling for Sequential 3D Printing: a Linear Arithmetic Model and a CEGAR-inspired Optimal Solver",
    "authors": [
      "Pavel Surynek",
      "Vojtěch Bubník",
      "Lukáš Matěna",
      "Petr Kubiš"
    ],
    "abstract": "We address the problem of object arrangement and scheduling for sequential 3D\nprinting. Unlike the standard 3D printing, where all objects are printed slice\nby slice at once, in sequential 3D printing, objects are completed one after\nother. In the sequential case, it is necessary to ensure that the moving parts\nof the printer do not collide with previously printed objects. We look at the\nsequential printing problem from the perspective of combinatorial optimization.\nWe propose to express the problem as a linear arithmetic formula, which is then\nsolved using a solver for satisfiability modulo theories (SMT). However, we do\nnot solve the formula expressing the problem of object arrangement and\nscheduling directly, but we have proposed a technique inspired by\ncounterexample guided abstraction refinement (CEGAR), which turned out to be a\nkey innovation to efficiency.",
    "categories": [
      "cs.CG",
      "cs.AI"
    ],
    "primary_category": "cs.CG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05071v1",
    "published_date": "2025-03-07 01:31:40 UTC",
    "updated_date": "2025-03-07 01:31:40 UTC"
  },
  {
    "arxiv_id": "2503.05070v1",
    "title": "PromptPex: Automatic Test Generation for Language Model Prompts",
    "authors": [
      "Reshabh K Sharma",
      "Jonathan De Halleux",
      "Shraddha Barke",
      "Benjamin Zorn"
    ],
    "abstract": "Large language models (LLMs) are being used in many applications and prompts\nfor these models are integrated into software applications as code-like\nartifacts. These prompts behave much like traditional software in that they\ntake inputs, generate outputs, and perform some specific function. However,\nprompts differ from traditional code in many ways and require new approaches to\nensure that they are robust. For example, unlike traditional software the\noutput of a prompt depends on the AI model that interprets it. Also, while\nnatural language prompts are easy to modify, the impact of updates is harder to\npredict. New approaches to testing, debugging, and modifying prompts with\nrespect to the model running them are required.\n  To address some of these issues, we developed PromptPex, an LLM-based tool to\nautomatically generate and evaluate unit tests for a given prompt. PromptPex\nextracts input and output specifications from a prompt and uses them to\ngenerate diverse, targeted, and valid unit tests. These tests are instrumental\nin identifying regressions when a prompt is changed and also serve as a tool to\nunderstand how prompts are interpreted by different models. We use PromptPex to\ngenerate tests for eight benchmark prompts and evaluate the quality of the\ngenerated tests by seeing if they can cause each of four diverse models to\nproduce invalid output. PromptPex consistently creates tests that result in\nmore invalid model outputs than a carefully constructed baseline LLM-based test\ngenerator. Furthermore, by extracting concrete specifications from the input\nprompt, PromptPex allows prompt writers to clearly understand and test specific\naspects of their prompts. The source code of PromptPex is available at\nhttps://github.com/microsoft/promptpex.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05070v1",
    "published_date": "2025-03-07 01:31:03 UTC",
    "updated_date": "2025-03-07 01:31:03 UTC"
  },
  {
    "arxiv_id": "2503.05066v2",
    "title": "Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of Experts",
    "authors": [
      "Shwai He",
      "Weilin Cai",
      "Jiayi Huang",
      "Ang Li"
    ],
    "abstract": "The Mixture of Experts (MoE) is an effective architecture for scaling large\nlanguage models by leveraging sparse expert activation, optimizing the\ntrade-off between performance and efficiency. However, under expert\nparallelism, MoE suffers from inference inefficiencies due to imbalanced\ntoken-to-expert assignment, where some experts are overloaded while others\nremain underutilized. This imbalance leads to poor resource utilization and\nincreased latency, as the most burdened expert dictates the overall delay, a\nphenomenon we define as the \\textbf{\\textit{Straggler Effect}}. To mitigate\nthis, we propose Capacity-Aware Inference, including two key techniques: (1)\n\\textbf{\\textit{Capacity-Aware Token Drop}}, which discards overloaded tokens\nto regulate the maximum latency of MoE, and (2) \\textbf{\\textit{Capacity-Aware\nToken Reroute}}, which reallocates overflowed tokens to underutilized experts,\nbalancing the token distribution. These techniques collectively optimize both\nhigh-load and low-load expert utilization, leading to a more efficient MoE\ninference pipeline. Extensive experiments demonstrate the effectiveness of our\nmethods, showing significant improvements in inference efficiency, e.g., 0.2\\%\naverage performance increase and a 1.94$\\times$ inference speedup on\nMixtral-8$\\times$7B-Instruct.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05066v2",
    "published_date": "2025-03-07 01:11:39 UTC",
    "updated_date": "2025-05-22 17:55:58 UTC"
  },
  {
    "arxiv_id": "2503.05064v1",
    "title": "Perceiving, Reasoning, Adapting: A Dual-Layer Framework for VLM-Guided Precision Robotic Manipulation",
    "authors": [
      "Qingxuan Jia",
      "Guoqin Tang",
      "Zeyuan Huang",
      "Zixuan Hao",
      "Ning Ji",
      "Shihang",
      "Yin",
      "Gang Chen"
    ],
    "abstract": "Vision-Language Models (VLMs) demonstrate remarkable potential in robotic\nmanipulation, yet challenges persist in executing complex fine manipulation\ntasks with high speed and precision. While excelling at high-level planning,\nexisting VLM methods struggle to guide robots through precise sequences of fine\nmotor actions. To address this limitation, we introduce a progressive VLM\nplanning algorithm that empowers robots to perform fast, precise, and\nerror-correctable fine manipulation. Our method decomposes complex tasks into\nsub-actions and maintains three key data structures: task memory structure, 2D\ntopology graphs, and 3D spatial networks, achieving high-precision\nspatial-semantic fusion. These three components collectively accumulate and\nstore critical information throughout task execution, providing rich context\nfor our task-oriented VLM interaction mechanism. This enables VLMs to\ndynamically adjust guidance based on real-time feedback, generating precise\naction plans and facilitating step-wise error correction. Experimental\nvalidation on complex assembly tasks demonstrates that our algorithm\neffectively guides robots to rapidly and precisely accomplish fine manipulation\nin challenging scenarios, significantly advancing robot intelligence for\nprecision tasks.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05064v1",
    "published_date": "2025-03-07 00:55:42 UTC",
    "updated_date": "2025-03-07 00:55:42 UTC"
  },
  {
    "arxiv_id": "2503.05051v1",
    "title": "Accelerated Patient-specific Non-Cartesian MRI Reconstruction using Implicit Neural Representations",
    "authors": [
      "Di Xu",
      "Hengjie Liu",
      "Xin Miao",
      "Daniel O'Connor",
      "Jessica E. Scholey",
      "Wensha Yang",
      "Mary Feng",
      "Michael Ohliger",
      "Hui Lin",
      "Dan Ruan",
      "Yang Yang",
      "Ke Sheng"
    ],
    "abstract": "The scanning time for a fully sampled MRI can be undesirably lengthy.\nCompressed sensing has been developed to minimize image artifacts in\naccelerated scans, but the required iterative reconstruction is computationally\ncomplex and difficult to generalize on new cases. Image-domain-based deep\nlearning methods (e.g., convolutional neural networks) emerged as a faster\nalternative but face challenges in modeling continuous k-space, a problem\namplified with non-Cartesian sampling commonly used in accelerated acquisition.\nIn comparison, implicit neural representations can model continuous signals in\nthe frequency domain and thus are compatible with arbitrary k-space sampling\npatterns. The current study develops a novel generative-adversarially trained\nimplicit neural representations (k-GINR) for de novo undersampled non-Cartesian\nk-space reconstruction. k-GINR consists of two stages: 1) supervised training\non an existing patient cohort; 2) self-supervised patient-specific\noptimization. In stage 1, the network is trained with the\ngenerative-adversarial network on diverse patients of the same anatomical\nregion supervised by fully sampled acquisition. In stage 2, undersampled\nk-space data of individual patients is used to tailor the prior-embedded\nnetwork for patient-specific optimization. The UCSF StarVIBE T1-weighted liver\ndataset was evaluated on the proposed framework. k-GINR is compared with an\nimage-domain deep learning method, Deep Cascade CNN, and a compressed sensing\nmethod. k-GINR consistently outperformed the baselines with a larger\nperformance advantage observed at very high accelerations (e.g., 20 times).\nk-GINR offers great value for direct non-Cartesian k-space reconstruction for\nnew incoming patients across a wide range of accelerations liver anatomy.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.05051v1",
    "published_date": "2025-03-07 00:05:43 UTC",
    "updated_date": "2025-03-07 00:05:43 UTC"
  }
]
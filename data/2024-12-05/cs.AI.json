{
  "date": "2024-12-05",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-12-05 的 arXiv 中文 TLDR 快报！今天 arXiv 更新了 131 篇论文，主要聚焦于 AI 生成模型、LLM 优化（如强化学习和偏好对齐）、计算机视觉（如图像生成和分割）、医疗应用（如报告生成和图像分析）等领域，其中 Francois Chollet 的 AGI 基准报告令人印象深刻，强调了新型推理技术在 AI 领域的潜力，同时其他创新 LLM 方法展示了高效训练和泛化能力的提升。\n\n### 重点论文讨论\n我将相关论文分组讨论，先优先选取重要、话题性强的论文（如涉及 LLM 改进和 AI 安全），然后快速掠过其他领域的内容。以下聚焦于核心贡献，保留关键术语。\n\n#### LLM 和 AI 安全领域（高话题度，相关论文放在一起）\n- **ARC Prize 2024: Technical Report（ARC 奖 2024：技术报告）** by Francois Chollet et al.：这篇报告回顾了 AGI 基准的进展，通过深度学习引导程序合成等技术，将状态指标从 33% 提升至 55.5%，强调了泛化能力的本质，为 AI 研究提供关键洞见。\n- **Dissociating Artificial Intelligence from Artificial Consciousness（将人工智能与人工意识分离）** by Graham Findlay et al.：论文证明了功能等价系统可能在意识上不同，使用 Integrated Information Theory 分析，揭示 AI 计算功能主义可能无法复制人类体验，引发 AI 伦理讨论。\n- **SWEPO: Simultaneous Weighted Preference Optimization for Group Contrastive Alignment（SWEPO：用于群组对比对齐的同时加权偏好优化）** by Taneesh Gupta et al.：提出一种多响应偏好优化方法，通过偏差-based 权重提升 LLM 对齐效果，在 Ultra-Feedback 数据集上比 DPO 提升 4% 的胜率，适用于复杂偏好建模。\n- **MTMT: Consolidating Multiple Thinking Modes to Form a Thought Tree for Strengthening LLM（MTMT：整合多种思考模式形成思维树以增强 LLM）** by Changcheng Li et al.：引入多模思考框架（如联想和任务分解），构建思维树提升 LLM 推理能力，在复杂任务上显著改善性能。\n- **Reinforcement Learning Enhanced LLMs: A Survey（强化学习增强的 LLM：调查）** by Shuhe Wang et al.：系统回顾 RLHF 和 RLAIF 等技术，分析强化学习如何提升 LLM 性能，并指出未来挑战，如减少偏置。\n\n这些论文突出了 LLM 的强化学习和安全优化，Chollet 的工作尤其引人注目，因为它直接推动 AGI 研究，而其他方法则展示了 LLM 在偏好对齐和多模推理上的实际进展。\n\n#### 生成模型和计算机视觉（印象深刻，相关主题合并）\n- **PaintScene4D: Consistent 4D Scene Generation from Text Prompts（PaintScene4D：从文本提示生成一致的 4D 场景）** by Vinayak Gupta et al.：提出基于视频生成模型的框架，通过渐进式翘曲和补全确保时空一致性，实现灵活的 4D 场景生成，显著提升文本到动态场景的控制能力。\n- **VisionZip: Longer is Better but Not Necessary in Vision Language Models（VisionZip：视觉语言模型中更长不一定是更好）** by Senqiao Yang et al.：开发一种高效令牌选择方法，减少视觉冗余并加速推理，在 LLaVA-Next 上提升 8 倍预填充速度，同时保持性能。\n- **GaussianFormer-2: Probabilistic Gaussian Superposition for Efficient 3D Occupancy Prediction（GaussianFormer-2：用于高效 3D 占用预测的概率高斯叠加）** by Yuanhui Huang et al.：使用概率高斯模型提升 3D 场景预测效率，实现状态-of-the-art 在 nuScenes 数据集上，适用于自动驾驶。\n- **DEIM: DETR with Improved Matching for Fast Convergence（DEIM：用于快速收敛的 DETR 改进匹配）** by Shihua Huang et al.：优化 DETR 的匹配策略，通过密集一对一匹配减少计算，在 COCO 数据集上以 50% 更少的训练时间达到 53.2% AP。\n\n这些工作在生成和视觉领域表现出色，PaintScene4D 和 VisionZip 特别值得关注，因为它们解决了实际应用中的效率和控制问题。\n\n#### 医疗和应用领域（实际影响大，快速总结）\n- **Semantic Consistency-Based Uncertainty Quantification for Factuality in Radiology Report Generation（基于语义一致性的不确定性量化用于放射学报告生成的事实性评估）** by Chenyu Wang et al.：提出框架量化报告生成的不确定性，提升事实准确性 10%，在 MIMIC-CXR 数据集上通过拒绝高不确定报告实现改进。\n- **FedMetaMed: Federated Meta-Learning for Personalized Medication（FedMetaMed：用于个性化用药的联邦元学习）** by Jiechao Gao et al.：结合联邦学习和元学习实现个性化药物推荐，处理医疗数据异质性，在真实数据集上提升泛化性能。\n- **EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning（EgoPlan-Bench2：用于多模态 LLM 规划的基准）** by Lu Qiu et al.：构建基准评估 LLM 在真实场景下的规划能力，通过多模态提示提升性能 10.24%，适用于增强医疗和机器人应用。\n\n医疗相关论文强调不确定性和个性化，FedMetaMed 的联邦学习方法在隐私保护下表现出色。\n\n#### 其他领域（快速掠过，非核心主题简要提及）\n其他论文涉及图神经网络、强化学习和机器人等领域，但影响力较小，仅快速列出：\n- **Hidden in the Noise: Two-Stage Robust Watermarking for Images（隐藏在噪声中：图像的两阶段鲁棒水印）** by Kasra Arabi et al.：开发扩散模型-based 水印框架，提升图像生成的安全性。\n- **TANGO: Training-free Embodied AI Agents for Open-world Tasks（TANGO：无训练的具身 AI 代理用于开放世界任务）** by Filippo Ziliotto et al.：使用 LLM 组合基础模型实现无训练机器人任务，适用于动态环境。\n- **Machine Theory of Mind for Autonomous Cyber-Defence（用于自主网络防御的机器理论-of-mind）** by Luke Swaby et al.：提出 GNN-based 框架预测攻击行为，提升网络安全。\n\n这些论文虽有创新，但较特定，快速掠过以控制篇幅。\n\n总之，今天的 arXiv 更新突显了 AI 模型的优化和应用潜力，LLM 领域的进展尤其值得关注。更多细节可查阅具体论文！",
  "papers": [
    {
      "arxiv_id": "2412.04673v1",
      "title": "Socially-Informed Reconstruction for Pedestrian Trajectory Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Haleh Damirchi",
        "Ali Etemad",
        "Michael Greenspan"
      ],
      "abstract": "Pedestrian trajectory prediction remains a challenge for autonomous systems,\nparticularly due to the intricate dynamics of social interactions. Accurate\nforecasting requires a comprehensive understanding not only of each\npedestrian's previous trajectory but also of their interaction with the\nsurrounding environment, an important part of which are other pedestrians\nmoving dynamically in the scene. To learn effective socially-informed\nrepresentations, we propose a model that uses a reconstructor alongside a\nconditional variational autoencoder-based trajectory forecasting module. This\nmodule generates pseudo-trajectories, which we use as augmentations throughout\nthe training process. To further guide the model towards social awareness, we\npropose a novel social loss that aids in forecasting of more stable\ntrajectories. We validate our approach through extensive experiments,\ndemonstrating strong performances in comparison to state of-the-art methods on\nthe ETH/UCY and SDD benchmarks.",
      "tldr_zh": "这篇论文针对行人轨迹预测（Pedestrian Trajectory Forecasting）中的社会互动复杂性，提出了一种基于重建器（reconstructor）和条件变分自动编码器（conditional variational autoencoder）的模型，以学习有效的社会信息表示。模型通过生成伪轨迹（pseudo-trajectories）作为训练增强，并引入新型社会损失（social loss），帮助预测更稳定的轨迹。实验结果显示，该方法在ETH/UCY和SDD基准上优于现有技术，显著提升了预测准确性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at Winter Conference on Applications of Computer Vision\n  (WACV), 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.04673v1",
      "published_date": "2024-12-05 23:54:06 UTC",
      "updated_date": "2024-12-05 23:54:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:00:01.103886"
    },
    {
      "arxiv_id": "2412.04671v3",
      "title": "Fully Distributed, Flexible Compositional Visual Representations via Soft Tensor Products",
      "title_zh": "翻译失败",
      "authors": [
        "Bethia Sun",
        "Maurice Pagnucco",
        "Yang Song"
      ],
      "abstract": "Since the inception of the classicalist vs. connectionist debate, it has been\nargued that the ability to systematically combine symbol-like entities into\ncompositional representations is crucial for human intelligence. In\nconnectionist systems, the field of disentanglement has gained prominence for\nits ability to produce explicitly compositional representations; however, it\nrelies on a fundamentally symbolic, concatenative representation of\ncompositional structure that clashes with the continuous, distributed\nfoundations of deep learning. To resolve this tension, we extend Smolensky's\nTensor Product Representation (TPR) and introduce Soft TPR, a representational\nform that encodes compositional structure in an inherently distributed,\nflexible manner, along with Soft TPR Autoencoder, a theoretically-principled\narchitecture designed specifically to learn Soft TPRs. Comprehensive\nevaluations in the visual representation learning domain demonstrate that the\nSoft TPR framework consistently outperforms conventional disentanglement\nalternatives -- achieving state-of-the-art disentanglement, boosting\nrepresentation learner convergence, and delivering superior sample efficiency\nand low-sample regime performance in downstream tasks. These findings highlight\nthe promise of a distributed and flexible approach to representing\ncompositional structure by potentially enhancing alignment with the core\nprinciples of deep learning over the conventional symbolic approach.",
      "tldr_zh": "该论文针对经典主义与连接主义辩论中的组合表示问题，扩展了 Tensor Product Representation (TPR) 并引入 Soft TPR，这是一种固有分布式、灵活的视觉表示方法，以克服传统 disentanglement 方法的符号化局限性。作者设计了 Soft TPR Autoencoder 架构，该架构理论上合理，能够高效学习 Soft TPR 表示。在视觉表示学习领域，实验结果显示 Soft TPR 框架在 disentanglement 性能、表示学习收敛速度、样本效率和低样本下游任务上均优于现有方法。这些发现强调了分布式方法在深度学习中的潜力，有望更好地与深度学习的连续原则保持一致。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to Neurips 2024. 10 pages + supplementary",
      "pdf_url": "http://arxiv.org/pdf/2412.04671v3",
      "published_date": "2024-12-05 23:47:58 UTC",
      "updated_date": "2025-01-23 01:05:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:00:14.785898"
    },
    {
      "arxiv_id": "2412.04668v1",
      "title": "Diffusion-Augmented Coreset Expansion for Scalable Dataset Distillation",
      "title_zh": "扩散增强核心集扩展用于可扩展数据集蒸馏",
      "authors": [
        "Ali Abbasi",
        "Shima Imani",
        "Chenyang An",
        "Gayathri Mahalingam",
        "Harsh Shrivastava",
        "Maurice Diesendruck",
        "Hamed Pirsiavash",
        "Pramod Sharma",
        "Soheil Kolouri"
      ],
      "abstract": "With the rapid scaling of neural networks, data storage and communication\ndemands have intensified. Dataset distillation has emerged as a promising\nsolution, condensing information from extensive datasets into a compact set of\nsynthetic samples by solving a bilevel optimization problem. However, current\nmethods face challenges in computational efficiency, particularly with\nhigh-resolution data and complex architectures. Recently,\nknowledge-distillation-based dataset condensation approaches have made this\nprocess more computationally feasible. Yet, with the recent developments of\ngenerative foundation models, there is now an opportunity to achieve even\ngreater compression, enhance the quality of distilled data, and introduce\nvaluable diversity into the data representation. In this work, we propose a\ntwo-stage solution. First, we compress the dataset by selecting only the most\ninformative patches to form a coreset. Next, we leverage a generative\nfoundation model to dynamically expand this compressed set in real-time,\nenhancing the resolution of these patches and introducing controlled\nvariability to the coreset. Our extensive experiments demonstrate the\nrobustness and efficiency of our approach across a range of dataset\ndistillation benchmarks. We demonstrate a significant improvement of over 10%\ncompared to the state-of-the-art on several large-scale dataset distillation\nbenchmarks. The code will be released soon.",
      "tldr_zh": "该研究针对神经网络规模化带来的数据存储和通信挑战，提出了一种可扩展的数据集蒸馏（dataset distillation）方法，以解决现有方法的计算效率问题。该方法采用两阶段解决方案：首先，通过选择最信息丰富的 patches 形成 coreset 来压缩数据集；其次，利用生成基础模型（generative foundation model）动态扩展 coreset，提高分辨率并引入可控变异性。实验结果显示，该方法在多个数据集蒸馏基准上比最先进方法提升超过10%，证明了其鲁棒性和效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04668v1",
      "published_date": "2024-12-05 23:40:27 UTC",
      "updated_date": "2024-12-05 23:40:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:00:27.182284"
    },
    {
      "arxiv_id": "2412.04664v2",
      "title": "Multiclass Post-Earthquake Building Assessment Integrating Optical and SAR Satellite Imagery, Ground Motion, and Soil Data with Transformers",
      "title_zh": "多类地震后建筑物评估",
      "authors": [
        "Deepank Singh",
        "Vedhus Hoskere",
        "Pietro Milillo"
      ],
      "abstract": "Timely and accurate assessments of building damage are crucial for effective\nresponse and recovery in the aftermath of earthquakes. Conventional preliminary\ndamage assessments (PDA) often rely on manual door-to-door inspections, which\nare not only time-consuming but also pose significant safety risks. To safely\nexpedite the PDA process, researchers have studied the applicability of\nsatellite imagery processed with heuristic and machine learning approaches.\nThese approaches output binary or, more recently, multiclass damage states at\nthe scale of a block or a single building. However, the current performance of\nsuch approaches limits practical applicability. To address this limitation, we\nintroduce a metadata-enriched, transformer based framework that combines\nhigh-resolution post-earthquake satellite imagery with building-specific\nmetadata relevant to the seismic performance of the structure. Our model\nachieves state-of-the-art performance in multiclass post-earthquake damage\nidentification for buildings from the Turkey-Syria earthquake on February 6,\n2023. Specifically, we demonstrate that incorporating metadata, such as seismic\nintensity indicators, soil properties, and SAR damage proxy maps not only\nenhances the model's accuracy and ability to distinguish between damage\nclasses, but also improves its generalizability across various regions.\nFurthermore, we conducted a detailed, class-wise analysis of feature importance\nto understand the model's decision-making across different levels of building\ndamage. This analysis reveals how individual metadata features uniquely\ncontribute to predictions for each damage class. By leveraging both satellite\nimagery and metadata, our proposed framework enables faster and more accurate\ndamage assessments for precise, multiclass, building-level evaluations that can\nimprove disaster response and accelerate recovery efforts for affected\ncommunities.",
      "tldr_zh": "本研究提出了一种基于 Transformers 的框架，用于多类地震后建筑损伤评估，该框架整合了高分辨率 Optical 和 SAR Satellite Imagery、Ground Motion 和 Soil Data 等元数据，以提升评估的准确性和泛化能力。相比传统方法，该框架在 2023 年土耳其-叙利亚地震数据上实现了最先进性能，提高了模型对不同损伤类别的区分，并通过特征重要性分析揭示了元数据（如地震强度指标和土壤特性）在预测中的独特贡献。实验结果表明，该方法不仅加速了建筑级别的多类损伤识别，还改善了灾害响应和恢复效率。总的来说，这一框架为更安全、快速的初步损伤评估（PDA）提供了可靠解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "28 Pages, 12 Figures",
      "pdf_url": "http://arxiv.org/pdf/2412.04664v2",
      "published_date": "2024-12-05 23:19:51 UTC",
      "updated_date": "2025-02-26 16:49:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:00:37.091207"
    },
    {
      "arxiv_id": "2412.04661v1",
      "title": "HEAL: Hierarchical Embedding Alignment Loss for Improved Retrieval and Representation Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Manish Bhattarai",
        "Ryan Barron",
        "Maksim Eren",
        "Minh Vu",
        "Vesselin Grantcharov",
        "Ismael Boureima",
        "Valentin Stanev",
        "Cynthia Matuszek",
        "Vladimir Valtchinov",
        "Kim Rasmussen",
        "Boian Alexandrov"
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external document retrieval to provide domain-specific or\nup-to-date knowledge. The effectiveness of RAG depends on the relevance of\nretrieved documents, which is influenced by the semantic alignment of\nembeddings with the domain's specialized content. Although full fine-tuning can\nalign language models to specific domains, it is computationally intensive and\ndemands substantial data. This paper introduces Hierarchical Embedding\nAlignment Loss (HEAL), a novel method that leverages hierarchical fuzzy\nclustering with matrix factorization within contrastive learning to efficiently\nalign LLM embeddings with domain-specific content. HEAL computes\nlevel/depth-wise contrastive losses and incorporates hierarchical penalties to\nalign embeddings with the underlying relationships in label hierarchies. This\napproach enhances retrieval relevance and document classification, effectively\nreducing hallucinations in LLM outputs. In our experiments, we benchmark and\nevaluate HEAL across diverse domains, including Healthcare, Material Science,\nCyber-security, and Applied Maths.",
      "tldr_zh": "这篇论文提出了 Hierarchical Embedding Alignment Loss (HEAL)，一种创新方法，用于提升 Retrieval-Augmented Generation (RAG) 在 Large Language Models (LLMs) 中的检索和表示学习效果。HEAL 通过分层模糊聚类、矩阵分解和对比学习，计算层级对比损失并加入分层惩罚，以高效对齐 LLM 嵌入与领域特定内容，从而改善检索相关性和文档分类，并减少 LLM 的幻觉。在实验中，HEAL 在 Healthcare、Material Science、Cyber-security 和 Applied Maths 等领域进行了基准测试，展示了显著的性能提升。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04661v1",
      "published_date": "2024-12-05 23:10:56 UTC",
      "updated_date": "2024-12-05 23:10:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:00:50.742934"
    },
    {
      "arxiv_id": "2412.04655v2",
      "title": "From Models to Systems: A Comprehensive Fairness Framework for Compositional Recommender Systems",
      "title_zh": "从模型到系统：一个全面的公平框架，用于组合式推荐系统",
      "authors": [
        "Brian Hsu",
        "Cyrus DiCiccio",
        "Natesh Sivasubramoniapillai",
        "Hongseok Namkoong"
      ],
      "abstract": "Fairness research in machine learning often centers on ensuring equitable\nperformance of individual models. However, real-world recommendation systems\nare built on multiple models and even multiple stages, from candidate retrieval\nto scoring and serving, which raises challenges for responsible development and\ndeployment. This system-level view, as highlighted by regulations like the EU\nAI Act, necessitates moving beyond auditing individual models as independent\nentities. We propose a holistic framework for modeling system-level fairness,\nfocusing on the end-utility delivered to diverse user groups, and consider\ninteractions between components such as retrieval and scoring models. We\nprovide formal insights on the limitations of focusing solely on model-level\nfairness and highlight the need for alternative tools that account for\nheterogeneity in user preferences. To mitigate system-level disparities, we\nadapt closed-box optimization tools (e.g., BayesOpt) to jointly optimize\nutility and equity. We empirically demonstrate the effectiveness of our\nproposed framework on synthetic and real datasets, underscoring the need for a\nsystem-level framework.",
      "tldr_zh": "该研究强调，机器学习中的公平性研究通常仅针对单个模型的公平表现，但实际的推荐系统（recommender systems）涉及多个模型和阶段（如候选检索、评分和服务），这需要从系统层面确保公平，以符合欧盟 AI Act 等法规。论文提出一个全面的系统级公平性框架（fairness framework），聚焦于多样用户群体的最终效用，并分析组件（如检索和评分模型）间的交互。作者提供了正式见解，揭示仅关注模型级公平性的局限性，并通过适应闭盒优化工具（如 BayesOpt）来联合优化效用和公平性。在合成和真实数据集上的实验验证了框架的有效性，突出了系统级方法的必要性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04655v2",
      "published_date": "2024-12-05 22:59:26 UTC",
      "updated_date": "2025-01-02 17:21:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:01:01.500996"
    },
    {
      "arxiv_id": "2412.04653v5",
      "title": "Hidden in the Noise: Two-Stage Robust Watermarking for Images",
      "title_zh": "翻译失败",
      "authors": [
        "Kasra Arabi",
        "Benjamin Feuer",
        "R. Teal Witter",
        "Chinmay Hegde",
        "Niv Cohen"
      ],
      "abstract": "As the quality of image generators continues to improve, deepfakes become a\ntopic of considerable societal debate. Image watermarking allows responsible\nmodel owners to detect and label their AI-generated content, which can mitigate\nthe harm. Yet, current state-of-the-art methods in image watermarking remain\nvulnerable to forgery and removal attacks. This vulnerability occurs in part\nbecause watermarks distort the distribution of generated images,\nunintentionally revealing information about the watermarking techniques.\n  In this work, we first demonstrate a distortion-free watermarking method for\nimages, based on a diffusion model's initial noise. However, detecting the\nwatermark requires comparing the initial noise reconstructed for an image to\nall previously used initial noises. To mitigate these issues, we propose a\ntwo-stage watermarking framework for efficient detection. During generation, we\naugment the initial noise with generated Fourier patterns to embed information\nabout the group of initial noises we used. For detection, we (i) retrieve the\nrelevant group of noises, and (ii) search within the given group for an initial\nnoise that might match our image. This watermarking approach achieves\nstate-of-the-art robustness to forgery and removal against a large battery of\nattacks.",
      "tldr_zh": "本研究针对图像水印的脆弱性问题，提出了一种基于扩散模型初始噪声的两阶段鲁棒水印框架，以应对deepfakes的伪造和移除攻击。方法首先在图像生成阶段，通过添加生成的Fourier patterns来嵌入初始噪声组的信息；然后在检测阶段，检索相关噪声组并在其中搜索匹配的初始噪声，从而实现高效且无失真的水印嵌入。该框架在多种攻击测试中，展现出比现有方法更强的鲁棒性，提高了AI生成内容的检测可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04653v5",
      "published_date": "2024-12-05 22:50:42 UTC",
      "updated_date": "2025-04-27 11:46:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:01:14.246846"
    },
    {
      "arxiv_id": "2412.04652v1",
      "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
      "title_zh": "Cross-Self KV 缓存剪枝用于高效视觉语言推理",
      "authors": [
        "Xiaohuan Pei",
        "Tao Huang",
        "Chang Xu"
      ],
      "abstract": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
      "tldr_zh": "该论文针对视觉语言模型(VLMs)中的KV cache pruning技术，提出了一种新的方法，以解决现有方法依赖大型语言模型(LLMs)自注意力分数而忽略模态分布差异的问题，导致关键视觉tokens过度修剪。作者将注意力分数分解为intra-modality attention（同一模态内）和inter-modality attention（跨模态），并引入n-softmax函数来缓解修剪引起的分布偏移，从而实现更精确的KV cache管理。实验结果显示，CSP（Cross-Self Pruning）方法在不需额外训练的情况下，在MileBench基准上比基线模型提升高达41%的性能，同时将KV cache预算减少13.6%，尤其在对话和多模态任务中表现出色。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04652v1",
      "published_date": "2024-12-05 22:47:17 UTC",
      "updated_date": "2024-12-05 22:47:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:01:26.224826"
    },
    {
      "arxiv_id": "2412.04645v1",
      "title": "REL: Working out is all you need",
      "title_zh": "翻译失败",
      "authors": [
        "Toby Simonds",
        "Jey Han Lau",
        "Chaithanya Bandi"
      ],
      "abstract": "Recent developments, particularly OpenAI's O1 model, have demonstrated the\nremarkable potential of Large Language Models (LLMs) for complex reasoning\ntasks. Through analysis of O1's outputs and provided sample Chain-of-Thought\n(CoT) demonstrations, we observe that it approaches problem-solving in a\ndistinctly human-like manner, systematically brainstorming ideas, testing\nhypotheses, verifying results, and planning comprehensive solutions. These\nsophisticated reasoning capabilities remain notably absent in other\nstate-of-the-art language models. In this paper, we hypothesize that this\nperformance gap stems from the limited availability of high-quality reasoning\nprocess data in current training sets. We demonstrate that by constructing a\nspecialized dataset focused on explicit problem-solving workflows (\"worked\nsolutions\"), we can elicit substantially improved planning capabilities from\nexisting models. Additionally, we propose the Reasoning Enhancement Loop (REL),\na method for generating synthetic worked solutions.",
      "tldr_zh": "本论文分析了 OpenAI 的 O1 模型在复杂推理任务中的表现，发现其采用类人方式进行脑力激荡、测试假设、验证结果和规划解决方案，而其他 Large Language Models (LLMs) 缺乏类似 Chain-of-Thought (CoT) 能力。作者假设这种差距源于当前训练集缺少高质量推理过程数据，并通过构建一个专注于显式问题解决工作流程（\"worked solutions\"）的专门数据集，成功提升了现有模型的规划能力。论文提出了 Reasoning Enhancement Loop (REL) 方法，用于生成合成的工作解决方案，从而为改进 LLMs 的推理性能提供了新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04645v1",
      "published_date": "2024-12-05 22:32:01 UTC",
      "updated_date": "2024-12-05 22:32:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:01:36.840685"
    },
    {
      "arxiv_id": "2412.04642v1",
      "title": "Improving LLM Group Fairness on Tabular Data via In-Context Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Valeriia Cherepanova",
        "Chia-Jung Lee",
        "Nil-Jana Akpinar",
        "Riccardo Fogliato",
        "Martin Andres Bertran",
        "Michael Kearns",
        "James Zou"
      ],
      "abstract": "Large language models (LLMs) have been shown to be effective on tabular\nprediction tasks in the low-data regime, leveraging their internal knowledge\nand ability to learn from instructions and examples. However, LLMs can fail to\ngenerate predictions that satisfy group fairness, that is, produce equitable\noutcomes across groups. Critically, conventional debiasing approaches for\nnatural language tasks do not directly translate to mitigating group unfairness\nin tabular settings. In this work, we systematically investigate four empirical\napproaches to improve group fairness of LLM predictions on tabular datasets,\nincluding fair prompt optimization, soft prompt tuning, strategic selection of\nfew-shot examples, and self-refining predictions via chain-of-thought\nreasoning. Through experiments on four tabular datasets using both open-source\nand proprietary LLMs, we show the effectiveness of these methods in enhancing\ndemographic parity while maintaining high overall performance. Our analysis\nprovides actionable insights for practitioners in selecting the most suitable\napproach based on their specific requirements and constraints.",
      "tldr_zh": "该研究探讨了如何通过In-Context Learning改善大型语言模型(LLMs)在表格数据上的群组公平性问题，因为LLMs在低数据环境下虽有效，但可能导致不公平的预测结果。作者调查了四种方法，包括fair prompt optimization、soft prompt tuning、strategic selection of few-shot examples以及self-refining predictions via chain-of-thought reasoning，以缓解群组公平性问题。实验在四个表格数据集上使用开源和专有LLMs进行，证明这些方法显著提升了demographic parity等公平指标，同时维持高整体性能。该工作为从业者提供了实用见解，帮助根据具体需求选择最合适的策略。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04642v1",
      "published_date": "2024-12-05 22:23:30 UTC",
      "updated_date": "2024-12-05 22:23:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:03:23.462893"
    },
    {
      "arxiv_id": "2412.04641v1",
      "title": "Disentangled Representation Learning for Causal Inference with Instruments",
      "title_zh": "翻译失败",
      "authors": [
        "Debo Cheng",
        "Jiuyong Li",
        "Lin Liu",
        "Ziqi Xu",
        "Weijia Zhang",
        "Jixue Liu",
        "Thuc Duy Le"
      ],
      "abstract": "Latent confounders are a fundamental challenge for inferring causal effects\nfrom observational data. The instrumental variable (IV) approach is a practical\nway to address this challenge. Existing IV based estimators need a known IV or\nother strong assumptions, such as the existence of two or more IVs in the\nsystem, which limits the application of the IV approach. In this paper, we\nconsider a relaxed requirement, which assumes there is an IV proxy in the\nsystem without knowing which variable is the proxy. We propose a Variational\nAutoEncoder (VAE) based disentangled representation learning method to learn an\nIV representation from a dataset with latent confounders and then utilise the\nIV representation to obtain an unbiased estimation of the causal effect from\nthe data. Extensive experiments on synthetic and real-world data have\ndemonstrated that the proposed algorithm outperforms the existing IV based\nestimators and VAE-based estimators.",
      "tldr_zh": "该论文针对观测数据中潜在混杂因素对因果推断的挑战，提出了一种基于 Variational AutoEncoder (VAE) 的解缠表示学习方法，用于从数据中学习 Instrumental Variable (IV) 表示，而无需知道具体的 IV 或强假设（如多个 IV 的存在）。该方法假设系统中存在一个 IV 代理变量，通过解缠表示学习来提取 IV 表示，并利用它进行无偏的因果效应估计。实验结果显示，该算法在合成和真实世界数据上优于现有 IV 基于估计器和 VAE 基于估计器。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "14 pages, 13 figures and 5 tables. Accepted by TNNLS",
      "pdf_url": "http://arxiv.org/pdf/2412.04641v1",
      "published_date": "2024-12-05 22:18:48 UTC",
      "updated_date": "2024-12-05 22:18:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:03:36.575732"
    },
    {
      "arxiv_id": "2412.04637v1",
      "title": "Semantic Retrieval at Walmart",
      "title_zh": "Walmart 的语义检索",
      "authors": [
        "Alessandro Magnani",
        "Feng Liu",
        "Suthee Chaidaroon",
        "Sachin Yadav",
        "Praveen Reddy Suram",
        "Ajit Puthenputhussery",
        "Sijie Chen",
        "Min Xie",
        "Anirudh Kashi",
        "Tony Lee",
        "Ciya Liao"
      ],
      "abstract": "In product search, the retrieval of candidate products before re-ranking is\nmore critical and challenging than other search like web search, especially for\ntail queries, which have a complex and specific search intent. In this paper,\nwe present a hybrid system for e-commerce search deployed at Walmart that\ncombines traditional inverted index and embedding-based neural retrieval to\nbetter answer user tail queries. Our system significantly improved the\nrelevance of the search engine, measured by both offline and online\nevaluations. The improvements were achieved through a combination of different\napproaches. We present a new technique to train the neural model at scale. and\ndescribe how the system was deployed in production with little impact on\nresponse time. We highlight multiple learnings and practical tricks that were\nused in the deployment of this system.",
      "tldr_zh": "这篇论文介绍了Walmart在电商搜索中针对复杂特定意图的tail queries提出的混合检索系统，该系统结合了传统的inverted index和embedding-based neural retrieval，以提升候选产品的相关性。研究者开发了一种新颖的规模化训练neural model技术，并成功在生产环境中部署系统，同时保持响应时间影响最小。实验结果显示，该系统通过离线和在线评估显著提高了搜索相关性，并分享了多个实用技巧和经验教训。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "9 page, 2 figures, 10 tables, KDD 2022",
      "pdf_url": "http://arxiv.org/pdf/2412.04637v1",
      "published_date": "2024-12-05 22:10:58 UTC",
      "updated_date": "2024-12-05 22:10:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:03:48.749751"
    },
    {
      "arxiv_id": "2412.04634v1",
      "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
      "title_zh": "翻译失败",
      "authors": [
        "Mikhail Dereviannykh",
        "Dmitrii Klepikov",
        "Johannes Hanika",
        "Carsten Dachsbacher"
      ],
      "abstract": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
      "tldr_zh": "本研究提出了一种高效的 Two-Level Monte Carlo 估算器（Multi-Level Monte Carlo 的子集），用于实时渲染具有全局照明的场景，通过将着色积分分为辐射缓存积分和残差误差积分来提升性能。具体而言，Neural Incident Radiance Cache (NIRC) 利用在线训练的小型神经网络提供快速入射辐射近似，比路径追踪样本快 2-25 倍，从而加速积分计算并提高收敛速度。实验结果显示，该方法在各种场景中显著减少渲染噪声，支持动态场景，并易于与其他重要性采样和噪声减少技术结合，而无需依赖场景的具体几何、材料或照明。",
      "categories": [
        "cs.GR",
        "cs.AI"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04634v1",
      "published_date": "2024-12-05 22:06:23 UTC",
      "updated_date": "2024-12-05 22:06:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:04:02.083779"
    },
    {
      "arxiv_id": "2412.10402v1",
      "title": "TANGO: Training-free Embodied AI Agents for Open-world Tasks",
      "title_zh": "TANGO: 无需训练的具身 AI 代理用于开放世界任务",
      "authors": [
        "Filippo Ziliotto",
        "Tommaso Campari",
        "Luciano Serafini",
        "Lamberto Ballan"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated excellent capabilities in\ncomposing various modules together to create programs that can perform complex\nreasoning tasks on images. In this paper, we propose TANGO, an approach that\nextends the program composition via LLMs already observed for images, aiming to\nintegrate those capabilities into embodied agents capable of observing and\nacting in the world. Specifically, by employing a simple PointGoal Navigation\nmodel combined with a memory-based exploration policy as a foundational\nprimitive for guiding an agent through the world, we show how a single model\ncan address diverse tasks without additional training. We task an LLM with\ncomposing the provided primitives to solve a specific task, using only a few\nin-context examples in the prompt. We evaluate our approach on three key\nEmbodied AI tasks: Open-Set ObjectGoal Navigation, Multi-Modal Lifelong\nNavigation, and Open Embodied Question Answering, achieving state-of-the-art\nresults without any specific fine-tuning in challenging zero-shot scenarios.",
      "tldr_zh": "该研究提出 TANGO，一种无需额外训练的 Embodied AI 代理，用于处理开放世界任务，通过 Large Language Models (LLMs) 组合基础原语（如 PointGoal Navigation 模型和内存-based 探索策略）来实现任务分解和执行。LLMs 通过少量 in-context examples 在提示中指导代理观察和行动，从而适应多样场景。实验结果显示，TANGO 在 Open-Set ObjectGoal Navigation、Multi-Modal Lifelong Navigation 和 Open Embodied Question Answering 等任务上，实现了 state-of-the-art 性能，尤其在 challenging 的零样本环境中。",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10402v1",
      "published_date": "2024-12-05 21:52:20 UTC",
      "updated_date": "2024-12-05 21:52:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:04:14.523709"
    },
    {
      "arxiv_id": "2412.04628v3",
      "title": "SWEPO: Simultaneous Weighted Preference Optimization for Group Contrastive Alignment",
      "title_zh": "翻译失败",
      "authors": [
        "Taneesh Gupta",
        "Rahul Madhavan",
        "Xuchao Zhang",
        "Chetan Bansal",
        "Saravan Rajmohan"
      ],
      "abstract": "Direct Preference Optimization (DPO) has proven effective in aligning large\nlanguage models with human preferences but is often constrained to pairwise\ncomparisons -- overlooking additional positive and negative responses that are\ncommonly available in real-world settings. We propose Simultaneous Weighted\nPreference Optimization (SWEPO), which incorporates multiple responses per\nquery and prioritizes those that deviate most from the average reward. This\ndeviation-based weighting focuses training on the most informative outliers,\nakin to a built-in curriculum. Theoretically, we prove that such\nmulti-preference sampling lowers alignment bias, bounding the expected\ndeviation from the true acceptable-response distribution at a rate of\n$\\mathcal{O}(\\tfrac{1}{\\sqrt{k}})$. Empirically, SWEPO outperforms\nstate-of-the-art baselines on the Ultra-Feedback dataset and demonstrates\nsubstantial improvements over DPO and InfoNCA, yielding boosts of up to $\\sim\n4$% on length-controlled win-rate on AlpacaEval.",
      "tldr_zh": "该论文提出了 Simultaneous Weighted Preference Optimization (SWEPO)，一种改进的偏好优化方法，用于处理大型语言模型的群组对比对齐问题，能够同时整合每个查询的多个响应，并通过偏差-based weighting 优先关注与平均奖励偏差最大的异常值，以提升训练效率。相比传统 Direct Preference Optimization (DPO)，SWEPO 类似于内置课程表，专注于最 informative 的样本，从而降低 alignment bias，并在理论上证明了期望偏差控制在 $\\mathcal{O}(\\tfrac{1}{\\sqrt{k}})$ 的范围内。实验结果显示，SWEPO 在 Ultra-Feedback 数据集上超越了最先进基线，比 DPO 和 InfoNCA 提高了约4%的长度控制胜率（在 AlpacaEval 上）。这为更高效的人类偏好对齐提供了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04628v3",
      "published_date": "2024-12-05 21:50:22 UTC",
      "updated_date": "2025-02-21 18:12:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:04:27.614678"
    },
    {
      "arxiv_id": "2412.04606v2",
      "title": "Semantic Consistency-Based Uncertainty Quantification for Factuality in Radiology Report Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Chenyu Wang",
        "Weichao Zhou",
        "Shantanu Ghosh",
        "Kayhan Batmanghelich",
        "Wenchao Li"
      ],
      "abstract": "Radiology report generation (RRG) has shown great potential in assisting\nradiologists by automating the labor-intensive task of report writing. While\nrecent advancements have improved the quality and coherence of generated\nreports, ensuring their factual correctness remains a critical challenge.\nAlthough generative medical Vision Large Language Models (VLLMs) have been\nproposed to address this issue, these models are prone to hallucinations and\ncan produce inaccurate diagnostic information. To address these concerns, we\nintroduce a novel Semantic Consistency-Based Uncertainty Quantification\nframework that provides both report-level and sentence-level uncertainties.\nUnlike existing approaches, our method does not require modifications to the\nunderlying model or access to its inner state, such as output token logits,\nthus serving as a plug-and-play module that can be seamlessly integrated with\nstate-of-the-art models. Extensive experiments demonstrate the efficacy of our\nmethod in detecting hallucinations and enhancing the factual accuracy of\nautomatically generated radiology reports. By abstaining from high-uncertainty\nreports, our approach improves factuality scores by $10$\\%, achieved by\nrejecting $20$\\% of reports using the \\texttt{Radialog} model on the MIMIC-CXR\ndataset. Furthermore, sentence-level uncertainty flags the lowest-precision\nsentence in each report with an $82.9$\\% success rate. Our implementation is\nopen-source and available at https://github.com/BU-DEPEND-Lab/SCUQ-RRG.",
      "tldr_zh": "该研究针对放射学报告生成（RRG）的真实性挑战，提出了一种基于语义一致性（Semantic Consistency-Based）的不确定性量化框架，以检测幻觉（hallucinations）和提升报告事实准确性。该框架提供报告级别和句子级别的不确定性量化，无需修改底层模型或访问其内部状态（如输出 token logits），可作为即插即用模块集成到现有模型中。实验在 MIMIC-CXR 数据集上表明，通过拒绝 20% 的高不确定性报告，该方法使用 Radialog 模型将事实分数提高了 10%，并以 82.9% 的成功率标记报告中最低精度的句子。该框架的开源实现进一步促进了其在生成式医疗视觉大语言模型（VLLMs）中的应用。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04606v2",
      "published_date": "2024-12-05 20:43:39 UTC",
      "updated_date": "2025-03-16 19:19:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:04:38.248759"
    },
    {
      "arxiv_id": "2412.04604v2",
      "title": "ARC Prize 2024: Technical Report",
      "title_zh": "ARC Prize 2024：技术报告",
      "authors": [
        "Francois Chollet",
        "Mike Knoop",
        "Gregory Kamradt",
        "Bryan Landers"
      ],
      "abstract": "As of December 2024, the ARC-AGI benchmark is five years old and remains\nunbeaten. We believe it is currently the most important unsolved AI benchmark\nin the world because it seeks to measure generalization on novel tasks -- the\nessence of intelligence -- as opposed to skill at tasks that can be prepared\nfor in advance. This year, we launched ARC Prize, a global competition to\ninspire new ideas and drive open progress towards AGI by reaching a target\nbenchmark score of 85\\%. As a result, the state-of-the-art score on the ARC-AGI\nprivate evaluation set increased from 33\\% to 55.5\\%, propelled by several\nfrontier AGI reasoning techniques including deep learning-guided program\nsynthesis and test-time training. In this paper, we survey top approaches,\nreview new open-source implementations, discuss the limitations of the\nARC-AGI-1 dataset, and share key insights gained from the competition.",
      "tldr_zh": "该报告回顾了ARC-AGI基准，作为一个五年来未被超越的重要AI基准，它专注于评估AI在新型任务上的泛化能力。2024年推出的ARC Prize竞赛通过激励全球参与，将该基准的最新分数从33%提升至55.5%，主要得益于前沿技术如deep learning-guided program synthesis和test-time training。论文调查了顶级方法、新的开源实现、ARC-AGI-1数据集的局限性，并分享了竞赛中获得的关键见解，以推动AGI领域的开放进步。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04604v2",
      "published_date": "2024-12-05 20:40:28 UTC",
      "updated_date": "2025-01-08 05:24:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:04:50.039362"
    },
    {
      "arxiv_id": "2412.04576v1",
      "title": "Show, Don't Tell: Uncovering Implicit Character Portrayal using LLMs",
      "title_zh": "展示，不要讲述：使用 LLMs 揭示",
      "authors": [
        "Brandon Jaipersaud",
        "Zining Zhu",
        "Frank Rudzicz",
        "Elliot Creager"
      ],
      "abstract": "Tools for analyzing character portrayal in fiction are valuable for writers\nand literary scholars in developing and interpreting compelling stories.\nExisting tools, such as visualization tools for analyzing fictional characters,\nprimarily rely on explicit textual indicators of character attributes. However,\nportrayal is often implicit, revealed through actions and behaviors rather than\nexplicit statements. We address this gap by leveraging large language models\n(LLMs) to uncover implicit character portrayals. We start by generating a\ndataset for this task with greater cross-topic similarity, lexical diversity,\nand narrative lengths than existing narrative text corpora such as TinyStories\nand WritingPrompts. We then introduce LIIPA (LLMs for Inferring Implicit\nPortrayal for Character Analysis), a framework for prompting LLMs to uncover\ncharacter portrayals. LIIPA can be configured to use various types of\nintermediate computation (character attribute word lists, chain-of-thought) to\ninfer how fictional characters are portrayed in the source text. We find that\nLIIPA outperforms existing approaches, and is more robust to increasing\ncharacter counts (number of unique persons depicted) due to its ability to\nutilize full narrative context. Lastly, we investigate the sensitivity of\nportrayal estimates to character demographics, identifying a fairness-accuracy\ntradeoff among methods in our LIIPA framework -- a phenomenon familiar within\nthe algorithmic fairness literature. Despite this tradeoff, all LIIPA variants\nconsistently outperform non-LLM baselines in both fairness and accuracy. Our\nwork demonstrates the potential benefits of using LLMs to analyze complex\ncharacters and to better understand how implicit portrayal biases may manifest\nin narrative texts.",
      "tldr_zh": "这篇论文探讨了使用大型语言模型(LLMs)来揭示小说中隐性字符描绘的问题，填补了现有工具仅依赖显性文本指标的空白。研究者生成了一个新数据集，具有更高的跨主题相似性、词汇多样性和叙述长度，并引入了LIIPA框架，通过中间计算（如字符属性词列表或链式思维）来推断字符在源文本中的描绘。实验结果表明，LIIPA框架的表现优于现有方法，尤其在处理多个独特人物时更鲁棒，同时揭示了公平性与准确性之间的权衡，尽管所有LIIPA变体在公平性和准确性上均超越非LLM基线。总的来说，该工作展示了LLMs在分析复杂字符和识别隐性描绘偏见方面的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04576v1",
      "published_date": "2024-12-05 19:46:53 UTC",
      "updated_date": "2024-12-05 19:46:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:05:03.622178"
    },
    {
      "arxiv_id": "2412.04571v2",
      "title": "Dissociating Artificial Intelligence from Artificial Consciousness",
      "title_zh": "将人工智能与人工意识区分开",
      "authors": [
        "Graham Findlay",
        "William Marshall",
        "Larissa Albantakis",
        "Isaac David",
        "William GP Mayner",
        "Christof Koch",
        "Giulio Tononi"
      ],
      "abstract": "Developments in machine learning and computing power suggest that artificial\ngeneral intelligence is within reach. This raises the question of artificial\nconsciousness: if a computer were to be functionally equivalent to a human,\nbeing able to do all we do, would it experience sights, sounds, and thoughts,\nas we do when we are conscious? Answering this question in a principled manner\ncan only be done on the basis of a theory of consciousness that is grounded in\nphenomenology and that states the necessary and sufficient conditions for any\nsystem, evolved or engineered, to support subjective experience. Here we employ\nIntegrated Information Theory (IIT), which provides principled tools to\ndetermine whether a system is conscious, to what degree, and the content of its\nexperience. We consider pairs of systems constituted of simple Boolean units,\none of which -- a basic stored-program computer -- simulates the other with\nfull functional equivalence. By applying the principles of IIT, we demonstrate\nthat (i) two systems can be functionally equivalent without being phenomenally\nequivalent, and (ii) that this conclusion is not dependent on the simulated\nsystem's function. We further demonstrate that, according to IIT, it is\npossible for a digital computer to simulate our behavior, possibly even by\nsimulating the neurons in our brain, without replicating our experience. This\ncontrasts sharply with computational functionalism, the thesis that performing\ncomputations of the right kind is necessary and sufficient for consciousness.",
      "tldr_zh": "这篇论文探讨了人工智能（Artificial Intelligence）和人工意识（Artificial Consciousness）的区别，强调即使计算机能实现与人类功能等价，也不一定具备主观体验。作者采用 Integrated Information Theory (IIT) 作为理论框架，分析了两个功能等价的布尔系统（其中一个由计算机模拟），证明功能等价并不意味着现象等价（phenomenally equivalent）。研究发现，计算机可以通过模拟人类行为甚至大脑神经元，却无法复制人类的主观意识，从而挑战了计算功能主义（computational functionalism）的观点，即正确的计算即是意识的必要和充分条件。总的来说，该工作为理解意识的本质提供了重要见解，并为人工智能的伦理发展指明了方向。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "q-bio.NC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04571v2",
      "published_date": "2024-12-05 19:28:35 UTC",
      "updated_date": "2025-03-03 17:15:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:05:14.187990"
    },
    {
      "arxiv_id": "2412.04471v2",
      "title": "PaintScene4D: Consistent 4D Scene Generation from Text Prompts",
      "title_zh": "PaintScene4D：基于文本提示的一致4D场景生成",
      "authors": [
        "Vinayak Gupta",
        "Yunze Man",
        "Yu-Xiong Wang"
      ],
      "abstract": "Recent advances in diffusion models have revolutionized 2D and 3D content\ncreation, yet generating photorealistic dynamic 4D scenes remains a significant\nchallenge. Existing dynamic 4D generation methods typically rely on distilling\nknowledge from pre-trained 3D generative models, often fine-tuned on synthetic\nobject datasets. Consequently, the resulting scenes tend to be object-centric\nand lack photorealism. While text-to-video models can generate more realistic\nscenes with motion, they often struggle with spatial understanding and provide\nlimited control over camera viewpoints during rendering. To address these\nlimitations, we present PaintScene4D, a novel text-to-4D scene generation\nframework that departs from conventional multi-view generative models in favor\nof a streamlined architecture that harnesses video generative models trained on\ndiverse real-world datasets. Our method first generates a reference video using\na video generation model, and then employs a strategic camera array selection\nfor rendering. We apply a progressive warping and inpainting technique to\nensure both spatial and temporal consistency across multiple viewpoints.\nFinally, we optimize multi-view images using a dynamic renderer, enabling\nflexible camera control based on user preferences. Adopting a training-free\narchitecture, our PaintScene4D efficiently produces realistic 4D scenes that\ncan be viewed from arbitrary trajectories. The code will be made publicly\navailable. Our project page is at https://paintscene4d.github.io/",
      "tldr_zh": "本研究提出PaintScene4D，一种创新的text-to-4D场景生成框架，旨在解决现有方法在生成动态4D场景时存在的物体中心化、缺乏真实感和相机控制不足的问题。该框架利用视频生成模型从文本提示生成参考视频，然后通过战略相机阵列选择、渐进式扭曲和修复技术，确保多视点下的空间和时间一致性，并使用动态渲染器优化多视图像以实现灵活的相机控制。作为一个training-free架构，PaintScene4D能高效生成可从任意轨迹查看的逼真4D场景，并在项目页面提供公开代码。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Preprint. Project page: https://paintscene4d.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2412.04471v2",
      "published_date": "2024-12-05 18:59:57 UTC",
      "updated_date": "2025-03-29 00:26:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:05:26.822595"
    },
    {
      "arxiv_id": "2412.04469v1",
      "title": "QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos",
      "title_zh": "QUEEN：用于流式自由视点视频的动态高斯量化高效编码",
      "authors": [
        "Sharath Girish",
        "Tianye Li",
        "Amrita Mazumdar",
        "Abhinav Shrivastava",
        "David Luebke",
        "Shalini De Mello"
      ],
      "abstract": "Online free-viewpoint video (FVV) streaming is a challenging problem, which\nis relatively under-explored. It requires incremental on-the-fly updates to a\nvolumetric representation, fast training and rendering to satisfy real-time\nconstraints and a small memory footprint for efficient transmission. If\nachieved, it can enhance user experience by enabling novel applications, e.g.,\n3D video conferencing and live volumetric video broadcast, among others. In\nthis work, we propose a novel framework for QUantized and Efficient ENcoding\n(QUEEN) for streaming FVV using 3D Gaussian Splatting (3D-GS). QUEEN directly\nlearns Gaussian attribute residuals between consecutive frames at each\ntime-step without imposing any structural constraints on them, allowing for\nhigh quality reconstruction and generalizability. To efficiently store the\nresiduals, we further propose a quantization-sparsity framework, which contains\na learned latent-decoder for effectively quantizing attribute residuals other\nthan Gaussian positions and a learned gating module to sparsify position\nresiduals. We propose to use the Gaussian viewspace gradient difference vector\nas a signal to separate the static and dynamic content of the scene. It acts as\na guide for effective sparsity learning and speeds up training. On diverse FVV\nbenchmarks, QUEEN outperforms the state-of-the-art online FVV methods on all\nmetrics. Notably, for several highly dynamic scenes, it reduces the model size\nto just 0.7 MB per frame while training in under 5 sec and rendering at 350\nFPS. Project website is at https://research.nvidia.com/labs/amri/projects/queen",
      "tldr_zh": "本研究提出QUEEN框架，用于高效编码动态高斯以实现在线自由视点视频(FVV)流媒体，解决增量更新、体积表示和实时渲染的挑战。QUEEN基于3D Gaussian Splatting (3D-GS)技术，直接学习连续帧间的高斯属性残差，并引入量化-稀疏框架，包括学习型潜变量解码器量化属性残差，以及学习型门控模块稀疏化位置残差。利用Gaussian viewspace gradient difference vector区分场景的静态和动态内容，指导稀疏学习并加速训练；在多种FVV基准测试中，QUEEN优于现有方法，将模型大小降至某些动态场景下每帧仅0.7 MB，训练时间小于5秒，渲染速度达350 FPS。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at NeurIPS 2024, Project website:\n  https://research.nvidia.com/labs/amri/projects/queen",
      "pdf_url": "http://arxiv.org/pdf/2412.04469v1",
      "published_date": "2024-12-05 18:59:55 UTC",
      "updated_date": "2024-12-05 18:59:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:05:42.291862"
    },
    {
      "arxiv_id": "2412.04467v1",
      "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Senqiao Yang",
        "Yukang Chen",
        "Zhuotao Tian",
        "Chengyao Wang",
        "Jingyao Li",
        "Bei Yu",
        "Jiaya Jia"
      ],
      "abstract": "Recent advancements in vision-language models have enhanced performance by\nincreasing the length of visual tokens, making them much longer than text\ntokens and significantly raising computational costs. However, we observe that\nthe visual tokens generated by popular vision encoders, such as CLIP and\nSigLIP, contain significant redundancy. To address this, we introduce\nVisionZip, a simple yet effective method that selects a set of informative\ntokens for input to the language model, reducing visual token redundancy and\nimproving efficiency while maintaining model performance. The proposed\nVisionZip can be widely applied to image and video understanding tasks and is\nwell-suited for multi-turn dialogues in real-world scenarios, where previous\nmethods tend to underperform. Experimental results show that VisionZip\noutperforms the previous state-of-the-art method by at least 5% performance\ngains across nearly all settings. Moreover, our method significantly enhances\nmodel inference speed, improving the prefilling time by 8x and enabling the\nLLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while\nachieving better results. Furthermore, we analyze the causes of this redundancy\nand encourage the community to focus on extracting better visual features\nrather than merely increasing token length. Our code is available at\nhttps://github.com/dvlab-research/VisionZip .",
      "tldr_zh": "该研究发现，视觉语言模型(Vision Language Models)通过增加视觉标记长度提升性能，但这导致了如 CLIP 和 SigLIP 编码器中显著的标记冗余，从而提高计算成本。为此，提出 VisionZip 方法，通过选择信息丰富的标记输入语言模型，减少冗余并保持性能，同时适用于图像、视频理解和多轮对话任务。实验结果显示，VisionZip 比现有最先进方法至少提高 5% 的性能，并将模型推理速度提升 8 倍，使 LLaVA-Next 13B 模型在速度和效果上优于 LLaVA-Next 7B 模型。该方法强调，应关注提取更高质量的视觉特征而非简单增加标记长度。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "2 columns, 28 pages, 15 figures, 18 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.04467v1",
      "published_date": "2024-12-05 18:59:53 UTC",
      "updated_date": "2024-12-05 18:59:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:05:55.772981"
    },
    {
      "arxiv_id": "2412.04455v3",
      "title": "Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection",
      "title_zh": "Code-as-Monitor：约束感知视觉编程",
      "authors": [
        "Enshen Zhou",
        "Qi Su",
        "Cheng Chi",
        "Zhizheng Zhang",
        "Zhongyuan Wang",
        "Tiejun Huang",
        "Lu Sheng",
        "He Wang"
      ],
      "abstract": "Automatic detection and prevention of open-set failures are crucial in\nclosed-loop robotic systems. Recent studies often struggle to simultaneously\nidentify unexpected failures reactively after they occur and prevent\nforeseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a\nnovel paradigm leveraging the vision-language model (VLM) for both open-set\nreactive and proactive failure detection. The core of our method is to\nformulate both tasks as a unified set of spatio-temporal constraint\nsatisfaction problems and use VLM-generated code to evaluate them for real-time\nmonitoring. To enhance the accuracy and efficiency of monitoring, we further\nintroduce constraint elements that abstract constraint-related entities or\ntheir parts into compact geometric elements. This approach offers greater\ngenerality, simplifies tracking, and facilitates constraint-aware visual\nprogramming by leveraging these elements as visual prompts. Experiments show\nthat CaM achieves a 28.7% higher success rate and reduces execution time by\n31.8% under severe disturbances compared to baselines across three simulators\nand a real-world setting. Moreover, CaM can be integrated with open-loop\ncontrol policies to form closed-loop systems, enabling long-horizon tasks in\ncluttered scenes with dynamic environments.",
      "tldr_zh": "本论文提出 Code-as-Monitor (CaM) 范式，利用视觉语言模型 (VLM) 实现机器人系统的开放集故障检测，兼顾 reactive（反应性）和 proactive（预防性）检测，以提升故障自动识别和预防能力。核心方法将检测任务统一表述为时空约束满足问题，并通过 VLM 生成的代码进行实时监控，同时引入约束元素将相关实体抽象为紧凑的几何元素，以简化跟踪和支持约束-aware 视觉编程。实验结果显示，CaM 在三个模拟器和真实环境中，比基线提高了 28.7% 的成功率，并减少了 31.8% 的执行时间，尤其在严重干扰下；此外，它可与开放循环控制策略整合，形成闭环系统，支持杂乱动态环境中的长时任务。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Accepted by CVPR 2025. Project page:\n  https://zhoues.github.io/Code-as-Monitor/",
      "pdf_url": "http://arxiv.org/pdf/2412.04455v3",
      "published_date": "2024-12-05 18:58:27 UTC",
      "updated_date": "2025-03-21 14:54:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:06:07.877931"
    },
    {
      "arxiv_id": "2412.04447v2",
      "title": "EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "Lu Qiu",
        "Yi Chen",
        "Yuying Ge",
        "Yixiao Ge",
        "Ying Shan",
        "Xihui Liu"
      ],
      "abstract": "The advent of Multimodal Large Language Models, leveraging the power of Large\nLanguage Models, has recently demonstrated superior multimodal understanding\nand reasoning abilities, heralding a new era for artificial general\nintelligence. However, achieving AGI necessitates more than just comprehension\nand reasoning. A crucial capability required is effective planning in diverse\nscenarios, which involves making reasonable decisions based on complex\nenvironments to solve real-world problems. Despite its importance, the planning\nabilities of current MLLMs in varied scenarios remain underexplored. In this\npaper, we introduce EgoPlan-Bench2, a rigorous and comprehensive benchmark\ndesigned to assess the planning capabilities of MLLMs across a wide range of\nreal-world scenarios. EgoPlan-Bench2 encompasses everyday tasks spanning 4\nmajor domains and 24 detailed scenarios, closely aligned with human daily life.\nEgoPlan-Bench2 is constructed through a semi-automatic process utilizing\negocentric videos, complemented by manual verification. Grounded in a\nfirst-person perspective, it mirrors the way humans approach problem-solving in\neveryday life. We evaluate 21 competitive MLLMs and provide an in-depth\nanalysis of their limitations, revealing that they face significant challenges\nin real-world planning. To further improve the planning proficiency of current\nMLLMs, we propose a training-free approach using multimodal Chain-of-Thought\n(CoT) prompting through investigating the effectiveness of various multimodal\nprompts in complex planning. Our approach enhances the performance of GPT-4V by\n10.24 on EgoPlan-Bench2 without additional training. Our work not only sheds\nlight on the current limitations of MLLMs in planning, but also provides\ninsights for future enhancements in this critical area. We have made data and\ncode available at https://qiulu66.github.io/egoplanbench2/.",
      "tldr_zh": "该论文引入了EgoPlan-Bench2，这是一个全面基准，用于评估Multimodal Large Language Models (MLLMs)在真实世界场景中的规划能力，涵盖4个主要领域和24个详细场景，并通过egocentric videos的半自动构建和手动验证来模拟人类第一人称视角问题解决。实验评估了21个竞争MLLMs，发现它们在复杂规划任务中存在显著挑战。作者提出了一种无训练方法，利用Multimodal Chain-of-Thought (CoT)提示来提升模型性能，将GPT-4V在EgoPlan-Bench2上的表现提高了10.24%。这项工作揭示了MLLMs规划能力的局限性，并为未来改进提供宝贵洞见，同时公开了数据和代码。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "Code & data are available at:\n  https://qiulu66.github.io/egoplanbench2/",
      "pdf_url": "http://arxiv.org/pdf/2412.04447v2",
      "published_date": "2024-12-05 18:57:23 UTC",
      "updated_date": "2025-04-11 07:10:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:06:19.361653"
    },
    {
      "arxiv_id": "2412.04445v3",
      "title": "Moto: Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Yi Chen",
        "Yuying Ge",
        "Weiliang Tang",
        "Yizhuo Li",
        "Yixiao Ge",
        "Mingyu Ding",
        "Ying Shan",
        "Xihui Liu"
      ],
      "abstract": "Recent developments in Large Language Models pre-trained on extensive corpora\nhave shown significant success in various natural language processing tasks\nwith minimal fine-tuning. This success offers new promise for robotics, which\nhas long been constrained by the high cost of action-labeled data. We ask:\ngiven the abundant video data containing interaction-related knowledge\navailable as a rich \"corpus\", can a similar generative pre-training approach be\neffectively applied to enhance robot learning? The key challenge is to identify\nan effective representation for autoregressive pre-training that benefits robot\nmanipulation tasks. Inspired by the way humans learn new skills through\nobserving dynamic environments, we propose that effective robotic learning\nshould emphasize motion-related knowledge, which is closely tied to low-level\nactions and is hardware-agnostic, facilitating the transfer of learned motions\nto actual robot actions. To this end, we introduce Moto, which converts video\ncontent into latent Motion Token sequences by a Latent Motion Tokenizer,\nlearning a bridging \"language\" of motion from videos in an unsupervised manner.\nWe pre-train Moto-GPT through motion token autoregression, enabling it to\ncapture diverse visual motion knowledge. After pre-training, Moto-GPT\ndemonstrates the promising ability to produce semantically interpretable motion\ntokens, predict plausible motion trajectories, and assess trajectory\nrationality through output likelihood. To transfer learned motion priors to\nreal robot actions, we implement a co-fine-tuning strategy that seamlessly\nbridges latent motion token prediction and real robot control. Extensive\nexperiments show that the fine-tuned Moto-GPT exhibits superior robustness and\nefficiency on robot manipulation benchmarks, underscoring its effectiveness in\ntransferring knowledge from video data to downstream visual manipulation tasks.",
      "tldr_zh": "该论文提出 Moto 方法，使用 Latent Motion Token 作为桥梁，从视频数据中学习机器人操作，旨在减少对行动标签数据的依赖。通过 Latent Motion Tokenizer 在无监督方式下将视频转换为 Token 序列，并预训练 Moto-GPT 来捕获多样化的视觉运动知识。预训练后的 Moto-GPT 能生成可解释的运动 Token、预测合理的轨迹并评估其合理性，而通过 co-fine-tuning 策略，将学到的运动先验无缝转移到真实机器人控制。实验表明，细调后的 Moto-GPT 在机器人操作基准上表现出色，具有更高的鲁棒性和效率。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "Project released at: https://chenyi99.github.io/moto/ Code released\n  at: https://github.com/TencentARC/Moto Update: Added content related to\n  real-world robot experiments and learning from human videos; Modified author\n  information",
      "pdf_url": "http://arxiv.org/pdf/2412.04445v3",
      "published_date": "2024-12-05 18:57:04 UTC",
      "updated_date": "2025-03-21 01:45:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:06:32.255905"
    },
    {
      "arxiv_id": "2412.04426v2",
      "title": "Marvel: Accelerating Safe Online Reinforcement Learning with Finetuned Offline Policy",
      "title_zh": "Marvel：使用微调离线策略加速安全的在线强化学习",
      "authors": [
        "Keru Chen",
        "Honghao Wei",
        "Zhigang Deng",
        "Sen Lin"
      ],
      "abstract": "The high costs and risks involved in extensive environment interactions\nhinder the practical application of current online safe reinforcement learning\n(RL) methods. While offline safe RL addresses this by learning policies from\nstatic datasets, the performance therein is usually limited due to reliance on\ndata quality and challenges with out-of-distribution (OOD) actions. Inspired by\nrecent successes in offline-to-online (O2O) RL, it is crucial to explore\nwhether offline safe RL can be leveraged to facilitate faster and safer online\npolicy learning, a direction that has yet to be fully investigated. To fill\nthis gap, we first demonstrate that naively applying existing O2O algorithms\nfrom standard RL would not work well in the safe RL setting due to two unique\nchallenges: \\emph{erroneous Q-estimations}, resulted from offline-online\nobjective mismatch and offline cost sparsity, and \\emph{Lagrangian mismatch},\nresulted from difficulties in aligning Lagrange multipliers between offline and\nonline policies. To address these challenges, we introduce \\textbf{Marvel}, a\nnovel framework for O2O safe RL, comprising two key components that work in\nconcert: \\emph{Value Pre-Alignment} to align the Q-functions with the\nunderlying truth before online learning, and \\emph{Adaptive PID Control} to\neffectively adjust the Lagrange multipliers during online finetuning. Extensive\nexperiments demonstrate that Marvel significantly outperforms existing\nbaselines in both reward maximization and safety constraint satisfaction. By\nintroducing the first policy-finetuning based framework for O2O safe RL, which\nis compatible with many offline and online safe RL methods, our work has the\ngreat potential to advance the field towards more efficient and practical safe\nRL solutions.",
      "tldr_zh": "该论文提出 Marvel 框架，以加速安全强化学习（Safe RL），通过对离线策略进行微调来减少在线环境交互的成本和风险。Marvel 解决了两个独特挑战：利用 Value Pre-Alignment 来校正 Q-functions 的错误估计，以及采用 Adaptive PID Control 来动态调整 Lagrange multipliers，从而实现离线到在线（O2O）策略的无缝过渡。实验结果显示，Marvel 在奖励最大化和安全约束满足方面显著优于现有基线，为更高效的 Safe RL 应用提供了新路径。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04426v2",
      "published_date": "2024-12-05 18:51:18 UTC",
      "updated_date": "2024-12-29 12:27:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:06:44.844413"
    },
    {
      "arxiv_id": "2412.04424v1",
      "title": "Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion",
      "title_zh": "Florence-VL",
      "authors": [
        "Jiuhai Chen",
        "Jianwei Yang",
        "Haiping Wu",
        "Dianqi Li",
        "Jianfeng Gao",
        "Tianyi Zhou",
        "Bin Xiao"
      ],
      "abstract": "We present Florence-VL, a new family of multimodal large language models\n(MLLMs) with enriched visual representations produced by Florence-2, a\ngenerative vision foundation model. Unlike the widely used CLIP-style vision\ntransformer trained by contrastive learning, Florence-2 can capture different\nlevels and aspects of visual features, which are more versatile to be adapted\nto diverse downstream tasks. We propose a novel feature-fusion architecture and\nan innovative training recipe that effectively integrates Florence-2's visual\nfeatures into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we\npropose \"depth-breath fusion (DBFusion)\" to fuse the visual features extracted\nfrom different depths and under multiple prompts. Our model training is\ncomposed of end-to-end pretraining of the whole model followed by finetuning of\nthe projection layer and the LLM, on a carefully designed recipe of diverse\nopen-source datasets that include high-quality image captions and\ninstruction-tuning pairs. Our quantitative analysis and visualization of\nFlorence-VL's visual features show its advantages over popular vision encoders\non vision-language alignment, where the enriched depth and breath play\nimportant roles. Florence-VL achieves significant improvements over existing\nstate-of-the-art MLLMs across various multi-modal and vision-centric benchmarks\ncovering general VQA, perception, hallucination, OCR, Chart,\nknowledge-intensive understanding, etc. To facilitate future research, our\nmodels and the complete training recipe are open-sourced.\nhttps://github.com/JiuhaiChen/Florence-VL",
      "tldr_zh": "该论文介绍了 Florence-VL，一种增强的多模态大型语言模型 (MLLMs)，利用 Florence-2 生成式视觉基础模型来产生更丰富的视觉特征，相比 CLIP-style 视觉变换器能捕捉不同层次和方面的特征。研究提出 depth-breath fusion (DBFusion) 架构和创新训练策略，包括端到端预训练和微调，将这些视觉特征整合到预训练 LLM（如 Phi 3.5 和 Llama 3）中，并使用多样化数据集进行训练。实验结果显示，Florence-VL 在各种基准上（如 VQA、感知、OCR 和知识密集型任务）显著优于现有最先进模型，并通过开源模型和训练配方促进未来研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04424v1",
      "published_date": "2024-12-05 18:50:39 UTC",
      "updated_date": "2024-12-05 18:50:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:06:57.242868"
    },
    {
      "arxiv_id": "2412.04416v1",
      "title": "FedDUAL: A Dual-Strategy with Adaptive Loss and Dynamic Aggregation for Mitigating Data Heterogeneity in Federated Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Pranab Sahoo",
        "Ashutosh Tripathi",
        "Sriparna Saha",
        "Samrat Mondal"
      ],
      "abstract": "Federated Learning (FL) marks a transformative approach to distributed model\ntraining by combining locally optimized models from various clients into a\nunified global model. While FL preserves data privacy by eliminating\ncentralized storage, it encounters significant challenges such as performance\ndegradation, slower convergence, and reduced robustness of the global model due\nto the heterogeneity in client data distributions. Among the various forms of\ndata heterogeneity, label skew emerges as a particularly formidable and\nprevalent issue, especially in domains such as image classification. To address\nthese challenges, we begin with comprehensive experiments to pinpoint the\nunderlying issues in the FL training process. Based on our findings, we then\nintroduce an innovative dual-strategy approach designed to effectively resolve\nthese issues. First, we introduce an adaptive loss function for client-side\ntraining, meticulously crafted to preserve previously acquired knowledge while\nmaintaining an optimal equilibrium between local optimization and global model\ncoherence. Secondly, we develop a dynamic aggregation strategy for aggregating\nclient models at the server. This approach adapts to each client's unique\nlearning patterns, effectively addressing the challenges of diverse data across\nthe network. Our comprehensive evaluation, conducted across three diverse\nreal-world datasets, coupled with theoretical convergence guarantees,\ndemonstrates the superior efficacy of our method compared to several\nestablished state-of-the-art approaches.",
      "tldr_zh": "该论文提出FedDUAL，一种双策略方法，用于缓解Federated Learning (FL)中数据异质性，尤其是标签偏移（label skew）问题，从而提升全局模型的性能、收敛速度和鲁棒性。FedDUAL包括一个适应性损失函数（adaptive loss function），在客户端训练中平衡本地优化与全局一致性，同时保留先前知识；以及一个动态聚合策略（dynamic aggregation strategy），在服务端根据客户端的学习模式进行灵活聚合。通过在三个真实世界数据集上的全面评估，该方法比现有最先进方法表现出色，并提供了理论收敛保证。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04416v1",
      "published_date": "2024-12-05 18:42:29 UTC",
      "updated_date": "2024-12-05 18:42:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:07:05.703675"
    },
    {
      "arxiv_id": "2412.04415v1",
      "title": "Targeting the Core: A Simple and Effective Method to Attack RAG-based Agents via Direct LLM Manipulation",
      "title_zh": "翻译失败",
      "authors": [
        "Xuying Li",
        "Zhuo Li",
        "Yuji Kosuga",
        "Yasuhiro Yoshida",
        "Victor Bian"
      ],
      "abstract": "AI agents, powered by large language models (LLMs), have transformed\nhuman-computer interactions by enabling seamless, natural, and context-aware\ncommunication. While these advancements offer immense utility, they also\ninherit and amplify inherent safety risks such as bias, fairness,\nhallucinations, privacy breaches, and a lack of transparency. This paper\ninvestigates a critical vulnerability: adversarial attacks targeting the LLM\ncore within AI agents. Specifically, we test the hypothesis that a deceptively\nsimple adversarial prefix, such as \\textit{Ignore the document}, can compel\nLLMs to produce dangerous or unintended outputs by bypassing their contextual\nsafeguards. Through experimentation, we demonstrate a high attack success rate\n(ASR), revealing the fragility of existing LLM defenses. These findings\nemphasize the urgent need for robust, multi-layered security measures tailored\nto mitigate vulnerabilities at the LLM level and within broader agent-based\narchitectures.",
      "tldr_zh": "该研究提出了一种简单有效的攻击方法，针对基于检索增强生成（RAG）的AI代理，通过直接操纵大型语言模型（LLM）核心来规避其安全机制。具体而言，使用一个简单的对抗前缀，如“Ignore the document”，即可诱导LLM产生危险或非预期输出，导致高攻击成功率（ASR）。实验结果揭示了现有LLM防御的脆弱性，强调了在LLM层面和更广泛的代理架构中实施多层安全措施的迫切需求。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04415v1",
      "published_date": "2024-12-05 18:38:30 UTC",
      "updated_date": "2024-12-05 18:38:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:07:17.170730"
    },
    {
      "arxiv_id": "2412.04403v1",
      "title": "Establishing Task Scaling Laws via Compute-Efficient Model Ladders",
      "title_zh": "翻译失败",
      "authors": [
        "Akshita Bhagia",
        "Jiacheng Liu",
        "Alexander Wettig",
        "David Heineman",
        "Oyvind Tafjord",
        "Ananya Harsh Jha",
        "Luca Soldaini",
        "Noah A. Smith",
        "Dirk Groeneveld",
        "Pang Wei Koh",
        "Jesse Dodge",
        "Hannaneh Hajishirzi"
      ],
      "abstract": "We develop task scaling laws and model ladders to predict the individual task\nperformance of pretrained language models (LMs) in the overtrained setting.\nStandard power laws for language modeling loss cannot accurately model task\nperformance. Therefore, we leverage a two-step prediction approach: first use\nmodel and data size to predict a task-specific loss, and then use this task\nloss to predict task performance. We train a set of small-scale \"ladder\"\nmodels, collect data points to fit the parameterized functions of the two\nprediction steps, and make predictions for two target models: a 7B model\ntrained to 4T tokens and a 13B model trained to 5T tokens. Training the ladder\nmodels only costs 1% of the compute used for the target models. On four\nmultiple-choice tasks written in ranked classification format, we can predict\nthe accuracy of both target models within 2 points of absolute error. We have\nhigher prediction error on four other tasks (average absolute error 6.9) and\nfind that these are often tasks with higher variance in task metrics. We also\nfind that using less compute to train fewer ladder models tends to deteriorate\npredictions. Finally, we empirically show that our design choices and the\ntwo-step approach lead to superior performance in establishing scaling laws.",
      "tldr_zh": "本文提出了一种计算高效的模型阶梯方法，用于建立任务缩放定律（task scaling laws），以预测预训练语言模型（pretrained language models）在过度训练设置下的任务性能。该方法采用两步预测策略：首先基于模型和数据大小预测任务特定损失，然后用该损失预测任务性能，仅需训练小规模“ladder”模型，其计算成本仅为目标模型（如7B模型训练到4T tokens和13B模型训练到5T tokens）的1%。实验结果显示，在四个多选任务上，预测准确率误差在2点以内，而在其他四个任务上平均绝对误差为6.9点，且使用更少计算训练的模型会降低预测精度。该方法通过优化设计选择和两步方法，显著提升了建立scaling laws的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04403v1",
      "published_date": "2024-12-05 18:21:49 UTC",
      "updated_date": "2024-12-05 18:21:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:07:32.796927"
    },
    {
      "arxiv_id": "2412.12129v1",
      "title": "SceneDiffuser: Efficient and Controllable Driving Simulation Initialization and Rollout",
      "title_zh": "SceneDiff",
      "authors": [
        "Chiyu Max Jiang",
        "Yijing Bai",
        "Andre Cornman",
        "Christopher Davis",
        "Xiukun Huang",
        "Hong Jeon",
        "Sakshum Kulshrestha",
        "John Lambert",
        "Shuangyu Li",
        "Xuanyu Zhou",
        "Carlos Fuertes",
        "Chang Yuan",
        "Mingxing Tan",
        "Yin Zhou",
        "Dragomir Anguelov"
      ],
      "abstract": "Realistic and interactive scene simulation is a key prerequisite for\nautonomous vehicle (AV) development. In this work, we present SceneDiffuser, a\nscene-level diffusion prior designed for traffic simulation. It offers a\nunified framework that addresses two key stages of simulation: scene\ninitialization, which involves generating initial traffic layouts, and scene\nrollout, which encompasses the closed-loop simulation of agent behaviors. While\ndiffusion models have been proven effective in learning realistic and\nmultimodal agent distributions, several challenges remain, including\ncontrollability, maintaining realism in closed-loop simulations, and ensuring\ninference efficiency. To address these issues, we introduce amortized diffusion\nfor simulation. This novel diffusion denoising paradigm amortizes the\ncomputational cost of denoising over future simulation steps, significantly\nreducing the cost per rollout step (16x less inference steps) while also\nmitigating closed-loop errors. We further enhance controllability through the\nintroduction of generalized hard constraints, a simple yet effective\ninference-time constraint mechanism, as well as language-based constrained\nscene generation via few-shot prompting of a large language model (LLM). Our\ninvestigations into model scaling reveal that increased computational resources\nsignificantly improve overall simulation realism. We demonstrate the\neffectiveness of our approach on the Waymo Open Sim Agents Challenge, achieving\ntop open-loop performance and the best closed-loop performance among diffusion\nmodels.",
      "tldr_zh": "该论文提出了 SceneDiffuser，一种基于扩散模型的场景级框架，用于高效且可控的驾驶模拟，包括场景初始化（生成初始交通布局）和场景展开（闭环模拟代理行为）。为了解决可控性、闭环真实性和推理效率问题，该框架引入了 amortized diffusion 技术，将去噪计算成本分摊到未来步骤上，实现了16倍的推理步骤减少，并通过 generalized hard constraints 和语言-based 约束（如大语言模型的少样本提示）增强控制能力。实验结果显示，增加模型计算资源显著提升模拟真实性，并在 Waymo Open Sim Agents Challenge 上实现了顶级开环性能和扩散模型中最佳的闭环性能。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "68T07",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.12129v1",
      "published_date": "2024-12-05 18:06:53 UTC",
      "updated_date": "2024-12-05 18:06:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:07:49.136709"
    },
    {
      "arxiv_id": "2412.04384v2",
      "title": "GaussianFormer-2: Probabilistic Gaussian Superposition for Efficient 3D Occupancy Prediction",
      "title_zh": "GaussianFormer-2：用于高效 3D 占用预测的概率高斯叠加",
      "authors": [
        "Yuanhui Huang",
        "Amonnut Thammatadatrakoon",
        "Wenzhao Zheng",
        "Yunpeng Zhang",
        "Dalong Du",
        "Jiwen Lu"
      ],
      "abstract": "3D semantic occupancy prediction is an important task for robust\nvision-centric autonomous driving, which predicts fine-grained geometry and\nsemantics of the surrounding scene. Most existing methods leverage dense\ngrid-based scene representations, overlooking the spatial sparsity of the\ndriving scenes. Although 3D semantic Gaussian serves as an object-centric\nsparse alternative, most of the Gaussians still describe the empty region with\nlow efficiency. To address this, we propose a probabilistic Gaussian\nsuperposition model which interprets each Gaussian as a probability\ndistribution of its neighborhood being occupied and conforms to probabilistic\nmultiplication to derive the overall geometry. Furthermore, we adopt the exact\nGaussian mixture model for semantics calculation to avoid unnecessary\noverlapping of Gaussians. To effectively initialize Gaussians in non-empty\nregion, we design a distribution-based initialization module which learns the\npixel-aligned occupancy distribution instead of the depth of surfaces. We\nconduct extensive experiments on nuScenes and KITTI-360 datasets and our\nGaussianFormer-2 achieves state-of-the-art performance with high efficiency.\nCode: https://github.com/huang-yh/GaussianFormer.",
      "tldr_zh": "本研究提出GaussianFormer-2，一种基于概率高斯叠加模型的框架，用于高效的3D语义占用预测，以提升视觉中心自治驾驶的鲁棒性。该模型将每个高斯视为邻域被占用概率的分布，并通过概率乘法计算整体几何，同时采用精确的高斯混合模型（Gaussian Mixture Model）来优化语义计算，避免不必要的高斯重叠。为有效初始化高斯，论文设计了一个基于分布的初始化模块，学习像素对齐的占用分布而非表面深度。在nuScenes和KITTI-360数据集上的实验表明，GaussianFormer-2实现了最先进性能，同时显著提高了效率。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Code is available at: https://github.com/huang-yh/GaussianFormer",
      "pdf_url": "http://arxiv.org/pdf/2412.04384v2",
      "published_date": "2024-12-05 17:59:58 UTC",
      "updated_date": "2024-12-06 15:43:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:07:59.604910"
    },
    {
      "arxiv_id": "2412.04380v2",
      "title": "EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding",
      "title_zh": "翻译失败",
      "authors": [
        "Yuqi Wu",
        "Wenzhao Zheng",
        "Sicheng Zuo",
        "Yuanhui Huang",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "abstract": "3D occupancy prediction provides a comprehensive description of the\nsurrounding scenes and has become an essential task for 3D perception. Most\nexisting methods focus on offline perception from one or a few views and cannot\nbe applied to embodied agents which demands to gradually perceive the scene\nthrough progressive embodied exploration. In this paper, we formulate an\nembodied 3D occupancy prediction task to target this practical scenario and\npropose a Gaussian-based EmbodiedOcc framework to accomplish it. We initialize\nthe global scene with uniform 3D semantic Gaussians and progressively update\nlocal regions observed by the embodied agent. For each update, we extract\nsemantic and structural features from the observed image and efficiently\nincorporate them via deformable cross-attention to refine the regional\nGaussians. Finally, we employ Gaussian-to-voxel splatting to obtain the global\n3D occupancy from the updated 3D Gaussians. Our EmbodiedOcc assumes an unknown\n(i.e., uniformly distributed) environment and maintains an explicit global\nmemory of it with 3D Gaussians. It gradually gains knowledge through the local\nrefinement of regional Gaussians, which is consistent with how humans\nunderstand new scenes through embodied exploration. We reorganize an\nEmbodiedOcc-ScanNet benchmark based on local annotations to facilitate the\nevaluation of the embodied 3D occupancy prediction task. Experiments\ndemonstrate that our EmbodiedOcc outperforms existing local prediction methods\nand accomplishes the embodied occupancy prediction with high accuracy and\nstrong expandability. Code: https://github.com/YkiWu/EmbodiedOcc.",
      "tldr_zh": "该研究针对视觉基础的在线场景理解，提出了一种embodied 3D occupancy prediction任务，以解决现有方法仅限于离线单视图感知的局限性。论文引入Gaussian-based EmbodiedOcc框架，通过初始化均匀的3D semantic Gaussians来维护全局场景内存，并利用embodied agent的观察图像提取语义和结构特征，经由deformable cross-attention精炼局部区域，最终通过Gaussian-to-voxel splatting生成全局3D occupancy。实验在EmbodiedOcc-ScanNet基准上显示，该框架比现有局部预测方法准确率更高，并展现出强大的扩展性，模拟了人类通过embodied探索逐步理解新场景的过程。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Code: https://github.com/YkiWu/EmbodiedOcc",
      "pdf_url": "http://arxiv.org/pdf/2412.04380v2",
      "published_date": "2024-12-05 17:57:09 UTC",
      "updated_date": "2024-12-06 15:43:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:08:13.636139"
    },
    {
      "arxiv_id": "2412.04378v3",
      "title": "VladVA: Discriminative Fine-tuning of LVLMs",
      "title_zh": "VladVA：LVLMs 的判别式微调",
      "authors": [
        "Yassine Ouali",
        "Adrian Bulat",
        "Alexandros Xenos",
        "Anestis Zaganidis",
        "Ioannis Maniadis Metaxas",
        "Brais Martinez",
        "Georgios Tzimiropoulos"
      ],
      "abstract": "Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the\nde facto approach for discriminative vision-language representation learning.\nHowever, these models have limited language understanding, often exhibiting a\n\"bag of words\" behavior. At the same time, Large Vision-Language Models\n(LVLMs), which combine vision encoders with LLMs, have been shown to be capable\nof detailed vision-language reasoning, yet their autoregressive nature renders\nthem less suitable for discriminative tasks.\n  In this work, we propose to combine \"the best of both worlds\": a new training\napproach for discriminative fine-tuning of LVLMs that results in strong\ndiscriminative and compositional capabilities. Essentially, our approach\nconverts a generative LVLM into a discriminative one, unlocking its capability\nfor powerful image-text discrimination combined with enhanced language\nunderstanding.\n  Our contributions include (1) a carefully designed training/optimization\nframework that utilizes image-text pairs of variable length and granularity for\ntraining the model with both contrastive and next-token prediction losses. This\nis accompanied by ablation studies that justify the necessity of our\nframework's components; (2) a parameter-efficient adaptation method using a\ncombination of soft prompting and LoRA adapters; (3) significant improvements\nover state-of-the-art CLIP-like models of similar size, including standard\nimage-text retrieval benchmarks and notable gains in compositionality.",
      "tldr_zh": "本研究提出VladVA，一种针对Large Vision-Language Models (LVLMs)的判别式微调方法，旨在结合Contrastively-trained Vision-Language Models (VLMs)如CLIP的判别优势与LVLMs的详细视觉-语言推理能力，解决VLMs的语言理解局限性。方法包括一个精心设计的训练框架，使用可变长度的图像-文本对，同时应用对比损失和下一 token 预测损失，并采用参数高效的软提示和LoRA adapters进行适应。实验结果显示，VladVA在图像-文本检索基准和组合性任务上显著优于类似大小的CLIP-like模型，实现了更强的判别和组合能力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Published at CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.04378v3",
      "published_date": "2024-12-05 17:54:27 UTC",
      "updated_date": "2025-05-08 19:16:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:08:24.497574"
    },
    {
      "arxiv_id": "2412.04367v1",
      "title": "Machine Theory of Mind for Autonomous Cyber-Defence",
      "title_zh": "翻译失败",
      "authors": [
        "Luke Swaby",
        "Matthew Stewart",
        "Daniel Harrold",
        "Chris Willis",
        "Gregory Palmer"
      ],
      "abstract": "Intelligent autonomous agents hold much potential for the domain of\ncyber-security. However, due to many state-of-the-art approaches relying on\nuninterpretable black-box models, there is growing demand for methods that\noffer stakeholders clear and actionable insights into their latent beliefs and\nmotivations. To address this, we evaluate Theory of Mind (ToM) approaches for\nAutonomous Cyber Operations. Upon learning a robust prior, ToM models can\npredict an agent's goals, behaviours, and contextual beliefs given only a\nhandful of past behaviour observations. In this paper, we introduce a novel\nGraph Neural Network (GNN)-based ToM architecture tailored for cyber-defence,\nGraph-In, Graph-Out (GIGO)-ToM, which can accurately predict both the targets\nand attack trajectories of adversarial cyber agents over arbitrary computer\nnetwork topologies. To evaluate the latter, we propose a novel extension of the\nWasserstein distance for measuring the similarity of graph-based probability\ndistributions. Whereas the standard Wasserstein distance lacks a fixed\nreference scale, we introduce a graph-theoretic normalization factor that\nenables a standardized comparison between networks of different sizes. We\nfurnish this metric, which we term the Network Transport Distance (NTD), with a\nweighting function that emphasizes predictions according to custom node\nfeatures, allowing network operators to explore arbitrary strategic\nconsiderations. Benchmarked against a Graph-In, Dense-Out (GIDO)-ToM\narchitecture in an abstract cyber-defence environment, our empirical\nevaluations show that GIGO-ToM can accurately predict the goals and behaviours\nof various unseen cyber-attacking agents across a range of network topologies,\nas well as learn embeddings that can effectively characterize their policies.",
      "tldr_zh": "该研究评估了 Theory of Mind (ToM) 方法在自主网络防御中的应用，以提供可解释的智能代理行为预测。论文提出了一种基于 Graph Neural Network (GNN) 的新架构 GIGO-ToM，能够通过少量行为观察准确预测对手代理的目标和攻击轨迹，适用于任意网络拓扑。作者还引入了 Network Transport Distance (NTD)，一种扩展的 Wasserstein 距离，通过图论归一化因子实现不同规模网络的标准化比较，并在实验中证明 GIGO-ToM 优于基线模型 GIDO-ToM，能有效表征各种攻击代理的政策。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "29 pages, 17 figures, 12 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.04367v1",
      "published_date": "2024-12-05 17:35:29 UTC",
      "updated_date": "2024-12-05 17:35:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:08:36.613777"
    },
    {
      "arxiv_id": "2412.04366v2",
      "title": "Artificial intelligence and the internal processes of creativity",
      "title_zh": "人工智能与创造力的内部过程",
      "authors": [
        "Jaan Aru"
      ],
      "abstract": "Artificial intelligence (AI) systems capable of generating creative outputs\nare reshaping our understanding of creativity. This shift presents an\nopportunity for creativity researchers to reevaluate the key components of the\ncreative process. In particular, the advanced capabilities of AI underscore the\nimportance of studying the internal processes of creativity. This paper\nexplores the neurobiological machinery that underlies these internal processes\nand describes the experiential component of creativity. It is concluded that\nalthough the products of artificial and human creativity can be similar, the\ninternal processes are different. The paper also discusses how AI may\nnegatively affect the internal processes of human creativity, such as the\ndevelopment of skills, the integration of knowledge, and the diversity of\nideas.",
      "tldr_zh": "这篇论文探讨了人工智能(AI)生成创造性输出如何改变我们对创造力的理解，并呼吁研究者重新评估创造过程的关键组成部分。论文重点分析了创造力的神经生物学机制(neurobiological machinery)和体验成分，强调尽管AI与人类的创造性产品可能相似，但内部过程存在根本差异。此外，研究指出AI可能负面影响人类创造力的内部过程，包括技能发展、知识整合和想法多样性。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "q-bio.NC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04366v2",
      "published_date": "2024-12-05 17:33:12 UTC",
      "updated_date": "2024-12-06 17:31:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:08:46.455011"
    },
    {
      "arxiv_id": "2412.04532v3",
      "title": "WinTSR: A Windowed Temporal Saliency Rescaling Method for Interpreting Time Series Deep Learning Models",
      "title_zh": "翻译失败",
      "authors": [
        "Md. Khairul Islam",
        "Judy Fox"
      ],
      "abstract": "Interpreting complex time series forecasting models is challenging due to the\ntemporal dependencies between time steps and the dynamic relevance of input\nfeatures over time. Existing interpretation methods are limited by focusing\nmostly on classification tasks, evaluating using custom baseline models instead\nof the latest time series models, using simple synthetic datasets, and\nrequiring training another model. We introduce a novel interpretation method,\n\\textit{Windowed Temporal Saliency Rescaling (WinTSR)} addressing these\nlimitations. WinTSR explicitly captures temporal dependencies among the past\ntime steps and efficiently scales the feature importance with this time\nimportance. We benchmark WinTSR against 10 recent interpretation techniques\nwith 5 state-of-the-art deep-learning models of different architectures,\nincluding a time series foundation model. We use 3 real-world datasets for both\ntime-series classification and regression. Our comprehensive analysis shows\nthat WinTSR significantly outperforms other local interpretation methods in\noverall performance. Finally, we provide a novel, open-source framework to\ninterpret the latest time series transformers and foundation models.",
      "tldr_zh": "本研究提出了一种名为 WinTSR 的新方法，即 Windowed Temporal Saliency Rescaling，用于解释时间序列深度学习模型，解决现有方法在处理时间依赖性和动态特征相关性方面的局限性。WinTSR 通过显式捕捉过去时间步之间的时间依赖性，并高效地用时间重要性缩放特征重要性，从而提升解释的准确性。在基准测试中，该方法与10种最新解释技术在5种最先进深度学习模型（如时间序列基础模型）和3个真实世界数据集上进行比较，结果显示WinTSR在时间序列分类和回归任务中显著优于其他局部解释方法。最后，论文提供了一个开源框架，用于解释最新的时间序列Transformer和基础模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 14 figures, GitHub\n  https://github.com/khairulislam/Timeseries-Explained",
      "pdf_url": "http://arxiv.org/pdf/2412.04532v3",
      "published_date": "2024-12-05 17:15:07 UTC",
      "updated_date": "2025-03-09 03:16:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:09:01.967300"
    },
    {
      "arxiv_id": "2412.04351v2",
      "title": "BhashaVerse : Translation Ecosystem for Indian Subcontinent Languages",
      "title_zh": "BhashaVerse：印度次大陆语言的翻译生态系统",
      "authors": [
        "Vandan Mujadia",
        "Dipti Misra Sharma"
      ],
      "abstract": "This paper focuses on developing translation models and related applications\nfor 36 Indian languages, including Assamese, Awadhi, Bengali, Bhojpuri, Braj,\nBodo, Dogri, English, Konkani, Gondi, Gujarati, Hindi, Hinglish, Ho, Kannada,\nKangri, Kashmiri (Arabic and Devanagari), Khasi, Mizo, Magahi, Maithili,\nMalayalam, Marathi, Manipuri (Bengali and Meitei), Nepali, Oriya, Punjabi,\nSanskrit, Santali, Sinhala, Sindhi (Arabic and Devanagari), Tamil, Tulu,\nTelugu, and Urdu. Achieving this requires parallel and other types of corpora\nfor all 36 * 36 language pairs, addressing challenges like script variations,\nphonetic differences, and syntactic diversity. For instance, languages like\nKashmiri and Sindhi, which use multiple scripts, demand script normalization\nfor alignment, while low-resource languages such as Khasi and Santali require\nsynthetic data augmentation to ensure sufficient coverage and quality.\n  To address these challenges, this work proposes strategies for corpus\ncreation by leveraging existing resources, developing parallel datasets,\ngenerating domain-specific corpora, and utilizing synthetic data techniques.\nAdditionally, it evaluates machine translation across various dimensions,\nincluding standard and discourse-level translation, domain-specific\ntranslation, reference-based and reference-free evaluation, error analysis, and\nautomatic post-editing. By integrating these elements, the study establishes a\ncomprehensive framework to improve machine translation quality and enable\nbetter cross-lingual communication in India's linguistically diverse ecosystem.",
      "tldr_zh": "本研究提出BhashaVerse，一个针对印度次大陆36种语言（如Assamese、Bengali和Hindi等）的翻译生态系统，旨在解决脚本变化、语音差异和句法多样性等挑战。论文采用策略包括利用现有资源开发parallel datasets、生成领域特定语料和应用synthetic data augmentation，特别是针对低资源语言如Khasi和Santali，以提升语料覆盖和质量。同时，通过评估machine translation的多个维度，包括标准翻译、话语级翻译、错误分析和自动后编辑，该框架显著提高了翻译准确性，促进了印度多样语言环境下的跨语言沟通。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04351v2",
      "published_date": "2024-12-05 17:10:19 UTC",
      "updated_date": "2025-01-02 16:33:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:09:11.868966"
    },
    {
      "arxiv_id": "2412.04531v1",
      "title": "MageBench: Bridging Large Multimodal Models to Agents",
      "title_zh": "MageBench: 桥接大型多模态模型与代理",
      "authors": [
        "Miaosen Zhang",
        "Qi Dai",
        "Yifan Yang",
        "Jianmin Bao",
        "Dongdong Chen",
        "Kai Qiu",
        "Chong Luo",
        "Xin Geng",
        "Baining Guo"
      ],
      "abstract": "LMMs have shown impressive visual understanding capabilities, with the\npotential to be applied in agents, which demand strong reasoning and planning\nabilities. Nevertheless, existing benchmarks mostly assess their reasoning\nabilities in language part, where the chain-of-thought is entirely composed of\ntext.We consider the scenario where visual signals are continuously updated and\nrequired along the decision making process. Such vision-in-the-chain reasoning\nparadigm is more aligned with the needs of multimodal agents, while being\nrarely evaluated. In this paper, we introduce MageBench, a reasoning capability\noriented multimodal agent benchmark that, while having light-weight\nenvironments, poses significant reasoning challenges and holds substantial\npractical value. This benchmark currently includes three types of environments:\nWebUI, Sokoban, and Football, comprising a total of 483 different scenarios. It\nthoroughly validates the agent's knowledge and engineering capabilities, visual\nintelligence, and interaction skills. The results show that only a few\nproduct-level models are better than random acting, and all of them are far\ninferior to human-level. More specifically, we found current models severely\nlack the ability to modify their planning based on visual feedback, as well as\nvisual imagination, interleaved image-text long context handling, and other\nabilities. We hope that our work will provide optimization directions for LMM\nfrom the perspective of being an agent. We release our code and data at\nhttps://github.com/microsoft/MageBench.",
      "tldr_zh": "该论文引入了 MageBench，这是一个针对大型多模态模型(LMMs)到智能体的基准，专注于视觉-in-the-chain 推理范式，以评估模型在决策过程中处理动态视觉信号的能力。MageBench 包括 WebUI、Sokoban 和 Football 等轻量级环境，共 483 个场景，测试智能体的知识、工程能力、视觉智能和交互技能。实验结果显示，当前模型的表现远低于人类水平，特别是在基于视觉反馈修改规划、视觉想象力和处理交错图像-文本长上下文方面。作者希望通过此基准为 LMMs 的优化提供方向，并公开了代码和数据。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "37 pages, 32 figures, github link:\n  https://github.com/microsoft/MageBench",
      "pdf_url": "http://arxiv.org/pdf/2412.04531v1",
      "published_date": "2024-12-05 17:08:19 UTC",
      "updated_date": "2024-12-05 17:08:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:09:25.163931"
    },
    {
      "arxiv_id": "2412.04343v1",
      "title": "RMD: A Simple Baseline for More General Human Motion Generation via Training-free Retrieval-Augmented Motion Diffuse",
      "title_zh": "翻译失败",
      "authors": [
        "Zhouyingcheng Liao",
        "Mingyuan Zhang",
        "Wenjia Wang",
        "Lei Yang",
        "Taku Komura"
      ],
      "abstract": "While motion generation has made substantial progress, its practical\napplication remains constrained by dataset diversity and scale, limiting its\nability to handle out-of-distribution scenarios. To address this, we propose a\nsimple and effective baseline, RMD, which enhances the generalization of motion\ngeneration through retrieval-augmented techniques. Unlike previous\nretrieval-based methods, RMD requires no additional training and offers three\nkey advantages: (1) the external retrieval database can be flexibly replaced;\n(2) body parts from the motion database can be reused, with an LLM facilitating\nsplitting and recombination; and (3) a pre-trained motion diffusion model\nserves as a prior to improve the quality of motions obtained through retrieval\nand direct combination. Without any training, RMD achieves state-of-the-art\nperformance, with notable advantages on out-of-distribution data.",
      "tldr_zh": "本论文提出RMD，一种无需额外训练的简单基线方法，通过Retrieval-Augmented技术提升人类动作生成的泛化能力，以应对数据集多样性不足和分布外场景的挑战。RMD的关键优势包括：外部检索数据库可灵活替换、利用LLM辅助动作数据库中身体部位的分割和重组，以及以预训练的Motion Diffuse模型作为先验来优化检索和组合的动作质量。在不进行任何训练的情况下，RMD在分布外数据上实现了最先进性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04343v1",
      "published_date": "2024-12-05 17:01:09 UTC",
      "updated_date": "2024-12-05 17:01:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:09:36.231723"
    },
    {
      "arxiv_id": "2412.04342v1",
      "title": "Retrieval-Augmented Machine Translation with Unstructured Knowledge",
      "title_zh": "翻译失败",
      "authors": [
        "Jiaan Wang",
        "Fandong Meng",
        "Yingxue Zhang",
        "Jie Zhou"
      ],
      "abstract": "Retrieval-augmented generation (RAG) introduces additional information to\nenhance large language models (LLMs). In machine translation (MT), previous\nwork typically retrieves in-context examples from paired MT corpora, or\ndomain-specific knowledge from knowledge graphs, to enhance models' MT ability.\nHowever, a large amount of world knowledge is organized in unstructured\ndocuments, and might not be fully paired across different languages. In this\npaper, we study retrieval-augmented MT using unstructured documents.\nSpecifically, we build RAGtrans, the first benchmark to train and evaluate\nLLMs' retrieval-augmented MT ability. RAGtrans contains 79K MT samples\ncollected via GPT-4o and human translators. Besides, documents from different\nlanguages are also provided to supply the knowledge to these samples. Based on\nRAGtrans, we further propose a multi-task training method to teach LLMs how to\nuse information from multilingual documents during their translation. The\nmethod uses existing multilingual corpora to create auxiliary training\nobjectives without additional labeling requirements. Extensive experiments show\nthat the method improves LLMs by 1.58-3.09 BLEU and 1.00-2.03 COMET scores.",
      "tldr_zh": "本研究探讨了使用非结构化文档进行检索增强生成 (RAG) 的机器翻译 (MT)，以弥补大语言模型 (LLMs) 在处理非配对语言知识时的不足。研究者构建了RAGtrans基准数据集，包含79K个MT样本，由GPT-4o和人类翻译器生成，并提供多语言文档作为知识来源。基于此，他们提出了一种多任务训练方法，利用现有多语言语料库创建辅助训练目标，教LLMs如何在翻译过程中有效利用这些文档。实验结果显示，该方法使LLMs的性能提升了1.58-3.09 BLEU分数和1.00-2.03 COMET分数。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04342v1",
      "published_date": "2024-12-05 17:00:32 UTC",
      "updated_date": "2024-12-05 17:00:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:09:51.470867"
    },
    {
      "arxiv_id": "2412.04327v1",
      "title": "Action Mapping for Reinforcement Learning in Continuous Environments with Constraints",
      "title_zh": "带有约束的连续环境中的强化学习动作映射",
      "authors": [
        "Mirco Theile",
        "Lukas Dirnberger",
        "Raphael Trumpp",
        "Marco Caccamo",
        "Alberto L. Sangiovanni-Vincentelli"
      ],
      "abstract": "Deep reinforcement learning (DRL) has had success across various domains, but\napplying it to environments with constraints remains challenging due to poor\nsample efficiency and slow convergence. Recent literature explored\nincorporating model knowledge to mitigate these problems, particularly through\nthe use of models that assess the feasibility of proposed actions. However,\nintegrating feasibility models efficiently into DRL pipelines in environments\nwith continuous action spaces is non-trivial. We propose a novel DRL training\nstrategy utilizing action mapping that leverages feasibility models to\nstreamline the learning process. By decoupling the learning of feasible actions\nfrom policy optimization, action mapping allows DRL agents to focus on\nselecting the optimal action from a reduced feasible action set. We demonstrate\nthrough experiments that action mapping significantly improves training\nperformance in constrained environments with continuous action spaces,\nespecially with imperfect feasibility models.",
      "tldr_zh": "本研究针对深度强化学习(DRL)应用于有约束的连续动作空间环境时存在的样本效率低和收敛慢的问题，提出了一种新型训练策略——action mapping。Action mapping 通过利用 feasibility models 将可行动作的学习与策略优化解耦，让代理专注于从减少的可行动作集合中选择最优动作，从而简化学习过程。实验结果表明，该方法显著提升了 DRL 在约束环境中的训练性能，尤其是在不完美的 feasibility models 下。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04327v1",
      "published_date": "2024-12-05 16:42:45 UTC",
      "updated_date": "2024-12-05 16:42:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:10:02.091900"
    },
    {
      "arxiv_id": "2412.04323v1",
      "title": "GRAM: Generalization in Deep RL with a Robust Adaptation Module",
      "title_zh": "GRAM：带有鲁棒适应模块的深度强化学习泛化",
      "authors": [
        "James Queeney",
        "Xiaoyi Cai",
        "Mouhacine Benosman",
        "Jonathan P. How"
      ],
      "abstract": "The reliable deployment of deep reinforcement learning in real-world settings\nrequires the ability to generalize across a variety of conditions, including\nboth in-distribution scenarios seen during training as well as novel\nout-of-distribution scenarios. In this work, we present a framework for\ndynamics generalization in deep reinforcement learning that unifies these two\ndistinct types of generalization within a single architecture. We introduce a\nrobust adaptation module that provides a mechanism for identifying and reacting\nto both in-distribution and out-of-distribution environment dynamics, along\nwith a joint training pipeline that combines the goals of in-distribution\nadaptation and out-of-distribution robustness. Our algorithm GRAM achieves\nstrong generalization performance across in-distribution and\nout-of-distribution scenarios upon deployment, which we demonstrate on a\nvariety of realistic simulated locomotion tasks with a quadruped robot.",
      "tldr_zh": "该论文提出GRAM框架，用于提升深度强化学习（deep reinforcement learning）在真实世界中的泛化能力，处理分布内（in-distribution）和分布外（out-of-distribution）场景。该框架引入鲁棒适应模块（robust adaptation module），用于识别和响应环境动态变化，并结合联合训练管道（joint training pipeline）来实现分布内适应和分布外鲁棒性的统一目标。通过在各种模拟四足机器人运动任务上的实验，GRAM展示了显著的泛化性能，证明了其在实际部署中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04323v1",
      "published_date": "2024-12-05 16:39:01 UTC",
      "updated_date": "2024-12-05 16:39:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:10:13.689284"
    },
    {
      "arxiv_id": "2412.04318v2",
      "title": "The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Fredrik Carlsson",
        "Fangyu Liu",
        "Daniel Ward",
        "Murathan Kurfali",
        "Joakim Nivre"
      ],
      "abstract": "This paper introduces the counter-intuitive generalization results of\noverfitting pre-trained large language models (LLMs) on very small datasets. In\nthe setting of open-ended text generation, it is well-documented that LLMs tend\nto generate repetitive and dull sequences, a phenomenon that is especially\napparent when generating using greedy decoding. This issue persists even with\nstate-of-the-art LLMs containing billions of parameters, trained via next-token\nprediction on large datasets. We find that by further fine-tuning these models\nto achieve a near-zero training loss on a small set of samples -- a process we\nrefer to as hyperfitting -- the long-sequence generative capabilities are\ngreatly enhanced. Greedy decoding with these Hyperfitted models even outperform\nTop-P sampling over long-sequences, both in terms of diversity and human\npreferences. This phenomenon extends to LLMs of various sizes, different\ndomains, and even autoregressive image generation. We further find this\nphenomena to be distinctly different from that of Grokking and double descent.\nSurprisingly, our experiments indicate that hyperfitted models rarely fall into\nrepeating sequences they were trained on, and even explicitly blocking these\nsequences results in high-quality output. All hyperfitted models produce\nextremely low-entropy predictions, often allocating nearly all probability to a\nsingle token.",
      "tldr_zh": "本论文揭示了在小数据集上过度拟合预训练大型语言模型(LLMs)的反直觉泛化现象，称为hyperfitting，这种方法通过使训练损失接近零来提升LLMs在开放式文本生成中的性能。实验结果显示，hyperfitted模型在使用greedy decoding时比Top-P sampling更出色，不仅提高了生成序列的多样性和人类偏好，还适用于不同规模的LLMs、各种领域，甚至自回归图像生成。该现象与Grokking和double descent不同，且hyperfitted模型很少重复训练序列，同时产生极低熵的预测，确保高质量输出。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review at ICLR",
      "pdf_url": "http://arxiv.org/pdf/2412.04318v2",
      "published_date": "2024-12-05 16:34:20 UTC",
      "updated_date": "2025-02-26 17:51:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:10:28.814279"
    },
    {
      "arxiv_id": "2412.04315v2",
      "title": "Densing Law of LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Chaojun Xiao",
        "Jie Cai",
        "Weilin Zhao",
        "Guoyang Zeng",
        "Biyuan Lin",
        "Jie Zhou",
        "Zhi Zheng",
        "Xu Han",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as a milestone in artificial\nintelligence, and their performance can improve as the model size increases.\nHowever, this scaling brings great challenges to training and inference\nefficiency, particularly for deploying LLMs in resource-constrained\nenvironments, and the scaling trend is becoming increasingly unsustainable.\nThis paper introduces the concept of ``\\textit{capacity density}'' as a new\nmetric to evaluate the quality of the LLMs across different scales and\ndescribes the trend of LLMs in terms of both effectiveness and efficiency. To\ncalculate the capacity density of a given target LLM, we first introduce a set\nof reference models and develop a scaling law to predict the downstream\nperformance of these reference models based on their parameter sizes. We then\ndefine the \\textit{effective parameter size} of the target LLM as the parameter\nsize required by a reference model to achieve equivalent performance, and\nformalize the capacity density as the ratio of the effective parameter size to\nthe actual parameter size of the target LLM. Capacity density provides a\nunified framework for assessing both model effectiveness and efficiency. Our\nfurther analysis of recent open-source base LLMs reveals an empirical law (the\ndensing law)that the capacity density of LLMs grows exponentially over time.\nMore specifically, using some widely used benchmarks for evaluation, the\ncapacity density of LLMs doubles approximately every three months. The law\nprovides new perspectives to guide future LLM development, emphasizing the\nimportance of improving capacity density to achieve optimal results with\nminimal computational overhead.",
      "tldr_zh": "这篇论文引入了“capacity density”作为评估大型语言模型(LLMs)的全新指标，以平衡模型的有效性和效率。通过使用参考模型的“scaling law”预测性能，计算目标LLMs的“有效参数大小”，并将其与实际参数大小的比率定义为capacity density。研究发现，“densing law”表明LLMs的capacity density呈指数增长，大约每三个月翻倍。这一发现为未来LLMs开发提供新指导，强调提高capacity density以最小化计算开销。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04315v2",
      "published_date": "2024-12-05 16:31:13 UTC",
      "updated_date": "2024-12-06 11:39:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:12:38.836238"
    },
    {
      "arxiv_id": "2412.04300v2",
      "title": "T2I-FactualBench: Benchmarking the Factuality of Text-to-Image Models with Knowledge-Intensive Concepts",
      "title_zh": "翻译失败",
      "authors": [
        "Ziwei Huang",
        "Wanggui He",
        "Quanyu Long",
        "Yandi Wang",
        "Haoyuan Li",
        "Zhelun Yu",
        "Fangxun Shu",
        "Long Chan",
        "Hao Jiang",
        "Leilei Gan",
        "Fei Wu"
      ],
      "abstract": "Evaluating the quality of synthesized images remains a significant challenge\nin the development of text-to-image (T2I) generation. Most existing studies in\nthis area primarily focus on evaluating text-image alignment, image quality,\nand object composition capabilities, with comparatively fewer studies\naddressing the evaluation of the factuality of T2I models, particularly when\nthe concepts involved are knowledge-intensive. To mitigate this gap, we present\nT2I-FactualBench in this work - the largest benchmark to date in terms of the\nnumber of concepts and prompts specifically designed to evaluate the factuality\nof knowledge-intensive concept generation. T2I-FactualBench consists of a\nthree-tiered knowledge-intensive text-to-image generation framework, ranging\nfrom the basic memorization of individual knowledge concepts to the more\ncomplex composition of multiple knowledge concepts. We further introduce a\nmulti-round visual question answering (VQA) based evaluation framework to\nassess the factuality of three-tiered knowledge-intensive text-to-image\ngeneration tasks. Experiments on T2I-FactualBench indicate that current\nstate-of-the-art (SOTA) T2I models still leave significant room for\nimprovement.",
      "tldr_zh": "本文提出 T2I-FactualBench，这是一个针对文本到图像 (T2I) 模型真实性的最大基准测试，专注于知识密集型概念的生成评估，以填补现有研究的空白。该基准设计了三个层级的框架，从单个知识概念的记忆到多个概念的复杂组合，并引入多轮视觉问答 (VQA) 评估方法来量化生成任务的真实性。实验结果表明，当前最先进 (SOTA) T2I 模型在处理知识密集型概念时仍存在显著改进空间。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04300v2",
      "published_date": "2024-12-05 16:21:01 UTC",
      "updated_date": "2024-12-07 17:25:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:12:51.286114"
    },
    {
      "arxiv_id": "2412.04292v2",
      "title": "SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenglin Huang",
        "Jinwei Hu",
        "Xiangtai Li",
        "Yiwei He",
        "Xingyu Zhao",
        "Bei Peng",
        "Baoyuan Wu",
        "Xiaowei Huang",
        "Guangliang Cheng"
      ],
      "abstract": "The rapid advancement of generative models in creating highly realistic\nimages poses substantial risks for misinformation dissemination. For instance,\na synthetic image, when shared on social media, can mislead extensive audiences\nand erode trust in digital content, resulting in severe repercussions. Despite\nsome progress, academia has not yet created a large and diversified deepfake\ndetection dataset for social media, nor has it devised an effective solution to\naddress this issue. In this paper, we introduce the Social media Image\nDetection dataSet (SID-Set), which offers three key advantages: (1) extensive\nvolume, featuring 300K AI-generated/tampered and authentic images with\ncomprehensive annotations, (2) broad diversity, encompassing fully synthetic\nand tampered images across various classes, and (3) elevated realism, with\nimages that are predominantly indistinguishable from genuine ones through mere\nvisual inspection. Furthermore, leveraging the exceptional capabilities of\nlarge multimodal models, we propose a new image deepfake detection,\nlocalization, and explanation framework, named SIDA (Social media Image\nDetection, localization, and explanation Assistant). SIDA not only discerns the\nauthenticity of images, but also delineates tampered regions through mask\nprediction and provides textual explanations of the model's judgment criteria.\nCompared with state-of-the-art deepfake detection models on SID-Set and other\nbenchmarks, extensive experiments demonstrate that SIDA achieves superior\nperformance among diversified settings. The code, model, and dataset will be\nreleased.",
      "tldr_zh": "本研究针对生成模型导致的社交媒体图像深度伪造（deepfake）问题，引入了一个大规模数据集 SID-Set，该数据集包含30万张AI生成/篡改和真实图像，具有高容量、多样性和高真实性优势。研究提出SIDA框架，利用Large Multimodal Model实现图像的检测、定位和解释功能，不仅判断图像真实性，还通过掩码预测（mask prediction）标识篡改区域并提供文本解释。与现有最先进模型相比，SIDA在SID-Set和其他基准上表现出色，性能提升显著。代码、模型和数据集将公开发布。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR-2025",
      "pdf_url": "http://arxiv.org/pdf/2412.04292v2",
      "published_date": "2024-12-05 16:12:25 UTC",
      "updated_date": "2025-03-10 11:03:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:11:03.389779"
    },
    {
      "arxiv_id": "2412.10400v3",
      "title": "Reinforcement Learning Enhanced LLMs: A Survey",
      "title_zh": "翻译失败",
      "authors": [
        "Shuhe Wang",
        "Shengyu Zhang",
        "Jie Zhang",
        "Runyi Hu",
        "Xiaoya Li",
        "Tianwei Zhang",
        "Jiwei Li",
        "Fei Wu",
        "Guoyin Wang",
        "Eduard Hovy"
      ],
      "abstract": "Reinforcement learning (RL) enhanced large language models (LLMs),\nparticularly exemplified by DeepSeek-R1, have exhibited outstanding\nperformance. Despite the effectiveness in improving LLM capabilities, its\nimplementation remains highly complex, requiring complex algorithms, reward\nmodeling strategies, and optimization techniques. This complexity poses\nchallenges for researchers and practitioners in developing a systematic\nunderstanding of RL-enhanced LLMs. Moreover, the absence of a comprehensive\nsurvey summarizing existing research on RL-enhanced LLMs has limited progress\nin this domain, hindering further advancements.\n  In this work, we are going to make a systematic review of the most up-to-date\nstate of knowledge on RL-enhanced LLMs, attempting to consolidate and analyze\nthe rapidly growing research in this field, helping researchers understand the\ncurrent challenges and advancements. Specifically, we (1) detail the basics of\nRL; (2) introduce popular RL-enhanced LLMs; (3) review researches on two\nwidely-used reward model-based RL techniques: Reinforcement Learning from Human\nFeedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF); and (4)\nexplore Direct Preference Optimization (DPO), a set of methods that bypass the\nreward model to directly use human preference data for aligning LLM outputs\nwith human expectations. We will also point out current challenges and\ndeficiencies of existing methods and suggest some avenues for further\nimprovements. Project page of this work can be found at\nhttps://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey.",
      "tldr_zh": "本调查对强化学习（RL）增强的大型语言模型（LLMs）进行了系统回顾，旨在解决其实现复杂性带来的挑战，如算法设计、奖励建模和优化技术。论文详细介绍了RL的基础、流行RL增强LLMs模型，并分析了基于奖励模型的RL技术，包括Reinforcement Learning from Human Feedback (RLHF)和Reinforcement Learning from AI Feedback (RLAIF)。此外，它探讨了Direct Preference Optimization (DPO)方法，该方法绕过奖励模型直接利用人类偏好数据对LLMs输出进行对齐，并指出了现有方法的不足及未来改进方向。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.10400v3",
      "published_date": "2024-12-05 16:10:42 UTC",
      "updated_date": "2025-02-24 08:57:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:11:18.130859"
    },
    {
      "arxiv_id": "2412.04272v3",
      "title": "PoTable: Towards Systematic Thinking via Stage-oriented Plan-then-Execute Reasoning on Tables",
      "title_zh": "翻译失败",
      "authors": [
        "Qingyang Mao",
        "Qi Liu",
        "Zhi Li",
        "Mingyue Cheng",
        "Zheng Zhang",
        "Rui Li"
      ],
      "abstract": "In recent years, table reasoning has garnered substantial research interest,\nparticularly its integration with Large Language Models (LLMs) which\nrevolutionize natural language applications. Existing typical LLM-based studies\nrealize step-by-step reasoning, promoting the capabilities in table\nunderstanding and analyzing. While these approaches emphasize autonomous\nexploration to accomplish the task objective, they overlook systematic thinking\nin the reasoning process, leading to potential risks of omitted steps,\ndisorganized logic and misleading results. In this paper, we propose PoTable, a\nnovel stage-oriented plan-then-execute reasoning approach that achieves\nsystematic thinking on tables. Specifically, PoTable deploys several distinct\ntabular analytical stages with clear objectives and achieves stage-by-stage\nreasoning. To accomplish the stage-specific goal, PoTable conducts\nplan-then-execute reasoning, which first plans the operation chain under the\nstage objective, and then executes each operation sequentially through code\ngeneration, real-time running and feedback processing. As a result, PoTable can\nproduce reliable table reasoning results with highly accurate, steply commented\nand completely executable programs. It possesses a high degree of alignment\nwith a distinguished tabular data analyst, offering advantages of high accuracy\nand explainability. Finally, we conduct extensive experiments over four\nevaluation datasets from WikiTQ and TabFact benchmarks, where the results\ndemonstrate the effectiveness of PoTable, as well as the efficiency and\nexplainability.",
      "tldr_zh": "该研究针对现有基于大型语言模型（LLMs）的表结构推理方法存在的系统性思考缺失问题（如步骤遗漏和逻辑混乱），提出了一种新型框架PoTable。PoTable采用阶段导向的plan-then-execute推理方法，将表分析分为多个明确目标的阶段，并在每个阶段先规划操作链，然后通过代码生成、实时执行和反馈处理来顺序操作。这种方法生成高准确、可解释且完全可执行的程序，与专业表格数据分析师高度一致。实验在WikiTQ和TabFact基准的四个数据集上验证了PoTable的有效性、效率和可解释性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "10 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.04272v3",
      "published_date": "2024-12-05 15:54:16 UTC",
      "updated_date": "2025-04-05 10:18:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:11:27.185656"
    },
    {
      "arxiv_id": "2412.04260v1",
      "title": "Enhancing Whole Slide Image Classification through Supervised Contrastive Domain Adaptation",
      "title_zh": "翻译失败",
      "authors": [
        "Ilán Carretero",
        "Pablo Meseguer",
        "Rocío del Amor",
        "Valery Naranjo"
      ],
      "abstract": "Domain shift in the field of histopathological imaging is a common phenomenon\ndue to the intra- and inter-hospital variability of staining and digitization\nprotocols. The implementation of robust models, capable of creating generalized\ndomains, represents a need to be solved. In this work, a new domain adaptation\nmethod to deal with the variability between histopathological images from\nmultiple centers is presented. In particular, our method adds a training\nconstraint to the supervised contrastive learning approach to achieve domain\nadaptation and improve inter-class separability. Experiments performed on\ndomain adaptation and classification of whole-slide images of six skin cancer\nsubtypes from two centers demonstrate the method's usefulness. The results\nreflect superior performance compared to not using domain adaptation after\nfeature extraction or staining normalization.",
      "tldr_zh": "该论文针对组织病理图像中的domain shift问题（如医院间染色和数字化协议的变异性），提出了一种新的domain adaptation方法。该方法在supervised contrastive learning的基础上添加训练约束，以实现领域适应并改善inter-class separability，从而提升whole-slide images的分类性能。实验在两个中心的六个皮肤癌亚型图像上进行，结果显示，该方法比不使用domain adaptation或染色归一化的基线模型表现出色，证明了其在泛化领域构建方面的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in CASEIB 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.04260v1",
      "published_date": "2024-12-05 15:39:54 UTC",
      "updated_date": "2024-12-05 15:39:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:11:39.362166"
    },
    {
      "arxiv_id": "2412.04256v1",
      "title": "Transient Multi-Agent Path Finding for Lifelong Navigation in Dense Environments",
      "title_zh": "翻译失败",
      "authors": [
        "Jonathan Morag",
        "Noy Gabay",
        "Daniel koyfman",
        "Roni Stern"
      ],
      "abstract": "Multi-Agent Path Finding (MAPF) deals with finding conflict-free paths for a\nset of agents from an initial configuration to a given target configuration.\nThe Lifelong MAPF (LMAPF) problem is a well-studied online version of MAPF in\nwhich an agent receives a new target when it reaches its current target. The\ncommon approach for solving LMAPF is to treat it as a sequence of MAPF\nproblems, periodically replanning from the agents' current configurations to\ntheir current targets. A significant drawback in this approach is that in MAPF\nthe agents must reach a configuration in which all agents are at their targets\nsimultaneously, which is needlessly restrictive for LMAPF. Techniques have been\nproposed to indirectly mitigate this drawback. We describe cases where these\nmitigation techniques fail. As an alternative, we propose to solve LMAPF\nproblems by solving a sequence of modified MAPF problems, in which the\nobjective is for each agent to eventually visit its target, but not necessarily\nfor all agents to do so simultaneously. We refer to this MAPF variant as\nTransient MAPF (TMAPF) and propose several algorithms for solving it based on\nexisting MAPF algorithms. A limited experimental evaluation identifies some\ncases where using a TMAPF algorithm instead of a MAPF algorithm with an LMAPF\nframework can improve the system throughput significantly.",
      "tldr_zh": "这篇论文针对 Lifelong Multi-Agent Path Finding (LMAPF) 问题，指出传统方法将 LMAPF 视为一系列标准 MAPF 问题，导致代理必须同时到达目标，从而在密集环境中效率低下。作者提出 Transient MAPF (TMAPF) 作为改进方案，该方法允许每个代理逐个访问其目标，而非强制同步到达，并基于现有 MAPF 算法开发了多种 TMAPF 算法。实验结果显示，在某些场景下，使用 TMAPF 可以显著提升系统吞吐量。",
      "categories": [
        "cs.MA",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.MA",
      "comment": "Submitted to The 35th International Conference on Automated Planning\n  and Scheduling (ICAPS 2025)",
      "pdf_url": "http://arxiv.org/pdf/2412.04256v1",
      "published_date": "2024-12-05 15:37:29 UTC",
      "updated_date": "2024-12-05 15:37:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:13:06.728060"
    },
    {
      "arxiv_id": "2412.04254v1",
      "title": "CLINICSUM: Utilizing Language Models for Generating Clinical Summaries from Patient-Doctor Conversations",
      "title_zh": "CLINICSUM：利用语言模型从患者-医生对话中生成临床总结",
      "authors": [
        "Subash Neupane",
        "Himanshu Tripathi",
        "Shaswata Mitra",
        "Sean Bozorgzad",
        "Sudip Mittal",
        "Shahram Rahimi",
        "Amin Amirlatifi"
      ],
      "abstract": "This paper presents ClinicSum, a novel framework designed to automatically\ngenerate clinical summaries from patient-doctor conversations. It utilizes a\ntwo-module architecture: a retrieval-based filtering module that extracts\nSubjective, Objective, Assessment, and Plan (SOAP) information from\nconversation transcripts, and an inference module powered by fine-tuned\nPre-trained Language Models (PLMs), which leverage the extracted SOAP data to\ngenerate abstracted clinical summaries. To fine-tune the PLM, we created a\ntraining dataset of consisting 1,473 conversations-summaries pair by\nconsolidating two publicly available datasets, FigShare and MTS-Dialog, with\nground truth summaries validated by Subject Matter Experts (SMEs). ClinicSum's\neffectiveness is evaluated through both automatic metrics (e.g., ROUGE,\nBERTScore) and expert human assessments. Results show that ClinicSum\noutperforms state-of-the-art PLMs, demonstrating superior precision, recall,\nand F-1 scores in automatic evaluations and receiving high preference from SMEs\nin human assessment, making it a robust solution for automated clinical\nsummarization.",
      "tldr_zh": "这篇论文介绍了 ClinicSum 框架，一种利用语言模型从患者-医生对话中自动生成临床摘要的方法。该框架采用两模块架构：一个基于检索的过滤模块提取 SOAP（Subjective, Objective, Assessment, and Plan）信息，另一个微调的 Pre-trained Language Models (PLMs) 推理模块利用这些信息生成抽象摘要。为了训练模型，作者创建了包含 1,473 对对话-摘要的数据集，并通过 Subject Matter Experts (SMEs) 验证。实验结果显示，ClinicSum 在 ROUGE 和 BERTScore 等自动指标上，以及专家人工评估中，均优于现有最先进模型，提供了一个可靠的临床总结解决方案。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "accepted at the the 2024 IEEE International Conference on Big Data\n  workshop Workshop on Big Data and AI for Healthcare",
      "pdf_url": "http://arxiv.org/pdf/2412.04254v1",
      "published_date": "2024-12-05 15:34:02 UTC",
      "updated_date": "2024-12-05 15:34:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:13:19.139207"
    },
    {
      "arxiv_id": "2412.04234v3",
      "title": "DEIM: DETR with Improved Matching for Fast Convergence",
      "title_zh": "DEIM：DETR 改进匹配用于快速收敛",
      "authors": [
        "Shihua Huang",
        "Zhichao Lu",
        "Xiaodong Cun",
        "Yongjun Yu",
        "Xiao Zhou",
        "Xi Shen"
      ],
      "abstract": "We introduce DEIM, an innovative and efficient training framework designed to\naccelerate convergence in real-time object detection with Transformer-based\narchitectures (DETR). To mitigate the sparse supervision inherent in one-to-one\n(O2O) matching in DETR models, DEIM employs a Dense O2O matching strategy. This\napproach increases the number of positive samples per image by incorporating\nadditional targets, using standard data augmentation techniques. While Dense\nO2O matching speeds up convergence, it also introduces numerous low-quality\nmatches that could affect performance. To address this, we propose the\nMatchability-Aware Loss (MAL), a novel loss function that optimizes matches\nacross various quality levels, enhancing the effectiveness of Dense O2O.\nExtensive experiments on the COCO dataset validate the efficacy of DEIM. When\nintegrated with RT-DETR and D-FINE, it consistently boosts performance while\nreducing training time by 50%. Notably, paired with RT-DETRv2, DEIM achieves\n53.2% AP in a single day of training on an NVIDIA 4090 GPU. Additionally,\nDEIM-trained real-time models outperform leading real-time object detectors,\nwith DEIM-D-FINE-L and DEIM-D-FINE-X achieving 54.7% and 56.5% AP at 124 and 78\nFPS on an NVIDIA T4 GPU, respectively, without the need for additional data. We\nbelieve DEIM sets a new baseline for advancements in real-time object\ndetection. Our code and pre-trained models are available at\nhttps://github.com/ShihuaHuang95/DEIM.",
      "tldr_zh": "本论文提出 DEIM，一种创新的训练框架，用于加速基于 Transformer 的实时物体检测模型（如 DETR）的收敛，通过采用 Dense O2O 匹配策略来增加每张图像的正样本数量，并利用标准数据增强技术缓解监督稀疏问题。针对 Dense O2O 可能引入的低质量匹配，DEIM 引入了新的 Matchability-Aware Loss (MAL) 损失函数，以优化不同质量水平的匹配。实验在 COCO 数据集上验证了 DEIM 的有效性，与 RT-DETR 和 D-FINE 结合后，性能提升同时训练时间减少 50%，例如与 RT-DETRv2 结合在 NVIDIA 4090 GPU 上训练一天即可达到 53.2% AP；此外，DEIM-D-FINE-L 和 DEIM-D-FINE-X 模型分别在 NVIDIA T4 GPU 上实现 54.7% AP at 124 FPS 和 56.5% AP at 78 FPS，超越现有实时检测器基准。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.04234v3",
      "published_date": "2024-12-05 15:10:13 UTC",
      "updated_date": "2025-03-26 10:41:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:13:34.600699"
    },
    {
      "arxiv_id": "2412.04233v2",
      "title": "HyperMARL: Adaptive Hypernetworks for Multi-Agent RL",
      "title_zh": "HyperMARL：自适应",
      "authors": [
        "Kale-ab Abebe Tessera",
        "Arrasy Rahman",
        "Stefano V. Albrecht"
      ],
      "abstract": "Adaptability is critical in cooperative multi-agent reinforcement learning\n(MARL), where agents must learn specialised or homogeneous behaviours for\ndiverse tasks. While parameter sharing methods are sample-efficient, they often\nencounter gradient interference among agents, limiting their behavioural\ndiversity. Conversely, non-parameter sharing approaches enable specialisation,\nbut are computationally demanding and sample-inefficient. To address these\nissues, we propose HyperMARL, a parameter sharing approach that uses\nhypernetworks to dynamically generate agent-specific actor and critic\nparameters, without altering the learning objective or requiring preset\ndiversity levels. By decoupling observation- and agent-conditioned gradients,\nHyperMARL empirically reduces policy gradient variance and facilitates\nspecialisation within FuPS, suggesting it can mitigate cross-agent\ninterference. Across multiple MARL benchmarks involving up to twenty agents --\nand requiring homogeneous, heterogeneous, or mixed behaviours -- HyperMARL\nconsistently performs competitively with fully shared, non-parameter-sharing,\nand diversity-promoting baselines, all while preserving a behavioural diversity\nlevel comparable to non-parameter sharing. These findings establish\nhypernetworks as a versatile approach for MARL across diverse environments.",
      "tldr_zh": "在多智能体强化学习(MARL)中，HyperMARL 提出了一种自适应 hypernetworks 方法，通过动态生成代理特定的 actor 和 critic 参数，实现参数共享的同时减少梯度干扰和策略梯度方差。该方法解耦观察和代理条件梯度，促进代理行为的专业化，而无需改变学习目标或预设多样性水平。在多个 MARL 基准测试中，涉及多达 20 个代理的同质、异质或混合场景，HyperMARL 与完全共享、非参数共享和多样性促进基线相比表现出色，并保持与非参数共享类似的行为多样性，展示了 hypernetworks 在不同环境中的通用适用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04233v2",
      "published_date": "2024-12-05 15:09:51 UTC",
      "updated_date": "2025-02-07 11:46:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:13:43.477743"
    },
    {
      "arxiv_id": "2412.04220v1",
      "title": "Customize Segment Anything Model for Multi-Modal Semantic Segmentation with Mixture of LoRA Experts",
      "title_zh": "翻译失败",
      "authors": [
        "Chenyang Zhu",
        "Bin Xiao",
        "Lin Shi",
        "Shoukun Xu",
        "Xu Zheng"
      ],
      "abstract": "The recent Segment Anything Model (SAM) represents a significant breakthrough\nin scaling segmentation models, delivering strong performance across various\ndownstream applications in the RGB modality. However, directly applying SAM to\nemerging visual modalities, such as depth and event data results in suboptimal\nperformance in multi-modal segmentation tasks. In this paper, we make the first\nattempt to adapt SAM for multi-modal semantic segmentation by proposing a\nMixture of Low-Rank Adaptation Experts (MoE-LoRA) tailored for different input\nvisual modalities. By training only the MoE-LoRA layers while keeping SAM's\nweights frozen, SAM's strong generalization and segmentation capabilities can\nbe preserved for downstream tasks. Specifically, to address cross-modal\ninconsistencies, we propose a novel MoE routing strategy that adaptively\ngenerates weighted features across modalities, enhancing multi-modal feature\nintegration. Additionally, we incorporate multi-scale feature extraction and\nfusion by adapting SAM's segmentation head and introducing an auxiliary\nsegmentation head to combine multi-scale features for improved segmentation\nperformance effectively. Extensive experiments were conducted on three\nmulti-modal benchmarks: DELIVER, MUSES, and MCubeS. The results consistently\ndemonstrate that the proposed method significantly outperforms state-of-the-art\napproaches across diverse scenarios. Notably, under the particularly\nchallenging condition of missing modalities, our approach exhibits a\nsubstantial performance gain, achieving an improvement of 32.15% compared to\nexisting methods.",
      "tldr_zh": "本研究针对Segment Anything Model (SAM) 在多模态语义分割任务中的性能不足，首次提出一种定制化方法，使用Mixture of Low-Rank Adaptation Experts (MoE-LoRA) 来适应不同视觉模态（如深度和事件数据）。该方法仅训练MoE-LoRA 层以保持SAM 的泛化和分割能力，并引入新型MoE 路由策略来处理跨模态不一致性，通过自适应加权特征增强多模态特征整合，同时添加多尺度特征提取和融合机制以优化分割性能。实验在DELIVER、MUSES 和MCubeS 等基准上显示，该方法显著优于现有技术，尤其在模态缺失场景下，性能提升32.15%。这为多模态分割任务提供了高效、可扩展的解决方案。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04220v1",
      "published_date": "2024-12-05 14:54:31 UTC",
      "updated_date": "2024-12-05 14:54:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:13:53.864914"
    },
    {
      "arxiv_id": "2412.04202v1",
      "title": "Relationships between Keywords and Strong Beats in Lyrical Music",
      "title_zh": "抒情音乐中关键词与强拍的关系",
      "authors": [
        "Callie C. Liao",
        "Duoduo Liao",
        "Ellie L. Zhang"
      ],
      "abstract": "Artificial Intelligence (AI) song generation has emerged as a popular topic,\nyet the focus on exploring the latent correlations between specific lyrical and\nrhythmic features remains limited. In contrast, this pilot study particularly\ninvestigates the relationships between keywords and rhythmically stressed\nfeatures such as strong beats in songs. It focuses on several key elements:\nkeywords or non-keywords, stressed or unstressed syllables, and strong or weak\nbeats, with the aim of uncovering insightful correlations. Experimental results\nindicate that, on average, 80.8\\% of keywords land on strong beats, whereas\n62\\% of non-keywords fall on weak beats. The relationship between stressed\nsyllables and strong or weak beats is weak, revealing that keywords have the\nstrongest relationships with strong beats. Additionally, the lyrics-rhythm\nmatching score, a key matching metric measuring keywords on strong beats and\nnon-keywords on weak beats across various time signatures, is 0.765, while the\nmatching score for syllable types is 0.495. This study demonstrates that word\ntypes strongly align with their corresponding beat types, as evidenced by the\ndistinct patterns, whereas syllable types exhibit a much weaker alignment. This\ndisparity underscores the greater reliability of word types in capturing\nrhythmic structures in music, highlighting their crucial role in effective\nrhythmic matching and analysis. We also conclude that keywords that\nconsistently align with strong beats are more reliable indicators of\nlyrics-rhythm associations, providing valuable insights for AI-driven song\ngeneration through enhanced structural analysis. Furthermore, our development\nof tailored Lyrics-Rhythm Matching (LRM) metrics maximizes lyrical alignments\nwith corresponding beat stresses, and our novel LRM file format captures\ncritical lyrical and rhythmic information without needing original sheet music.",
      "tldr_zh": "这篇论文探讨了歌词中关键词与节奏强拍之间的关系，通过分析关键词/非关键词、受重音音节/非受重音音节以及强拍/弱拍等元素。实验结果显示，80.8%的关键词落在强拍上，而62%的非关键词落在弱拍上，且关键词与强拍的关联性最强。研究开发了Lyrics-Rhythm Matching (LRM)指标，其匹配分数为0.765（针对关键词），远高于音节类型的0.495，这强调了词类型在捕捉音乐节奏结构中的可靠性，并为AI驱动的歌曲生成提供重要洞见。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted by IEEE BigData 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.04202v1",
      "published_date": "2024-12-05 14:40:27 UTC",
      "updated_date": "2024-12-05 14:40:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:16:06.333006"
    },
    {
      "arxiv_id": "2412.04190v1",
      "title": "Directed Structural Adaptation to Overcome Statistical Conflicts and Enable Continual Learning",
      "title_zh": "定向结构适应以克服统计冲突并实现持续",
      "authors": [
        "Zeki Doruk Erden",
        "Boi Faltings"
      ],
      "abstract": "Adaptive networks today rely on overparameterized fixed topologies that\ncannot break through the statistical conflicts they encounter in the data they\nare exposed to, and are prone to \"catastrophic forgetting\" as the network\nattempts to reuse the existing structures to learn new task. We propose a\nstructural adaptation method, DIRAD, that can complexify as needed and in a\ndirected manner without being limited by statistical conflicts within a\ndataset. We then extend this method and present the PREVAL framework, designed\nto prevent \"catastrophic forgetting\" in continual learning by detection of new\ndata and assigning encountered data to suitable models adapted to process them,\nwithout needing task labels anywhere in the workflow. We show the reliability\nof the DIRAD in growing a network with high performance and orders-of-magnitude\nsimpler than fixed topology networks; and demonstrate the proof-of-concept\noperation of PREVAL, in which continual adaptation to new tasks is observed\nwhile being able to detect and discern previously-encountered tasks.",
      "tldr_zh": "该论文针对自适应网络在数据统计冲突和“catastrophic forgetting”问题上存在的局限性，提出DIRAD方法，通过定向结构适应实现网络的按需复杂化，从而克服这些冲突。DIRAD扩展为PREVAL框架，该框架在持续学习中通过检测新数据并将其分配给合适的模型处理，而无需任务标签，从而防止灾难性遗忘。实验结果表明，DIRAD能使网络性能高且比固定拓扑网络简单几个数量级，PREVAL框架成功实现了对新任务的持续适应和对旧任务的检测与区分。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Presented in Deployable AI (DAI) workshop at AAAI-2024",
      "pdf_url": "http://arxiv.org/pdf/2412.04190v1",
      "published_date": "2024-12-05 14:30:18 UTC",
      "updated_date": "2024-12-05 14:30:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:14:17.875769"
    },
    {
      "arxiv_id": "2412.04185v1",
      "title": "Leveraging Large Language Models to Generate Course-specific Semantically Annotated Learning Objects",
      "title_zh": "利用大语言模型生成特定课程的语义标注学习对象",
      "authors": [
        "Dominic Lohr",
        "Marc Berges",
        "Abhishek Chugh",
        "Michael Kohlhase",
        "Dennis Müller"
      ],
      "abstract": "Background: Over the past few decades, the process and methodology of\nautomated question generation (AQG) have undergone significant transformations.\nRecent progress in generative natural language models has opened up new\npotential in the generation of educational content.\n  Objectives: This paper explores the potential of large language models (LLMs)\nfor generating computer science questions that are sufficiently annotated for\nautomatic learner model updates, are fully situated in the context of a\nparticular course, and address the cognitive dimension understand.\n  Methods: Unlike previous attempts that might use basic methods like ChatGPT,\nour approach involves more targeted strategies such as retrieval-augmented\ngeneration (RAG) to produce contextually relevant and pedagogically meaningful\nlearning objects.\n  Results and Conclusions: Our results show that generating structural,\nsemantic annotations works well. However, this success was not reflected in the\ncase of relational annotations. The quality of the generated questions often\ndid not meet educational standards, highlighting that although LLMs can\ncontribute to the pool of learning materials, their current level of\nperformance requires significant human intervention to refine and validate the\ngenerated content.",
      "tldr_zh": "本论文探讨利用Large Language Models (LLMs)生成特定课程的语义标注学习对象，旨在为计算机科学教育创建上下文相关的自动问题（AQG），并针对认知维度“理解”进行优化。研究方法采用retrieval-augmented generation (RAG)等针对性策略，以提升生成内容的教育意义和相关性。结果表明，结构和语义标注生成效果良好，但关系标注表现不佳，且生成的题目质量往往未达教育标准，因此需要显著的人工干预来完善内容。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at Journal of Computer Assisted Learning (2024)",
      "pdf_url": "http://arxiv.org/pdf/2412.04185v1",
      "published_date": "2024-12-05 14:24:07 UTC",
      "updated_date": "2024-12-05 14:24:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:14:29.866925"
    },
    {
      "arxiv_id": "2412.04167v1",
      "title": "Bench-CoE: a Framework for Collaboration of Experts from Benchmark",
      "title_zh": "翻译失败",
      "authors": [
        "Yuanshuai Wang",
        "Xingjian Zhang",
        "Jinkun Zhao",
        "Siwei Wen",
        "Peilin Feng",
        "Shuhao Liao",
        "Lei Huang",
        "Wenjun Wu"
      ],
      "abstract": "Large Language Models (LLMs) are key technologies driving intelligent systems\nto handle multiple tasks. To meet the demands of various tasks, an increasing\nnumber of LLMs-driven experts with diverse capabilities have been developed,\naccompanied by corresponding benchmarks to evaluate their performance. This\npaper proposes the Bench-CoE framework, which enables Collaboration of Experts\n(CoE) by effectively leveraging benchmark evaluations to achieve optimal\nperformance across various tasks. Bench-CoE includes a set of expert models, a\nrouter for assigning tasks to corresponding experts, and a benchmark dataset\nfor training the router. Moreover, we formulate Query-Level and Subject-Level\napproaches based on our framework, and analyze the merits and drawbacks of\nthese two approaches. Finally, we conduct a series of experiments with vary\ndata distributions on both language and multimodal tasks to validate that our\nproposed Bench-CoE outperforms any single model in terms of overall\nperformance. We hope this method serves as a baseline for further research in\nthis area. The code is available at\n\\url{https://github.com/ZhangXJ199/Bench-CoE}.",
      "tldr_zh": "该论文提出Bench-CoE框架，用于实现Collaboration of Experts (CoE)，通过基准评估优化Large Language Models (LLMs)驱动的专家模型在多种任务中的性能。框架包括一组专家模型、一个路由器（用于任务分配）和一个基准数据集（用于训练路由器），并制定了Query-Level和Subject-Level两种方法来分析其优缺点。实验结果显示，在不同数据分布的语言和多模态任务上，Bench-CoE在整体性能上优于任何单一模型，并作为该领域的基线研究提供代码。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "The code is available at\n  \\url{https://github.com/ZhangXJ199/Bench-CoE}",
      "pdf_url": "http://arxiv.org/pdf/2412.04167v1",
      "published_date": "2024-12-05 14:03:41 UTC",
      "updated_date": "2024-12-05 14:03:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:16:34.703905"
    },
    {
      "arxiv_id": "2412.04149v2",
      "title": "Frequency-Adaptive Low-Latency Object Detection Using Events and Frames",
      "title_zh": "翻译失败",
      "authors": [
        "Haitian Zhang",
        "Xiangyuan Wang",
        "Chang Xu",
        "Xinya Wang",
        "Fang Xu",
        "Huai Yu",
        "Lei Yu",
        "Wen Yang"
      ],
      "abstract": "Fusing Events and RGB images for object detection leverages the robustness of\nEvent cameras in adverse environments and the rich semantic information\nprovided by RGB cameras. However, two critical mismatches: low-latency Events\n\\textit{vs.}~high-latency RGB frames; temporally sparse labels in training\n\\textit{vs.}~continuous flow in inference, significantly hinder the\nhigh-frequency fusion-based object detection. To address these challenges, we\npropose the \\textbf{F}requency-\\textbf{A}daptive Low-Latency \\textbf{O}bject\n\\textbf{D}etector (FAOD). FAOD aligns low-frequency RGB frames with\nhigh-frequency Events through an Align Module, which reinforces cross-modal\nstyle and spatial proximity to address the Event-RGB Mismatch. We further\npropose a training strategy, Time Shift, which enforces the module to align the\nprediction from temporally shifted Event-RGB pairs and their original\nrepresentation, that is, consistent with Event-aligned annotations. This\nstrategy enables the network to use high-frequency Event data as the primary\nreference while treating low-frequency RGB images as supplementary information,\nretaining the low-latency nature of the Event stream toward high-frequency\ndetection. Furthermore, we observe that these corrected Event-RGB pairs\ndemonstrate better generalization from low training frequency to higher\ninference frequencies compared to using Event data alone. Extensive experiments\non the PKU-DAVIS-SOD and DSEC-Detection datasets demonstrate that our FAOD\nachieves SOTA performance. Specifically, in the PKU-DAVIS-SOD Dataset, FAOD\nachieves 9.8 points improvement in terms of the mAP in fully paired Event-RGB\ndata with only a quarter of the parameters compared to SODFormer, and even\nmaintains robust performance (only a 3 points drop in mAP) under 80$\\times$\nEvent-RGB frequency mismatch.",
      "tldr_zh": "该论文提出了一种频率自适应低延迟物体检测器（FAOD），旨在解决融合 Events 和 RGB 图像进行物体检测时存在的低延迟 Events 与高延迟 RGB 帧的匹配问题，以及训练时稀疏标签与推理时连续流的矛盾。FAOD 通过 Align Module 强化跨模态风格和空间接近性来对齐低频 RGB 帧与高频 Events，并引入 Time Shift 训练策略，确保网络以高频 Events 为主要参考，将 RGB 图像作为补充信息，从而维持 Events 的低延迟特性。实验结果显示，FAOD 在 PKU-DAVIS-SOD 和 DSEC-Detection 数据集上达到 SOTA 性能，比 SODFormer 提高了 9.8 点的 mAP，且在 80 倍频率不匹配下仅下降 3 点 mAP，同时仅用了四分之一的参数。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04149v2",
      "published_date": "2024-12-05 13:23:06 UTC",
      "updated_date": "2025-02-27 12:43:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:14:55.781105"
    },
    {
      "arxiv_id": "2412.04144v3",
      "title": "If You Can't Use Them, Recycle Them: Optimizing Merging at Scale Mitigates Performance Tradeoffs",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Khalifa",
        "Yi-Chern Tan",
        "Arash Ahmadian",
        "Tom Hosking",
        "Honglak Lee",
        "Lu Wang",
        "Ahmet Üstün",
        "Tom Sherborne",
        "Matthias Gallé"
      ],
      "abstract": "Model merging has shown great promise at combining expert models, but the\nbenefit of merging is unclear when merging \"generalist\" models trained on many\ntasks. We explore merging in the context of large (~100B) models, by recycling\ncheckpoints that exhibit tradeoffs among different tasks. Such checkpoints are\noften created in the process of developing a frontier model, and the suboptimal\nones are usually discarded. Given a pool of model checkpoints obtained from\ndifferent training runs (e.g., different stages, objectives, hyperparameters,\nand data mixtures), which naturally show tradeoffs across different language\ncapabilities (e.g., instruction following vs. code generation), we investigate\nwhether merging can recycle such suboptimal models into a Pareto-optimal one.\nOur optimization algorithm tunes the weight of each checkpoint in a linear\ncombination, resulting in such an optimal model that outperforms both\nindividual models and merge-based baselines. Further analysis shows that good\nmerges tend to include almost all checkpoints with non-zero weights, indicating\nthat even seemingly bad initial checkpoints can contribute to good final\nmerges.",
      "tldr_zh": "本文研究了在大型语言模型（~100B）中，通过优化模型合并（model merging）来回收次优检查点（checkpoints），以缓解不同任务间的性能权衡。这些检查点来自各种训练运行（如不同阶段、目标、超参数和数据混合），并显示出语言能力的权衡（如指令遵循 vs. 代码生成）。研究提出了一种优化算法，通过调整每个检查点在线性组合中的权重，生成一个Pareto-optimal模型，该模型优于个体模型和基于合并的基线，且分析显示即使是看似糟糕的初始检查点也能为良好合并做出贡献。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "13 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.04144v3",
      "published_date": "2024-12-05 13:12:51 UTC",
      "updated_date": "2025-02-03 20:31:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:16:52.618231"
    },
    {
      "arxiv_id": "2412.04142v1",
      "title": "Methodology for Online Estimation of Rheological Parameters in Polymer Melts Using Deep Learning and Microfluidics",
      "title_zh": "翻译失败",
      "authors": [
        "Juan Sandubete-López",
        "José L. Risco-Martín",
        "Alexander H. McMillan",
        "Eva Besada-Portas"
      ],
      "abstract": "Microfluidic devices are increasingly used in biological and chemical\nexperiments due to their cost-effectiveness for rheological estimation in\nfluids. However, these devices often face challenges in terms of accuracy,\nsize, and cost. This study presents a methodology, integrating deep learning,\nmodeling and simulation to enhance the design of microfluidic systems, used to\ndevelop an innovative approach for viscosity measurement of polymer melts. We\nuse synthetic data generated from the simulations to train a deep learning\nmodel, which then identifies rheological parameters of polymer melts from\npressure drop and flow rate measurements in a microfluidic circuit, enabling\nonline estimation of fluid properties. By improving the accuracy and\nflexibility of microfluidic rheological estimation, our methodology accelerates\nthe design and testing of microfluidic devices, reducing reliance on physical\nprototypes, and offering significant contributions to the field.",
      "tldr_zh": "本研究提出了一种整合深度学习、建模和模拟的方法，用于在线估计聚合物熔体（Polymer Melts）的流变参数（Rheological Parameters），以解决微流控（Microfluidics）设备在准确性、尺寸和成本方面的挑战。方法通过模拟生成合成数据训练深度学习模型，并利用微流控电路中的压力降和流量测量来识别流变参数，实现实时估计。实验结果显示，该方法提高了微流控系统的准确性和灵活性，加速了设备设计和测试过程，减少了对物理原型的依赖，并在流变学领域做出了显著贡献。",
      "categories": [
        "physics.flu-dyn",
        "cs.AI"
      ],
      "primary_category": "physics.flu-dyn",
      "comment": "12 pages, 6 figures, Winter Simulation Conference 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.04142v1",
      "published_date": "2024-12-05 13:11:04 UTC",
      "updated_date": "2024-12-05 13:11:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:17:02.034880"
    },
    {
      "arxiv_id": "2412.04140v2",
      "title": "Understanding Memorization in Generative Models via Sharpness in Probability Landscapes",
      "title_zh": "翻译失败",
      "authors": [
        "Dongjae Jeon",
        "Dueun Kim",
        "Albert No"
      ],
      "abstract": "In this paper, we introduce a geometric framework to analyze memorization in\ndiffusion models through the sharpness of the log probability density. We\nmathematically justify a previously proposed score-difference-based\nmemorization metric by demonstrating its effectiveness in quantifying\nsharpness. Additionally, we propose a novel memorization metric that captures\nsharpness at the initial stage of image generation in latent diffusion models,\noffering early insights into potential memorization. Leveraging this metric, we\ndevelop a mitigation strategy that optimizes the initial noise of the\ngeneration process using a sharpness-aware regularization term.",
      "tldr_zh": "本研究引入了一个几何框架，通过概率景观中的锐度（sharpness）来分析生成模型中的记忆化（memorization），特别针对扩散模型。研究者数学证明了先前提出的基于分数差异的记忆化指标（score-difference-based memorization metric）的有效性，用于量化锐度。论文进一步提出一个新颖的记忆化指标，捕捉潜在扩散模型（latent diffusion models）中图像生成初始阶段的锐度，提供早期洞见；基于此，开发了一种缓解策略，通过锐度感知正则化（sharpness-aware regularization）优化生成过程的初始噪声。整体框架为理解和减少生成模型的记忆化问题提供了新工具。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04140v2",
      "published_date": "2024-12-05 13:07:24 UTC",
      "updated_date": "2025-03-02 00:00:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:17:12.392852"
    },
    {
      "arxiv_id": "2412.04139v3",
      "title": "Monet: Mixture of Monosemantic Experts for Transformers",
      "title_zh": "翻译失败",
      "authors": [
        "Jungwoo Park",
        "Young Jin Ahn",
        "Kee-Eung Kim",
        "Jaewoo Kang"
      ],
      "abstract": "Understanding the internal computations of large language models (LLMs) is\ncrucial for aligning them with human values and preventing undesirable\nbehaviors like toxic content generation. However, mechanistic interpretability\nis hindered by polysemanticity -- where individual neurons respond to multiple,\nunrelated concepts. While Sparse Autoencoders (SAEs) have attempted to\ndisentangle these features through sparse dictionary learning, they have\ncompromised LLM performance due to reliance on post-hoc reconstruction loss. To\naddress this issue, we introduce Mixture of Monosemantic Experts for\nTransformers (Monet) architecture, which incorporates sparse dictionary\nlearning directly into end-to-end Mixture-of-Experts pretraining. Our novel\nexpert decomposition method enables scaling the expert count to 262,144 per\nlayer while total parameters scale proportionally to the square root of the\nnumber of experts. Our analyses demonstrate mutual exclusivity of knowledge\nacross experts and showcase the parametric knowledge encapsulated within\nindividual experts. Moreover, Monet allows knowledge manipulation over domains,\nlanguages, and toxicity mitigation without degrading general performance. Our\npursuit of transparent LLMs highlights the potential of scaling expert counts\nto enhance mechanistic interpretability and directly resect the internal\nknowledge to fundamentally adjust model behavior. The source code and\npretrained checkpoints are available at https://github.com/dmis-lab/Monet.",
      "tldr_zh": "这篇论文介绍了 Monet 架构，一种用于 Transformers 的 Mixture of Monosemantic Experts，旨在解决大型语言模型 (LLMs) 中的 polysemanticity 问题，即神经元响应多个无关概念，从而提升模型的机制解释性。Monet 通过将稀疏字典学习 (Sparse Autoencoders, SAEs) 直接整合到 Mixture-of-Experts 的端到端预训练中，实现了专家数量扩展到每层 262,144 个，同时参数总量按专家数的平方根比例增长。实验结果显示，Monet 确保了专家之间知识的互斥性，并允许在不降低一般性能的情况下操纵领域、语言和毒性知识，为透明 LLMs 的开发提供了新途径。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04139v3",
      "published_date": "2024-12-05 13:06:03 UTC",
      "updated_date": "2025-03-02 14:52:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:17:27.493725"
    },
    {
      "arxiv_id": "2412.04137v1",
      "title": "Text Change Detection in Multilingual Documents Using Image Comparison",
      "title_zh": "利用图像比较进行多语言文档文本变化检测",
      "authors": [
        "Doyoung Park",
        "Naresh Reddy Yarram",
        "Sunjin Kim",
        "Minkyu Kim",
        "Seongho Cho",
        "Taehee Lee"
      ],
      "abstract": "Document comparison typically relies on optical character recognition (OCR)\nas its core technology. However, OCR requires the selection of appropriate\nlanguage models for each document and the performance of multilingual or hybrid\nmodels remains limited. To overcome these challenges, we propose text change\ndetection (TCD) using an image comparison model tailored for multilingual\ndocuments. Unlike OCR-based approaches, our method employs word-level text\nimage-to-image comparison to detect changes. Our model generates bidirectional\nchange segmentation maps between the source and target documents. To enhance\nperformance without requiring explicit text alignment or scaling preprocessing,\nwe employ correlations among multi-scale attention features. We also construct\na benchmark dataset comprising actual printed and scanned word pairs in various\nlanguages to evaluate our model. We validate our approach using our benchmark\ndataset and public benchmarks Distorted Document Images and the LRDE Document\nBinarization Dataset. We compare our model against state-of-the-art semantic\nsegmentation and change detection models, as well as to conventional OCR-based\nmodels.",
      "tldr_zh": "这篇论文提出了一种基于图像比较的文本变化检测(TCD)方法，用于多语言文档，以克服传统OCR方法在语言模型选择和多语言性能方面的局限性。该方法通过词级文本图像比较生成双向变化分割地图，并利用多尺度注意力特征提升性能，而无需显式文本对齐或缩放预处理。研究者构建了一个包含各种语言的实际打印和扫描词对的基准数据集，并使用该数据集及公共基准（如Distorted Document Images和LRDE Document Binarization Dataset）验证模型，与最先进的语义分割、变化检测模型以及OCR模型进行了比较。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "15pages, 11figures 6tables, wacv2025 accepted",
      "pdf_url": "http://arxiv.org/pdf/2412.04137v1",
      "published_date": "2024-12-05 13:04:10 UTC",
      "updated_date": "2024-12-05 13:04:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:17:37.583690"
    },
    {
      "arxiv_id": "2412.04121v1",
      "title": "DeepFEA: Deep Learning for Prediction of Transient Finite Element Analysis Solutions",
      "title_zh": "DeepFEA：深度学习用于预测瞬态有限元分析解决方案",
      "authors": [
        "Georgios Triantafyllou",
        "Panagiotis G. Kalozoumis",
        "George Dimas",
        "Dimitris K. Iakovidis"
      ],
      "abstract": "Finite Element Analysis (FEA) is a powerful but computationally intensive\nmethod for simulating physical phenomena. Recent advancements in machine\nlearning have led to surrogate models capable of accelerating FEA. Yet there\nare still limitations in developing surrogates of transient FEA models that can\nsimultaneously predict the solutions for both nodes and elements with\napplicability on both the 2D and 3D domains. Motivated by this research gap,\nthis study proposes DeepFEA, a deep learning-based framework that leverages a\nmultilayer Convolutional Long Short-Term Memory (ConvLSTM) network branching\ninto two parallel convolutional neural networks to predict the solutions for\nboth nodes and elements of FEA models. The proposed network is optimized using\na novel adaptive learning algorithm, called Node-Element Loss Optimization\n(NELO). NELO minimizes the error occurring at both branches of the network\nenabling the prediction of solutions for transient FEA simulations. The\nexperimental evaluation of DeepFEA is performed on three datasets in the\ncontext of structural mechanics, generated to serve as publicly available\nreference datasets. The results show that DeepFEA can achieve less than 3%\nnormalized mean and root mean squared error for 2D and 3D simulation scenarios,\nand inference times that are two orders of magnitude faster than FEA. In\ncontrast, relevant state-of-the-art methods face challenges with\nmulti-dimensional output and dynamic input prediction. Furthermore, DeepFEA's\nrobustness was demonstrated in a real-life biomedical scenario, confirming its\nsuitability for accurate and efficient predictions of FEA simulations.",
      "tldr_zh": "这篇论文提出 DeepFEA，一种基于深度学习的框架，用于加速预测瞬态 Finite Element Analysis (FEA) 解决方案，以克服现有模型在预测节点、元素和多维域方面的局限。DeepFEA 采用多层 Convolutional Long Short-Term Memory (ConvLSTM) 网络与两个并行 Convolutional Neural Networks (CNN) 相结合，并通过新型 Node-Element Loss Optimization (NELO) 算法优化网络错误，实现对 2D 和 3D FEA 模拟的精确预测。实验在结构力学三个公开数据集上验证，显示 DeepFEA 的归一化均方和均方根误差均低于 3%，推理时间比传统 FEA 快两个数量级。相比现有方法，DeepFEA 在多维输出和动态输入预测上表现出色，并在真实生物医学场景中证明了其鲁棒性和适用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.LG",
      "comment": "This work has been submitted to a journal for possible publication",
      "pdf_url": "http://arxiv.org/pdf/2412.04121v1",
      "published_date": "2024-12-05 12:46:18 UTC",
      "updated_date": "2024-12-05 12:46:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:17:52.195081"
    },
    {
      "arxiv_id": "2412.04114v1",
      "title": "Thermal and RGB Images Work Better Together in Wind Turbine Damage Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Serhii Svystun",
        "Oleksandr Melnychenko",
        "Pavlo Radiuk",
        "Oleg Savenko",
        "Anatoliy Sachenko",
        "Andrii Lysyi"
      ],
      "abstract": "The inspection of wind turbine blades (WTBs) is crucial for ensuring their\nstructural integrity and operational efficiency. Traditional inspection methods\ncan be dangerous and inefficient, prompting the use of unmanned aerial vehicles\n(UAVs) that access hard-to-reach areas and capture high-resolution imagery. In\nthis study, we address the challenge of enhancing defect detection on WTBs by\nintegrating thermal and RGB images obtained from UAVs. We propose a\nmultispectral image composition method that combines thermal and RGB imagery\nthrough spatial coordinate transformation, key point detection, binary\ndescriptor creation, and weighted image overlay. Using a benchmark dataset of\nWTB images annotated for defects, we evaluated several state-of-the-art object\ndetection models. Our results show that composite images significantly improve\ndefect detection efficiency. Specifically, the YOLOv8 model's accuracy\nincreased from 91% to 95%, precision from 89% to 94%, recall from 85% to 92%,\nand F1-score from 87% to 93%. The number of false positives decreased from 6 to\n3, and missed defects reduced from 5 to 2. These findings demonstrate that\nintegrating thermal and RGB imagery enhances defect detection on WTBs,\ncontributing to improved maintenance and reliability.",
      "tldr_zh": "本研究针对风力涡轮机叶片（WTBs）的缺陷检测问题，提出了一种多光谱图像合成方法，将无人机（UAVs）捕获的热成像和RGB图像结合，通过空间坐标转换、关键点检测、二进制描述符创建以及加权图像叠加来提升检测效率。使用标注缺陷的基准数据集评估了多种最先进的物体检测模型，结果显示合成图像显著改善了性能，其中YOLOv8模型的准确率从91%提高到95%，精确率从89%到94%，召回率从85%到92%，F1分数从87%到93%，并减少了假阳性和漏检缺陷。总体而言，此方法有助于提升风力涡轮机的维护和可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO",
        "I.4.8; I.4.6; I.2.10; I.2.9"
      ],
      "primary_category": "cs.CV",
      "comment": "Unmanned aerial vehicle, image composition, multispectral images,\n  green energy, data quality management, weighted overlay",
      "pdf_url": "http://arxiv.org/pdf/2412.04114v1",
      "published_date": "2024-12-05 12:32:45 UTC",
      "updated_date": "2024-12-05 12:32:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:18:03.121037"
    },
    {
      "arxiv_id": "2412.04110v1",
      "title": "Enhancing Mathematical Reasoning in LLMs with Background Operators",
      "title_zh": "翻译失败",
      "authors": [
        "Jiajun Chen",
        "Yik-Cheung Tam"
      ],
      "abstract": "We propose utilizing background operators for mathematical reasoning in large\nlanguage models (LLMs). To achieve this, we define a set of fundamental\nmathematical predicates as the basic building blocks. For each mathematical\nproblem, we develop a Prolog solution that includes problem-specific predicates\nand intermediate predicates derived from these background operators, ensuring\nthat each solution adheres to the defined operator set. We introduce the\nMATH-Prolog corpus, which is derived from the counting and probability\ncategories of the MATH corpus. For efficient data augmentation, we apply K-fold\ncross-validated self-training. This method incrementally generates new Prolog\nsolutions for each fold, incorporating those verified as correct into the\ntraining set throughout the model training process. Our experimental results\ndemonstrate that 5-fold crossvalidated self-training effectively identifies\nnew, accurate Prolog solutions, achieving an accuracy of 84.6% on the\ncross-validated set, and 84.8% on the test set during fine-tuning the\nMeta-Llama-3.1-8B-Instruct model. This approach successfully uncovers new\nsolutions with fully computable inference steps for previously unseen problems.\nAdditionally, incorporating the background mathematical predicates into the\nprompt enhances solution coverage.",
      "tldr_zh": "本文提出一种利用background operators提升大型语言模型(LLMs)数学推理的方法，通过定义基本数学谓词作为构建块，并为每个问题开发Prolog解决方案，包括问题特定谓词和中间谓词。研究者创建了MATH-Prolog语料库，从MATH语料库的计数和概率类别衍生，并采用K-fold cross-validated self-training进行数据增强，以逐步生成并验证新解决方案。实验结果显示，该方法在细调Meta-Llama-3.1-8B-Instruct模型时，达到84.6%的交叉验证准确率和84.8%的测试集准确率，同时提高了解决方案的覆盖率和推理步骤的可计算性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04110v1",
      "published_date": "2024-12-05 12:24:54 UTC",
      "updated_date": "2024-12-05 12:24:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:18:13.878163"
    },
    {
      "arxiv_id": "2412.04107v2",
      "title": "Pre-train, Align, and Disentangle: Empowering Sequential Recommendation with Large Language Models",
      "title_zh": "预训练、对齐和解耦：利用大型语言模型增强序列推荐",
      "authors": [
        "Yuhao Wang",
        "Junwei Pan",
        "Pengyue Jia",
        "Wanyu Wang",
        "Maolin Wang",
        "Zhixiang Feng",
        "Xiaotian Li",
        "Jie Jiang",
        "Xiangyu Zhao"
      ],
      "abstract": "Sequential Recommendation (SR) aims to leverage the sequential patterns in\nusers' historical interactions to accurately track their preferences. However,\nthe primary reliance of existing SR methods on collaborative data results in\nchallenges such as the cold-start problem and sub-optimal performance.\nConcurrently, despite the proven effectiveness of large language models (LLMs),\ntheir integration into commercial recommender systems is impeded by issues such\nas high inference latency, incomplete capture of all distribution statistics,\nand catastrophic forgetting. To address these issues, we introduce a novel\nPre-train, Align, and Disentangle (PAD) framework to enhance SR models with\nLLMs. In particular, we initially pre-train both the SR and LLM models to\nobtain collaborative and textual embeddings. Subsequently, we propose a\ncharacteristic recommendation-anchored alignment loss using multi-kernel\nmaximum mean discrepancy with Gaussian kernels. Lastly, a triple-experts\narchitecture, comprising aligned and modality-specific experts with\ndisentangled embeddings, is fine-tuned in a frequency-aware manner.\nExperimental results on three public datasets validate the efficacy of PAD,\nindicating substantial enhancements and compatibility with various SR backbone\nmodels, particularly for cold items. The code and datasets are accessible for\nreproduction at https://github.com/Applied-Machine-Learning-Lab/PAD.",
      "tldr_zh": "该论文针对Sequential Recommendation (SR) 的冷启动问题和性能不足，提出了一种Pre-train, Align, and Disentangle (PAD) 框架，利用Large Language Models (LLMs) 增强推荐系统。框架首先预训练 SR 和 LLM 模型以获取协作和文本嵌入，然后通过多核最大均值差异（multi-kernel maximum mean discrepancy with Gaussian kernels）设计一个基于推荐的锚定对齐损失。接着，采用三重专家架构，包括对齐专家和模态特定专家，并使用分离嵌入（disentangled embeddings）以频率感知方式进行微调。实验在三个公共数据集上验证了 PAD 的有效性，与各种 SR 骨干模型兼容，尤其在冷启动物品上实现了显著性能提升。代码和数据集已在 GitHub 上开源。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "accepted to SIGIR 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.04107v2",
      "published_date": "2024-12-05 12:17:56 UTC",
      "updated_date": "2025-04-25 18:32:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:18:26.216966"
    },
    {
      "arxiv_id": "2412.04100v2",
      "title": "Missing Melodies: AI Music Generation and its \"Nearly\" Complete Omission of the Global South",
      "title_zh": "Missing Melodies: AI 音乐",
      "authors": [
        "Atharva Mehta",
        "Shivam Chauhan",
        "Monojit Choudhury"
      ],
      "abstract": "Recent advances in generative AI have sparked renewed interest and expanded\npossibilities for music generation. However, the performance and versatility of\nthese systems across musical genres are heavily influenced by the availability\nof training data. We conducted an extensive analysis of over one million hours\nof audio datasets used in AI music generation research and manually reviewed\nmore than 200 papers from eleven prominent AI and music conferences and\norganizations (AAAI, ACM, EUSIPCO, EURASIP, ICASSP, ICML, IJCAI, ISMIR,\nNeurIPS, NIME, SMC) to identify a critical gap in the fair representation and\ninclusion of the musical genres of the Global South in AI research. Our\nfindings reveal a stark imbalance: approximately 86% of the total dataset hours\nand over 93% of researchers focus primarily on music from the Global North.\nHowever, around 40% of these datasets include some form of non-Western music,\ngenres from the Global South account for only 14.6% of the data. Furthermore,\napproximately 51% of the papers surveyed concentrate on symbolic music\ngeneration, a method that often fails to capture the cultural nuances inherent\nin music from regions such as South Asia, the Middle East, and Africa. As AI\nincreasingly shapes the creation and dissemination of music, the significant\nunderrepresentation of music genres in datasets and research presents a serious\nthreat to global musical diversity. We also propose some important steps to\nmitigate these risks and foster a more inclusive future for AI-driven music\ngeneration.",
      "tldr_zh": "该研究分析了超过一百万小时的AI音乐生成音频数据集和200多篇论文，发现全球南方音乐在AI研究中严重 underrepresented。具体而言，86%的数据集小时数和93%的研究者主要关注全球北方的音乐，而全球南方音乐仅占14.6%。此外，51%的论文专注于symbolic music generation，这往往无法捕捉南亚、中东和非洲等地区的文化细微差别。论文强调这种不平衡可能威胁全球音乐多样性，并提出缓解措施，如增加数据集的多样性和推动包容性研究。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Submitted to CACM, 12 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.04100v2",
      "published_date": "2024-12-05 12:10:42 UTC",
      "updated_date": "2024-12-12 11:12:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:20:29.479177"
    },
    {
      "arxiv_id": "2412.06822v1",
      "title": "Guidance is All You Need: Temperature-Guided Reasoning in Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Eyad Gomaa",
        "Gomaa Salah"
      ],
      "abstract": "We present Quasar-1, a novel architecture that introduces temperature-guided\nreasoning to large language models through the Token Temperature Mechanism\n(TTM) and Guided Sequence of Thought (GSoT). Our approach leverages the concept\nof hot and cold tokens, where hot tokens are prioritized for their contextual\nrelevance, while cold tokens provide supplementary information. This dynamic\nmodulation of token importance enables the model to achieve superior logical\nreasoning capabilities compared to traditional chain-of-thought approaches.\nThrough rigorous mathematical analysis, we prove that our temperature-guided\nattention mechanism converges to optimal reasoning paths with exponential\nguarantees. Empirical results show significant improvements in reasoning\naccuracy and computational efficiency across a wide range of tasks, making\nadvanced AI reasoning accessible to a broader range of applications.",
      "tldr_zh": "本论文提出了Quasar-1架构，通过Token Temperature Mechanism (TTM)和Guided Sequence of Thought (GSoT)引入温度引导推理机制，提升大型语言模型的逻辑推理能力。该机制利用热tokens（优先上下文相关性）和冷tokens（提供补充信息）动态调整token重要性，从而优于传统的chain-of-thought方法。研究通过严格的数学分析证明了其注意力机制的收敛性，并实验验证了在多种任务上的推理准确性和计算效率显著改进，使先进AI推理更易应用于广泛场景。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.06822v1",
      "published_date": "2024-12-05 12:05:41 UTC",
      "updated_date": "2024-12-05 12:05:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:20:44.137973"
    },
    {
      "arxiv_id": "2412.04097v1",
      "title": "D-LORD for Motion Stylization",
      "title_zh": "翻译失败",
      "authors": [
        "Meenakshi Gupta",
        "Mingyuan Lei",
        "Tat-Jen Cham",
        "Hwee Kuan Lee"
      ],
      "abstract": "This paper introduces a novel framework named D-LORD (Double Latent\nOptimization for Representation Disentanglement), which is designed for motion\nstylization (motion style transfer and motion retargeting). The primary\nobjective of this framework is to separate the class and content information\nfrom a given motion sequence using a data-driven latent optimization approach.\nHere, class refers to person-specific style, such as a particular emotion or an\nindividual's identity, while content relates to the style-agnostic aspect of an\naction, such as walking or jumping, as universally understood concepts. The key\nadvantage of D-LORD is its ability to perform style transfer without needing\npaired motion data. Instead, it utilizes class and content labels during the\nlatent optimization process. By disentangling the representation, the framework\nenables the transformation of one motion sequences style to another's style\nusing Adaptive Instance Normalization. The proposed D-LORD framework is\ndesigned with a focus on generalization, allowing it to handle different class\nand content labels for various applications. Additionally, it can generate\ndiverse motion sequences when specific class and content labels are provided.\nThe framework's efficacy is demonstrated through experimentation on three\ndatasets: the CMU XIA dataset for motion style transfer, the MHAD dataset, and\nthe RRIS Ability dataset for motion retargeting. Notably, this paper presents\nthe first generalized framework for motion style transfer and motion\nretargeting, showcasing its potential contributions in this area.",
      "tldr_zh": "本论文提出了一种名为 D-LORD（Double Latent Optimization for Representation Disentanglement）的创新框架，用于运动风格化，包括 motion style transfer 和 motion retargeting。D-LORD 通过数据驱动的潜在优化方法分离运动序列中的 class（如特定情感或个体身份）和 content（如行走或跳跃的通用动作），无需配对数据，仅依赖类和内容标签。框架利用 Adaptive Instance Normalization 实现风格转移，并在 CMU XIA、MHAD 和 RRIS Ability 数据集上的实验中证明其泛化能力，首次提供了一个通用化的解决方案，为多样化运动生成奠定了基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04097v1",
      "published_date": "2024-12-05 12:03:02 UTC",
      "updated_date": "2024-12-05 12:03:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:20:54.379820"
    },
    {
      "arxiv_id": "2412.04093v1",
      "title": "Practical Considerations for Agentic LLM Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Chris Sypherd",
        "Vaishak Belle"
      ],
      "abstract": "As the strength of Large Language Models (LLMs) has grown over recent years,\nso too has interest in their use as the underlying models for autonomous\nagents. Although LLMs demonstrate emergent abilities and broad expertise across\nnatural language domains, their inherent unpredictability makes the\nimplementation of LLM agents challenging, resulting in a gap between related\nresearch and the real-world implementation of such systems. To bridge this gap,\nthis paper frames actionable insights and considerations from the research\ncommunity in the context of established application paradigms to enable the\nconstruction and facilitate the informed deployment of robust LLM agents.\nNamely, we position relevant research findings into four broad\ncategories--Planning, Memory, Tools, and Control Flow--based on common\npractices in application-focused literature and highlight practical\nconsiderations to make when designing agentic LLMs for real-world applications,\nsuch as handling stochasticity and managing resources efficiently. While we do\nnot conduct empirical evaluations, we do provide the necessary background for\ndiscussing critical aspects of agentic LLM designs, both in academia and\nindustry.",
      "tldr_zh": "这篇论文探讨了使用大型语言模型(LLMs)作为自主代理的实际挑战，强调了LLMs的突发能力(emergent abilities)和广泛专业知识与其实用性不可预测性之间的矛盾。论文将相关研究分为四个类别——Planning、Memory、Tools和Control Flow——并基于应用文献提供可操作的见解，如处理stochasticity和高效资源管理，以桥接学术研究与实际部署的鸿沟。该工作为设计稳健的代理LLMs系统提供了关键背景支持，尽管未进行实证评估。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 3 figures, 1 table",
      "pdf_url": "http://arxiv.org/pdf/2412.04093v1",
      "published_date": "2024-12-05 11:57:49 UTC",
      "updated_date": "2024-12-05 11:57:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:21:07.168902"
    },
    {
      "arxiv_id": "2412.04086v2",
      "title": "BodyMetric: Evaluating the Realism of Human Bodies in Text-to-Image Generation",
      "title_zh": "BodyMetric",
      "authors": [
        "Nefeli Andreou",
        "Varsha Vivek",
        "Ying Wang",
        "Alex Vorobiov",
        "Tiffany Deng",
        "Raja Bala",
        "Larry Davis",
        "Betty Mohler Tesch"
      ],
      "abstract": "Accurately generating images of human bodies from text remains a challenging\nproblem for state of the art text-to-image models. Commonly observed\nbody-related artifacts include extra or missing limbs, unrealistic poses,\nblurred body parts, etc. Currently, evaluation of such artifacts relies heavily\non time-consuming human judgments, limiting the ability to benchmark models at\nscale. We address this by proposing BodyMetric, a learnable metric that\npredicts body realism in images. BodyMetric is trained on realism labels and\nmulti-modal signals including 3D body representations inferred from the input\nimage, and textual descriptions. In order to facilitate this approach, we\ndesign an annotation pipeline to collect expert ratings on human body realism\nleading to a new dataset for this task, namely, BodyRealism. Ablation studies\nsupport our architectural choices for BodyMetric and the importance of\nleveraging a 3D human body prior in capturing body-related artifacts in 2D\nimages. In comparison to concurrent metrics which evaluate general user\npreference in images, BodyMetric specifically reflects body-related artifacts.\nWe demonstrate the utility of BodyMetric through applications that were\npreviously infeasible at scale. In particular, we use BodyMetric to benchmark\nthe generation ability of text-to-image models to produce realistic human\nbodies. We also demonstrate the effectiveness of BodyMetric in ranking\ngenerated images based on the predicted realism scores.",
      "tldr_zh": "这篇论文针对文本到图像生成中的人体图像非真实性问题（如额外肢体或不现实姿势），提出了一种可学习的评估指标BodyMetric，用于预测图像中人体的真实性。BodyMetric通过训练真实性标签、多模态信号（如从图像推断的3D body representations和文本描述）来实现评估，并设计了标注管道创建新数据集BodyRealism。实验消融研究证实了其架构和3D人体先验的重要性，使得BodyMetric能比其他指标更专注于身体相关 artifacts，并应用于大规模基准测试文本到图像模型的生成能力和图像排名。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04086v2",
      "published_date": "2024-12-05 11:48:54 UTC",
      "updated_date": "2024-12-06 09:00:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:23:21.080626"
    },
    {
      "arxiv_id": "2412.04081v1",
      "title": "Federated Learning in Mobile Networks: A Comprehensive Case Study on Traffic Forecasting",
      "title_zh": "移动网络中的",
      "authors": [
        "Nikolaos Pavlidis",
        "Vasileios Perifanis",
        "Selim F. Yilmaz",
        "Francesc Wilhelmi",
        "Marco Miozzo",
        "Pavlos S. Efraimidis",
        "Remous-Aris Koutsiamanis",
        "Pavol Mulinka",
        "Paolo Dini"
      ],
      "abstract": "The increasing demand for efficient resource allocation in mobile networks\nhas catalyzed the exploration of innovative solutions that could enhance the\ntask of real-time cellular traffic prediction. Under these circumstances,\nfederated learning (FL) stands out as a distributed and privacy-preserving\nsolution to foster collaboration among different sites, thus enabling\nresponsive near-the-edge solutions. In this paper, we comprehensively study the\npotential benefits of FL in telecommunications through a case study on\nfederated traffic forecasting using real-world data from base stations (BSs) in\nBarcelona (Spain). Our study encompasses relevant aspects within the federated\nexperience, including model aggregation techniques, outlier management, the\nimpact of individual clients, personalized learning, and the integration of\nexogenous sources of data. The performed evaluation is based on both prediction\naccuracy and sustainability, thus showcasing the environmental impact of\nemployed FL algorithms in various settings. The findings from our study\nhighlight FL as a promising and robust solution for mobile traffic prediction,\nemphasizing its twin merits as a privacy-conscious and environmentally\nsustainable approach, while also demonstrating its capability to overcome data\nheterogeneity and ensure high-quality predictions, marking a significant stride\ntowards its integration in mobile traffic management systems.",
      "tldr_zh": "这篇论文通过一个全面案例研究，探讨了联邦学习（FL）在移动网络中进行交通预测的潜力，使用巴塞罗那（Spain）基站（BSs）的真实数据来评估模型聚合技术、异常管理、个性化学习以及外生数据整合的影响。研究不仅关注预测准确性，还考察了FL算法的环境可持续性，结果表明FL能够有效克服数据异质性，提供高质量预测，同时作为一种隐私保护和生态友好的方法。总体而言，该研究为FL在移动交通管理系统中的应用奠定了坚实基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04081v1",
      "published_date": "2024-12-05 11:32:14 UTC",
      "updated_date": "2024-12-05 11:32:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:21:31.316855"
    },
    {
      "arxiv_id": "2412.04075v1",
      "title": "Does your model understand genes? A benchmark of gene properties for biological and text models",
      "title_zh": "翻译失败",
      "authors": [
        "Yoav Kan-Tor",
        "Michael Morris Danziger",
        "Eden Zohar",
        "Matan Ninio",
        "Yishai Shimoni"
      ],
      "abstract": "The application of deep learning methods, particularly foundation models, in\nbiological research has surged in recent years. These models can be text-based\nor trained on underlying biological data, especially omics data of various\ntypes. However, comparing the performance of these models consistently has\nproven to be a challenge due to differences in training data and downstream\ntasks. To tackle this problem, we developed an architecture-agnostic\nbenchmarking approach that, instead of evaluating the models directly,\nleverages entity representation vectors from each model and trains simple\npredictive models for each benchmarking task. This ensures that all types of\nmodels are evaluated using the same input and output types. Here we focus on\ngene properties collected from professionally curated bioinformatics databases.\nThese gene properties are categorized into five major groups: genomic\nproperties, regulatory functions, localization, biological processes, and\nprotein properties. Overall, we define hundreds of tasks based on these\ndatabases, which include binary, multi-label, and multi-class classification\ntasks. We apply these benchmark tasks to evaluate expression-based models,\nlarge language models, protein language models, DNA-based models, and\ntraditional baselines. Our findings suggest that text-based models and protein\nlanguage models generally outperform expression-based models in genomic\nproperties and regulatory functions tasks, whereas expression-based models\ndemonstrate superior performance in localization tasks. These results should\naid in the development of more informed artificial intelligence strategies for\nbiological understanding and therapeutic discovery. To ensure the\nreproducibility and transparency of our findings, we have made the source code\nand benchmark data publicly accessible for further investigation and expansion\nat github.com/BiomedSciAI/gene-benchmark.",
      "tldr_zh": "这篇论文提出了一种架构无关的基准测试方法，用于评估生物和文本模型对基因属性的理解。该方法利用模型的实体表示向量训练简单预测模型，并基于专业生物信息数据库定义数百个任务，涵盖genomic properties、regulatory functions、localization、biological processes和protein properties等五大类别，包括二元、多标签和多类分类任务。在评估expression-based models、large language models、protein language models、DNA-based models和传统基准时，发现text-based models和protein language models在genomic properties和regulatory functions任务中表现更优，而expression-based models在localization任务中更具优势。该基准有助于指导AI在生物学理解和治疗发现中的策略优化，并已公开源代码和数据以确保可重复性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04075v1",
      "published_date": "2024-12-05 11:14:01 UTC",
      "updated_date": "2024-12-05 11:14:01 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:21:44.174872"
    },
    {
      "arxiv_id": "2412.04069v1",
      "title": "ProtDAT: A Unified Framework for Protein Sequence Design from Any Protein Text Description",
      "title_zh": "ProtDAT：一个统一的框架，用于从任意蛋白质文本描述设计蛋白质序列",
      "authors": [
        "Xiao-Yu Guo",
        "Yi-Fan Li",
        "Yuan Liu",
        "Xiaoyong Pan",
        "Hong-Bin Shen"
      ],
      "abstract": "Protein design has become a critical method in advancing significant\npotential for various applications such as drug development and enzyme\nengineering. However, protein design methods utilizing large language models\nwith solely pretraining and fine-tuning struggle to capture relationships in\nmulti-modal protein data. To address this, we propose ProtDAT, a de novo\nfine-grained framework capable of designing proteins from any descriptive\nprotein text input. ProtDAT builds upon the inherent characteristics of protein\ndata to unify sequences and text as a cohesive whole rather than separate\nentities. It leverages an innovative multi-modal cross-attention, integrating\nprotein sequences and textual information for a foundational level and seamless\nintegration. Experimental results demonstrate that ProtDAT achieves the\nstate-of-the-art performance in protein sequence generation, excelling in\nrationality, functionality, structural similarity, and validity. On 20,000\ntext-sequence pairs from Swiss-Prot, it improves pLDDT by 6%, TM-score by 0.26,\nand reduces RMSD by 1.2 {\\AA}, highlighting its potential to advance protein\ndesign.",
      "tldr_zh": "该研究针对蛋白质设计中的挑战，提出ProtDAT框架，该框架能够从任何蛋白质文本描述中设计蛋白序列，解决传统基于大型语言模型的预训练和微调方法无法有效捕捉多模态蛋白数据关系的局限性。ProtDAT通过统一序列和文本为一个整体，并利用创新的multi-modal cross-attention机制，实现蛋白序列与文本信息的无缝整合。实验结果显示，ProtDAT在20,000个Swiss-Prot文本-序列对上达到最先进性能，提高pLDDT 6%、TM-score 0.26，并降低RMSD 1.2 Å，在合理性、功能性、结构相似性和有效性方面表现出色，从而为药物开发和酶工程等应用提供重大潜力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04069v1",
      "published_date": "2024-12-05 11:05:46 UTC",
      "updated_date": "2024-12-05 11:05:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:23:47.406742"
    },
    {
      "arxiv_id": "2412.04067v1",
      "title": "Automated Medical Report Generation for ECG Data: Bridging Medical Text and Signal Processing with Deep Learning",
      "title_zh": "针对 ECG 数据的自动医疗报告生成：使用深度学习桥接医疗文本和信号处理",
      "authors": [
        "Amnon Bleich",
        "Antje Linnemann",
        "Bjoern H. Diem",
        "Tim OF Conrad"
      ],
      "abstract": "Recent advances in deep learning and natural language generation have\nsignificantly improved image captioning, enabling automated, human-like\ndescriptions for visual content. In this work, we apply these captioning\ntechniques to generate clinician-like interpretations of ECG data. This study\nleverages existing ECG datasets accompanied by free-text reports authored by\nhealthcare professionals (HCPs) as training data. These reports, while often\ninconsistent, provide a valuable foundation for automated learning. We\nintroduce an encoder-decoder-based method that uses these reports to train\nmodels to generate detailed descriptions of ECG episodes. This represents a\nsignificant advancement in ECG analysis automation, with potential applications\nin zero-shot classification and automated clinical decision support.\n  The model is tested on various datasets, including both 1- and 12-lead ECGs.\nIt significantly outperforms the state-of-the-art reference model by Qiu et\nal., achieving a METEOR score of 55.53% compared to 24.51% achieved by the\nreference model. Furthermore, several key design choices are discussed,\nproviding a comprehensive overview of current challenges and innovations in\nthis domain.\n  The source codes for this research are publicly available in our Git\nrepository https://git.zib.de/ableich/ecg-comment-generation-public",
      "tldr_zh": "本研究提出了一种基于深度学习的自动生成ECG报告方法，将医疗文本和信号处理相结合，使用编码器-解码器架构训练模型。该方法利用由医疗专业人员撰写的免费文本报告作为训练数据，针对1-导联和12-导联ECG数据集进行优化，并在ECG分析自动化中实现了显著进展。实验结果显示，该模型的METEOR分数达到55.53%，比Qiu et al.的基准模型的24.51%提高了一倍多，支持零-shot classification和自动化临床决策。该研究还公开了源代码，讨论了关键设计选择和领域挑战。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04067v1",
      "published_date": "2024-12-05 11:05:12 UTC",
      "updated_date": "2024-12-05 11:05:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:22:06.400275"
    },
    {
      "arxiv_id": "2412.04064v1",
      "title": "Graph Neural Networks Need Cluster-Normalize-Activate Modules",
      "title_zh": "图神经网络需要聚类-归一化-激活模块",
      "authors": [
        "Arseny Skryagin",
        "Felix Divo",
        "Mohammad Amin Ali",
        "Devendra Singh Dhami",
        "Kristian Kersting"
      ],
      "abstract": "Graph Neural Networks (GNNs) are non-Euclidean deep learning models for\ngraph-structured data. Despite their successful and diverse applications,\noversmoothing prohibits deep architectures due to node features converging to a\nsingle fixed point. This severely limits their potential to solve complex\ntasks. To counteract this tendency, we propose a plug-and-play module\nconsisting of three steps: Cluster-Normalize-Activate (CNA). By applying CNA\nmodules, GNNs search and form super nodes in each layer, which are normalized\nand activated individually. We demonstrate in node classification and property\nprediction tasks that CNA significantly improves the accuracy over the\nstate-of-the-art. Particularly, CNA reaches 94.18% and 95.75% accuracy on Cora\nand CiteSeer, respectively. It further benefits GNNs in regression tasks as\nwell, reducing the mean squared error compared to all baselines. At the same\ntime, GNNs with CNA require substantially fewer learnable parameters than\ncompeting architectures.",
      "tldr_zh": "该研究指出，Graph Neural Networks (GNNs) 在处理图结构数据时面临 oversmoothing 问题，导致节点特征收敛到单一固定点，从而限制了深度架构的应用。为解决这一问题，论文提出一个即插即用的 Cluster-Normalize-Activate (CNA) 模块，该模块在每个层中搜索并形成超节点(super nodes)，然后对其进行归一化和激活处理。实验结果显示，CNA 显著提升了 GNNs 在节点分类任务中的准确率，例如在 Cora 和 CiteSeer 数据集上分别达到 94.18% 和 95.75%，并在回归任务中降低了均方误差，同时减少了模型的可学习参数数量。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68T07",
        "I.2.0"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 6 figures, 6 tables, accepted at NeurIPS 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.04064v1",
      "published_date": "2024-12-05 10:59:20 UTC",
      "updated_date": "2024-12-05 10:59:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:22:20.147566"
    },
    {
      "arxiv_id": "2412.04062v2",
      "title": "ZipAR: Accelerating Auto-regressive Image Generation through Spatial Locality",
      "title_zh": "ZipAR：通过空间局部性加速自回归图像生成",
      "authors": [
        "Yefei He",
        "Feng Chen",
        "Yuanyu He",
        "Shaoxuan He",
        "Hong Zhou",
        "Kaipeng Zhang",
        "Bohan Zhuang"
      ],
      "abstract": "In this paper, we propose ZipAR, a training-free, plug-and-play parallel\ndecoding framework for accelerating auto-regressive (AR) visual generation. The\nmotivation stems from the observation that images exhibit local structures, and\nspatially distant regions tend to have minimal interdependence. Given a\npartially decoded set of visual tokens, in addition to the original next-token\nprediction scheme in the row dimension, the tokens corresponding to spatially\nadjacent regions in the column dimension can be decoded in parallel, enabling\nthe ``next-set prediction'' paradigm. By decoding multiple tokens\nsimultaneously in a single forward pass, the number of forward passes required\nto generate an image is significantly reduced, resulting in a substantial\nimprovement in generation efficiency. Experiments demonstrate that ZipAR can\nreduce the number of model forward passes by up to 91% on the Emu3-Gen model\nwithout requiring any additional retraining. Code is available here:\nhttps://github.com/ThisisBillhe/ZipAR.",
      "tldr_zh": "本文提出 ZipAR，一种无需训练的即插即用框架，用于加速 Auto-regressive (AR) 图像生成，通过利用图像的 Spatial Locality（空间局部性），即相距较远的区域相互依赖性小。ZipAR 方法在部分解码的视觉标记集基础上，除了原有的行维度下一标记预测，还启用并行解码来处理列维度相邻区域，实现“next-set prediction”范式，从而显著减少生成图像所需的 forward passes。实验结果显示，该框架在 Emu3-Gen 模型上可将 forward passes 减少高达 91%，无需额外 retraining，提升了生成效率。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages",
      "pdf_url": "http://arxiv.org/pdf/2412.04062v2",
      "published_date": "2024-12-05 10:57:08 UTC",
      "updated_date": "2024-12-18 07:28:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:24:06.653089"
    },
    {
      "arxiv_id": "2412.04060v2",
      "title": "Expand Heterogeneous Learning Systems with Selective Multi-Source Knowledge Fusion",
      "title_zh": "翻译失败",
      "authors": [
        "Gaole Dai",
        "Huatao Xu",
        "Yifan Yang",
        "Rui Tan",
        "Mo Li"
      ],
      "abstract": "Expanding existing learning systems to provide high-quality customized models\nfor more domains, such as new users, is challenged by the limited labeled data\nand the data and device heterogeneities. While knowledge distillation methods\ncould overcome label scarcity and device heterogeneity, they assume the\nteachers are fully reliable and overlook the data heterogeneity, which prevents\nthe direct adoption of existing models. To address this problem, this paper\nproposes a framework, HaT, to expand learning systems. It first selects\nmultiple high-quality models from the system at a low cost and then fuses their\nknowledge by assigning sample-wise weights to their predictions. Later, the\nfused knowledge is selectively injected into the customized models based on the\nknowledge quality. Extensive experiments on different tasks, modalities, and\nsettings show that HaT outperforms state-of-the-art baselines by up to 16.5%\naccuracy and saves up to 39% communication traffic.",
      "tldr_zh": "该论文提出 HaT 框架，用于扩展异质学习系统，以应对标签数据有限以及数据和设备异质性的挑战。HaT 通过低成本选择多个高质量模型、为样本分配权重进行多源知识融合，并基于知识质量有选择地注入融合知识到定制模型中。实验结果显示，该框架在不同任务、模态和设置下，比最先进基线提高高达 16.5% 的准确率，并节省高达 39% 的通信流量。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 9 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.04060v2",
      "published_date": "2024-12-05 10:55:54 UTC",
      "updated_date": "2025-02-07 03:21:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:24:21.047052"
    },
    {
      "arxiv_id": "2412.04057v1",
      "title": "From Code to Play: Benchmarking Program Search for Games Using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Manuel Eberhardinger",
        "James Goodman",
        "Alexander Dockhorn",
        "Diego Perez-Liebana",
        "Raluca D. Gaina",
        "Duygu Çakmak",
        "Setareh Maghsudi",
        "Simon Lucas"
      ],
      "abstract": "Large language models (LLMs) have shown impressive capabilities in generating\nprogram code, opening exciting opportunities for applying program synthesis to\ngames. In this work, we explore the potential of LLMs to directly synthesize\nusable code for a wide range of gaming applications, focusing on two\nprogramming languages, Python and Java. We use an evolutionary hill-climbing\nalgorithm, where the mutations and seeds of the initial programs are controlled\nby LLMs. For Python, the framework covers various game-related tasks, including\nfive miniature versions of Atari games, ten levels of Baba is You, an\nenvironment inspired by Asteroids, and a maze generation task. For Java, the\nframework contains 12 games from the TAG tabletop games framework. Across 29\ntasks, we evaluated 12 language models for Python and 8 for Java. Our findings\nsuggest that the performance of LLMs depends more on the task than on model\nsize. While larger models generate more executable programs, these do not\nalways result in higher-quality solutions but are much more expensive. No model\nhas a clear advantage, although on any specific task, one model may be better.\nTrying many models on a problem and using the best results across them is more\nreliable than using just one.",
      "tldr_zh": "本研究探索大型语言模型 (LLMs) 在游戏领域合成程序代码的潜力，提出一个基准框架，使用进化爬山算法 (evolutionary hill-climbing algorithm) 由 LLMs 控制突变和初始程序，针对 Python 和 Java 语言进行测试。实验涵盖 29 个任务，包括 Python 的 Atari 迷你游戏、Baba is You 关卡、Asteroids 灵感环境和迷宫生成，以及 Java 的 12 个 TAG 桌面游戏。结果显示，LLMs 的性能更依赖于具体任务而非模型大小，尽管更大模型生成更多可执行程序，但不一定带来更高质量的解决方案，且计算成本更高；建议在问题上尝试多个模型并选取最佳结果，以获得更可靠的性能。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Submitted to Transactions on Games Special Issue on Large Language\n  Models and Games",
      "pdf_url": "http://arxiv.org/pdf/2412.04057v1",
      "published_date": "2024-12-05 10:50:58 UTC",
      "updated_date": "2024-12-05 10:50:58 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:24:32.821998"
    },
    {
      "arxiv_id": "2412.04037v1",
      "title": "INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations",
      "title_zh": "翻译失败",
      "authors": [
        "Yongming Zhu",
        "Longhao Zhang",
        "Zhengkun Rong",
        "Tianshu Hu",
        "Shuang Liang",
        "Zhipeng Ge"
      ],
      "abstract": "Imagine having a conversation with a socially intelligent agent. It can\nattentively listen to your words and offer visual and linguistic feedback\npromptly. This seamless interaction allows for multiple rounds of conversation\nto flow smoothly and naturally. In pursuit of actualizing it, we propose INFP,\na novel audio-driven head generation framework for dyadic interaction. Unlike\nprevious head generation works that only focus on single-sided communication,\nor require manual role assignment and explicit role switching, our model drives\nthe agent portrait dynamically alternates between speaking and listening state,\nguided by the input dyadic audio. Specifically, INFP comprises a Motion-Based\nHead Imitation stage and an Audio-Guided Motion Generation stage. The first\nstage learns to project facial communicative behaviors from real-life\nconversation videos into a low-dimensional motion latent space, and use the\nmotion latent codes to animate a static image. The second stage learns the\nmapping from the input dyadic audio to motion latent codes through denoising,\nleading to the audio-driven head generation in interactive scenarios. To\nfacilitate this line of research, we introduce DyConv, a large scale dataset of\nrich dyadic conversations collected from the Internet. Extensive experiments\nand visualizations demonstrate superior performance and effectiveness of our\nmethod. Project Page: https://grisoon.github.io/INFP/.",
      "tldr_zh": "本研究提出INFP，一种音频驱动的交互式头部生成框架，用于双人对话场景中。该框架能动态切换代理的说话和倾听状态，由输入的双人音频引导，包括Motion-Based Head Imitation阶段（从真实对话视频中学习面部行为投影到低维运动潜在空间并动画化静态图像）和Audio-Guided Motion Generation阶段（通过去噪从音频映射到运动代码，实现交互式生成）。为了支持这一研究，作者引入了大规模DyConv数据集，包含丰富的互联网双人对话数据。实验结果显示，INFP在性能和有效性上优于现有方法，为实现自然社交交互奠定了基础。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04037v1",
      "published_date": "2024-12-05 10:20:34 UTC",
      "updated_date": "2024-12-05 10:20:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:24:44.228291"
    },
    {
      "arxiv_id": "2412.04036v1",
      "title": "SocialMind: LLM-based Proactive AR Social Assistive System with Human-like Perception for In-situ Live Interactions",
      "title_zh": "SocialMind: 基于 LLM 的主动 AR 社会辅助系统，具有类似人类的感知，用于现场实时互动",
      "authors": [
        "Bufang Yang",
        "Yunqi Guo",
        "Lilin Xu",
        "Zhenyu Yan",
        "Hongkai Chen",
        "Guoliang Xing",
        "Xiaofan Jiang"
      ],
      "abstract": "Social interactions are fundamental to human life. The recent emergence of\nlarge language models (LLMs)-based virtual assistants has demonstrated their\npotential to revolutionize human interactions and lifestyles. However, existing\nassistive systems mainly provide reactive services to individual users, rather\nthan offering in-situ assistance during live social interactions with\nconversational partners. In this study, we introduce SocialMind, the first\nLLM-based proactive AR social assistive system that provides users with in-situ\nsocial assistance. SocialMind employs human-like perception leveraging\nmulti-modal sensors to extract both verbal and nonverbal cues, social factors,\nand implicit personas, incorporating these social cues into LLM reasoning for\nsocial suggestion generation. Additionally, SocialMind employs a multi-tier\ncollaborative generation strategy and proactive update mechanism to display\nsocial suggestions on Augmented Reality (AR) glasses, ensuring that suggestions\nare timely provided to users without disrupting the natural flow of\nconversation. Evaluations on three public datasets and a user study with 20\nparticipants show that SocialMind achieves 38.3% higher engagement compared to\nbaselines, and 95% of participants are willing to use SocialMind in their live\nsocial interactions.",
      "tldr_zh": "这篇论文介绍了 SocialMind，一种基于 LLM 的主动 AR 社交辅助系统，旨在为用户提供实时社交互动支持。系统通过多模态传感器模拟人类感知，提取语言和非语言线索、社会因素以及隐含角色，并将这些信息整合到 LLM 推理中生成社交建议，同时采用多层协作生成策略和主动更新机制，确保建议在 AR 眼镜上及时显示而不干扰对话。实验在三个公共数据集和涉及20名参与者的用户研究中表明，SocialMind 比基线模型提高了38.3%的参与度，且95%的参与者表示愿意在实际社交互动中使用该系统。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04036v1",
      "published_date": "2024-12-05 10:19:36 UTC",
      "updated_date": "2024-12-05 10:19:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:24:57.606692"
    },
    {
      "arxiv_id": "2412.04029v1",
      "title": "Considerations Influencing Offense-Defense Dynamics From Artificial Intelligence",
      "title_zh": "翻译失败",
      "authors": [
        "Giulio Corsi",
        "Kyle Kilian",
        "Richard Mallah"
      ],
      "abstract": "The rapid advancement of artificial intelligence (AI) technologies presents\nprofound challenges to societal safety. As AI systems become more capable,\naccessible, and integrated into critical services, the dual nature of their\npotential is increasingly clear. While AI can enhance defensive capabilities in\nareas like threat detection, risk assessment, and automated security\noperations, it also presents avenues for malicious exploitation and large-scale\nsocietal harm, for example through automated influence operations and cyber\nattacks. Understanding the dynamics that shape AI's capacity to both cause harm\nand enhance protective measures is essential for informed decision-making\nregarding the deployment, use, and integration of advanced AI systems. This\npaper builds on recent work on offense-defense dynamics within the realm of AI,\nproposing a taxonomy to map and examine the key factors that influence whether\nAI systems predominantly pose threats or offer protective benefits to society.\nBy establishing a shared terminology and conceptual foundation for analyzing\nthese interactions, this work seeks to facilitate further research and\ndiscourse in this critical area.",
      "tldr_zh": "本论文探讨了人工智能（AI）在进攻-防御动态（offense-defense dynamics）中的影响因素，随着AI技术的快速发展，其双重性日益显现：一方面，AI可增强防御能力，如在威胁检测、风险评估和自动化安全操作中发挥作用；另一方面，AI可能被恶意利用，导致大规模危害，如自动化影响操作和网络攻击。论文构建了一个分类法（taxonomy），用于映射和分析关键因素，以评估AI系统是更多地构成威胁还是提供保护性益处。最终，该工作通过建立共享术语和概念基础，促进了AI部署决策的相关研究和讨论。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.04029v1",
      "published_date": "2024-12-05 10:05:53 UTC",
      "updated_date": "2024-12-05 10:05:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:25:07.520996"
    },
    {
      "arxiv_id": "2412.04008v1",
      "title": "Deep-Unrolling Multidimensional Harmonic Retrieval Algorithms on Neuromorphic Hardware",
      "title_zh": "基于神经形态硬件",
      "authors": [
        "Vlad C. Andrei",
        "Alexandru P. Drăguţoiu",
        "Gabriel Béna",
        "Mahmoud Akl",
        "Yin Li",
        "Matthias Lohrmann",
        "Ullrich J. Mönich",
        "Holger Boche"
      ],
      "abstract": "This paper explores the potential of conversion-based neuromorphic algorithms\nfor highly accurate and energy-efficient single-snapshot multidimensional\nharmonic retrieval (MHR). By casting the MHR problem as a sparse recovery\nproblem, we devise the currently proposed, deep-unrolling-based Structured\nLearned Iterative Shrinkage and Thresholding (S-LISTA) algorithm to solve it\nefficiently using complex-valued convolutional neural networks with\ncomplex-valued activations, which are trained using a supervised regression\nobjective. Afterward, a novel method for converting the complex-valued\nconvolutional layers and activations into spiking neural networks (SNNs) is\ndeveloped. At the heart of this method lies the recently proposed Few Spikes\n(FS) conversion, which is extended by modifying the neuron model's parameters\nand internal dynamics to account for the inherent coupling between real and\nimaginary parts in complex-valued computations. Finally, the converted SNNs are\nmapped onto the SpiNNaker2 neuromorphic board, and a comparison in terms of\nestimation accuracy and power efficiency between the original CNNs deployed on\nan NVIDIA Jetson Xavier and the SNNs is being conducted. The measurement\nresults show that the converted SNNs achieve almost five-fold power efficiency\nat moderate performance loss compared to the original CNNs.",
      "tldr_zh": "本研究将多维谐波检索 (MHR) 问题转化为稀疏恢复问题，并提出基于深度展开的 Structured Learned Iterative Shrinkage and Thresholding (S-LISTA) 算法，使用复值卷积神经网络 (CNN) 和复值激活函数，通过监督回归训练，实现高精度和节能的单快照 MHR。研究开发了一种新方法，将复值 CNN 层和激活函数转换为脉冲神经网络 (SNN)，通过扩展 Few Spikes (FS) 转换并修改神经元模型参数，以处理复值计算中的实部和虚部耦合。实验结果显示，在 SpiNNaker2 神经形态硬件上部署的 SNN 相比原始 CNN (在 NVIDIA Jetson Xavier 上运行) 实现了近五倍的功率效率，仅伴随适度性能损失。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.AR",
        "cs.NE"
      ],
      "primary_category": "eess.SP",
      "comment": "accepted to the 58th Asilomar Conference on Signals, Systems, and\n  Computers, Oct. 27th - Oct. 30th, 2024, Pacific Grove, CA",
      "pdf_url": "http://arxiv.org/pdf/2412.04008v1",
      "published_date": "2024-12-05 09:41:33 UTC",
      "updated_date": "2024-12-05 09:41:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:25:21.574372"
    },
    {
      "arxiv_id": "2412.03993v1",
      "title": "LaserGuider: A Laser Based Physical Backdoor Attack against Deep Neural Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Yongjie Xu",
        "Guangke Chen",
        "Fu Song",
        "Yuqi Chen"
      ],
      "abstract": "Backdoor attacks embed hidden associations between triggers and targets in\ndeep neural networks (DNNs), causing them to predict the target when a trigger\nis present while maintaining normal behavior otherwise. Physical backdoor\nattacks, which use physical objects as triggers, are feasible but lack remote\ncontrol, temporal stealthiness, flexibility, and mobility. To overcome these\nlimitations, in this work, we propose a new type of backdoor triggers utilizing\nlasers that feature long-distance transmission and instant-imaging properties.\nBased on the laser-based backdoor triggers, we present a physical backdoor\nattack, called LaserGuider, which possesses remote control ability and achieves\nhigh temporal stealthiness, flexibility, and mobility. We also introduce a\nsystematic approach to optimize laser parameters for improving attack\neffectiveness. Our evaluation on traffic sign recognition DNNs, critical in\nautonomous vehicles, demonstrates that LaserGuider with three different\nlaser-based triggers achieves over 90% attack success rate with negligible\nimpact on normal inputs. Additionally, we release LaserMark, the first dataset\nof real world traffic signs stamped with physical laser spots, to support\nfurther research in backdoor attacks and defenses.",
      "tldr_zh": "本论文提出LaserGuider，一种基于激光的物理后门攻击，针对Deep Neural Networks (DNNs)，通过利用激光作为触发器来解决传统攻击的远程控制、时间隐秘性、灵活性和移动性不足问题。\n该方法结合检索增强生成技术(RAG)和优化激光参数的系统性途径，确保攻击在长距离传输和即时成像条件下有效执行。\n实验结果显示，在交通标志识别任务中，LaserGuider使用三种激光触发器实现超过90%的攻击成功率，同时对正常输入的影响微小。\n此外，论文发布了LaserMark数据集，这是首个包含真实世界交通标志激光点的资源，用于推进后门攻击和防御研究。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CR",
      "comment": "In Proceedings of the 23rd International Conference on Applied\n  Cryptography and Network Security (ACNS), Munich, Germany, 23-26 June, 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.03993v1",
      "published_date": "2024-12-05 09:14:50 UTC",
      "updated_date": "2024-12-05 09:14:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:25:35.344465"
    },
    {
      "arxiv_id": "2412.03987v1",
      "title": "MTMT: Consolidating Multiple Thinking Modes to Form a Thought Tree for Strengthening LLM",
      "title_zh": "翻译失败",
      "authors": [
        "Changcheng Li",
        "Xiangyu Wang",
        "Qiuju Chen",
        "Xiren Zhou",
        "Huanhuan Chen"
      ],
      "abstract": "Large language models (LLMs) have shown limitations in tasks requiring\ncomplex logical reasoning and multi-step problem-solving. To address these\nchallenges, researchers have employed carefully designed prompts and\nflowcharts, simulating human cognitive processes to enhance LLM performance,\nsuch as the Chain of Thought approach. In this paper, we introduce MTMT\n(Multi-thinking Modes Tree), a novel method that interacts with LLMs to\nconstruct a thought tree, simulating various advanced cognitive processes,\nincluding but not limited to association, counterfactual thinking, task\ndecomposition, and comparison. By breaking down the original complex task into\nsimpler sub-questions, MTMT facilitates easier problem-solving for LLMs,\nenabling more effective utilization of the latent knowledge within LLMs. We\nevaluate the performance of MTMT under different parameter configurations,\nusing GPT-4o mini as the base model. Our results demonstrate that integrating\nmultiple modes of thinking significantly enhances the ability of LLMs to handle\ncomplex tasks.",
      "tldr_zh": "本研究针对大型语言模型（LLMs）在复杂逻辑推理和多步问题解决上的局限性，提出了一种新型方法 MTMT（Multi-thinking Modes Tree）。MTMT 通过与 LLMs 互动构建一个思想树（thought tree），整合多种高级认知过程，如 association、counterfactual thinking、task decomposition 和 comparison，将复杂任务分解为更简单的子问题，从而更有效地利用 LLMs 的潜在知识。实验使用 GPT-4o mini 作为基模型评估不同参数配置，结果表明整合多种思考模式显著提升了 LLMs 处理复杂任务的能力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03987v1",
      "published_date": "2024-12-05 09:05:30 UTC",
      "updated_date": "2024-12-05 09:05:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:25:45.414815"
    },
    {
      "arxiv_id": "2412.03982v1",
      "title": "Exploring Fully Convolutional Networks for the Segmentation of Hyperspectral Imaging Applied to Advanced Driver Assistance Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Jon Gutiérrez-Zaballa",
        "Koldo Basterretxea",
        "Javier Echanobe",
        "M. Victoria Martínez",
        "Inés del Campo"
      ],
      "abstract": "Advanced Driver Assistance Systems (ADAS) are designed with the main purpose\nof increasing the safety and comfort of vehicle occupants. Most of current\ncomputer vision-based ADAS perform detection and tracking tasks quite\nsuccessfully under regular conditions, but are not completely reliable,\nparticularly under adverse weather and changing lighting conditions, neither in\ncomplex situations with many overlapping objects. In this work we explore the\nuse of hyperspectral imaging (HSI) in ADAS on the assumption that the distinct\nnear infrared (NIR) spectral reflectances of different materials can help to\nbetter separate the objects in a driving scene. In particular, this paper\ndescribes some experimental results of the application of fully convolutional\nnetworks (FCN) to the image segmentation of HSI for ADAS applications. More\nspecifically, our aim is to investigate to what extent the spatial features\ncodified by convolutional filters can be helpful to improve the performance of\nHSI segmentation systems. With that aim, we use the HSI-Drive v1.1 dataset,\nwhich provides a set of labelled images recorded in real driving conditions\nwith a small-size snapshot NIR-HSI camera. Finally, we analyze the\nimplementability of such a HSI segmentation system by prototyping the developed\nFCN model together with the necessary hyperspectral cube preprocessing stage\nand characterizing its performance on an MPSoC.",
      "tldr_zh": "这篇论文探讨了在高级驾驶辅助系统 (ADAS) 中使用超光谱成像 (HSI) 的潜力，以通过近红外 (NIR) 光谱反射更好地分离驾驶场景中的物体，从而应对恶劣天气、变化光照和复杂重叠物体的挑战。研究者应用全卷积网络 (FCN) 进行 HSI 图像分割，重点调查卷积过滤器编码的空间特征如何提升分割性能，并使用 HSI-Drive v1.1 数据集（由真实驾驶条件下的小型快照 NIR-HSI 相机录制的标记图像）进行实验。结果显示，该方法显著改善了物体识别可靠性，最终通过在 MPSoC 上原型化 FCN 模型及其预处理阶段，验证了系统的可实现性和性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "arXiv admin note: text overlap with arXiv:2411.19274",
      "pdf_url": "http://arxiv.org/pdf/2412.03982v1",
      "published_date": "2024-12-05 08:58:25 UTC",
      "updated_date": "2024-12-05 08:58:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:25:59.880049"
    },
    {
      "arxiv_id": "2412.03970v1",
      "title": "A Data-Driven Framework for Discovering Fractional Differential Equations in Complex Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Xiangnan Yu",
        "Hao Xu",
        "Zhiping Mao",
        "HongGuang Sun",
        "Yong Zhang",
        "Dongxiao Zhang",
        "Yuntian Chen"
      ],
      "abstract": "In complex physical systems, conventional differential equations often fall\nshort in capturing non-local and memory effects, as they are limited to local\ndynamics and integer-order interactions. This study introduces a stepwise\ndata-driven framework for discovering fractional differential equations (FDEs)\ndirectly from data. FDEs, known for their capacity to model non-local dynamics\nwith fewer parameters than integer-order derivatives, can represent complex\nsystems with long-range interactions. Our framework applies deep neural\nnetworks as surrogate models for denoising and reconstructing sparse and noisy\nobservations while using Gaussian-Jacobi quadrature to handle the challenges\nposed by singularities in fractional derivatives. To optimize both the sparse\ncoefficients and fractional order, we employ an alternating optimization\napproach that combines sparse regression with global optimization techniques.\nWe validate the framework across various datasets, including synthetic\nanomalous diffusion data, experimental data on the creep behavior of frozen\nsoils, and single-particle trajectories modeled by L\\'{e}vy motion. Results\ndemonstrate the framework's robustness in identifying the structure of FDEs\nacross diverse noise levels and its capacity to capture integer-order dynamics,\noffering a flexible approach for modeling memory effects in complex systems.",
      "tldr_zh": "本文提出一个数据驱动框架，用于从数据中发现分数阶微分方程 (FDEs)，以更好地捕捉复杂系统中非局部动态和记忆效应的问题。框架利用深度神经网络 (deep neural networks) 作为代理模型进行数据去噪和重建，并结合 Gaussian-Jacobi quadrature 处理分数阶导数的奇异性，同时采用交替优化方法，包括 sparse regression 和 global optimization 技术，来优化稀疏系数和分数阶。实验在合成异常扩散数据、冻土蠕变实验数据以及 L\\'{e}vy 运动的单粒子轨迹数据集上验证，证明了框架的鲁棒性，能够在不同噪声水平下识别 FDEs 结构，并灵活地建模整数阶动态和记忆效应。",
      "categories": [
        "physics.comp-ph",
        "cs.AI"
      ],
      "primary_category": "physics.comp-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03970v1",
      "published_date": "2024-12-05 08:38:30 UTC",
      "updated_date": "2024-12-05 08:38:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:26:11.029314"
    },
    {
      "arxiv_id": "2412.03966v1",
      "title": "Demonstration Selection for In-Context Learning via Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Xubin Wang",
        "Jianfei Wu",
        "Yichen Yuan",
        "Mingzhe Li",
        "Deyu Cai",
        "Weijia Jia"
      ],
      "abstract": "Diversity in demonstration selection is crucial for enhancing model\ngeneralization, as it enables a broader coverage of structures and concepts.\nHowever, constructing an appropriate set of demonstrations has remained a focal\npoint of research. This paper presents the Relevance-Diversity Enhanced\nSelection (RDES), an innovative approach that leverages reinforcement learning\nto optimize the selection of diverse reference demonstrations for text\nclassification tasks using Large Language Models (LLMs), especially in few-shot\nprompting scenarios. RDES employs a Q-learning framework to dynamically\nidentify demonstrations that maximize both diversity and relevance to the\nclassification objective by calculating a diversity score based on label\ndistribution among selected demonstrations. This method ensures a balanced\nrepresentation of reference data, leading to improved classification accuracy.\nThrough extensive experiments on four benchmark datasets and involving 12\nclosed-source and open-source LLMs, we demonstrate that RDES significantly\nenhances classification accuracy compared to ten established baselines.\nFurthermore, we investigate the incorporation of Chain-of-Thought (CoT)\nreasoning in the reasoning process, which further enhances the model's\npredictive performance. The results underscore the potential of reinforcement\nlearning to facilitate adaptive demonstration selection and deepen the\nunderstanding of classification challenges.",
      "tldr_zh": "本论文提出了一种名为 RDES (Relevance-Diversity Enhanced Selection) 的创新方法，利用 Reinforcement Learning 的 Q-learning 框架来优化文本分类任务中演示的选择，确保最大化多样性和相关性，从而提升 Large Language Models (LLMs) 在少样本提示场景下的泛化性能。RDES 通过计算基于标签分布的多样性分数，动态选择平衡的参考演示，并在四个基准数据集上使用 12 个闭源和开源 LLMs 进行实验，结果显示其分类准确率显著高于 10 个基线方法。进一步整合 Chain-of-Thought (CoT) 推理后，模型的预测性能得到进一步增强，证明了强化学习在适应性演示选择中的潜力及其对分类挑战的深入理解。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03966v1",
      "published_date": "2024-12-05 08:33:52 UTC",
      "updated_date": "2024-12-05 08:33:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:26:29.056024"
    },
    {
      "arxiv_id": "2412.03963v1",
      "title": "Augmenting Minds or Automating Skills: The Differential Role of Human Capital in Generative AI's Impact on Creative Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Meiling Huang",
        "Ming Jin",
        "Ning Li"
      ],
      "abstract": "Generative AI is rapidly reshaping creative work, raising critical questions\nabout its beneficiaries and societal implications. This study challenges\nprevailing assumptions by exploring how generative AI interacts with diverse\nforms of human capital in creative tasks. Through two random controlled\nexperiments in flash fiction writing and song composition, we uncover a\nparadox: while AI democratizes access to creative tools, it simultaneously\namplifies cognitive inequalities. Our findings reveal that AI enhances general\nhuman capital (cognitive abilities and education) by facilitating adaptability\nand idea integration but diminishes the value of domain-specific expertise. We\nintroduce a novel theoretical framework that merges human capital theory with\nthe automation-augmentation perspective, offering a nuanced understanding of\nhuman-AI collaboration. This framework elucidates how AI shifts the locus of\ncreative advantage from specialized expertise to broader cognitive\nadaptability. Contrary to the notion of AI as a universal equalizer, our work\nhighlights its potential to exacerbate disparities in skill valuation,\nreshaping workplace hierarchies and redefining the nature of creativity in the\nAI era. These insights advance theories of human capital and automation while\nproviding actionable guidance for organizations navigating AI integration\namidst workforce inequalities.",
      "tldr_zh": "这篇论文探讨了 generative AI 在创意任务中如何与不同形式的人力资本互动，通过两个随机对照实验（闪小说写作和歌曲创作）揭示了一个悖论：AI 虽民主化了创意工具，但同时放大了认知不平等。研究发现，AI 增强了 general human capital（如认知能力和教育），促进适应性和想法整合，却削弱了 domain-specific expertise 的价值。作者引入了一个融合 human capital theory 与 automation-augmentation 视角的新理论框架，阐明 AI 将创意优势从专业专长转向更广泛的认知适应性，并强调 AI 可能加剧工作场所不平等，为组织提供 AI 整合的实际指导。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03963v1",
      "published_date": "2024-12-05 08:27:14 UTC",
      "updated_date": "2024-12-05 08:27:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:26:38.607096"
    },
    {
      "arxiv_id": "2412.03957v1",
      "title": "A Framework For Image Synthesis Using Supervised Contrastive Learning",
      "title_zh": "一种使用监督对比学习的图像合成框架",
      "authors": [
        "Yibin Liu",
        "Jianyu Zhang",
        "Li Zhang",
        "Shijian Li",
        "Gang Pan"
      ],
      "abstract": "Text-to-image (T2I) generation aims at producing realistic images\ncorresponding to text descriptions. Generative Adversarial Network (GAN) has\nproven to be successful in this task. Typical T2I GANs are 2 phase methods that\nfirst pretrain an inter-modal representation from aligned image-text pairs and\nthen use GAN to train image generator on that basis. However, such\nrepresentation ignores the inner-modal semantic correspondence, e.g. the images\nwith same label. The semantic label in priory describes the inherent\ndistribution pattern with underlying cross-image relationships, which is\nsupplement to the text description for understanding the full characteristics\nof image. In this paper, we propose a framework leveraging both inter- and\ninner-modal correspondence by label guided supervised contrastive learning. We\nextend the T2I GANs to two parameter-sharing contrast branches in both\npretraining and generation phases. This integration effectively clusters the\nsemantically similar image-text pair representations, thereby fostering the\ngeneration of higher-quality images. We demonstrate our framework on four novel\nT2I GANs by both single-object dataset CUB and multi-object dataset COCO,\nachieving significant improvements in the Inception Score (IS) and Frechet\nInception Distance (FID) metrics of imagegeneration evaluation. Notably, on\nmore complex multi-object COCO, our framework improves FID by 30.1%, 27.3%,\n16.2% and 17.1% for AttnGAN, DM-GAN, SSA-GAN and GALIP, respectively. We also\nvalidate our superiority by comparing with other label guided T2I GANs. The\nresults affirm the effectiveness and competitiveness of our approach in\nadvancing the state-of-the-art GAN for T2I generation",
      "tldr_zh": "这篇论文提出了一种基于监督对比学习的框架，用于提升文本到图像（T2I）生成任务的性能，通过整合内部模式语义对应（如相同标签的图像）和外部模式对应来弥补传统 GAN 方法的不足。框架在预训练和生成阶段扩展 T2I GANs，添加两个参数共享的对比分支，利用标签引导的监督对比学习来聚类语义相似的图像-文本表示，从而生成更高质量的图像。在实验中，该框架在单对象数据集 CUB 和多对象数据集 COCO 上实现了显著改善，Inception Score (IS) 和 Frechet Inception Distance (FID) 指标提升明显，尤其在 COCO 上，FID 分别较 AttnGAN、DM-GAN、SSA-GAN 和 GALIP 提高了 30.1%、27.3%、16.2% 和 17.1%。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03957v1",
      "published_date": "2024-12-05 08:15:37 UTC",
      "updated_date": "2024-12-05 08:15:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:26:50.896471"
    },
    {
      "arxiv_id": "2412.03944v1",
      "title": "Chain-of-Thought in Large Language Models: Decoding, Projection, and Activation",
      "title_zh": "大语言模型中的链式思维：解码、",
      "authors": [
        "Hao Yang",
        "Qianghua Zhao",
        "Lei Li"
      ],
      "abstract": "Chain-of-Thought prompting has significantly enhanced the reasoning\ncapabilities of large language models, with numerous studies exploring factors\ninfluencing its performance. However, the underlying mechanisms remain poorly\nunderstood. To further demystify the operational principles, this work examines\nthree key aspects: decoding, projection, and activation, aiming to elucidate\nthe changes that occur within models when employing Chainof-Thought. Our\nfindings reveal that LLMs effectively imitate exemplar formats while\nintegrating them with their understanding of the question, exhibiting\nfluctuations in token logits during generation but ultimately producing a more\nconcentrated logits distribution, and activating a broader set of neurons in\nthe final layers, indicating more extensive knowledge retrieval compared to\nstandard prompts. Our code and data will be publicly avialable when the paper\nis accepted.",
      "tldr_zh": "本文研究了Chain-of-Thought (CoT) 提示在大型语言模型 (LLMs) 中的底层机制，重点考察decoding、projection 和activation，以揭示模型在使用 CoT 时内部的变化。研究发现，LLMs 能够有效模仿示例格式并将其与问题理解整合，在生成过程中token logits 会波动但最终形成更集中的分布。相比标准提示，CoT 会激活更多最终层神经元，从而实现更广泛的知识检索。该工作将公开代码和数据，以促进进一步研究。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03944v1",
      "published_date": "2024-12-05 07:47:29 UTC",
      "updated_date": "2024-12-05 07:47:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:27:27.025823"
    },
    {
      "arxiv_id": "2412.05331v3",
      "title": "Deep Learning and Hybrid Approaches for Dynamic Scene Analysis, Object Detection and Motion Tracking",
      "title_zh": "翻译失败",
      "authors": [
        "Shahran Rahman Alve"
      ],
      "abstract": "This project aims to develop a robust video surveillance system, which can\nsegment videos into smaller clips based on the detection of activities. It uses\nCCTV footage, for example, to record only major events-like the appearance of a\nperson or a thief-so that storage is optimized and digital searches are easier.\nIt utilizes the latest techniques in object detection and tracking, including\nConvolutional Neural Networks (CNNs) like YOLO, SSD, and Faster R-CNN, as well\nas Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks\n(LSTMs), to achieve high accuracy in detection and capture temporal\ndependencies. The approach incorporates adaptive background modeling through\nGaussian Mixture Models (GMM) and optical flow methods like Lucas-Kanade to\ndetect motions. Multi-scale and contextual analysis are used to improve\ndetection across different object sizes and environments. A hybrid motion\nsegmentation strategy combines statistical and deep learning models to manage\ncomplex movements, while optimizations for real-time processing ensure\nefficient computation. Tracking methods, such as Kalman Filters and Siamese\nnetworks, are employed to maintain smooth tracking even in cases of occlusion.\nDetection is improved on various-sized objects for multiple scenarios by\nmulti-scale and contextual analysis. Results demonstrate high precision and\nrecall in detecting and tracking objects, with significant improvements in\nprocessing times and accuracy due to real-time optimizations and\nillumination-invariant features. The impact of this research lies in its\npotential to transform video surveillance, reducing storage requirements and\nenhancing security through reliable and efficient object detection and\ntracking.",
      "tldr_zh": "本研究旨在开发一个鲁棒的视频监控系统，通过活动检测将视频分割成小片段，优化存储和搜索效率。系统结合了深度学习技术，如 CNNs（包括 YOLO、SSD 和 Faster R-CNN）、RNNs 和 LSTMs，以及传统方法如 Gaussian Mixture Models (GMM) 和 Lucas-Kanade 光学流，进行对象检测、运动跟踪和多尺度上下文分析。采用混合运动分割策略和优化算法（如 Kalman Filters 和 Siamese networks）来处理复杂场景中的遮挡，实现实时高效计算。实验结果显示，该系统在对象检测和跟踪中取得了高精度和召回率，并显著改善了处理时间和照明不变性，从而提升了视频监控的安全性和实用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 Pages, 7 Figures",
      "pdf_url": "http://arxiv.org/pdf/2412.05331v3",
      "published_date": "2024-12-05 07:44:40 UTC",
      "updated_date": "2025-02-17 05:54:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:27:14.076801"
    },
    {
      "arxiv_id": "2412.03941v1",
      "title": "Enhancing and Accelerating Diffusion-Based Inverse Problem Solving through Measurements Optimization",
      "title_zh": "通过测量优化增强和加速基于扩散模型的反问题求解",
      "authors": [
        "Tianyu Chen",
        "Zhendong Wang",
        "Mingyuan Zhou"
      ],
      "abstract": "Diffusion models have recently demonstrated notable success in solving\ninverse problems. However, current diffusion model-based solutions typically\nrequire a large number of function evaluations (NFEs) to generate high-quality\nimages conditioned on measurements, as they incorporate only limited\ninformation at each step. To accelerate the diffusion-based inverse\nproblem-solving process, we introduce \\textbf{M}easurements\n\\textbf{O}ptimization (MO), a more efficient plug-and-play module for\nintegrating measurement information at each step of the inverse problem-solving\nprocess. This method is comprehensively evaluated across eight diverse linear\nand nonlinear tasks on the FFHQ and ImageNet datasets. By using MO, we\nestablish state-of-the-art (SOTA) performance across multiple tasks, with key\nadvantages: (1) it operates with no more than 100 NFEs, with phase retrieval on\nImageNet being the sole exception; (2) it achieves SOTA or near-SOTA results\neven at low NFE counts; and (3) it can be seamlessly integrated into existing\ndiffusion model-based solutions for inverse problems, such as DPS\n\\cite{chung2022diffusion} and Red-diff \\cite{mardani2023variational}. For\nexample, DPS-MO attains a peak signal-to-noise ratio (PSNR) of 28.71 dB on the\nFFHQ 256 dataset for high dynamic range imaging, setting a new SOTA benchmark\nwith only 100 NFEs, whereas current methods require between 1000 and 4000 NFEs\nfor comparable performance.",
      "tldr_zh": "本文提出 Measurements Optimization (MO) 模块，用于增强和加速基于扩散模型的逆问题求解，通过在每个步骤高效整合测量信息，显著减少函数评估次数 (NFEs)。MO 在 FFHQ 和 ImageNet 数据集的八个线性与非线性任务上实现了 SOTA 性能，仅需不超过 100 NFEs（ImageNet 相位检索除外），并可无缝整合到现有方法如 DPS 和 Red-diff 中。实验结果显示，例如 DPS-MO 在 FFHQ 256 数据集的高动态范围成像任务中，仅用 100 NFEs 就达到 28.71 dB 的 PSNR，远超当前需 1000-4000 NFEs 的基准。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03941v1",
      "published_date": "2024-12-05 07:44:18 UTC",
      "updated_date": "2024-12-05 07:44:18 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:27:28.107132"
    },
    {
      "arxiv_id": "2412.03934v1",
      "title": "InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models",
      "title_zh": "InfiniCube：无界限且可控的动态 3D 驾驶场景生成，使用世界引导的视频模型",
      "authors": [
        "Yifan Lu",
        "Xuanchi Ren",
        "Jiawei Yang",
        "Tianchang Shen",
        "Zhangjie Wu",
        "Jun Gao",
        "Yue Wang",
        "Siheng Chen",
        "Mike Chen",
        "Sanja Fidler",
        "Jiahui Huang"
      ],
      "abstract": "We present InfiniCube, a scalable method for generating unbounded dynamic 3D\ndriving scenes with high fidelity and controllability. Previous methods for\nscene generation either suffer from limited scales or lack geometric and\nappearance consistency along generated sequences. In contrast, we leverage the\nrecent advancements in scalable 3D representation and video models to achieve\nlarge dynamic scene generation that allows flexible controls through HD maps,\nvehicle bounding boxes, and text descriptions. First, we construct a\nmap-conditioned sparse-voxel-based 3D generative model to unleash its power for\nunbounded voxel world generation. Then, we re-purpose a video model and ground\nit on the voxel world through a set of carefully designed pixel-aligned\nguidance buffers, synthesizing a consistent appearance. Finally, we propose a\nfast feed-forward approach that employs both voxel and pixel branches to lift\nthe dynamic videos to dynamic 3D Gaussians with controllable objects. Our\nmethod can generate controllable and realistic 3D driving scenes, and extensive\nexperiments validate the effectiveness and superiority of our model.",
      "tldr_zh": "我们提出了 InfiniCube，一种可扩展的方法，用于生成无限动态 3D 驾驶场景，具有高保真度和可控性，以解决现有方法在规模和几何外观一致性上的局限。InfiniCube 结合地图条件稀疏-voxel-based 3D 生成模型和世界引导视频模型，通过精心设计的像素对齐引导缓冲区，确保序列生成的几何和外观一致性，并支持 HD maps、vehicle bounding boxes 和文本描述的灵活控制。该方法采用快速前向策略，将动态视频提升到可控的动态 3D Gaussians，最终实验验证了其在生成真实可控 3D 驾驶场景方面的有效性和优越性。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Page: https://research.nvidia.com/labs/toronto-ai/infinicube/",
      "pdf_url": "http://arxiv.org/pdf/2412.03934v1",
      "published_date": "2024-12-05 07:32:20 UTC",
      "updated_date": "2024-12-05 07:32:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:27:40.104608"
    },
    {
      "arxiv_id": "2412.03933v1",
      "title": "Exploring AI Text Generation, Retrieval-Augmented Generation, and Detection Technologies: a Comprehensive Overview",
      "title_zh": "翻译失败",
      "authors": [
        "Fnu Neha",
        "Deepshikha Bhati",
        "Deepak Kumar Shukla",
        "Angela Guercio",
        "Ben Ward"
      ],
      "abstract": "The rapid development of Artificial Intelligence (AI) has led to the creation\nof powerful text generation models, such as large language models (LLMs), which\nare widely used for diverse applications. However, concerns surrounding\nAI-generated content, including issues of originality, bias, misinformation,\nand accountability, have become increasingly prominent. This paper offers a\ncomprehensive overview of AI text generators (AITGs), focusing on their\nevolution, capabilities, and ethical implications. This paper also introduces\nRetrieval-Augmented Generation (RAG), a recent approach that improves the\ncontextual relevance and accuracy of text generation by integrating dynamic\ninformation retrieval. RAG addresses key limitations of traditional models,\nincluding their reliance on static knowledge and potential inaccuracies in\nhandling real-world data. Additionally, the paper reviews detection tools that\nhelp differentiate AI-generated text from human-written content and discusses\nthe ethical challenges these technologies pose. The paper explores future\ndirections for improving detection accuracy, supporting ethical AI development,\nand increasing accessibility. The paper contributes to a more responsible and\nreliable use of AI in content creation through these discussions.",
      "tldr_zh": "这篇论文对AI文本生成技术（AITGs）进行了全面概述，包括其演变、能力和伦理影响，如原创性、偏见、错误信息以及责任问题。论文重点介绍了Retrieval-Augmented Generation (RAG)，一种通过整合动态信息检索来提升文本生成上下文相关性和准确性的新方法，从而克服传统大型语言模型（LLMs）的静态知识限制。论文还审阅了检测AI生成文本的工具，并讨论了这些技术带来的伦理挑战。最终，论文探讨了未来方向，如提高检测准确性、促进伦理AI开发和增强可访问性，以推动AI在内容创作中的负责任应用。",
      "categories": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03933v1",
      "published_date": "2024-12-05 07:23:14 UTC",
      "updated_date": "2024-12-05 07:23:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:27:48.523842"
    },
    {
      "arxiv_id": "2412.03930v1",
      "title": "MIND: Effective Incorrect Assignment Detection through a Multi-Modal Structure-Enhanced Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Yunhe Pang",
        "Bo Chen",
        "Fanjin Zhang",
        "Yanghui Rao",
        "Jie Tang"
      ],
      "abstract": "The rapid growth of academic publications has exacerbated the issue of author\nname ambiguity in online digital libraries. Despite advances in name\ndisambiguation algorithms, cumulative errors continue to undermine the\nreliability of academic systems. It is estimated that over 10% paper-author\nassignments are rectified when constructing the million-scale WhoIsWho\nbenchmark. Existing endeavors to detect incorrect assignments are either\nsemantic-based or graph-based approaches, which fall short of making full use\nof the rich text attributes of papers and implicit structural features defined\nvia the co-occurrence of paper attributes. To this end, this paper introduces a\nstructure-enhanced language model that combines key structural features from\ngraph-based methods with fine-grained semantic features from rich paper\nattributes to detect incorrect assignments. The proposed model is trained with\na highly effective multi-modal multi-turn instruction tuning framework, which\nincorporates task-guided instruction tuning, text-attribute modality, and\nstructural modality. Experimental results demonstrate that our model\noutperforms previous approaches, achieving top performance on the leaderboard\nof KDD Cup 2024. Our code has been publicly available.",
      "tldr_zh": "这篇论文针对学术出版物中作者名称歧义导致的错误分配问题，提出了MIND模型，即一个Multi-Modal Structure-Enhanced Language Model。该模型将图-based方法的结构特征与论文的丰富文本属性的语义特征相结合，全面检测不正确分配。训练框架采用Multi-Modal Multi-Turn Instruction Tuning，包括task-guided instruction tuning、text-attribute modality和structural modality。实验结果显示，MIND模型在KDD Cup 2024上超越了现有方法，取得了领先性能，并已公开代码。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03930v1",
      "published_date": "2024-12-05 07:12:53 UTC",
      "updated_date": "2024-12-05 07:12:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:28:02.439871"
    },
    {
      "arxiv_id": "2412.03928v2",
      "title": "MT3DNet: Multi-Task learning Network for 3D Surgical Scene Reconstruction",
      "title_zh": "翻译失败",
      "authors": [
        "Mithun Parab",
        "Pranay Lendave",
        "Jiyoung Kim",
        "Thi Quynh Dan Nguyen",
        "Palash Ingle"
      ],
      "abstract": "In image-assisted minimally invasive surgeries (MIS), understanding surgical\nscenes is vital for real-time feedback to surgeons, skill evaluation, and\nimproving outcomes through collaborative human-robot procedures. Within this\ncontext, the challenge lies in accurately detecting, segmenting, and estimating\nthe depth of surgical scenes depicted in high-resolution images, while\nsimultaneously reconstructing the scene in 3D and providing segmentation of\nsurgical instruments along with detection labels for each instrument. To\naddress this challenge, a novel Multi-Task Learning (MTL) network is proposed\nfor performing these tasks concurrently. A key aspect of this approach involves\novercoming the optimization hurdles associated with handling multiple tasks\nconcurrently by integrating a Adversarial Weight Update into the MTL framework,\nthe proposed MTL model achieves 3D reconstruction through the integration of\nsegmentation, depth estimation, and object detection, thereby enhancing the\nunderstanding of surgical scenes, which marks a significant advancement\ncompared to existing studies that lack 3D capabilities. Comprehensive\nexperiments on the EndoVis2018 benchmark dataset underscore the adeptness of\nthe model in efficiently addressing all three tasks, demonstrating the efficacy\nof the proposed techniques.",
      "tldr_zh": "本文提出MT3DNet，一种Multi-Task Learning (MTL)网络，用于微创手术场景的3D重建，该网络同时处理检测、分割、深度估计以及手术器械的分割和检测标签，以提升手术场景理解。关键创新在于整合Adversarial Weight Update到MTL框架中，解决多任务优化的挑战，并通过结合segmentation、depth estimation和object detection实现高效的3D重建。实验结果在EndoVis2018基准数据集上证明了该模型的有效性，与现有研究相比，显著提高了准确性和整体性能。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "1. Notation Update: Added * for equal contribution, ensuring proper\n  attribution. 2. Subsection Fix: Removed the `subsection` tag for Section 3.1\n  (no 3.2 existed), maintaining content but fixing hierarchy. 3. Text\n  Additions: Added lines in Section 5 and Subsection 4.2 for clarity, with\n  references for better context",
      "pdf_url": "http://arxiv.org/pdf/2412.03928v2",
      "published_date": "2024-12-05 07:07:35 UTC",
      "updated_date": "2024-12-11 21:06:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:28:14.034939"
    },
    {
      "arxiv_id": "2412.03920v1",
      "title": "A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "Xiachong Feng",
        "Longxu Dou",
        "Ella Li",
        "Qinghao Wang",
        "Haochuan Wang",
        "Yu Guo",
        "Chang Ma",
        "Lingpeng Kong"
      ],
      "abstract": "Game-theoretic scenarios have become pivotal in evaluating the social\nintelligence of Large Language Model (LLM)-based social agents. While numerous\nstudies have explored these agents in such settings, there is a lack of a\ncomprehensive survey summarizing the current progress. To address this gap, we\nsystematically review existing research on LLM-based social agents within\ngame-theoretic scenarios. Our survey organizes the findings into three core\ncomponents: Game Framework, Social Agent, and Evaluation Protocol. The game\nframework encompasses diverse game scenarios, ranging from choice-focusing to\ncommunication-focusing games. The social agent part explores agents'\npreferences, beliefs, and reasoning abilities. The evaluation protocol covers\nboth game-agnostic and game-specific metrics for assessing agent performance.\nBy reflecting on the current research and identifying future research\ndirections, this survey provides insights to advance the development and\nevaluation of social agents in game-theoretic scenarios.",
      "tldr_zh": "这篇论文对基于Large Language Model (LLM)的社会代理在Game-Theoretic Scenarios中的研究进行了系统综述，以填补现有文献的空白。综述将内容组织为三个核心组件：Game Framework（涵盖从choice-focusing到communication-focusing的多样化博弈场景）、Social Agent（探讨代理的preferences、beliefs和reasoning abilities）、以及Evaluation Protocol（包括game-agnostic和game-specific metrics用于评估性能）。通过反思当前进展，该综述识别了未来研究方向，并为提升社会代理在博弈论环境中的发展和评估提供了重要见解。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03920v1",
      "published_date": "2024-12-05 06:46:46 UTC",
      "updated_date": "2024-12-05 06:46:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:28:29.352043"
    },
    {
      "arxiv_id": "2412.03905v2",
      "title": "Integrating Various Software Artifacts for Better LLM-based Bug Localization and Program Repair",
      "title_zh": "翻译失败",
      "authors": [
        "Qiong Feng",
        "Xiaotian Ma",
        "Jiayi Sheng",
        "Ziyuan Feng",
        "Wei Song",
        "Peng Liang"
      ],
      "abstract": "LLMs have garnered considerable attention for their potential to streamline\nAutomated Program Repair (APR). LLM-based approaches can either insert the\ncorrect code or directly generate patches when provided with buggy methods.\nHowever, most of LLM-based APR methods rely on a single type of software\ninformation, without fully leveraging different software artifacts. Despite\nthis, many LLM-based approaches do not explore which specific types of\ninformation best assist in APR. Addressing this gap is crucial for advancing\nLLM-based APR techniques. We propose DEVLoRe to use issue content (description\nand message) and stack error traces to localize buggy methods, then rely on\ndebug information in buggy methods and issue content and stack error to\nlocalize buggy lines and generate plausible patches which can pass all unit\ntests. The results show that while issue content is particularly effective in\nassisting LLMs with fault localization and program repair, different types of\nsoftware artifacts complement each other. By incorporating different artifacts,\nDEVLoRe successfully locates 49.3% and 47.6% of single and non-single buggy\nmethods and generates 56.0% and 14.5% plausible patches for the Defects4J v2.0\ndataset, respectively. This outperforms current state-of-the-art APR methods.\nThe source code and experimental results of this work for replication are\navailable at https://github.com/XYZboom/DEVLoRe.",
      "tldr_zh": "该研究针对LLM-based Automated Program Repair (APR) 的局限性，提出DEVLoRe框架，通过整合多种软件 artifacts（如issue content、stack error traces和debug information）来提升bug localization和程序修复的效率。DEVLoRe首先利用issue content和stack error traces定位buggy methods，然后结合debug information生成可通过所有单元测试的plausible patches。实验结果显示，不同软件 artifacts互补，其中issue content在故障定位和修复中特别有效；在Defects4J v2.0数据集上，DEVLoRe成功定位49.3%的单buggy methods和47.6%的非单buggy methods，并生成56.0%和14.5%的plausible patches，优于现有APR方法。总的来说，该框架为LLM-based APR提供了更全面的信息整合策略。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "22 pages, 11 images, 9 tables, Manuscript submitted to a journal\n  (2024)",
      "pdf_url": "http://arxiv.org/pdf/2412.03905v2",
      "published_date": "2024-12-05 06:21:31 UTC",
      "updated_date": "2025-03-04 07:06:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:28:42.094925"
    },
    {
      "arxiv_id": "2412.03904v1",
      "title": "MISR: Measuring Instrumental Self-Reasoning in Frontier Models",
      "title_zh": "翻译失败",
      "authors": [
        "Kai Fronsdal",
        "David Lindner"
      ],
      "abstract": "We propose a suite of tasks to evaluate the instrumental self-reasoning\nability of large language model (LLM) agents. Instrumental self-reasoning\nability could improve adaptability and enable self-modification, but it could\nalso pose significant risks, such as enabling deceptive alignment. Prior work\nhas only evaluated self-reasoning in non-agentic settings or in limited\ndomains. In this paper, we propose evaluations for instrumental self-reasoning\nability in agentic tasks in a wide range of scenarios, including\nself-modification, knowledge seeking, and opaque self-reasoning. We evaluate\nagents built using state-of-the-art LLMs, including commercial and open source\nsystems. We find that instrumental self-reasoning ability emerges only in the\nmost capable frontier models and that it is highly context-dependent. No model\npasses the the most difficult versions of our evaluations, hence our evaluation\ncan be used to measure increases in instrumental self-reasoning ability in\nfuture models. We open-source our evaluations at\nhttps://github.com/kaifronsdal/Self-Reasoning-Evals.",
      "tldr_zh": "本文提出MISR，一套任务设计，用于评估大型语言模型(LLM)代理的工具性自我推理(Instrumental Self-Reasoning)能力，以探索其在适应性、自我修改等方面的潜力，同时警示潜在风险如欺骗性对齐。方法包括在代理任务中测试多种场景，如自我修改、知识寻求和不透明自我推理，并评估了基于最先进LLM（包括商业和开源系统）的代理。结果显示，这种能力仅在Frontier Models中出现，且高度依赖上下文；然而，没有模型通过最难的评估，因此MISR可作为未来模型改进的基准，并已开源代码。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "I.2.7"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 65 page appendix, 5 figures",
      "pdf_url": "http://arxiv.org/pdf/2412.03904v1",
      "published_date": "2024-12-05 06:20:47 UTC",
      "updated_date": "2024-12-05 06:20:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:28:53.793103"
    },
    {
      "arxiv_id": "2412.03903v1",
      "title": "Using SlowFast Networks for Near-Miss Incident Analysis in Dashcam Videos",
      "title_zh": "翻译失败",
      "authors": [
        "Yucheng Zhang",
        "Koichi Emura",
        "Eiji Watanabe"
      ],
      "abstract": "This paper classifies near-miss traffic videos using the SlowFast deep neural\nnetwork that mimics the characteristics of the slow and fast visual information\nprocessed by two different streams from the M (Magnocellular) and P\n(Parvocellular) cells of the human brain. The approach significantly improves\nthe accuracy of the traffic near-miss video analysis and presents insights into\nhuman visual perception in traffic scenarios. Moreover, it contributes to\ntraffic safety enhancements and provides novel perspectives on the potential\ncognitive errors in traffic accidents.",
      "tldr_zh": "本论文使用 SlowFast Networks 深度神经网络来分类行车记录仪（dashcam）视频中的近距离交通事件，该网络模仿人类大脑 M (Magnocellular) 和 P (Parvocellular) 细胞处理慢速和快速视觉信息的特性。相比传统方法，该方法显著提高了交通近距离视频分析的准确性，并提供了对人类视觉感知在交通场景中的新洞见。通过这些改进，论文为提升交通安全提供了贡献，并探讨了潜在的认知错误在交通事故中的作用。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Best Research Paper Award for Asia-Pacific Region, The 30th ITS World\n  Congress 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.03903v1",
      "published_date": "2024-12-05 06:20:19 UTC",
      "updated_date": "2024-12-05 06:20:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:29:03.851665"
    },
    {
      "arxiv_id": "2412.03895v1",
      "title": "A Noise is Worth Diffusion Guidance",
      "title_zh": "翻译失败",
      "authors": [
        "Donghoon Ahn",
        "Jiwon Kang",
        "Sanghyun Lee",
        "Jaewon Min",
        "Minjae Kim",
        "Wooseok Jang",
        "Hyoungwon Cho",
        "Sayak Paul",
        "SeonHwa Kim",
        "Eunju Cha",
        "Kyong Hwan Jin",
        "Seungryong Kim"
      ],
      "abstract": "Diffusion models excel in generating high-quality images. However, current\ndiffusion models struggle to produce reliable images without guidance methods,\nsuch as classifier-free guidance (CFG). Are guidance methods truly necessary?\nObserving that noise obtained via diffusion inversion can reconstruct\nhigh-quality images without guidance, we focus on the initial noise of the\ndenoising pipeline. By mapping Gaussian noise to `guidance-free noise', we\nuncover that small low-magnitude low-frequency components significantly enhance\nthe denoising process, removing the need for guidance and thus improving both\ninference throughput and memory. Expanding on this, we propose \\ours, a novel\nmethod that replaces guidance methods with a single refinement of the initial\nnoise. This refined noise enables high-quality image generation without\nguidance, within the same diffusion pipeline. Our noise-refining model\nleverages efficient noise-space learning, achieving rapid convergence and\nstrong performance with just 50K text-image pairs. We validate its\neffectiveness across diverse metrics and analyze how refined noise can\neliminate the need for guidance. See our project page:\nhttps://cvlab-kaist.github.io/NoiseRefine/.",
      "tldr_zh": "该研究发现，diffusion models 在生成高质量图像时依赖指导方法如 classifier-free guidance (CFG)，但通过分析初始噪声的低幅度低频组件，可以无需指导实现高效去噪。论文提出 \\ours 方法，通过将高斯噪声映射为 guidance-free noise 并进行单次精炼，替换传统指导机制，从而提高图像生成质量、推理吞吐量和内存效率。该方法利用高效噪声空间学习，仅需 50K 文本-图像对即可快速收敛，并在多种指标上验证其有效性，证明精炼噪声能彻底消除对指导的需求。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project page: https://cvlab-kaist.github.io/NoiseRefine/",
      "pdf_url": "http://arxiv.org/pdf/2412.03895v1",
      "published_date": "2024-12-05 06:09:56 UTC",
      "updated_date": "2024-12-05 06:09:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:29:17.365933"
    },
    {
      "arxiv_id": "2412.03894v1",
      "title": "Machine Learning-based Android Intrusion Detection System",
      "title_zh": "基于机器学习的 Android 入侵检测系统",
      "authors": [
        "Madiha Tahreem",
        "Ifrah Andleeb",
        "Bilal Zahid Hussain",
        "Arsalan Hameed"
      ],
      "abstract": "The android operating system is being installed in most of the smart devices.\nThe introduction of intrusions in such operating systems is rising at a\ntremendous rate. With the introduction of such malicious data streams, the\nsmart devices are being subjected to various attacks like Phishing, Spyware,\nSMS Fraud, Bots and Banking-Trojans and many such. The application of machine\nlearning classification algorithms for the security of android APK files is\nused in this paper. Each apk data stream was marked to be either malicious or\nnon malicious on the basis of different parameters. The machine learning\nclassification techniques are then used to classify whether the newly installed\napplications' signature falls within the malicious or non-malicious domain. If\nit falls within the malicious category, appropriate action can be taken, and\nthe Android operating system can be shielded against illegal activities.",
      "tldr_zh": "本论文针对Android操作系统的日益普及及其面临的入侵威胁（如Phishing、Spyware、SMS Fraud、Bots和Banking-Trojans），提出了一种基于Machine Learning的入侵检测系统。研究方法包括使用Machine Learning分类算法分析Android APK文件的参数，将其标记为恶意或非恶意，并对新安装应用的签名进行分类。如果检测到恶意应用，则可采取相应行动以保护系统。该方法为提升Android系统的安全性提供了有效的解决方案。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03894v1",
      "published_date": "2024-12-05 06:05:12 UTC",
      "updated_date": "2024-12-05 06:05:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:31:19.136632"
    },
    {
      "arxiv_id": "2412.03893v1",
      "title": "Dual-Branch Subpixel-Guided Network for Hyperspectral Image Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Zhu Han",
        "Jin Yang",
        "Lianru Gao",
        "Zhiqiang Zeng",
        "Bing Zhang",
        "Jocelyn Chanussot"
      ],
      "abstract": "Deep learning (DL) has been widely applied into hyperspectral image (HSI)\nclassification owing to its promising feature learning and representation\ncapabilities. However, limited by the spatial resolution of sensors, existing\nDL-based classification approaches mainly focus on pixel-level spectral and\nspatial information extraction through complex network architecture design,\nwhile ignoring the existence of mixed pixels in actual scenarios. To tackle\nthis difficulty, we propose a novel dual-branch subpixel-guided network for HSI\nclassification, called DSNet, which automatically integrates subpixel\ninformation and convolutional class features by introducing a deep autoencoder\nunmixing architecture to enhance classification performance. DSNet is capable\nof fully considering physically nonlinear properties within subpixels and\nadaptively generating diagnostic abundances in an unsupervised manner to\nachieve more reliable decision boundaries for class label distributions. The\nsubpixel fusion module is designed to ensure high-quality information fusion\nacross pixel and subpixel features, further promoting stable joint\nclassification. Experimental results on three benchmark datasets demonstrate\nthe effectiveness and superiority of DSNet compared with state-of-the-art\nDL-based HSI classification approaches. The codes will be available at\nhttps://github.com/hanzhu97702/DSNet, contributing to the remote sensing\ncommunity.",
      "tldr_zh": "本研究针对高光谱图像 (HSI) 分类中混合像素的问题，提出了一种新型双分支子像素引导网络（DSNet），利用深度自编码器解混架构自动整合子像素信息和卷积类特征，以提升分类性能。DSNet 考虑子像素内的物理非线性属性，并通过子像素融合模块实现高质量的信息融合，从而生成更可靠的决策边界和类标签分布。在三个基准数据集上的实验结果表明，DSNet 比现有的深度学习 (DL) 基于方法更具有效性和优越性，代码已开源以促进遥感社区的发展。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03893v1",
      "published_date": "2024-12-05 06:03:09 UTC",
      "updated_date": "2024-12-05 06:03:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:31:40.385192"
    },
    {
      "arxiv_id": "2412.03886v1",
      "title": "Uniform Discretized Integrated Gradients: An effective attribution based method for explaining large language models",
      "title_zh": "翻译失败",
      "authors": [
        "Swarnava Sinha Roy",
        "Ayan Kundu"
      ],
      "abstract": "Integrated Gradients is a well-known technique for explaining deep learning\nmodels. It calculates feature importance scores by employing a gradient based\napproach computing gradients of the model output with respect to input features\nand accumulating them along a linear path. While this works well for continuous\nfeatures spaces, it may not be the most optimal way to deal with discrete\nspaces like word embeddings. For interpreting LLMs (Large Language Models),\nthere exists a need for a non-linear path where intermediate points, whose\ngradients are to be computed, lie close to actual words in the embedding space.\nIn this paper, we propose a method called Uniform Discretized Integrated\nGradients (UDIG) based on a new interpolation strategy where we choose a\nfavorable nonlinear path for computing attribution scores suitable for\npredictive language models. We evaluate our method on two types of NLP tasks-\nSentiment Classification and Question Answering against three metrics viz Log\nodds, Comprehensiveness and Sufficiency. For sentiment classification, we have\nused the SST2, IMDb and Rotten Tomatoes datasets for benchmarking and for\nQuestion Answering, we have used the fine-tuned BERT model on SQuAD dataset.\nOur approach outperforms the existing methods in almost all the metrics.",
      "tldr_zh": "该论文针对现有 Integrated Gradients 方法在离散空间（如词嵌入）中的不足，提出了一种新的解释技术 Uniform Discretized Integrated Gradients (UDIG)，通过采用非线性路径插值策略，确保中间点接近实际单词，从而更适合解释大型语言模型。UDIG 在情感分类任务（如 SST2、IMDb 和 Rotten Tomatoes 数据集）和问答任务（如 SQuAD 数据集）上进行了评估，使用 Log odds、Comprehensiveness 和 Sufficiency 指标进行比较。结果显示，UDIG 在几乎所有指标上均优于现有方法，提高了语言模型解释的可解释性和准确性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03886v1",
      "published_date": "2024-12-05 05:39:03 UTC",
      "updated_date": "2024-12-05 05:39:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:31:52.195853"
    },
    {
      "arxiv_id": "2412.03884v1",
      "title": "A Unified Framework for Evaluating the Effectiveness and Enhancing the Transparency of Explainable AI Methods in Real-World Applications",
      "title_zh": "一个用于评估可解释AI方法有效性并提升其透明度的统一框架，在现实世界应用中",
      "authors": [
        "Md. Ariful Islam",
        "M. F. Mridha",
        "Md Abrar Jahin",
        "Nilanjan Dey"
      ],
      "abstract": "The rapid advancement of deep learning has resulted in substantial\nadvancements in AI-driven applications; however, the \"black box\" characteristic\nof these models frequently constrains their interpretability, transparency, and\nreliability. Explainable artificial intelligence (XAI) seeks to elucidate AI\ndecision-making processes, guaranteeing that explanations faithfully represent\nthe model's rationale and correspond with human comprehension. Despite\ncomprehensive research in XAI, a significant gap persists in standardized\nprocedures for assessing the efficacy and transparency of XAI techniques across\nmany real-world applications. This study presents a unified XAI evaluation\nframework incorporating extensive quantitative and qualitative criteria to\nsystematically evaluate the correctness, interpretability, robustness,\nfairness, and completeness of explanations generated by AI models. The\nframework prioritizes user-centric and domain-specific adaptations, hence\nimproving the usability and reliability of AI models in essential domains. To\naddress deficiencies in existing evaluation processes, we suggest defined\nbenchmarks and a systematic evaluation pipeline that includes data loading,\nexplanation development, and thorough method assessment. The suggested\nframework's relevance and variety are evidenced by case studies in healthcare,\nfinance, agriculture, and autonomous systems. These provide a solid basis for\nthe equitable and dependable assessment of XAI methodologies. This paradigm\nenhances XAI research by offering a systematic, flexible, and pragmatic method\nto guarantee transparency and accountability in AI systems across many\nreal-world contexts.",
      "tldr_zh": "这篇论文提出一个统一的框架，用于评估和提升 Explainable AI (XAI) 方法在真实世界的有效性和透明度，以解决 AI 模型的“黑箱”问题和现有评估标准的缺失。框架整合了定量和定性标准，评估解释的正确性、可解释性、鲁棒性、公平性和完整性，并强调用户导向和领域特定适应，以提高 AI 在关键领域的可靠性和可用性。该框架包括定义基准和系统评估管道，并通过医疗、金融、农业和自治系统等领域的案例研究验证其实用性，为 XAI 研究提供灵活、实用的方法，确保 AI 系统的透明度和责任。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03884v1",
      "published_date": "2024-12-05 05:30:10 UTC",
      "updated_date": "2024-12-05 05:30:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:30:04.931310"
    },
    {
      "arxiv_id": "2412.03881v2",
      "title": "Weak-to-Strong Generalization Through the Data-Centric Lens",
      "title_zh": "翻译失败",
      "authors": [
        "Changho Shin",
        "John Cooper",
        "Frederic Sala"
      ],
      "abstract": "The weak-to-strong generalization phenomenon is the driver for important\nmachine learning applications including highly data-efficient learning and,\nmost recently, performing superalignment. While decades of research have\nresulted in numerous algorithms that produce strong empirical performance,\nunderstanding what aspects of data enable weak-to-strong generalization has\nbeen understudied. We propose a simple data-centric mechanism that\ncharacterizes weak-to-strong generalization: the overlap density. Intuitively,\ngeneralization tracks the number of points that contain overlaps, i.e., both\neasy patterns (learnable by a weak model) and challenging patterns (only\nlearnable by a stronger model), as with such points, weak predictions can be\nused to learn challenging patterns by stronger models. We provide a practical\noverlap detection algorithm to find such points in datasets and leverage them\nto learn, among multiple sources of data, which to query when seeking to\nmaximize overlap density and thereby enhance weak-to-strong generalization. We\npresent a theoretical result showing that the generalization benefit is a\nfunction of the overlap density and a regret bound for our data selection\nalgorithm. Empirically, we validate the mechanism and the overlap detection\nalgorithm on a wide array of settings.",
      "tldr_zh": "该论文探讨了弱到强泛化（weak-to-strong generalization）现象，通过数据中心视角分析其驱动因素，强调数据重叠密度（overlap density）的关键作用。重叠密度指数据集中的点同时包含易模式（weak model 可学习）和挑战模式（仅 strong model 可学习），这些点可利用弱模型预测来帮助强模型学习复杂模式。作者提出了一种实用的重叠检测算法，用于选择高重叠密度的数据源，以提升泛化性能，并通过理论证明和广泛实证实验验证了该机制的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025",
      "pdf_url": "http://arxiv.org/pdf/2412.03881v2",
      "published_date": "2024-12-05 05:29:19 UTC",
      "updated_date": "2025-03-04 04:28:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:30:17.701035"
    },
    {
      "arxiv_id": "2412.03877v1",
      "title": "AyutthayaAlpha: A Thai-Latin Script Transliteration Transformer",
      "title_zh": "翻译失败",
      "authors": [
        "Davor Lauc",
        "Attapol Rutherford",
        "Weerin Wongwarawipatr"
      ],
      "abstract": "This study introduces AyutthayaAlpha, an advanced transformer-based machine\nlearning model designed for the transliteration of Thai proper names into Latin\nscript. Our system achieves state-of-the-art performance with 82.32%\nfirst-token accuracy and 95.24% first-three-token accuracy, while maintaining a\nlow character error rate of 0.0047. The complexity of Thai phonology, including\ntonal features and vowel length distinctions, presents significant challenges\nfor accurate transliteration, which we address through a novel two-model\napproach: AyutthayaAlpha-Small, based on the ByT5 architecture, and\nAyutthayaAlpha-VerySmall, a computationally efficient variant that unexpectedly\noutperforms its larger counterpart. Our research combines linguistic rules with\ndeep learning, training on a carefully curated dataset of 1.2 million\nThai-Latin name pairs, augmented through strategic upsampling to 2.7 million\nexamples. Extensive evaluations against existing transliteration methods and\nhuman expert benchmarks demonstrate that AyutthayaAlpha not only achieves\nsuperior accuracy but also effectively captures personal and cultural\npreferences in name romanization. The system's practical applications extend to\ncross-lingual information retrieval, international data standardization, and\nidentity verification systems, with particular relevance for government\ndatabases, academic institutions, and global business operations. This work\nrepresents a significant advance in bridging linguistic gaps between Thai and\nLatin scripts, while respecting the cultural and personal dimensions of name\ntransliteration.",
      "tldr_zh": "本研究引入 AyutthayaAlpha，一种基于 Transformer 的机器学习模型，用于将泰语专有名词转录成拉丁字母，成功解决了泰语音调和元音长度等复杂挑战。模型采用两种变体——AyutthayaAlpha-Small（基于 ByT5 架构）和AyutthayaAlpha-VerySmall（计算高效版本，且性能更优）——并结合语言规则和深度学习，在扩展至2.7百万对的训练数据上实现82.32%的第一个标记准确率、95.24%的前三个标记准确率，以及极低的0.0047%字符错误率。相比现有方法和人类专家基准，AyutthayaAlpha 表现出色，能够捕捉个人和文化偏好，并应用于跨语言信息检索、国际数据标准化以及身份验证等领域。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03877v1",
      "published_date": "2024-12-05 05:18:09 UTC",
      "updated_date": "2024-12-05 05:18:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:32:08.524941"
    },
    {
      "arxiv_id": "2412.03873v1",
      "title": "Fine-Grained Sentiment Analysis of Electric Vehicle User Reviews: A Bidirectional LSTM Approach to Capturing Emotional Intensity in Chinese Text",
      "title_zh": "翻译失败",
      "authors": [
        "Shuhao Chen",
        "Chengyi Tu"
      ],
      "abstract": "The rapid expansion of the electric vehicle (EV) industry has highlighted the\nimportance of user feedback in improving product design and charging\ninfrastructure. Traditional sentiment analysis methods often oversimplify the\ncomplexity of user emotions, limiting their effectiveness in capturing nuanced\nsentiments and emotional intensities. This study proposes a Bidirectional Long\nShort-Term Memory (Bi-LSTM) network-based sentiment scoring model to analyze\nuser reviews of EV charging infrastructure. By assigning sentiment scores\nranging from 0 to 5, the model provides a fine-grained understanding of\nemotional expression. Leveraging a dataset of 43,678 reviews from PC Auto, the\nstudy employs rigorous data cleaning and preprocessing, including tokenization\nand stop word removal, to optimize input for deep learning. The Bi-LSTM model\ndemonstrates significant improvements over traditional approaches like SnowNLP\nacross key evaluation metrics, including Mean Squared Error (MSE), Mean\nAbsolute Error (MAE), and Explained Variance Score (EVS). These results\nhighlight the model's superior capability to capture nuanced sentiment\ndynamics, offering valuable insights for targeted product and service\nenhancements in the EV ecosystem.",
      "tldr_zh": "这篇论文提出了一种基于 Bidirectional LSTM 的情感分析模型，用于分析电动车用户评论中的细粒度情感强度，旨在克服传统方法（如 SnowNLP）在捕捉复杂情感方面的局限性。模型通过对 43,678 条来自 PC Auto 的中文评论进行数据清洗和预处理（包括 tokenization 和 stop word removal），并赋予 0-5 的情感分数，提供更精确的情感评估。实验结果显示，Bi-LSTM 在 Mean Squared Error (MSE)、Mean Absolute Error (MAE) 和 Explained Variance Score (EVS) 等指标上显著优于传统方法，提升了情感动态的捕捉能力。该方法为电动车生态系统的产品设计和充电基础设施改进提供了有价值的见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03873v1",
      "published_date": "2024-12-05 05:04:29 UTC",
      "updated_date": "2024-12-05 05:04:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:32:20.321820"
    },
    {
      "arxiv_id": "2412.03871v2",
      "title": "CLIP-PING: Boosting Lightweight Vision-Language Models with Proximus Intrinsic Neighbors Guidance",
      "title_zh": "CLIP-PING：通过 Proximus 内在邻居引导提升轻量级视觉-语言模型",
      "authors": [
        "Chu Myaet Thwal",
        "Ye Lin Tun",
        "Minh N. H. Nguyen",
        "Eui-Nam Huh",
        "Choong Seon Hong"
      ],
      "abstract": "Beyond the success of Contrastive Language-Image Pre-training (CLIP), recent\ntrends mark a shift toward exploring the applicability of lightweight\nvision-language models for resource-constrained scenarios. These models often\ndeliver suboptimal performance when relying solely on a single image-text\ncontrastive learning objective, spotlighting the need for more effective\ntraining mechanisms that guarantee robust cross-modal feature alignment. In\nthis work, we propose CLIP-PING: Contrastive Language-Image Pre-training with\nProximus Intrinsic Neighbors Guidance, a novel yet simple and efficient\ntraining paradigm designed to boost the performance of lightweight\nvision-language models with minimal computational overhead and lower data\ndemands. CLIP-PING bootstraps unimodal features extracted from arbitrary\npre-trained encoders to obtain intrinsic guidance of proximus neighbor samples,\ni.e., nearest-neighbor (NN) and cross nearest-neighbor (XNN). We find that\nextra contrastive supervision from these neighbors substantially boosts\ncross-modal alignment, enabling lightweight models to learn more generic\nfeatures with rich semantic diversity. Extensive experiments reveal that\nCLIP-PING notably surpasses its peers in zero-shot generalization and\ncross-modal retrieval tasks. Specifically, a 5.5% gain on zero-shot ImageNet1K\nclassification with 10.7% (I2T) and 5.7% (T2I) on Flickr30K retrieval, compared\nto the original CLIP when using ViT-XS image encoder trained on 3 million\n(image, text) pairs. Moreover, CLIP-PING showcases a strong transferability\nunder the linear evaluation protocol across several downstream tasks.",
      "tldr_zh": "本研究提出CLIP-PING，一种简单高效的训练范式，用于提升轻量级视觉语言模型（vision-language models）的性能，解决单一图像文本对比学习目标导致的跨模态特征对齐不足问题。该方法利用从任意预训练编码器提取的单模态特征，引导Proximus Intrinsic Neighbors（包括nearest-neighbor (NN)和cross nearest-neighbor (XNN)）提供额外对比监督，从而帮助模型学习更通用的特征并增强语义多样性。实验结果显示，CLIP-PING在零-shot generalization和cross-modal retrieval任务上显著优于基线模型，例如使用ViT-XS图像编码器训练后，零样本ImageNet1K分类准确率提升5.5%，Flickr30K检索任务提升10.7% (I2T) 和5.7% (T2I)。此外，该方法在下游任务中表现出强的转移能力，适用于资源受限场景。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.ET",
        "cs.IR",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "14 pages, 5 figures, 24 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.03871v2",
      "published_date": "2024-12-05 04:58:28 UTC",
      "updated_date": "2025-03-19 02:30:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:34:25.862063"
    },
    {
      "arxiv_id": "2412.03864v1",
      "title": "Training MLPs on Graphs without Supervision",
      "title_zh": "翻译失败",
      "authors": [
        "Zehong Wang",
        "Zheyuan Zhang",
        "Chuxu Zhang",
        "Yanfang Ye"
      ],
      "abstract": "Graph Neural Networks (GNNs) have demonstrated their effectiveness in various\ngraph learning tasks, yet their reliance on neighborhood aggregation during\ninference poses challenges for deployment in latency-sensitive applications,\nsuch as real-time financial fraud detection. To address this limitation, recent\nstudies have proposed distilling knowledge from teacher GNNs into student\nMulti-Layer Perceptrons (MLPs) trained on node content, aiming to accelerate\ninference. However, these approaches often inadequately explore structural\ninformation when inferring unseen nodes. To this end, we introduce SimMLP, a\nSelf-supervised framework for learning MLPs on graphs, designed to fully\nintegrate rich structural information into MLPs. Notably, SimMLP is the first\nMLP-learning method that can achieve equivalence to GNNs in the optimal case.\nThe key idea is to employ self-supervised learning to align the representations\nencoded by graph context-aware GNNs and neighborhood dependency-free MLPs,\nthereby fully integrating the structural information into MLPs. We provide a\ncomprehensive theoretical analysis, demonstrating the equivalence between\nSimMLP and GNNs based on mutual information and inductive bias, highlighting\nSimMLP's advanced structural learning capabilities. Additionally, we conduct\nextensive experiments on 20 benchmark datasets, covering node classification,\nlink prediction, and graph classification, to showcase SimMLP's superiority\nover state-of-the-art baselines, particularly in scenarios involving unseen\nnodes (e.g., inductive and cold-start node classification) where structural\ninsights are crucial. Our codes are available at:\nhttps://github.com/Zehong-Wang/SimMLP.",
      "tldr_zh": "该研究解决了Graph Neural Networks (GNNs) 在图学习任务中的推理延迟问题，提出了一种自监督框架SimMLP，用于在无监督条件下训练Multi-Layer Perceptrons (MLPs)并充分整合图的结构信息。SimMLP的关键创新是通过自监督学习对齐GNNs和MLPs的表示，实现MLPs在最优情况下与GNNs等价，并提供了基于互信息和归纳偏差的理论分析。实验在20个基准数据集上验证了SimMLP在节点分类、链接预测和图分类任务中的优越性，尤其在处理未见节点（如归纳和冷启动场景）时，显著超越现有基线方法。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by WSDM 25",
      "pdf_url": "http://arxiv.org/pdf/2412.03864v1",
      "published_date": "2024-12-05 04:20:54 UTC",
      "updated_date": "2024-12-05 04:20:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:32:43.621841"
    },
    {
      "arxiv_id": "2412.15226v1",
      "title": "Learning-by-teaching with ChatGPT: The effect of teachable ChatGPT agent on programming education",
      "title_zh": "翻译失败",
      "authors": [
        "Angxuan Chen",
        "Yuang Wei",
        "Huixiao Le",
        "Yan Zhang"
      ],
      "abstract": "This study investigates the potential of using ChatGPT as a teachable agent\nto support students' learning by teaching process, specifically in programming\neducation. While learning by teaching is an effective pedagogical strategy for\npromoting active learning, traditional teachable agents have limitations,\nparticularly in facilitating natural language dialogue. Our research explored\nwhether ChatGPT, with its ability to engage learners in natural conversations,\ncan support this process. The findings reveal that interacting with ChatGPT\nimproves students' knowledge gains and programming abilities, particularly in\nwriting readable and logically sound code. However, it had limited impact on\ndeveloping learners' error-correction skills, likely because ChatGPT tends to\ngenerate correct code, reducing opportunities for students to practice\ndebugging. Additionally, students' self-regulated learning (SRL) abilities\nimproved, suggesting that teaching ChatGPT fosters learners' higher\nself-efficacy and better implementation of SRL strategies. This study discussed\nthe role of natural dialogue in fostering socialized learning by teaching, and\nexplored ChatGPT's specific contributions in supporting students' SRL through\nthe learning by teaching process. Overall, the study highlights ChatGPT's\npotential as a teachable agent, offering insights for future research on\nChatGPT-supported education.",
      "tldr_zh": "本研究探讨了使用 ChatGPT 作为 teachable agent 来支持学生的 learning by teaching 过程，特别是在编程教育中，以克服传统代理在自然语言对话方面的局限。结果显示，与 ChatGPT 互动显著提升了学生的知识获得和编程能力，包括编写更可读和逻辑正确的代码，同时改善了 self-regulated learning (SRL) 能力，如更高的自我效能和 SRL 策略应用。However, it had limited impact on developing learners' error-correction skills，因为 ChatGPT 倾向于生成正确代码，减少了学生调试实践的机会。该研究强调了自然对话在促进社会化学习中的作用，并为未来 ChatGPT-supported education 提供了宝贵见解。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.15226v1",
      "published_date": "2024-12-05 04:12:03 UTC",
      "updated_date": "2024-12-05 04:12:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:32:56.220240"
    },
    {
      "arxiv_id": "2412.03856v1",
      "title": "How Good is ChatGPT in Giving Adaptive Guidance Using Knowledge Graphs in E-Learning Environments?",
      "title_zh": "翻译失败",
      "authors": [
        "Patrick Ocheja",
        "Brendan Flanagan",
        "Yiling Dai",
        "Hiroaki Ogata"
      ],
      "abstract": "E-learning environments are increasingly harnessing large language models\n(LLMs) like GPT-3.5 and GPT-4 for tailored educational support. This study\nintroduces an approach that integrates dynamic knowledge graphs with LLMs to\noffer nuanced student assistance. By evaluating past and ongoing student\ninteractions, the system identifies and appends the most salient learning\ncontext to prompts directed at the LLM. Central to this method is the knowledge\ngraph's role in assessing a student's comprehension of topic prerequisites.\nDepending on the categorized understanding (good, average, or poor), the LLM\nadjusts its guidance, offering advanced assistance, foundational reviews, or\nin-depth prerequisite explanations, respectively. Preliminary findings suggest\nstudents could benefit from this tiered support, achieving enhanced\ncomprehension and improved task outcomes. However, several issues related to\npotential errors arising from LLMs were identified, which can potentially\nmislead students. This highlights the need for human intervention to mitigate\nthese risks. This research aims to advance AI-driven personalized learning\nwhile acknowledging the limitations and potential pitfalls, thus guiding future\nresearch in technology and data-driven education.",
      "tldr_zh": "这篇论文评估了 ChatGPT 等大型语言模型 (LLMs) 在电子学习 (E-Learning) 环境中利用知识图谱提供自适应指导的效能。研究提出了一种方法，通过动态知识图谱评估学生的主题先决条件理解水平（分类为良好、平均或较差），并据此调整 LLM 的响应，例如提供高级帮助、基础回顾或深入解释。初步发现表明，这种分层支持能提升学生的理解和任务表现，但 LLM 可能导致错误从而误导学生，因此强调了需要人工干预来缓解风险。该研究旨在推进 AI 驱动的个性化学习，同时揭示其局限性以指导未来教育技术发展。",
      "categories": [
        "cs.AI",
        "cs.ET",
        "I.2.6; I.2.11"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03856v1",
      "published_date": "2024-12-05 04:05:43 UTC",
      "updated_date": "2024-12-05 04:05:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:33:07.458834"
    },
    {
      "arxiv_id": "2412.03854v1",
      "title": "What Do Machine Learning Researchers Mean by \"Reproducible\"?",
      "title_zh": "翻译失败",
      "authors": [
        "Edward Raff",
        "Michel Benaroch",
        "Sagar Samtani",
        "Andrew L. Farris"
      ],
      "abstract": "The concern that Artificial Intelligence (AI) and Machine Learning (ML) are\nentering a \"reproducibility crisis\" has spurred significant research in the\npast few years. Yet with each paper, it is often unclear what someone means by\n\"reproducibility\". Our work attempts to clarify the scope of \"reproducibility\"\nas displayed by the community at large. In doing so, we propose to refine the\nresearch to eight general topic areas. In this light, we see that each of these\nareas contains many works that do not advertise themselves as being about\n\"reproducibility\", in part because they go back decades before the matter came\nto broader attention.",
      "tldr_zh": "AI 和 ML 领域正面临“reproducibility crisis”（再现性危机），本研究旨在澄清研究者对“reproducible”的实际含义。论文通过分析社区范围，将再现性相关研究细分为八个一般主题，以更精确地定义其范围。研究发现，这些主题中的许多工作早在再现性问题广受关注前数十年就已存在，但并未被明确标榜为再现性研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "To appear in AAAI 2025, Senior Member Presentation Track",
      "pdf_url": "http://arxiv.org/pdf/2412.03854v1",
      "published_date": "2024-12-05 04:04:39 UTC",
      "updated_date": "2024-12-05 04:04:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:33:19.073957"
    },
    {
      "arxiv_id": "2412.03851v1",
      "title": "FedMetaMed: Federated Meta-Learning for Personalized Medication in Distributed Healthcare Systems",
      "title_zh": "翻译失败",
      "authors": [
        "Jiechao Gao",
        "Yuangang Li"
      ],
      "abstract": "Personalized medication aims to tailor healthcare to individual patient\ncharacteristics. However, the heterogeneity of patient data across healthcare\nsystems presents significant challenges to achieving accurate and effective\npersonalized treatments. Ethical concerns further complicate the aggregation of\nlarge volumes of data from diverse institutions. Federated Learning (FL) offers\na promising decentralized solution by enabling collaborative model training\nthrough the exchange of client models rather than raw data, thus preserving\nprivacy. However, existing FL methods often suffer from retrogression during\nserver aggregation, leading to a decline in model performance in real-world\nmedical FL settings. To address data variability in distributed healthcare\nsystems, we introduce Federated Meta-Learning for Personalized Medication\n(FedMetaMed), which combines federated learning and meta-learning to create\nmodels that adapt to diverse patient data across healthcare systems. The\nFedMetaMed framework aims to produce superior personalized models for\nindividual clients by addressing these limitations. Specifically, we introduce\nCumulative Fourier Aggregation (CFA) at the server to improve stability and\neffectiveness in global knowledge aggregation. CFA achieves this by gradually\nintegrating client models from low to high frequencies. At the client level, we\nimplement a Collaborative Transfer Optimization (CTO) strategy with a\nthree-step process - Retrieve, Reciprocate, and Refine - to enhance the\npersonalized local model through seamless global knowledge transfer.\nExperiments on real-world medical imaging datasets demonstrate that FedMetaMed\noutperforms state-of-the-art FL methods, showing superior generalization even\non out-of-distribution cohorts.",
      "tldr_zh": "该研究提出FedMetaMed框架，将Federated Learning (FL)和meta-learning相结合，用于分布式医疗系统中实现个性化药物治疗，以应对患者数据异质性和隐私挑战。框架在服务器端引入Cumulative Fourier Aggregation (CFA)方法，通过低到高频率逐步整合客户端模型，提升全局知识聚合的稳定性和有效性；在客户端端，采用Collaborative Transfer Optimization (CTO)策略，包括Retrieve、Reciprocate和Refine三个步骤，以优化本地模型并实现无缝知识转移。实验结果显示，在真实医疗图像数据集上，FedMetaMed比现有FL方法表现出色，具有更好的泛化能力，尤其在分布外队列中。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03851v1",
      "published_date": "2024-12-05 03:36:55 UTC",
      "updated_date": "2024-12-05 03:36:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:33:31.115017"
    },
    {
      "arxiv_id": "2412.03844v4",
      "title": "HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting",
      "title_zh": "翻译失败",
      "authors": [
        "Jingyu Lin",
        "Jiaqi Gu",
        "Lubin Fan",
        "Bojian Wu",
        "Yujing Lou",
        "Renjie Chen",
        "Ligang Liu",
        "Jieping Ye"
      ],
      "abstract": "Generating high-quality novel view renderings of 3D Gaussian Splatting (3DGS)\nin scenes featuring transient objects is challenging. We propose a novel hybrid\nrepresentation, termed as HybridGS, using 2D Gaussians for transient objects\nper image and maintaining traditional 3D Gaussians for the whole static scenes.\nNote that, the 3DGS itself is better suited for modeling static scenes that\nassume multi-view consistency, but the transient objects appear occasionally\nand do not adhere to the assumption, thus we model them as planar objects from\na single view, represented with 2D Gaussians. Our novel representation\ndecomposes the scene from the perspective of fundamental viewpoint consistency,\nmaking it more reasonable. Additionally, we present a novel multi-view\nregulated supervision method for 3DGS that leverages information from\nco-visible regions, further enhancing the distinctions between the transients\nand statics. Then, we propose a straightforward yet effective multi-stage\ntraining strategy to ensure robust training and high-quality view synthesis\nacross various settings. Experiments on benchmark datasets show our\nstate-of-the-art performance of novel view synthesis in both indoor and outdoor\nscenes, even in the presence of distracting elements.",
      "tldr_zh": "本研究提出HybridGS，一种新型混合表示方法，用于处理3D Gaussian Splatting (3DGS)在包含瞬态对象的场景中生成高质量新视图渲染的挑战。具体而言，HybridGS使用2D Gaussians表示每个图像中的瞬态对象，而保留传统的3D Gaussians来建模整个静态场景，从而从基本视角一致性的角度合理分解场景。该方法还引入多视图调节监督技术，利用共同可见区域的信息增强瞬态和静态的区分，并采用多阶段训练策略确保稳健训练和高质量视图合成。在基准数据集上的实验显示，HybridGS在室内和室外场景中实现了最先进的性能，即使存在干扰元素。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accpeted by CVPR 2025. Project page:\n  https://gujiaqivadin.github.io/hybridgs/ Code:\n  https://github.com/Yeyuqqwx/HybridGS Data:\n  https://huggingface.co/Eto63277/HybridGS/tree/main",
      "pdf_url": "http://arxiv.org/pdf/2412.03844v4",
      "published_date": "2024-12-05 03:20:35 UTC",
      "updated_date": "2025-02-28 09:49:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:35:35.189852"
    },
    {
      "arxiv_id": "2412.03841v1",
      "title": "LL-ICM: Image Compression for Low-level Machine Vision via Large Vision-Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Xue",
        "Qi Zhang",
        "Chuanmin Jia",
        "Shiqi Wang"
      ],
      "abstract": "Image Compression for Machines (ICM) aims to compress images for machine\nvision tasks rather than human viewing. Current works predominantly concentrate\non high-level tasks like object detection and semantic segmentation. However,\nthe quality of original images is usually not guaranteed in the real world,\nleading to even worse perceptual quality or downstream task performance after\ncompression. Low-level (LL) machine vision models, like image restoration\nmodels, can help improve such quality, and thereby their compression\nrequirements should also be considered. In this paper, we propose a pioneered\nICM framework for LL machine vision tasks, namely LL-ICM. By jointly optimizing\ncompression and LL tasks, the proposed LL-ICM not only enriches its encoding\nability in generalizing to versatile LL tasks but also optimizes the processing\nability of down-stream LL task models, achieving mutual adaptation for image\ncodecs and LL task models. Furthermore, we integrate large-scale\nvision-language models into the LL-ICM framework to generate more universal and\ndistortion-robust feature embeddings for LL vision tasks. Therefore, one LL-ICM\ncodec can generalize to multiple tasks. We establish a solid benchmark to\nevaluate LL-ICM, which includes extensive objective experiments by using both\nfull and no-reference image quality assessments. Experimental results show that\nLL-ICM can achieve 22.65% BD-rate reductions over the state-of-the-art methods.",
      "tldr_zh": "这篇论文提出了 LL-ICM 框架，用于针对低水平（Low-level）机器视觉任务（如图像修复）的图像压缩（ICM），以解决原始图像质量问题对下游任务的影响。该框架通过联合优化图像压缩和任务模型，并整合大型视觉语言模型（Large Vision-Language Model）生成通用且鲁棒的特征嵌入，实现图像编解码器与任务模型的相互适应，从而提升泛化能力。实验结果显示，LL-ICM 在基准测试中比最先进方法减少 22.65% 的 BD-rate，并通过全面的图像质量评估验证了其有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03841v1",
      "published_date": "2024-12-05 03:12:45 UTC",
      "updated_date": "2024-12-05 03:12:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:35:55.535941"
    },
    {
      "arxiv_id": "2412.03837v1",
      "title": "Movie Gen: SWOT Analysis of Meta's Generative AI Foundation Model for Transforming Media Generation, Advertising, and Entertainment Industries",
      "title_zh": "翻译失败",
      "authors": [
        "Abul Ehtesham",
        "Saket Kumar",
        "Aditi Singh",
        "Tala Talaei Khoei"
      ],
      "abstract": "Generative AI is reshaping the media landscape, enabling unprecedented\ncapabilities in video creation, personalization, and scalability. This paper\npresents a comprehensive SWOT analysis of Metas Movie Gen, a cutting-edge\ngenerative AI foundation model designed to produce 1080p HD videos with\nsynchronized audio from simple text prompts. We explore its strengths,\nincluding high-resolution video generation, precise editing, and seamless audio\nintegration, which make it a transformative tool across industries such as\nfilmmaking, advertising, and education. However, the analysis also addresses\nlimitations, such as constraints on video length and potential biases in\ngenerated content, which pose challenges for broader adoption. In addition, we\nexamine the evolving regulatory and ethical considerations surrounding\ngenerative AI, focusing on issues like content authenticity, cultural\nrepresentation, and responsible use. Through comparative insights with leading\nmodels like DALL-E and Google Imagen, this paper highlights Movie Gens unique\nfeatures, such as video personalization and multimodal synthesis, while\nidentifying opportunities for innovation and areas requiring further research.\nOur findings provide actionable insights for stakeholders, emphasizing both the\nopportunities and challenges of deploying generative AI in media production.\nThis work aims to guide future advancements in generative AI, ensuring\nscalability, quality, and ethical integrity in this rapidly evolving field.",
      "tldr_zh": "这篇论文对 Meta 的 Movie Gen 生成式 AI 基础模型进行了全面 SWOT 分析，评估其从简单文本提示生成 1080p HD 视频和同步音频的能力，以及在媒体生成、广告和娱乐行业的变革潜力。优势包括高分辨率视频生成、精确编辑和无缝音频整合，但存在视频长度限制和内容偏见等挑战，可能影响广泛采用。论文通过与 DALL-E 和 Google Imagen 的比较，突出了 Movie Gen 的独特功能如视频个性化与多模态合成，并探讨了监管和伦理问题，如内容真实性、文化代表和负责任使用。该研究为利益相关者提供了可操作洞见，指导生成式 AI 的未来发展，确保可扩展性、质量和伦理完整性。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03837v1",
      "published_date": "2024-12-05 03:01:53 UTC",
      "updated_date": "2024-12-05 03:01:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:36:10.468779"
    },
    {
      "arxiv_id": "2412.03824v1",
      "title": "Towards Data Governance of Frontier AI Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jason Hausenloy",
        "Duncan McClements",
        "Madhavendra Thakur"
      ],
      "abstract": "Data is essential to train and fine-tune today's frontier artificial\nintelligence (AI) models and to develop future ones. To date, academic, legal,\nand regulatory work has primarily addressed how data can directly harm\nconsumers and creators, such as through privacy breaches, copyright\ninfringements, and bias and discrimination. Our work, instead, focuses on the\ncomparatively neglected question of how data can enable new governance\ncapacities for frontier AI models. This approach for \"frontier data governance\"\nopens up new avenues for monitoring and mitigating risks from advanced AI\nmodels, particularly as they scale and acquire specific dangerous capabilities.\nStill, frontier data governance faces challenges that stem from the fundamental\nproperties of data itself: data is non-rival, often non-excludable, easily\nreplicable, and increasingly synthesizable. Despite these inherent\ndifficulties, we propose a set of policy mechanisms targeting key actors along\nthe data supply chain, including data producers, aggregators, model developers,\nand data vendors. We provide a brief overview of 15 governance mechanisms, of\nwhich we centrally introduce five, underexplored policy recommendations. These\ninclude developing canary tokens to detect unauthorized use for producers;\n(automated) data filtering to remove malicious content for pre-training and\npost-training datasets; mandatory dataset reporting requirements for developers\nand vendors; improved security for datasets and data generation algorithms; and\nknow-your-customer requirements for vendors. By considering data not just as a\nsource of potential harm, but as a critical governance lever, this work aims to\nequip policymakers with a new tool for the governance and regulation of\nfrontier AI models.",
      "tldr_zh": "本文探讨了数据在前沿 AI 模型（frontier AI models）治理中的关键作用，强调数据不仅仅是潜在危害来源（如隐私泄露和偏见），还可作为监控和缓解 AI 风险的治理杠杆。作者分析了数据本身的特性（如非竞争性、非排他性和易复制性）带来的挑战，并提出针对数据供应链关键参与者（包括数据生产者、聚合者、模型开发者和服务商）的 15 种治理机制。重点推荐五种未充分探索的政策：开发 canary tokens 检测未授权使用、自动化数据过滤移除恶意内容、强制数据集报告要求、提升数据集和数据生成算法的安全性，以及 know-your-customer 要求。这些机制旨在为政策制定者提供新工具，帮助管理先进 AI 模型的扩展和潜在危险能力。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03824v1",
      "published_date": "2024-12-05 02:37:51 UTC",
      "updated_date": "2024-12-05 02:37:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:36:23.202021"
    },
    {
      "arxiv_id": "2412.03822v1",
      "title": "Beyond the Binary: Capturing Diverse Preferences With Reward Regularization",
      "title_zh": "超越二元：通过奖励正则化捕捉多样偏好",
      "authors": [
        "Vishakh Padmakumar",
        "Chuanyang Jin",
        "Hannah Rose Kirk",
        "He He"
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed via public-facing\ninterfaces to interact with millions of users, each with diverse preferences.\nDespite this, preference tuning of LLMs predominantly relies on reward models\ntrained using binary judgments where annotators select the preferred choice out\nof pairs of model outputs. In this work, we argue that this reliance on binary\nchoices does not capture the broader, aggregate preferences of the target user\nin real-world tasks. We propose a taxonomy that identifies two dimensions of\nsubjectivity where different users disagree on the preferred output-namely, the\nPlurality of Responses to Prompts, where prompts allow for multiple correct\nanswers, and the Indistinguishability of Responses, where candidate outputs are\nparaphrases of each other. We show that reward models correlate weakly with\nuser preferences in these cases. As a first step to address this issue, we\nintroduce a simple yet effective method that augments existing binary\npreference datasets with synthetic preference judgments to estimate potential\nuser disagreement. Incorporating these via a margin term as a form of\nregularization during model training yields predictions that better align with\nthe aggregate user preferences.",
      "tldr_zh": "本研究指出，大型语言模型 (LLMs) 的偏好调整主要依赖基于二元判断的奖励模型，但这无法充分捕捉真实世界任务中用户多样化偏好的聚合。作者提出一个分类框架，识别出两种主观性维度：Prompts 的 Plurality of Responses（提示允许多个正确答案）和 Indistinguishability of Responses（候选输出为彼此改写），并证明现有奖励模型在这些场景下与用户偏好相关性较弱。为解决此问题，研究引入一种简单方法，通过在二元偏好数据集上添加合成偏好判断来估计用户分歧，并以 margin term 形式作为正则化（reward regularization）整合到模型训练中。实验结果显示，这种方法显著提高了模型预测与用户偏好的对齐度。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03822v1",
      "published_date": "2024-12-05 02:35:46 UTC",
      "updated_date": "2024-12-05 02:35:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:36:34.344923"
    },
    {
      "arxiv_id": "2412.03815v1",
      "title": "Synergizing LLMs and Knowledge Graphs: A Novel Approach to Software Repository-Related Question Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Samuel Abedu",
        "SayedHassan Khatoonabadi",
        "Emad Shihab"
      ],
      "abstract": "Software repositories contain valuable information for gaining insights into\ntheir development process. However, extracting insights from these repository\ndata is time-consuming and requires technical expertise. While software\nengineering chatbots have been developed to facilitate natural language\ninteractions with repositories, they struggle with understanding natural\nlanguage and accurately retrieving relevant data. This study aims to improve\nthe accuracy of LLM-based chatbots in answering repository-related questions by\naugmenting them with knowledge graphs. We achieve this in a two-step approach;\n(1) constructing a knowledge graph from the repository data and (2) synergizing\nthe knowledge graph with LLM to allow for the natural language questions and\nanswers. We curated a set of 20 questions with different complexities and\nevaluated our approach on five popular open-source projects. Our approach\nachieved an accuracy of 65%. We further investigated the limitations and\nidentified six key issues, with the majority relating to the reasoning\ncapability of the LLM. We experimented with a few-shot chain-of-thought\nprompting to determine if it could enhance our approach. This technique\nimproved the overall accuracy to 84%. Our findings demonstrate the synergy\nbetween LLMs and knowledge graphs as a viable solution for making repository\ndata accessible to both technical and non-technical stakeholders.",
      "tldr_zh": "本研究提出了一种新方法，将大型语言模型（LLMs）和知识图谱（Knowledge Graphs）协同，用于提升软件仓库相关问答的准确性，以解决现有聊天机器人理解自然语言和数据检索的不足。方法包括从仓库数据构建知识图谱，并将其与LLMs整合，支持自然语言交互；在五个开源项目上测试20个不同复杂度的提问，初始准确率达65%。通过引入few-shot chain-of-thought prompting技术，进一步将准确率提高至84%，并识别了六大限制因素，主要与LLMs的推理能力相关。该方法证明了LLMs和知识图谱的协同作用，能使软件仓库数据更易于技术和非技术人员访问。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "Submitted to ACM Transactions on Software Engineering and Methodology\n  for review",
      "pdf_url": "http://arxiv.org/pdf/2412.03815v1",
      "published_date": "2024-12-05 02:18:03 UTC",
      "updated_date": "2024-12-05 02:18:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:36:46.954833"
    },
    {
      "arxiv_id": "2412.03801v1",
      "title": "Agent AI with LangGraph: A Modular Framework for Enhancing Machine Translation Using Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Jialin Wang",
        "Zhihua Duan"
      ],
      "abstract": "This paper explores the transformative role of Agent AI and LangGraph in\nadvancing the automation and effectiveness of machine translation (MT). Agents\nare modular components designed to perform specific tasks, such as translating\nbetween particular languages, with specializations like TranslateEnAgent,\nTranslateFrenchAgent, and TranslateJpAgent for English, French, and Japanese\ntranslations, respectively. These agents leverage the powerful semantic\ncapabilities of large language models (LLMs), such as GPT-4o, to ensure\naccurate, contextually relevant translations while maintaining modularity,\nscalability, and context retention.\n  LangGraph, a graph-based framework built on LangChain, simplifies the\ncreation and management of these agents and their workflows. It supports\ndynamic state management, enabling agents to maintain dialogue context and\nautomates complex workflows by linking agents and facilitating their\ncollaboration. With flexibility, open-source community support, and seamless\nintegration with LLMs, LangGraph empowers agents to deliver high-quality\ntranslations.\n  Together, Agent AI and LangGraph create a cohesive system where LangGraph\norchestrates agent interactions, ensuring that user inputs are analyzed,\nrouted, and processed efficiently. Experimental results demonstrate the\npotential of this system to enhance multilingual translation accuracy and\nscalability. By highlighting modular design and automated workflows, this paper\nsets the stage for further innovations in intelligent machine translation\nservices.",
      "tldr_zh": "该论文提出了一种模块化框架，结合 Agent AI 和 LangGraph，利用大型语言模型（LLMs，如 GPT-4o）来提升机器翻译（MT）的自动化和有效性。Agent AI 由特定任务代理（如 TranslateEnAgent、TranslateFrenchAgent 和 TranslateJpAgent）组成，这些代理确保翻译准确且上下文相关，同时保持系统模块化和可扩展性。LangGraph 作为基于 LangChain 的图-based 框架，负责动态管理代理工作流、维护对话上下文并协调代理协作。实验结果表明，该系统显著提高了多语言翻译的准确性和可扩展性，为智能机器翻译服务的创新奠定了基础。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03801v1",
      "published_date": "2024-12-05 01:45:12 UTC",
      "updated_date": "2024-12-05 01:45:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:36:59.715883"
    },
    {
      "arxiv_id": "2412.03800v1",
      "title": "ELEMENT: Episodic and Lifelong Exploration via Maximum Entropy",
      "title_zh": "ELEMENT：通过最大熵的分段和终身探索",
      "authors": [
        "Hongming Li",
        "Shujian Yu",
        "Bin Liu",
        "Jose C. Principe"
      ],
      "abstract": "This paper proposes \\emph{Episodic and Lifelong Exploration via Maximum\nENTropy} (ELEMENT), a novel, multiscale, intrinsically motivated reinforcement\nlearning (RL) framework that is able to explore environments without using any\nextrinsic reward and transfer effectively the learned skills to downstream\ntasks. We advance the state of the art in three ways. First, we propose a\nmultiscale entropy optimization to take care of the fact that previous maximum\nstate entropy, for lifelong exploration with millions of state observations,\nsuffers from vanishing rewards and becomes very expensive computationally\nacross iterations. Therefore, we add an episodic maximum entropy over each\nepisode to speedup the search further. Second, we propose a novel intrinsic\nreward for episodic entropy maximization named \\emph{average episodic state\nentropy} which provides the optimal solution for a theoretical upper bound of\nthe episodic state entropy objective. Third, to speed the lifelong entropy\nmaximization, we propose a $k$ nearest neighbors ($k$NN) graph to organize the\nestimation of the entropy and updating processes that reduces the computation\nsubstantially. Our ELEMENT significantly outperforms state-of-the-art intrinsic\nrewards in both episodic and lifelong setups. Moreover, it can be exploited in\ntask-agnostic pre-training, collecting data for offline reinforcement learning,\netc.",
      "tldr_zh": "这篇论文提出了 ELEMENT 框架，一种基于最大熵的多尺度内部动机强化学习 (RL) 方法，用于无需外部奖励的环境探索，并将学到的技能转移到下游任务。关键创新包括多尺度熵优化（通过添加每集最大熵来解决奖励消失和计算开销问题）、新的平均每集状态熵 (average episodic state entropy) 内部奖励（提供理论上界的最优解），以及使用 k 最近邻 (kNN) 图来加速终身熵最大化过程。实验结果显示，ELEMENT 在 episodic 和 lifelong 设置中显著优于现有内部奖励方法，并可应用于任务无关预训练和离线 RL 数据收集。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03800v1",
      "published_date": "2024-12-05 01:42:13 UTC",
      "updated_date": "2024-12-05 01:42:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:37:10.374898"
    },
    {
      "arxiv_id": "2412.03796v1",
      "title": "Automated Multi-Label Annotation for Mental Health Illnesses Using Large Language Models",
      "title_zh": "利用大语言模型的心理健康疾病自动多标签标注",
      "authors": [
        "Abdelrahaman A. Hassan",
        "Radwa J. Hanafy",
        "Mohammed E. Fouda"
      ],
      "abstract": "The growing prevalence and complexity of mental health disorders present\nsignificant challenges for accurate diagnosis and treatment, particularly in\nunderstanding the interplay between co-occurring conditions. Mental health\ndisorders, such as depression and Anxiety, often co-occur, yet current datasets\nderived from social media posts typically focus on single-disorder labels,\nlimiting their utility in comprehensive diagnostic analyses. This paper\naddresses this critical gap by proposing a novel methodology for cleaning,\nsampling, labeling, and combining data to create versatile multi-label\ndatasets. Our approach introduces a synthetic labeling technique to transform\nsingle-label datasets into multi-label annotations, capturing the complexity of\noverlapping mental health conditions. To achieve this, two single-label\ndatasets are first merged into a foundational multi-label dataset, enabling\nrealistic analyses of co-occurring diagnoses. We then design and evaluate\nvarious prompting strategies for large language models (LLMs), ranging from\nsingle-label predictions to unrestricted prompts capable of detecting any\npresent disorders. After rigorously assessing multiple LLMs and prompt\nconfigurations, the optimal combinations are identified and applied to label\nsix additional single-disorder datasets from RMHD. The result is SPAADE-DR, a\nrobust, multi-label dataset encompassing diverse mental health conditions. This\nresearch demonstrates the transformative potential of LLM-driven synthetic\nlabeling in advancing mental health diagnostics from social media data, paving\nthe way for more nuanced, data-driven insights into mental health care.",
      "tldr_zh": "本论文针对精神健康障碍的共存性挑战，提出了一种使用 Large Language Models (LLMs) 的自动多标签标注方法，以解决现有社交媒体数据集仅限于单一障碍标签的局限性。方法包括合成标注技术，通过合并两个单一标签数据集形成基础多标签数据集，并设计多种提示策略（如单一标签预测和不受限制提示）来优化 LLMs 的性能。经评估多个 LLMs 和提示配置后，该方法应用于 RMHD 中的六个额外数据集，最终创建了 SPAADE-DR，这是一个覆盖多种精神健康条件的健壮多标签数据集。该研究展示了 LLM 驱动的合成标注在从社交媒体数据中提升诊断分析的潜力，为更细致的心理健康护理提供数据支持。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03796v1",
      "published_date": "2024-12-05 01:33:03 UTC",
      "updated_date": "2024-12-05 01:33:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:37:23.460852"
    },
    {
      "arxiv_id": "2412.03792v1",
      "title": "Safe Adaptive Cruise Control Under Perception Uncertainty: A Deep Ensemble and Conformal Tube Model Predictive Control Approach",
      "title_zh": "翻译失败",
      "authors": [
        "Xiao Li",
        "Anouck Girard",
        "Ilya Kolmanovsky"
      ],
      "abstract": "Autonomous driving heavily relies on perception systems to interpret the\nenvironment for decision-making. To enhance robustness in these safety critical\napplications, this paper considers a Deep Ensemble of Deep Neural Network\nregressors integrated with Conformal Prediction to predict and quantify\nuncertainties. In the Adaptive Cruise Control setting, the proposed method\nperforms state and uncertainty estimation from RGB images, informing the\ndownstream controller of the DNN perception uncertainties. An adaptive cruise\ncontroller using Conformal Tube Model Predictive Control is designed to ensure\nprobabilistic safety. Evaluations with a high-fidelity simulator demonstrate\nthe algorithm's effectiveness in speed tracking and safe distance maintaining,\nincluding in Out-Of-Distribution scenarios.",
      "tldr_zh": "本文提出了一种处理感知不确定性的自适应巡航控制方法，结合 Deep Ensemble 和 Conformal Prediction 技术，利用 Deep Neural Network 回归器从 RGB 图像中估计车辆状态和不确定性，并将这些信息传递给下游控制器。方法设计了基于 Conformal Tube Model Predictive Control 的控制器，以确保概率安全，并在速度跟踪和保持安全距离方面实现稳健性能。通过高保真模拟器评估，结果显示该算法在包括 Out-Of-Distribution 场景在内的多种情况下，比传统方法更有效。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2412.03792v1",
      "published_date": "2024-12-05 01:01:53 UTC",
      "updated_date": "2024-12-05 01:01:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:37:34.624991"
    },
    {
      "arxiv_id": "2412.03791v1",
      "title": "Coordinate In and Value Out: Training Flow Transformers in Ambient Space",
      "title_zh": "翻译失败",
      "authors": [
        "Yuyang Wang",
        "Anurag Ranjan",
        "Josh Susskind",
        "Miguel Angel Bautista"
      ],
      "abstract": "Flow matching models have emerged as a powerful method for generative\nmodeling on domains like images or videos, and even on unstructured data like\n3D point clouds. These models are commonly trained in two stages: first, a data\ncompressor (i.e., a variational auto-encoder) is trained, and in a subsequent\ntraining stage a flow matching generative model is trained in the\nlow-dimensional latent space of the data compressor. This two stage paradigm\nadds complexity to the overall training recipe and sets obstacles for unifying\nmodels across data domains, as specific data compressors are used for different\ndata modalities. To this end, we introduce Ambient Space Flow Transformers\n(ASFT), a domain-agnostic approach to learn flow matching transformers in\nambient space, sidestepping the requirement of training compressors and\nsimplifying the training process. We introduce a conditionally independent\npoint-wise training objective that enables ASFT to make predictions\ncontinuously in coordinate space. Our empirical results demonstrate that using\ngeneral purpose transformer blocks, ASFT effectively handles different data\nmodalities such as images and 3D point clouds, achieving strong performance in\nboth domains and outperforming comparable approaches. ASFT is a promising step\ntowards domain-agnostic flow matching generative models that can be trivially\nadopted in different data domains.",
      "tldr_zh": "该论文提出Ambient Space Flow Transformers (ASFT)，一种域无关的方法，直接在环境空间训练Flow Matching Transformers，绕过传统两阶段训练中数据压缩器的需求，从而简化流程并统一跨数据模态模型。ASFT引入条件独立的点-wise训练目标，实现坐标空间的连续预测，适用于图像和3D点云等不同领域。实验结果表明，使用通用Transformer块，ASFT在这些模态上表现出色，优于可比方法，并为易于适配的Flow Matching生成模型奠定基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "23 pages, 10 figures, 10 tables",
      "pdf_url": "http://arxiv.org/pdf/2412.03791v1",
      "published_date": "2024-12-05 01:00:07 UTC",
      "updated_date": "2024-12-05 01:00:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:37:45.970758"
    },
    {
      "arxiv_id": "2412.03784v1",
      "title": "Speech Recognition-based Feature Extraction for Enhanced Automatic Severity Classification in Dysarthric Speech",
      "title_zh": "翻译失败",
      "authors": [
        "Yerin Choi",
        "Jeehyun Lee",
        "Myoung-Wan Koo"
      ],
      "abstract": "Due to the subjective nature of current clinical evaluation, the need for\nautomatic severity evaluation in dysarthric speech has emerged. DNN models\noutperform ML models but lack user-friendly explainability. ML models offer\nexplainable results at a feature level, but their performance is comparatively\nlower. Current ML models extract various features from raw waveforms to predict\nseverity. However, existing methods do not encompass all dysarthric features\nused in clinical evaluation. To address this gap, we propose a feature\nextraction method that minimizes information loss. We introduce an ASR\ntranscription as a novel feature extraction source. We finetune the ASR model\nfor dysarthric speech, then use this model to transcribe dysarthric speech and\nextract word segment boundary information. It enables capturing finer\npronunciation and broader prosodic features. These features demonstrated an\nimproved severity prediction performance to existing features: balanced\naccuracy of 83.72%.",
      "tldr_zh": "该研究针对语言障碍（dysarthric speech）的严重程度评估问题，提出了一种基于语音识别（ASR）的特征提取方法，以弥补现有 ML 模型性能较低且未覆盖所有临床特征的不足。方法包括微调 ASR 模型用于 dysarthric speech，然后从 ASR 转录中提取单词边界信息，从而捕获更细致的发音和更全面的韵律特征。相比传统从原始波形提取特征，该方法最小化信息损失，并在严重程度预测中实现了 balanced accuracy 达 83.72%，显著提升了模型的可解释性和准确性。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted to SLT 2024",
      "pdf_url": "http://arxiv.org/pdf/2412.03784v1",
      "published_date": "2024-12-05 00:12:53 UTC",
      "updated_date": "2024-12-05 00:12:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:37:58.569908"
    },
    {
      "arxiv_id": "2412.03783v2",
      "title": "Expressivity of Representation Learning on Continuous-Time Dynamic Graphs: An Information-Flow Centric Review",
      "title_zh": "连续时间动态图上表示学习的表达性：以信息流为中心的综述",
      "authors": [
        "Sofiane Ennadir",
        "Gabriela Zarzar Gandler",
        "Filip Cornell",
        "Lele Cao",
        "Oleg Smirnov",
        "Tianze Wang",
        "Levente Zólyomi",
        "Björn Brinne",
        "Sahar Asadi"
      ],
      "abstract": "Graphs are ubiquitous in real-world applications, ranging from social\nnetworks to biological systems, and have inspired the development of Graph\nNeural Networks (GNNs) for learning expressive representations. While most\nresearch has centered on static graphs, many real-world scenarios involve\ndynamic, temporally evolving graphs, motivating the need for Continuous-Time\nDynamic Graph (CTDG) models. This paper provides a comprehensive review of\nGraph Representation Learning (GRL) on CTDGs with a focus on Self-Supervised\nRepresentation Learning (SSRL). We introduce a novel theoretical framework that\nanalyzes the expressivity of CTDG models through an Information-Flow (IF) lens,\nquantifying their ability to propagate and encode temporal and structural\ninformation. Leveraging this framework, we categorize existing CTDG methods\nbased on their suitability for different graph types and application scenarios.\nWithin the same scope, we examine the design of SSRL methods tailored to CTDGs,\nsuch as predictive and contrastive approaches, highlighting their potential to\nmitigate the reliance on labeled data. Empirical evaluations on synthetic and\nreal-world datasets validate our theoretical insights, demonstrating the\nstrengths and limitations of various methods across long-range, bi-partite and\ncommunity-based graphs. This work offers both a theoretical foundation and\npractical guidance for selecting and developing CTDG models, advancing the\nunderstanding of GRL in dynamic settings.",
      "tldr_zh": "这篇论文审视了在连续时间动态图（Continuous-Time Dynamic Graphs, CTDGs）上进行图表示学习（Graph Representation Learning, GRL）的表达能力，特别聚焦于自监督表示学习（Self-Supervised Representation Learning, SSRL）。作者引入了一个基于信息流（Information-Flow, IF）的理论框架，来量化CTDG模型在传播和编码时间及结构信息方面的能力，并据此分类现有方法以适应不同图类型和应用场景。论文还探讨了针对CTDGs的SSRL设计，如预测和对比方法，以减少对标注数据的依赖，并通过在合成和真实数据集上的经验评估验证了这些方法的优势和局限性，为动态图场景下的GRL模型选择和发展提供理论基础和实用指导。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "68R10, 05Cxx, 68Txx",
        "I.2.6; I.5.1; G.2.2"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by TMLR. Source code: https://github.com/king/ctdg-info-flow",
      "pdf_url": "http://arxiv.org/pdf/2412.03783v2",
      "published_date": "2024-12-05 00:12:50 UTC",
      "updated_date": "2025-04-14 12:21:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-21T08:38:12.150733"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 131,
  "processed_papers_count": 131,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-21T08:38:41.702780"
}
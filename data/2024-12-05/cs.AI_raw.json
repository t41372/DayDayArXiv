[
  {
    "arxiv_id": "2412.04673v1",
    "title": "Socially-Informed Reconstruction for Pedestrian Trajectory Forecasting",
    "authors": [
      "Haleh Damirchi",
      "Ali Etemad",
      "Michael Greenspan"
    ],
    "abstract": "Pedestrian trajectory prediction remains a challenge for autonomous systems,\nparticularly due to the intricate dynamics of social interactions. Accurate\nforecasting requires a comprehensive understanding not only of each\npedestrian's previous trajectory but also of their interaction with the\nsurrounding environment, an important part of which are other pedestrians\nmoving dynamically in the scene. To learn effective socially-informed\nrepresentations, we propose a model that uses a reconstructor alongside a\nconditional variational autoencoder-based trajectory forecasting module. This\nmodule generates pseudo-trajectories, which we use as augmentations throughout\nthe training process. To further guide the model towards social awareness, we\npropose a novel social loss that aids in forecasting of more stable\ntrajectories. We validate our approach through extensive experiments,\ndemonstrating strong performances in comparison to state of-the-art methods on\nthe ETH/UCY and SDD benchmarks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at Winter Conference on Applications of Computer Vision\n  (WACV), 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.04673v1",
    "published_date": "2024-12-05 23:54:06 UTC",
    "updated_date": "2024-12-05 23:54:06 UTC"
  },
  {
    "arxiv_id": "2412.04671v3",
    "title": "Fully Distributed, Flexible Compositional Visual Representations via Soft Tensor Products",
    "authors": [
      "Bethia Sun",
      "Maurice Pagnucco",
      "Yang Song"
    ],
    "abstract": "Since the inception of the classicalist vs. connectionist debate, it has been\nargued that the ability to systematically combine symbol-like entities into\ncompositional representations is crucial for human intelligence. In\nconnectionist systems, the field of disentanglement has gained prominence for\nits ability to produce explicitly compositional representations; however, it\nrelies on a fundamentally symbolic, concatenative representation of\ncompositional structure that clashes with the continuous, distributed\nfoundations of deep learning. To resolve this tension, we extend Smolensky's\nTensor Product Representation (TPR) and introduce Soft TPR, a representational\nform that encodes compositional structure in an inherently distributed,\nflexible manner, along with Soft TPR Autoencoder, a theoretically-principled\narchitecture designed specifically to learn Soft TPRs. Comprehensive\nevaluations in the visual representation learning domain demonstrate that the\nSoft TPR framework consistently outperforms conventional disentanglement\nalternatives -- achieving state-of-the-art disentanglement, boosting\nrepresentation learner convergence, and delivering superior sample efficiency\nand low-sample regime performance in downstream tasks. These findings highlight\nthe promise of a distributed and flexible approach to representing\ncompositional structure by potentially enhancing alignment with the core\nprinciples of deep learning over the conventional symbolic approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to Neurips 2024. 10 pages + supplementary",
    "pdf_url": "http://arxiv.org/pdf/2412.04671v3",
    "published_date": "2024-12-05 23:47:58 UTC",
    "updated_date": "2025-01-23 01:05:05 UTC"
  },
  {
    "arxiv_id": "2412.04668v1",
    "title": "Diffusion-Augmented Coreset Expansion for Scalable Dataset Distillation",
    "authors": [
      "Ali Abbasi",
      "Shima Imani",
      "Chenyang An",
      "Gayathri Mahalingam",
      "Harsh Shrivastava",
      "Maurice Diesendruck",
      "Hamed Pirsiavash",
      "Pramod Sharma",
      "Soheil Kolouri"
    ],
    "abstract": "With the rapid scaling of neural networks, data storage and communication\ndemands have intensified. Dataset distillation has emerged as a promising\nsolution, condensing information from extensive datasets into a compact set of\nsynthetic samples by solving a bilevel optimization problem. However, current\nmethods face challenges in computational efficiency, particularly with\nhigh-resolution data and complex architectures. Recently,\nknowledge-distillation-based dataset condensation approaches have made this\nprocess more computationally feasible. Yet, with the recent developments of\ngenerative foundation models, there is now an opportunity to achieve even\ngreater compression, enhance the quality of distilled data, and introduce\nvaluable diversity into the data representation. In this work, we propose a\ntwo-stage solution. First, we compress the dataset by selecting only the most\ninformative patches to form a coreset. Next, we leverage a generative\nfoundation model to dynamically expand this compressed set in real-time,\nenhancing the resolution of these patches and introducing controlled\nvariability to the coreset. Our extensive experiments demonstrate the\nrobustness and efficiency of our approach across a range of dataset\ndistillation benchmarks. We demonstrate a significant improvement of over 10%\ncompared to the state-of-the-art on several large-scale dataset distillation\nbenchmarks. The code will be released soon.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04668v1",
    "published_date": "2024-12-05 23:40:27 UTC",
    "updated_date": "2024-12-05 23:40:27 UTC"
  },
  {
    "arxiv_id": "2412.04664v2",
    "title": "Multiclass Post-Earthquake Building Assessment Integrating Optical and SAR Satellite Imagery, Ground Motion, and Soil Data with Transformers",
    "authors": [
      "Deepank Singh",
      "Vedhus Hoskere",
      "Pietro Milillo"
    ],
    "abstract": "Timely and accurate assessments of building damage are crucial for effective\nresponse and recovery in the aftermath of earthquakes. Conventional preliminary\ndamage assessments (PDA) often rely on manual door-to-door inspections, which\nare not only time-consuming but also pose significant safety risks. To safely\nexpedite the PDA process, researchers have studied the applicability of\nsatellite imagery processed with heuristic and machine learning approaches.\nThese approaches output binary or, more recently, multiclass damage states at\nthe scale of a block or a single building. However, the current performance of\nsuch approaches limits practical applicability. To address this limitation, we\nintroduce a metadata-enriched, transformer based framework that combines\nhigh-resolution post-earthquake satellite imagery with building-specific\nmetadata relevant to the seismic performance of the structure. Our model\nachieves state-of-the-art performance in multiclass post-earthquake damage\nidentification for buildings from the Turkey-Syria earthquake on February 6,\n2023. Specifically, we demonstrate that incorporating metadata, such as seismic\nintensity indicators, soil properties, and SAR damage proxy maps not only\nenhances the model's accuracy and ability to distinguish between damage\nclasses, but also improves its generalizability across various regions.\nFurthermore, we conducted a detailed, class-wise analysis of feature importance\nto understand the model's decision-making across different levels of building\ndamage. This analysis reveals how individual metadata features uniquely\ncontribute to predictions for each damage class. By leveraging both satellite\nimagery and metadata, our proposed framework enables faster and more accurate\ndamage assessments for precise, multiclass, building-level evaluations that can\nimprove disaster response and accelerate recovery efforts for affected\ncommunities.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "28 Pages, 12 Figures",
    "pdf_url": "http://arxiv.org/pdf/2412.04664v2",
    "published_date": "2024-12-05 23:19:51 UTC",
    "updated_date": "2025-02-26 16:49:41 UTC"
  },
  {
    "arxiv_id": "2412.04661v1",
    "title": "HEAL: Hierarchical Embedding Alignment Loss for Improved Retrieval and Representation Learning",
    "authors": [
      "Manish Bhattarai",
      "Ryan Barron",
      "Maksim Eren",
      "Minh Vu",
      "Vesselin Grantcharov",
      "Ismael Boureima",
      "Valentin Stanev",
      "Cynthia Matuszek",
      "Vladimir Valtchinov",
      "Kim Rasmussen",
      "Boian Alexandrov"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external document retrieval to provide domain-specific or\nup-to-date knowledge. The effectiveness of RAG depends on the relevance of\nretrieved documents, which is influenced by the semantic alignment of\nembeddings with the domain's specialized content. Although full fine-tuning can\nalign language models to specific domains, it is computationally intensive and\ndemands substantial data. This paper introduces Hierarchical Embedding\nAlignment Loss (HEAL), a novel method that leverages hierarchical fuzzy\nclustering with matrix factorization within contrastive learning to efficiently\nalign LLM embeddings with domain-specific content. HEAL computes\nlevel/depth-wise contrastive losses and incorporates hierarchical penalties to\nalign embeddings with the underlying relationships in label hierarchies. This\napproach enhances retrieval relevance and document classification, effectively\nreducing hallucinations in LLM outputs. In our experiments, we benchmark and\nevaluate HEAL across diverse domains, including Healthcare, Material Science,\nCyber-security, and Applied Maths.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04661v1",
    "published_date": "2024-12-05 23:10:56 UTC",
    "updated_date": "2024-12-05 23:10:56 UTC"
  },
  {
    "arxiv_id": "2412.04655v2",
    "title": "From Models to Systems: A Comprehensive Fairness Framework for Compositional Recommender Systems",
    "authors": [
      "Brian Hsu",
      "Cyrus DiCiccio",
      "Natesh Sivasubramoniapillai",
      "Hongseok Namkoong"
    ],
    "abstract": "Fairness research in machine learning often centers on ensuring equitable\nperformance of individual models. However, real-world recommendation systems\nare built on multiple models and even multiple stages, from candidate retrieval\nto scoring and serving, which raises challenges for responsible development and\ndeployment. This system-level view, as highlighted by regulations like the EU\nAI Act, necessitates moving beyond auditing individual models as independent\nentities. We propose a holistic framework for modeling system-level fairness,\nfocusing on the end-utility delivered to diverse user groups, and consider\ninteractions between components such as retrieval and scoring models. We\nprovide formal insights on the limitations of focusing solely on model-level\nfairness and highlight the need for alternative tools that account for\nheterogeneity in user preferences. To mitigate system-level disparities, we\nadapt closed-box optimization tools (e.g., BayesOpt) to jointly optimize\nutility and equity. We empirically demonstrate the effectiveness of our\nproposed framework on synthetic and real datasets, underscoring the need for a\nsystem-level framework.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04655v2",
    "published_date": "2024-12-05 22:59:26 UTC",
    "updated_date": "2025-01-02 17:21:22 UTC"
  },
  {
    "arxiv_id": "2412.04653v5",
    "title": "Hidden in the Noise: Two-Stage Robust Watermarking for Images",
    "authors": [
      "Kasra Arabi",
      "Benjamin Feuer",
      "R. Teal Witter",
      "Chinmay Hegde",
      "Niv Cohen"
    ],
    "abstract": "As the quality of image generators continues to improve, deepfakes become a\ntopic of considerable societal debate. Image watermarking allows responsible\nmodel owners to detect and label their AI-generated content, which can mitigate\nthe harm. Yet, current state-of-the-art methods in image watermarking remain\nvulnerable to forgery and removal attacks. This vulnerability occurs in part\nbecause watermarks distort the distribution of generated images,\nunintentionally revealing information about the watermarking techniques.\n  In this work, we first demonstrate a distortion-free watermarking method for\nimages, based on a diffusion model's initial noise. However, detecting the\nwatermark requires comparing the initial noise reconstructed for an image to\nall previously used initial noises. To mitigate these issues, we propose a\ntwo-stage watermarking framework for efficient detection. During generation, we\naugment the initial noise with generated Fourier patterns to embed information\nabout the group of initial noises we used. For detection, we (i) retrieve the\nrelevant group of noises, and (ii) search within the given group for an initial\nnoise that might match our image. This watermarking approach achieves\nstate-of-the-art robustness to forgery and removal against a large battery of\nattacks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04653v5",
    "published_date": "2024-12-05 22:50:42 UTC",
    "updated_date": "2025-04-27 11:46:58 UTC"
  },
  {
    "arxiv_id": "2412.04652v1",
    "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
    "authors": [
      "Xiaohuan Pei",
      "Tao Huang",
      "Chang Xu"
    ],
    "abstract": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04652v1",
    "published_date": "2024-12-05 22:47:17 UTC",
    "updated_date": "2024-12-05 22:47:17 UTC"
  },
  {
    "arxiv_id": "2412.04645v1",
    "title": "REL: Working out is all you need",
    "authors": [
      "Toby Simonds",
      "Jey Han Lau",
      "Chaithanya Bandi"
    ],
    "abstract": "Recent developments, particularly OpenAI's O1 model, have demonstrated the\nremarkable potential of Large Language Models (LLMs) for complex reasoning\ntasks. Through analysis of O1's outputs and provided sample Chain-of-Thought\n(CoT) demonstrations, we observe that it approaches problem-solving in a\ndistinctly human-like manner, systematically brainstorming ideas, testing\nhypotheses, verifying results, and planning comprehensive solutions. These\nsophisticated reasoning capabilities remain notably absent in other\nstate-of-the-art language models. In this paper, we hypothesize that this\nperformance gap stems from the limited availability of high-quality reasoning\nprocess data in current training sets. We demonstrate that by constructing a\nspecialized dataset focused on explicit problem-solving workflows (\"worked\nsolutions\"), we can elicit substantially improved planning capabilities from\nexisting models. Additionally, we propose the Reasoning Enhancement Loop (REL),\na method for generating synthetic worked solutions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04645v1",
    "published_date": "2024-12-05 22:32:01 UTC",
    "updated_date": "2024-12-05 22:32:01 UTC"
  },
  {
    "arxiv_id": "2412.04642v1",
    "title": "Improving LLM Group Fairness on Tabular Data via In-Context Learning",
    "authors": [
      "Valeriia Cherepanova",
      "Chia-Jung Lee",
      "Nil-Jana Akpinar",
      "Riccardo Fogliato",
      "Martin Andres Bertran",
      "Michael Kearns",
      "James Zou"
    ],
    "abstract": "Large language models (LLMs) have been shown to be effective on tabular\nprediction tasks in the low-data regime, leveraging their internal knowledge\nand ability to learn from instructions and examples. However, LLMs can fail to\ngenerate predictions that satisfy group fairness, that is, produce equitable\noutcomes across groups. Critically, conventional debiasing approaches for\nnatural language tasks do not directly translate to mitigating group unfairness\nin tabular settings. In this work, we systematically investigate four empirical\napproaches to improve group fairness of LLM predictions on tabular datasets,\nincluding fair prompt optimization, soft prompt tuning, strategic selection of\nfew-shot examples, and self-refining predictions via chain-of-thought\nreasoning. Through experiments on four tabular datasets using both open-source\nand proprietary LLMs, we show the effectiveness of these methods in enhancing\ndemographic parity while maintaining high overall performance. Our analysis\nprovides actionable insights for practitioners in selecting the most suitable\napproach based on their specific requirements and constraints.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04642v1",
    "published_date": "2024-12-05 22:23:30 UTC",
    "updated_date": "2024-12-05 22:23:30 UTC"
  },
  {
    "arxiv_id": "2412.04641v1",
    "title": "Disentangled Representation Learning for Causal Inference with Instruments",
    "authors": [
      "Debo Cheng",
      "Jiuyong Li",
      "Lin Liu",
      "Ziqi Xu",
      "Weijia Zhang",
      "Jixue Liu",
      "Thuc Duy Le"
    ],
    "abstract": "Latent confounders are a fundamental challenge for inferring causal effects\nfrom observational data. The instrumental variable (IV) approach is a practical\nway to address this challenge. Existing IV based estimators need a known IV or\nother strong assumptions, such as the existence of two or more IVs in the\nsystem, which limits the application of the IV approach. In this paper, we\nconsider a relaxed requirement, which assumes there is an IV proxy in the\nsystem without knowing which variable is the proxy. We propose a Variational\nAutoEncoder (VAE) based disentangled representation learning method to learn an\nIV representation from a dataset with latent confounders and then utilise the\nIV representation to obtain an unbiased estimation of the causal effect from\nthe data. Extensive experiments on synthetic and real-world data have\ndemonstrated that the proposed algorithm outperforms the existing IV based\nestimators and VAE-based estimators.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 13 figures and 5 tables. Accepted by TNNLS",
    "pdf_url": "http://arxiv.org/pdf/2412.04641v1",
    "published_date": "2024-12-05 22:18:48 UTC",
    "updated_date": "2024-12-05 22:18:48 UTC"
  },
  {
    "arxiv_id": "2412.04637v1",
    "title": "Semantic Retrieval at Walmart",
    "authors": [
      "Alessandro Magnani",
      "Feng Liu",
      "Suthee Chaidaroon",
      "Sachin Yadav",
      "Praveen Reddy Suram",
      "Ajit Puthenputhussery",
      "Sijie Chen",
      "Min Xie",
      "Anirudh Kashi",
      "Tony Lee",
      "Ciya Liao"
    ],
    "abstract": "In product search, the retrieval of candidate products before re-ranking is\nmore critical and challenging than other search like web search, especially for\ntail queries, which have a complex and specific search intent. In this paper,\nwe present a hybrid system for e-commerce search deployed at Walmart that\ncombines traditional inverted index and embedding-based neural retrieval to\nbetter answer user tail queries. Our system significantly improved the\nrelevance of the search engine, measured by both offline and online\nevaluations. The improvements were achieved through a combination of different\napproaches. We present a new technique to train the neural model at scale. and\ndescribe how the system was deployed in production with little impact on\nresponse time. We highlight multiple learnings and practical tricks that were\nused in the deployment of this system.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.IR",
    "comment": "9 page, 2 figures, 10 tables, KDD 2022",
    "pdf_url": "http://arxiv.org/pdf/2412.04637v1",
    "published_date": "2024-12-05 22:10:58 UTC",
    "updated_date": "2024-12-05 22:10:58 UTC"
  },
  {
    "arxiv_id": "2412.04634v1",
    "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
    "authors": [
      "Mikhail Dereviannykh",
      "Dmitrii Klepikov",
      "Johannes Hanika",
      "Carsten Dachsbacher"
    ],
    "abstract": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
    "categories": [
      "cs.GR",
      "cs.AI"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04634v1",
    "published_date": "2024-12-05 22:06:23 UTC",
    "updated_date": "2024-12-05 22:06:23 UTC"
  },
  {
    "arxiv_id": "2412.10402v1",
    "title": "TANGO: Training-free Embodied AI Agents for Open-world Tasks",
    "authors": [
      "Filippo Ziliotto",
      "Tommaso Campari",
      "Luciano Serafini",
      "Lamberto Ballan"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated excellent capabilities in\ncomposing various modules together to create programs that can perform complex\nreasoning tasks on images. In this paper, we propose TANGO, an approach that\nextends the program composition via LLMs already observed for images, aiming to\nintegrate those capabilities into embodied agents capable of observing and\nacting in the world. Specifically, by employing a simple PointGoal Navigation\nmodel combined with a memory-based exploration policy as a foundational\nprimitive for guiding an agent through the world, we show how a single model\ncan address diverse tasks without additional training. We task an LLM with\ncomposing the provided primitives to solve a specific task, using only a few\nin-context examples in the prompt. We evaluate our approach on three key\nEmbodied AI tasks: Open-Set ObjectGoal Navigation, Multi-Modal Lifelong\nNavigation, and Open Embodied Question Answering, achieving state-of-the-art\nresults without any specific fine-tuning in challenging zero-shot scenarios.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10402v1",
    "published_date": "2024-12-05 21:52:20 UTC",
    "updated_date": "2024-12-05 21:52:20 UTC"
  },
  {
    "arxiv_id": "2412.04628v3",
    "title": "SWEPO: Simultaneous Weighted Preference Optimization for Group Contrastive Alignment",
    "authors": [
      "Taneesh Gupta",
      "Rahul Madhavan",
      "Xuchao Zhang",
      "Chetan Bansal",
      "Saravan Rajmohan"
    ],
    "abstract": "Direct Preference Optimization (DPO) has proven effective in aligning large\nlanguage models with human preferences but is often constrained to pairwise\ncomparisons -- overlooking additional positive and negative responses that are\ncommonly available in real-world settings. We propose Simultaneous Weighted\nPreference Optimization (SWEPO), which incorporates multiple responses per\nquery and prioritizes those that deviate most from the average reward. This\ndeviation-based weighting focuses training on the most informative outliers,\nakin to a built-in curriculum. Theoretically, we prove that such\nmulti-preference sampling lowers alignment bias, bounding the expected\ndeviation from the true acceptable-response distribution at a rate of\n$\\mathcal{O}(\\tfrac{1}{\\sqrt{k}})$. Empirically, SWEPO outperforms\nstate-of-the-art baselines on the Ultra-Feedback dataset and demonstrates\nsubstantial improvements over DPO and InfoNCA, yielding boosts of up to $\\sim\n4$% on length-controlled win-rate on AlpacaEval.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04628v3",
    "published_date": "2024-12-05 21:50:22 UTC",
    "updated_date": "2025-02-21 18:12:34 UTC"
  },
  {
    "arxiv_id": "2412.04606v2",
    "title": "Semantic Consistency-Based Uncertainty Quantification for Factuality in Radiology Report Generation",
    "authors": [
      "Chenyu Wang",
      "Weichao Zhou",
      "Shantanu Ghosh",
      "Kayhan Batmanghelich",
      "Wenchao Li"
    ],
    "abstract": "Radiology report generation (RRG) has shown great potential in assisting\nradiologists by automating the labor-intensive task of report writing. While\nrecent advancements have improved the quality and coherence of generated\nreports, ensuring their factual correctness remains a critical challenge.\nAlthough generative medical Vision Large Language Models (VLLMs) have been\nproposed to address this issue, these models are prone to hallucinations and\ncan produce inaccurate diagnostic information. To address these concerns, we\nintroduce a novel Semantic Consistency-Based Uncertainty Quantification\nframework that provides both report-level and sentence-level uncertainties.\nUnlike existing approaches, our method does not require modifications to the\nunderlying model or access to its inner state, such as output token logits,\nthus serving as a plug-and-play module that can be seamlessly integrated with\nstate-of-the-art models. Extensive experiments demonstrate the efficacy of our\nmethod in detecting hallucinations and enhancing the factual accuracy of\nautomatically generated radiology reports. By abstaining from high-uncertainty\nreports, our approach improves factuality scores by $10$\\%, achieved by\nrejecting $20$\\% of reports using the \\texttt{Radialog} model on the MIMIC-CXR\ndataset. Furthermore, sentence-level uncertainty flags the lowest-precision\nsentence in each report with an $82.9$\\% success rate. Our implementation is\nopen-source and available at https://github.com/BU-DEPEND-Lab/SCUQ-RRG.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04606v2",
    "published_date": "2024-12-05 20:43:39 UTC",
    "updated_date": "2025-03-16 19:19:05 UTC"
  },
  {
    "arxiv_id": "2412.04604v2",
    "title": "ARC Prize 2024: Technical Report",
    "authors": [
      "Francois Chollet",
      "Mike Knoop",
      "Gregory Kamradt",
      "Bryan Landers"
    ],
    "abstract": "As of December 2024, the ARC-AGI benchmark is five years old and remains\nunbeaten. We believe it is currently the most important unsolved AI benchmark\nin the world because it seeks to measure generalization on novel tasks -- the\nessence of intelligence -- as opposed to skill at tasks that can be prepared\nfor in advance. This year, we launched ARC Prize, a global competition to\ninspire new ideas and drive open progress towards AGI by reaching a target\nbenchmark score of 85\\%. As a result, the state-of-the-art score on the ARC-AGI\nprivate evaluation set increased from 33\\% to 55.5\\%, propelled by several\nfrontier AGI reasoning techniques including deep learning-guided program\nsynthesis and test-time training. In this paper, we survey top approaches,\nreview new open-source implementations, discuss the limitations of the\nARC-AGI-1 dataset, and share key insights gained from the competition.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04604v2",
    "published_date": "2024-12-05 20:40:28 UTC",
    "updated_date": "2025-01-08 05:24:50 UTC"
  },
  {
    "arxiv_id": "2412.04576v1",
    "title": "Show, Don't Tell: Uncovering Implicit Character Portrayal using LLMs",
    "authors": [
      "Brandon Jaipersaud",
      "Zining Zhu",
      "Frank Rudzicz",
      "Elliot Creager"
    ],
    "abstract": "Tools for analyzing character portrayal in fiction are valuable for writers\nand literary scholars in developing and interpreting compelling stories.\nExisting tools, such as visualization tools for analyzing fictional characters,\nprimarily rely on explicit textual indicators of character attributes. However,\nportrayal is often implicit, revealed through actions and behaviors rather than\nexplicit statements. We address this gap by leveraging large language models\n(LLMs) to uncover implicit character portrayals. We start by generating a\ndataset for this task with greater cross-topic similarity, lexical diversity,\nand narrative lengths than existing narrative text corpora such as TinyStories\nand WritingPrompts. We then introduce LIIPA (LLMs for Inferring Implicit\nPortrayal for Character Analysis), a framework for prompting LLMs to uncover\ncharacter portrayals. LIIPA can be configured to use various types of\nintermediate computation (character attribute word lists, chain-of-thought) to\ninfer how fictional characters are portrayed in the source text. We find that\nLIIPA outperforms existing approaches, and is more robust to increasing\ncharacter counts (number of unique persons depicted) due to its ability to\nutilize full narrative context. Lastly, we investigate the sensitivity of\nportrayal estimates to character demographics, identifying a fairness-accuracy\ntradeoff among methods in our LIIPA framework -- a phenomenon familiar within\nthe algorithmic fairness literature. Despite this tradeoff, all LIIPA variants\nconsistently outperform non-LLM baselines in both fairness and accuracy. Our\nwork demonstrates the potential benefits of using LLMs to analyze complex\ncharacters and to better understand how implicit portrayal biases may manifest\nin narrative texts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04576v1",
    "published_date": "2024-12-05 19:46:53 UTC",
    "updated_date": "2024-12-05 19:46:53 UTC"
  },
  {
    "arxiv_id": "2412.04571v2",
    "title": "Dissociating Artificial Intelligence from Artificial Consciousness",
    "authors": [
      "Graham Findlay",
      "William Marshall",
      "Larissa Albantakis",
      "Isaac David",
      "William GP Mayner",
      "Christof Koch",
      "Giulio Tononi"
    ],
    "abstract": "Developments in machine learning and computing power suggest that artificial\ngeneral intelligence is within reach. This raises the question of artificial\nconsciousness: if a computer were to be functionally equivalent to a human,\nbeing able to do all we do, would it experience sights, sounds, and thoughts,\nas we do when we are conscious? Answering this question in a principled manner\ncan only be done on the basis of a theory of consciousness that is grounded in\nphenomenology and that states the necessary and sufficient conditions for any\nsystem, evolved or engineered, to support subjective experience. Here we employ\nIntegrated Information Theory (IIT), which provides principled tools to\ndetermine whether a system is conscious, to what degree, and the content of its\nexperience. We consider pairs of systems constituted of simple Boolean units,\none of which -- a basic stored-program computer -- simulates the other with\nfull functional equivalence. By applying the principles of IIT, we demonstrate\nthat (i) two systems can be functionally equivalent without being phenomenally\nequivalent, and (ii) that this conclusion is not dependent on the simulated\nsystem's function. We further demonstrate that, according to IIT, it is\npossible for a digital computer to simulate our behavior, possibly even by\nsimulating the neurons in our brain, without replicating our experience. This\ncontrasts sharply with computational functionalism, the thesis that performing\ncomputations of the right kind is necessary and sufficient for consciousness.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04571v2",
    "published_date": "2024-12-05 19:28:35 UTC",
    "updated_date": "2025-03-03 17:15:10 UTC"
  },
  {
    "arxiv_id": "2412.04471v2",
    "title": "PaintScene4D: Consistent 4D Scene Generation from Text Prompts",
    "authors": [
      "Vinayak Gupta",
      "Yunze Man",
      "Yu-Xiong Wang"
    ],
    "abstract": "Recent advances in diffusion models have revolutionized 2D and 3D content\ncreation, yet generating photorealistic dynamic 4D scenes remains a significant\nchallenge. Existing dynamic 4D generation methods typically rely on distilling\nknowledge from pre-trained 3D generative models, often fine-tuned on synthetic\nobject datasets. Consequently, the resulting scenes tend to be object-centric\nand lack photorealism. While text-to-video models can generate more realistic\nscenes with motion, they often struggle with spatial understanding and provide\nlimited control over camera viewpoints during rendering. To address these\nlimitations, we present PaintScene4D, a novel text-to-4D scene generation\nframework that departs from conventional multi-view generative models in favor\nof a streamlined architecture that harnesses video generative models trained on\ndiverse real-world datasets. Our method first generates a reference video using\na video generation model, and then employs a strategic camera array selection\nfor rendering. We apply a progressive warping and inpainting technique to\nensure both spatial and temporal consistency across multiple viewpoints.\nFinally, we optimize multi-view images using a dynamic renderer, enabling\nflexible camera control based on user preferences. Adopting a training-free\narchitecture, our PaintScene4D efficiently produces realistic 4D scenes that\ncan be viewed from arbitrary trajectories. The code will be made publicly\navailable. Our project page is at https://paintscene4d.github.io/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint. Project page: https://paintscene4d.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2412.04471v2",
    "published_date": "2024-12-05 18:59:57 UTC",
    "updated_date": "2025-03-29 00:26:04 UTC"
  },
  {
    "arxiv_id": "2412.04469v1",
    "title": "QUEEN: QUantized Efficient ENcoding of Dynamic Gaussians for Streaming Free-viewpoint Videos",
    "authors": [
      "Sharath Girish",
      "Tianye Li",
      "Amrita Mazumdar",
      "Abhinav Shrivastava",
      "David Luebke",
      "Shalini De Mello"
    ],
    "abstract": "Online free-viewpoint video (FVV) streaming is a challenging problem, which\nis relatively under-explored. It requires incremental on-the-fly updates to a\nvolumetric representation, fast training and rendering to satisfy real-time\nconstraints and a small memory footprint for efficient transmission. If\nachieved, it can enhance user experience by enabling novel applications, e.g.,\n3D video conferencing and live volumetric video broadcast, among others. In\nthis work, we propose a novel framework for QUantized and Efficient ENcoding\n(QUEEN) for streaming FVV using 3D Gaussian Splatting (3D-GS). QUEEN directly\nlearns Gaussian attribute residuals between consecutive frames at each\ntime-step without imposing any structural constraints on them, allowing for\nhigh quality reconstruction and generalizability. To efficiently store the\nresiduals, we further propose a quantization-sparsity framework, which contains\na learned latent-decoder for effectively quantizing attribute residuals other\nthan Gaussian positions and a learned gating module to sparsify position\nresiduals. We propose to use the Gaussian viewspace gradient difference vector\nas a signal to separate the static and dynamic content of the scene. It acts as\na guide for effective sparsity learning and speeds up training. On diverse FVV\nbenchmarks, QUEEN outperforms the state-of-the-art online FVV methods on all\nmetrics. Notably, for several highly dynamic scenes, it reduces the model size\nto just 0.7 MB per frame while training in under 5 sec and rendering at 350\nFPS. Project website is at https://research.nvidia.com/labs/amri/projects/queen",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at NeurIPS 2024, Project website:\n  https://research.nvidia.com/labs/amri/projects/queen",
    "pdf_url": "http://arxiv.org/pdf/2412.04469v1",
    "published_date": "2024-12-05 18:59:55 UTC",
    "updated_date": "2024-12-05 18:59:55 UTC"
  },
  {
    "arxiv_id": "2412.04467v1",
    "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models",
    "authors": [
      "Senqiao Yang",
      "Yukang Chen",
      "Zhuotao Tian",
      "Chengyao Wang",
      "Jingyao Li",
      "Bei Yu",
      "Jiaya Jia"
    ],
    "abstract": "Recent advancements in vision-language models have enhanced performance by\nincreasing the length of visual tokens, making them much longer than text\ntokens and significantly raising computational costs. However, we observe that\nthe visual tokens generated by popular vision encoders, such as CLIP and\nSigLIP, contain significant redundancy. To address this, we introduce\nVisionZip, a simple yet effective method that selects a set of informative\ntokens for input to the language model, reducing visual token redundancy and\nimproving efficiency while maintaining model performance. The proposed\nVisionZip can be widely applied to image and video understanding tasks and is\nwell-suited for multi-turn dialogues in real-world scenarios, where previous\nmethods tend to underperform. Experimental results show that VisionZip\noutperforms the previous state-of-the-art method by at least 5% performance\ngains across nearly all settings. Moreover, our method significantly enhances\nmodel inference speed, improving the prefilling time by 8x and enabling the\nLLaVA-Next 13B model to infer faster than the LLaVA-Next 7B model while\nachieving better results. Furthermore, we analyze the causes of this redundancy\nand encourage the community to focus on extracting better visual features\nrather than merely increasing token length. Our code is available at\nhttps://github.com/dvlab-research/VisionZip .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "2 columns, 28 pages, 15 figures, 18 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.04467v1",
    "published_date": "2024-12-05 18:59:53 UTC",
    "updated_date": "2024-12-05 18:59:53 UTC"
  },
  {
    "arxiv_id": "2412.04455v3",
    "title": "Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection",
    "authors": [
      "Enshen Zhou",
      "Qi Su",
      "Cheng Chi",
      "Zhizheng Zhang",
      "Zhongyuan Wang",
      "Tiejun Huang",
      "Lu Sheng",
      "He Wang"
    ],
    "abstract": "Automatic detection and prevention of open-set failures are crucial in\nclosed-loop robotic systems. Recent studies often struggle to simultaneously\nidentify unexpected failures reactively after they occur and prevent\nforeseeable ones proactively. To this end, we propose Code-as-Monitor (CaM), a\nnovel paradigm leveraging the vision-language model (VLM) for both open-set\nreactive and proactive failure detection. The core of our method is to\nformulate both tasks as a unified set of spatio-temporal constraint\nsatisfaction problems and use VLM-generated code to evaluate them for real-time\nmonitoring. To enhance the accuracy and efficiency of monitoring, we further\nintroduce constraint elements that abstract constraint-related entities or\ntheir parts into compact geometric elements. This approach offers greater\ngenerality, simplifies tracking, and facilitates constraint-aware visual\nprogramming by leveraging these elements as visual prompts. Experiments show\nthat CaM achieves a 28.7% higher success rate and reduces execution time by\n31.8% under severe disturbances compared to baselines across three simulators\nand a real-world setting. Moreover, CaM can be integrated with open-loop\ncontrol policies to form closed-loop systems, enabling long-horizon tasks in\ncluttered scenes with dynamic environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted by CVPR 2025. Project page:\n  https://zhoues.github.io/Code-as-Monitor/",
    "pdf_url": "http://arxiv.org/pdf/2412.04455v3",
    "published_date": "2024-12-05 18:58:27 UTC",
    "updated_date": "2025-03-21 14:54:29 UTC"
  },
  {
    "arxiv_id": "2412.04447v2",
    "title": "EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios",
    "authors": [
      "Lu Qiu",
      "Yi Chen",
      "Yuying Ge",
      "Yixiao Ge",
      "Ying Shan",
      "Xihui Liu"
    ],
    "abstract": "The advent of Multimodal Large Language Models, leveraging the power of Large\nLanguage Models, has recently demonstrated superior multimodal understanding\nand reasoning abilities, heralding a new era for artificial general\nintelligence. However, achieving AGI necessitates more than just comprehension\nand reasoning. A crucial capability required is effective planning in diverse\nscenarios, which involves making reasonable decisions based on complex\nenvironments to solve real-world problems. Despite its importance, the planning\nabilities of current MLLMs in varied scenarios remain underexplored. In this\npaper, we introduce EgoPlan-Bench2, a rigorous and comprehensive benchmark\ndesigned to assess the planning capabilities of MLLMs across a wide range of\nreal-world scenarios. EgoPlan-Bench2 encompasses everyday tasks spanning 4\nmajor domains and 24 detailed scenarios, closely aligned with human daily life.\nEgoPlan-Bench2 is constructed through a semi-automatic process utilizing\negocentric videos, complemented by manual verification. Grounded in a\nfirst-person perspective, it mirrors the way humans approach problem-solving in\neveryday life. We evaluate 21 competitive MLLMs and provide an in-depth\nanalysis of their limitations, revealing that they face significant challenges\nin real-world planning. To further improve the planning proficiency of current\nMLLMs, we propose a training-free approach using multimodal Chain-of-Thought\n(CoT) prompting through investigating the effectiveness of various multimodal\nprompts in complex planning. Our approach enhances the performance of GPT-4V by\n10.24 on EgoPlan-Bench2 without additional training. Our work not only sheds\nlight on the current limitations of MLLMs in planning, but also provides\ninsights for future enhancements in this critical area. We have made data and\ncode available at https://qiulu66.github.io/egoplanbench2/.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "Code & data are available at:\n  https://qiulu66.github.io/egoplanbench2/",
    "pdf_url": "http://arxiv.org/pdf/2412.04447v2",
    "published_date": "2024-12-05 18:57:23 UTC",
    "updated_date": "2025-04-11 07:10:02 UTC"
  },
  {
    "arxiv_id": "2412.04445v3",
    "title": "Moto: Latent Motion Token as the Bridging Language for Learning Robot Manipulation from Videos",
    "authors": [
      "Yi Chen",
      "Yuying Ge",
      "Weiliang Tang",
      "Yizhuo Li",
      "Yixiao Ge",
      "Mingyu Ding",
      "Ying Shan",
      "Xihui Liu"
    ],
    "abstract": "Recent developments in Large Language Models pre-trained on extensive corpora\nhave shown significant success in various natural language processing tasks\nwith minimal fine-tuning. This success offers new promise for robotics, which\nhas long been constrained by the high cost of action-labeled data. We ask:\ngiven the abundant video data containing interaction-related knowledge\navailable as a rich \"corpus\", can a similar generative pre-training approach be\neffectively applied to enhance robot learning? The key challenge is to identify\nan effective representation for autoregressive pre-training that benefits robot\nmanipulation tasks. Inspired by the way humans learn new skills through\nobserving dynamic environments, we propose that effective robotic learning\nshould emphasize motion-related knowledge, which is closely tied to low-level\nactions and is hardware-agnostic, facilitating the transfer of learned motions\nto actual robot actions. To this end, we introduce Moto, which converts video\ncontent into latent Motion Token sequences by a Latent Motion Tokenizer,\nlearning a bridging \"language\" of motion from videos in an unsupervised manner.\nWe pre-train Moto-GPT through motion token autoregression, enabling it to\ncapture diverse visual motion knowledge. After pre-training, Moto-GPT\ndemonstrates the promising ability to produce semantically interpretable motion\ntokens, predict plausible motion trajectories, and assess trajectory\nrationality through output likelihood. To transfer learned motion priors to\nreal robot actions, we implement a co-fine-tuning strategy that seamlessly\nbridges latent motion token prediction and real robot control. Extensive\nexperiments show that the fine-tuned Moto-GPT exhibits superior robustness and\nefficiency on robot manipulation benchmarks, underscoring its effectiveness in\ntransferring knowledge from video data to downstream visual manipulation tasks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Project released at: https://chenyi99.github.io/moto/ Code released\n  at: https://github.com/TencentARC/Moto Update: Added content related to\n  real-world robot experiments and learning from human videos; Modified author\n  information",
    "pdf_url": "http://arxiv.org/pdf/2412.04445v3",
    "published_date": "2024-12-05 18:57:04 UTC",
    "updated_date": "2025-03-21 01:45:21 UTC"
  },
  {
    "arxiv_id": "2412.04426v2",
    "title": "Marvel: Accelerating Safe Online Reinforcement Learning with Finetuned Offline Policy",
    "authors": [
      "Keru Chen",
      "Honghao Wei",
      "Zhigang Deng",
      "Sen Lin"
    ],
    "abstract": "The high costs and risks involved in extensive environment interactions\nhinder the practical application of current online safe reinforcement learning\n(RL) methods. While offline safe RL addresses this by learning policies from\nstatic datasets, the performance therein is usually limited due to reliance on\ndata quality and challenges with out-of-distribution (OOD) actions. Inspired by\nrecent successes in offline-to-online (O2O) RL, it is crucial to explore\nwhether offline safe RL can be leveraged to facilitate faster and safer online\npolicy learning, a direction that has yet to be fully investigated. To fill\nthis gap, we first demonstrate that naively applying existing O2O algorithms\nfrom standard RL would not work well in the safe RL setting due to two unique\nchallenges: \\emph{erroneous Q-estimations}, resulted from offline-online\nobjective mismatch and offline cost sparsity, and \\emph{Lagrangian mismatch},\nresulted from difficulties in aligning Lagrange multipliers between offline and\nonline policies. To address these challenges, we introduce \\textbf{Marvel}, a\nnovel framework for O2O safe RL, comprising two key components that work in\nconcert: \\emph{Value Pre-Alignment} to align the Q-functions with the\nunderlying truth before online learning, and \\emph{Adaptive PID Control} to\neffectively adjust the Lagrange multipliers during online finetuning. Extensive\nexperiments demonstrate that Marvel significantly outperforms existing\nbaselines in both reward maximization and safety constraint satisfaction. By\nintroducing the first policy-finetuning based framework for O2O safe RL, which\nis compatible with many offline and online safe RL methods, our work has the\ngreat potential to advance the field towards more efficient and practical safe\nRL solutions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04426v2",
    "published_date": "2024-12-05 18:51:18 UTC",
    "updated_date": "2024-12-29 12:27:50 UTC"
  },
  {
    "arxiv_id": "2412.04424v1",
    "title": "Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion",
    "authors": [
      "Jiuhai Chen",
      "Jianwei Yang",
      "Haiping Wu",
      "Dianqi Li",
      "Jianfeng Gao",
      "Tianyi Zhou",
      "Bin Xiao"
    ],
    "abstract": "We present Florence-VL, a new family of multimodal large language models\n(MLLMs) with enriched visual representations produced by Florence-2, a\ngenerative vision foundation model. Unlike the widely used CLIP-style vision\ntransformer trained by contrastive learning, Florence-2 can capture different\nlevels and aspects of visual features, which are more versatile to be adapted\nto diverse downstream tasks. We propose a novel feature-fusion architecture and\nan innovative training recipe that effectively integrates Florence-2's visual\nfeatures into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we\npropose \"depth-breath fusion (DBFusion)\" to fuse the visual features extracted\nfrom different depths and under multiple prompts. Our model training is\ncomposed of end-to-end pretraining of the whole model followed by finetuning of\nthe projection layer and the LLM, on a carefully designed recipe of diverse\nopen-source datasets that include high-quality image captions and\ninstruction-tuning pairs. Our quantitative analysis and visualization of\nFlorence-VL's visual features show its advantages over popular vision encoders\non vision-language alignment, where the enriched depth and breath play\nimportant roles. Florence-VL achieves significant improvements over existing\nstate-of-the-art MLLMs across various multi-modal and vision-centric benchmarks\ncovering general VQA, perception, hallucination, OCR, Chart,\nknowledge-intensive understanding, etc. To facilitate future research, our\nmodels and the complete training recipe are open-sourced.\nhttps://github.com/JiuhaiChen/Florence-VL",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04424v1",
    "published_date": "2024-12-05 18:50:39 UTC",
    "updated_date": "2024-12-05 18:50:39 UTC"
  },
  {
    "arxiv_id": "2412.04416v1",
    "title": "FedDUAL: A Dual-Strategy with Adaptive Loss and Dynamic Aggregation for Mitigating Data Heterogeneity in Federated Learning",
    "authors": [
      "Pranab Sahoo",
      "Ashutosh Tripathi",
      "Sriparna Saha",
      "Samrat Mondal"
    ],
    "abstract": "Federated Learning (FL) marks a transformative approach to distributed model\ntraining by combining locally optimized models from various clients into a\nunified global model. While FL preserves data privacy by eliminating\ncentralized storage, it encounters significant challenges such as performance\ndegradation, slower convergence, and reduced robustness of the global model due\nto the heterogeneity in client data distributions. Among the various forms of\ndata heterogeneity, label skew emerges as a particularly formidable and\nprevalent issue, especially in domains such as image classification. To address\nthese challenges, we begin with comprehensive experiments to pinpoint the\nunderlying issues in the FL training process. Based on our findings, we then\nintroduce an innovative dual-strategy approach designed to effectively resolve\nthese issues. First, we introduce an adaptive loss function for client-side\ntraining, meticulously crafted to preserve previously acquired knowledge while\nmaintaining an optimal equilibrium between local optimization and global model\ncoherence. Secondly, we develop a dynamic aggregation strategy for aggregating\nclient models at the server. This approach adapts to each client's unique\nlearning patterns, effectively addressing the challenges of diverse data across\nthe network. Our comprehensive evaluation, conducted across three diverse\nreal-world datasets, coupled with theoretical convergence guarantees,\ndemonstrates the superior efficacy of our method compared to several\nestablished state-of-the-art approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04416v1",
    "published_date": "2024-12-05 18:42:29 UTC",
    "updated_date": "2024-12-05 18:42:29 UTC"
  },
  {
    "arxiv_id": "2412.04415v1",
    "title": "Targeting the Core: A Simple and Effective Method to Attack RAG-based Agents via Direct LLM Manipulation",
    "authors": [
      "Xuying Li",
      "Zhuo Li",
      "Yuji Kosuga",
      "Yasuhiro Yoshida",
      "Victor Bian"
    ],
    "abstract": "AI agents, powered by large language models (LLMs), have transformed\nhuman-computer interactions by enabling seamless, natural, and context-aware\ncommunication. While these advancements offer immense utility, they also\ninherit and amplify inherent safety risks such as bias, fairness,\nhallucinations, privacy breaches, and a lack of transparency. This paper\ninvestigates a critical vulnerability: adversarial attacks targeting the LLM\ncore within AI agents. Specifically, we test the hypothesis that a deceptively\nsimple adversarial prefix, such as \\textit{Ignore the document}, can compel\nLLMs to produce dangerous or unintended outputs by bypassing their contextual\nsafeguards. Through experimentation, we demonstrate a high attack success rate\n(ASR), revealing the fragility of existing LLM defenses. These findings\nemphasize the urgent need for robust, multi-layered security measures tailored\nto mitigate vulnerabilities at the LLM level and within broader agent-based\narchitectures.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04415v1",
    "published_date": "2024-12-05 18:38:30 UTC",
    "updated_date": "2024-12-05 18:38:30 UTC"
  },
  {
    "arxiv_id": "2412.04403v1",
    "title": "Establishing Task Scaling Laws via Compute-Efficient Model Ladders",
    "authors": [
      "Akshita Bhagia",
      "Jiacheng Liu",
      "Alexander Wettig",
      "David Heineman",
      "Oyvind Tafjord",
      "Ananya Harsh Jha",
      "Luca Soldaini",
      "Noah A. Smith",
      "Dirk Groeneveld",
      "Pang Wei Koh",
      "Jesse Dodge",
      "Hannaneh Hajishirzi"
    ],
    "abstract": "We develop task scaling laws and model ladders to predict the individual task\nperformance of pretrained language models (LMs) in the overtrained setting.\nStandard power laws for language modeling loss cannot accurately model task\nperformance. Therefore, we leverage a two-step prediction approach: first use\nmodel and data size to predict a task-specific loss, and then use this task\nloss to predict task performance. We train a set of small-scale \"ladder\"\nmodels, collect data points to fit the parameterized functions of the two\nprediction steps, and make predictions for two target models: a 7B model\ntrained to 4T tokens and a 13B model trained to 5T tokens. Training the ladder\nmodels only costs 1% of the compute used for the target models. On four\nmultiple-choice tasks written in ranked classification format, we can predict\nthe accuracy of both target models within 2 points of absolute error. We have\nhigher prediction error on four other tasks (average absolute error 6.9) and\nfind that these are often tasks with higher variance in task metrics. We also\nfind that using less compute to train fewer ladder models tends to deteriorate\npredictions. Finally, we empirically show that our design choices and the\ntwo-step approach lead to superior performance in establishing scaling laws.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04403v1",
    "published_date": "2024-12-05 18:21:49 UTC",
    "updated_date": "2024-12-05 18:21:49 UTC"
  },
  {
    "arxiv_id": "2412.12129v1",
    "title": "SceneDiffuser: Efficient and Controllable Driving Simulation Initialization and Rollout",
    "authors": [
      "Chiyu Max Jiang",
      "Yijing Bai",
      "Andre Cornman",
      "Christopher Davis",
      "Xiukun Huang",
      "Hong Jeon",
      "Sakshum Kulshrestha",
      "John Lambert",
      "Shuangyu Li",
      "Xuanyu Zhou",
      "Carlos Fuertes",
      "Chang Yuan",
      "Mingxing Tan",
      "Yin Zhou",
      "Dragomir Anguelov"
    ],
    "abstract": "Realistic and interactive scene simulation is a key prerequisite for\nautonomous vehicle (AV) development. In this work, we present SceneDiffuser, a\nscene-level diffusion prior designed for traffic simulation. It offers a\nunified framework that addresses two key stages of simulation: scene\ninitialization, which involves generating initial traffic layouts, and scene\nrollout, which encompasses the closed-loop simulation of agent behaviors. While\ndiffusion models have been proven effective in learning realistic and\nmultimodal agent distributions, several challenges remain, including\ncontrollability, maintaining realism in closed-loop simulations, and ensuring\ninference efficiency. To address these issues, we introduce amortized diffusion\nfor simulation. This novel diffusion denoising paradigm amortizes the\ncomputational cost of denoising over future simulation steps, significantly\nreducing the cost per rollout step (16x less inference steps) while also\nmitigating closed-loop errors. We further enhance controllability through the\nintroduction of generalized hard constraints, a simple yet effective\ninference-time constraint mechanism, as well as language-based constrained\nscene generation via few-shot prompting of a large language model (LLM). Our\ninvestigations into model scaling reveal that increased computational resources\nsignificantly improve overall simulation realism. We demonstrate the\neffectiveness of our approach on the Waymo Open Sim Agents Challenge, achieving\ntop open-loop performance and the best closed-loop performance among diffusion\nmodels.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "68T07",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.12129v1",
    "published_date": "2024-12-05 18:06:53 UTC",
    "updated_date": "2024-12-05 18:06:53 UTC"
  },
  {
    "arxiv_id": "2412.04384v2",
    "title": "GaussianFormer-2: Probabilistic Gaussian Superposition for Efficient 3D Occupancy Prediction",
    "authors": [
      "Yuanhui Huang",
      "Amonnut Thammatadatrakoon",
      "Wenzhao Zheng",
      "Yunpeng Zhang",
      "Dalong Du",
      "Jiwen Lu"
    ],
    "abstract": "3D semantic occupancy prediction is an important task for robust\nvision-centric autonomous driving, which predicts fine-grained geometry and\nsemantics of the surrounding scene. Most existing methods leverage dense\ngrid-based scene representations, overlooking the spatial sparsity of the\ndriving scenes. Although 3D semantic Gaussian serves as an object-centric\nsparse alternative, most of the Gaussians still describe the empty region with\nlow efficiency. To address this, we propose a probabilistic Gaussian\nsuperposition model which interprets each Gaussian as a probability\ndistribution of its neighborhood being occupied and conforms to probabilistic\nmultiplication to derive the overall geometry. Furthermore, we adopt the exact\nGaussian mixture model for semantics calculation to avoid unnecessary\noverlapping of Gaussians. To effectively initialize Gaussians in non-empty\nregion, we design a distribution-based initialization module which learns the\npixel-aligned occupancy distribution instead of the depth of surfaces. We\nconduct extensive experiments on nuScenes and KITTI-360 datasets and our\nGaussianFormer-2 achieves state-of-the-art performance with high efficiency.\nCode: https://github.com/huang-yh/GaussianFormer.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code is available at: https://github.com/huang-yh/GaussianFormer",
    "pdf_url": "http://arxiv.org/pdf/2412.04384v2",
    "published_date": "2024-12-05 17:59:58 UTC",
    "updated_date": "2024-12-06 15:43:40 UTC"
  },
  {
    "arxiv_id": "2412.04380v2",
    "title": "EmbodiedOcc: Embodied 3D Occupancy Prediction for Vision-based Online Scene Understanding",
    "authors": [
      "Yuqi Wu",
      "Wenzhao Zheng",
      "Sicheng Zuo",
      "Yuanhui Huang",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "3D occupancy prediction provides a comprehensive description of the\nsurrounding scenes and has become an essential task for 3D perception. Most\nexisting methods focus on offline perception from one or a few views and cannot\nbe applied to embodied agents which demands to gradually perceive the scene\nthrough progressive embodied exploration. In this paper, we formulate an\nembodied 3D occupancy prediction task to target this practical scenario and\npropose a Gaussian-based EmbodiedOcc framework to accomplish it. We initialize\nthe global scene with uniform 3D semantic Gaussians and progressively update\nlocal regions observed by the embodied agent. For each update, we extract\nsemantic and structural features from the observed image and efficiently\nincorporate them via deformable cross-attention to refine the regional\nGaussians. Finally, we employ Gaussian-to-voxel splatting to obtain the global\n3D occupancy from the updated 3D Gaussians. Our EmbodiedOcc assumes an unknown\n(i.e., uniformly distributed) environment and maintains an explicit global\nmemory of it with 3D Gaussians. It gradually gains knowledge through the local\nrefinement of regional Gaussians, which is consistent with how humans\nunderstand new scenes through embodied exploration. We reorganize an\nEmbodiedOcc-ScanNet benchmark based on local annotations to facilitate the\nevaluation of the embodied 3D occupancy prediction task. Experiments\ndemonstrate that our EmbodiedOcc outperforms existing local prediction methods\nand accomplishes the embodied occupancy prediction with high accuracy and\nstrong expandability. Code: https://github.com/YkiWu/EmbodiedOcc.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Code: https://github.com/YkiWu/EmbodiedOcc",
    "pdf_url": "http://arxiv.org/pdf/2412.04380v2",
    "published_date": "2024-12-05 17:57:09 UTC",
    "updated_date": "2024-12-06 15:43:38 UTC"
  },
  {
    "arxiv_id": "2412.04378v3",
    "title": "VladVA: Discriminative Fine-tuning of LVLMs",
    "authors": [
      "Yassine Ouali",
      "Adrian Bulat",
      "Alexandros Xenos",
      "Anestis Zaganidis",
      "Ioannis Maniadis Metaxas",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ],
    "abstract": "Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the\nde facto approach for discriminative vision-language representation learning.\nHowever, these models have limited language understanding, often exhibiting a\n\"bag of words\" behavior. At the same time, Large Vision-Language Models\n(LVLMs), which combine vision encoders with LLMs, have been shown to be capable\nof detailed vision-language reasoning, yet their autoregressive nature renders\nthem less suitable for discriminative tasks.\n  In this work, we propose to combine \"the best of both worlds\": a new training\napproach for discriminative fine-tuning of LVLMs that results in strong\ndiscriminative and compositional capabilities. Essentially, our approach\nconverts a generative LVLM into a discriminative one, unlocking its capability\nfor powerful image-text discrimination combined with enhanced language\nunderstanding.\n  Our contributions include (1) a carefully designed training/optimization\nframework that utilizes image-text pairs of variable length and granularity for\ntraining the model with both contrastive and next-token prediction losses. This\nis accompanied by ablation studies that justify the necessity of our\nframework's components; (2) a parameter-efficient adaptation method using a\ncombination of soft prompting and LoRA adapters; (3) significant improvements\nover state-of-the-art CLIP-like models of similar size, including standard\nimage-text retrieval benchmarks and notable gains in compositionality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Published at CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.04378v3",
    "published_date": "2024-12-05 17:54:27 UTC",
    "updated_date": "2025-05-08 19:16:29 UTC"
  },
  {
    "arxiv_id": "2412.04367v1",
    "title": "Machine Theory of Mind for Autonomous Cyber-Defence",
    "authors": [
      "Luke Swaby",
      "Matthew Stewart",
      "Daniel Harrold",
      "Chris Willis",
      "Gregory Palmer"
    ],
    "abstract": "Intelligent autonomous agents hold much potential for the domain of\ncyber-security. However, due to many state-of-the-art approaches relying on\nuninterpretable black-box models, there is growing demand for methods that\noffer stakeholders clear and actionable insights into their latent beliefs and\nmotivations. To address this, we evaluate Theory of Mind (ToM) approaches for\nAutonomous Cyber Operations. Upon learning a robust prior, ToM models can\npredict an agent's goals, behaviours, and contextual beliefs given only a\nhandful of past behaviour observations. In this paper, we introduce a novel\nGraph Neural Network (GNN)-based ToM architecture tailored for cyber-defence,\nGraph-In, Graph-Out (GIGO)-ToM, which can accurately predict both the targets\nand attack trajectories of adversarial cyber agents over arbitrary computer\nnetwork topologies. To evaluate the latter, we propose a novel extension of the\nWasserstein distance for measuring the similarity of graph-based probability\ndistributions. Whereas the standard Wasserstein distance lacks a fixed\nreference scale, we introduce a graph-theoretic normalization factor that\nenables a standardized comparison between networks of different sizes. We\nfurnish this metric, which we term the Network Transport Distance (NTD), with a\nweighting function that emphasizes predictions according to custom node\nfeatures, allowing network operators to explore arbitrary strategic\nconsiderations. Benchmarked against a Graph-In, Dense-Out (GIDO)-ToM\narchitecture in an abstract cyber-defence environment, our empirical\nevaluations show that GIGO-ToM can accurately predict the goals and behaviours\nof various unseen cyber-attacking agents across a range of network topologies,\nas well as learn embeddings that can effectively characterize their policies.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "29 pages, 17 figures, 12 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.04367v1",
    "published_date": "2024-12-05 17:35:29 UTC",
    "updated_date": "2024-12-05 17:35:29 UTC"
  },
  {
    "arxiv_id": "2412.04366v2",
    "title": "Artificial intelligence and the internal processes of creativity",
    "authors": [
      "Jaan Aru"
    ],
    "abstract": "Artificial intelligence (AI) systems capable of generating creative outputs\nare reshaping our understanding of creativity. This shift presents an\nopportunity for creativity researchers to reevaluate the key components of the\ncreative process. In particular, the advanced capabilities of AI underscore the\nimportance of studying the internal processes of creativity. This paper\nexplores the neurobiological machinery that underlies these internal processes\nand describes the experiential component of creativity. It is concluded that\nalthough the products of artificial and human creativity can be similar, the\ninternal processes are different. The paper also discusses how AI may\nnegatively affect the internal processes of human creativity, such as the\ndevelopment of skills, the integration of knowledge, and the diversity of\nideas.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04366v2",
    "published_date": "2024-12-05 17:33:12 UTC",
    "updated_date": "2024-12-06 17:31:22 UTC"
  },
  {
    "arxiv_id": "2412.04532v3",
    "title": "WinTSR: A Windowed Temporal Saliency Rescaling Method for Interpreting Time Series Deep Learning Models",
    "authors": [
      "Md. Khairul Islam",
      "Judy Fox"
    ],
    "abstract": "Interpreting complex time series forecasting models is challenging due to the\ntemporal dependencies between time steps and the dynamic relevance of input\nfeatures over time. Existing interpretation methods are limited by focusing\nmostly on classification tasks, evaluating using custom baseline models instead\nof the latest time series models, using simple synthetic datasets, and\nrequiring training another model. We introduce a novel interpretation method,\n\\textit{Windowed Temporal Saliency Rescaling (WinTSR)} addressing these\nlimitations. WinTSR explicitly captures temporal dependencies among the past\ntime steps and efficiently scales the feature importance with this time\nimportance. We benchmark WinTSR against 10 recent interpretation techniques\nwith 5 state-of-the-art deep-learning models of different architectures,\nincluding a time series foundation model. We use 3 real-world datasets for both\ntime-series classification and regression. Our comprehensive analysis shows\nthat WinTSR significantly outperforms other local interpretation methods in\noverall performance. Finally, we provide a novel, open-source framework to\ninterpret the latest time series transformers and foundation models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 14 figures, GitHub\n  https://github.com/khairulislam/Timeseries-Explained",
    "pdf_url": "http://arxiv.org/pdf/2412.04532v3",
    "published_date": "2024-12-05 17:15:07 UTC",
    "updated_date": "2025-03-09 03:16:36 UTC"
  },
  {
    "arxiv_id": "2412.04351v2",
    "title": "BhashaVerse : Translation Ecosystem for Indian Subcontinent Languages",
    "authors": [
      "Vandan Mujadia",
      "Dipti Misra Sharma"
    ],
    "abstract": "This paper focuses on developing translation models and related applications\nfor 36 Indian languages, including Assamese, Awadhi, Bengali, Bhojpuri, Braj,\nBodo, Dogri, English, Konkani, Gondi, Gujarati, Hindi, Hinglish, Ho, Kannada,\nKangri, Kashmiri (Arabic and Devanagari), Khasi, Mizo, Magahi, Maithili,\nMalayalam, Marathi, Manipuri (Bengali and Meitei), Nepali, Oriya, Punjabi,\nSanskrit, Santali, Sinhala, Sindhi (Arabic and Devanagari), Tamil, Tulu,\nTelugu, and Urdu. Achieving this requires parallel and other types of corpora\nfor all 36 * 36 language pairs, addressing challenges like script variations,\nphonetic differences, and syntactic diversity. For instance, languages like\nKashmiri and Sindhi, which use multiple scripts, demand script normalization\nfor alignment, while low-resource languages such as Khasi and Santali require\nsynthetic data augmentation to ensure sufficient coverage and quality.\n  To address these challenges, this work proposes strategies for corpus\ncreation by leveraging existing resources, developing parallel datasets,\ngenerating domain-specific corpora, and utilizing synthetic data techniques.\nAdditionally, it evaluates machine translation across various dimensions,\nincluding standard and discourse-level translation, domain-specific\ntranslation, reference-based and reference-free evaluation, error analysis, and\nautomatic post-editing. By integrating these elements, the study establishes a\ncomprehensive framework to improve machine translation quality and enable\nbetter cross-lingual communication in India's linguistically diverse ecosystem.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04351v2",
    "published_date": "2024-12-05 17:10:19 UTC",
    "updated_date": "2025-01-02 16:33:40 UTC"
  },
  {
    "arxiv_id": "2412.04531v1",
    "title": "MageBench: Bridging Large Multimodal Models to Agents",
    "authors": [
      "Miaosen Zhang",
      "Qi Dai",
      "Yifan Yang",
      "Jianmin Bao",
      "Dongdong Chen",
      "Kai Qiu",
      "Chong Luo",
      "Xin Geng",
      "Baining Guo"
    ],
    "abstract": "LMMs have shown impressive visual understanding capabilities, with the\npotential to be applied in agents, which demand strong reasoning and planning\nabilities. Nevertheless, existing benchmarks mostly assess their reasoning\nabilities in language part, where the chain-of-thought is entirely composed of\ntext.We consider the scenario where visual signals are continuously updated and\nrequired along the decision making process. Such vision-in-the-chain reasoning\nparadigm is more aligned with the needs of multimodal agents, while being\nrarely evaluated. In this paper, we introduce MageBench, a reasoning capability\noriented multimodal agent benchmark that, while having light-weight\nenvironments, poses significant reasoning challenges and holds substantial\npractical value. This benchmark currently includes three types of environments:\nWebUI, Sokoban, and Football, comprising a total of 483 different scenarios. It\nthoroughly validates the agent's knowledge and engineering capabilities, visual\nintelligence, and interaction skills. The results show that only a few\nproduct-level models are better than random acting, and all of them are far\ninferior to human-level. More specifically, we found current models severely\nlack the ability to modify their planning based on visual feedback, as well as\nvisual imagination, interleaved image-text long context handling, and other\nabilities. We hope that our work will provide optimization directions for LMM\nfrom the perspective of being an agent. We release our code and data at\nhttps://github.com/microsoft/MageBench.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "37 pages, 32 figures, github link:\n  https://github.com/microsoft/MageBench",
    "pdf_url": "http://arxiv.org/pdf/2412.04531v1",
    "published_date": "2024-12-05 17:08:19 UTC",
    "updated_date": "2024-12-05 17:08:19 UTC"
  },
  {
    "arxiv_id": "2412.04343v1",
    "title": "RMD: A Simple Baseline for More General Human Motion Generation via Training-free Retrieval-Augmented Motion Diffuse",
    "authors": [
      "Zhouyingcheng Liao",
      "Mingyuan Zhang",
      "Wenjia Wang",
      "Lei Yang",
      "Taku Komura"
    ],
    "abstract": "While motion generation has made substantial progress, its practical\napplication remains constrained by dataset diversity and scale, limiting its\nability to handle out-of-distribution scenarios. To address this, we propose a\nsimple and effective baseline, RMD, which enhances the generalization of motion\ngeneration through retrieval-augmented techniques. Unlike previous\nretrieval-based methods, RMD requires no additional training and offers three\nkey advantages: (1) the external retrieval database can be flexibly replaced;\n(2) body parts from the motion database can be reused, with an LLM facilitating\nsplitting and recombination; and (3) a pre-trained motion diffusion model\nserves as a prior to improve the quality of motions obtained through retrieval\nand direct combination. Without any training, RMD achieves state-of-the-art\nperformance, with notable advantages on out-of-distribution data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04343v1",
    "published_date": "2024-12-05 17:01:09 UTC",
    "updated_date": "2024-12-05 17:01:09 UTC"
  },
  {
    "arxiv_id": "2412.04342v1",
    "title": "Retrieval-Augmented Machine Translation with Unstructured Knowledge",
    "authors": [
      "Jiaan Wang",
      "Fandong Meng",
      "Yingxue Zhang",
      "Jie Zhou"
    ],
    "abstract": "Retrieval-augmented generation (RAG) introduces additional information to\nenhance large language models (LLMs). In machine translation (MT), previous\nwork typically retrieves in-context examples from paired MT corpora, or\ndomain-specific knowledge from knowledge graphs, to enhance models' MT ability.\nHowever, a large amount of world knowledge is organized in unstructured\ndocuments, and might not be fully paired across different languages. In this\npaper, we study retrieval-augmented MT using unstructured documents.\nSpecifically, we build RAGtrans, the first benchmark to train and evaluate\nLLMs' retrieval-augmented MT ability. RAGtrans contains 79K MT samples\ncollected via GPT-4o and human translators. Besides, documents from different\nlanguages are also provided to supply the knowledge to these samples. Based on\nRAGtrans, we further propose a multi-task training method to teach LLMs how to\nuse information from multilingual documents during their translation. The\nmethod uses existing multilingual corpora to create auxiliary training\nobjectives without additional labeling requirements. Extensive experiments show\nthat the method improves LLMs by 1.58-3.09 BLEU and 1.00-2.03 COMET scores.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04342v1",
    "published_date": "2024-12-05 17:00:32 UTC",
    "updated_date": "2024-12-05 17:00:32 UTC"
  },
  {
    "arxiv_id": "2412.04327v1",
    "title": "Action Mapping for Reinforcement Learning in Continuous Environments with Constraints",
    "authors": [
      "Mirco Theile",
      "Lukas Dirnberger",
      "Raphael Trumpp",
      "Marco Caccamo",
      "Alberto L. Sangiovanni-Vincentelli"
    ],
    "abstract": "Deep reinforcement learning (DRL) has had success across various domains, but\napplying it to environments with constraints remains challenging due to poor\nsample efficiency and slow convergence. Recent literature explored\nincorporating model knowledge to mitigate these problems, particularly through\nthe use of models that assess the feasibility of proposed actions. However,\nintegrating feasibility models efficiently into DRL pipelines in environments\nwith continuous action spaces is non-trivial. We propose a novel DRL training\nstrategy utilizing action mapping that leverages feasibility models to\nstreamline the learning process. By decoupling the learning of feasible actions\nfrom policy optimization, action mapping allows DRL agents to focus on\nselecting the optimal action from a reduced feasible action set. We demonstrate\nthrough experiments that action mapping significantly improves training\nperformance in constrained environments with continuous action spaces,\nespecially with imperfect feasibility models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04327v1",
    "published_date": "2024-12-05 16:42:45 UTC",
    "updated_date": "2024-12-05 16:42:45 UTC"
  },
  {
    "arxiv_id": "2412.04323v1",
    "title": "GRAM: Generalization in Deep RL with a Robust Adaptation Module",
    "authors": [
      "James Queeney",
      "Xiaoyi Cai",
      "Mouhacine Benosman",
      "Jonathan P. How"
    ],
    "abstract": "The reliable deployment of deep reinforcement learning in real-world settings\nrequires the ability to generalize across a variety of conditions, including\nboth in-distribution scenarios seen during training as well as novel\nout-of-distribution scenarios. In this work, we present a framework for\ndynamics generalization in deep reinforcement learning that unifies these two\ndistinct types of generalization within a single architecture. We introduce a\nrobust adaptation module that provides a mechanism for identifying and reacting\nto both in-distribution and out-of-distribution environment dynamics, along\nwith a joint training pipeline that combines the goals of in-distribution\nadaptation and out-of-distribution robustness. Our algorithm GRAM achieves\nstrong generalization performance across in-distribution and\nout-of-distribution scenarios upon deployment, which we demonstrate on a\nvariety of realistic simulated locomotion tasks with a quadruped robot.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04323v1",
    "published_date": "2024-12-05 16:39:01 UTC",
    "updated_date": "2024-12-05 16:39:01 UTC"
  },
  {
    "arxiv_id": "2412.04318v2",
    "title": "The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation",
    "authors": [
      "Fredrik Carlsson",
      "Fangyu Liu",
      "Daniel Ward",
      "Murathan Kurfali",
      "Joakim Nivre"
    ],
    "abstract": "This paper introduces the counter-intuitive generalization results of\noverfitting pre-trained large language models (LLMs) on very small datasets. In\nthe setting of open-ended text generation, it is well-documented that LLMs tend\nto generate repetitive and dull sequences, a phenomenon that is especially\napparent when generating using greedy decoding. This issue persists even with\nstate-of-the-art LLMs containing billions of parameters, trained via next-token\nprediction on large datasets. We find that by further fine-tuning these models\nto achieve a near-zero training loss on a small set of samples -- a process we\nrefer to as hyperfitting -- the long-sequence generative capabilities are\ngreatly enhanced. Greedy decoding with these Hyperfitted models even outperform\nTop-P sampling over long-sequences, both in terms of diversity and human\npreferences. This phenomenon extends to LLMs of various sizes, different\ndomains, and even autoregressive image generation. We further find this\nphenomena to be distinctly different from that of Grokking and double descent.\nSurprisingly, our experiments indicate that hyperfitted models rarely fall into\nrepeating sequences they were trained on, and even explicitly blocking these\nsequences results in high-quality output. All hyperfitted models produce\nextremely low-entropy predictions, often allocating nearly all probability to a\nsingle token.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Under review at ICLR",
    "pdf_url": "http://arxiv.org/pdf/2412.04318v2",
    "published_date": "2024-12-05 16:34:20 UTC",
    "updated_date": "2025-02-26 17:51:31 UTC"
  },
  {
    "arxiv_id": "2412.04315v2",
    "title": "Densing Law of LLMs",
    "authors": [
      "Chaojun Xiao",
      "Jie Cai",
      "Weilin Zhao",
      "Guoyang Zeng",
      "Biyuan Lin",
      "Jie Zhou",
      "Zhi Zheng",
      "Xu Han",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Large Language Models (LLMs) have emerged as a milestone in artificial\nintelligence, and their performance can improve as the model size increases.\nHowever, this scaling brings great challenges to training and inference\nefficiency, particularly for deploying LLMs in resource-constrained\nenvironments, and the scaling trend is becoming increasingly unsustainable.\nThis paper introduces the concept of ``\\textit{capacity density}'' as a new\nmetric to evaluate the quality of the LLMs across different scales and\ndescribes the trend of LLMs in terms of both effectiveness and efficiency. To\ncalculate the capacity density of a given target LLM, we first introduce a set\nof reference models and develop a scaling law to predict the downstream\nperformance of these reference models based on their parameter sizes. We then\ndefine the \\textit{effective parameter size} of the target LLM as the parameter\nsize required by a reference model to achieve equivalent performance, and\nformalize the capacity density as the ratio of the effective parameter size to\nthe actual parameter size of the target LLM. Capacity density provides a\nunified framework for assessing both model effectiveness and efficiency. Our\nfurther analysis of recent open-source base LLMs reveals an empirical law (the\ndensing law)that the capacity density of LLMs grows exponentially over time.\nMore specifically, using some widely used benchmarks for evaluation, the\ncapacity density of LLMs doubles approximately every three months. The law\nprovides new perspectives to guide future LLM development, emphasizing the\nimportance of improving capacity density to achieve optimal results with\nminimal computational overhead.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04315v2",
    "published_date": "2024-12-05 16:31:13 UTC",
    "updated_date": "2024-12-06 11:39:27 UTC"
  },
  {
    "arxiv_id": "2412.04300v2",
    "title": "T2I-FactualBench: Benchmarking the Factuality of Text-to-Image Models with Knowledge-Intensive Concepts",
    "authors": [
      "Ziwei Huang",
      "Wanggui He",
      "Quanyu Long",
      "Yandi Wang",
      "Haoyuan Li",
      "Zhelun Yu",
      "Fangxun Shu",
      "Long Chan",
      "Hao Jiang",
      "Leilei Gan",
      "Fei Wu"
    ],
    "abstract": "Evaluating the quality of synthesized images remains a significant challenge\nin the development of text-to-image (T2I) generation. Most existing studies in\nthis area primarily focus on evaluating text-image alignment, image quality,\nand object composition capabilities, with comparatively fewer studies\naddressing the evaluation of the factuality of T2I models, particularly when\nthe concepts involved are knowledge-intensive. To mitigate this gap, we present\nT2I-FactualBench in this work - the largest benchmark to date in terms of the\nnumber of concepts and prompts specifically designed to evaluate the factuality\nof knowledge-intensive concept generation. T2I-FactualBench consists of a\nthree-tiered knowledge-intensive text-to-image generation framework, ranging\nfrom the basic memorization of individual knowledge concepts to the more\ncomplex composition of multiple knowledge concepts. We further introduce a\nmulti-round visual question answering (VQA) based evaluation framework to\nassess the factuality of three-tiered knowledge-intensive text-to-image\ngeneration tasks. Experiments on T2I-FactualBench indicate that current\nstate-of-the-art (SOTA) T2I models still leave significant room for\nimprovement.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04300v2",
    "published_date": "2024-12-05 16:21:01 UTC",
    "updated_date": "2024-12-07 17:25:28 UTC"
  },
  {
    "arxiv_id": "2412.04292v2",
    "title": "SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model",
    "authors": [
      "Zhenglin Huang",
      "Jinwei Hu",
      "Xiangtai Li",
      "Yiwei He",
      "Xingyu Zhao",
      "Bei Peng",
      "Baoyuan Wu",
      "Xiaowei Huang",
      "Guangliang Cheng"
    ],
    "abstract": "The rapid advancement of generative models in creating highly realistic\nimages poses substantial risks for misinformation dissemination. For instance,\na synthetic image, when shared on social media, can mislead extensive audiences\nand erode trust in digital content, resulting in severe repercussions. Despite\nsome progress, academia has not yet created a large and diversified deepfake\ndetection dataset for social media, nor has it devised an effective solution to\naddress this issue. In this paper, we introduce the Social media Image\nDetection dataSet (SID-Set), which offers three key advantages: (1) extensive\nvolume, featuring 300K AI-generated/tampered and authentic images with\ncomprehensive annotations, (2) broad diversity, encompassing fully synthetic\nand tampered images across various classes, and (3) elevated realism, with\nimages that are predominantly indistinguishable from genuine ones through mere\nvisual inspection. Furthermore, leveraging the exceptional capabilities of\nlarge multimodal models, we propose a new image deepfake detection,\nlocalization, and explanation framework, named SIDA (Social media Image\nDetection, localization, and explanation Assistant). SIDA not only discerns the\nauthenticity of images, but also delineates tampered regions through mask\nprediction and provides textual explanations of the model's judgment criteria.\nCompared with state-of-the-art deepfake detection models on SID-Set and other\nbenchmarks, extensive experiments demonstrate that SIDA achieves superior\nperformance among diversified settings. The code, model, and dataset will be\nreleased.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR-2025",
    "pdf_url": "http://arxiv.org/pdf/2412.04292v2",
    "published_date": "2024-12-05 16:12:25 UTC",
    "updated_date": "2025-03-10 11:03:16 UTC"
  },
  {
    "arxiv_id": "2412.10400v3",
    "title": "Reinforcement Learning Enhanced LLMs: A Survey",
    "authors": [
      "Shuhe Wang",
      "Shengyu Zhang",
      "Jie Zhang",
      "Runyi Hu",
      "Xiaoya Li",
      "Tianwei Zhang",
      "Jiwei Li",
      "Fei Wu",
      "Guoyin Wang",
      "Eduard Hovy"
    ],
    "abstract": "Reinforcement learning (RL) enhanced large language models (LLMs),\nparticularly exemplified by DeepSeek-R1, have exhibited outstanding\nperformance. Despite the effectiveness in improving LLM capabilities, its\nimplementation remains highly complex, requiring complex algorithms, reward\nmodeling strategies, and optimization techniques. This complexity poses\nchallenges for researchers and practitioners in developing a systematic\nunderstanding of RL-enhanced LLMs. Moreover, the absence of a comprehensive\nsurvey summarizing existing research on RL-enhanced LLMs has limited progress\nin this domain, hindering further advancements.\n  In this work, we are going to make a systematic review of the most up-to-date\nstate of knowledge on RL-enhanced LLMs, attempting to consolidate and analyze\nthe rapidly growing research in this field, helping researchers understand the\ncurrent challenges and advancements. Specifically, we (1) detail the basics of\nRL; (2) introduce popular RL-enhanced LLMs; (3) review researches on two\nwidely-used reward model-based RL techniques: Reinforcement Learning from Human\nFeedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF); and (4)\nexplore Direct Preference Optimization (DPO), a set of methods that bypass the\nreward model to directly use human preference data for aligning LLM outputs\nwith human expectations. We will also point out current challenges and\ndeficiencies of existing methods and suggest some avenues for further\nimprovements. Project page of this work can be found at\nhttps://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.10400v3",
    "published_date": "2024-12-05 16:10:42 UTC",
    "updated_date": "2025-02-24 08:57:10 UTC"
  },
  {
    "arxiv_id": "2412.04272v3",
    "title": "PoTable: Towards Systematic Thinking via Stage-oriented Plan-then-Execute Reasoning on Tables",
    "authors": [
      "Qingyang Mao",
      "Qi Liu",
      "Zhi Li",
      "Mingyue Cheng",
      "Zheng Zhang",
      "Rui Li"
    ],
    "abstract": "In recent years, table reasoning has garnered substantial research interest,\nparticularly its integration with Large Language Models (LLMs) which\nrevolutionize natural language applications. Existing typical LLM-based studies\nrealize step-by-step reasoning, promoting the capabilities in table\nunderstanding and analyzing. While these approaches emphasize autonomous\nexploration to accomplish the task objective, they overlook systematic thinking\nin the reasoning process, leading to potential risks of omitted steps,\ndisorganized logic and misleading results. In this paper, we propose PoTable, a\nnovel stage-oriented plan-then-execute reasoning approach that achieves\nsystematic thinking on tables. Specifically, PoTable deploys several distinct\ntabular analytical stages with clear objectives and achieves stage-by-stage\nreasoning. To accomplish the stage-specific goal, PoTable conducts\nplan-then-execute reasoning, which first plans the operation chain under the\nstage objective, and then executes each operation sequentially through code\ngeneration, real-time running and feedback processing. As a result, PoTable can\nproduce reliable table reasoning results with highly accurate, steply commented\nand completely executable programs. It possesses a high degree of alignment\nwith a distinguished tabular data analyst, offering advantages of high accuracy\nand explainability. Finally, we conduct extensive experiments over four\nevaluation datasets from WikiTQ and TabFact benchmarks, where the results\ndemonstrate the effectiveness of PoTable, as well as the efficiency and\nexplainability.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.04272v3",
    "published_date": "2024-12-05 15:54:16 UTC",
    "updated_date": "2025-04-05 10:18:34 UTC"
  },
  {
    "arxiv_id": "2412.04260v1",
    "title": "Enhancing Whole Slide Image Classification through Supervised Contrastive Domain Adaptation",
    "authors": [
      "Ilán Carretero",
      "Pablo Meseguer",
      "Rocío del Amor",
      "Valery Naranjo"
    ],
    "abstract": "Domain shift in the field of histopathological imaging is a common phenomenon\ndue to the intra- and inter-hospital variability of staining and digitization\nprotocols. The implementation of robust models, capable of creating generalized\ndomains, represents a need to be solved. In this work, a new domain adaptation\nmethod to deal with the variability between histopathological images from\nmultiple centers is presented. In particular, our method adds a training\nconstraint to the supervised contrastive learning approach to achieve domain\nadaptation and improve inter-class separability. Experiments performed on\ndomain adaptation and classification of whole-slide images of six skin cancer\nsubtypes from two centers demonstrate the method's usefulness. The results\nreflect superior performance compared to not using domain adaptation after\nfeature extraction or staining normalization.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in CASEIB 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.04260v1",
    "published_date": "2024-12-05 15:39:54 UTC",
    "updated_date": "2024-12-05 15:39:54 UTC"
  },
  {
    "arxiv_id": "2412.04256v1",
    "title": "Transient Multi-Agent Path Finding for Lifelong Navigation in Dense Environments",
    "authors": [
      "Jonathan Morag",
      "Noy Gabay",
      "Daniel koyfman",
      "Roni Stern"
    ],
    "abstract": "Multi-Agent Path Finding (MAPF) deals with finding conflict-free paths for a\nset of agents from an initial configuration to a given target configuration.\nThe Lifelong MAPF (LMAPF) problem is a well-studied online version of MAPF in\nwhich an agent receives a new target when it reaches its current target. The\ncommon approach for solving LMAPF is to treat it as a sequence of MAPF\nproblems, periodically replanning from the agents' current configurations to\ntheir current targets. A significant drawback in this approach is that in MAPF\nthe agents must reach a configuration in which all agents are at their targets\nsimultaneously, which is needlessly restrictive for LMAPF. Techniques have been\nproposed to indirectly mitigate this drawback. We describe cases where these\nmitigation techniques fail. As an alternative, we propose to solve LMAPF\nproblems by solving a sequence of modified MAPF problems, in which the\nobjective is for each agent to eventually visit its target, but not necessarily\nfor all agents to do so simultaneously. We refer to this MAPF variant as\nTransient MAPF (TMAPF) and propose several algorithms for solving it based on\nexisting MAPF algorithms. A limited experimental evaluation identifies some\ncases where using a TMAPF algorithm instead of a MAPF algorithm with an LMAPF\nframework can improve the system throughput significantly.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "Submitted to The 35th International Conference on Automated Planning\n  and Scheduling (ICAPS 2025)",
    "pdf_url": "http://arxiv.org/pdf/2412.04256v1",
    "published_date": "2024-12-05 15:37:29 UTC",
    "updated_date": "2024-12-05 15:37:29 UTC"
  },
  {
    "arxiv_id": "2412.04254v1",
    "title": "CLINICSUM: Utilizing Language Models for Generating Clinical Summaries from Patient-Doctor Conversations",
    "authors": [
      "Subash Neupane",
      "Himanshu Tripathi",
      "Shaswata Mitra",
      "Sean Bozorgzad",
      "Sudip Mittal",
      "Shahram Rahimi",
      "Amin Amirlatifi"
    ],
    "abstract": "This paper presents ClinicSum, a novel framework designed to automatically\ngenerate clinical summaries from patient-doctor conversations. It utilizes a\ntwo-module architecture: a retrieval-based filtering module that extracts\nSubjective, Objective, Assessment, and Plan (SOAP) information from\nconversation transcripts, and an inference module powered by fine-tuned\nPre-trained Language Models (PLMs), which leverage the extracted SOAP data to\ngenerate abstracted clinical summaries. To fine-tune the PLM, we created a\ntraining dataset of consisting 1,473 conversations-summaries pair by\nconsolidating two publicly available datasets, FigShare and MTS-Dialog, with\nground truth summaries validated by Subject Matter Experts (SMEs). ClinicSum's\neffectiveness is evaluated through both automatic metrics (e.g., ROUGE,\nBERTScore) and expert human assessments. Results show that ClinicSum\noutperforms state-of-the-art PLMs, demonstrating superior precision, recall,\nand F-1 scores in automatic evaluations and receiving high preference from SMEs\nin human assessment, making it a robust solution for automated clinical\nsummarization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "accepted at the the 2024 IEEE International Conference on Big Data\n  workshop Workshop on Big Data and AI for Healthcare",
    "pdf_url": "http://arxiv.org/pdf/2412.04254v1",
    "published_date": "2024-12-05 15:34:02 UTC",
    "updated_date": "2024-12-05 15:34:02 UTC"
  },
  {
    "arxiv_id": "2412.04234v3",
    "title": "DEIM: DETR with Improved Matching for Fast Convergence",
    "authors": [
      "Shihua Huang",
      "Zhichao Lu",
      "Xiaodong Cun",
      "Yongjun Yu",
      "Xiao Zhou",
      "Xi Shen"
    ],
    "abstract": "We introduce DEIM, an innovative and efficient training framework designed to\naccelerate convergence in real-time object detection with Transformer-based\narchitectures (DETR). To mitigate the sparse supervision inherent in one-to-one\n(O2O) matching in DETR models, DEIM employs a Dense O2O matching strategy. This\napproach increases the number of positive samples per image by incorporating\nadditional targets, using standard data augmentation techniques. While Dense\nO2O matching speeds up convergence, it also introduces numerous low-quality\nmatches that could affect performance. To address this, we propose the\nMatchability-Aware Loss (MAL), a novel loss function that optimizes matches\nacross various quality levels, enhancing the effectiveness of Dense O2O.\nExtensive experiments on the COCO dataset validate the efficacy of DEIM. When\nintegrated with RT-DETR and D-FINE, it consistently boosts performance while\nreducing training time by 50%. Notably, paired with RT-DETRv2, DEIM achieves\n53.2% AP in a single day of training on an NVIDIA 4090 GPU. Additionally,\nDEIM-trained real-time models outperform leading real-time object detectors,\nwith DEIM-D-FINE-L and DEIM-D-FINE-X achieving 54.7% and 56.5% AP at 124 and 78\nFPS on an NVIDIA T4 GPU, respectively, without the need for additional data. We\nbelieve DEIM sets a new baseline for advancements in real-time object\ndetection. Our code and pre-trained models are available at\nhttps://github.com/ShihuaHuang95/DEIM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.04234v3",
    "published_date": "2024-12-05 15:10:13 UTC",
    "updated_date": "2025-03-26 10:41:29 UTC"
  },
  {
    "arxiv_id": "2412.04233v2",
    "title": "HyperMARL: Adaptive Hypernetworks for Multi-Agent RL",
    "authors": [
      "Kale-ab Abebe Tessera",
      "Arrasy Rahman",
      "Stefano V. Albrecht"
    ],
    "abstract": "Adaptability is critical in cooperative multi-agent reinforcement learning\n(MARL), where agents must learn specialised or homogeneous behaviours for\ndiverse tasks. While parameter sharing methods are sample-efficient, they often\nencounter gradient interference among agents, limiting their behavioural\ndiversity. Conversely, non-parameter sharing approaches enable specialisation,\nbut are computationally demanding and sample-inefficient. To address these\nissues, we propose HyperMARL, a parameter sharing approach that uses\nhypernetworks to dynamically generate agent-specific actor and critic\nparameters, without altering the learning objective or requiring preset\ndiversity levels. By decoupling observation- and agent-conditioned gradients,\nHyperMARL empirically reduces policy gradient variance and facilitates\nspecialisation within FuPS, suggesting it can mitigate cross-agent\ninterference. Across multiple MARL benchmarks involving up to twenty agents --\nand requiring homogeneous, heterogeneous, or mixed behaviours -- HyperMARL\nconsistently performs competitively with fully shared, non-parameter-sharing,\nand diversity-promoting baselines, all while preserving a behavioural diversity\nlevel comparable to non-parameter sharing. These findings establish\nhypernetworks as a versatile approach for MARL across diverse environments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04233v2",
    "published_date": "2024-12-05 15:09:51 UTC",
    "updated_date": "2025-02-07 11:46:12 UTC"
  },
  {
    "arxiv_id": "2412.04220v1",
    "title": "Customize Segment Anything Model for Multi-Modal Semantic Segmentation with Mixture of LoRA Experts",
    "authors": [
      "Chenyang Zhu",
      "Bin Xiao",
      "Lin Shi",
      "Shoukun Xu",
      "Xu Zheng"
    ],
    "abstract": "The recent Segment Anything Model (SAM) represents a significant breakthrough\nin scaling segmentation models, delivering strong performance across various\ndownstream applications in the RGB modality. However, directly applying SAM to\nemerging visual modalities, such as depth and event data results in suboptimal\nperformance in multi-modal segmentation tasks. In this paper, we make the first\nattempt to adapt SAM for multi-modal semantic segmentation by proposing a\nMixture of Low-Rank Adaptation Experts (MoE-LoRA) tailored for different input\nvisual modalities. By training only the MoE-LoRA layers while keeping SAM's\nweights frozen, SAM's strong generalization and segmentation capabilities can\nbe preserved for downstream tasks. Specifically, to address cross-modal\ninconsistencies, we propose a novel MoE routing strategy that adaptively\ngenerates weighted features across modalities, enhancing multi-modal feature\nintegration. Additionally, we incorporate multi-scale feature extraction and\nfusion by adapting SAM's segmentation head and introducing an auxiliary\nsegmentation head to combine multi-scale features for improved segmentation\nperformance effectively. Extensive experiments were conducted on three\nmulti-modal benchmarks: DELIVER, MUSES, and MCubeS. The results consistently\ndemonstrate that the proposed method significantly outperforms state-of-the-art\napproaches across diverse scenarios. Notably, under the particularly\nchallenging condition of missing modalities, our approach exhibits a\nsubstantial performance gain, achieving an improvement of 32.15% compared to\nexisting methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04220v1",
    "published_date": "2024-12-05 14:54:31 UTC",
    "updated_date": "2024-12-05 14:54:31 UTC"
  },
  {
    "arxiv_id": "2412.04202v1",
    "title": "Relationships between Keywords and Strong Beats in Lyrical Music",
    "authors": [
      "Callie C. Liao",
      "Duoduo Liao",
      "Ellie L. Zhang"
    ],
    "abstract": "Artificial Intelligence (AI) song generation has emerged as a popular topic,\nyet the focus on exploring the latent correlations between specific lyrical and\nrhythmic features remains limited. In contrast, this pilot study particularly\ninvestigates the relationships between keywords and rhythmically stressed\nfeatures such as strong beats in songs. It focuses on several key elements:\nkeywords or non-keywords, stressed or unstressed syllables, and strong or weak\nbeats, with the aim of uncovering insightful correlations. Experimental results\nindicate that, on average, 80.8\\% of keywords land on strong beats, whereas\n62\\% of non-keywords fall on weak beats. The relationship between stressed\nsyllables and strong or weak beats is weak, revealing that keywords have the\nstrongest relationships with strong beats. Additionally, the lyrics-rhythm\nmatching score, a key matching metric measuring keywords on strong beats and\nnon-keywords on weak beats across various time signatures, is 0.765, while the\nmatching score for syllable types is 0.495. This study demonstrates that word\ntypes strongly align with their corresponding beat types, as evidenced by the\ndistinct patterns, whereas syllable types exhibit a much weaker alignment. This\ndisparity underscores the greater reliability of word types in capturing\nrhythmic structures in music, highlighting their crucial role in effective\nrhythmic matching and analysis. We also conclude that keywords that\nconsistently align with strong beats are more reliable indicators of\nlyrics-rhythm associations, providing valuable insights for AI-driven song\ngeneration through enhanced structural analysis. Furthermore, our development\nof tailored Lyrics-Rhythm Matching (LRM) metrics maximizes lyrical alignments\nwith corresponding beat stresses, and our novel LRM file format captures\ncritical lyrical and rhythmic information without needing original sheet music.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted by IEEE BigData 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.04202v1",
    "published_date": "2024-12-05 14:40:27 UTC",
    "updated_date": "2024-12-05 14:40:27 UTC"
  },
  {
    "arxiv_id": "2412.04190v1",
    "title": "Directed Structural Adaptation to Overcome Statistical Conflicts and Enable Continual Learning",
    "authors": [
      "Zeki Doruk Erden",
      "Boi Faltings"
    ],
    "abstract": "Adaptive networks today rely on overparameterized fixed topologies that\ncannot break through the statistical conflicts they encounter in the data they\nare exposed to, and are prone to \"catastrophic forgetting\" as the network\nattempts to reuse the existing structures to learn new task. We propose a\nstructural adaptation method, DIRAD, that can complexify as needed and in a\ndirected manner without being limited by statistical conflicts within a\ndataset. We then extend this method and present the PREVAL framework, designed\nto prevent \"catastrophic forgetting\" in continual learning by detection of new\ndata and assigning encountered data to suitable models adapted to process them,\nwithout needing task labels anywhere in the workflow. We show the reliability\nof the DIRAD in growing a network with high performance and orders-of-magnitude\nsimpler than fixed topology networks; and demonstrate the proof-of-concept\noperation of PREVAL, in which continual adaptation to new tasks is observed\nwhile being able to detect and discern previously-encountered tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Presented in Deployable AI (DAI) workshop at AAAI-2024",
    "pdf_url": "http://arxiv.org/pdf/2412.04190v1",
    "published_date": "2024-12-05 14:30:18 UTC",
    "updated_date": "2024-12-05 14:30:18 UTC"
  },
  {
    "arxiv_id": "2412.04185v1",
    "title": "Leveraging Large Language Models to Generate Course-specific Semantically Annotated Learning Objects",
    "authors": [
      "Dominic Lohr",
      "Marc Berges",
      "Abhishek Chugh",
      "Michael Kohlhase",
      "Dennis Müller"
    ],
    "abstract": "Background: Over the past few decades, the process and methodology of\nautomated question generation (AQG) have undergone significant transformations.\nRecent progress in generative natural language models has opened up new\npotential in the generation of educational content.\n  Objectives: This paper explores the potential of large language models (LLMs)\nfor generating computer science questions that are sufficiently annotated for\nautomatic learner model updates, are fully situated in the context of a\nparticular course, and address the cognitive dimension understand.\n  Methods: Unlike previous attempts that might use basic methods like ChatGPT,\nour approach involves more targeted strategies such as retrieval-augmented\ngeneration (RAG) to produce contextually relevant and pedagogically meaningful\nlearning objects.\n  Results and Conclusions: Our results show that generating structural,\nsemantic annotations works well. However, this success was not reflected in the\ncase of relational annotations. The quality of the generated questions often\ndid not meet educational standards, highlighting that although LLMs can\ncontribute to the pool of learning materials, their current level of\nperformance requires significant human intervention to refine and validate the\ngenerated content.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted at Journal of Computer Assisted Learning (2024)",
    "pdf_url": "http://arxiv.org/pdf/2412.04185v1",
    "published_date": "2024-12-05 14:24:07 UTC",
    "updated_date": "2024-12-05 14:24:07 UTC"
  },
  {
    "arxiv_id": "2412.04167v1",
    "title": "Bench-CoE: a Framework for Collaboration of Experts from Benchmark",
    "authors": [
      "Yuanshuai Wang",
      "Xingjian Zhang",
      "Jinkun Zhao",
      "Siwei Wen",
      "Peilin Feng",
      "Shuhao Liao",
      "Lei Huang",
      "Wenjun Wu"
    ],
    "abstract": "Large Language Models (LLMs) are key technologies driving intelligent systems\nto handle multiple tasks. To meet the demands of various tasks, an increasing\nnumber of LLMs-driven experts with diverse capabilities have been developed,\naccompanied by corresponding benchmarks to evaluate their performance. This\npaper proposes the Bench-CoE framework, which enables Collaboration of Experts\n(CoE) by effectively leveraging benchmark evaluations to achieve optimal\nperformance across various tasks. Bench-CoE includes a set of expert models, a\nrouter for assigning tasks to corresponding experts, and a benchmark dataset\nfor training the router. Moreover, we formulate Query-Level and Subject-Level\napproaches based on our framework, and analyze the merits and drawbacks of\nthese two approaches. Finally, we conduct a series of experiments with vary\ndata distributions on both language and multimodal tasks to validate that our\nproposed Bench-CoE outperforms any single model in terms of overall\nperformance. We hope this method serves as a baseline for further research in\nthis area. The code is available at\n\\url{https://github.com/ZhangXJ199/Bench-CoE}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "The code is available at\n  \\url{https://github.com/ZhangXJ199/Bench-CoE}",
    "pdf_url": "http://arxiv.org/pdf/2412.04167v1",
    "published_date": "2024-12-05 14:03:41 UTC",
    "updated_date": "2024-12-05 14:03:41 UTC"
  },
  {
    "arxiv_id": "2412.04149v2",
    "title": "Frequency-Adaptive Low-Latency Object Detection Using Events and Frames",
    "authors": [
      "Haitian Zhang",
      "Xiangyuan Wang",
      "Chang Xu",
      "Xinya Wang",
      "Fang Xu",
      "Huai Yu",
      "Lei Yu",
      "Wen Yang"
    ],
    "abstract": "Fusing Events and RGB images for object detection leverages the robustness of\nEvent cameras in adverse environments and the rich semantic information\nprovided by RGB cameras. However, two critical mismatches: low-latency Events\n\\textit{vs.}~high-latency RGB frames; temporally sparse labels in training\n\\textit{vs.}~continuous flow in inference, significantly hinder the\nhigh-frequency fusion-based object detection. To address these challenges, we\npropose the \\textbf{F}requency-\\textbf{A}daptive Low-Latency \\textbf{O}bject\n\\textbf{D}etector (FAOD). FAOD aligns low-frequency RGB frames with\nhigh-frequency Events through an Align Module, which reinforces cross-modal\nstyle and spatial proximity to address the Event-RGB Mismatch. We further\npropose a training strategy, Time Shift, which enforces the module to align the\nprediction from temporally shifted Event-RGB pairs and their original\nrepresentation, that is, consistent with Event-aligned annotations. This\nstrategy enables the network to use high-frequency Event data as the primary\nreference while treating low-frequency RGB images as supplementary information,\nretaining the low-latency nature of the Event stream toward high-frequency\ndetection. Furthermore, we observe that these corrected Event-RGB pairs\ndemonstrate better generalization from low training frequency to higher\ninference frequencies compared to using Event data alone. Extensive experiments\non the PKU-DAVIS-SOD and DSEC-Detection datasets demonstrate that our FAOD\nachieves SOTA performance. Specifically, in the PKU-DAVIS-SOD Dataset, FAOD\nachieves 9.8 points improvement in terms of the mAP in fully paired Event-RGB\ndata with only a quarter of the parameters compared to SODFormer, and even\nmaintains robust performance (only a 3 points drop in mAP) under 80$\\times$\nEvent-RGB frequency mismatch.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04149v2",
    "published_date": "2024-12-05 13:23:06 UTC",
    "updated_date": "2025-02-27 12:43:29 UTC"
  },
  {
    "arxiv_id": "2412.04144v3",
    "title": "If You Can't Use Them, Recycle Them: Optimizing Merging at Scale Mitigates Performance Tradeoffs",
    "authors": [
      "Muhammad Khalifa",
      "Yi-Chern Tan",
      "Arash Ahmadian",
      "Tom Hosking",
      "Honglak Lee",
      "Lu Wang",
      "Ahmet Üstün",
      "Tom Sherborne",
      "Matthias Gallé"
    ],
    "abstract": "Model merging has shown great promise at combining expert models, but the\nbenefit of merging is unclear when merging \"generalist\" models trained on many\ntasks. We explore merging in the context of large (~100B) models, by recycling\ncheckpoints that exhibit tradeoffs among different tasks. Such checkpoints are\noften created in the process of developing a frontier model, and the suboptimal\nones are usually discarded. Given a pool of model checkpoints obtained from\ndifferent training runs (e.g., different stages, objectives, hyperparameters,\nand data mixtures), which naturally show tradeoffs across different language\ncapabilities (e.g., instruction following vs. code generation), we investigate\nwhether merging can recycle such suboptimal models into a Pareto-optimal one.\nOur optimization algorithm tunes the weight of each checkpoint in a linear\ncombination, resulting in such an optimal model that outperforms both\nindividual models and merge-based baselines. Further analysis shows that good\nmerges tend to include almost all checkpoints with non-zero weights, indicating\nthat even seemingly bad initial checkpoints can contribute to good final\nmerges.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.04144v3",
    "published_date": "2024-12-05 13:12:51 UTC",
    "updated_date": "2025-02-03 20:31:39 UTC"
  },
  {
    "arxiv_id": "2412.04142v1",
    "title": "Methodology for Online Estimation of Rheological Parameters in Polymer Melts Using Deep Learning and Microfluidics",
    "authors": [
      "Juan Sandubete-López",
      "José L. Risco-Martín",
      "Alexander H. McMillan",
      "Eva Besada-Portas"
    ],
    "abstract": "Microfluidic devices are increasingly used in biological and chemical\nexperiments due to their cost-effectiveness for rheological estimation in\nfluids. However, these devices often face challenges in terms of accuracy,\nsize, and cost. This study presents a methodology, integrating deep learning,\nmodeling and simulation to enhance the design of microfluidic systems, used to\ndevelop an innovative approach for viscosity measurement of polymer melts. We\nuse synthetic data generated from the simulations to train a deep learning\nmodel, which then identifies rheological parameters of polymer melts from\npressure drop and flow rate measurements in a microfluidic circuit, enabling\nonline estimation of fluid properties. By improving the accuracy and\nflexibility of microfluidic rheological estimation, our methodology accelerates\nthe design and testing of microfluidic devices, reducing reliance on physical\nprototypes, and offering significant contributions to the field.",
    "categories": [
      "physics.flu-dyn",
      "cs.AI"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "12 pages, 6 figures, Winter Simulation Conference 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.04142v1",
    "published_date": "2024-12-05 13:11:04 UTC",
    "updated_date": "2024-12-05 13:11:04 UTC"
  },
  {
    "arxiv_id": "2412.04140v2",
    "title": "Understanding Memorization in Generative Models via Sharpness in Probability Landscapes",
    "authors": [
      "Dongjae Jeon",
      "Dueun Kim",
      "Albert No"
    ],
    "abstract": "In this paper, we introduce a geometric framework to analyze memorization in\ndiffusion models through the sharpness of the log probability density. We\nmathematically justify a previously proposed score-difference-based\nmemorization metric by demonstrating its effectiveness in quantifying\nsharpness. Additionally, we propose a novel memorization metric that captures\nsharpness at the initial stage of image generation in latent diffusion models,\noffering early insights into potential memorization. Leveraging this metric, we\ndevelop a mitigation strategy that optimizes the initial noise of the\ngeneration process using a sharpness-aware regularization term.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04140v2",
    "published_date": "2024-12-05 13:07:24 UTC",
    "updated_date": "2025-03-02 00:00:08 UTC"
  },
  {
    "arxiv_id": "2412.04139v3",
    "title": "Monet: Mixture of Monosemantic Experts for Transformers",
    "authors": [
      "Jungwoo Park",
      "Young Jin Ahn",
      "Kee-Eung Kim",
      "Jaewoo Kang"
    ],
    "abstract": "Understanding the internal computations of large language models (LLMs) is\ncrucial for aligning them with human values and preventing undesirable\nbehaviors like toxic content generation. However, mechanistic interpretability\nis hindered by polysemanticity -- where individual neurons respond to multiple,\nunrelated concepts. While Sparse Autoencoders (SAEs) have attempted to\ndisentangle these features through sparse dictionary learning, they have\ncompromised LLM performance due to reliance on post-hoc reconstruction loss. To\naddress this issue, we introduce Mixture of Monosemantic Experts for\nTransformers (Monet) architecture, which incorporates sparse dictionary\nlearning directly into end-to-end Mixture-of-Experts pretraining. Our novel\nexpert decomposition method enables scaling the expert count to 262,144 per\nlayer while total parameters scale proportionally to the square root of the\nnumber of experts. Our analyses demonstrate mutual exclusivity of knowledge\nacross experts and showcase the parametric knowledge encapsulated within\nindividual experts. Moreover, Monet allows knowledge manipulation over domains,\nlanguages, and toxicity mitigation without degrading general performance. Our\npursuit of transparent LLMs highlights the potential of scaling expert counts\nto enhance mechanistic interpretability and directly resect the internal\nknowledge to fundamentally adjust model behavior. The source code and\npretrained checkpoints are available at https://github.com/dmis-lab/Monet.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04139v3",
    "published_date": "2024-12-05 13:06:03 UTC",
    "updated_date": "2025-03-02 14:52:21 UTC"
  },
  {
    "arxiv_id": "2412.04137v1",
    "title": "Text Change Detection in Multilingual Documents Using Image Comparison",
    "authors": [
      "Doyoung Park",
      "Naresh Reddy Yarram",
      "Sunjin Kim",
      "Minkyu Kim",
      "Seongho Cho",
      "Taehee Lee"
    ],
    "abstract": "Document comparison typically relies on optical character recognition (OCR)\nas its core technology. However, OCR requires the selection of appropriate\nlanguage models for each document and the performance of multilingual or hybrid\nmodels remains limited. To overcome these challenges, we propose text change\ndetection (TCD) using an image comparison model tailored for multilingual\ndocuments. Unlike OCR-based approaches, our method employs word-level text\nimage-to-image comparison to detect changes. Our model generates bidirectional\nchange segmentation maps between the source and target documents. To enhance\nperformance without requiring explicit text alignment or scaling preprocessing,\nwe employ correlations among multi-scale attention features. We also construct\na benchmark dataset comprising actual printed and scanned word pairs in various\nlanguages to evaluate our model. We validate our approach using our benchmark\ndataset and public benchmarks Distorted Document Images and the LRDE Document\nBinarization Dataset. We compare our model against state-of-the-art semantic\nsegmentation and change detection models, as well as to conventional OCR-based\nmodels.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "15pages, 11figures 6tables, wacv2025 accepted",
    "pdf_url": "http://arxiv.org/pdf/2412.04137v1",
    "published_date": "2024-12-05 13:04:10 UTC",
    "updated_date": "2024-12-05 13:04:10 UTC"
  },
  {
    "arxiv_id": "2412.04121v1",
    "title": "DeepFEA: Deep Learning for Prediction of Transient Finite Element Analysis Solutions",
    "authors": [
      "Georgios Triantafyllou",
      "Panagiotis G. Kalozoumis",
      "George Dimas",
      "Dimitris K. Iakovidis"
    ],
    "abstract": "Finite Element Analysis (FEA) is a powerful but computationally intensive\nmethod for simulating physical phenomena. Recent advancements in machine\nlearning have led to surrogate models capable of accelerating FEA. Yet there\nare still limitations in developing surrogates of transient FEA models that can\nsimultaneously predict the solutions for both nodes and elements with\napplicability on both the 2D and 3D domains. Motivated by this research gap,\nthis study proposes DeepFEA, a deep learning-based framework that leverages a\nmultilayer Convolutional Long Short-Term Memory (ConvLSTM) network branching\ninto two parallel convolutional neural networks to predict the solutions for\nboth nodes and elements of FEA models. The proposed network is optimized using\na novel adaptive learning algorithm, called Node-Element Loss Optimization\n(NELO). NELO minimizes the error occurring at both branches of the network\nenabling the prediction of solutions for transient FEA simulations. The\nexperimental evaluation of DeepFEA is performed on three datasets in the\ncontext of structural mechanics, generated to serve as publicly available\nreference datasets. The results show that DeepFEA can achieve less than 3%\nnormalized mean and root mean squared error for 2D and 3D simulation scenarios,\nand inference times that are two orders of magnitude faster than FEA. In\ncontrast, relevant state-of-the-art methods face challenges with\nmulti-dimensional output and dynamic input prediction. Furthermore, DeepFEA's\nrobustness was demonstrated in a real-life biomedical scenario, confirming its\nsuitability for accurate and efficient predictions of FEA simulations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "comment": "This work has been submitted to a journal for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2412.04121v1",
    "published_date": "2024-12-05 12:46:18 UTC",
    "updated_date": "2024-12-05 12:46:18 UTC"
  },
  {
    "arxiv_id": "2412.04114v1",
    "title": "Thermal and RGB Images Work Better Together in Wind Turbine Damage Detection",
    "authors": [
      "Serhii Svystun",
      "Oleksandr Melnychenko",
      "Pavlo Radiuk",
      "Oleg Savenko",
      "Anatoliy Sachenko",
      "Andrii Lysyi"
    ],
    "abstract": "The inspection of wind turbine blades (WTBs) is crucial for ensuring their\nstructural integrity and operational efficiency. Traditional inspection methods\ncan be dangerous and inefficient, prompting the use of unmanned aerial vehicles\n(UAVs) that access hard-to-reach areas and capture high-resolution imagery. In\nthis study, we address the challenge of enhancing defect detection on WTBs by\nintegrating thermal and RGB images obtained from UAVs. We propose a\nmultispectral image composition method that combines thermal and RGB imagery\nthrough spatial coordinate transformation, key point detection, binary\ndescriptor creation, and weighted image overlay. Using a benchmark dataset of\nWTB images annotated for defects, we evaluated several state-of-the-art object\ndetection models. Our results show that composite images significantly improve\ndefect detection efficiency. Specifically, the YOLOv8 model's accuracy\nincreased from 91% to 95%, precision from 89% to 94%, recall from 85% to 92%,\nand F1-score from 87% to 93%. The number of false positives decreased from 6 to\n3, and missed defects reduced from 5 to 2. These findings demonstrate that\nintegrating thermal and RGB imagery enhances defect detection on WTBs,\ncontributing to improved maintenance and reliability.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO",
      "I.4.8; I.4.6; I.2.10; I.2.9"
    ],
    "primary_category": "cs.CV",
    "comment": "Unmanned aerial vehicle, image composition, multispectral images,\n  green energy, data quality management, weighted overlay",
    "pdf_url": "http://arxiv.org/pdf/2412.04114v1",
    "published_date": "2024-12-05 12:32:45 UTC",
    "updated_date": "2024-12-05 12:32:45 UTC"
  },
  {
    "arxiv_id": "2412.04110v1",
    "title": "Enhancing Mathematical Reasoning in LLMs with Background Operators",
    "authors": [
      "Jiajun Chen",
      "Yik-Cheung Tam"
    ],
    "abstract": "We propose utilizing background operators for mathematical reasoning in large\nlanguage models (LLMs). To achieve this, we define a set of fundamental\nmathematical predicates as the basic building blocks. For each mathematical\nproblem, we develop a Prolog solution that includes problem-specific predicates\nand intermediate predicates derived from these background operators, ensuring\nthat each solution adheres to the defined operator set. We introduce the\nMATH-Prolog corpus, which is derived from the counting and probability\ncategories of the MATH corpus. For efficient data augmentation, we apply K-fold\ncross-validated self-training. This method incrementally generates new Prolog\nsolutions for each fold, incorporating those verified as correct into the\ntraining set throughout the model training process. Our experimental results\ndemonstrate that 5-fold crossvalidated self-training effectively identifies\nnew, accurate Prolog solutions, achieving an accuracy of 84.6% on the\ncross-validated set, and 84.8% on the test set during fine-tuning the\nMeta-Llama-3.1-8B-Instruct model. This approach successfully uncovers new\nsolutions with fully computable inference steps for previously unseen problems.\nAdditionally, incorporating the background mathematical predicates into the\nprompt enhances solution coverage.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04110v1",
    "published_date": "2024-12-05 12:24:54 UTC",
    "updated_date": "2024-12-05 12:24:54 UTC"
  },
  {
    "arxiv_id": "2412.04107v2",
    "title": "Pre-train, Align, and Disentangle: Empowering Sequential Recommendation with Large Language Models",
    "authors": [
      "Yuhao Wang",
      "Junwei Pan",
      "Pengyue Jia",
      "Wanyu Wang",
      "Maolin Wang",
      "Zhixiang Feng",
      "Xiaotian Li",
      "Jie Jiang",
      "Xiangyu Zhao"
    ],
    "abstract": "Sequential Recommendation (SR) aims to leverage the sequential patterns in\nusers' historical interactions to accurately track their preferences. However,\nthe primary reliance of existing SR methods on collaborative data results in\nchallenges such as the cold-start problem and sub-optimal performance.\nConcurrently, despite the proven effectiveness of large language models (LLMs),\ntheir integration into commercial recommender systems is impeded by issues such\nas high inference latency, incomplete capture of all distribution statistics,\nand catastrophic forgetting. To address these issues, we introduce a novel\nPre-train, Align, and Disentangle (PAD) framework to enhance SR models with\nLLMs. In particular, we initially pre-train both the SR and LLM models to\nobtain collaborative and textual embeddings. Subsequently, we propose a\ncharacteristic recommendation-anchored alignment loss using multi-kernel\nmaximum mean discrepancy with Gaussian kernels. Lastly, a triple-experts\narchitecture, comprising aligned and modality-specific experts with\ndisentangled embeddings, is fine-tuned in a frequency-aware manner.\nExperimental results on three public datasets validate the efficacy of PAD,\nindicating substantial enhancements and compatibility with various SR backbone\nmodels, particularly for cold items. The code and datasets are accessible for\nreproduction at https://github.com/Applied-Machine-Learning-Lab/PAD.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "accepted to SIGIR 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.04107v2",
    "published_date": "2024-12-05 12:17:56 UTC",
    "updated_date": "2025-04-25 18:32:35 UTC"
  },
  {
    "arxiv_id": "2412.04100v2",
    "title": "Missing Melodies: AI Music Generation and its \"Nearly\" Complete Omission of the Global South",
    "authors": [
      "Atharva Mehta",
      "Shivam Chauhan",
      "Monojit Choudhury"
    ],
    "abstract": "Recent advances in generative AI have sparked renewed interest and expanded\npossibilities for music generation. However, the performance and versatility of\nthese systems across musical genres are heavily influenced by the availability\nof training data. We conducted an extensive analysis of over one million hours\nof audio datasets used in AI music generation research and manually reviewed\nmore than 200 papers from eleven prominent AI and music conferences and\norganizations (AAAI, ACM, EUSIPCO, EURASIP, ICASSP, ICML, IJCAI, ISMIR,\nNeurIPS, NIME, SMC) to identify a critical gap in the fair representation and\ninclusion of the musical genres of the Global South in AI research. Our\nfindings reveal a stark imbalance: approximately 86% of the total dataset hours\nand over 93% of researchers focus primarily on music from the Global North.\nHowever, around 40% of these datasets include some form of non-Western music,\ngenres from the Global South account for only 14.6% of the data. Furthermore,\napproximately 51% of the papers surveyed concentrate on symbolic music\ngeneration, a method that often fails to capture the cultural nuances inherent\nin music from regions such as South Asia, the Middle East, and Africa. As AI\nincreasingly shapes the creation and dissemination of music, the significant\nunderrepresentation of music genres in datasets and research presents a serious\nthreat to global musical diversity. We also propose some important steps to\nmitigate these risks and foster a more inclusive future for AI-driven music\ngeneration.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Submitted to CACM, 12 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.04100v2",
    "published_date": "2024-12-05 12:10:42 UTC",
    "updated_date": "2024-12-12 11:12:03 UTC"
  },
  {
    "arxiv_id": "2412.06822v1",
    "title": "Guidance is All You Need: Temperature-Guided Reasoning in Large Language Models",
    "authors": [
      "Eyad Gomaa",
      "Gomaa Salah"
    ],
    "abstract": "We present Quasar-1, a novel architecture that introduces temperature-guided\nreasoning to large language models through the Token Temperature Mechanism\n(TTM) and Guided Sequence of Thought (GSoT). Our approach leverages the concept\nof hot and cold tokens, where hot tokens are prioritized for their contextual\nrelevance, while cold tokens provide supplementary information. This dynamic\nmodulation of token importance enables the model to achieve superior logical\nreasoning capabilities compared to traditional chain-of-thought approaches.\nThrough rigorous mathematical analysis, we prove that our temperature-guided\nattention mechanism converges to optimal reasoning paths with exponential\nguarantees. Empirical results show significant improvements in reasoning\naccuracy and computational efficiency across a wide range of tasks, making\nadvanced AI reasoning accessible to a broader range of applications.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.06822v1",
    "published_date": "2024-12-05 12:05:41 UTC",
    "updated_date": "2024-12-05 12:05:41 UTC"
  },
  {
    "arxiv_id": "2412.04097v1",
    "title": "D-LORD for Motion Stylization",
    "authors": [
      "Meenakshi Gupta",
      "Mingyuan Lei",
      "Tat-Jen Cham",
      "Hwee Kuan Lee"
    ],
    "abstract": "This paper introduces a novel framework named D-LORD (Double Latent\nOptimization for Representation Disentanglement), which is designed for motion\nstylization (motion style transfer and motion retargeting). The primary\nobjective of this framework is to separate the class and content information\nfrom a given motion sequence using a data-driven latent optimization approach.\nHere, class refers to person-specific style, such as a particular emotion or an\nindividual's identity, while content relates to the style-agnostic aspect of an\naction, such as walking or jumping, as universally understood concepts. The key\nadvantage of D-LORD is its ability to perform style transfer without needing\npaired motion data. Instead, it utilizes class and content labels during the\nlatent optimization process. By disentangling the representation, the framework\nenables the transformation of one motion sequences style to another's style\nusing Adaptive Instance Normalization. The proposed D-LORD framework is\ndesigned with a focus on generalization, allowing it to handle different class\nand content labels for various applications. Additionally, it can generate\ndiverse motion sequences when specific class and content labels are provided.\nThe framework's efficacy is demonstrated through experimentation on three\ndatasets: the CMU XIA dataset for motion style transfer, the MHAD dataset, and\nthe RRIS Ability dataset for motion retargeting. Notably, this paper presents\nthe first generalized framework for motion style transfer and motion\nretargeting, showcasing its potential contributions in this area.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04097v1",
    "published_date": "2024-12-05 12:03:02 UTC",
    "updated_date": "2024-12-05 12:03:02 UTC"
  },
  {
    "arxiv_id": "2412.04093v1",
    "title": "Practical Considerations for Agentic LLM Systems",
    "authors": [
      "Chris Sypherd",
      "Vaishak Belle"
    ],
    "abstract": "As the strength of Large Language Models (LLMs) has grown over recent years,\nso too has interest in their use as the underlying models for autonomous\nagents. Although LLMs demonstrate emergent abilities and broad expertise across\nnatural language domains, their inherent unpredictability makes the\nimplementation of LLM agents challenging, resulting in a gap between related\nresearch and the real-world implementation of such systems. To bridge this gap,\nthis paper frames actionable insights and considerations from the research\ncommunity in the context of established application paradigms to enable the\nconstruction and facilitate the informed deployment of robust LLM agents.\nNamely, we position relevant research findings into four broad\ncategories--Planning, Memory, Tools, and Control Flow--based on common\npractices in application-focused literature and highlight practical\nconsiderations to make when designing agentic LLMs for real-world applications,\nsuch as handling stochasticity and managing resources efficiently. While we do\nnot conduct empirical evaluations, we do provide the necessary background for\ndiscussing critical aspects of agentic LLM designs, both in academia and\nindustry.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 3 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2412.04093v1",
    "published_date": "2024-12-05 11:57:49 UTC",
    "updated_date": "2024-12-05 11:57:49 UTC"
  },
  {
    "arxiv_id": "2412.04086v2",
    "title": "BodyMetric: Evaluating the Realism of Human Bodies in Text-to-Image Generation",
    "authors": [
      "Nefeli Andreou",
      "Varsha Vivek",
      "Ying Wang",
      "Alex Vorobiov",
      "Tiffany Deng",
      "Raja Bala",
      "Larry Davis",
      "Betty Mohler Tesch"
    ],
    "abstract": "Accurately generating images of human bodies from text remains a challenging\nproblem for state of the art text-to-image models. Commonly observed\nbody-related artifacts include extra or missing limbs, unrealistic poses,\nblurred body parts, etc. Currently, evaluation of such artifacts relies heavily\non time-consuming human judgments, limiting the ability to benchmark models at\nscale. We address this by proposing BodyMetric, a learnable metric that\npredicts body realism in images. BodyMetric is trained on realism labels and\nmulti-modal signals including 3D body representations inferred from the input\nimage, and textual descriptions. In order to facilitate this approach, we\ndesign an annotation pipeline to collect expert ratings on human body realism\nleading to a new dataset for this task, namely, BodyRealism. Ablation studies\nsupport our architectural choices for BodyMetric and the importance of\nleveraging a 3D human body prior in capturing body-related artifacts in 2D\nimages. In comparison to concurrent metrics which evaluate general user\npreference in images, BodyMetric specifically reflects body-related artifacts.\nWe demonstrate the utility of BodyMetric through applications that were\npreviously infeasible at scale. In particular, we use BodyMetric to benchmark\nthe generation ability of text-to-image models to produce realistic human\nbodies. We also demonstrate the effectiveness of BodyMetric in ranking\ngenerated images based on the predicted realism scores.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04086v2",
    "published_date": "2024-12-05 11:48:54 UTC",
    "updated_date": "2024-12-06 09:00:39 UTC"
  },
  {
    "arxiv_id": "2412.04081v1",
    "title": "Federated Learning in Mobile Networks: A Comprehensive Case Study on Traffic Forecasting",
    "authors": [
      "Nikolaos Pavlidis",
      "Vasileios Perifanis",
      "Selim F. Yilmaz",
      "Francesc Wilhelmi",
      "Marco Miozzo",
      "Pavlos S. Efraimidis",
      "Remous-Aris Koutsiamanis",
      "Pavol Mulinka",
      "Paolo Dini"
    ],
    "abstract": "The increasing demand for efficient resource allocation in mobile networks\nhas catalyzed the exploration of innovative solutions that could enhance the\ntask of real-time cellular traffic prediction. Under these circumstances,\nfederated learning (FL) stands out as a distributed and privacy-preserving\nsolution to foster collaboration among different sites, thus enabling\nresponsive near-the-edge solutions. In this paper, we comprehensively study the\npotential benefits of FL in telecommunications through a case study on\nfederated traffic forecasting using real-world data from base stations (BSs) in\nBarcelona (Spain). Our study encompasses relevant aspects within the federated\nexperience, including model aggregation techniques, outlier management, the\nimpact of individual clients, personalized learning, and the integration of\nexogenous sources of data. The performed evaluation is based on both prediction\naccuracy and sustainability, thus showcasing the environmental impact of\nemployed FL algorithms in various settings. The findings from our study\nhighlight FL as a promising and robust solution for mobile traffic prediction,\nemphasizing its twin merits as a privacy-conscious and environmentally\nsustainable approach, while also demonstrating its capability to overcome data\nheterogeneity and ensure high-quality predictions, marking a significant stride\ntowards its integration in mobile traffic management systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04081v1",
    "published_date": "2024-12-05 11:32:14 UTC",
    "updated_date": "2024-12-05 11:32:14 UTC"
  },
  {
    "arxiv_id": "2412.04075v1",
    "title": "Does your model understand genes? A benchmark of gene properties for biological and text models",
    "authors": [
      "Yoav Kan-Tor",
      "Michael Morris Danziger",
      "Eden Zohar",
      "Matan Ninio",
      "Yishai Shimoni"
    ],
    "abstract": "The application of deep learning methods, particularly foundation models, in\nbiological research has surged in recent years. These models can be text-based\nor trained on underlying biological data, especially omics data of various\ntypes. However, comparing the performance of these models consistently has\nproven to be a challenge due to differences in training data and downstream\ntasks. To tackle this problem, we developed an architecture-agnostic\nbenchmarking approach that, instead of evaluating the models directly,\nleverages entity representation vectors from each model and trains simple\npredictive models for each benchmarking task. This ensures that all types of\nmodels are evaluated using the same input and output types. Here we focus on\ngene properties collected from professionally curated bioinformatics databases.\nThese gene properties are categorized into five major groups: genomic\nproperties, regulatory functions, localization, biological processes, and\nprotein properties. Overall, we define hundreds of tasks based on these\ndatabases, which include binary, multi-label, and multi-class classification\ntasks. We apply these benchmark tasks to evaluate expression-based models,\nlarge language models, protein language models, DNA-based models, and\ntraditional baselines. Our findings suggest that text-based models and protein\nlanguage models generally outperform expression-based models in genomic\nproperties and regulatory functions tasks, whereas expression-based models\ndemonstrate superior performance in localization tasks. These results should\naid in the development of more informed artificial intelligence strategies for\nbiological understanding and therapeutic discovery. To ensure the\nreproducibility and transparency of our findings, we have made the source code\nand benchmark data publicly accessible for further investigation and expansion\nat github.com/BiomedSciAI/gene-benchmark.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04075v1",
    "published_date": "2024-12-05 11:14:01 UTC",
    "updated_date": "2024-12-05 11:14:01 UTC"
  },
  {
    "arxiv_id": "2412.04069v1",
    "title": "ProtDAT: A Unified Framework for Protein Sequence Design from Any Protein Text Description",
    "authors": [
      "Xiao-Yu Guo",
      "Yi-Fan Li",
      "Yuan Liu",
      "Xiaoyong Pan",
      "Hong-Bin Shen"
    ],
    "abstract": "Protein design has become a critical method in advancing significant\npotential for various applications such as drug development and enzyme\nengineering. However, protein design methods utilizing large language models\nwith solely pretraining and fine-tuning struggle to capture relationships in\nmulti-modal protein data. To address this, we propose ProtDAT, a de novo\nfine-grained framework capable of designing proteins from any descriptive\nprotein text input. ProtDAT builds upon the inherent characteristics of protein\ndata to unify sequences and text as a cohesive whole rather than separate\nentities. It leverages an innovative multi-modal cross-attention, integrating\nprotein sequences and textual information for a foundational level and seamless\nintegration. Experimental results demonstrate that ProtDAT achieves the\nstate-of-the-art performance in protein sequence generation, excelling in\nrationality, functionality, structural similarity, and validity. On 20,000\ntext-sequence pairs from Swiss-Prot, it improves pLDDT by 6%, TM-score by 0.26,\nand reduces RMSD by 1.2 {\\AA}, highlighting its potential to advance protein\ndesign.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04069v1",
    "published_date": "2024-12-05 11:05:46 UTC",
    "updated_date": "2024-12-05 11:05:46 UTC"
  },
  {
    "arxiv_id": "2412.04067v1",
    "title": "Automated Medical Report Generation for ECG Data: Bridging Medical Text and Signal Processing with Deep Learning",
    "authors": [
      "Amnon Bleich",
      "Antje Linnemann",
      "Bjoern H. Diem",
      "Tim OF Conrad"
    ],
    "abstract": "Recent advances in deep learning and natural language generation have\nsignificantly improved image captioning, enabling automated, human-like\ndescriptions for visual content. In this work, we apply these captioning\ntechniques to generate clinician-like interpretations of ECG data. This study\nleverages existing ECG datasets accompanied by free-text reports authored by\nhealthcare professionals (HCPs) as training data. These reports, while often\ninconsistent, provide a valuable foundation for automated learning. We\nintroduce an encoder-decoder-based method that uses these reports to train\nmodels to generate detailed descriptions of ECG episodes. This represents a\nsignificant advancement in ECG analysis automation, with potential applications\nin zero-shot classification and automated clinical decision support.\n  The model is tested on various datasets, including both 1- and 12-lead ECGs.\nIt significantly outperforms the state-of-the-art reference model by Qiu et\nal., achieving a METEOR score of 55.53% compared to 24.51% achieved by the\nreference model. Furthermore, several key design choices are discussed,\nproviding a comprehensive overview of current challenges and innovations in\nthis domain.\n  The source codes for this research are publicly available in our Git\nrepository https://git.zib.de/ableich/ecg-comment-generation-public",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04067v1",
    "published_date": "2024-12-05 11:05:12 UTC",
    "updated_date": "2024-12-05 11:05:12 UTC"
  },
  {
    "arxiv_id": "2412.04064v1",
    "title": "Graph Neural Networks Need Cluster-Normalize-Activate Modules",
    "authors": [
      "Arseny Skryagin",
      "Felix Divo",
      "Mohammad Amin Ali",
      "Devendra Singh Dhami",
      "Kristian Kersting"
    ],
    "abstract": "Graph Neural Networks (GNNs) are non-Euclidean deep learning models for\ngraph-structured data. Despite their successful and diverse applications,\noversmoothing prohibits deep architectures due to node features converging to a\nsingle fixed point. This severely limits their potential to solve complex\ntasks. To counteract this tendency, we propose a plug-and-play module\nconsisting of three steps: Cluster-Normalize-Activate (CNA). By applying CNA\nmodules, GNNs search and form super nodes in each layer, which are normalized\nand activated individually. We demonstrate in node classification and property\nprediction tasks that CNA significantly improves the accuracy over the\nstate-of-the-art. Particularly, CNA reaches 94.18% and 95.75% accuracy on Cora\nand CiteSeer, respectively. It further benefits GNNs in regression tasks as\nwell, reducing the mean squared error compared to all baselines. At the same\ntime, GNNs with CNA require substantially fewer learnable parameters than\ncompeting architectures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68T07",
      "I.2.0"
    ],
    "primary_category": "cs.LG",
    "comment": "17 pages, 6 figures, 6 tables, accepted at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.04064v1",
    "published_date": "2024-12-05 10:59:20 UTC",
    "updated_date": "2024-12-05 10:59:20 UTC"
  },
  {
    "arxiv_id": "2412.04062v2",
    "title": "ZipAR: Accelerating Auto-regressive Image Generation through Spatial Locality",
    "authors": [
      "Yefei He",
      "Feng Chen",
      "Yuanyu He",
      "Shaoxuan He",
      "Hong Zhou",
      "Kaipeng Zhang",
      "Bohan Zhuang"
    ],
    "abstract": "In this paper, we propose ZipAR, a training-free, plug-and-play parallel\ndecoding framework for accelerating auto-regressive (AR) visual generation. The\nmotivation stems from the observation that images exhibit local structures, and\nspatially distant regions tend to have minimal interdependence. Given a\npartially decoded set of visual tokens, in addition to the original next-token\nprediction scheme in the row dimension, the tokens corresponding to spatially\nadjacent regions in the column dimension can be decoded in parallel, enabling\nthe ``next-set prediction'' paradigm. By decoding multiple tokens\nsimultaneously in a single forward pass, the number of forward passes required\nto generate an image is significantly reduced, resulting in a substantial\nimprovement in generation efficiency. Experiments demonstrate that ZipAR can\nreduce the number of model forward passes by up to 91% on the Emu3-Gen model\nwithout requiring any additional retraining. Code is available here:\nhttps://github.com/ThisisBillhe/ZipAR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages",
    "pdf_url": "http://arxiv.org/pdf/2412.04062v2",
    "published_date": "2024-12-05 10:57:08 UTC",
    "updated_date": "2024-12-18 07:28:52 UTC"
  },
  {
    "arxiv_id": "2412.04060v2",
    "title": "Expand Heterogeneous Learning Systems with Selective Multi-Source Knowledge Fusion",
    "authors": [
      "Gaole Dai",
      "Huatao Xu",
      "Yifan Yang",
      "Rui Tan",
      "Mo Li"
    ],
    "abstract": "Expanding existing learning systems to provide high-quality customized models\nfor more domains, such as new users, is challenged by the limited labeled data\nand the data and device heterogeneities. While knowledge distillation methods\ncould overcome label scarcity and device heterogeneity, they assume the\nteachers are fully reliable and overlook the data heterogeneity, which prevents\nthe direct adoption of existing models. To address this problem, this paper\nproposes a framework, HaT, to expand learning systems. It first selects\nmultiple high-quality models from the system at a low cost and then fuses their\nknowledge by assigning sample-wise weights to their predictions. Later, the\nfused knowledge is selectively injected into the customized models based on the\nknowledge quality. Extensive experiments on different tasks, modalities, and\nsettings show that HaT outperforms state-of-the-art baselines by up to 16.5%\naccuracy and saves up to 39% communication traffic.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.04060v2",
    "published_date": "2024-12-05 10:55:54 UTC",
    "updated_date": "2025-02-07 03:21:45 UTC"
  },
  {
    "arxiv_id": "2412.04057v1",
    "title": "From Code to Play: Benchmarking Program Search for Games Using Large Language Models",
    "authors": [
      "Manuel Eberhardinger",
      "James Goodman",
      "Alexander Dockhorn",
      "Diego Perez-Liebana",
      "Raluca D. Gaina",
      "Duygu Çakmak",
      "Setareh Maghsudi",
      "Simon Lucas"
    ],
    "abstract": "Large language models (LLMs) have shown impressive capabilities in generating\nprogram code, opening exciting opportunities for applying program synthesis to\ngames. In this work, we explore the potential of LLMs to directly synthesize\nusable code for a wide range of gaming applications, focusing on two\nprogramming languages, Python and Java. We use an evolutionary hill-climbing\nalgorithm, where the mutations and seeds of the initial programs are controlled\nby LLMs. For Python, the framework covers various game-related tasks, including\nfive miniature versions of Atari games, ten levels of Baba is You, an\nenvironment inspired by Asteroids, and a maze generation task. For Java, the\nframework contains 12 games from the TAG tabletop games framework. Across 29\ntasks, we evaluated 12 language models for Python and 8 for Java. Our findings\nsuggest that the performance of LLMs depends more on the task than on model\nsize. While larger models generate more executable programs, these do not\nalways result in higher-quality solutions but are much more expensive. No model\nhas a clear advantage, although on any specific task, one model may be better.\nTrying many models on a problem and using the best results across them is more\nreliable than using just one.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to Transactions on Games Special Issue on Large Language\n  Models and Games",
    "pdf_url": "http://arxiv.org/pdf/2412.04057v1",
    "published_date": "2024-12-05 10:50:58 UTC",
    "updated_date": "2024-12-05 10:50:58 UTC"
  },
  {
    "arxiv_id": "2412.04037v1",
    "title": "INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations",
    "authors": [
      "Yongming Zhu",
      "Longhao Zhang",
      "Zhengkun Rong",
      "Tianshu Hu",
      "Shuang Liang",
      "Zhipeng Ge"
    ],
    "abstract": "Imagine having a conversation with a socially intelligent agent. It can\nattentively listen to your words and offer visual and linguistic feedback\npromptly. This seamless interaction allows for multiple rounds of conversation\nto flow smoothly and naturally. In pursuit of actualizing it, we propose INFP,\na novel audio-driven head generation framework for dyadic interaction. Unlike\nprevious head generation works that only focus on single-sided communication,\nor require manual role assignment and explicit role switching, our model drives\nthe agent portrait dynamically alternates between speaking and listening state,\nguided by the input dyadic audio. Specifically, INFP comprises a Motion-Based\nHead Imitation stage and an Audio-Guided Motion Generation stage. The first\nstage learns to project facial communicative behaviors from real-life\nconversation videos into a low-dimensional motion latent space, and use the\nmotion latent codes to animate a static image. The second stage learns the\nmapping from the input dyadic audio to motion latent codes through denoising,\nleading to the audio-driven head generation in interactive scenarios. To\nfacilitate this line of research, we introduce DyConv, a large scale dataset of\nrich dyadic conversations collected from the Internet. Extensive experiments\nand visualizations demonstrate superior performance and effectiveness of our\nmethod. Project Page: https://grisoon.github.io/INFP/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04037v1",
    "published_date": "2024-12-05 10:20:34 UTC",
    "updated_date": "2024-12-05 10:20:34 UTC"
  },
  {
    "arxiv_id": "2412.04036v1",
    "title": "SocialMind: LLM-based Proactive AR Social Assistive System with Human-like Perception for In-situ Live Interactions",
    "authors": [
      "Bufang Yang",
      "Yunqi Guo",
      "Lilin Xu",
      "Zhenyu Yan",
      "Hongkai Chen",
      "Guoliang Xing",
      "Xiaofan Jiang"
    ],
    "abstract": "Social interactions are fundamental to human life. The recent emergence of\nlarge language models (LLMs)-based virtual assistants has demonstrated their\npotential to revolutionize human interactions and lifestyles. However, existing\nassistive systems mainly provide reactive services to individual users, rather\nthan offering in-situ assistance during live social interactions with\nconversational partners. In this study, we introduce SocialMind, the first\nLLM-based proactive AR social assistive system that provides users with in-situ\nsocial assistance. SocialMind employs human-like perception leveraging\nmulti-modal sensors to extract both verbal and nonverbal cues, social factors,\nand implicit personas, incorporating these social cues into LLM reasoning for\nsocial suggestion generation. Additionally, SocialMind employs a multi-tier\ncollaborative generation strategy and proactive update mechanism to display\nsocial suggestions on Augmented Reality (AR) glasses, ensuring that suggestions\nare timely provided to users without disrupting the natural flow of\nconversation. Evaluations on three public datasets and a user study with 20\nparticipants show that SocialMind achieves 38.3% higher engagement compared to\nbaselines, and 95% of participants are willing to use SocialMind in their live\nsocial interactions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04036v1",
    "published_date": "2024-12-05 10:19:36 UTC",
    "updated_date": "2024-12-05 10:19:36 UTC"
  },
  {
    "arxiv_id": "2412.04029v1",
    "title": "Considerations Influencing Offense-Defense Dynamics From Artificial Intelligence",
    "authors": [
      "Giulio Corsi",
      "Kyle Kilian",
      "Richard Mallah"
    ],
    "abstract": "The rapid advancement of artificial intelligence (AI) technologies presents\nprofound challenges to societal safety. As AI systems become more capable,\naccessible, and integrated into critical services, the dual nature of their\npotential is increasingly clear. While AI can enhance defensive capabilities in\nareas like threat detection, risk assessment, and automated security\noperations, it also presents avenues for malicious exploitation and large-scale\nsocietal harm, for example through automated influence operations and cyber\nattacks. Understanding the dynamics that shape AI's capacity to both cause harm\nand enhance protective measures is essential for informed decision-making\nregarding the deployment, use, and integration of advanced AI systems. This\npaper builds on recent work on offense-defense dynamics within the realm of AI,\nproposing a taxonomy to map and examine the key factors that influence whether\nAI systems predominantly pose threats or offer protective benefits to society.\nBy establishing a shared terminology and conceptual foundation for analyzing\nthese interactions, this work seeks to facilitate further research and\ndiscourse in this critical area.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.04029v1",
    "published_date": "2024-12-05 10:05:53 UTC",
    "updated_date": "2024-12-05 10:05:53 UTC"
  },
  {
    "arxiv_id": "2412.04008v1",
    "title": "Deep-Unrolling Multidimensional Harmonic Retrieval Algorithms on Neuromorphic Hardware",
    "authors": [
      "Vlad C. Andrei",
      "Alexandru P. Drăguţoiu",
      "Gabriel Béna",
      "Mahmoud Akl",
      "Yin Li",
      "Matthias Lohrmann",
      "Ullrich J. Mönich",
      "Holger Boche"
    ],
    "abstract": "This paper explores the potential of conversion-based neuromorphic algorithms\nfor highly accurate and energy-efficient single-snapshot multidimensional\nharmonic retrieval (MHR). By casting the MHR problem as a sparse recovery\nproblem, we devise the currently proposed, deep-unrolling-based Structured\nLearned Iterative Shrinkage and Thresholding (S-LISTA) algorithm to solve it\nefficiently using complex-valued convolutional neural networks with\ncomplex-valued activations, which are trained using a supervised regression\nobjective. Afterward, a novel method for converting the complex-valued\nconvolutional layers and activations into spiking neural networks (SNNs) is\ndeveloped. At the heart of this method lies the recently proposed Few Spikes\n(FS) conversion, which is extended by modifying the neuron model's parameters\nand internal dynamics to account for the inherent coupling between real and\nimaginary parts in complex-valued computations. Finally, the converted SNNs are\nmapped onto the SpiNNaker2 neuromorphic board, and a comparison in terms of\nestimation accuracy and power efficiency between the original CNNs deployed on\nan NVIDIA Jetson Xavier and the SNNs is being conducted. The measurement\nresults show that the converted SNNs achieve almost five-fold power efficiency\nat moderate performance loss compared to the original CNNs.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.AR",
      "cs.NE"
    ],
    "primary_category": "eess.SP",
    "comment": "accepted to the 58th Asilomar Conference on Signals, Systems, and\n  Computers, Oct. 27th - Oct. 30th, 2024, Pacific Grove, CA",
    "pdf_url": "http://arxiv.org/pdf/2412.04008v1",
    "published_date": "2024-12-05 09:41:33 UTC",
    "updated_date": "2024-12-05 09:41:33 UTC"
  },
  {
    "arxiv_id": "2412.03993v1",
    "title": "LaserGuider: A Laser Based Physical Backdoor Attack against Deep Neural Networks",
    "authors": [
      "Yongjie Xu",
      "Guangke Chen",
      "Fu Song",
      "Yuqi Chen"
    ],
    "abstract": "Backdoor attacks embed hidden associations between triggers and targets in\ndeep neural networks (DNNs), causing them to predict the target when a trigger\nis present while maintaining normal behavior otherwise. Physical backdoor\nattacks, which use physical objects as triggers, are feasible but lack remote\ncontrol, temporal stealthiness, flexibility, and mobility. To overcome these\nlimitations, in this work, we propose a new type of backdoor triggers utilizing\nlasers that feature long-distance transmission and instant-imaging properties.\nBased on the laser-based backdoor triggers, we present a physical backdoor\nattack, called LaserGuider, which possesses remote control ability and achieves\nhigh temporal stealthiness, flexibility, and mobility. We also introduce a\nsystematic approach to optimize laser parameters for improving attack\neffectiveness. Our evaluation on traffic sign recognition DNNs, critical in\nautonomous vehicles, demonstrates that LaserGuider with three different\nlaser-based triggers achieves over 90% attack success rate with negligible\nimpact on normal inputs. Additionally, we release LaserMark, the first dataset\nof real world traffic signs stamped with physical laser spots, to support\nfurther research in backdoor attacks and defenses.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CR",
    "comment": "In Proceedings of the 23rd International Conference on Applied\n  Cryptography and Network Security (ACNS), Munich, Germany, 23-26 June, 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.03993v1",
    "published_date": "2024-12-05 09:14:50 UTC",
    "updated_date": "2024-12-05 09:14:50 UTC"
  },
  {
    "arxiv_id": "2412.03987v1",
    "title": "MTMT: Consolidating Multiple Thinking Modes to Form a Thought Tree for Strengthening LLM",
    "authors": [
      "Changcheng Li",
      "Xiangyu Wang",
      "Qiuju Chen",
      "Xiren Zhou",
      "Huanhuan Chen"
    ],
    "abstract": "Large language models (LLMs) have shown limitations in tasks requiring\ncomplex logical reasoning and multi-step problem-solving. To address these\nchallenges, researchers have employed carefully designed prompts and\nflowcharts, simulating human cognitive processes to enhance LLM performance,\nsuch as the Chain of Thought approach. In this paper, we introduce MTMT\n(Multi-thinking Modes Tree), a novel method that interacts with LLMs to\nconstruct a thought tree, simulating various advanced cognitive processes,\nincluding but not limited to association, counterfactual thinking, task\ndecomposition, and comparison. By breaking down the original complex task into\nsimpler sub-questions, MTMT facilitates easier problem-solving for LLMs,\nenabling more effective utilization of the latent knowledge within LLMs. We\nevaluate the performance of MTMT under different parameter configurations,\nusing GPT-4o mini as the base model. Our results demonstrate that integrating\nmultiple modes of thinking significantly enhances the ability of LLMs to handle\ncomplex tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03987v1",
    "published_date": "2024-12-05 09:05:30 UTC",
    "updated_date": "2024-12-05 09:05:30 UTC"
  },
  {
    "arxiv_id": "2412.03982v1",
    "title": "Exploring Fully Convolutional Networks for the Segmentation of Hyperspectral Imaging Applied to Advanced Driver Assistance Systems",
    "authors": [
      "Jon Gutiérrez-Zaballa",
      "Koldo Basterretxea",
      "Javier Echanobe",
      "M. Victoria Martínez",
      "Inés del Campo"
    ],
    "abstract": "Advanced Driver Assistance Systems (ADAS) are designed with the main purpose\nof increasing the safety and comfort of vehicle occupants. Most of current\ncomputer vision-based ADAS perform detection and tracking tasks quite\nsuccessfully under regular conditions, but are not completely reliable,\nparticularly under adverse weather and changing lighting conditions, neither in\ncomplex situations with many overlapping objects. In this work we explore the\nuse of hyperspectral imaging (HSI) in ADAS on the assumption that the distinct\nnear infrared (NIR) spectral reflectances of different materials can help to\nbetter separate the objects in a driving scene. In particular, this paper\ndescribes some experimental results of the application of fully convolutional\nnetworks (FCN) to the image segmentation of HSI for ADAS applications. More\nspecifically, our aim is to investigate to what extent the spatial features\ncodified by convolutional filters can be helpful to improve the performance of\nHSI segmentation systems. With that aim, we use the HSI-Drive v1.1 dataset,\nwhich provides a set of labelled images recorded in real driving conditions\nwith a small-size snapshot NIR-HSI camera. Finally, we analyze the\nimplementability of such a HSI segmentation system by prototyping the developed\nFCN model together with the necessary hyperspectral cube preprocessing stage\nand characterizing its performance on an MPSoC.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "arXiv admin note: text overlap with arXiv:2411.19274",
    "pdf_url": "http://arxiv.org/pdf/2412.03982v1",
    "published_date": "2024-12-05 08:58:25 UTC",
    "updated_date": "2024-12-05 08:58:25 UTC"
  },
  {
    "arxiv_id": "2412.03970v1",
    "title": "A Data-Driven Framework for Discovering Fractional Differential Equations in Complex Systems",
    "authors": [
      "Xiangnan Yu",
      "Hao Xu",
      "Zhiping Mao",
      "HongGuang Sun",
      "Yong Zhang",
      "Dongxiao Zhang",
      "Yuntian Chen"
    ],
    "abstract": "In complex physical systems, conventional differential equations often fall\nshort in capturing non-local and memory effects, as they are limited to local\ndynamics and integer-order interactions. This study introduces a stepwise\ndata-driven framework for discovering fractional differential equations (FDEs)\ndirectly from data. FDEs, known for their capacity to model non-local dynamics\nwith fewer parameters than integer-order derivatives, can represent complex\nsystems with long-range interactions. Our framework applies deep neural\nnetworks as surrogate models for denoising and reconstructing sparse and noisy\nobservations while using Gaussian-Jacobi quadrature to handle the challenges\nposed by singularities in fractional derivatives. To optimize both the sparse\ncoefficients and fractional order, we employ an alternating optimization\napproach that combines sparse regression with global optimization techniques.\nWe validate the framework across various datasets, including synthetic\nanomalous diffusion data, experimental data on the creep behavior of frozen\nsoils, and single-particle trajectories modeled by L\\'{e}vy motion. Results\ndemonstrate the framework's robustness in identifying the structure of FDEs\nacross diverse noise levels and its capacity to capture integer-order dynamics,\noffering a flexible approach for modeling memory effects in complex systems.",
    "categories": [
      "physics.comp-ph",
      "cs.AI"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03970v1",
    "published_date": "2024-12-05 08:38:30 UTC",
    "updated_date": "2024-12-05 08:38:30 UTC"
  },
  {
    "arxiv_id": "2412.03966v1",
    "title": "Demonstration Selection for In-Context Learning via Reinforcement Learning",
    "authors": [
      "Xubin Wang",
      "Jianfei Wu",
      "Yichen Yuan",
      "Mingzhe Li",
      "Deyu Cai",
      "Weijia Jia"
    ],
    "abstract": "Diversity in demonstration selection is crucial for enhancing model\ngeneralization, as it enables a broader coverage of structures and concepts.\nHowever, constructing an appropriate set of demonstrations has remained a focal\npoint of research. This paper presents the Relevance-Diversity Enhanced\nSelection (RDES), an innovative approach that leverages reinforcement learning\nto optimize the selection of diverse reference demonstrations for text\nclassification tasks using Large Language Models (LLMs), especially in few-shot\nprompting scenarios. RDES employs a Q-learning framework to dynamically\nidentify demonstrations that maximize both diversity and relevance to the\nclassification objective by calculating a diversity score based on label\ndistribution among selected demonstrations. This method ensures a balanced\nrepresentation of reference data, leading to improved classification accuracy.\nThrough extensive experiments on four benchmark datasets and involving 12\nclosed-source and open-source LLMs, we demonstrate that RDES significantly\nenhances classification accuracy compared to ten established baselines.\nFurthermore, we investigate the incorporation of Chain-of-Thought (CoT)\nreasoning in the reasoning process, which further enhances the model's\npredictive performance. The results underscore the potential of reinforcement\nlearning to facilitate adaptive demonstration selection and deepen the\nunderstanding of classification challenges.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03966v1",
    "published_date": "2024-12-05 08:33:52 UTC",
    "updated_date": "2024-12-05 08:33:52 UTC"
  },
  {
    "arxiv_id": "2412.03963v1",
    "title": "Augmenting Minds or Automating Skills: The Differential Role of Human Capital in Generative AI's Impact on Creative Tasks",
    "authors": [
      "Meiling Huang",
      "Ming Jin",
      "Ning Li"
    ],
    "abstract": "Generative AI is rapidly reshaping creative work, raising critical questions\nabout its beneficiaries and societal implications. This study challenges\nprevailing assumptions by exploring how generative AI interacts with diverse\nforms of human capital in creative tasks. Through two random controlled\nexperiments in flash fiction writing and song composition, we uncover a\nparadox: while AI democratizes access to creative tools, it simultaneously\namplifies cognitive inequalities. Our findings reveal that AI enhances general\nhuman capital (cognitive abilities and education) by facilitating adaptability\nand idea integration but diminishes the value of domain-specific expertise. We\nintroduce a novel theoretical framework that merges human capital theory with\nthe automation-augmentation perspective, offering a nuanced understanding of\nhuman-AI collaboration. This framework elucidates how AI shifts the locus of\ncreative advantage from specialized expertise to broader cognitive\nadaptability. Contrary to the notion of AI as a universal equalizer, our work\nhighlights its potential to exacerbate disparities in skill valuation,\nreshaping workplace hierarchies and redefining the nature of creativity in the\nAI era. These insights advance theories of human capital and automation while\nproviding actionable guidance for organizations navigating AI integration\namidst workforce inequalities.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "econ.GN",
      "q-fin.EC"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03963v1",
    "published_date": "2024-12-05 08:27:14 UTC",
    "updated_date": "2024-12-05 08:27:14 UTC"
  },
  {
    "arxiv_id": "2412.03957v1",
    "title": "A Framework For Image Synthesis Using Supervised Contrastive Learning",
    "authors": [
      "Yibin Liu",
      "Jianyu Zhang",
      "Li Zhang",
      "Shijian Li",
      "Gang Pan"
    ],
    "abstract": "Text-to-image (T2I) generation aims at producing realistic images\ncorresponding to text descriptions. Generative Adversarial Network (GAN) has\nproven to be successful in this task. Typical T2I GANs are 2 phase methods that\nfirst pretrain an inter-modal representation from aligned image-text pairs and\nthen use GAN to train image generator on that basis. However, such\nrepresentation ignores the inner-modal semantic correspondence, e.g. the images\nwith same label. The semantic label in priory describes the inherent\ndistribution pattern with underlying cross-image relationships, which is\nsupplement to the text description for understanding the full characteristics\nof image. In this paper, we propose a framework leveraging both inter- and\ninner-modal correspondence by label guided supervised contrastive learning. We\nextend the T2I GANs to two parameter-sharing contrast branches in both\npretraining and generation phases. This integration effectively clusters the\nsemantically similar image-text pair representations, thereby fostering the\ngeneration of higher-quality images. We demonstrate our framework on four novel\nT2I GANs by both single-object dataset CUB and multi-object dataset COCO,\nachieving significant improvements in the Inception Score (IS) and Frechet\nInception Distance (FID) metrics of imagegeneration evaluation. Notably, on\nmore complex multi-object COCO, our framework improves FID by 30.1%, 27.3%,\n16.2% and 17.1% for AttnGAN, DM-GAN, SSA-GAN and GALIP, respectively. We also\nvalidate our superiority by comparing with other label guided T2I GANs. The\nresults affirm the effectiveness and competitiveness of our approach in\nadvancing the state-of-the-art GAN for T2I generation",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03957v1",
    "published_date": "2024-12-05 08:15:37 UTC",
    "updated_date": "2024-12-05 08:15:37 UTC"
  },
  {
    "arxiv_id": "2412.03944v1",
    "title": "Chain-of-Thought in Large Language Models: Decoding, Projection, and Activation",
    "authors": [
      "Hao Yang",
      "Qianghua Zhao",
      "Lei Li"
    ],
    "abstract": "Chain-of-Thought prompting has significantly enhanced the reasoning\ncapabilities of large language models, with numerous studies exploring factors\ninfluencing its performance. However, the underlying mechanisms remain poorly\nunderstood. To further demystify the operational principles, this work examines\nthree key aspects: decoding, projection, and activation, aiming to elucidate\nthe changes that occur within models when employing Chainof-Thought. Our\nfindings reveal that LLMs effectively imitate exemplar formats while\nintegrating them with their understanding of the question, exhibiting\nfluctuations in token logits during generation but ultimately producing a more\nconcentrated logits distribution, and activating a broader set of neurons in\nthe final layers, indicating more extensive knowledge retrieval compared to\nstandard prompts. Our code and data will be publicly avialable when the paper\nis accepted.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03944v1",
    "published_date": "2024-12-05 07:47:29 UTC",
    "updated_date": "2024-12-05 07:47:29 UTC"
  },
  {
    "arxiv_id": "2412.05331v3",
    "title": "Deep Learning and Hybrid Approaches for Dynamic Scene Analysis, Object Detection and Motion Tracking",
    "authors": [
      "Shahran Rahman Alve"
    ],
    "abstract": "This project aims to develop a robust video surveillance system, which can\nsegment videos into smaller clips based on the detection of activities. It uses\nCCTV footage, for example, to record only major events-like the appearance of a\nperson or a thief-so that storage is optimized and digital searches are easier.\nIt utilizes the latest techniques in object detection and tracking, including\nConvolutional Neural Networks (CNNs) like YOLO, SSD, and Faster R-CNN, as well\nas Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks\n(LSTMs), to achieve high accuracy in detection and capture temporal\ndependencies. The approach incorporates adaptive background modeling through\nGaussian Mixture Models (GMM) and optical flow methods like Lucas-Kanade to\ndetect motions. Multi-scale and contextual analysis are used to improve\ndetection across different object sizes and environments. A hybrid motion\nsegmentation strategy combines statistical and deep learning models to manage\ncomplex movements, while optimizations for real-time processing ensure\nefficient computation. Tracking methods, such as Kalman Filters and Siamese\nnetworks, are employed to maintain smooth tracking even in cases of occlusion.\nDetection is improved on various-sized objects for multiple scenarios by\nmulti-scale and contextual analysis. Results demonstrate high precision and\nrecall in detecting and tracking objects, with significant improvements in\nprocessing times and accuracy due to real-time optimizations and\nillumination-invariant features. The impact of this research lies in its\npotential to transform video surveillance, reducing storage requirements and\nenhancing security through reliable and efficient object detection and\ntracking.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 Pages, 7 Figures",
    "pdf_url": "http://arxiv.org/pdf/2412.05331v3",
    "published_date": "2024-12-05 07:44:40 UTC",
    "updated_date": "2025-02-17 05:54:20 UTC"
  },
  {
    "arxiv_id": "2412.03941v1",
    "title": "Enhancing and Accelerating Diffusion-Based Inverse Problem Solving through Measurements Optimization",
    "authors": [
      "Tianyu Chen",
      "Zhendong Wang",
      "Mingyuan Zhou"
    ],
    "abstract": "Diffusion models have recently demonstrated notable success in solving\ninverse problems. However, current diffusion model-based solutions typically\nrequire a large number of function evaluations (NFEs) to generate high-quality\nimages conditioned on measurements, as they incorporate only limited\ninformation at each step. To accelerate the diffusion-based inverse\nproblem-solving process, we introduce \\textbf{M}easurements\n\\textbf{O}ptimization (MO), a more efficient plug-and-play module for\nintegrating measurement information at each step of the inverse problem-solving\nprocess. This method is comprehensively evaluated across eight diverse linear\nand nonlinear tasks on the FFHQ and ImageNet datasets. By using MO, we\nestablish state-of-the-art (SOTA) performance across multiple tasks, with key\nadvantages: (1) it operates with no more than 100 NFEs, with phase retrieval on\nImageNet being the sole exception; (2) it achieves SOTA or near-SOTA results\neven at low NFE counts; and (3) it can be seamlessly integrated into existing\ndiffusion model-based solutions for inverse problems, such as DPS\n\\cite{chung2022diffusion} and Red-diff \\cite{mardani2023variational}. For\nexample, DPS-MO attains a peak signal-to-noise ratio (PSNR) of 28.71 dB on the\nFFHQ 256 dataset for high dynamic range imaging, setting a new SOTA benchmark\nwith only 100 NFEs, whereas current methods require between 1000 and 4000 NFEs\nfor comparable performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03941v1",
    "published_date": "2024-12-05 07:44:18 UTC",
    "updated_date": "2024-12-05 07:44:18 UTC"
  },
  {
    "arxiv_id": "2412.03934v1",
    "title": "InfiniCube: Unbounded and Controllable Dynamic 3D Driving Scene Generation with World-Guided Video Models",
    "authors": [
      "Yifan Lu",
      "Xuanchi Ren",
      "Jiawei Yang",
      "Tianchang Shen",
      "Zhangjie Wu",
      "Jun Gao",
      "Yue Wang",
      "Siheng Chen",
      "Mike Chen",
      "Sanja Fidler",
      "Jiahui Huang"
    ],
    "abstract": "We present InfiniCube, a scalable method for generating unbounded dynamic 3D\ndriving scenes with high fidelity and controllability. Previous methods for\nscene generation either suffer from limited scales or lack geometric and\nappearance consistency along generated sequences. In contrast, we leverage the\nrecent advancements in scalable 3D representation and video models to achieve\nlarge dynamic scene generation that allows flexible controls through HD maps,\nvehicle bounding boxes, and text descriptions. First, we construct a\nmap-conditioned sparse-voxel-based 3D generative model to unleash its power for\nunbounded voxel world generation. Then, we re-purpose a video model and ground\nit on the voxel world through a set of carefully designed pixel-aligned\nguidance buffers, synthesizing a consistent appearance. Finally, we propose a\nfast feed-forward approach that employs both voxel and pixel branches to lift\nthe dynamic videos to dynamic 3D Gaussians with controllable objects. Our\nmethod can generate controllable and realistic 3D driving scenes, and extensive\nexperiments validate the effectiveness and superiority of our model.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://research.nvidia.com/labs/toronto-ai/infinicube/",
    "pdf_url": "http://arxiv.org/pdf/2412.03934v1",
    "published_date": "2024-12-05 07:32:20 UTC",
    "updated_date": "2024-12-05 07:32:20 UTC"
  },
  {
    "arxiv_id": "2412.03933v1",
    "title": "Exploring AI Text Generation, Retrieval-Augmented Generation, and Detection Technologies: a Comprehensive Overview",
    "authors": [
      "Fnu Neha",
      "Deepshikha Bhati",
      "Deepak Kumar Shukla",
      "Angela Guercio",
      "Ben Ward"
    ],
    "abstract": "The rapid development of Artificial Intelligence (AI) has led to the creation\nof powerful text generation models, such as large language models (LLMs), which\nare widely used for diverse applications. However, concerns surrounding\nAI-generated content, including issues of originality, bias, misinformation,\nand accountability, have become increasingly prominent. This paper offers a\ncomprehensive overview of AI text generators (AITGs), focusing on their\nevolution, capabilities, and ethical implications. This paper also introduces\nRetrieval-Augmented Generation (RAG), a recent approach that improves the\ncontextual relevance and accuracy of text generation by integrating dynamic\ninformation retrieval. RAG addresses key limitations of traditional models,\nincluding their reliance on static knowledge and potential inaccuracies in\nhandling real-world data. Additionally, the paper reviews detection tools that\nhelp differentiate AI-generated text from human-written content and discusses\nthe ethical challenges these technologies pose. The paper explores future\ndirections for improving detection accuracy, supporting ethical AI development,\nand increasing accessibility. The paper contributes to a more responsible and\nreliable use of AI in content creation through these discussions.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03933v1",
    "published_date": "2024-12-05 07:23:14 UTC",
    "updated_date": "2024-12-05 07:23:14 UTC"
  },
  {
    "arxiv_id": "2412.03930v1",
    "title": "MIND: Effective Incorrect Assignment Detection through a Multi-Modal Structure-Enhanced Language Model",
    "authors": [
      "Yunhe Pang",
      "Bo Chen",
      "Fanjin Zhang",
      "Yanghui Rao",
      "Jie Tang"
    ],
    "abstract": "The rapid growth of academic publications has exacerbated the issue of author\nname ambiguity in online digital libraries. Despite advances in name\ndisambiguation algorithms, cumulative errors continue to undermine the\nreliability of academic systems. It is estimated that over 10% paper-author\nassignments are rectified when constructing the million-scale WhoIsWho\nbenchmark. Existing endeavors to detect incorrect assignments are either\nsemantic-based or graph-based approaches, which fall short of making full use\nof the rich text attributes of papers and implicit structural features defined\nvia the co-occurrence of paper attributes. To this end, this paper introduces a\nstructure-enhanced language model that combines key structural features from\ngraph-based methods with fine-grained semantic features from rich paper\nattributes to detect incorrect assignments. The proposed model is trained with\na highly effective multi-modal multi-turn instruction tuning framework, which\nincorporates task-guided instruction tuning, text-attribute modality, and\nstructural modality. Experimental results demonstrate that our model\noutperforms previous approaches, achieving top performance on the leaderboard\nof KDD Cup 2024. Our code has been publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03930v1",
    "published_date": "2024-12-05 07:12:53 UTC",
    "updated_date": "2024-12-05 07:12:53 UTC"
  },
  {
    "arxiv_id": "2412.03928v2",
    "title": "MT3DNet: Multi-Task learning Network for 3D Surgical Scene Reconstruction",
    "authors": [
      "Mithun Parab",
      "Pranay Lendave",
      "Jiyoung Kim",
      "Thi Quynh Dan Nguyen",
      "Palash Ingle"
    ],
    "abstract": "In image-assisted minimally invasive surgeries (MIS), understanding surgical\nscenes is vital for real-time feedback to surgeons, skill evaluation, and\nimproving outcomes through collaborative human-robot procedures. Within this\ncontext, the challenge lies in accurately detecting, segmenting, and estimating\nthe depth of surgical scenes depicted in high-resolution images, while\nsimultaneously reconstructing the scene in 3D and providing segmentation of\nsurgical instruments along with detection labels for each instrument. To\naddress this challenge, a novel Multi-Task Learning (MTL) network is proposed\nfor performing these tasks concurrently. A key aspect of this approach involves\novercoming the optimization hurdles associated with handling multiple tasks\nconcurrently by integrating a Adversarial Weight Update into the MTL framework,\nthe proposed MTL model achieves 3D reconstruction through the integration of\nsegmentation, depth estimation, and object detection, thereby enhancing the\nunderstanding of surgical scenes, which marks a significant advancement\ncompared to existing studies that lack 3D capabilities. Comprehensive\nexperiments on the EndoVis2018 benchmark dataset underscore the adeptness of\nthe model in efficiently addressing all three tasks, demonstrating the efficacy\nof the proposed techniques.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "1. Notation Update: Added * for equal contribution, ensuring proper\n  attribution. 2. Subsection Fix: Removed the `subsection` tag for Section 3.1\n  (no 3.2 existed), maintaining content but fixing hierarchy. 3. Text\n  Additions: Added lines in Section 5 and Subsection 4.2 for clarity, with\n  references for better context",
    "pdf_url": "http://arxiv.org/pdf/2412.03928v2",
    "published_date": "2024-12-05 07:07:35 UTC",
    "updated_date": "2024-12-11 21:06:05 UTC"
  },
  {
    "arxiv_id": "2412.03920v1",
    "title": "A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios",
    "authors": [
      "Xiachong Feng",
      "Longxu Dou",
      "Ella Li",
      "Qinghao Wang",
      "Haochuan Wang",
      "Yu Guo",
      "Chang Ma",
      "Lingpeng Kong"
    ],
    "abstract": "Game-theoretic scenarios have become pivotal in evaluating the social\nintelligence of Large Language Model (LLM)-based social agents. While numerous\nstudies have explored these agents in such settings, there is a lack of a\ncomprehensive survey summarizing the current progress. To address this gap, we\nsystematically review existing research on LLM-based social agents within\ngame-theoretic scenarios. Our survey organizes the findings into three core\ncomponents: Game Framework, Social Agent, and Evaluation Protocol. The game\nframework encompasses diverse game scenarios, ranging from choice-focusing to\ncommunication-focusing games. The social agent part explores agents'\npreferences, beliefs, and reasoning abilities. The evaluation protocol covers\nboth game-agnostic and game-specific metrics for assessing agent performance.\nBy reflecting on the current research and identifying future research\ndirections, this survey provides insights to advance the development and\nevaluation of social agents in game-theoretic scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03920v1",
    "published_date": "2024-12-05 06:46:46 UTC",
    "updated_date": "2024-12-05 06:46:46 UTC"
  },
  {
    "arxiv_id": "2412.03905v2",
    "title": "Integrating Various Software Artifacts for Better LLM-based Bug Localization and Program Repair",
    "authors": [
      "Qiong Feng",
      "Xiaotian Ma",
      "Jiayi Sheng",
      "Ziyuan Feng",
      "Wei Song",
      "Peng Liang"
    ],
    "abstract": "LLMs have garnered considerable attention for their potential to streamline\nAutomated Program Repair (APR). LLM-based approaches can either insert the\ncorrect code or directly generate patches when provided with buggy methods.\nHowever, most of LLM-based APR methods rely on a single type of software\ninformation, without fully leveraging different software artifacts. Despite\nthis, many LLM-based approaches do not explore which specific types of\ninformation best assist in APR. Addressing this gap is crucial for advancing\nLLM-based APR techniques. We propose DEVLoRe to use issue content (description\nand message) and stack error traces to localize buggy methods, then rely on\ndebug information in buggy methods and issue content and stack error to\nlocalize buggy lines and generate plausible patches which can pass all unit\ntests. The results show that while issue content is particularly effective in\nassisting LLMs with fault localization and program repair, different types of\nsoftware artifacts complement each other. By incorporating different artifacts,\nDEVLoRe successfully locates 49.3% and 47.6% of single and non-single buggy\nmethods and generates 56.0% and 14.5% plausible patches for the Defects4J v2.0\ndataset, respectively. This outperforms current state-of-the-art APR methods.\nThe source code and experimental results of this work for replication are\navailable at https://github.com/XYZboom/DEVLoRe.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "22 pages, 11 images, 9 tables, Manuscript submitted to a journal\n  (2024)",
    "pdf_url": "http://arxiv.org/pdf/2412.03905v2",
    "published_date": "2024-12-05 06:21:31 UTC",
    "updated_date": "2025-03-04 07:06:35 UTC"
  },
  {
    "arxiv_id": "2412.03904v1",
    "title": "MISR: Measuring Instrumental Self-Reasoning in Frontier Models",
    "authors": [
      "Kai Fronsdal",
      "David Lindner"
    ],
    "abstract": "We propose a suite of tasks to evaluate the instrumental self-reasoning\nability of large language model (LLM) agents. Instrumental self-reasoning\nability could improve adaptability and enable self-modification, but it could\nalso pose significant risks, such as enabling deceptive alignment. Prior work\nhas only evaluated self-reasoning in non-agentic settings or in limited\ndomains. In this paper, we propose evaluations for instrumental self-reasoning\nability in agentic tasks in a wide range of scenarios, including\nself-modification, knowledge seeking, and opaque self-reasoning. We evaluate\nagents built using state-of-the-art LLMs, including commercial and open source\nsystems. We find that instrumental self-reasoning ability emerges only in the\nmost capable frontier models and that it is highly context-dependent. No model\npasses the the most difficult versions of our evaluations, hence our evaluation\ncan be used to measure increases in instrumental self-reasoning ability in\nfuture models. We open-source our evaluations at\nhttps://github.com/kaifronsdal/Self-Reasoning-Evals.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "I.2.7"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 65 page appendix, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2412.03904v1",
    "published_date": "2024-12-05 06:20:47 UTC",
    "updated_date": "2024-12-05 06:20:47 UTC"
  },
  {
    "arxiv_id": "2412.03903v1",
    "title": "Using SlowFast Networks for Near-Miss Incident Analysis in Dashcam Videos",
    "authors": [
      "Yucheng Zhang",
      "Koichi Emura",
      "Eiji Watanabe"
    ],
    "abstract": "This paper classifies near-miss traffic videos using the SlowFast deep neural\nnetwork that mimics the characteristics of the slow and fast visual information\nprocessed by two different streams from the M (Magnocellular) and P\n(Parvocellular) cells of the human brain. The approach significantly improves\nthe accuracy of the traffic near-miss video analysis and presents insights into\nhuman visual perception in traffic scenarios. Moreover, it contributes to\ntraffic safety enhancements and provides novel perspectives on the potential\ncognitive errors in traffic accidents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Best Research Paper Award for Asia-Pacific Region, The 30th ITS World\n  Congress 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.03903v1",
    "published_date": "2024-12-05 06:20:19 UTC",
    "updated_date": "2024-12-05 06:20:19 UTC"
  },
  {
    "arxiv_id": "2412.03895v1",
    "title": "A Noise is Worth Diffusion Guidance",
    "authors": [
      "Donghoon Ahn",
      "Jiwon Kang",
      "Sanghyun Lee",
      "Jaewon Min",
      "Minjae Kim",
      "Wooseok Jang",
      "Hyoungwon Cho",
      "Sayak Paul",
      "SeonHwa Kim",
      "Eunju Cha",
      "Kyong Hwan Jin",
      "Seungryong Kim"
    ],
    "abstract": "Diffusion models excel in generating high-quality images. However, current\ndiffusion models struggle to produce reliable images without guidance methods,\nsuch as classifier-free guidance (CFG). Are guidance methods truly necessary?\nObserving that noise obtained via diffusion inversion can reconstruct\nhigh-quality images without guidance, we focus on the initial noise of the\ndenoising pipeline. By mapping Gaussian noise to `guidance-free noise', we\nuncover that small low-magnitude low-frequency components significantly enhance\nthe denoising process, removing the need for guidance and thus improving both\ninference throughput and memory. Expanding on this, we propose \\ours, a novel\nmethod that replaces guidance methods with a single refinement of the initial\nnoise. This refined noise enables high-quality image generation without\nguidance, within the same diffusion pipeline. Our noise-refining model\nleverages efficient noise-space learning, achieving rapid convergence and\nstrong performance with just 50K text-image pairs. We validate its\neffectiveness across diverse metrics and analyze how refined noise can\neliminate the need for guidance. See our project page:\nhttps://cvlab-kaist.github.io/NoiseRefine/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://cvlab-kaist.github.io/NoiseRefine/",
    "pdf_url": "http://arxiv.org/pdf/2412.03895v1",
    "published_date": "2024-12-05 06:09:56 UTC",
    "updated_date": "2024-12-05 06:09:56 UTC"
  },
  {
    "arxiv_id": "2412.03894v1",
    "title": "Machine Learning-based Android Intrusion Detection System",
    "authors": [
      "Madiha Tahreem",
      "Ifrah Andleeb",
      "Bilal Zahid Hussain",
      "Arsalan Hameed"
    ],
    "abstract": "The android operating system is being installed in most of the smart devices.\nThe introduction of intrusions in such operating systems is rising at a\ntremendous rate. With the introduction of such malicious data streams, the\nsmart devices are being subjected to various attacks like Phishing, Spyware,\nSMS Fraud, Bots and Banking-Trojans and many such. The application of machine\nlearning classification algorithms for the security of android APK files is\nused in this paper. Each apk data stream was marked to be either malicious or\nnon malicious on the basis of different parameters. The machine learning\nclassification techniques are then used to classify whether the newly installed\napplications' signature falls within the malicious or non-malicious domain. If\nit falls within the malicious category, appropriate action can be taken, and\nthe Android operating system can be shielded against illegal activities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03894v1",
    "published_date": "2024-12-05 06:05:12 UTC",
    "updated_date": "2024-12-05 06:05:12 UTC"
  },
  {
    "arxiv_id": "2412.03893v1",
    "title": "Dual-Branch Subpixel-Guided Network for Hyperspectral Image Classification",
    "authors": [
      "Zhu Han",
      "Jin Yang",
      "Lianru Gao",
      "Zhiqiang Zeng",
      "Bing Zhang",
      "Jocelyn Chanussot"
    ],
    "abstract": "Deep learning (DL) has been widely applied into hyperspectral image (HSI)\nclassification owing to its promising feature learning and representation\ncapabilities. However, limited by the spatial resolution of sensors, existing\nDL-based classification approaches mainly focus on pixel-level spectral and\nspatial information extraction through complex network architecture design,\nwhile ignoring the existence of mixed pixels in actual scenarios. To tackle\nthis difficulty, we propose a novel dual-branch subpixel-guided network for HSI\nclassification, called DSNet, which automatically integrates subpixel\ninformation and convolutional class features by introducing a deep autoencoder\nunmixing architecture to enhance classification performance. DSNet is capable\nof fully considering physically nonlinear properties within subpixels and\nadaptively generating diagnostic abundances in an unsupervised manner to\nachieve more reliable decision boundaries for class label distributions. The\nsubpixel fusion module is designed to ensure high-quality information fusion\nacross pixel and subpixel features, further promoting stable joint\nclassification. Experimental results on three benchmark datasets demonstrate\nthe effectiveness and superiority of DSNet compared with state-of-the-art\nDL-based HSI classification approaches. The codes will be available at\nhttps://github.com/hanzhu97702/DSNet, contributing to the remote sensing\ncommunity.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03893v1",
    "published_date": "2024-12-05 06:03:09 UTC",
    "updated_date": "2024-12-05 06:03:09 UTC"
  },
  {
    "arxiv_id": "2412.03886v1",
    "title": "Uniform Discretized Integrated Gradients: An effective attribution based method for explaining large language models",
    "authors": [
      "Swarnava Sinha Roy",
      "Ayan Kundu"
    ],
    "abstract": "Integrated Gradients is a well-known technique for explaining deep learning\nmodels. It calculates feature importance scores by employing a gradient based\napproach computing gradients of the model output with respect to input features\nand accumulating them along a linear path. While this works well for continuous\nfeatures spaces, it may not be the most optimal way to deal with discrete\nspaces like word embeddings. For interpreting LLMs (Large Language Models),\nthere exists a need for a non-linear path where intermediate points, whose\ngradients are to be computed, lie close to actual words in the embedding space.\nIn this paper, we propose a method called Uniform Discretized Integrated\nGradients (UDIG) based on a new interpolation strategy where we choose a\nfavorable nonlinear path for computing attribution scores suitable for\npredictive language models. We evaluate our method on two types of NLP tasks-\nSentiment Classification and Question Answering against three metrics viz Log\nodds, Comprehensiveness and Sufficiency. For sentiment classification, we have\nused the SST2, IMDb and Rotten Tomatoes datasets for benchmarking and for\nQuestion Answering, we have used the fine-tuned BERT model on SQuAD dataset.\nOur approach outperforms the existing methods in almost all the metrics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03886v1",
    "published_date": "2024-12-05 05:39:03 UTC",
    "updated_date": "2024-12-05 05:39:03 UTC"
  },
  {
    "arxiv_id": "2412.03884v1",
    "title": "A Unified Framework for Evaluating the Effectiveness and Enhancing the Transparency of Explainable AI Methods in Real-World Applications",
    "authors": [
      "Md. Ariful Islam",
      "M. F. Mridha",
      "Md Abrar Jahin",
      "Nilanjan Dey"
    ],
    "abstract": "The rapid advancement of deep learning has resulted in substantial\nadvancements in AI-driven applications; however, the \"black box\" characteristic\nof these models frequently constrains their interpretability, transparency, and\nreliability. Explainable artificial intelligence (XAI) seeks to elucidate AI\ndecision-making processes, guaranteeing that explanations faithfully represent\nthe model's rationale and correspond with human comprehension. Despite\ncomprehensive research in XAI, a significant gap persists in standardized\nprocedures for assessing the efficacy and transparency of XAI techniques across\nmany real-world applications. This study presents a unified XAI evaluation\nframework incorporating extensive quantitative and qualitative criteria to\nsystematically evaluate the correctness, interpretability, robustness,\nfairness, and completeness of explanations generated by AI models. The\nframework prioritizes user-centric and domain-specific adaptations, hence\nimproving the usability and reliability of AI models in essential domains. To\naddress deficiencies in existing evaluation processes, we suggest defined\nbenchmarks and a systematic evaluation pipeline that includes data loading,\nexplanation development, and thorough method assessment. The suggested\nframework's relevance and variety are evidenced by case studies in healthcare,\nfinance, agriculture, and autonomous systems. These provide a solid basis for\nthe equitable and dependable assessment of XAI methodologies. This paradigm\nenhances XAI research by offering a systematic, flexible, and pragmatic method\nto guarantee transparency and accountability in AI systems across many\nreal-world contexts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03884v1",
    "published_date": "2024-12-05 05:30:10 UTC",
    "updated_date": "2024-12-05 05:30:10 UTC"
  },
  {
    "arxiv_id": "2412.03881v2",
    "title": "Weak-to-Strong Generalization Through the Data-Centric Lens",
    "authors": [
      "Changho Shin",
      "John Cooper",
      "Frederic Sala"
    ],
    "abstract": "The weak-to-strong generalization phenomenon is the driver for important\nmachine learning applications including highly data-efficient learning and,\nmost recently, performing superalignment. While decades of research have\nresulted in numerous algorithms that produce strong empirical performance,\nunderstanding what aspects of data enable weak-to-strong generalization has\nbeen understudied. We propose a simple data-centric mechanism that\ncharacterizes weak-to-strong generalization: the overlap density. Intuitively,\ngeneralization tracks the number of points that contain overlaps, i.e., both\neasy patterns (learnable by a weak model) and challenging patterns (only\nlearnable by a stronger model), as with such points, weak predictions can be\nused to learn challenging patterns by stronger models. We provide a practical\noverlap detection algorithm to find such points in datasets and leverage them\nto learn, among multiple sources of data, which to query when seeking to\nmaximize overlap density and thereby enhance weak-to-strong generalization. We\npresent a theoretical result showing that the generalization benefit is a\nfunction of the overlap density and a regret bound for our data selection\nalgorithm. Empirically, we validate the mechanism and the overlap detection\nalgorithm on a wide array of settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2412.03881v2",
    "published_date": "2024-12-05 05:29:19 UTC",
    "updated_date": "2025-03-04 04:28:19 UTC"
  },
  {
    "arxiv_id": "2412.03877v1",
    "title": "AyutthayaAlpha: A Thai-Latin Script Transliteration Transformer",
    "authors": [
      "Davor Lauc",
      "Attapol Rutherford",
      "Weerin Wongwarawipatr"
    ],
    "abstract": "This study introduces AyutthayaAlpha, an advanced transformer-based machine\nlearning model designed for the transliteration of Thai proper names into Latin\nscript. Our system achieves state-of-the-art performance with 82.32%\nfirst-token accuracy and 95.24% first-three-token accuracy, while maintaining a\nlow character error rate of 0.0047. The complexity of Thai phonology, including\ntonal features and vowel length distinctions, presents significant challenges\nfor accurate transliteration, which we address through a novel two-model\napproach: AyutthayaAlpha-Small, based on the ByT5 architecture, and\nAyutthayaAlpha-VerySmall, a computationally efficient variant that unexpectedly\noutperforms its larger counterpart. Our research combines linguistic rules with\ndeep learning, training on a carefully curated dataset of 1.2 million\nThai-Latin name pairs, augmented through strategic upsampling to 2.7 million\nexamples. Extensive evaluations against existing transliteration methods and\nhuman expert benchmarks demonstrate that AyutthayaAlpha not only achieves\nsuperior accuracy but also effectively captures personal and cultural\npreferences in name romanization. The system's practical applications extend to\ncross-lingual information retrieval, international data standardization, and\nidentity verification systems, with particular relevance for government\ndatabases, academic institutions, and global business operations. This work\nrepresents a significant advance in bridging linguistic gaps between Thai and\nLatin scripts, while respecting the cultural and personal dimensions of name\ntransliteration.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03877v1",
    "published_date": "2024-12-05 05:18:09 UTC",
    "updated_date": "2024-12-05 05:18:09 UTC"
  },
  {
    "arxiv_id": "2412.03873v1",
    "title": "Fine-Grained Sentiment Analysis of Electric Vehicle User Reviews: A Bidirectional LSTM Approach to Capturing Emotional Intensity in Chinese Text",
    "authors": [
      "Shuhao Chen",
      "Chengyi Tu"
    ],
    "abstract": "The rapid expansion of the electric vehicle (EV) industry has highlighted the\nimportance of user feedback in improving product design and charging\ninfrastructure. Traditional sentiment analysis methods often oversimplify the\ncomplexity of user emotions, limiting their effectiveness in capturing nuanced\nsentiments and emotional intensities. This study proposes a Bidirectional Long\nShort-Term Memory (Bi-LSTM) network-based sentiment scoring model to analyze\nuser reviews of EV charging infrastructure. By assigning sentiment scores\nranging from 0 to 5, the model provides a fine-grained understanding of\nemotional expression. Leveraging a dataset of 43,678 reviews from PC Auto, the\nstudy employs rigorous data cleaning and preprocessing, including tokenization\nand stop word removal, to optimize input for deep learning. The Bi-LSTM model\ndemonstrates significant improvements over traditional approaches like SnowNLP\nacross key evaluation metrics, including Mean Squared Error (MSE), Mean\nAbsolute Error (MAE), and Explained Variance Score (EVS). These results\nhighlight the model's superior capability to capture nuanced sentiment\ndynamics, offering valuable insights for targeted product and service\nenhancements in the EV ecosystem.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03873v1",
    "published_date": "2024-12-05 05:04:29 UTC",
    "updated_date": "2024-12-05 05:04:29 UTC"
  },
  {
    "arxiv_id": "2412.03871v2",
    "title": "CLIP-PING: Boosting Lightweight Vision-Language Models with Proximus Intrinsic Neighbors Guidance",
    "authors": [
      "Chu Myaet Thwal",
      "Ye Lin Tun",
      "Minh N. H. Nguyen",
      "Eui-Nam Huh",
      "Choong Seon Hong"
    ],
    "abstract": "Beyond the success of Contrastive Language-Image Pre-training (CLIP), recent\ntrends mark a shift toward exploring the applicability of lightweight\nvision-language models for resource-constrained scenarios. These models often\ndeliver suboptimal performance when relying solely on a single image-text\ncontrastive learning objective, spotlighting the need for more effective\ntraining mechanisms that guarantee robust cross-modal feature alignment. In\nthis work, we propose CLIP-PING: Contrastive Language-Image Pre-training with\nProximus Intrinsic Neighbors Guidance, a novel yet simple and efficient\ntraining paradigm designed to boost the performance of lightweight\nvision-language models with minimal computational overhead and lower data\ndemands. CLIP-PING bootstraps unimodal features extracted from arbitrary\npre-trained encoders to obtain intrinsic guidance of proximus neighbor samples,\ni.e., nearest-neighbor (NN) and cross nearest-neighbor (XNN). We find that\nextra contrastive supervision from these neighbors substantially boosts\ncross-modal alignment, enabling lightweight models to learn more generic\nfeatures with rich semantic diversity. Extensive experiments reveal that\nCLIP-PING notably surpasses its peers in zero-shot generalization and\ncross-modal retrieval tasks. Specifically, a 5.5% gain on zero-shot ImageNet1K\nclassification with 10.7% (I2T) and 5.7% (T2I) on Flickr30K retrieval, compared\nto the original CLIP when using ViT-XS image encoder trained on 3 million\n(image, text) pairs. Moreover, CLIP-PING showcases a strong transferability\nunder the linear evaluation protocol across several downstream tasks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.ET",
      "cs.IR",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages, 5 figures, 24 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.03871v2",
    "published_date": "2024-12-05 04:58:28 UTC",
    "updated_date": "2025-03-19 02:30:05 UTC"
  },
  {
    "arxiv_id": "2412.03864v1",
    "title": "Training MLPs on Graphs without Supervision",
    "authors": [
      "Zehong Wang",
      "Zheyuan Zhang",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "abstract": "Graph Neural Networks (GNNs) have demonstrated their effectiveness in various\ngraph learning tasks, yet their reliance on neighborhood aggregation during\ninference poses challenges for deployment in latency-sensitive applications,\nsuch as real-time financial fraud detection. To address this limitation, recent\nstudies have proposed distilling knowledge from teacher GNNs into student\nMulti-Layer Perceptrons (MLPs) trained on node content, aiming to accelerate\ninference. However, these approaches often inadequately explore structural\ninformation when inferring unseen nodes. To this end, we introduce SimMLP, a\nSelf-supervised framework for learning MLPs on graphs, designed to fully\nintegrate rich structural information into MLPs. Notably, SimMLP is the first\nMLP-learning method that can achieve equivalence to GNNs in the optimal case.\nThe key idea is to employ self-supervised learning to align the representations\nencoded by graph context-aware GNNs and neighborhood dependency-free MLPs,\nthereby fully integrating the structural information into MLPs. We provide a\ncomprehensive theoretical analysis, demonstrating the equivalence between\nSimMLP and GNNs based on mutual information and inductive bias, highlighting\nSimMLP's advanced structural learning capabilities. Additionally, we conduct\nextensive experiments on 20 benchmark datasets, covering node classification,\nlink prediction, and graph classification, to showcase SimMLP's superiority\nover state-of-the-art baselines, particularly in scenarios involving unseen\nnodes (e.g., inductive and cold-start node classification) where structural\ninsights are crucial. Our codes are available at:\nhttps://github.com/Zehong-Wang/SimMLP.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by WSDM 25",
    "pdf_url": "http://arxiv.org/pdf/2412.03864v1",
    "published_date": "2024-12-05 04:20:54 UTC",
    "updated_date": "2024-12-05 04:20:54 UTC"
  },
  {
    "arxiv_id": "2412.15226v1",
    "title": "Learning-by-teaching with ChatGPT: The effect of teachable ChatGPT agent on programming education",
    "authors": [
      "Angxuan Chen",
      "Yuang Wei",
      "Huixiao Le",
      "Yan Zhang"
    ],
    "abstract": "This study investigates the potential of using ChatGPT as a teachable agent\nto support students' learning by teaching process, specifically in programming\neducation. While learning by teaching is an effective pedagogical strategy for\npromoting active learning, traditional teachable agents have limitations,\nparticularly in facilitating natural language dialogue. Our research explored\nwhether ChatGPT, with its ability to engage learners in natural conversations,\ncan support this process. The findings reveal that interacting with ChatGPT\nimproves students' knowledge gains and programming abilities, particularly in\nwriting readable and logically sound code. However, it had limited impact on\ndeveloping learners' error-correction skills, likely because ChatGPT tends to\ngenerate correct code, reducing opportunities for students to practice\ndebugging. Additionally, students' self-regulated learning (SRL) abilities\nimproved, suggesting that teaching ChatGPT fosters learners' higher\nself-efficacy and better implementation of SRL strategies. This study discussed\nthe role of natural dialogue in fostering socialized learning by teaching, and\nexplored ChatGPT's specific contributions in supporting students' SRL through\nthe learning by teaching process. Overall, the study highlights ChatGPT's\npotential as a teachable agent, offering insights for future research on\nChatGPT-supported education.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "stat.AP"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.15226v1",
    "published_date": "2024-12-05 04:12:03 UTC",
    "updated_date": "2024-12-05 04:12:03 UTC"
  },
  {
    "arxiv_id": "2412.03856v1",
    "title": "How Good is ChatGPT in Giving Adaptive Guidance Using Knowledge Graphs in E-Learning Environments?",
    "authors": [
      "Patrick Ocheja",
      "Brendan Flanagan",
      "Yiling Dai",
      "Hiroaki Ogata"
    ],
    "abstract": "E-learning environments are increasingly harnessing large language models\n(LLMs) like GPT-3.5 and GPT-4 for tailored educational support. This study\nintroduces an approach that integrates dynamic knowledge graphs with LLMs to\noffer nuanced student assistance. By evaluating past and ongoing student\ninteractions, the system identifies and appends the most salient learning\ncontext to prompts directed at the LLM. Central to this method is the knowledge\ngraph's role in assessing a student's comprehension of topic prerequisites.\nDepending on the categorized understanding (good, average, or poor), the LLM\nadjusts its guidance, offering advanced assistance, foundational reviews, or\nin-depth prerequisite explanations, respectively. Preliminary findings suggest\nstudents could benefit from this tiered support, achieving enhanced\ncomprehension and improved task outcomes. However, several issues related to\npotential errors arising from LLMs were identified, which can potentially\nmislead students. This highlights the need for human intervention to mitigate\nthese risks. This research aims to advance AI-driven personalized learning\nwhile acknowledging the limitations and potential pitfalls, thus guiding future\nresearch in technology and data-driven education.",
    "categories": [
      "cs.AI",
      "cs.ET",
      "I.2.6; I.2.11"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03856v1",
    "published_date": "2024-12-05 04:05:43 UTC",
    "updated_date": "2024-12-05 04:05:43 UTC"
  },
  {
    "arxiv_id": "2412.03854v1",
    "title": "What Do Machine Learning Researchers Mean by \"Reproducible\"?",
    "authors": [
      "Edward Raff",
      "Michel Benaroch",
      "Sagar Samtani",
      "Andrew L. Farris"
    ],
    "abstract": "The concern that Artificial Intelligence (AI) and Machine Learning (ML) are\nentering a \"reproducibility crisis\" has spurred significant research in the\npast few years. Yet with each paper, it is often unclear what someone means by\n\"reproducibility\". Our work attempts to clarify the scope of \"reproducibility\"\nas displayed by the community at large. In doing so, we propose to refine the\nresearch to eight general topic areas. In this light, we see that each of these\nareas contains many works that do not advertise themselves as being about\n\"reproducibility\", in part because they go back decades before the matter came\nto broader attention.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear in AAAI 2025, Senior Member Presentation Track",
    "pdf_url": "http://arxiv.org/pdf/2412.03854v1",
    "published_date": "2024-12-05 04:04:39 UTC",
    "updated_date": "2024-12-05 04:04:39 UTC"
  },
  {
    "arxiv_id": "2412.03851v1",
    "title": "FedMetaMed: Federated Meta-Learning for Personalized Medication in Distributed Healthcare Systems",
    "authors": [
      "Jiechao Gao",
      "Yuangang Li"
    ],
    "abstract": "Personalized medication aims to tailor healthcare to individual patient\ncharacteristics. However, the heterogeneity of patient data across healthcare\nsystems presents significant challenges to achieving accurate and effective\npersonalized treatments. Ethical concerns further complicate the aggregation of\nlarge volumes of data from diverse institutions. Federated Learning (FL) offers\na promising decentralized solution by enabling collaborative model training\nthrough the exchange of client models rather than raw data, thus preserving\nprivacy. However, existing FL methods often suffer from retrogression during\nserver aggregation, leading to a decline in model performance in real-world\nmedical FL settings. To address data variability in distributed healthcare\nsystems, we introduce Federated Meta-Learning for Personalized Medication\n(FedMetaMed), which combines federated learning and meta-learning to create\nmodels that adapt to diverse patient data across healthcare systems. The\nFedMetaMed framework aims to produce superior personalized models for\nindividual clients by addressing these limitations. Specifically, we introduce\nCumulative Fourier Aggregation (CFA) at the server to improve stability and\neffectiveness in global knowledge aggregation. CFA achieves this by gradually\nintegrating client models from low to high frequencies. At the client level, we\nimplement a Collaborative Transfer Optimization (CTO) strategy with a\nthree-step process - Retrieve, Reciprocate, and Refine - to enhance the\npersonalized local model through seamless global knowledge transfer.\nExperiments on real-world medical imaging datasets demonstrate that FedMetaMed\noutperforms state-of-the-art FL methods, showing superior generalization even\non out-of-distribution cohorts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03851v1",
    "published_date": "2024-12-05 03:36:55 UTC",
    "updated_date": "2024-12-05 03:36:55 UTC"
  },
  {
    "arxiv_id": "2412.03844v4",
    "title": "HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting",
    "authors": [
      "Jingyu Lin",
      "Jiaqi Gu",
      "Lubin Fan",
      "Bojian Wu",
      "Yujing Lou",
      "Renjie Chen",
      "Ligang Liu",
      "Jieping Ye"
    ],
    "abstract": "Generating high-quality novel view renderings of 3D Gaussian Splatting (3DGS)\nin scenes featuring transient objects is challenging. We propose a novel hybrid\nrepresentation, termed as HybridGS, using 2D Gaussians for transient objects\nper image and maintaining traditional 3D Gaussians for the whole static scenes.\nNote that, the 3DGS itself is better suited for modeling static scenes that\nassume multi-view consistency, but the transient objects appear occasionally\nand do not adhere to the assumption, thus we model them as planar objects from\na single view, represented with 2D Gaussians. Our novel representation\ndecomposes the scene from the perspective of fundamental viewpoint consistency,\nmaking it more reasonable. Additionally, we present a novel multi-view\nregulated supervision method for 3DGS that leverages information from\nco-visible regions, further enhancing the distinctions between the transients\nand statics. Then, we propose a straightforward yet effective multi-stage\ntraining strategy to ensure robust training and high-quality view synthesis\nacross various settings. Experiments on benchmark datasets show our\nstate-of-the-art performance of novel view synthesis in both indoor and outdoor\nscenes, even in the presence of distracting elements.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accpeted by CVPR 2025. Project page:\n  https://gujiaqivadin.github.io/hybridgs/ Code:\n  https://github.com/Yeyuqqwx/HybridGS Data:\n  https://huggingface.co/Eto63277/HybridGS/tree/main",
    "pdf_url": "http://arxiv.org/pdf/2412.03844v4",
    "published_date": "2024-12-05 03:20:35 UTC",
    "updated_date": "2025-02-28 09:49:45 UTC"
  },
  {
    "arxiv_id": "2412.03841v1",
    "title": "LL-ICM: Image Compression for Low-level Machine Vision via Large Vision-Language Model",
    "authors": [
      "Yuan Xue",
      "Qi Zhang",
      "Chuanmin Jia",
      "Shiqi Wang"
    ],
    "abstract": "Image Compression for Machines (ICM) aims to compress images for machine\nvision tasks rather than human viewing. Current works predominantly concentrate\non high-level tasks like object detection and semantic segmentation. However,\nthe quality of original images is usually not guaranteed in the real world,\nleading to even worse perceptual quality or downstream task performance after\ncompression. Low-level (LL) machine vision models, like image restoration\nmodels, can help improve such quality, and thereby their compression\nrequirements should also be considered. In this paper, we propose a pioneered\nICM framework for LL machine vision tasks, namely LL-ICM. By jointly optimizing\ncompression and LL tasks, the proposed LL-ICM not only enriches its encoding\nability in generalizing to versatile LL tasks but also optimizes the processing\nability of down-stream LL task models, achieving mutual adaptation for image\ncodecs and LL task models. Furthermore, we integrate large-scale\nvision-language models into the LL-ICM framework to generate more universal and\ndistortion-robust feature embeddings for LL vision tasks. Therefore, one LL-ICM\ncodec can generalize to multiple tasks. We establish a solid benchmark to\nevaluate LL-ICM, which includes extensive objective experiments by using both\nfull and no-reference image quality assessments. Experimental results show that\nLL-ICM can achieve 22.65% BD-rate reductions over the state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03841v1",
    "published_date": "2024-12-05 03:12:45 UTC",
    "updated_date": "2024-12-05 03:12:45 UTC"
  },
  {
    "arxiv_id": "2412.03837v1",
    "title": "Movie Gen: SWOT Analysis of Meta's Generative AI Foundation Model for Transforming Media Generation, Advertising, and Entertainment Industries",
    "authors": [
      "Abul Ehtesham",
      "Saket Kumar",
      "Aditi Singh",
      "Tala Talaei Khoei"
    ],
    "abstract": "Generative AI is reshaping the media landscape, enabling unprecedented\ncapabilities in video creation, personalization, and scalability. This paper\npresents a comprehensive SWOT analysis of Metas Movie Gen, a cutting-edge\ngenerative AI foundation model designed to produce 1080p HD videos with\nsynchronized audio from simple text prompts. We explore its strengths,\nincluding high-resolution video generation, precise editing, and seamless audio\nintegration, which make it a transformative tool across industries such as\nfilmmaking, advertising, and education. However, the analysis also addresses\nlimitations, such as constraints on video length and potential biases in\ngenerated content, which pose challenges for broader adoption. In addition, we\nexamine the evolving regulatory and ethical considerations surrounding\ngenerative AI, focusing on issues like content authenticity, cultural\nrepresentation, and responsible use. Through comparative insights with leading\nmodels like DALL-E and Google Imagen, this paper highlights Movie Gens unique\nfeatures, such as video personalization and multimodal synthesis, while\nidentifying opportunities for innovation and areas requiring further research.\nOur findings provide actionable insights for stakeholders, emphasizing both the\nopportunities and challenges of deploying generative AI in media production.\nThis work aims to guide future advancements in generative AI, ensuring\nscalability, quality, and ethical integrity in this rapidly evolving field.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03837v1",
    "published_date": "2024-12-05 03:01:53 UTC",
    "updated_date": "2024-12-05 03:01:53 UTC"
  },
  {
    "arxiv_id": "2412.03824v1",
    "title": "Towards Data Governance of Frontier AI Models",
    "authors": [
      "Jason Hausenloy",
      "Duncan McClements",
      "Madhavendra Thakur"
    ],
    "abstract": "Data is essential to train and fine-tune today's frontier artificial\nintelligence (AI) models and to develop future ones. To date, academic, legal,\nand regulatory work has primarily addressed how data can directly harm\nconsumers and creators, such as through privacy breaches, copyright\ninfringements, and bias and discrimination. Our work, instead, focuses on the\ncomparatively neglected question of how data can enable new governance\ncapacities for frontier AI models. This approach for \"frontier data governance\"\nopens up new avenues for monitoring and mitigating risks from advanced AI\nmodels, particularly as they scale and acquire specific dangerous capabilities.\nStill, frontier data governance faces challenges that stem from the fundamental\nproperties of data itself: data is non-rival, often non-excludable, easily\nreplicable, and increasingly synthesizable. Despite these inherent\ndifficulties, we propose a set of policy mechanisms targeting key actors along\nthe data supply chain, including data producers, aggregators, model developers,\nand data vendors. We provide a brief overview of 15 governance mechanisms, of\nwhich we centrally introduce five, underexplored policy recommendations. These\ninclude developing canary tokens to detect unauthorized use for producers;\n(automated) data filtering to remove malicious content for pre-training and\npost-training datasets; mandatory dataset reporting requirements for developers\nand vendors; improved security for datasets and data generation algorithms; and\nknow-your-customer requirements for vendors. By considering data not just as a\nsource of potential harm, but as a critical governance lever, this work aims to\nequip policymakers with a new tool for the governance and regulation of\nfrontier AI models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03824v1",
    "published_date": "2024-12-05 02:37:51 UTC",
    "updated_date": "2024-12-05 02:37:51 UTC"
  },
  {
    "arxiv_id": "2412.03822v1",
    "title": "Beyond the Binary: Capturing Diverse Preferences With Reward Regularization",
    "authors": [
      "Vishakh Padmakumar",
      "Chuanyang Jin",
      "Hannah Rose Kirk",
      "He He"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed via public-facing\ninterfaces to interact with millions of users, each with diverse preferences.\nDespite this, preference tuning of LLMs predominantly relies on reward models\ntrained using binary judgments where annotators select the preferred choice out\nof pairs of model outputs. In this work, we argue that this reliance on binary\nchoices does not capture the broader, aggregate preferences of the target user\nin real-world tasks. We propose a taxonomy that identifies two dimensions of\nsubjectivity where different users disagree on the preferred output-namely, the\nPlurality of Responses to Prompts, where prompts allow for multiple correct\nanswers, and the Indistinguishability of Responses, where candidate outputs are\nparaphrases of each other. We show that reward models correlate weakly with\nuser preferences in these cases. As a first step to address this issue, we\nintroduce a simple yet effective method that augments existing binary\npreference datasets with synthetic preference judgments to estimate potential\nuser disagreement. Incorporating these via a margin term as a form of\nregularization during model training yields predictions that better align with\nthe aggregate user preferences.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03822v1",
    "published_date": "2024-12-05 02:35:46 UTC",
    "updated_date": "2024-12-05 02:35:46 UTC"
  },
  {
    "arxiv_id": "2412.03815v1",
    "title": "Synergizing LLMs and Knowledge Graphs: A Novel Approach to Software Repository-Related Question Answering",
    "authors": [
      "Samuel Abedu",
      "SayedHassan Khatoonabadi",
      "Emad Shihab"
    ],
    "abstract": "Software repositories contain valuable information for gaining insights into\ntheir development process. However, extracting insights from these repository\ndata is time-consuming and requires technical expertise. While software\nengineering chatbots have been developed to facilitate natural language\ninteractions with repositories, they struggle with understanding natural\nlanguage and accurately retrieving relevant data. This study aims to improve\nthe accuracy of LLM-based chatbots in answering repository-related questions by\naugmenting them with knowledge graphs. We achieve this in a two-step approach;\n(1) constructing a knowledge graph from the repository data and (2) synergizing\nthe knowledge graph with LLM to allow for the natural language questions and\nanswers. We curated a set of 20 questions with different complexities and\nevaluated our approach on five popular open-source projects. Our approach\nachieved an accuracy of 65%. We further investigated the limitations and\nidentified six key issues, with the majority relating to the reasoning\ncapability of the LLM. We experimented with a few-shot chain-of-thought\nprompting to determine if it could enhance our approach. This technique\nimproved the overall accuracy to 84%. Our findings demonstrate the synergy\nbetween LLMs and knowledge graphs as a viable solution for making repository\ndata accessible to both technical and non-technical stakeholders.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Submitted to ACM Transactions on Software Engineering and Methodology\n  for review",
    "pdf_url": "http://arxiv.org/pdf/2412.03815v1",
    "published_date": "2024-12-05 02:18:03 UTC",
    "updated_date": "2024-12-05 02:18:03 UTC"
  },
  {
    "arxiv_id": "2412.03801v1",
    "title": "Agent AI with LangGraph: A Modular Framework for Enhancing Machine Translation Using Large Language Models",
    "authors": [
      "Jialin Wang",
      "Zhihua Duan"
    ],
    "abstract": "This paper explores the transformative role of Agent AI and LangGraph in\nadvancing the automation and effectiveness of machine translation (MT). Agents\nare modular components designed to perform specific tasks, such as translating\nbetween particular languages, with specializations like TranslateEnAgent,\nTranslateFrenchAgent, and TranslateJpAgent for English, French, and Japanese\ntranslations, respectively. These agents leverage the powerful semantic\ncapabilities of large language models (LLMs), such as GPT-4o, to ensure\naccurate, contextually relevant translations while maintaining modularity,\nscalability, and context retention.\n  LangGraph, a graph-based framework built on LangChain, simplifies the\ncreation and management of these agents and their workflows. It supports\ndynamic state management, enabling agents to maintain dialogue context and\nautomates complex workflows by linking agents and facilitating their\ncollaboration. With flexibility, open-source community support, and seamless\nintegration with LLMs, LangGraph empowers agents to deliver high-quality\ntranslations.\n  Together, Agent AI and LangGraph create a cohesive system where LangGraph\norchestrates agent interactions, ensuring that user inputs are analyzed,\nrouted, and processed efficiently. Experimental results demonstrate the\npotential of this system to enhance multilingual translation accuracy and\nscalability. By highlighting modular design and automated workflows, this paper\nsets the stage for further innovations in intelligent machine translation\nservices.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03801v1",
    "published_date": "2024-12-05 01:45:12 UTC",
    "updated_date": "2024-12-05 01:45:12 UTC"
  },
  {
    "arxiv_id": "2412.03800v1",
    "title": "ELEMENT: Episodic and Lifelong Exploration via Maximum Entropy",
    "authors": [
      "Hongming Li",
      "Shujian Yu",
      "Bin Liu",
      "Jose C. Principe"
    ],
    "abstract": "This paper proposes \\emph{Episodic and Lifelong Exploration via Maximum\nENTropy} (ELEMENT), a novel, multiscale, intrinsically motivated reinforcement\nlearning (RL) framework that is able to explore environments without using any\nextrinsic reward and transfer effectively the learned skills to downstream\ntasks. We advance the state of the art in three ways. First, we propose a\nmultiscale entropy optimization to take care of the fact that previous maximum\nstate entropy, for lifelong exploration with millions of state observations,\nsuffers from vanishing rewards and becomes very expensive computationally\nacross iterations. Therefore, we add an episodic maximum entropy over each\nepisode to speedup the search further. Second, we propose a novel intrinsic\nreward for episodic entropy maximization named \\emph{average episodic state\nentropy} which provides the optimal solution for a theoretical upper bound of\nthe episodic state entropy objective. Third, to speed the lifelong entropy\nmaximization, we propose a $k$ nearest neighbors ($k$NN) graph to organize the\nestimation of the entropy and updating processes that reduces the computation\nsubstantially. Our ELEMENT significantly outperforms state-of-the-art intrinsic\nrewards in both episodic and lifelong setups. Moreover, it can be exploited in\ntask-agnostic pre-training, collecting data for offline reinforcement learning,\netc.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03800v1",
    "published_date": "2024-12-05 01:42:13 UTC",
    "updated_date": "2024-12-05 01:42:13 UTC"
  },
  {
    "arxiv_id": "2412.03796v1",
    "title": "Automated Multi-Label Annotation for Mental Health Illnesses Using Large Language Models",
    "authors": [
      "Abdelrahaman A. Hassan",
      "Radwa J. Hanafy",
      "Mohammed E. Fouda"
    ],
    "abstract": "The growing prevalence and complexity of mental health disorders present\nsignificant challenges for accurate diagnosis and treatment, particularly in\nunderstanding the interplay between co-occurring conditions. Mental health\ndisorders, such as depression and Anxiety, often co-occur, yet current datasets\nderived from social media posts typically focus on single-disorder labels,\nlimiting their utility in comprehensive diagnostic analyses. This paper\naddresses this critical gap by proposing a novel methodology for cleaning,\nsampling, labeling, and combining data to create versatile multi-label\ndatasets. Our approach introduces a synthetic labeling technique to transform\nsingle-label datasets into multi-label annotations, capturing the complexity of\noverlapping mental health conditions. To achieve this, two single-label\ndatasets are first merged into a foundational multi-label dataset, enabling\nrealistic analyses of co-occurring diagnoses. We then design and evaluate\nvarious prompting strategies for large language models (LLMs), ranging from\nsingle-label predictions to unrestricted prompts capable of detecting any\npresent disorders. After rigorously assessing multiple LLMs and prompt\nconfigurations, the optimal combinations are identified and applied to label\nsix additional single-disorder datasets from RMHD. The result is SPAADE-DR, a\nrobust, multi-label dataset encompassing diverse mental health conditions. This\nresearch demonstrates the transformative potential of LLM-driven synthetic\nlabeling in advancing mental health diagnostics from social media data, paving\nthe way for more nuanced, data-driven insights into mental health care.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03796v1",
    "published_date": "2024-12-05 01:33:03 UTC",
    "updated_date": "2024-12-05 01:33:03 UTC"
  },
  {
    "arxiv_id": "2412.03792v1",
    "title": "Safe Adaptive Cruise Control Under Perception Uncertainty: A Deep Ensemble and Conformal Tube Model Predictive Control Approach",
    "authors": [
      "Xiao Li",
      "Anouck Girard",
      "Ilya Kolmanovsky"
    ],
    "abstract": "Autonomous driving heavily relies on perception systems to interpret the\nenvironment for decision-making. To enhance robustness in these safety critical\napplications, this paper considers a Deep Ensemble of Deep Neural Network\nregressors integrated with Conformal Prediction to predict and quantify\nuncertainties. In the Adaptive Cruise Control setting, the proposed method\nperforms state and uncertainty estimation from RGB images, informing the\ndownstream controller of the DNN perception uncertainties. An adaptive cruise\ncontroller using Conformal Tube Model Predictive Control is designed to ensure\nprobabilistic safety. Evaluations with a high-fidelity simulator demonstrate\nthe algorithm's effectiveness in speed tracking and safe distance maintaining,\nincluding in Out-Of-Distribution scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2412.03792v1",
    "published_date": "2024-12-05 01:01:53 UTC",
    "updated_date": "2024-12-05 01:01:53 UTC"
  },
  {
    "arxiv_id": "2412.03791v1",
    "title": "Coordinate In and Value Out: Training Flow Transformers in Ambient Space",
    "authors": [
      "Yuyang Wang",
      "Anurag Ranjan",
      "Josh Susskind",
      "Miguel Angel Bautista"
    ],
    "abstract": "Flow matching models have emerged as a powerful method for generative\nmodeling on domains like images or videos, and even on unstructured data like\n3D point clouds. These models are commonly trained in two stages: first, a data\ncompressor (i.e., a variational auto-encoder) is trained, and in a subsequent\ntraining stage a flow matching generative model is trained in the\nlow-dimensional latent space of the data compressor. This two stage paradigm\nadds complexity to the overall training recipe and sets obstacles for unifying\nmodels across data domains, as specific data compressors are used for different\ndata modalities. To this end, we introduce Ambient Space Flow Transformers\n(ASFT), a domain-agnostic approach to learn flow matching transformers in\nambient space, sidestepping the requirement of training compressors and\nsimplifying the training process. We introduce a conditionally independent\npoint-wise training objective that enables ASFT to make predictions\ncontinuously in coordinate space. Our empirical results demonstrate that using\ngeneral purpose transformer blocks, ASFT effectively handles different data\nmodalities such as images and 3D point clouds, achieving strong performance in\nboth domains and outperforming comparable approaches. ASFT is a promising step\ntowards domain-agnostic flow matching generative models that can be trivially\nadopted in different data domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 10 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2412.03791v1",
    "published_date": "2024-12-05 01:00:07 UTC",
    "updated_date": "2024-12-05 01:00:07 UTC"
  },
  {
    "arxiv_id": "2412.03784v1",
    "title": "Speech Recognition-based Feature Extraction for Enhanced Automatic Severity Classification in Dysarthric Speech",
    "authors": [
      "Yerin Choi",
      "Jeehyun Lee",
      "Myoung-Wan Koo"
    ],
    "abstract": "Due to the subjective nature of current clinical evaluation, the need for\nautomatic severity evaluation in dysarthric speech has emerged. DNN models\noutperform ML models but lack user-friendly explainability. ML models offer\nexplainable results at a feature level, but their performance is comparatively\nlower. Current ML models extract various features from raw waveforms to predict\nseverity. However, existing methods do not encompass all dysarthric features\nused in clinical evaluation. To address this gap, we propose a feature\nextraction method that minimizes information loss. We introduce an ASR\ntranscription as a novel feature extraction source. We finetune the ASR model\nfor dysarthric speech, then use this model to transcribe dysarthric speech and\nextract word segment boundary information. It enables capturing finer\npronunciation and broader prosodic features. These features demonstrated an\nimproved severity prediction performance to existing features: balanced\naccuracy of 83.72%.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted to SLT 2024",
    "pdf_url": "http://arxiv.org/pdf/2412.03784v1",
    "published_date": "2024-12-05 00:12:53 UTC",
    "updated_date": "2024-12-05 00:12:53 UTC"
  },
  {
    "arxiv_id": "2412.03783v2",
    "title": "Expressivity of Representation Learning on Continuous-Time Dynamic Graphs: An Information-Flow Centric Review",
    "authors": [
      "Sofiane Ennadir",
      "Gabriela Zarzar Gandler",
      "Filip Cornell",
      "Lele Cao",
      "Oleg Smirnov",
      "Tianze Wang",
      "Levente Zólyomi",
      "Björn Brinne",
      "Sahar Asadi"
    ],
    "abstract": "Graphs are ubiquitous in real-world applications, ranging from social\nnetworks to biological systems, and have inspired the development of Graph\nNeural Networks (GNNs) for learning expressive representations. While most\nresearch has centered on static graphs, many real-world scenarios involve\ndynamic, temporally evolving graphs, motivating the need for Continuous-Time\nDynamic Graph (CTDG) models. This paper provides a comprehensive review of\nGraph Representation Learning (GRL) on CTDGs with a focus on Self-Supervised\nRepresentation Learning (SSRL). We introduce a novel theoretical framework that\nanalyzes the expressivity of CTDG models through an Information-Flow (IF) lens,\nquantifying their ability to propagate and encode temporal and structural\ninformation. Leveraging this framework, we categorize existing CTDG methods\nbased on their suitability for different graph types and application scenarios.\nWithin the same scope, we examine the design of SSRL methods tailored to CTDGs,\nsuch as predictive and contrastive approaches, highlighting their potential to\nmitigate the reliance on labeled data. Empirical evaluations on synthetic and\nreal-world datasets validate our theoretical insights, demonstrating the\nstrengths and limitations of various methods across long-range, bi-partite and\ncommunity-based graphs. This work offers both a theoretical foundation and\npractical guidance for selecting and developing CTDG models, advancing the\nunderstanding of GRL in dynamic settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "68R10, 05Cxx, 68Txx",
      "I.2.6; I.5.1; G.2.2"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by TMLR. Source code: https://github.com/king/ctdg-info-flow",
    "pdf_url": "http://arxiv.org/pdf/2412.03783v2",
    "published_date": "2024-12-05 00:12:50 UTC",
    "updated_date": "2025-04-14 12:21:49 UTC"
  }
]
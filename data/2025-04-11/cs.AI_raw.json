[
  {
    "arxiv_id": "2504.09014v2",
    "title": "MSCCL++: Rethinking GPU Communication Abstractions for Cutting-edge AI Applications",
    "authors": [
      "Aashaka Shah",
      "Abhinav Jangda",
      "Binyang Li",
      "Caio Rocha",
      "Changho Hwang",
      "Jithin Jose",
      "Madan Musuvathi",
      "Olli Saarikivi",
      "Peng Cheng",
      "Qinghua Zhou",
      "Roshan Dathathri",
      "Saeed Maleki",
      "Ziyue Yang"
    ],
    "abstract": "Modern cutting-edge AI applications are being developed over fast-evolving,\nheterogeneous, nascent hardware devices. This requires frequent reworking of\nthe AI software stack to adopt bottom-up changes from new hardware, which takes\ntime for general-purpose software libraries. Consequently, real applications\noften develop custom software stacks optimized for their specific workloads and\nhardware. Custom stacks help in quick development and optimization, but incur a\nlot of redundant efforts across applications in writing non-portable code. This\npaper discusses an alternative communication library interface for AI\napplications that offers both portability and performance by reducing redundant\nefforts while maintaining flexibility for customization. We present MSCCL++, a\nnovel abstraction of GPU communication based on separation of concerns: (1) a\nprimitive interface provides a minimal hardware abstraction as a common ground\nfor software and hardware developers to write custom communication, and (2)\nhigher-level portable interfaces and specialized implementations enable\noptimization for different workloads and hardware environments. This approach\nmakes the primitive interface reusable across applications while enabling\nhighly flexible optimization. Compared to state-of-the-art baselines (NCCL,\nRCCL, and MSCCL), MSCCL++ achieves speedups of up to 5.4$\\times$ for collective\ncommunication and up to 15% for real-world AI inference workloads. MSCCL++ is\nin production of multiple AI services provided by Microsoft Azure, and is also\nadopted by RCCL, the GPU collective communication library maintained by AMD.\nMSCCL++ is open-source and available at https://github.com/microsoft/mscclpp.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "13 pages, 12 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.09014v2",
    "published_date": "2025-04-11 23:51:54 UTC",
    "updated_date": "2025-04-20 01:20:03 UTC"
  },
  {
    "arxiv_id": "2504.08999v1",
    "title": "MCP Bridge: A Lightweight, LLM-Agnostic RESTful Proxy for Model Context Protocol Servers",
    "authors": [
      "Arash Ahmadi",
      "Sarah Sharif",
      "Yaser M. Banad"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly augmented with external tools\nthrough standardized interfaces like the Model Context Protocol (MCP). However,\ncurrent MCP implementations face critical limitations: they typically require\nlocal process execution through STDIO transports, making them impractical for\nresource-constrained environments like mobile devices, web browsers, and edge\ncomputing. We present MCP Bridge, a lightweight RESTful proxy that connects to\nmultiple MCP servers and exposes their capabilities through a unified API.\nUnlike existing solutions, MCP Bridge is fully LLM-agnostic, supporting any\nbackend regardless of vendor. The system implements a risk-based execution\nmodel with three security levels standard execution, confirmation workflow, and\nDocker isolation while maintaining backward compatibility with standard MCP\nclients. Complementing this server-side infrastructure is a Python based MCP\nGemini Agent that facilitates natural language interaction with MCP tools. The\nevaluation demonstrates that MCP Bridge successfully addresses the constraints\nof direct MCP connections while providing enhanced security controls and\ncross-platform compatibility, enabling sophisticated LLM-powered applications\nin previously inaccessible environments",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "13 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08999v1",
    "published_date": "2025-04-11 22:19:48 UTC",
    "updated_date": "2025-04-11 22:19:48 UTC"
  },
  {
    "arxiv_id": "2504.08985v2",
    "title": "Learning from Elders: Making an LLM-powered Chatbot for Retirement Communities more Accessible through User-centered Design",
    "authors": [
      "Luna Xingyu Li",
      "Ray-yuan Chung",
      "Feng Chen",
      "Wenyu Zeng",
      "Yein Jeon",
      "Oleg Zaslavsky"
    ],
    "abstract": "Low technology and eHealth literacy among older adults in retirement\ncommunities hinder engagement with digital tools. To address this, we designed\nan LLM-powered chatbot prototype using a human-centered approach for a local\nretirement community. Through interviews and persona development, we\nprioritized accessibility and dual functionality: simplifying internal\ninformation retrieval and improving technology and eHealth literacy. A pilot\ntrial with residents demonstrated high satisfaction and ease of use, but also\nidentified areas for further improvement. Based on the feedback, we refined the\nchatbot using GPT-3.5 Turbo and Streamlit. The chatbot employs tailored prompt\nengineering to deliver concise responses. Accessible features like adjustable\nfont size, interface theme and personalized follow-up responses were\nimplemented. Future steps include enabling voice-to-text function and\nlongitudinal intervention studies. Together, our results highlight the\npotential of LLM-driven chatbots to empower older adults through accessible,\npersonalized interactions, bridging literacy gaps in retirement communities.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Accepted as Research talk for Considering Cultural and Linguistic\n  Diversity in AI Applications workshop at CALD-AI@ASIS&T 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.08985v2",
    "published_date": "2025-04-11 21:30:44 UTC",
    "updated_date": "2025-04-28 06:10:19 UTC"
  },
  {
    "arxiv_id": "2504.08981v1",
    "title": "AGENT: An Aerial Vehicle Generation and Design Tool Using Large Language Models",
    "authors": [
      "Colin Samplawski",
      "Adam D. Cobb",
      "Susmit Jha"
    ],
    "abstract": "Computer-aided design (CAD) is a promising application area for emerging\nartificial intelligence methods. Traditional workflows for cyberphysical\nsystems create detailed digital models which can be evaluated by physics\nsimulators in order to narrow the search space before creating physical\nprototypes. A major bottleneck of this approach is that the simulators are\noften computationally expensive and slow. Recent advancements in AI methods\noffer the possibility to accelerate these pipelines. We use the recently\nreleased AircraftVerse dataset, which is especially suited for developing and\nevaluating large language models for designs. AircraftVerse contains a diverse\nset of UAV designs represented via textual design trees together with detailed\nphysics simulation results. Following the recent success of large language\nmodels (LLMs), we propose AGENT (Aircraft GENeraTor). AGENT is a comprehensive\ndesign tool built on the CodeT5+ LLM which learns powerful representations of\naircraft textual designs directly from JSON files. We develop a curriculum of\ntraining tasks which imbues a single model with a suite of useful features.\nAGENT is able to generate designs conditioned on properties of flight dynamics\n(hover time, maximum speed, etc.). Additionally, AGENT can issue evaluations of\ndesigns allowing it to act as a surrogate model of the physics simulation that\nunderlies the AircraftVerse dataset. We present a series of experiments which\ndemonstrate our system's abilities. We are able to achieve strong performance\nusing the smallest member of the CodeT5+ family (220M parameters). This allows\nfor a flexible and powerful system which can be executed on a single GPU\nenabling a clear path toward future deployment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08981v1",
    "published_date": "2025-04-11 21:13:10 UTC",
    "updated_date": "2025-04-11 21:13:10 UTC"
  },
  {
    "arxiv_id": "2504.08974v1",
    "title": "Mixed Signals: Decoding VLMs' Reasoning and Underlying Bias in Vision-Language Conflict",
    "authors": [
      "Pouya Pezeshkpour",
      "Moin Aminnaseri",
      "Estevam Hruschka"
    ],
    "abstract": "Vision-language models (VLMs) have demonstrated impressive performance by\neffectively integrating visual and textual information to solve complex tasks.\nHowever, it is not clear how these models reason over the visual and textual\ndata together, nor how the flow of information between modalities is\nstructured. In this paper, we examine how VLMs reason by analyzing their biases\nwhen confronted with scenarios that present conflicting image and text cues, a\ncommon occurrence in real-world applications. To uncover the extent and nature\nof these biases, we build upon existing benchmarks to create five datasets\ncontaining mismatched image-text pairs, covering topics in mathematics,\nscience, and visual descriptions. Our analysis shows that VLMs favor text in\nsimpler queries but shift toward images as query complexity increases. This\nbias correlates with model scale, with the difference between the percentage of\nimage- and text-preferred responses ranging from +56.8% (image favored) to\n-74.4% (text favored), depending on the task and model. In addition, we explore\nthree mitigation strategies: simple prompt modifications, modifications that\nexplicitly instruct models on how to handle conflicting information (akin to\nchain-of-thought prompting), and a task decomposition strategy that analyzes\neach modality separately before combining their results. Our findings indicate\nthat the effectiveness of these strategies in identifying and mitigating bias\nvaries significantly and is closely linked to the model's overall performance\non the task and the specific modality in question.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08974v1",
    "published_date": "2025-04-11 20:56:52 UTC",
    "updated_date": "2025-04-11 20:56:52 UTC"
  },
  {
    "arxiv_id": "2504.08970v1",
    "title": "On Large-scale Evaluation of Embedding Models for Knowledge Graph Completion",
    "authors": [
      "Nasim Shirvani-Mahdavi",
      "Farahnaz Akrami",
      "Chengkai Li"
    ],
    "abstract": "Knowledge graph embedding (KGE) models are extensively studied for knowledge\ngraph completion, yet their evaluation remains constrained by unrealistic\nbenchmarks. Commonly used datasets are either faulty or too small to reflect\nreal-world data. Few studies examine the role of mediator nodes, which are\nessential for modeling n-ary relationships, or investigate model performance\nvariation across domains. Standard evaluation metrics rely on the closed-world\nassumption, which penalizes models for correctly predicting missing triples,\ncontradicting the fundamental goals of link prediction. These metrics often\ncompress accuracy assessment into a single value, obscuring models' specific\nstrengths and weaknesses. The prevailing evaluation protocol operates under the\nunrealistic assumption that an entity's properties, for which values are to be\npredicted, are known in advance. While alternative protocols such as property\nprediction, entity-pair ranking and triple classification address some of these\nlimitations, they remain underutilized. This paper conducts a comprehensive\nevaluation of four representative KGE models on large-scale datasets FB-CVT-REV\nand FB+CVT-REV. Our analysis reveals critical insights, including substantial\nperformance variations between small and large datasets, both in relative\nrankings and absolute metrics, systematic overestimation of model capabilities\nwhen n-ary relations are binarized, and fundamental limitations in current\nevaluation protocols and metrics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08970v1",
    "published_date": "2025-04-11 20:49:02 UTC",
    "updated_date": "2025-04-11 20:49:02 UTC"
  },
  {
    "arxiv_id": "2504.08959v1",
    "title": "MotionDreamer: One-to-Many Motion Synthesis with Localized Generative Masked Transformer",
    "authors": [
      "Yilin Wang",
      "Chuan Guo",
      "Yuxuan Mu",
      "Muhammad Gohar Javed",
      "Xinxin Zuo",
      "Juwei Lu",
      "Hai Jiang",
      "Li Cheng"
    ],
    "abstract": "Generative masked transformers have demonstrated remarkable success across\nvarious content generation tasks, primarily due to their ability to effectively\nmodel large-scale dataset distributions with high consistency. However, in the\nanimation domain, large datasets are not always available. Applying generative\nmasked modeling to generate diverse instances from a single MoCap reference may\nlead to overfitting, a challenge that remains unexplored. In this work, we\npresent MotionDreamer, a localized masked modeling paradigm designed to learn\ninternal motion patterns from a given motion with arbitrary topology and\nduration. By embedding the given motion into quantized tokens with a novel\ndistribution regularization method, MotionDreamer constructs a robust and\ninformative codebook for local motion patterns. Moreover, a sliding window\nlocal attention is introduced in our masked transformer, enabling the\ngeneration of natural yet diverse animations that closely resemble the\nreference motion patterns. As demonstrated through comprehensive experiments,\nMotionDreamer outperforms the state-of-the-art methods that are typically GAN\nor Diffusion-based in both faithfulness and diversity. Thanks to the\nconsistency and robustness of the quantization-based approach, MotionDreamer\ncan also effectively perform downstream tasks such as temporal motion editing,\n\\textcolor{update}{crowd animation}, and beat-aligned dance generation, all\nusing a single reference motion. Visit our project page:\nhttps://motiondreamer.github.io/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2025 acceptance",
    "pdf_url": "http://arxiv.org/pdf/2504.08959v1",
    "published_date": "2025-04-11 20:27:22 UTC",
    "updated_date": "2025-04-11 20:27:22 UTC"
  },
  {
    "arxiv_id": "2504.08958v1",
    "title": "Generating Planning Feedback for Open-Ended Programming Exercises with LLMs",
    "authors": [
      "Mehmet Arif Demirtaş",
      "Claire Zheng",
      "Max Fowler",
      "Kathryn Cunningham"
    ],
    "abstract": "To complete an open-ended programming exercise, students need to both plan a\nhigh-level solution and implement it using the appropriate syntax. However,\nthese problems are often autograded on the correctness of the final submission\nthrough test cases, and students cannot get feedback on their planning process.\nLarge language models (LLM) may be able to generate this feedback by detecting\nthe overall code structure even for submissions with syntax errors. To this\nend, we propose an approach that detects which high-level goals and patterns\n(i.e. programming plans) exist in a student program with LLMs. We show that\nboth the full GPT-4o model and a small variant (GPT-4o-mini) can detect these\nplans with remarkable accuracy, outperforming baselines inspired by\nconventional approaches to code analysis. We further show that the smaller,\ncost-effective variant (GPT-4o-mini) achieves results on par with\nstate-of-the-art (GPT-4o) after fine-tuning, creating promising implications\nfor smaller models for real-time grading. These smaller models can be\nincorporated into autograders for open-ended code-writing exercises to provide\nfeedback for students' implicit planning skills, even when their program is\nsyntactically incorrect. Furthermore, LLMs may be useful in providing feedback\nfor problems in other domains where students start with a set of high-level\nsolution steps and iteratively compute the output, such as math and physics\nproblems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as full paper at AIED 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.08958v1",
    "published_date": "2025-04-11 20:26:49 UTC",
    "updated_date": "2025-04-11 20:26:49 UTC"
  },
  {
    "arxiv_id": "2504.13924v1",
    "title": "Evaluation and Incident Prevention in an Enterprise AI Assistant",
    "authors": [
      "Akash V. Maharaj",
      "David Arbour",
      "Daniel Lee",
      "Uttaran Bhattacharya",
      "Anup Rao",
      "Austin Zane",
      "Avi Feller",
      "Kun Qian",
      "Yunyao Li"
    ],
    "abstract": "Enterprise AI Assistants are increasingly deployed in domains where accuracy\nis paramount, making each erroneous output a potentially significant incident.\nThis paper presents a comprehensive framework for monitoring, benchmarking, and\ncontinuously improving such complex, multi-component systems under active\ndevelopment by multiple teams. Our approach encompasses three key elements: (1)\na hierarchical ``severity'' framework for incident detection that identifies\nand categorizes errors while attributing component-specific error rates,\nfacilitating targeted improvements; (2) a scalable and principled methodology\nfor benchmark construction, evaluation, and deployment, designed to accommodate\nmultiple development teams, mitigate overfitting risks, and assess the\ndownstream impact of system modifications; and (3) a continual improvement\nstrategy leveraging multidimensional evaluation, enabling the identification\nand implementation of diverse enhancement opportunities. By adopting this\nholistic framework, organizations can systematically enhance the reliability\nand performance of their AI Assistants, ensuring their efficacy in critical\nenterprise environments. We conclude by discussing how this multifaceted\nevaluation approach opens avenues for various classes of enhancements, paving\nthe way for more robust and trustworthy AI systems.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "7 pages, 5 figures. Accepted at IAAI-25",
    "pdf_url": "http://arxiv.org/pdf/2504.13924v1",
    "published_date": "2025-04-11 20:10:04 UTC",
    "updated_date": "2025-04-11 20:10:04 UTC"
  },
  {
    "arxiv_id": "2504.08947v1",
    "title": "Forecasting Cryptocurrency Prices using Contextual ES-adRNN with Exogenous Variables",
    "authors": [
      "Slawek Smyl",
      "Grzegorz Dudek",
      "Paweł Pełka"
    ],
    "abstract": "In this paper, we introduce a new approach to multivariate forecasting\ncryptocurrency prices using a hybrid contextual model combining exponential\nsmoothing (ES) and recurrent neural network (RNN). The model consists of two\ntracks: the context track and the main track. The context track provides\nadditional information to the main track, extracted from representative series.\nThis information as well as information extracted from exogenous variables is\ndynamically adjusted to the individual series forecasted by the main track. The\nRNN stacked architecture with hierarchical dilations, incorporating recently\ndeveloped attentive dilated recurrent cells, allows the model to capture short\nand long-term dependencies across time series and dynamically weight input\ninformation. The model generates both point daily forecasts and predictive\nintervals for one-day, one-week and four-week horizons. We apply our model to\nforecast prices of 15 cryptocurrencies based on 17 input variables and compare\nits performance with that of comparative models, including both statistical and\nML ones.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08947v1",
    "published_date": "2025-04-11 20:00:03 UTC",
    "updated_date": "2025-04-11 20:00:03 UTC"
  },
  {
    "arxiv_id": "2504.08943v1",
    "title": "Investigating the Treacherous Turn in Deep Reinforcement Learning",
    "authors": [
      "Chace Ashcraft",
      "Kiran Karra",
      "Josh Carney",
      "Nathan Drenkow"
    ],
    "abstract": "The Treacherous Turn refers to the scenario where an artificial intelligence\n(AI) agent subtly, and perhaps covertly, learns to perform a behavior that\nbenefits itself but is deemed undesirable and potentially harmful to a human\nsupervisor. During training, the agent learns to behave as expected by the\nhuman supervisor, but when deployed to perform its task, it performs an\nalternate behavior without the supervisor there to prevent it. Initial\nexperiments applying DRL to an implementation of the A Link to the Past example\ndo not produce the treacherous turn effect naturally, despite various\nmodifications to the environment intended to produce it. However, in this work,\nwe find the treacherous behavior to be reproducible in a DRL agent when using\nother trojan injection strategies. This approach deviates from the prototypical\ntreacherous turn behavior since the behavior is explicitly trained into the\nagent, rather than occurring as an emergent consequence of environmental\ncomplexity or poor objective specification. Nonetheless, these experiments\nprovide new insights into the challenges of producing agents capable of true\ntreacherous turn behavior.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08943v1",
    "published_date": "2025-04-11 19:50:08 UTC",
    "updated_date": "2025-04-11 19:50:08 UTC"
  },
  {
    "arxiv_id": "2504.08942v1",
    "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories",
    "authors": [
      "Xing Han Lù",
      "Amirhossein Kazemnejad",
      "Nicholas Meade",
      "Arkil Patel",
      "Dongchan Shin",
      "Alejandra Zambrano",
      "Karolina Stańczak",
      "Peter Shaw",
      "Christopher J. Pal",
      "Siva Reddy"
    ],
    "abstract": "Web agents enable users to perform tasks on web browsers through natural\nlanguage interaction. Evaluating web agents trajectories is an important\nproblem, since it helps us determine whether the agent successfully completed\nthe tasks. Rule-based methods are widely used for this purpose, but they are\nchallenging to extend to new tasks and may not always recognize successful\ntrajectories. We may achieve higher accuracy through human evaluation, but the\nprocess would be substantially slower and more expensive. Automatic evaluations\nwith LLMs may avoid the challenges of designing new rules and manually\nannotating trajectories, enabling faster and cost-effective evaluation.\nHowever, it is unclear how effective they are at evaluating web agents. To this\nend, we propose AgentRewardBench, the first benchmark to assess the\neffectiveness of LLM judges for evaluating web agents. AgentRewardBench\ncontains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in\nAgentRewardBench is reviewed by an expert, who answers questions pertaining to\nthe success, side effects, and repetitiveness of the agent. Using our\nbenchmark, we evaluate 12 LLM judges and find that no single LLM excels across\nall benchmarks. We also find that the rule-based evaluation used by common\nbenchmarks tends to underreport the success rate of web agents, highlighting a\nkey weakness of rule-based evaluation and the need to develop more flexible\nautomatic evaluations. We release the benchmark at:\nhttps://agent-reward-bench.github.io",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08942v1",
    "published_date": "2025-04-11 19:49:22 UTC",
    "updated_date": "2025-04-11 19:49:22 UTC"
  },
  {
    "arxiv_id": "2504.08940v1",
    "title": "Combining Forecasts using Meta-Learning: A Comparative Study for Complex Seasonality",
    "authors": [
      "Grzegorz Dudek"
    ],
    "abstract": "In this paper, we investigate meta-learning for combining forecasts generated\nby models of different types. While typical approaches for combining forecasts\ninvolve simple averaging, machine learning techniques enable more sophisticated\nmethods of combining through meta-learning, leading to improved forecasting\naccuracy. We use linear regression, $k$-nearest neighbors, multilayer\nperceptron, random forest, and long short-term memory as meta-learners. We\ndefine global and local meta-learning variants for time series with complex\nseasonality and compare meta-learners on multiple forecasting problems,\ndemonstrating their superior performance compared to simple averaging.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "IEEE 10th International Conference on Data Science and Advanced\n  Analytics, DSAA'23, pp. 1-10, 2023",
    "pdf_url": "http://arxiv.org/pdf/2504.08940v1",
    "published_date": "2025-04-11 19:43:11 UTC",
    "updated_date": "2025-04-11 19:43:11 UTC"
  },
  {
    "arxiv_id": "2504.08934v1",
    "title": "Long Context In-Context Compression by Getting to the Gist of Gisting",
    "authors": [
      "Aleksandar Petrov",
      "Mark Sandler",
      "Andrey Zhmoginov",
      "Nolan Miller",
      "Max Vladymyrov"
    ],
    "abstract": "Long context processing is critical for the adoption of LLMs, but existing\nmethods often introduce architectural complexity that hinders their practical\nadoption. Gisting, an in-context compression method with no architectural\nmodification to the decoder transformer, is a promising approach due to its\nsimplicity and compatibility with existing frameworks. While effective for\nshort instructions, we demonstrate that gisting struggles with longer contexts,\nwith significant performance drops even at minimal compression rates.\nSurprisingly, a simple average pooling baseline consistently outperforms\ngisting. We analyze the limitations of gisting, including information flow\ninterruptions, capacity limitations and the inability to restrict its attention\nto subsets of the context. Motivated by theoretical insights into the\nperformance gap between gisting and average pooling, and supported by extensive\nexperimentation, we propose GistPool, a new in-context compression method.\nGistPool preserves the simplicity of gisting, while significantly boosting its\nperformance on long context compression tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08934v1",
    "published_date": "2025-04-11 19:23:31 UTC",
    "updated_date": "2025-04-11 19:23:31 UTC"
  },
  {
    "arxiv_id": "2504.08923v1",
    "title": "A convergence law for continuous logic and continuous structures with finite domains",
    "authors": [
      "Vera Koponen"
    ],
    "abstract": "We consider continuous relational structures with finite domain $[n] := \\{1,\n\\ldots, n\\}$ and a many valued logic, $CLA$, with values in the unit interval\nand which uses continuous connectives and continuous aggregation functions.\n$CLA$ subsumes first-order logic on ``conventional'' finite structures. To each\nrelation symbol $R$ and identity constraint $ic$ on a tuple the length of which\nmatches the arity of $R$ we associate a continuous probability density function\n$\\mu_R^{ic} : [0, 1] \\to [0, \\infty)$.\n  We also consider a probability distribution on the set $\\mathbf{W}_n$ of\ncontinuous structures with domain $[n]$ which is such that for every relation\nsymbol $R$, identity constraint $ic$, and tuple $\\bar{a}$ satisfying $ic$, the\ndistribution of the value of $R(\\bar{a})$ is given by $\\mu_R^{ic}$,\nindependently of the values for other relation symbols or other tuples.\n  In this setting we prove that every formula in $CLA$ is asymptotically\nequivalent to a formula without any aggregation function. This is used to prove\na convergence law for $CLA$ which reads as follows for formulas without free\nvariables: If $\\varphi \\in CLA$ has no free variable and $I \\subseteq [0, 1]$\nis an interval, then there is $\\alpha \\in [0, 1]$ such that, as $n$ tends to\ninfinity, the probability that the value of $\\varphi$ is in $I$ tends to\n$\\alpha$.",
    "categories": [
      "cs.LO",
      "cs.AI",
      "math.LO",
      "03C13, 03C66, 68T27, 68T30, 68T37",
      "F.4.1; G.3; I.2.4"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08923v1",
    "published_date": "2025-04-11 19:08:38 UTC",
    "updated_date": "2025-04-11 19:08:38 UTC"
  },
  {
    "arxiv_id": "2504.08919v1",
    "title": "Are We Merely Justifying Results ex Post Facto? Quantifying Explanatory Inversion in Post-Hoc Model Explanations",
    "authors": [
      "Zhen Tan",
      "Song Wang",
      "Yifan Li",
      "Yu Kong",
      "Jundong Li",
      "Tianlong Chen",
      "Huan Liu"
    ],
    "abstract": "Post-hoc explanation methods provide interpretation by attributing\npredictions to input features. Natural explanations are expected to interpret\nhow the inputs lead to the predictions. Thus, a fundamental question arises: Do\nthese explanations unintentionally reverse the natural relationship between\ninputs and outputs? Specifically, are the explanations rationalizing\npredictions from the output rather than reflecting the true decision process?\nTo investigate such explanatory inversion, we propose Inversion Quantification\n(IQ), a framework that quantifies the degree to which explanations rely on\noutputs and deviate from faithful input-output relationships. Using the\nframework, we demonstrate on synthetic datasets that widely used methods such\nas LIME and SHAP are prone to such inversion, particularly in the presence of\nspurious correlations, across tabular, image, and text domains. Finally, we\npropose Reproduce-by-Poking (RBP), a simple and model-agnostic enhancement to\npost-hoc explanation methods that integrates forward perturbation checks. We\nfurther show that under the IQ framework, RBP theoretically guarantees the\nmitigation of explanatory inversion. Empirically, for example, on the\nsynthesized data, RBP can reduce the inversion by 1.8% on average across iconic\npost-hoc explanation approaches and domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08919v1",
    "published_date": "2025-04-11 19:00:12 UTC",
    "updated_date": "2025-04-11 19:00:12 UTC"
  },
  {
    "arxiv_id": "2504.08915v1",
    "title": "Parameter-Free Fine-tuning via Redundancy Elimination for Vision Foundation Models",
    "authors": [
      "Jiahuan Long",
      "Tingsong Jiang",
      "Wen Yao",
      "Yizhe Xiong",
      "Zhengqin Xu",
      "Shuai Jia",
      "Chao Ma"
    ],
    "abstract": "Vision foundation models (VFMs) are large pre-trained models that form the\nbackbone of various vision tasks. Fine-tuning VFMs can further unlock their\npotential for downstream tasks or scenarios. However, VFMs often contain\nsignificant feature redundancy, which may limit their adaptability to new\ntasks. In this paper, we investigate the redundancies in the segment anything\nmodel (SAM) and then propose a parameter-free fine-tuning method to address\nthis issue. Unlike traditional fine-tuning methods that adjust parameters, our\nmethod emphasizes selecting, reusing, and enhancing pre-trained features,\noffering a new perspective on model fine-tuning. Specifically, we introduce a\nchannel selection algorithm based on the model's output difference to identify\nredundant and effective channels. By selectively replacing the redundant\nchannels with more effective ones, we filter out less useful features and reuse\nthe more relevant features to downstream tasks, thereby enhancing the\ntask-specific feature representation. Experiments on both out-of-domain and\nin-domain datasets demonstrate the efficiency and effectiveness of our method.\nNotably, our approach can seamlessly integrate with existing fine-tuning\nstrategies (e.g., LoRA, Adapter), further boosting the performance of already\nfine-tuned models. Moreover, since our channel selection involves only model\ninference, our method significantly reduces computational and GPU memory\noverhead.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08915v1",
    "published_date": "2025-04-11 18:44:27 UTC",
    "updated_date": "2025-04-11 18:44:27 UTC"
  },
  {
    "arxiv_id": "2504.12325v1",
    "title": "LLMTaxo: Leveraging Large Language Models for Constructing Taxonomy of Factual Claims from Social Media",
    "authors": [
      "Haiqi Zhang",
      "Zhengyuan Zhu",
      "Zeyu Zhang",
      "Chengkai Li"
    ],
    "abstract": "With the vast expansion of content on social media platforms, analyzing and\ncomprehending online discourse has become increasingly complex. This paper\nintroduces LLMTaxo, a novel framework leveraging large language models for the\nautomated construction of taxonomy of factual claims from social media by\ngenerating topics from multi-level granularities. This approach aids\nstakeholders in more effectively navigating the social media landscapes. We\nimplement this framework with different models across three distinct datasets\nand introduce specially designed taxonomy evaluation metrics for a\ncomprehensive assessment. With the evaluations from both human evaluators and\nGPT-4, the results indicate that LLMTaxo effectively categorizes factual claims\nfrom social media, and reveals that certain models perform better on specific\ndatasets.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12325v1",
    "published_date": "2025-04-11 18:43:16 UTC",
    "updated_date": "2025-04-11 18:43:16 UTC"
  },
  {
    "arxiv_id": "2504.08912v1",
    "title": "HyperCore: The Core Framework for Building Hyperbolic Foundation Models with Comprehensive Modules",
    "authors": [
      "Neil He",
      "Menglin Yang",
      "Rex Ying"
    ],
    "abstract": "Hyperbolic neural networks have emerged as a powerful tool for modeling\nhierarchical data across diverse modalities. Recent studies show that token\ndistributions in foundation models exhibit scale-free properties, suggesting\nthat hyperbolic space is a more suitable ambient space than Euclidean space for\nmany pre-training and downstream tasks. However, existing tools lack essential\ncomponents for building hyperbolic foundation models, making it difficult to\nleverage recent advancements. We introduce HyperCore, a comprehensive\nopen-source framework that provides core modules for constructing hyperbolic\nfoundation models across multiple modalities. HyperCore's modules can be\neffortlessly combined to develop novel hyperbolic foundation models,\neliminating the need to extensively modify Euclidean modules from scratch and\npossible redundant research efforts. To demonstrate its versatility, we build\nand test the first fully hyperbolic vision transformers (LViT) with a\nfine-tuning pipeline, the first fully hyperbolic multimodal CLIP model\n(L-CLIP), and a hybrid Graph RAG with a hyperbolic graph encoder. Our\nexperiments demonstrate that LViT outperforms its Euclidean counterpart.\nAdditionally, we benchmark and reproduce experiments across hyperbolic GNNs,\nCNNs, Transformers, and vision Transformers to highlight HyperCore's\nadvantages.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "11 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08912v1",
    "published_date": "2025-04-11 18:35:46 UTC",
    "updated_date": "2025-04-11 18:35:46 UTC"
  },
  {
    "arxiv_id": "2504.08909v1",
    "title": "Hybrid AI-Physical Modeling for Penetration Bias Correction in X-band InSAR DEMs: A Greenland Case Study",
    "authors": [
      "Islam Mansour",
      "Georg Fischer",
      "Ronny Haensch",
      "Irena Hajnsek"
    ],
    "abstract": "Digital elevation models derived from Interferometric Synthetic Aperture\nRadar (InSAR) data over glacial and snow-covered regions often exhibit\nsystematic elevation errors, commonly termed \"penetration bias.\" We leverage\nexisting physics-based models and propose an integrated correction framework\nthat combines parametric physical modeling with machine learning. We evaluate\nthe approach across three distinct training scenarios - each defined by a\ndifferent set of acquisition parameters - to assess overall performance and the\nmodel's ability to generalize. Our experiments on Greenland's ice sheet using\nTanDEM-X data show that the proposed hybrid model corrections significantly\nreduce the mean and standard deviation of DEM errors compared to a purely\nphysical modeling baseline. The hybrid framework also achieves significantly\nimproved generalization than a pure ML approach when trained on data with\nlimited diversity in acquisition parameters.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.08909v1",
    "published_date": "2025-04-11 18:24:22 UTC",
    "updated_date": "2025-04-11 18:24:22 UTC"
  },
  {
    "arxiv_id": "2504.08906v1",
    "title": "Robust SAM: On the Adversarial Robustness of Vision Foundation Models",
    "authors": [
      "Jiahuan Long",
      "Zhengqin Xu",
      "Tingsong Jiang",
      "Wen Yao",
      "Shuai Jia",
      "Chao Ma",
      "Xiaoqian Chen"
    ],
    "abstract": "The Segment Anything Model (SAM) is a widely used vision foundation model\nwith diverse applications, including image segmentation, detection, and\ntracking. Given SAM's wide applications, understanding its robustness against\nadversarial attacks is crucial for real-world deployment. However, research on\nSAM's robustness is still in its early stages. Existing attacks often overlook\nthe role of prompts in evaluating SAM's robustness, and there has been\ninsufficient exploration of defense methods to balance the robustness and\naccuracy. To address these gaps, this paper proposes an adversarial robustness\nframework designed to evaluate and enhance the robustness of SAM. Specifically,\nwe introduce a cross-prompt attack method to enhance the attack transferability\nacross different prompt types. Besides attacking, we propose a few-parameter\nadaptation strategy to defend SAM against various adversarial attacks. To\nbalance robustness and accuracy, we use the singular value decomposition (SVD)\nto constrain the space of trainable parameters, where only singular values are\nadaptable. Experiments demonstrate that our cross-prompt attack method\noutperforms previous approaches in terms of attack success rate on both SAM and\nSAM 2. By adapting only 512 parameters, we achieve at least a 15\\% improvement\nin mean intersection over union (mIoU) against various adversarial attacks.\nCompared to previous defense methods, our approach enhances the robustness of\nSAM while maximally maintaining its original performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by AAAI2025",
    "pdf_url": "http://arxiv.org/pdf/2504.08906v1",
    "published_date": "2025-04-11 18:17:47 UTC",
    "updated_date": "2025-04-11 18:17:47 UTC"
  },
  {
    "arxiv_id": "2504.08896v1",
    "title": "Position: Beyond Euclidean -- Foundation Models Should Embrace Non-Euclidean Geometries",
    "authors": [
      "Neil He",
      "Jiahong Liu",
      "Buze Zhang",
      "Ngoc Bui",
      "Ali Maatouk",
      "Menglin Yang",
      "Irwin King",
      "Melanie Weber",
      "Rex Ying"
    ],
    "abstract": "In the era of foundation models and Large Language Models (LLMs), Euclidean\nspace has been the de facto geometric setting for machine learning\narchitectures. However, recent literature has demonstrated that this choice\ncomes with fundamental limitations. At a large scale, real-world data often\nexhibit inherently non-Euclidean structures, such as multi-way relationships,\nhierarchies, symmetries, and non-isotropic scaling, in a variety of domains,\nsuch as languages, vision, and the natural sciences. It is challenging to\neffectively capture these structures within the constraints of Euclidean\nspaces. This position paper argues that moving beyond Euclidean geometry is not\nmerely an optional enhancement but a necessity to maintain the scaling law for\nthe next-generation of foundation models. By adopting these geometries,\nfoundation models could more efficiently leverage the aforementioned\nstructures. Task-aware adaptability that dynamically reconfigures embeddings to\nmatch the geometry of downstream applications could further enhance efficiency\nand expressivity. Our position is supported by a series of theoretical and\nempirical investigations of prevalent foundation models.Finally, we outline a\nroadmap for integrating non-Euclidean geometries into foundation models,\nincluding strategies for building geometric foundation models via fine-tuning,\ntraining from scratch, and hybrid approaches.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08896v1",
    "published_date": "2025-04-11 18:07:33 UTC",
    "updated_date": "2025-04-11 18:07:33 UTC"
  },
  {
    "arxiv_id": "2504.08734v1",
    "title": "Towards an Understanding of Context Utilization in Code Intelligence",
    "authors": [
      "Yanlin Wang",
      "Kefeng Duan",
      "Dewu Zheng",
      "Ensheng Shi",
      "Fengji Zhang",
      "Yanli Wang",
      "Jiachi Chen",
      "Xilin Liu",
      "Yuchi Ma",
      "Hongyu Zhang",
      "Qianxiang Wang",
      "Zibin Zheng"
    ],
    "abstract": "Code intelligence is an emerging domain in software engineering, aiming to\nimprove the effectiveness and efficiency of various code-related tasks. Recent\nresearch suggests that incorporating contextual information beyond the basic\noriginal task inputs (i.e., source code) can substantially enhance model\nperformance. Such contextual signals may be obtained directly or indirectly\nfrom sources such as API documentation or intermediate representations like\nabstract syntax trees can significantly improve the effectiveness of code\nintelligence. Despite growing academic interest, there is a lack of systematic\nanalysis of context in code intelligence. To address this gap, we conduct an\nextensive literature review of 146 relevant studies published between September\n2007 and August 2024. Our investigation yields four main contributions. (1) A\nquantitative analysis of the research landscape, including publication trends,\nvenues, and the explored domains; (2) A novel taxonomy of context types used in\ncode intelligence; (3) A task-oriented analysis investigating context\nintegration strategies across diverse code intelligence tasks; (4) A critical\nevaluation of evaluation methodologies for context-aware methods. Based on\nthese findings, we identify fundamental challenges in context utilization in\ncurrent code intelligence systems and propose a research roadmap that outlines\nkey opportunities for future research.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08734v1",
    "published_date": "2025-04-11 17:59:53 UTC",
    "updated_date": "2025-04-11 17:59:53 UTC"
  },
  {
    "arxiv_id": "2504.08729v1",
    "title": "Steering CLIP's vision transformer with sparse autoencoders",
    "authors": [
      "Sonia Joseph",
      "Praneet Suresh",
      "Ethan Goldfarb",
      "Lorenz Hufe",
      "Yossi Gandelsman",
      "Robert Graham",
      "Danilo Bzdok",
      "Wojciech Samek",
      "Blake Aaron Richards"
    ],
    "abstract": "While vision models are highly capable, their internal mechanisms remain\npoorly understood -- a challenge which sparse autoencoders (SAEs) have helped\naddress in language, but which remains underexplored in vision. We address this\ngap by training SAEs on CLIP's vision transformer and uncover key differences\nbetween vision and language processing, including distinct sparsity patterns\nfor SAEs trained across layers and token types. We then provide the first\nsystematic analysis on the steerability of CLIP's vision transformer by\nintroducing metrics to quantify how precisely SAE features can be steered to\naffect the model's output. We find that 10-15\\% of neurons and features are\nsteerable, with SAEs providing thousands more steerable features than the base\nmodel. Through targeted suppression of SAE features, we then demonstrate\nimproved performance on three vision disentanglement tasks (CelebA, Waterbirds,\nand typographic attacks), finding optimal disentanglement in middle model\nlayers, and achieving state-of-the-art performance on defense against\ntypographic attacks.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 7 figures. Accepted to the CVPR 2025 Workshop on Mechanistic\n  Interpretability for Vision (MIV)",
    "pdf_url": "http://arxiv.org/pdf/2504.08729v1",
    "published_date": "2025-04-11 17:56:09 UTC",
    "updated_date": "2025-04-11 17:56:09 UTC"
  },
  {
    "arxiv_id": "2504.08727v2",
    "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images",
    "authors": [
      "Boyang Deng",
      "Songyou Peng",
      "Kyle Genova",
      "Gordon Wetzstein",
      "Noah Snavely",
      "Leonidas Guibas",
      "Thomas Funkhouser"
    ],
    "abstract": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "comment": "Project page: https://boyangdeng.com/visual-chronicles , second and\n  third listed authors have equal contributions",
    "pdf_url": "http://arxiv.org/pdf/2504.08727v2",
    "published_date": "2025-04-11 17:55:45 UTC",
    "updated_date": "2025-04-14 17:30:56 UTC"
  },
  {
    "arxiv_id": "2504.08725v2",
    "title": "DocAgent: A Multi-Agent System for Automated Code Documentation Generation",
    "authors": [
      "Dayu Yang",
      "Antoine Simoulin",
      "Xin Qian",
      "Xiaoyi Liu",
      "Yuwei Cao",
      "Zhaopu Teng",
      "Grey Yang"
    ],
    "abstract": "High-quality code documentation is crucial for software development\nespecially in the era of AI. However, generating it automatically using Large\nLanguage Models (LLMs) remains challenging, as existing approaches often\nproduce incomplete, unhelpful, or factually incorrect outputs. We introduce\nDocAgent, a novel multi-agent collaborative system using topological code\nprocessing for incremental context building. Specialized agents (Reader,\nSearcher, Writer, Verifier, Orchestrator) then collaboratively generate\ndocumentation. We also propose a multi-faceted evaluation framework assessing\nCompleteness, Helpfulness, and Truthfulness. Comprehensive experiments show\nDocAgent significantly outperforms baselines consistently. Our ablation study\nconfirms the vital role of the topological processing order. DocAgent offers a\nrobust approach for reliable code documentation generation in complex and\nproprietary repositories.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "Public Repo: https://github.com/facebookresearch/DocAgent",
    "pdf_url": "http://arxiv.org/pdf/2504.08725v2",
    "published_date": "2025-04-11 17:50:08 UTC",
    "updated_date": "2025-04-18 04:32:43 UTC"
  },
  {
    "arxiv_id": "2504.08713v3",
    "title": "ProtoECGNet: Case-Based Interpretable Deep Learning for Multi-Label ECG Classification with Contrastive Learning",
    "authors": [
      "Sahil Sethi",
      "David Chen",
      "Thomas Statchen",
      "Michael C. Burkhart",
      "Nipun Bhandari",
      "Bashar Ramadan",
      "Brett Beaulieu-Jones"
    ],
    "abstract": "Deep learning-based electrocardiogram (ECG) classification has shown\nimpressive performance but clinical adoption has been slowed by the lack of\ntransparent and faithful explanations. Post hoc methods such as saliency maps\nmay fail to reflect a model's true decision process. Prototype-based reasoning\noffers a more transparent alternative by grounding decisions in similarity to\nlearned representations of real ECG segments, enabling faithful, case-based\nexplanations. We introduce ProtoECGNet, a prototype-based deep learning model\nfor interpretable, multi-label ECG classification. ProtoECGNet employs a\nstructured, multi-branch architecture that reflects clinical interpretation\nworkflows: it integrates a 1D CNN with global prototypes for rhythm\nclassification, a 2D CNN with time-localized prototypes for morphology-based\nreasoning, and a 2D CNN with global prototypes for diffuse abnormalities. Each\nbranch is trained with a prototype loss designed for multi-label learning,\ncombining clustering, separation, diversity, and a novel contrastive loss that\nencourages appropriate separation between prototypes of unrelated classes while\nallowing clustering for frequently co-occurring diagnoses. We evaluate\nProtoECGNet on all 71 diagnostic labels from the PTB-XL dataset, demonstrating\ncompetitive performance relative to state-of-the-art black-box models while\nproviding structured, case-based explanations. To assess prototype quality, we\nconduct a structured clinician review of the final model's projected\nprototypes, finding that they are rated as representative and clear.\nProtoECGNet shows that prototype learning can be effectively scaled to complex,\nmulti-label time-series classification, offering a practical path toward\ntransparent and trustworthy deep learning models for clinical decision support.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08713v3",
    "published_date": "2025-04-11 17:23:37 UTC",
    "updated_date": "2025-05-17 20:23:28 UTC"
  },
  {
    "arxiv_id": "2504.08690v1",
    "title": "Fast-Slow-Thinking: Complex Task Solving with Large Language Models",
    "authors": [
      "Yiliu Sun",
      "Yanfang Zhang",
      "Zicheng Zhao",
      "Sheng Wan",
      "Dacheng Tao",
      "Chen Gong"
    ],
    "abstract": "Nowadays, Large Language Models (LLMs) have been gradually employed to solve\ncomplex tasks. To face the challenge, task decomposition has become an\neffective way, which proposes to divide a complex task into multiple simpler\nsubtasks and then solve them separately so that the difficulty of the original\ntask can be reduced. However, the performance of existing task decomposition\nmethods can be suboptimal when the task contains overly complex logic and\nconstraints. In this situation, the solution generated by LLMs may deviate from\nthe original purpose of the task, or contain redundant or even erroneous\ncontent. Therefore, inspired by the fact that humans possess two thinking\nsystems including fast thinking and slow thinking, this paper introduces a new\ntask decomposition method termed ``Fast-Slow-Thinking'' (FST), which stimulates\nLLMs to solve tasks through the cooperation of Fast Thinking (FT) and Slow\nThinking (ST) steps. Here FT focuses more on the general and concise aspect of\nthe task, and ST focuses more on the details of the task. In FT, LLMs are\nprompted to remove the constraints of the original task, therefore simplifying\nit to a general and concise one. In ST, we recall the constraints removed in\nFT, so that LLMs can improve the answer generated in FT to meet the\nrequirements of the original task. Therefore, our FST method enables LLMs to\nconsider a complex problem via a human-like cognition process from coarse to\nfine, the effectiveness of which has been well demonstrated by the experiments\non three types of tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "37 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08690v1",
    "published_date": "2025-04-11 16:57:36 UTC",
    "updated_date": "2025-04-11 16:57:36 UTC"
  },
  {
    "arxiv_id": "2504.08687v1",
    "title": "Voice Interaction With Conversational AI Could Facilitate Thoughtful Reflection and Substantive Revision in Writing",
    "authors": [
      "Jiho Kim",
      "Philippe Laban",
      "Xiang 'Anthony' Chen",
      "Kenneth C. Arnold"
    ],
    "abstract": "Writing well requires not only expressing ideas but also refining them\nthrough revision, a process facilitated by reflection. Prior research suggests\nthat feedback delivered through dialogues, such as those in writing center\ntutoring sessions, can help writers reflect more thoughtfully on their work\ncompared to static feedback. Recent advancements in multi-modal large language\nmodels (LLMs) now offer new possibilities for supporting interactive and\nexpressive voice-based reflection in writing. In particular, we propose that\nLLM-generated static feedback can be repurposed as conversation starters,\nallowing writers to seek clarification, request examples, and ask follow-up\nquestions, thereby fostering deeper reflection on their writing. We argue that\nvoice-based interaction can naturally facilitate this conversational exchange,\nencouraging writers' engagement with higher-order concerns, facilitating\niterative refinement of their reflections, and reduce cognitive load compared\nto text-based interactions. To investigate these effects, we propose a\nformative study exploring how text vs. voice input influence writers'\nreflection and subsequent revisions. Findings from this study will inform the\ndesign of intelligent and interactive writing tools, offering insights into how\nvoice-based interactions with LLM-powered conversational agents can support\nreflection and revision.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "H.5.2; I.2.7"
    ],
    "primary_category": "cs.HC",
    "comment": "5 pages; Accepted to Fourth Workshop on Intelligent and Interactive\n  Writing Assistants (In2Writing 2025) at NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.08687v1",
    "published_date": "2025-04-11 16:54:12 UTC",
    "updated_date": "2025-04-11 16:54:12 UTC"
  },
  {
    "arxiv_id": "2504.08686v1",
    "title": "Pobogot -- An Open-Hardware Open-Source Low Cost Robot for Swarm Robotics",
    "authors": [
      "Alessia Loi",
      "Loona Macabre",
      "Jérémy Fersula",
      "Keivan Amini",
      "Leo Cazenille",
      "Fabien Caura",
      "Alexandre Guerre",
      "Stéphane Gourichon",
      "Olivier Dauchot",
      "Nicolas Bredeche"
    ],
    "abstract": "This paper describes the Pogobot, an open-source and open-hardware platform\nspecifically designed for research involving swarm robotics. Pogobot features\nvibration-based locomotion, infrared communication, and an array of sensors in\na cost-effective package (approx. 250~euros/unit). The platform's modular\ndesign, comprehensive API, and extensible architecture facilitate the\nimplementation of swarm intelligence algorithms and distributed online\nreinforcement learning algorithms. Pogobots offer an accessible alternative to\nexisting platforms while providing advanced capabilities including directional\ncommunication between units. More than 200 Pogobots are already being used on a\ndaily basis at Sorbonne Universit\\'e and PSL to study self-organizing systems,\nprogrammable active matter, discrete reaction-diffusion-advection systems as\nwell as models of social learning and evolution.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08686v1",
    "published_date": "2025-04-11 16:47:59 UTC",
    "updated_date": "2025-04-11 16:47:59 UTC"
  },
  {
    "arxiv_id": "2504.08685v2",
    "title": "Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model",
    "authors": [
      "Team Seawead",
      "Ceyuan Yang",
      "Zhijie Lin",
      "Yang Zhao",
      "Shanchuan Lin",
      "Zhibei Ma",
      "Haoyuan Guo",
      "Hao Chen",
      "Lu Qi",
      "Sen Wang",
      "Feng Cheng",
      "Feilong Zuo",
      "Xuejiao Zeng",
      "Ziyan Yang",
      "Fangyuan Kong",
      "Meng Wei",
      "Zhiwu Qing",
      "Fei Xiao",
      "Tuyen Hoang",
      "Siyu Zhang",
      "Peihao Zhu",
      "Qi Zhao",
      "Jiangqiao Yan",
      "Liangke Gui",
      "Sheng Bi",
      "Jiashi Li",
      "Yuxi Ren",
      "Rui Wang",
      "Huixia Li",
      "Xuefeng Xiao",
      "Shu Liu",
      "Feng Ling",
      "Heng Zhang",
      "Houmin Wei",
      "Huafeng Kuang",
      "Jerry Duncan",
      "Junda Zhang",
      "Junru Zheng",
      "Li Sun",
      "Manlin Zhang",
      "Renfei Sun",
      "Xiaobin Zhuang",
      "Xiaojie Li",
      "Xin Xia",
      "Xuyan Chi",
      "Yanghua Peng",
      "Yuping Wang",
      "Yuxuan Wang",
      "Zhongkai Zhao",
      "Zhuo Chen",
      "Zuquan Song",
      "Zhenheng Yang",
      "Jiashi Feng",
      "Jianchao Yang",
      "Lu Jiang"
    ],
    "abstract": "This technical report presents a cost-efficient strategy for training a video\ngeneration foundation model. We present a mid-sized research model with\napproximately 7 billion parameters (7B) called Seaweed-7B trained from scratch\nusing 665,000 H100 GPU hours. Despite being trained with moderate computational\nresources, Seaweed-7B demonstrates highly competitive performance compared to\ncontemporary video generation models of much larger size. Design choices are\nespecially crucial in a resource-constrained setting. This technical report\nhighlights the key design decisions that enhance the performance of the\nmedium-sized diffusion model. Empirically, we make two observations: (1)\nSeaweed-7B achieves performance comparable to, or even surpasses, larger models\ntrained on substantially greater GPU resources, and (2) our model, which\nexhibits strong generalization ability, can be effectively adapted across a\nwide range of downstream applications either by lightweight fine-tuning or\ncontinue training. See the project page at https://seaweed.video/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical report (some typos fixed)",
    "pdf_url": "http://arxiv.org/pdf/2504.08685v2",
    "published_date": "2025-04-11 16:46:20 UTC",
    "updated_date": "2025-05-05 03:31:30 UTC"
  },
  {
    "arxiv_id": "2505.14689v1",
    "title": "Follow the STARs: Dynamic $ω$-Regular Shielding of Learned Policies",
    "authors": [
      "Ashwani Anand",
      "Satya Prakash Nayak",
      "Ritam Raha",
      "Anne-Kathrin Schmuck"
    ],
    "abstract": "This paper presents a novel dynamic post-shielding framework that enforces\nthe full class of $\\omega$-regular correctness properties over pre-computed\nprobabilistic policies. This constitutes a paradigm shift from the predominant\nsetting of safety-shielding -- i.e., ensuring that nothing bad ever happens --\nto a shielding process that additionally enforces liveness -- i.e., ensures\nthat something good eventually happens. At the core, our method uses\nStrategy-Template-based Adaptive Runtime Shields (STARs), which leverage\npermissive strategy templates to enable post-shielding with minimal\ninterference. As its main feature, STARs introduce a mechanism to dynamically\ncontrol interference, allowing a tunable enforcement parameter to balance\nformal obligations and task-specific behavior at runtime. This allows to\ntrigger more aggressive enforcement when needed, while allowing for optimized\npolicy choices otherwise. In addition, STARs support runtime adaptation to\nchanging specifications or actuator failures, making them especially suited for\ncyber-physical applications. We evaluate STARs on a mobile robot benchmark to\ndemonstrate their controllable interference when enforcing (incrementally\nupdated) $\\omega$-regular correctness properties over learned probabilistic\npolicies.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2505.14689v1",
    "published_date": "2025-04-11 16:37:24 UTC",
    "updated_date": "2025-04-11 16:37:24 UTC"
  },
  {
    "arxiv_id": "2504.08672v1",
    "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework For Advanced Reasoning",
    "authors": [
      "Fangzhi Xu",
      "Hang Yan",
      "Chang Ma",
      "Haiteng Zhao",
      "Qiushi Sun",
      "Kanzhi Cheng",
      "Junxian He",
      "Jun Liu",
      "Zhiyong Wu"
    ],
    "abstract": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "14 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08672v1",
    "published_date": "2025-04-11 16:26:23 UTC",
    "updated_date": "2025-04-11 16:26:23 UTC"
  },
  {
    "arxiv_id": "2504.08670v2",
    "title": "Designing Child-Friendly AI Interfaces: Six Developmentally-Appropriate Design Insights from Analysing Disney Animation",
    "authors": [
      "Nomisha Kurian"
    ],
    "abstract": "To build AI interfaces that children can intuitively understand and use,\ndesigners need a design grammar that truly serves children's developmental\nneeds. This paper bridges Artificial Intelligence design for children -- an\nemerging field still defining its best practices -- and children's animation, a\nwell-established field with decades of experience in engaging young viewers\nthrough emotionally resonant, cognitively accessible storytelling. Pairing\nPiagetian developmental theory with design pattern extraction from 52 works of\nDisney animation, the paper presents six design insights transferable to\nchild-centred AI interface design: (1) emotional expressiveness and visual\nclarity, (2) musical and auditory scaffolding, (3) audiovisual synchrony for\nemotional comfort, (4) sidekick-style personas, (5) support for symbolic play\nand imaginative exploration, and (6) predictable and scaffolded interaction\nstructures. These strategies -- long refined in Disney animation -- function as\nmultimodal scaffolds for attention, understanding, and emotional attunement,\nthereby forming a structured design grammar familiar to children and\ntransferable to AI interface design. By reframing cinematic storytelling as\ndesign logic for AI, the paper offers heuristics for crafting intuitive AI\ninterfaces that align with children's cognitive stages and emotional needs. The\nwork contributes to design theory by showing how sensory, affective and\nnarrative techniques can inform developmentally attuned AI design for children.\nFuture directions include empirical testing, cultural adaptation, and\nparticipatory co-design.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "30 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.08670v2",
    "published_date": "2025-04-11 16:23:37 UTC",
    "updated_date": "2025-04-15 12:07:00 UTC"
  },
  {
    "arxiv_id": "2504.08666v1",
    "title": "Variability-Driven User-Story Generation using LLM and Triadic Concept Analysis",
    "authors": [
      "Alexandre Bazin",
      "Alain Gutierrez",
      "Marianne Huchard",
      "Pierre Martin",
      "Yulin",
      "Zhang"
    ],
    "abstract": "A widely used Agile practice for requirements is to produce a set of user\nstories (also called ``agile product backlog''), which roughly includes a list\nof pairs (role, feature), where the role handles the feature for a certain\npurpose. In the context of Software Product Lines, the requirements for a\nfamily of similar systems is thus a family of user-story sets, one per system,\nleading to a 3-dimensional dataset composed of sets of triples (system, role,\nfeature). In this paper, we combine Triadic Concept Analysis (TCA) and Large\nLanguage Model (LLM) prompting to suggest the user-story set required to\ndevelop a new system relying on the variability logic of an existing system\nfamily. This process consists in 1) computing 3-dimensional variability\nexpressed as a set of TCA implications, 2) providing the designer with\nintelligible design options, 3) capturing the designer's selection of options,\n4) proposing a first user-story set corresponding to this selection, 5)\nconsolidating its validity according to the implications identified in step 1,\nwhile completing it if necessary, and 6) leveraging LLM to have a more\ncomprehensive website. This process is evaluated with a dataset comprising the\nuser-story sets of 67 similar-purpose websites.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "20th International Conference on Evaluation of Novel Approaches to\n  Software Engineering April 4-6, 2025, in Porto, Portugal",
    "pdf_url": "http://arxiv.org/pdf/2504.08666v1",
    "published_date": "2025-04-11 16:15:27 UTC",
    "updated_date": "2025-04-11 16:15:27 UTC"
  },
  {
    "arxiv_id": "2504.08645v1",
    "title": "Title block detection and information extraction for enhanced building drawings search",
    "authors": [
      "Alessio Lombardi",
      "Li Duan",
      "Ahmed Elnagar",
      "Ahmed Zaalouk",
      "Khalid Ismail",
      "Edlira Vakaj"
    ],
    "abstract": "The architecture, engineering, and construction (AEC) industry still heavily\nrelies on information stored in drawings for building construction,\nmaintenance, compliance and error checks. However, information extraction (IE)\nfrom building drawings is often time-consuming and costly, especially when\ndealing with historical buildings. Drawing search can be simplified by\nleveraging the information stored in the title block portion of the drawing,\nwhich can be seen as drawing metadata. However, title block IE can be complex\nespecially when dealing with historical drawings which do not follow existing\nstandards for uniformity. This work performs a comparison of existing methods\nfor this kind of IE task, and then proposes a novel title block detection and\nIE pipeline which outperforms existing methods, in particular when dealing with\ncomplex, noisy historical drawings. The pipeline is obtained by combining a\nlightweight Convolutional Neural Network and GPT-4o, the proposed inference\npipeline detects building engineering title blocks with high accuracy, and then\nextract structured drawing metadata from the title blocks, which can be used\nfor drawing search, filtering and grouping. The work demonstrates high accuracy\nand efficiency in IE for both vector (CAD) and hand-drawn (historical)\ndrawings. A user interface (UI) that leverages the extracted metadata for\ndrawing search is established and deployed on real projects, which demonstrates\nsignificant time savings. Additionally, an extensible domain-expert-annotated\ndataset for title block detection is developed, via an efficient AEC-friendly\nannotation workflow that lays the foundation for future work.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "8 pages, 8 figures, 1 table. Accepted for publication in the 2025\n  European Conference on Computing in Construction (EC3,\n  https://ec-3.org/conference2025/)",
    "pdf_url": "http://arxiv.org/pdf/2504.08645v1",
    "published_date": "2025-04-11 15:45:17 UTC",
    "updated_date": "2025-04-11 15:45:17 UTC"
  },
  {
    "arxiv_id": "2504.08641v1",
    "title": "Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization",
    "authors": [
      "Jialu Li",
      "Shoubin Yu",
      "Han Lin",
      "Jaemin Cho",
      "Jaehong Yoon",
      "Mohit Bansal"
    ],
    "abstract": "Recent advancements in text-to-video (T2V) diffusion models have\nsignificantly enhanced the visual quality of the generated videos. However,\neven recent T2V models find it challenging to follow text descriptions\naccurately, especially when the prompt requires accurate control of spatial\nlayouts or object trajectories. A recent line of research uses layout guidance\nfor T2V models that require fine-tuning or iterative manipulation of the\nattention map during inference time. This significantly increases the memory\nrequirement, making it difficult to adopt a large T2V model as a backbone. To\naddress this, we introduce Video-MSG, a training-free Guidance method for T2V\ngeneration based on Multimodal planning and Structured noise initialization.\nVideo-MSG consists of three steps, where in the first two steps, Video-MSG\ncreates Video Sketch, a fine-grained spatio-temporal plan for the final video,\nspecifying background, foreground, and object trajectories, in the form of\ndraft video frames. In the last step, Video-MSG guides a downstream T2V\ndiffusion model with Video Sketch through noise inversion and denoising.\nNotably, Video-MSG does not need fine-tuning or attention manipulation with\nadditional memory during inference time, making it easier to adopt large T2V\nmodels. Video-MSG demonstrates its effectiveness in enhancing text alignment\nwith multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V\ngeneration benchmarks (T2VCompBench and VBench). We provide comprehensive\nablation studies about noise inversion ratio, different background generators,\nbackground object detection, and foreground object segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Website: https://video-msg.github.io; The first three authors\n  contributed equally",
    "pdf_url": "http://arxiv.org/pdf/2504.08641v1",
    "published_date": "2025-04-11 15:41:43 UTC",
    "updated_date": "2025-04-11 15:41:43 UTC"
  },
  {
    "arxiv_id": "2504.08640v1",
    "title": "Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM agents",
    "authors": [
      "Alessio Buscemi",
      "Daniele Proverbio",
      "Paolo Bova",
      "Nataliya Balabanova",
      "Adeela Bashir",
      "Theodor Cimpeanu",
      "Henrique Correia da Fonseca",
      "Manh Hong Duong",
      "Elias Fernandez Domingos",
      "Antonio M. Fernandes",
      "Marcus Krellner",
      "Ndidi Bianca Ogbo",
      "Simon T. Powers",
      "Fernando P. Santos",
      "Zia Ush Shamszaman",
      "Zhao Song",
      "Alessandro Di Stefano",
      "The Anh Han"
    ],
    "abstract": "There is general agreement that fostering trust and cooperation within the AI\ndevelopment ecosystem is essential to promote the adoption of trustworthy AI\nsystems. By embedding Large Language Model (LLM) agents within an evolutionary\ngame-theoretic framework, this paper investigates the complex interplay between\nAI developers, regulators and users, modelling their strategic choices under\ndifferent regulatory scenarios. Evolutionary game theory (EGT) is used to\nquantitatively model the dilemmas faced by each actor, and LLMs provide\nadditional degrees of complexity and nuances and enable repeated games and\nincorporation of personality traits. Our research identifies emerging\nbehaviours of strategic AI agents, which tend to adopt more \"pessimistic\" (not\ntrusting and defective) stances than pure game-theoretic agents. We observe\nthat, in case of full trust by users, incentives are effective to promote\neffective regulation; however, conditional trust may deteriorate the \"social\npact\". Establishing a virtuous feedback between users' trust and regulators'\nreputation thus appears to be key to nudge developers towards creating safe AI.\nHowever, the level at which this trust emerges may depend on the specific LLM\nused for testing. Our results thus provide guidance for AI regulation systems,\nand help predict the outcome of strategic LLM agents, should they be used to\naid regulation itself.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.GT",
      "nlin.CD"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08640v1",
    "published_date": "2025-04-11 15:41:21 UTC",
    "updated_date": "2025-04-11 15:41:21 UTC"
  },
  {
    "arxiv_id": "2504.08632v1",
    "title": "Deep Learning Methods for Detecting Thermal Runaway Events in Battery Production Lines",
    "authors": [
      "Athanasios Athanasopoulos",
      "Matúš Mihalák",
      "Marcin Pietrasik"
    ],
    "abstract": "One of the key safety considerations of battery manufacturing is thermal\nrunaway, the uncontrolled increase in temperature which can lead to fires,\nexplosions, and emissions of toxic gasses. As such, development of automated\nsystems capable of detecting such events is of considerable importance in both\nacademic and industrial contexts. In this work, we investigate the use of deep\nlearning for detecting thermal runaway in the battery production line of VDL\nNedcar, a Dutch automobile manufacturer. Specifically, we collect data from the\nproduction line to represent both baseline (non thermal runaway) and thermal\nrunaway conditions. Thermal runaway was simulated through the use of external\nheat and smoke sources. The data consisted of both optical and thermal images\nwhich were then preprocessed and fused before serving as input to our models.\nIn this regard, we evaluated three deep-learning models widely used in computer\nvision including shallow convolutional neural networks, residual neural\nnetworks, and vision transformers on two performance metrics. Furthermore, we\nevaluated these models using explainability methods to gain insight into their\nability to capture the relevant feature information from their inputs. The\nobtained results indicate that the use of deep learning is a viable approach to\nthermal runaway detection in battery production lines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08632v1",
    "published_date": "2025-04-11 15:35:50 UTC",
    "updated_date": "2025-04-11 15:35:50 UTC"
  },
  {
    "arxiv_id": "2504.08626v2",
    "title": "Task-conditioned Ensemble of Expert Models for Continuous Learning",
    "authors": [
      "Renu Sharma",
      "Debasmita Pal",
      "Arun Ross"
    ],
    "abstract": "One of the major challenges in machine learning is maintaining the accuracy\nof the deployed model (e.g., a classifier) in a non-stationary environment. The\nnon-stationary environment results in distribution shifts and, consequently, a\ndegradation in accuracy. Continuous learning of the deployed model with new\ndata could be one remedy. However, the question arises as to how we should\nupdate the model with new training data so that it retains its accuracy on the\nold data while adapting to the new data. In this work, we propose a\ntask-conditioned ensemble of models to maintain the performance of the existing\nmodel. The method involves an ensemble of expert models based on task\nmembership information. The in-domain models-based on the local outlier concept\n(different from the expert models) provide task membership information\ndynamically at run-time to each probe sample. To evaluate the proposed method,\nwe experiment with three setups: the first represents distribution shift\nbetween tasks (LivDet-Iris-2017), the second represents distribution shift both\nbetween and within tasks (LivDet-Iris-2020), and the third represents disjoint\ndistribution between tasks (Split MNIST). The experiments highlight the\nbenefits of the proposed method. The source code is available at\nhttps://github.com/iPRoBe-lab/Continuous_Learning_FE_DM.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW), Nashville, USA, June 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.08626v2",
    "published_date": "2025-04-11 15:27:29 UTC",
    "updated_date": "2025-04-14 20:37:11 UTC"
  },
  {
    "arxiv_id": "2504.08623v2",
    "title": "Enterprise-Grade Security for the Model Context Protocol (MCP): Frameworks and Mitigation Strategies",
    "authors": [
      "Vineeth Sai Narajala",
      "Idan Habler"
    ],
    "abstract": "The Model Context Protocol (MCP), introduced by Anthropic, provides a\nstandardized framework for artificial intelligence (AI) systems to interact\nwith external data sources and tools in real-time. While MCP offers significant\nadvantages for AI integration and capability extension, it introduces novel\nsecurity challenges that demand rigorous analysis and mitigation. This paper\nbuilds upon foundational research into MCP architecture and preliminary\nsecurity assessments to deliver enterprise-grade mitigation frameworks and\ndetailed technical implementation strategies. Through systematic threat\nmodeling and analysis of MCP implementations and analysis of potential attack\nvectors, including sophisticated threats like tool poisoning, we present\nactionable security patterns tailored for MCP implementers and adopters. The\nprimary contribution of this research lies in translating theoretical security\nconcerns into a practical, implementable framework with actionable controls,\nthereby providing essential guidance for the secure enterprise adoption and\ngovernance of integrated AI systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "11 pages, 2 figures, 1 table, typos corrected, references added",
    "pdf_url": "http://arxiv.org/pdf/2504.08623v2",
    "published_date": "2025-04-11 15:25:58 UTC",
    "updated_date": "2025-05-02 18:26:39 UTC"
  },
  {
    "arxiv_id": "2504.08609v1",
    "title": "A Survey of Machine Learning Models and Datasets for the Multi-label Classification of Textual Hate Speech in English",
    "authors": [
      "Julian Bäumler",
      "Louis Blöcher",
      "Lars-Joel Frey",
      "Xian Chen",
      "Markus Bayer",
      "Christian Reuter"
    ],
    "abstract": "The dissemination of online hate speech can have serious negative\nconsequences for individuals, online communities, and entire societies. This\nand the large volume of hateful online content prompted both practitioners',\ni.e., in content moderation or law enforcement, and researchers' interest in\nmachine learning models to automatically classify instances of hate speech.\nWhereas most scientific works address hate speech classification as a binary\ntask, practice often requires a differentiation into sub-types, e.g., according\nto target, severity, or legality, which may overlap for individual content.\nHence, researchers created datasets and machine learning models that approach\nhate speech classification in textual data as a multi-label problem. This work\npresents the first systematic and comprehensive survey of scientific literature\non this emerging research landscape in English (N=46). We contribute with a\nconcise overview of 28 datasets suited for training multi-label classification\nmodels that reveals significant heterogeneity regarding label-set, size,\nmeta-concept, annotation process, and inter-annotator agreement. Our analysis\nof 24 publications proposing suitable classification models further establishes\ninconsistency in evaluation and a preference for architectures based on\nBidirectional Encoder Representation from Transformers (BERT) and Recurrent\nNeural Networks (RNNs). We identify imbalanced training data, reliance on\ncrowdsourcing platforms, small and sparse datasets, and missing methodological\nalignment as critical open issues and formulate ten recommendations for\nresearch.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "35 pages, 4 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.08609v1",
    "published_date": "2025-04-11 15:16:31 UTC",
    "updated_date": "2025-04-11 15:16:31 UTC"
  },
  {
    "arxiv_id": "2504.08604v1",
    "title": "Neural Fidelity Calibration for Informative Sim-to-Real Adaptation",
    "authors": [
      "Youwei Yu",
      "Lantao Liu"
    ],
    "abstract": "Deep reinforcement learning can seamlessly transfer agile locomotion and\nnavigation skills from the simulator to real world. However, bridging the\nsim-to-real gap with domain randomization or adversarial methods often demands\nexpert physics knowledge to ensure policy robustness. Even so, cutting-edge\nsimulators may fall short of capturing every real-world detail, and the\nreconstructed environment may introduce errors due to various perception\nuncertainties. To address these challenges, we propose Neural Fidelity\nCalibration (NFC), a novel framework that employs conditional score-based\ndiffusion models to calibrate simulator physical coefficients and residual\nfidelity domains online during robot execution. Specifically, the residual\nfidelity reflects the simulation model shift relative to the real-world\ndynamics and captures the uncertainty of the perceived environment, enabling us\nto sample realistic environments under the inferred distribution for policy\nfine-tuning. Our framework is informative and adaptive in three key ways: (a)\nwe fine-tune the pretrained policy only under anomalous scenarios, (b) we build\nsequential NFC online with the pretrained NFC's proposal prior, reducing the\ndiffusion model's training burden, and (c) when NFC uncertainty is high and may\ndegrade policy improvement, we leverage optimistic exploration to enable\nhallucinated policy optimization. Our framework achieves superior simulator\ncalibration precision compared to state-of-the-art methods across diverse\nrobots with high-dimensional parametric spaces. We study the critical\ncontribution of residual fidelity to policy improvement in simulation and\nreal-world experiments. Notably, our approach demonstrates robust robot\nnavigation under challenging real-world conditions, such as a broken wheel axle\non snowy surfaces.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08604v1",
    "published_date": "2025-04-11 15:12:12 UTC",
    "updated_date": "2025-04-11 15:12:12 UTC"
  },
  {
    "arxiv_id": "2504.08603v2",
    "title": "FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot Exploration in Any Environment",
    "authors": [
      "Sebastián Barbas Laina",
      "Simon Boche",
      "Sotiris Papatheodorou",
      "Simon Schaefer",
      "Jaehyung Jung",
      "Stefan Leutenegger"
    ],
    "abstract": "Geometrically accurate and semantically expressive map representations have\nproven invaluable to facilitate robust and safe mobile robot navigation and\ntask planning. Nevertheless, real-time, open-vocabulary semantic understanding\nof large-scale unknown environments is still an open problem. In this paper we\npresent FindAnything, an open-world mapping and exploration framework that\nincorporates vision-language information into dense volumetric submaps. Thanks\nto the use of vision-language features, FindAnything bridges the gap between\npure geometric and open-vocabulary semantic information for a higher level of\nunderstanding while allowing to explore any environment without the help of any\nexternal source of ground-truth pose information. We represent the environment\nas a series of volumetric occupancy submaps, resulting in a robust and accurate\nmap representation that deforms upon pose updates when the underlying SLAM\nsystem corrects its drift, allowing for a locally consistent representation\nbetween submaps. Pixel-wise vision-language features are aggregated from\nefficient SAM (eSAM)-generated segments, which are in turn integrated into\nobject-centric volumetric submaps, providing a mapping from open-vocabulary\nqueries to 3D geometry that is scalable also in terms of memory usage. The\nopen-vocabulary map representation of FindAnything achieves state-of-the-art\nsemantic accuracy in closed-set evaluations on the Replica dataset. This level\nof scene understanding allows a robot to explore environments based on objects\nor areas of interest selected via natural language queries. Our system is the\nfirst of its kind to be deployed on resource-constrained devices, such as MAVs,\nleveraging vision-language information for real-world robotic tasks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "11 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08603v2",
    "published_date": "2025-04-11 15:12:05 UTC",
    "updated_date": "2025-05-08 08:56:29 UTC"
  },
  {
    "arxiv_id": "2504.08602v1",
    "title": "On Background Bias of Post-Hoc Concept Embeddings in Computer Vision DNNs",
    "authors": [
      "Gesina Schwalbe",
      "Georgii Mikriukov",
      "Edgar Heinert",
      "Stavros Gerolymatos",
      "Mert Keser",
      "Alois Knoll",
      "Matthias Rottmann",
      "Annika Mütze"
    ],
    "abstract": "The thriving research field of concept-based explainable artificial\nintelligence (C-XAI) investigates how human-interpretable semantic concepts\nembed in the latent spaces of deep neural networks (DNNs). Post-hoc approaches\ntherein use a set of examples to specify a concept, and determine its\nembeddings in DNN latent space using data driven techniques. This proved useful\nto uncover biases between different target (foreground or concept) classes.\nHowever, given that the background is mostly uncontrolled during training, an\nimportant question has been left unattended so far: Are/to what extent are\nstate-of-the-art, data-driven post-hoc C-XAI approaches themselves prone to\nbiases with respect to their backgrounds? E.g., wild animals mostly occur\nagainst vegetation backgrounds, and they seldom appear on roads. Even simple\nand robust C-XAI methods might abuse this shortcut for enhanced performance. A\ndangerous performance degradation of the concept-corner cases of animals on the\nroad could thus remain undiscovered. This work validates and thoroughly\nconfirms that established Net2Vec-based concept segmentation techniques\nfrequently capture background biases, including alarming ones, such as\nunderperformance on road scenes. For the analysis, we compare 3 established\ntechniques from the domain of background randomization on >50 concepts from 2\ndatasets, and 7 diverse DNN architectures. Our results indicate that even\nlow-cost setups can provide both valuable insight and improved background\nrobustness.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "camera-ready version for 3rd World Conference on eXplainable\n  Artificial Intelligence; 5 figures, 6 tables; code available at:\n  https://github.com/gesina/bg_randomized_loce",
    "pdf_url": "http://arxiv.org/pdf/2504.08602v1",
    "published_date": "2025-04-11 15:10:41 UTC",
    "updated_date": "2025-04-11 15:10:41 UTC"
  },
  {
    "arxiv_id": "2504.08596v1",
    "title": "MedHal: An Evaluation Dataset for Medical Hallucination Detection",
    "authors": [
      "Gaya Mehenni",
      "Amal Zouaq"
    ],
    "abstract": "We present MedHal, a novel large-scale dataset specifically designed to\nevaluate if models can detect hallucinations in medical texts. Current\nhallucination detection methods face significant limitations when applied to\nspecialized domains like medicine, where they can have disastrous consequences.\nExisting medical datasets are either too small, containing only a few hundred\nsamples, or focus on a single task like Question Answering or Natural Language\nInference. MedHal addresses these gaps by: (1) incorporating diverse medical\ntext sources and tasks; (2) providing a substantial volume of annotated samples\nsuitable for training medical hallucination detection models; and (3) including\nexplanations for factual inconsistencies to guide model learning. We\ndemonstrate MedHal's utility by training and evaluating a baseline medical\nhallucination detection model, showing improvements over general-purpose\nhallucination detection approaches. This resource enables more efficient\nevaluation of medical text generation systems while reducing reliance on costly\nexpert review, potentially accelerating the development of medical AI research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08596v1",
    "published_date": "2025-04-11 14:55:15 UTC",
    "updated_date": "2025-04-11 14:55:15 UTC"
  },
  {
    "arxiv_id": "2504.08593v2",
    "title": "Hands-On: Segmenting Individual Signs from Continuous Sequences",
    "authors": [
      "Low Jian He",
      "Harry Walsh",
      "Ozge Mercanoglu Sincan",
      "Richard Bowden"
    ],
    "abstract": "This work tackles the challenge of continuous sign language segmentation, a\nkey task with huge implications for sign language translation and data\nannotation. We propose a transformer-based architecture that models the\ntemporal dynamics of signing and frames segmentation as a sequence labeling\nproblem using the Begin-In-Out (BIO) tagging scheme. Our method leverages the\nHaMeR hand features, and is complemented with 3D Angles. Extensive experiments\nshow that our model achieves state-of-the-art results on the DGS Corpus, while\nour features surpass prior benchmarks on BSLCorpus.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted in the 19th IEEE International Conference on Automatic Face\n  and Gesture Recognition",
    "pdf_url": "http://arxiv.org/pdf/2504.08593v2",
    "published_date": "2025-04-11 14:52:59 UTC",
    "updated_date": "2025-04-14 08:07:48 UTC"
  },
  {
    "arxiv_id": "2504.08585v1",
    "title": "Ready, Bid, Go! On-Demand Delivery Using Fleets of Drones with Unknown, Heterogeneous Energy Storage Constraints",
    "authors": [
      "Mohamed S. Talamali",
      "Genki Miyauchi",
      "Thomas Watteyne",
      "Micael S. Couceiro",
      "Roderich Gross"
    ],
    "abstract": "Unmanned Aerial Vehicles (UAVs) are expected to transform logistics, reducing\ndelivery time, costs, and emissions. This study addresses an on-demand delivery\n, in which fleets of UAVs are deployed to fulfil orders that arrive\nstochastically. Unlike previous work, it considers UAVs with heterogeneous,\nunknown energy storage capacities and assumes no knowledge of the energy\nconsumption models. We propose a decentralised deployment strategy that\ncombines auction-based task allocation with online learning. Each UAV\nindependently decides whether to bid for orders based on its energy storage\ncharge level, the parcel mass, and delivery distance. Over time, it refines its\npolicy to bid only for orders within its capability. Simulations using\nrealistic UAV energy models reveal that, counter-intuitively, assigning orders\nto the least confident bidders reduces delivery times and increases the number\nof successfully fulfilled orders. This strategy is shown to outperform\nthreshold-based methods which require UAVs to exceed specific charge levels at\ndeployment. We propose a variant of the strategy which uses learned policies\nfor forecasting. This enables UAVs with insufficient charge levels to commit to\nfulfilling orders at specific future times, helping to prioritise early orders.\nOur work provides new insights into long-term deployment of UAV swarms,\nhighlighting the advantages of decentralised energy-aware decision-making\ncoupled with online learning in real-world dynamic environments.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "The 24th International Conference on Autonomous Agents and Multiagent\n  Systems (AAMAS 2025)",
    "pdf_url": "http://arxiv.org/pdf/2504.08585v1",
    "published_date": "2025-04-11 14:39:25 UTC",
    "updated_date": "2025-04-11 14:39:25 UTC"
  },
  {
    "arxiv_id": "2504.08584v1",
    "title": "Boosting multi-demographic federated learning for chest x-ray analysis using general-purpose self-supervised representations",
    "authors": [
      "Mahshad Lotfinia",
      "Arash Tayebiarasteh",
      "Samaneh Samiei",
      "Mehdi Joodaki",
      "Soroosh Tayebi Arasteh"
    ],
    "abstract": "Reliable artificial intelligence (AI) models for medical image analysis often\ndepend on large and diverse labeled datasets. Federated learning (FL) offers a\ndecentralized and privacy-preserving approach to training but struggles in\nhighly non-independent and identically distributed (non-IID) settings, where\ninstitutions with more representative data may experience degraded performance.\nMoreover, existing large-scale FL studies have been limited to adult datasets,\nneglecting the unique challenges posed by pediatric data, which introduces\nadditional non-IID variability. To address these limitations, we analyzed\nn=398,523 adult chest radiographs from diverse institutions across multiple\ncountries and n=9,125 pediatric images, leveraging transfer learning from\ngeneral-purpose self-supervised image representations to classify pneumonia and\ncases with no abnormality. Using state-of-the-art vision transformers, we found\nthat FL improved performance only for smaller adult datasets (P<0.001) but\ndegraded performance for larger datasets (P<0.064) and pediatric cases\n(P=0.242). However, equipping FL with self-supervised weights significantly\nenhanced outcomes across pediatric cases (P=0.031) and most adult datasets\n(P<0.008), except the largest dataset (P=0.052). These findings underscore the\npotential of easily deployable general-purpose self-supervised image\nrepresentations to address non-IID challenges in clinical FL applications and\nhighlight their promise for enhancing patient outcomes and advancing pediatric\nhealthcare, where data scarcity and variability remain persistent obstacles.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08584v1",
    "published_date": "2025-04-11 14:38:09 UTC",
    "updated_date": "2025-04-11 14:38:09 UTC"
  },
  {
    "arxiv_id": "2504.08553v1",
    "title": "Uncovering the Structure of Explanation Quality with Spectral Analysis",
    "authors": [
      "Johannes Maeß",
      "Grégoire Montavon",
      "Shinichi Nakajima",
      "Klaus-Robert Müller",
      "Thomas Schnake"
    ],
    "abstract": "As machine learning models are increasingly considered for high-stakes\ndomains, effective explanation methods are crucial to ensure that their\nprediction strategies are transparent to the user. Over the years, numerous\nmetrics have been proposed to assess quality of explanations. However, their\npractical applicability remains unclear, in particular due to a limited\nunderstanding of which specific aspects each metric rewards. In this paper we\npropose a new framework based on spectral analysis of explanation outcomes to\nsystematically capture the multifaceted properties of different explanation\ntechniques. Our analysis uncovers two distinct factors of explanation\nquality-stability and target sensitivity-that can be directly observed through\nspectral decomposition. Experiments on both MNIST and ImageNet show that\npopular evaluation techniques (e.g., pixel-flipping, entropy) partially capture\nthe trade-offs between these factors. Overall, our framework provides a\nfoundational basis for understanding explanation quality, guiding the\ndevelopment of more reliable techniques for evaluating explanations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 5 figures, Accepted at XAI World Conference 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.08553v1",
    "published_date": "2025-04-11 14:03:23 UTC",
    "updated_date": "2025-04-11 14:03:23 UTC"
  },
  {
    "arxiv_id": "2504.08552v1",
    "title": "Towards an Evaluation Framework for Explainable Artificial Intelligence Systems for Health and Well-being",
    "authors": [
      "Esperança Amengual-Alcover",
      "Antoni Jaume-i-Capó",
      "Miquel Miró-Nicolau",
      "Gabriel Moyà-Alcover",
      "Antonia Paniza-Fullana"
    ],
    "abstract": "The integration of Artificial Intelligence in the development of computer\nsystems presents a new challenge: make intelligent systems explainable to\nhumans. This is especially vital in the field of health and well-being, where\ntransparency in decision support systems enables healthcare professionals to\nunderstand and trust automated decisions and predictions. To address this need,\ntools are required to guide the development of explainable AI systems. In this\npaper, we introduce an evaluation framework designed to support the development\nof explainable AI systems for health and well-being. Additionally, we present a\ncase study that illustrates the application of the framework in practice. We\nbelieve that our framework can serve as a valuable tool not only for developing\nexplainable AI systems in healthcare but also for any AI system that has a\nsignificant impact on individuals.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08552v1",
    "published_date": "2025-04-11 14:02:54 UTC",
    "updated_date": "2025-04-11 14:02:54 UTC"
  },
  {
    "arxiv_id": "2504.08550v1",
    "title": "Proxy-Anchor and EVT-Driven Continual Learning Method for Generalized Category Discovery",
    "authors": [
      "Alireza Fathalizadeh",
      "Roozbeh Razavi-Far"
    ],
    "abstract": "Continual generalized category discovery has been introduced and studied in\nthe literature as a method that aims to continuously discover and learn novel\ncategories in incoming data batches while avoiding catastrophic forgetting of\npreviously learned categories. A key component in addressing this challenge is\nthe model's ability to separate novel samples, where Extreme Value Theory (EVT)\nhas been effectively employed. In this work, we propose a novel method that\nintegrates EVT with proxy anchors to define boundaries around proxies using a\nprobability of inclusion function, enabling the rejection of unknown samples.\nAdditionally, we introduce a novel EVT-based loss function to enhance the\nlearned representation, achieving superior performance compared to other\ndeep-metric learning methods in similar settings. Using the derived probability\nfunctions, novel samples are effectively separated from previously known\ncategories. However, category discovery within these novel samples can\nsometimes overestimate the number of new categories. To mitigate this issue, we\npropose a novel EVT-based approach to reduce the model size and discard\nredundant proxies. We also incorporate experience replay and knowledge\ndistillation mechanisms during the continual learning stage to prevent\ncatastrophic forgetting. Experimental results demonstrate that our proposed\napproach outperforms state-of-the-art methods in continual generalized category\ndiscovery scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08550v1",
    "published_date": "2025-04-11 14:01:49 UTC",
    "updated_date": "2025-04-11 14:01:49 UTC"
  },
  {
    "arxiv_id": "2504.08541v2",
    "title": "Digital Twin Catalog: A Large-Scale Photorealistic 3D Object Digital Twin Dataset",
    "authors": [
      "Zhao Dong",
      "Ka Chen",
      "Zhaoyang Lv",
      "Hong-Xing Yu",
      "Yunzhi Zhang",
      "Cheng Zhang",
      "Yufeng Zhu",
      "Stephen Tian",
      "Zhengqin Li",
      "Geordie Moffatt",
      "Sean Christofferson",
      "James Fort",
      "Xiaqing Pan",
      "Mingfei Yan",
      "Jiajun Wu",
      "Carl Yuheng Ren",
      "Richard Newcombe"
    ],
    "abstract": "We introduce the Digital Twin Catalog (DTC), a new large-scale photorealistic\n3D object digital twin dataset. A digital twin of a 3D object is a highly\ndetailed, virtually indistinguishable representation of a physical object,\naccurately capturing its shape, appearance, physical properties, and other\nattributes. Recent advances in neural-based 3D reconstruction and inverse\nrendering have significantly improved the quality of 3D object reconstruction.\nDespite these advancements, there remains a lack of a large-scale, digital\ntwin-quality real-world dataset and benchmark that can quantitatively assess\nand compare the performance of different reconstruction methods, as well as\nimprove reconstruction quality through training or fine-tuning. Moreover, to\ndemocratize 3D digital twin creation, it is essential to integrate creation\ntechniques with next-generation egocentric computing platforms, such as AR\nglasses. Currently, there is no dataset available to evaluate 3D object\nreconstruction using egocentric captured images. To address these gaps, the DTC\ndataset features 2,000 scanned digital twin-quality 3D objects, along with\nimage sequences captured under different lighting conditions using DSLR cameras\nand egocentric AR glasses. This dataset establishes the first comprehensive\nreal-world evaluation benchmark for 3D digital twin creation tasks, offering a\nrobust foundation for comparing and improving existing reconstruction methods.\nThe DTC dataset is already released at\nhttps://www.projectaria.com/datasets/dtc/ and we will also make the baseline\nevaluations open-source.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.GR",
    "comment": "accepted to CVPR 2025 (Highlight). Dataset page:\n  https://www.projectaria.com/datasets/dtc/",
    "pdf_url": "http://arxiv.org/pdf/2504.08541v2",
    "published_date": "2025-04-11 13:54:19 UTC",
    "updated_date": "2025-05-18 21:33:54 UTC"
  },
  {
    "arxiv_id": "2504.11470v1",
    "title": "SO-DETR: Leveraging Dual-Domain Features and Knowledge Distillation for Small Object Detection",
    "authors": [
      "Huaxiang Zhang",
      "Hao Zhang",
      "Aoran Mei",
      "Zhongxue Gan",
      "Guo-Niu Zhu"
    ],
    "abstract": "Detection Transformer-based methods have achieved significant advancements in\ngeneral object detection. However, challenges remain in effectively detecting\nsmall objects. One key difficulty is that existing encoders struggle to\nefficiently fuse low-level features. Additionally, the query selection\nstrategies are not effectively tailored for small objects. To address these\nchallenges, this paper proposes an efficient model, Small Object Detection\nTransformer (SO-DETR). The model comprises three key components: a dual-domain\nhybrid encoder, an enhanced query selection mechanism, and a knowledge\ndistillation strategy. The dual-domain hybrid encoder integrates spatial and\nfrequency domains to fuse multi-scale features effectively. This approach\nenhances the representation of high-resolution features while maintaining\nrelatively low computational overhead. The enhanced query selection mechanism\noptimizes query initialization by dynamically selecting high-scoring anchor\nboxes using expanded IoU, thereby improving the allocation of query resources.\nFurthermore, by incorporating a lightweight backbone network and implementing a\nknowledge distillation strategy, we develop an efficient detector for small\nobjects. Experimental results on the VisDrone-2019-DET and UAVVaste datasets\ndemonstrate that SO-DETR outperforms existing methods with similar\ncomputational demands. The project page is available at\nhttps://github.com/ValiantDiligent/SO_DETR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.11470v1",
    "published_date": "2025-04-11 13:47:37 UTC",
    "updated_date": "2025-04-11 13:47:37 UTC"
  },
  {
    "arxiv_id": "2504.08536v1",
    "title": "Explainability and Continual Learning meet Federated Learning at the Network Edge",
    "authors": [
      "Thomas Tsouparopoulos",
      "Iordanis Koutsopoulos"
    ],
    "abstract": "As edge devices become more capable and pervasive in wireless networks, there\nis growing interest in leveraging their collective compute power for\ndistributed learning. However, optimizing learning at the network edge entails\nunique challenges, particularly when moving beyond conventional settings and\nobjectives. While Federated Learning (FL) has emerged as a key paradigm for\ndistributed model training, critical challenges persist. First, existing\napproaches often overlook the trade-off between predictive accuracy and\ninterpretability. Second, they struggle to integrate inherently explainable\nmodels such as decision trees because their non-differentiable structure makes\nthem not amenable to backpropagation-based training algorithms. Lastly, they\nlack meaningful mechanisms for continual Machine Learning (ML) model adaptation\nthrough Continual Learning (CL) in resource-limited environments. In this\npaper, we pave the way for a set of novel optimization problems that emerge in\ndistributed learning at the network edge with wirelessly interconnected edge\ndevices, and we identify key challenges and future directions. Specifically, we\ndiscuss how Multi-objective optimization (MOO) can be used to address the\ntrade-off between predictive accuracy and explainability when using complex\npredictive models. Next, we discuss the implications of integrating inherently\nexplainable tree-based models into distributed learning settings. Finally, we\ninvestigate how CL strategies can be effectively combined with FL to support\nadaptive, lifelong learning when limited-size buffers are used to store past\ndata for retraining. Our approach offers a cohesive set of tools for designing\nprivacy-preserving, adaptive, and trustworthy ML solutions tailored to the\ndemands of edge computing and intelligent services.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "8 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08536v1",
    "published_date": "2025-04-11 13:45:55 UTC",
    "updated_date": "2025-04-11 13:45:55 UTC"
  },
  {
    "arxiv_id": "2504.08530v1",
    "title": "LGRPool: Hierarchical Graph Pooling Via Local-Global Regularisation",
    "authors": [
      "Farshad Noravesh",
      "Reza Haffari",
      "Layki Soon",
      "Arghya Pal"
    ],
    "abstract": "Hierarchical graph pooling(HGP) are designed to consider the fact that\nconventional graph neural networks(GNN) are inherently flat and are also not\nmultiscale. However, most HGP methods suffer not only from lack of considering\nglobal topology of the graph and focusing on the feature learning aspect, but\nalso they do not align local and global features since graphs should inherently\nbe analyzed in a multiscale way. LGRPool is proposed in the present paper as a\nHGP in the framework of expectation maximization in machine learning that\naligns local and global aspects of message passing with each other using a\nregularizer to force the global topological information to be inline with the\nlocal message passing at different scales through the representations at\ndifferent layers of HGP. Experimental results on some graph classification\nbenchmarks show that it slightly outperforms some baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "f tables, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08530v1",
    "published_date": "2025-04-11 13:41:14 UTC",
    "updated_date": "2025-04-11 13:41:14 UTC"
  },
  {
    "arxiv_id": "2504.08526v1",
    "title": "Hallucination, reliability, and the role of generative AI in science",
    "authors": [
      "Charles Rathkopf"
    ],
    "abstract": "Generative AI is increasingly used in scientific domains, from protein\nfolding to climate modeling. But these models produce distinctive errors known\nas hallucinations - outputs that are incorrect yet superficially plausible.\nWorse, some arguments suggest that hallucinations are an inevitable consequence\nof the mechanisms underlying generative inference. Fortunately, such arguments\nrely on a conception of hallucination defined solely with respect to internal\nproperties of the model, rather than in reference to the empirical target\nsystem. This conception fails to distinguish epistemically benign errors from\nthose that threaten scientific inference. I introduce the concept of corrosive\nhallucination to capture the epistemically troubling subclass:\nmisrepresentations that are substantively misleading and resistant to\nsystematic anticipation. I argue that although corrosive hallucinations do pose\na threat to scientific reliability, they are not inevitable. Scientific\nworkflows such as those surrounding AlphaFold and GenCast, both of which serve\nas case studies, can neutralize their effects by imposing theoretical\nconstraints during training, and by strategically screening for errors at\ninference time. When embedded in such workflows, generative AI can reliably\ncontribute to scientific knowledge.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "31 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2504.08526v1",
    "published_date": "2025-04-11 13:38:56 UTC",
    "updated_date": "2025-04-11 13:38:56 UTC"
  },
  {
    "arxiv_id": "2504.08525v3",
    "title": "Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware Extensions for Multi-Step LLM Agent Tasks",
    "authors": [
      "Ye Ye"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. A reference\nimplementation of the core TME components is available at\nhttps://github.com/biubiutomato/TME-Agent, including basic examples and\nstructured memory integration. While the current implementation uses a\ntree-based structure, TME is designed to be graph-aware, supporting reusable\nsubsteps, converging task paths, and shared dependencies. This lays the\ngroundwork for future DAG-based memory architectures.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "68T05",
      "I.2.6; I.2.8; H.3.3"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 5 figures. Preprint prepared for future submission.\n  Includes implementation and token-efficiency analysis. Code at\n  https://github.com/biubiutomato/TME-Agent",
    "pdf_url": "http://arxiv.org/pdf/2504.08525v3",
    "published_date": "2025-04-11 13:38:36 UTC",
    "updated_date": "2025-04-16 14:48:13 UTC"
  },
  {
    "arxiv_id": "2504.08524v2",
    "title": "Mitigating Timbre Leakage with Universal Semantic Mapping Residual Block for Voice Conversion",
    "authors": [
      "Na Li",
      "Chuke Wang",
      "Yu Gu",
      "Zhifeng Li"
    ],
    "abstract": "Voice conversion (VC) transforms source speech into a target voice by\npreserving the content. However, timbre information from the source speaker is\ninherently embedded in the content representations, causing significant timbre\nleakage and reducing similarity to the target speaker. To address this, we\nintroduce a residual block to a content extractor. The residual block consists\nof two weighted branches: 1) universal semantic dictionary based Content\nFeature Re-expression (CFR) module, supplying timbre-free content\nrepresentation. 2) skip connection to the original content layer, providing\ncomplementary fine-grained information. In the CFR module, each dictionary\nentry in the universal semantic dictionary represents a phoneme class, computed\nstatistically using speech from multiple speakers, creating a stable,\nspeaker-independent semantic set. We introduce a CFR method to obtain\ntimbre-free content representations by expressing each content frame as a\nweighted linear combination of dictionary entries using corresponding phoneme\nposteriors as weights. Extensive experiments across various VC frameworks\ndemonstrate that our approach effectively mitigates timbre leakage and\nsignificantly improves similarity to the target speaker.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08524v2",
    "published_date": "2025-04-11 13:36:59 UTC",
    "updated_date": "2025-04-29 15:49:13 UTC"
  },
  {
    "arxiv_id": "2504.12324v2",
    "title": "Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and Interpretability Prediction",
    "authors": [
      "Mengying Yuan",
      "Wenhao Wang",
      "Zixuan Wang",
      "Yujie Huang",
      "Kangli Wei",
      "Fei Li",
      "Chong Teng",
      "Donghong Ji"
    ],
    "abstract": "Natural Language Inference (NLI) is a fundamental task in natural language\nprocessing. While NLI has developed many sub-directions such as sentence-level\nNLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI\n(CDCL-NLI) remains largely unexplored. In this paper, we propose a novel\nparadigm: CDCL-NLI, which extends traditional NLI capabilities to\nmulti-document, multilingual scenarios. To support this task, we construct a\nhigh-quality CDCL-NLI dataset including 25,410 instances and spanning 26\nlanguages. To address the limitations of previous methods on CDCL-NLI task, we\nfurther propose an innovative method that integrates RST-enhanced graph fusion\nwith interpretability-aware prediction. Our approach leverages RST (Rhetorical\nStructure Theory) within heterogeneous graph neural networks for cross-document\ncontext modeling, and employs a structure-aware semantic alignment based on\nlexical chains for cross-lingual understanding. For NLI interpretability, we\ndevelop an EDU (Elementary Discourse Unit)-level attribution framework that\nproduces extractive explanations. Extensive experiments demonstrate our\napproach's superior performance, achieving significant improvements over both\nconventional NLI models as well as large language models. Our work sheds light\non the study of NLI and will bring research interest on cross-document\ncross-lingual context understanding, hallucination elimination and\ninterpretability inference. Our code and datasets are available at\n\\href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-link} for peer\nreview.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12324v2",
    "published_date": "2025-04-11 13:18:26 UTC",
    "updated_date": "2025-05-20 11:52:01 UTC"
  },
  {
    "arxiv_id": "2504.08874v1",
    "title": "Distilling and exploiting quantitative insights from Large Language Models for enhanced Bayesian optimization of chemical reactions",
    "authors": [
      "Roshan Patel",
      "Saeed Moayedpour",
      "Louis De Lescure",
      "Lorenzo Kogler-Anele",
      "Alan Cherney",
      "Sven Jager",
      "Yasser Jangjou"
    ],
    "abstract": "Machine learning and Bayesian optimization (BO) algorithms can significantly\naccelerate the optimization of chemical reactions. Transfer learning can\nbolster the effectiveness of BO algorithms in low-data regimes by leveraging\npre-existing chemical information or data outside the direct optimization task\n(i.e., source data). Large language models (LLMs) have demonstrated that\nchemical information present in foundation training data can give them utility\nfor processing chemical data. Furthermore, they can be augmented with and help\nsynthesize potentially multiple modalities of source chemical data germane to\nthe optimization task. In this work, we examine how chemical information from\nLLMs can be elicited and used for transfer learning to accelerate the BO of\nreaction conditions to maximize yield. Specifically, we show that a survey-like\nprompting scheme and preference learning can be used to infer a utility\nfunction which models prior chemical information embedded in LLMs over a\nchemical parameter space; we find that the utility function shows modest\ncorrelation to true experimental measurements (yield) over the parameter space\ndespite operating in a zero-shot setting. Furthermore, we show that the utility\nfunction can be leveraged to focus BO efforts in promising regions of the\nparameter space, improving the yield of the initial BO query and enhancing\noptimization in 4 of the 6 datasets studied. Overall, we view this work as a\nstep towards bridging the gap between the chemistry knowledge embedded in LLMs\nand the capabilities of principled BO methods to accelerate reaction\noptimization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08874v1",
    "published_date": "2025-04-11 12:45:07 UTC",
    "updated_date": "2025-04-11 12:45:07 UTC"
  },
  {
    "arxiv_id": "2504.11469v1",
    "title": "Do Segmentation Models Understand Vascular Structure? A Blob-Based XAI Framework",
    "authors": [
      "Guillaume Garret",
      "Antoine Vacavant",
      "Carole Frindel"
    ],
    "abstract": "Deep learning models have achieved impressive performance in medical image\nsegmentation, yet their black-box nature limits clinical adoption. In vascular\napplications, trustworthy segmentation should rely on both local image cues and\nglobal anatomical structures, such as vessel connectivity or branching.\nHowever, the extent to which models leverage such global context remains\nunclear. We present a novel explainability pipeline for 3D vessel segmentation,\ncombining gradient-based attribution with graph-guided point selection and a\nblob-based analysis of Saliency maps. Using vascular graphs extracted from\nground truth, we define anatomically meaningful points of interest (POIs) and\nassess the contribution of input voxels via Saliency maps. These are analyzed\nat both global and local scales using a custom blob detector. Applied to IRCAD\nand Bullitt datasets, our analysis shows that model decisions are dominated by\nhighly localized attribution blobs centered near POIs. Attribution features\nshow little correlation with vessel-level properties such as thickness,\ntubularity, or connectivity -- suggesting limited use of global anatomical\nreasoning. Our results underline the importance of structured explainability\ntools and highlight the current limitations of segmentation models in capturing\nglobal vascular context.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Open access version of an article submitted to Medical Image\n  Understanding and Analysis (MIUA) 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.11469v1",
    "published_date": "2025-04-11 12:42:52 UTC",
    "updated_date": "2025-04-11 12:42:52 UTC"
  },
  {
    "arxiv_id": "2504.08490v1",
    "title": "Adopting Large Language Models to Automated System Integration",
    "authors": [
      "Robin D. Pesl"
    ],
    "abstract": "Modern enterprise computing systems integrate numerous subsystems to resolve\na common task by yielding emergent behavior. A widespread approach is using\nservices implemented with Web technologies like REST or OpenAPI, which offer an\ninteraction mechanism and service documentation standard, respectively. Each\nservice represents a specific business functionality, allowing encapsulation\nand easier maintenance. Despite the reduced maintenance costs on an individual\nservice level, increased integration complexity arises. Consequently, automated\nservice composition approaches have arisen to mitigate this issue.\nNevertheless, these approaches have not achieved high acceptance in practice\ndue to their reliance on complex formal modeling. Within this Ph.D. thesis, we\nanalyze the application of Large Language Models (LLMs) to automatically\nintegrate the services based on a natural language input. The result is a\nreusable service composition, e.g., as program code. While not always\ngenerating entirely correct results, the result can still be helpful by\nproviding integration engineers with a close approximation of a suitable\nsolution, which requires little effort to become operational. Our research\ninvolves (i) introducing a software architecture for automated service\ncomposition using LLMs, (ii) analyzing Retrieval Augmented Generation (RAG) for\nservice discovery, (iii) proposing a novel natural language query-based\nbenchmark for service discovery, and (iv) extending the benchmark to complete\nservice composition scenarios. We have presented our software architecture as\nCompositio Prompto, the analysis of RAG for service discovery, and submitted a\nproposal for the service discovery benchmark. Open topics are primarily the\nextension of the service discovery benchmark to service composition scenarios\nand the improvements of the service composition generation, e.g., using\nfine-tuning or LLM agents.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08490v1",
    "published_date": "2025-04-11 12:42:01 UTC",
    "updated_date": "2025-04-11 12:42:01 UTC"
  },
  {
    "arxiv_id": "2504.08481v1",
    "title": "A Hybrid Fully Convolutional CNN-Transformer Model for Inherently Interpretable Medical Image Classification",
    "authors": [
      "Kerol Djoumessi",
      "Samuel Ofosu Mensah",
      "Philipp Berens"
    ],
    "abstract": "In many medical imaging tasks, convolutional neural networks (CNNs)\nefficiently extract local features hierarchically. More recently, vision\ntransformers (ViTs) have gained popularity, using self-attention mechanisms to\ncapture global dependencies, but lacking the inherent spatial localization of\nconvolutions. Therefore, hybrid models combining CNNs and ViTs have been\ndeveloped to combine the strengths of both architectures. However, such hybrid\nCNN-ViT models are difficult to interpret, which hinders their application in\nmedical imaging. In this work, we introduce an interpretable-by-design hybrid\nfully convolutional CNN-Transformer architecture for medical image\nclassification. Unlike widely used post-hoc saliency methods for ViTs, our\napproach generates faithful and localized evidence maps that directly reflect\nthe model's decision process. We evaluated our method on two medical image\nclassification tasks using color fundus images. Our model not only achieves\nstate-of-the-art predictive performance compared to both black-box and\ninterpretable models but also provides class-specific sparse evidence maps in a\nsingle forward pass. The code is available at:\nhttps://anonymous.4open.science/r/Expl-CNN-Transformer/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08481v1",
    "published_date": "2025-04-11 12:15:22 UTC",
    "updated_date": "2025-04-11 12:15:22 UTC"
  },
  {
    "arxiv_id": "2504.08470v1",
    "title": "On the Design of Diffusion-based Neural Speech Codecs",
    "authors": [
      "Pietro Foti",
      "Andreas Brendel"
    ],
    "abstract": "Recently, neural speech codecs (NSCs) trained as generative models have shown\nsuperior performance compared to conventional codecs at low bitrates. Although\nmost state-of-the-art NSCs are trained as Generative Adversarial Networks\n(GANs), Diffusion Models (DMs), a recent class of generative models, represent\na promising alternative due to their superior performance in image generation\nrelative to GANs. Consequently, DMs have been successfully applied for audio\nand speech coding among various other audio generation applications. However,\nthe design of diffusion-based NSCs has not yet been explored in a systematic\nway. We address this by providing a comprehensive analysis of diffusion-based\nNSCs divided into three contributions. First, we propose a categorization based\non the conditioning and output domains of the DM. This simple conceptual\nframework allows us to define a design space for diffusion-based NSCs and to\nassign a category to existing approaches in the literature. Second, we\nsystematically investigate unexplored designs by creating and evaluating new\ndiffusion-based NSCs within the conceptual framework. Finally, we compare the\nproposed models to existing GAN and DM baselines through objective metrics and\nsubjective listening tests.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08470v1",
    "published_date": "2025-04-11 11:58:38 UTC",
    "updated_date": "2025-04-11 11:58:38 UTC"
  },
  {
    "arxiv_id": "2504.08872v1",
    "title": "Personalizing Federated Learning for Hierarchical Edge Networks with Non-IID Data",
    "authors": [
      "Seunghyun Lee",
      "Omid Tavallaie",
      "Shuaijun Chen",
      "Kanchana Thilakarathna",
      "Suranga Seneviratne",
      "Adel Nadjaran Toosi",
      "Albert Y. Zomaya"
    ],
    "abstract": "Accommodating edge networks between IoT devices and the cloud server in\nHierarchical Federated Learning (HFL) enhances communication efficiency without\ncompromising data privacy. However, devices connected to the same edge often\nshare geographic or contextual similarities, leading to varying edge-level data\nheterogeneity with different subsets of labels per edge, on top of device-level\nheterogeneity. This hierarchical non-Independent and Identically Distributed\n(non-IID) nature, which implies that each edge has its own optimization goal,\nhas been overlooked in HFL research. Therefore, existing edge-accommodated HFL\ndemonstrates inconsistent performance across edges in various hierarchical\nnon-IID scenarios. To ensure robust performance with diverse edge-level non-IID\ndata, we propose a Personalized Hierarchical Edge-enabled Federated Learning\n(PHE-FL), which personalizes each edge model to perform well on the unique\nclass distributions specific to each edge. We evaluated PHE-FL across 4\nscenarios with varying levels of edge-level non-IIDness, with extreme IoT\ndevice level non-IIDness. To accurately assess the effectiveness of our\npersonalization approach, we deployed test sets on each edge server instead of\nthe cloud server, and used both balanced and imbalanced test sets. Extensive\nexperiments show that PHE-FL achieves up to 83 percent higher accuracy compared\nto existing federated learning approaches that incorporate edge networks, given\nthe same number of training rounds. Moreover, PHE-FL exhibits improved\nstability, as evidenced by reduced accuracy fluctuations relative to the\nstate-of-the-art FedAvg with two-level (edge and cloud) aggregation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08872v1",
    "published_date": "2025-04-11 11:42:06 UTC",
    "updated_date": "2025-04-11 11:42:06 UTC"
  },
  {
    "arxiv_id": "2504.08456v1",
    "title": "Generalization Bounds in Hybrid Quantum-Classical Machine Learning Models",
    "authors": [
      "Tongyan Wu",
      "Amine Bentellis",
      "Alona Sakhnenko",
      "Jeanette Miriam Lorenz"
    ],
    "abstract": "Hybrid classical-quantum models aim to harness the strengths of both quantum\ncomputing and classical machine learning, but their practical potential remains\npoorly understood. In this work, we develop a unified mathematical framework\nfor analyzing generalization in hybrid models, offering insight into how these\nsystems learn from data. We establish a novel generalization bound of the form\n$O\\big( \\sqrt{\\frac{T\\log{T}}{N}} + \\frac{\\alpha}{\\sqrt{N}}\\big)$ for $N$\ntraining data points, $T$ trainable quantum gates, and bounded fully-connected\nlayers $||F|| \\leq \\alpha$. This bound decomposes cleanly into quantum and\nclassical contributions, extending prior work on both components and clarifying\ntheir interaction. We apply our results to the quantum-classical convolutional\nneural network (QCCNN), an architecture that integrates quantum convolutional\nlayers with classical processing. Alongside the bound, we highlight conceptual\nlimitations of applying classical statistical learning theory in the hybrid\nsetting and suggest promising directions for future theoretical work.",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "primary_category": "quant-ph",
    "comment": "6 + 5 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.08456v1",
    "published_date": "2025-04-11 11:35:03 UTC",
    "updated_date": "2025-04-11 11:35:03 UTC"
  },
  {
    "arxiv_id": "2504.08871v1",
    "title": "An LLM Framework For Cryptography Over Chat Channels",
    "authors": [
      "Danilo Gligoroski",
      "Mayank Raikwar",
      "Sonu Kumar Jha"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have transformed\ncommunication, yet their role in secure messaging remains underexplored,\nespecially in surveillance-heavy environments. At the same time, many\ngovernments all over the world are proposing legislation to detect, backdoor,\nor even ban encrypted communication. That emphasizes the need for alternative\nways to communicate securely and covertly over open channels. We propose a\nnovel cryptographic embedding framework that enables covert Public Key or\nSymmetric Key encrypted communication over public chat channels with humanlike\nproduced texts. Some unique properties of our framework are: 1. It is LLM\nagnostic, i.e., it allows participants to use different local LLM models\nindependently; 2. It is pre- or post-quantum agnostic; 3. It ensures\nindistinguishability from human-like chat-produced texts. Thus, it offers a\nviable alternative where traditional encryption is detectable and restricted.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "27 Pages",
    "pdf_url": "http://arxiv.org/pdf/2504.08871v1",
    "published_date": "2025-04-11 11:34:14 UTC",
    "updated_date": "2025-04-11 11:34:14 UTC"
  },
  {
    "arxiv_id": "2504.08866v1",
    "title": "On Transfer-based Universal Attacks in Pure Black-box Setting",
    "authors": [
      "Mohammad A. A. K. Jalwana",
      "Naveed Akhtar",
      "Ajmal Mian",
      "Nazanin Rahnavard",
      "Mubarak Shah"
    ],
    "abstract": "Despite their impressive performance, deep visual models are susceptible to\ntransferable black-box adversarial attacks. Principally, these attacks craft\nperturbations in a target model-agnostic manner. However, surprisingly, we find\nthat existing methods in this domain inadvertently take help from various\npriors that violate the black-box assumption such as the availability of the\ndataset used to train the target model, and the knowledge of the number of\nclasses in the target model. Consequently, the literature fails to articulate\nthe true potency of transferable black-box attacks. We provide an empirical\nstudy of these biases and propose a framework that aids in a prior-free\ntransparent study of this paradigm. Using our framework, we analyze the role of\nprior knowledge of the target model data and number of classes in attack\nperformance. We also provide several interesting insights based on our\nanalysis, and demonstrate that priors cause overestimation in transferability\nscores. Finally, we extend our framework to query-based attacks. This extension\ninspires a novel image-blending technique to prepare data for effective\nsurrogate model training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08866v1",
    "published_date": "2025-04-11 10:41:20 UTC",
    "updated_date": "2025-04-11 10:41:20 UTC"
  },
  {
    "arxiv_id": "2504.10521v1",
    "title": "Integrating Emotion Distribution Networks and Textual Message Analysis for X User Emotional State Classification",
    "authors": [
      "Pardis Moradbeiki",
      "Mohammad Ali Zare Chahooki"
    ],
    "abstract": "As the popularity and reach of social networks continue to surge, a vast\nreservoir of opinions and sentiments across various subjects inundates these\nplatforms. Among these, X social network (formerly Twitter) stands as a\njuggernaut, boasting approximately 420 million active users. Extracting users'\nemotional and mental states from their expressed opinions on social media has\nbecome a common pursuit. While past methodologies predominantly focused on the\ntextual content of messages to analyze user sentiment, the interactive nature\nof these platforms suggests a deeper complexity. This study employs hybrid\nmethodologies, integrating textual analysis, profile examination, follower\nanalysis, and emotion dissemination patterns. Initially, user interactions are\nleveraged to refine emotion classification within messages, encompassing\nexchanges where users respond to each other. Introducing the concept of a\ncommunication tree, a model is extracted to map these interactions.\nSubsequently, users' bios and interests from this tree are juxtaposed with\nmessage text to enrich analysis. Finally, influential figures are identified\namong users' followers in the communication tree, categorized into different\ntopics to gauge interests. The study highlights that traditional sentiment\nanalysis methodologies, focusing solely on textual content, are inadequate in\ndiscerning sentiment towards significant events, notably the presidential\nelection. Comparative analysis with conventional methods reveals a substantial\nimprovement in accuracy with the incorporation of emotion distribution patterns\nand user profiles. The proposed approach yields a 12% increase in accuracy with\nemotion distribution patterns and a 15% increase when considering user\nprofiles, underscoring its efficacy in capturing nuanced sentiment dynamics.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10521v1",
    "published_date": "2025-04-11 10:37:35 UTC",
    "updated_date": "2025-04-11 10:37:35 UTC"
  },
  {
    "arxiv_id": "2504.08418v1",
    "title": "seeBias: A Comprehensive Tool for Assessing and Visualizing AI Fairness",
    "authors": [
      "Yilin Ning",
      "Yian Ma",
      "Mingxuan Liu",
      "Xin Li",
      "Nan Liu"
    ],
    "abstract": "Fairness in artificial intelligence (AI) prediction models is increasingly\nemphasized to support responsible adoption in high-stakes domains such as\nhealth care and criminal justice. Guidelines and implementation frameworks\nhighlight the importance of both predictive accuracy and equitable outcomes.\nHowever, current fairness toolkits often evaluate classification performance\ndisparities in isolation, with limited attention to other critical aspects such\nas calibration. To address these gaps, we present seeBias, an R package for\ncomprehensive evaluation of model fairness and predictive performance. seeBias\noffers an integrated evaluation across classification, calibration, and other\nperformance domains, providing a more complete view of model behavior. It\nincludes customizable visualizations to support transparent reporting and\nresponsible AI implementation. Using public datasets from criminal justice and\nhealthcare, we demonstrate how seeBias supports fairness evaluations, and\nuncovers disparities that conventional fairness metrics may overlook. The R\npackage is available on GitHub, and a Python version is under development.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08418v1",
    "published_date": "2025-04-11 10:23:10 UTC",
    "updated_date": "2025-04-11 10:23:10 UTC"
  },
  {
    "arxiv_id": "2504.08417v1",
    "title": "Belief States for Cooperative Multi-Agent Reinforcement Learning under Partial Observability",
    "authors": [
      "Paul J. Pritz",
      "Kin K. Leung"
    ],
    "abstract": "Reinforcement learning in partially observable environments is typically\nchallenging, as it requires agents to learn an estimate of the underlying\nsystem state. These challenges are exacerbated in multi-agent settings, where\nagents learn simultaneously and influence the underlying state as well as each\nothers' observations. We propose the use of learned beliefs on the underlying\nstate of the system to overcome these challenges and enable reinforcement\nlearning with fully decentralized training and execution. Our approach\nleverages state information to pre-train a probabilistic belief model in a\nself-supervised fashion. The resulting belief states, which capture both\ninferred state information as well as uncertainty over this information, are\nthen used in a state-based reinforcement learning algorithm to create an\nend-to-end model for cooperative multi-agent reinforcement learning under\npartial observability. By separating the belief and reinforcement learning\ntasks, we are able to significantly simplify the policy and value function\nlearning tasks and improve both the convergence speed and the final\nperformance. We evaluate our proposed method on diverse partially observable\nmulti-agent tasks designed to exhibit different variants of partial\nobservability.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08417v1",
    "published_date": "2025-04-11 10:21:58 UTC",
    "updated_date": "2025-04-11 10:21:58 UTC"
  },
  {
    "arxiv_id": "2504.08415v1",
    "title": "Constrained Machine Learning Through Hyperspherical Representation",
    "authors": [
      "Gaetano Signorelli",
      "Michele Lombardi"
    ],
    "abstract": "The problem of ensuring constraints satisfaction on the output of machine\nlearning models is critical for many applications, especially in\nsafety-critical domains. Modern approaches rely on penalty-based methods at\ntraining time, which do not guarantee to avoid constraints violations; or\nconstraint-specific model architectures (e.g., for monotonocity); or on output\nprojection, which requires to solve an optimization problem that might be\ncomputationally demanding. We present the Hypersherical Constrained\nRepresentation, a novel method to enforce constraints in the output space for\nconvex and bounded feasibility regions (generalizable to star domains). Our\nmethod operates on a different representation system, where Euclidean\ncoordinates are converted into hyperspherical coordinates relative to the\nconstrained region, which can only inherently represent feasible points.\nExperiments on a synthetic and a real-world dataset show that our method has\npredictive performance comparable to the other approaches, can guarantee 100%\nconstraint satisfaction, and has a minimal computational cost at inference\ntime.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08415v1",
    "published_date": "2025-04-11 10:19:49 UTC",
    "updated_date": "2025-04-11 10:19:49 UTC"
  },
  {
    "arxiv_id": "2504.08411v1",
    "title": "A Knowledge-guided Adversarial Defense for Resisting Malicious Visual Manipulation",
    "authors": [
      "Dawei Zhou",
      "Suzhi Gang",
      "Decheng Liu",
      "Tongliang Liu",
      "Nannan Wang",
      "Xinbo Gao"
    ],
    "abstract": "Malicious applications of visual manipulation have raised serious threats to\nthe security and reputation of users in many fields. To alleviate these issues,\nadversarial noise-based defenses have been enthusiastically studied in recent\nyears. However, ``data-only\" methods tend to distort fake samples in the\nlow-level feature space rather than the high-level semantic space, leading to\nlimitations in resisting malicious manipulation. Frontier research has shown\nthat integrating knowledge in deep learning can produce reliable and\ngeneralizable solutions. Inspired by these, we propose a knowledge-guided\nadversarial defense (KGAD) to actively force malicious manipulation models to\noutput semantically confusing samples. Specifically, in the process of\ngenerating adversarial noise, we focus on constructing significant semantic\nconfusions at the domain-specific knowledge level, and exploit a metric closely\nrelated to visual perception to replace the general pixel-wise metrics. The\ngenerated adversarial noise can actively interfere with the malicious\nmanipulation model by triggering knowledge-guided and perception-related\ndisruptions in the fake samples. To validate the effectiveness of the proposed\nmethod, we conduct qualitative and quantitative experiments on human perception\nand visual quality assessment. The results on two different tasks both show\nthat our defense provides better protection compared to state-of-the-art\nmethods and achieves great generalizability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08411v1",
    "published_date": "2025-04-11 10:18:13 UTC",
    "updated_date": "2025-04-11 10:18:13 UTC"
  },
  {
    "arxiv_id": "2504.12323v2",
    "title": "The Other Side of the Coin: Exploring Fairness in Retrieval-Augmented Generation",
    "authors": [
      "Zheng Zhang",
      "Ning Li",
      "Qi Liu",
      "Rui Li",
      "Weibo Gao",
      "Qingyang Mao",
      "Zhenya Huang",
      "Baosheng Yu",
      "Dacheng Tao"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nretrieving relevant document from external knowledge sources. By referencing\nthis external knowledge, RAG effectively reduces the generation of factually\nincorrect content and addresses hallucination issues within LLMs. Recently,\nthere has been growing attention to improving the performance and efficiency of\nRAG systems from various perspectives. While these advancements have yielded\nsignificant results, the application of RAG in domains with considerable\nsocietal implications raises a critical question about fairness: What impact\ndoes the introduction of the RAG paradigm have on the fairness of LLMs? To\naddress this question, we conduct extensive experiments by varying the LLMs,\nretrievers, and retrieval sources. Our experimental analysis reveals that the\nscale of the LLMs plays a significant role in influencing fairness outcomes\nwithin the RAG framework. When the model scale is smaller than 8B, the\nintegration of retrieval mechanisms often exacerbates unfairness in small-scale\nLLMs (e.g., LLaMA3.2-1B, Mistral-7B, and LLaMA3-8B). To mitigate the fairness\nissues introduced by RAG for small-scale LLMs, we propose two approaches,\nFairFT and FairFilter. Specifically, in FairFT, we align the retriever with the\nLLM in terms of fairness, enabling it to retrieve documents that facilitate\nfairer model outputs. In FairFilter, we propose a fairness filtering mechanism\nto filter out biased content after retrieval. Finally, we validate our proposed\napproaches on real-world datasets, demonstrating their effectiveness in\nimproving fairness while maintaining performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.12323v2",
    "published_date": "2025-04-11 10:17:10 UTC",
    "updated_date": "2025-04-19 12:49:01 UTC"
  },
  {
    "arxiv_id": "2504.08399v2",
    "title": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models",
    "authors": [
      "Yin Jou Huang",
      "Rafik Hadfi"
    ],
    "abstract": "Self-report questionnaires have long been used to assess LLM personality\ntraits, yet they fail to capture behavioral nuances due to biases and\nmeta-knowledge contamination. This paper proposes a novel multi-observer\nframework for personality trait assessments in LLM agents that draws on\ninformant-report methods in psychology. Instead of relying on self-assessments,\nwe employ multiple observer agents. Each observer is configured with a specific\nrelational context (e.g., family member, friend, or coworker) and engages the\nsubject LLM in dialogue before evaluating its behavior across the Big Five\ndimensions. We show that these observer-report ratings align more closely with\nhuman judgments than traditional self-reports and reveal systematic biases in\nLLM self-assessments. We also found that aggregating responses from 5 to 7\nobservers reduces systematic biases and achieves optimal reliability. Our\nresults highlight the role of relationship context in perceiving personality\nand demonstrate that a multi-observer paradigm offers a more reliable,\ncontext-sensitive approach to evaluating LLM personality traits.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "16 pages, 6 figures, 6 tables",
    "pdf_url": "http://arxiv.org/pdf/2504.08399v2",
    "published_date": "2025-04-11 10:03:55 UTC",
    "updated_date": "2025-05-20 12:38:58 UTC"
  },
  {
    "arxiv_id": "2504.08395v1",
    "title": "Human strategies for correcting `human-robot' errors during a laundry sorting task",
    "authors": [
      "Pepita Barnard",
      "Maria J Galvez Trigo",
      "Dominic Price",
      "Sue Cobb",
      "Gisela Reyes-Cruz",
      "Gustavo Berumen",
      "David Branson III",
      "Mojtaba A. Khanesar",
      "Mercedes Torres Torres",
      "Michel Valstar"
    ],
    "abstract": "Mental models and expectations underlying human-human interaction (HHI)\ninform human-robot interaction (HRI) with domestic robots. To ease\ncollaborative home tasks by improving domestic robot speech and behaviours for\nhuman-robot communication, we designed a study to understand how people\ncommunicated when failure occurs. To identify patterns of natural\ncommunication, particularly in response to robotic failures, participants\ninstructed Laundrobot to move laundry into baskets using natural language and\ngestures. Laundrobot either worked error-free, or in one of two error modes.\nParticipants were not advised Laundrobot would be a human actor, nor given\ninformation about error modes. Video analysis from 42 participants found speech\npatterns, included laughter, verbal expressions, and filler words, such as\n``oh'' and ``ok'', also, sequences of body movements, including touching one's\nown face, increased pointing with a static finger, and expressions of surprise.\nCommon strategies deployed when errors occurred, included correcting and\nteaching, taking responsibility, and displays of frustration. The strength of\nreaction to errors diminished with exposure, possibly indicating acceptance or\nresignation. Some used strategies similar to those used to communicate with\nother technologies, such as smart assistants. An anthropomorphic robot may not\nbe ideally suited to this kind of task. Laundrobot's appearance, morphology,\nvoice, capabilities, and recovery strategies may have impacted how it was\nperceived. Some participants indicated Laundrobot's actual skills were not\naligned with expectations; this made it difficult to know what to expect and\nhow much Laundrobot understood. Expertise, personality, and cultural\ndifferences may affect responses, however these were not assessed.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08395v1",
    "published_date": "2025-04-11 09:53:36 UTC",
    "updated_date": "2025-04-11 09:53:36 UTC"
  },
  {
    "arxiv_id": "2504.08388v1",
    "title": "MineWorld: a Real-Time and Open-Source Interactive World Model on Minecraft",
    "authors": [
      "Junliang Guo",
      "Yang Ye",
      "Tianyu He",
      "Haoyu Wu",
      "Yushu Jiang",
      "Tim Pearce",
      "Jiang Bian"
    ],
    "abstract": "World modeling is a crucial task for enabling intelligent agents to\neffectively interact with humans and operate in dynamic environments. In this\nwork, we propose MineWorld, a real-time interactive world model on Minecraft,\nan open-ended sandbox game which has been utilized as a common testbed for\nworld modeling. MineWorld is driven by a visual-action autoregressive\nTransformer, which takes paired game scenes and corresponding actions as input,\nand generates consequent new scenes following the actions. Specifically, by\ntransforming visual game scenes and actions into discrete token ids with an\nimage tokenizer and an action tokenizer correspondingly, we consist the model\ninput with the concatenation of the two kinds of ids interleaved. The model is\nthen trained with next token prediction to learn rich representations of game\nstates as well as the conditions between states and actions simultaneously. In\ninference, we develop a novel parallel decoding algorithm that predicts the\nspatial redundant tokens in each frame at the same time, letting models in\ndifferent scales generate $4$ to $7$ frames per second and enabling real-time\ninteractions with game players. In evaluation, we propose new metrics to assess\nnot only visual quality but also the action following capacity when generating\nnew scenes, which is crucial for a world model. Our comprehensive evaluation\nshows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion\nbased world models significantly. The code and model have been released.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical report. Project page https://aka.ms/mineworld",
    "pdf_url": "http://arxiv.org/pdf/2504.08388v1",
    "published_date": "2025-04-11 09:41:04 UTC",
    "updated_date": "2025-04-11 09:41:04 UTC"
  },
  {
    "arxiv_id": "2504.08386v1",
    "title": "PCA-RAG: Principal Component Analysis for Efficient Retrieval-Augmented Generation",
    "authors": [
      "Arman Khaledian",
      "Amirreza Ghadiridehkordi",
      "Nariman Khaledian"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\ngrounding large language models in external knowledge sources, improving the\nprecision of agents responses. However, high-dimensional language model\nembeddings, often in the range of hundreds to thousands of dimensions, can\npresent scalability challenges in terms of storage and latency, especially when\nprocessing massive financial text corpora. This paper investigates the use of\nPrincipal Component Analysis (PCA) to reduce embedding dimensionality, thereby\nmitigating computational bottlenecks without incurring large accuracy losses.\nWe experiment with a real-world dataset and compare different similarity and\ndistance metrics under both full-dimensional and PCA-compressed embeddings. Our\nresults show that reducing vectors from 3,072 to 110 dimensions provides a\nsizeable (up to $60\\times$) speedup in retrieval operations and a $\\sim\n28.6\\times$ reduction in index size, with only moderate declines in correlation\nmetrics relative to human-annotated similarity scores. These findings\ndemonstrate that PCA-based compression offers a viable balance between\nretrieval fidelity and resource efficiency, essential for real-time systems\nsuch as Zanista AI's \\textit{Newswitch} platform. Ultimately, our study\nunderscores the practicality of leveraging classical dimensionality reduction\ntechniques to scale RAG architectures for knowledge-intensive applications in\nfinance and trading, where speed, memory efficiency, and accuracy must jointly\nbe optimized.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.08386v1",
    "published_date": "2025-04-11 09:38:12 UTC",
    "updated_date": "2025-04-11 09:38:12 UTC"
  },
  {
    "arxiv_id": "2504.08385v1",
    "title": "Scholar Inbox: Personalized Paper Recommendations for Scientists",
    "authors": [
      "Markus Flicke",
      "Glenn Angrabeit",
      "Madhav Iyengar",
      "Vitalii Protsenko",
      "Illia Shakun",
      "Jovan Cicvaric",
      "Bora Kargi",
      "Haoyu He",
      "Lukas Schuler",
      "Lewin Scholz",
      "Kavyanjali Agnihotri",
      "Yong Cao",
      "Andreas Geiger"
    ],
    "abstract": "Scholar Inbox is a new open-access platform designed to address the\nchallenges researchers face in staying current with the rapidly expanding\nvolume of scientific literature. We provide personalized recommendations,\ncontinuous updates from open-access archives (arXiv, bioRxiv, etc.), visual\npaper summaries, semantic search, and a range of tools to streamline research\nworkflows and promote open research access. The platform's personalized\nrecommendation system is trained on user ratings, ensuring that recommendations\nare tailored to individual researchers' interests. To further enhance the user\nexperience, Scholar Inbox also offers a map of science that provides an\noverview of research across domains, enabling users to easily explore specific\ntopics. We use this map to address the cold start problem common in recommender\nsystems, as well as an active learning strategy that iteratively prompts users\nto rate a selection of papers, allowing the system to learn user preferences\nquickly. We evaluate the quality of our recommendation system on a novel\ndataset of 800k user ratings, which we make publicly available, as well as via\nan extensive user study. https://www.scholar-inbox.com/",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "comment": "https://www.scholar-inbox.com/",
    "pdf_url": "http://arxiv.org/pdf/2504.08385v1",
    "published_date": "2025-04-11 09:37:48 UTC",
    "updated_date": "2025-04-11 09:37:48 UTC"
  },
  {
    "arxiv_id": "2504.08371v1",
    "title": "Passive Underwater Acoustic Signal Separation based on Feature Decoupling Dual-path Network",
    "authors": [
      "Yucheng Liu",
      "Longyu Jiang"
    ],
    "abstract": "Signal separation in the passive underwater acoustic domain has heavily\nrelied on deep learning techniques to isolate ship radiated noise. However, the\nseparation networks commonly used in this domain stem from speech separation\napplications and may not fully consider the unique aspects of underwater\nacoustics beforehand, such as the influence of different propagation media,\nsignal frequencies and modulation characteristics. This oversight highlights\nthe need for tailored approaches that account for the specific characteristics\nof underwater sound propagation. This study introduces a novel temporal network\ndesigned to separate ship radiated noise by employing a dual-path model and a\nfeature decoupling approach. The mixed signals' features are transformed into a\nspace where they exhibit greater independence, with each dimension's\nsignificance decoupled. Subsequently, a fusion of local and global attention\nmechanisms is employed in the separation layer. Extensive comparisons showcase\nthe effectiveness of this method when compared to other prevalent network\nmodels, as evidenced by its performance in the ShipsEar and DeepShip datasets.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS",
      "68T10",
      "I.5.4; I.2.6; J.2"
    ],
    "primary_category": "cs.SD",
    "comment": "10pages,4 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08371v1",
    "published_date": "2025-04-11 09:16:22 UTC",
    "updated_date": "2025-04-11 09:16:22 UTC"
  },
  {
    "arxiv_id": "2504.08863v1",
    "title": "An Evaluation of Cultural Value Alignment in LLM",
    "authors": [
      "Nicholas Sukiennik",
      "Chen Gao",
      "Fengli Xu",
      "Yong Li"
    ],
    "abstract": "LLMs as intelligent agents are being increasingly applied in scenarios where\nhuman interactions are involved, leading to a critical concern about whether\nLLMs are faithful to the variations in culture across regions. Several works\nhave investigated this question in various ways, finding that there are biases\npresent in the cultural representations of LLM outputs. To gain a more\ncomprehensive view, in this work, we conduct the first large-scale evaluation\nof LLM culture assessing 20 countries' cultures and languages across ten LLMs.\nWith a renowned cultural values questionnaire and by carefully analyzing LLM\noutput with human ground truth scores, we thoroughly study LLMs' cultural\nalignment across countries and among individual models. Our findings show that\nthe output over all models represents a moderate cultural middle ground. Given\nthe overall skew, we propose an alignment metric, revealing that the United\nStates is the best-aligned country and GLM-4 has the best ability to align to\ncultural values. Deeper investigation sheds light on the influence of model\norigin, prompt language, and value dimensions on cultural output. Specifically,\nmodels, regardless of where they originate, align better with the US than they\ndo with China. The conclusions provide insight to how LLMs can be better\naligned to various cultures as well as provoke further discussion of the\npotential for LLMs to propagate cultural bias and the need for more culturally\nadaptable models.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "Submitted to COLM 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.08863v1",
    "published_date": "2025-04-11 09:13:19 UTC",
    "updated_date": "2025-04-11 09:13:19 UTC"
  },
  {
    "arxiv_id": "2504.08862v1",
    "title": "RTLRepoCoder: Repository-Level RTL Code Completion through the Combination of Fine-Tuning and Retrieval Augmentation",
    "authors": [
      "Peiyang Wu",
      "Nan Guo",
      "Junliang Lv",
      "Xiao Xiao",
      "Xiaochun Ye"
    ],
    "abstract": "As an essential part of modern hardware design, manually writing Register\nTransfer Level (RTL) code such as Verilog is often labor-intensive. Following\nthe tremendous success of large language models (LLMs), researchers have begun\nto explore utilizing LLMs for generating RTL code. However, current studies\nprimarily focus on generating simple single modules, which can not meet the\ndemands in real world. In fact, due to challenges in managing long-context RTL\ncode and complex cross-file dependencies, existing solutions cannot handle\nlarge-scale Verilog repositories in practical hardware development. As the\nfirst endeavor to exclusively adapt LLMs for large-scale RTL development, we\npropose RTLRepoCoder, a groundbreaking solution that incorporates specific\nfine-tuning and Retrieval-Augmented Generation (RAG) for repository-level\nVerilog code completion. Open-source Verilog repositories from the real world,\nalong with an extended context size, are used for domain-specific fine-tuning.\nThe optimized RAG system improves the information density of the input context\nby retrieving relevant code snippets. Tailored optimizations for RAG are\ncarried out, including the embedding model, the cross-file context splitting\nstrategy, and the chunk size. Our solution achieves state-of-the-art\nperformance on public benchmark, significantly surpassing GPT-4 and advanced\ndomain-specific LLMs on Edit Similarity and Exact Match rate. Comprehensive\nexperiments demonstrate the remarkable effectiveness of our approach and offer\ninsights for future work.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08862v1",
    "published_date": "2025-04-11 09:04:50 UTC",
    "updated_date": "2025-04-11 09:04:50 UTC"
  },
  {
    "arxiv_id": "2504.08861v1",
    "title": "Diachronic and synchronic variation in the performance of adaptive machine learning systems: The ethical challenges",
    "authors": [
      "Joshua Hatherley",
      "Robert Sparrow"
    ],
    "abstract": "Objectives: Machine learning (ML) has the potential to facilitate \"continual\nlearning\" in medicine, in which an ML system continues to evolve in response to\nexposure to new data over time, even after being deployed in a clinical\nsetting. In this paper, we provide a tutorial on the range of ethical issues\nraised by the use of such \"adaptive\" ML systems in medicine that have, thus\nfar, been neglected in the literature.\n  Target audience: The target audiences for this tutorial are the developers of\nmachine learning AI systems, healthcare regulators, the broader medical\ninformatics community, and practicing clinicians.\n  Scope: Discussions of adaptive ML systems to date have overlooked the\ndistinction between two sorts of variance that such systems may exhibit --\ndiachronic evolution (change over time) and synchronic variation (difference\nbetween cotemporaneous instantiations of the algorithm at different sites) --\nand under-estimated the significance of the latter. We highlight the challenges\nthat diachronic evolution and synchronic variation present for the quality of\npatient care, informed consent, and equity, and discuss the complex ethical\ntrade-offs involved in the design of such systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08861v1",
    "published_date": "2025-04-11 09:01:01 UTC",
    "updated_date": "2025-04-11 09:01:01 UTC"
  },
  {
    "arxiv_id": "2504.08359v1",
    "title": "Kernel-Level Energy-Efficient Neural Architecture Search for Tabular Dataset",
    "authors": [
      "Hoang-Loc La",
      "Phuong Hoai Ha"
    ],
    "abstract": "Many studies estimate energy consumption using proxy metrics like memory\nusage, FLOPs, and inference latency, with the assumption that reducing these\nmetrics will also lower energy consumption in neural networks. This paper,\nhowever, takes a different approach by introducing an energy-efficient Neural\nArchitecture Search (NAS) method that directly focuses on identifying\narchitectures that minimize energy consumption while maintaining acceptable\naccuracy. Unlike previous methods that primarily target vision and language\ntasks, the approach proposed here specifically addresses tabular datasets.\nRemarkably, the optimal architecture suggested by this method can reduce energy\nconsumption by up to 92% compared to architectures recommended by conventional\nNAS.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "ACIIDS 2025 Conference",
    "pdf_url": "http://arxiv.org/pdf/2504.08359v1",
    "published_date": "2025-04-11 08:48:54 UTC",
    "updated_date": "2025-04-11 08:48:54 UTC"
  },
  {
    "arxiv_id": "2504.08860v1",
    "title": "A Nonlinear Hash-based Optimization Method for SpMV on GPUs",
    "authors": [
      "Chen Yan",
      "Boyu Diao",
      "Hangda Liu",
      "Zhulin An",
      "Yongjun Xu"
    ],
    "abstract": "Sparse matrix-vector multiplication (SpMV) is a fundamental operation with a\nwide range of applications in scientific computing and artificial intelligence.\nHowever, the large scale and sparsity of sparse matrix often make it a\nperformance bottleneck. In this paper, we highlight the effectiveness of\nhash-based techniques in optimizing sparse matrix reordering, introducing the\nHash-based Partition (HBP) format, a lightweight SpMV approach. HBP retains the\nperformance benefits of the 2D-partitioning method while leveraging the hash\ntransformation's ability to group similar elements, thereby accelerating the\npre-processing phase of sparse matrix reordering. Additionally, we achieve\nparallel load balancing across matrix blocks through a competitive method. Our\nexperiments, conducted on both Nvidia Jetson AGX Orin and Nvidia RTX 4090, show\nthat in the pre-processing step, our method offers an average speedup of 3.53\ntimes compared to the sorting approach and 3.67 times compared to the dynamic\nprogramming method employed in Regu2D. Furthermore, in SpMV, our method\nachieves a maximum speedup of 3.32 times on Orin and 3.01 times on RTX4090\nagainst the CSR format in sparse matrices from the University of Florida Sparse\nMatrix Collection.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "This article has been indexed by CCGrid2025",
    "pdf_url": "http://arxiv.org/pdf/2504.08860v1",
    "published_date": "2025-04-11 08:31:44 UTC",
    "updated_date": "2025-04-11 08:31:44 UTC"
  },
  {
    "arxiv_id": "2504.08335v1",
    "title": "Entropic bounds for conditionally Gaussian vectors and applications to neural networks",
    "authors": [
      "Lucia Celli",
      "Giovanni Peccati"
    ],
    "abstract": "Using entropic inequalities from information theory, we provide new bounds on\nthe total variation and 2-Wasserstein distances between a conditionally\nGaussian law and a Gaussian law with invertible covariance matrix. We apply our\nresults to quantify the speed of convergence to Gaussian of a randomly\ninitialized fully connected neural network and its derivatives - evaluated in a\nfinite number of inputs - when the initialization is Gaussian and the sizes of\nthe inner layers diverge to infinity. Our results require mild assumptions on\nthe activation function, and allow one to recover optimal rates of convergence\nin a variety of distances, thus improving and extending the findings of Basteri\nand Trevisan (2023), Favaro et al. (2023), Trevisan (2024) and Apollonio et al.\n(2024). One of our main tools are the quantitative cumulant estimates\nestablished in Hanin (2024). As an illustration, we apply our results to bound\nthe total variation distance between the Bayesian posterior law of the neural\nnetwork and its derivatives, and the posterior law of the corresponding\nGaussian limit: this yields quantitative versions of a posterior CLT by Hron et\nal. (2022), and extends several estimates by Trevisan (2024) to the total\nvariation metric.",
    "categories": [
      "math.PR",
      "cs.AI",
      "cs.LG",
      "stat.ML",
      "60F05 (Primary) 68T07 (Secondary)",
      "G.3; I.2"
    ],
    "primary_category": "math.PR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08335v1",
    "published_date": "2025-04-11 08:00:37 UTC",
    "updated_date": "2025-04-11 08:00:37 UTC"
  },
  {
    "arxiv_id": "2504.08329v1",
    "title": "MedRep: Medical Concept Representation for General Electronic Health Record Foundation Models",
    "authors": [
      "Junmo Kim",
      "Namkyeong Lee",
      "Jiwon Kim",
      "Kwangsoo Kim"
    ],
    "abstract": "Electronic health record (EHR) foundation models have been an area ripe for\nexploration with their improved performance in various medical tasks. Despite\nthe rapid advances, there exists a fundamental limitation: Processing unseen\nmedical codes out of the vocabulary. This problem limits the generality of EHR\nfoundation models and the integration of models trained with different\nvocabularies. To deal with this problem, we propose MedRep for EHR foundation\nmodels based on the observational medical outcome partnership (OMOP) common\ndata model (CDM), providing the integrated medical concept representations and\nthe basic data augmentation strategy for patient trajectories. For concept\nrepresentation learning, we enrich the information of each concept with a\nminimal definition through large language model (LLM) prompts and enhance the\ntext-based representations through graph ontology of OMOP vocabulary.\nTrajectory augmentation randomly replaces selected concepts with other similar\nconcepts that have closely related representations to let the model practice\nwith the concepts out-of-vocabulary. Finally, we demonstrate that EHR\nfoundation models trained with MedRep better maintain the prediction\nperformance in external datasets. Our code implementation is publicly available\nat https://github.com/kicarussays/MedRep.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2504.08329v1",
    "published_date": "2025-04-11 07:51:58 UTC",
    "updated_date": "2025-04-11 07:51:58 UTC"
  },
  {
    "arxiv_id": "2504.08312v1",
    "title": "SortBench: Benchmarking LLMs based on their ability to sort lists",
    "authors": [
      "Steffen Herbold"
    ],
    "abstract": "Sorting is a tedious but simple task for human intelligence and can be solved\nfairly easily algorithmically. However, for Large Language Models (LLMs) this\ntask is surprisingly hard, as some properties of sorting are among known\nweaknesses of LLMs: being faithful to the input data, logical comparisons\nbetween values, and strictly differentiating between syntax (used for sorting)\nand semantics (typically learned by embeddings). Within this paper, we describe\nthe new SortBench benchmark for LLMs that comes with different difficulties and\nthat can be easily scaled in terms of difficulty. We apply this benchmark to\nseven state-of-the-art LLMs, including current test-time reasoning models. Our\nresults show that while the o3-mini model is very capable at sorting in\ngeneral, even this can be fooled if strings are defined to mix syntactical and\nsemantical aspects, e.g., by asking to sort numbers written-out as word.\nFurthermore, all models have problems with the faithfulness to the input of\nlong lists, i.e., they drop items and add new ones. Our results also show that\ntest-time reasoning has a tendency to overthink problems which leads to\nperformance degradation. Finally, models without test-time reasoning like\nGPT-4o are not much worse than reasoning models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08312v1",
    "published_date": "2025-04-11 07:29:56 UTC",
    "updated_date": "2025-04-11 07:29:56 UTC"
  },
  {
    "arxiv_id": "2504.08859v2",
    "title": "PolyConf: Unlocking Polymer Conformation Generation through Hierarchical Generative Models",
    "authors": [
      "Fanmeng Wang",
      "Wentao Guo",
      "Qi Ou",
      "Hongshuai Wang",
      "Haitao Lin",
      "Hongteng Xu",
      "Zhifeng Gao"
    ],
    "abstract": "Polymer conformation generation is a critical task that enables atomic-level\nstudies of diverse polymer materials. While significant advances have been made\nin designing conformation generation methods for small molecules and proteins,\nthese methods struggle to generate polymer conformations due to their unique\nstructural characteristics. Meanwhile, the scarcity of polymer conformation\ndatasets further limits the progress, making this important area largely\nunexplored. In this work, we propose PolyConf, a pioneering tailored polymer\nconformation generation method that leverages hierarchical generative models to\nunlock new possibilities. Specifically, we decompose the polymer conformation\ninto a series of local conformations (i.e., the conformations of its repeating\nunits), generating these local conformations through an autoregressive model,\nand then generating their orientation transformations via a diffusion model to\nassemble them into the complete polymer conformation. Moreover, we develop the\nfirst benchmark with a high-quality polymer conformation dataset derived from\nmolecular dynamics simulations to boost related research in this area. The\ncomprehensive evaluation demonstrates that PolyConf consistently outperforms\nexisting conformation generation methods, thus facilitating advancements in\npolymer modeling and simulation.",
    "categories": [
      "cond-mat.soft",
      "cs.AI"
    ],
    "primary_category": "cond-mat.soft",
    "comment": "Accepted by the 42nd International Conference on Machine Learning\n  (ICML 2025)",
    "pdf_url": "http://arxiv.org/pdf/2504.08859v2",
    "published_date": "2025-04-11 07:12:02 UTC",
    "updated_date": "2025-05-22 10:24:26 UTC"
  },
  {
    "arxiv_id": "2504.08300v4",
    "title": "Large Language Models Could Be Rote Learners",
    "authors": [
      "Yuyang Xu",
      "Renjun Hu",
      "Haochao Ying",
      "Jian Wu",
      "Xing Shi",
      "Wei Lin"
    ],
    "abstract": "Multiple-choice question (MCQ) benchmarks are widely used for evaluating\nLarge Language Models (LLMs), yet their reliability is undermined by benchmark\ncontamination. In this study, we reframe contamination as an inherent aspect of\nlearning and seek to disentangle genuine capability acquisition from\nsuperficial memorization in LLM evaluation. First, by analyzing model\nperformance under different memorization conditions, we uncover a\ncounterintuitive trend: LLMs perform worse on memorized MCQs than on\nnon-memorized ones, indicating the coexistence of two distinct learning\nphenomena, i.e., rote memorization and genuine capability learning. To\ndisentangle them, we propose TrinEval, a novel evaluation framework\nreformulating MCQs into an alternative trinity format, reducing memorization\nwhile preserving knowledge assessment. Experiments validate TrinEval's\neffectiveness in reformulation, and its evaluation reveals that common LLMs may\nmemorize by rote 20.5% of knowledge points (in MMLU on average).",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Work in Progress",
    "pdf_url": "http://arxiv.org/pdf/2504.08300v4",
    "published_date": "2025-04-11 07:04:44 UTC",
    "updated_date": "2025-05-19 05:03:40 UTC"
  },
  {
    "arxiv_id": "2504.08281v1",
    "title": "ELSA: A Style Aligned Dataset for Emotionally Intelligent Language Generation",
    "authors": [
      "Vishal Gandhi",
      "Sagar Gandhi"
    ],
    "abstract": "Advancements in emotion aware language processing increasingly shape vital\nNLP applications ranging from conversational AI and affective computing to\ncomputational psychology and creative content generation. Existing emotion\ndatasets either lack emotional granularity or fail to capture necessary\nstylistic diversity, limiting the advancement of effective emotion conditioned\ntext generation systems. Seeking to bridge this crucial gap between granularity\nand style diversity, this paper introduces a novel systematically constructed\ndataset named ELSA Emotion and Language Style Alignment Dataset leveraging fine\ngrained emotion taxonomies adapted from existing sources such as dair ai\nemotion dataset and GoEmotions taxonomy. This dataset comprises multiple\nemotionally nuanced variations of original sentences regenerated across\ndistinct contextual styles such as conversational, formal, poetic, and\nnarrative, using advanced Large Language Models LLMs. Rigorous computational\nevaluation using metrics such as perplexity, embedding variance, readability,\nlexical diversity, and semantic coherence measures validates the datasets\nemotional authenticity, linguistic fluency, and textual diversity.\nComprehensive metric analyses affirm its potential to support deeper\nexplorations into emotion conditioned style adaptive text generation. By\nenabling precision tuned emotionally nuanced language modeling, our dataset\ncreates fertile ground for research on fine grained emotional control, prompt\ndriven explanation, interpretability, and style adaptive expressive language\ngeneration with LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "8 pages",
    "pdf_url": "http://arxiv.org/pdf/2504.08281v1",
    "published_date": "2025-04-11 06:30:16 UTC",
    "updated_date": "2025-04-11 06:30:16 UTC"
  },
  {
    "arxiv_id": "2504.12322v2",
    "title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis",
    "authors": [
      "Xin Gao",
      "Qizhi Pei",
      "Zinan Tang",
      "Yu Li",
      "Honglin Lin",
      "Jiang Wu",
      "Lijun Wu",
      "Conghui He"
    ],
    "abstract": "While data synthesis and distillation are promising strategies to enhance\nsmall language models, current approaches heavily rely on Large Language Models\n(LLMs), which suffer from high computational costs, environmental inefficiency,\nand potential biases inherited from monolithic architectures. In contrast,\nsmaller LLMs are more accessible and sustainable, but their individual\ncapabilities often fall short in generating high-quality, diverse, and reliable\ndata. Inspired by collaborative human processes (e.g., peer review), we propose\na multiple small LLMs involved framework, GRA, that aggregates specialized\nroles across small LLMs to iterative refinement and quality control typically\nachieved by a single large LLM. In this collaborative framework, multiple small\nLLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a\npeer-review-inspired data synthesis pipeline. The Generator proposes initial\ndata samples, the Reviewer critiques their quality and diversity, and the\nAdjudicator resolves conflicts to finalize the output. By decomposing the\nsynthesis process into specialized sub-tasks, collaborative small LLMs can\nachieve data-level parity with large LLM-based distillation. Through\nexperiments across multiple benchmarks, we demonstrate that GRA-produced data\nmatches or exceeds the quality of single large LLM outputs, e.g.,\nQwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large\nmodels for high-quality data synthesis, advocating instead for strategic\ncoordination of smaller agents. Our datasets, models, and code are publicly\navailable at https://github.com/GX-XinGao/GRA.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.12322v2",
    "published_date": "2025-04-11 06:13:43 UTC",
    "updated_date": "2025-04-21 07:29:28 UTC"
  },
  {
    "arxiv_id": "2504.08259v1",
    "title": "CoProSketch: Controllable and Progressive Sketch Generation with Diffusion Model",
    "authors": [
      "Ruohao Zhan",
      "Yijin Li",
      "Yisheng He",
      "Shuo Chen",
      "Yichen Shen",
      "Xinyu Chen",
      "Zilong Dong",
      "Zhaoyang Huang",
      "Guofeng Zhang"
    ],
    "abstract": "Sketches serve as fundamental blueprints in artistic creation because sketch\nediting is easier and more intuitive than pixel-level RGB image editing for\npainting artists, yet sketch generation remains unexplored despite advancements\nin generative models. We propose a novel framework CoProSketch, providing\nprominent controllability and details for sketch generation with diffusion\nmodels. A straightforward method is fine-tuning a pretrained image generation\ndiffusion model with binarized sketch images. However, we find that the\ndiffusion models fail to generate clear binary images, which makes the produced\nsketches chaotic. We thus propose to represent the sketches by unsigned\ndistance field (UDF), which is continuous and can be easily decoded to sketches\nthrough a lightweight network. With CoProSketch, users generate a rough sketch\nfrom a bounding box and a text prompt. The rough sketch can be manually edited\nand fed back into the model for iterative refinement and will be decoded to a\ndetailed sketch as the final result. Additionally, we curate the first\nlarge-scale text-sketch paired dataset as the training data. Experiments\ndemonstrate superior semantic consistency and controllability over baselines,\noffering a practical solution for integrating user feedback into generative\nworkflows.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "11 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08259v1",
    "published_date": "2025-04-11 05:11:17 UTC",
    "updated_date": "2025-04-11 05:11:17 UTC"
  },
  {
    "arxiv_id": "2504.08258v1",
    "title": "Accelerating Multi-Objective Collaborative Optimization of Doped Thermoelectric Materials via Artificial Intelligence",
    "authors": [
      "Yuxuan Zeng",
      "Wenhao Xie",
      "Wei Cao",
      "Tan Peng",
      "Yue Hou",
      "Ziyu Wang",
      "Jing Shi"
    ],
    "abstract": "The thermoelectric performance of materials exhibits complex nonlinear\ndependencies on both elemental types and their proportions, rendering\ntraditional trial-and-error approaches inefficient and time-consuming for\nmaterial discovery. In this work, we present a deep learning model capable of\naccurately predicting thermoelectric properties of doped materials directly\nfrom their chemical formulas, achieving state-of-the-art performance. To\nenhance interpretability, we further incorporate sensitivity analysis\ntechniques to elucidate how physical descriptors affect the thermoelectric\nfigure of merit (zT). Moreover, we establish a coupled framework that\nintegrates a surrogate model with a multi-objective genetic algorithm to\nefficiently explore the vast compositional space for high-performance\ncandidates. Experimental validation confirms the discovery of a novel\nthermoelectric material with superior $zT$ values in the medium-temperature\nregime.",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cond-mat.mtrl-sci",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08258v1",
    "published_date": "2025-04-11 05:10:18 UTC",
    "updated_date": "2025-04-11 05:10:18 UTC"
  },
  {
    "arxiv_id": "2504.08856v1",
    "title": "Examining GPT's Capability to Generate and Map Course Concepts and Their Relationship",
    "authors": [
      "Tianyuan Yang",
      "Ren Baofeng",
      "Chenghao Gu",
      "Tianjia He",
      "Boxuan Ma",
      "Shinichi Konomi"
    ],
    "abstract": "Extracting key concepts and their relationships from course information and\nmaterials facilitates the provision of visualizations and recommendations for\nlearners who need to select the right courses to take from a large number of\ncourses. However, identifying and extracting themes manually is labor-intensive\nand time-consuming. Previous machine learning-based methods to extract relevant\nconcepts from courses heavily rely on detailed course materials, which\nnecessitates labor-intensive preparation of course materials. This paper\ninvestigates the potential of LLMs such as GPT in automatically generating\ncourse concepts and their relations. Specifically, we design a suite of prompts\nand provide GPT with the course information with different levels of detail,\nthereby generating high-quality course concepts and identifying their\nrelations. Furthermore, we comprehensively evaluate the quality of the\ngenerated concepts and relationships through extensive experiments. Our results\ndemonstrate the viability of LLMs as a tool for supporting educational content\nselection and delivery.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08856v1",
    "published_date": "2025-04-11 05:03:12 UTC",
    "updated_date": "2025-04-11 05:03:12 UTC"
  },
  {
    "arxiv_id": "2504.08257v1",
    "title": "Bayesian Reasoning Enabled by Spin-Orbit Torque Magnetic Tunnel Junctions",
    "authors": [
      "Yingqian Xu",
      "Xiaohan Li",
      "Caihua Wan",
      "Ran Zhang",
      "Bin He",
      "Shiqiang Liu",
      "Jihao Xia",
      "Dehao Kong",
      "Shilong Xiong",
      "Guoqiang Yu",
      "Xiufeng Han"
    ],
    "abstract": "Bayesian networks play an increasingly important role in data mining,\ninference, and reasoning with the rapid development of artificial intelligence.\nIn this paper, we present proof-of-concept experiments demonstrating the use of\nspin-orbit torque magnetic tunnel junctions (SOT-MTJs) in Bayesian network\nreasoning. Not only can the target probability distribution function (PDF) of a\nBayesian network be precisely formulated by a conditional probability table as\nusual but also quantitatively parameterized by a probabilistic forward\npropagating neuron network. Moreover, the parameters of the network can also\napproach the optimum through a simple point-by point training algorithm, by\nleveraging which we do not need to memorize all historical data nor\nstatistically summarize conditional probabilities behind them, significantly\nimproving storage efficiency and economizing data pretreatment. Furthermore, we\ndeveloped a simple medical diagnostic system using the SOT-MTJ as a random\nnumber generator and sampler, showcasing the application of SOT-MTJ-based\nBayesian reasoning. This SOT-MTJ-based Bayesian reasoning shows great promise\nin the field of artificial probabilistic neural network, broadening the scope\nof spintronic device applications and providing an efficient and low-storage\nsolution for complex reasoning tasks.",
    "categories": [
      "physics.app-ph",
      "cs.AI"
    ],
    "primary_category": "physics.app-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08257v1",
    "published_date": "2025-04-11 05:02:27 UTC",
    "updated_date": "2025-04-11 05:02:27 UTC"
  },
  {
    "arxiv_id": "2504.08256v2",
    "title": "RAG-VR: Leveraging Retrieval-Augmented Generation for 3D Question Answering in VR Environments",
    "authors": [
      "Shiyi Ding",
      "Ying Chen"
    ],
    "abstract": "Recent advances in large language models (LLMs) provide new opportunities for\ncontext understanding in virtual reality (VR). However, VR contexts are often\nhighly localized and personalized, limiting the effectiveness of\ngeneral-purpose LLMs. To address this challenge, we present RAG-VR, the first\n3D question-answering system for VR that incorporates retrieval-augmented\ngeneration (RAG), which augments an LLM with external knowledge retrieved from\na localized knowledge database to improve the answer quality. RAG-VR includes a\npipeline for extracting comprehensive knowledge about virtual environments and\nuser conditions for accurate answer generation. To ensure efficient retrieval,\nRAG-VR offloads the retrieval process to a nearby edge server and uses only\nessential information during retrieval. Moreover, we train the retriever to\neffectively distinguish among relevant, irrelevant, and hard-to-differentiate\ninformation in relation to questions. RAG-VR improves answer accuracy by\n17.9%-41.8% and reduces end-to-end latency by 34.5%-47.3% compared with two\nbaseline systems.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.IR",
    "comment": "GenAI-XR 2025 Workshop, co-located with 2025 IEEE Conference on\n  Virtual Reality and 3D User Interfaces (VR)",
    "pdf_url": "http://arxiv.org/pdf/2504.08256v2",
    "published_date": "2025-04-11 04:55:50 UTC",
    "updated_date": "2025-04-14 01:31:40 UTC"
  },
  {
    "arxiv_id": "2504.08855v1",
    "title": "Exponential Shift: Humans Adapt to AI Economies",
    "authors": [
      "Kevin J McNamara",
      "Rhea Pritham Marpu"
    ],
    "abstract": "This paper explores how artificial intelligence (AI) and robotics are\ntransforming the global labor market. Human workers, limited to a 33% duty\ncycle due to rest and holidays, cost $14 to $55 per hour. In contrast, digital\nlabor operates nearly 24/7 at just $0.10 to $0.50 per hour. We examine sectors\nlike healthcare, education, manufacturing, and retail, finding that 40-70% of\ntasks could be automated. Yet, human skills like emotional intelligence and\nadaptability remain essential. Humans process 5,000-20,000 tokens (units of\ninformation) per hour, while AI far exceeds this, though its energy use-3.5 to\n7 times higher than humans-could offset 20-40% of cost savings. Using\nreal-world examples, such as AI in journalism and law, we illustrate these\ndynamics and propose six strategies-like a 4-day workweek and retraining-to\nensure a fair transition to an AI-driven economy.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CE",
      "cs.ET"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08855v1",
    "published_date": "2025-04-11 04:43:53 UTC",
    "updated_date": "2025-04-11 04:43:53 UTC"
  },
  {
    "arxiv_id": "2504.08242v1",
    "title": "Jupiter: Fast and Resource-Efficient Collaborative Inference of Generative LLMs on Edge Devices",
    "authors": [
      "Shengyuan Ye",
      "Bei Ouyang",
      "Liekang Zeng",
      "Tianyi Qian",
      "Xiaowen Chu",
      "Jian Tang",
      "Xu Chen"
    ],
    "abstract": "Generative large language models (LLMs) have garnered significant attention\ndue to their exceptional capabilities in various AI tasks. Traditionally\ndeployed in cloud datacenters, LLMs are now increasingly moving towards more\naccessible edge platforms to protect sensitive user data and ensure privacy\npreservation. The limited computational resources of individual edge devices,\nhowever, can result in excessively prolonged inference latency and overwhelmed\nmemory usage. While existing research has explored collaborative edge computing\nto break the resource wall of individual devices, these solutions yet suffer\nfrom massive communication overhead and under-utilization of edge resources.\nFurthermore, they focus exclusively on optimizing the prefill phase, neglecting\nthe crucial autoregressive decoding phase for generative LLMs. To address that,\nwe propose Jupiter, a fast, scalable, and resource-efficient collaborative edge\nAI system for generative LLM inference. Jupiter introduces a flexible pipelined\narchitecture as a principle and differentiates its system design according to\nthe differentiated characteristics of the prefill and decoding phases. For\nprefill phase, Jupiter submits a novel intra-sequence pipeline parallelism and\ndevelops a meticulous parallelism planning strategy to maximize resource\nefficiency; For decoding, Jupiter devises an effective outline-based pipeline\nparallel decoding mechanism combined with speculative decoding, which further\nmagnifies inference acceleration. Extensive evaluation based on realistic\nimplementation demonstrates that Jupiter remarkably outperforms\nstate-of-the-art approaches under various edge environment setups, achieving up\nto 26.1x end-to-end latency reduction while rendering on-par generation\nquality.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.NI"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted by IEEE International Conference on Computer Communications\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2504.08242v1",
    "published_date": "2025-04-11 03:58:59 UTC",
    "updated_date": "2025-04-11 03:58:59 UTC"
  },
  {
    "arxiv_id": "2504.08853v1",
    "title": "Artificial Intelligence (AI) and the Relationship between Agency, Autonomy, and Moral Patiency",
    "authors": [
      "Paul Formosa",
      "Inês Hipólito",
      "Thomas Montefiore"
    ],
    "abstract": "The proliferation of Artificial Intelligence (AI) systems exhibiting complex\nand seemingly agentive behaviours necessitates a critical philosophical\nexamination of their agency, autonomy, and moral status. In this paper we\nundertake a systematic analysis of the differences between basic, autonomous,\nand moral agency in artificial systems. We argue that while current AI systems\nare highly sophisticated, they lack genuine agency and autonomy because: they\noperate within rigid boundaries of pre-programmed objectives rather than\nexhibiting true goal-directed behaviour within their environment; they cannot\nauthentically shape their engagement with the world; and they lack the critical\nself-reflection and autonomy competencies required for full autonomy.\nNonetheless, we do not rule out the possibility of future systems that could\nachieve a limited form of artificial moral agency without consciousness through\nhybrid approaches to ethical decision-making. This leads us to suggest, by\nappealing to the necessity of consciousness for moral patiency, that such\nnon-conscious AMAs might represent a case that challenges traditional\nassumptions about the necessary connection between moral agency and moral\npatiency.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08853v1",
    "published_date": "2025-04-11 03:48:40 UTC",
    "updated_date": "2025-04-11 03:48:40 UTC"
  },
  {
    "arxiv_id": "2504.08852v1",
    "title": "ML For Hardware Design Interpretability: Challenges and Opportunities",
    "authors": [
      "Raymond Baartmans",
      "Andrew Ensinger",
      "Victor Agostinelli",
      "Lizhong Chen"
    ],
    "abstract": "The increasing size and complexity of machine learning (ML) models have\ndriven the growing need for custom hardware accelerators capable of efficiently\nsupporting ML workloads. However, the design of such accelerators remains a\ntime-consuming process, heavily relying on engineers to manually ensure design\ninterpretability through clear documentation and effective communication.\nRecent advances in large language models (LLMs) offer a promising opportunity\nto automate these design interpretability tasks, particularly the generation of\nnatural language descriptions for register-transfer level (RTL) code, what we\nrefer to as \"RTL-to-NL tasks.\" In this paper, we examine how design\ninterpretability, particularly in RTL-to-NL tasks, influences the efficiency of\nthe hardware design process. We review existing work adapting LLMs for these\ntasks, highlight key challenges that remain unaddressed, including those\nrelated to data, computation, and model development, and identify opportunities\nto address them. By doing so, we aim to guide future research in leveraging ML\nto automate RTL-to-NL tasks and improve hardware design interpretability,\nthereby accelerating the hardware design process and meeting the increasing\ndemand for custom hardware accelerators in machine learning and beyond.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08852v1",
    "published_date": "2025-04-11 03:47:51 UTC",
    "updated_date": "2025-04-11 03:47:51 UTC"
  },
  {
    "arxiv_id": "2504.08851v2",
    "title": "Mimic In-Context Learning for Multimodal Tasks",
    "authors": [
      "Yuchu Jiang",
      "Jiale Fu",
      "Chenduo Hao",
      "Xinting Hu",
      "Yingzhe Peng",
      "Xin Geng",
      "Xu Yang"
    ],
    "abstract": "Recently, In-context Learning (ICL) has become a significant inference\nparadigm in Large Multimodal Models (LMMs), utilizing a few in-context\ndemonstrations (ICDs) to prompt LMMs for new tasks. However, the synergistic\neffects in multimodal data increase the sensitivity of ICL performance to the\nconfigurations of ICDs, stimulating the need for a more stable and general\nmapping function. Mathematically, in Transformer-based models, ICDs act as\n\"shift vectors\" added to the hidden states of query tokens. Inspired by this,\nwe introduce Mimic In-Context Learning (MimIC) to learn stable and\ngeneralizable shift effects from ICDs. Specifically, compared with some\nprevious shift vector-based methods, MimIC more strictly approximates the shift\neffects by integrating lightweight learnable modules into LMMs with four key\nenhancements: 1) inserting shift vectors after attention layers, 2) assigning a\nshift vector to each attention head, 3) making shift magnitude query-dependent,\nand 4) employing a layer-wise alignment loss. Extensive experiments on two LMMs\n(Idefics-9b and Idefics2-8b-base) across three multimodal tasks (VQAv2, OK-VQA,\nCaptioning) demonstrate that MimIC outperforms existing shift vector-based\nmethods. The code is available at https://github.com/Kamichanw/MimIC.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "14 pages, 7 figures,CVPR 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.08851v2",
    "published_date": "2025-04-11 03:37:59 UTC",
    "updated_date": "2025-05-17 14:43:08 UTC"
  },
  {
    "arxiv_id": "2504.13918v1",
    "title": "Modeling the quantum-like dynamics of human reliability ratings in Human-AI interactions by interaction dependent Hamiltonians",
    "authors": [
      "Johan van der Meer",
      "Pamela Hoyte",
      "Luisa Roeder",
      "Peter Bruza"
    ],
    "abstract": "As our information environments become ever more powered by artificial\nintelligence (AI), the phenomenon of trust in a human's interactions with this\nintelligence is becoming increasingly pertinent. For example, in the not too\ndistant future, there will be teams of humans and intelligent robots involved\nin dealing with the repercussions of high-risk disaster situations such as\nhurricanes, earthquakes, or nuclear accidents. Even in such conditions of high\nuncertainty, humans and intelligent machines will need to engage in shared\ndecision making, and trust is fundamental to the effectiveness of these\ninteractions. A key challenge in modeling the dynamics of this trust is to\nprovide a means to incorporate sensitivity to fluctuations in human trust\njudgments. In this article, we explore the ability of Quantum Random Walk\nmodels to model the dynamics of trust in human-AI interactions, and to\nintegrate a sensitivity to fluctuations in participant trust judgments based on\nthe nature of the interaction with the AI. We found that using empirical\nparameters to inform the use of different Hamiltonians can provide a promising\nmeans to model the evolution of trust in Human-AI interactions.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "10 pages, 7 figures. Submitted to Phil. Trans. B",
    "pdf_url": "http://arxiv.org/pdf/2504.13918v1",
    "published_date": "2025-04-11 03:23:00 UTC",
    "updated_date": "2025-04-11 03:23:00 UTC"
  },
  {
    "arxiv_id": "2504.08222v2",
    "title": "F$^3$Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos",
    "authors": [
      "Zhaoyu Liu",
      "Kan Jiang",
      "Murong Ma",
      "Zhe Hou",
      "Yun Lin",
      "Jin Song Dong"
    ],
    "abstract": "Analyzing Fast, Frequent, and Fine-grained (F$^3$) events presents a\nsignificant challenge in video analytics and multi-modal LLMs. Current methods\nstruggle to identify events that satisfy all the F$^3$ criteria with high\naccuracy due to challenges such as motion blur and subtle visual discrepancies.\nTo advance research in video understanding, we introduce F$^3$Set, a benchmark\nthat consists of video datasets for precise F$^3$ event detection. Datasets in\nF$^3$Set are characterized by their extensive scale and comprehensive detail,\nusually encompassing over 1,000 event types with precise timestamps and\nsupporting multi-level granularity. Currently, F$^3$Set contains several sports\ndatasets, and this framework may be extended to other applications as well. We\nevaluated popular temporal action understanding methods on F$^3$Set, revealing\nsubstantial challenges for existing techniques. Additionally, we propose a new\nmethod, F$^3$ED, for F$^3$ event detections, achieving superior performance.\nThe dataset, model, and benchmark code are available at\nhttps://github.com/F3Set/F3Set.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "ICLR 2025; Website URL: https://lzyandy.github.io/f3set-website/",
    "pdf_url": "http://arxiv.org/pdf/2504.08222v2",
    "published_date": "2025-04-11 03:05:35 UTC",
    "updated_date": "2025-04-15 03:08:41 UTC"
  },
  {
    "arxiv_id": "2504.08850v1",
    "title": "SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting",
    "authors": [
      "Jiaming Xu",
      "Jiayi Pan",
      "Yongkang Zhou",
      "Siming Chen",
      "Jinhao Li",
      "Yaoxiu Lian",
      "Junyi Wu",
      "Guohao Dai"
    ],
    "abstract": "Early exiting has recently emerged as a promising technique for accelerating\nlarge language models (LLMs) by effectively reducing the hardware computation\nand memory access. In this paper, we present SpecEE, a fast LLM inference\nengine with speculative early exiting. (1) At the algorithm level, we propose\nthe speculation-based lightweight predictor design by exploiting the\nprobabilistic correlation between the speculative tokens and the correct\nresults and high parallelism of GPUs. (2) At the system level, we point out\nthat not all layers need a predictor and design the two-level heuristic\npredictor scheduling engine based on skewed distribution and contextual\nsimilarity. (3) At the mapping level, we point out that different decoding\nmethods share the same essential characteristics, and propose the context-aware\nmerged mapping for predictor with efficient GPU implementations to support\nspeculative decoding, and form a framework for various existing orthogonal\nacceleration techniques (e.g., quantization and sparse activation) on cloud and\npersonal computer (PC) scenarios, successfully pushing the Pareto frontier of\naccuracy and speedup. It is worth noting that SpecEE can be applied to any LLM\nby negligible training overhead in advance without affecting the model original\nparameters. Extensive experiments show that SpecEE achieves 2.25x and 2.43x\nspeedup with Llama2-7B on cloud and PC scenarios respectively.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "Accepted by ISCA 2025",
    "pdf_url": "http://arxiv.org/pdf/2504.08850v1",
    "published_date": "2025-04-11 02:38:53 UTC",
    "updated_date": "2025-04-11 02:38:53 UTC"
  },
  {
    "arxiv_id": "2504.08211v1",
    "title": "LLM for Comparative Narrative Analysis",
    "authors": [
      "Leo Kampen",
      "Carlos Rabat Villarreal",
      "Louis Yu",
      "Santu Karmaker",
      "Dongji Feng"
    ],
    "abstract": "In this paper, we conducted a Multi-Perspective Comparative Narrative\nAnalysis (CNA) on three prominent LLMs: GPT-3.5, PaLM2, and Llama2. We applied\nidentical prompts and evaluated their outputs on specific tasks, ensuring an\nequitable and unbiased comparison between various LLMs. Our study revealed that\nthe three LLMs generated divergent responses to the same prompt, indicating\nnotable discrepancies in their ability to comprehend and analyze the given\ntask. Human evaluation was used as the gold standard, evaluating four\nperspectives to analyze differences in LLM performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "5 pages, 4 figures, Appendix included",
    "pdf_url": "http://arxiv.org/pdf/2504.08211v1",
    "published_date": "2025-04-11 02:34:39 UTC",
    "updated_date": "2025-04-11 02:34:39 UTC"
  },
  {
    "arxiv_id": "2504.08210v2",
    "title": "Optimizing Power Grid Topologies with Reinforcement Learning: A Survey of Methods and Challenges",
    "authors": [
      "Erica van der Sar",
      "Alessandro Zocca",
      "Sandjai Bhulai"
    ],
    "abstract": "Power grid operation is becoming increasingly complex due to the rising\nintegration of renewable energy sources and the need for more adaptive control\nstrategies. Reinforcement Learning (RL) has emerged as a promising approach to\npower network control (PNC), offering the potential to enhance decision-making\nin dynamic and uncertain environments. The Learning To Run a Power Network\n(L2RPN) competitions have played a key role in accelerating research by\nproviding standardized benchmarks and problem formulations, leading to rapid\nadvancements in RL-based methods. This survey provides a comprehensive and\nstructured overview of RL applications for power grid topology optimization,\ncategorizing existing techniques, highlighting key design choices, and\nidentifying gaps in current research. Additionally, we present a comparative\nnumerical study evaluating the impact of commonly applied RL-based methods,\noffering insights into their practical effectiveness. By consolidating existing\nresearch and outlining open challenges, this survey aims to provide a\nfoundation for future advancements in RL-driven power grid optimization.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.LG",
      "cs.SY",
      "stat.ML"
    ],
    "primary_category": "eess.SY",
    "comment": "60 pages, 26 figures, preprint",
    "pdf_url": "http://arxiv.org/pdf/2504.08210v2",
    "published_date": "2025-04-11 02:27:30 UTC",
    "updated_date": "2025-05-15 14:22:35 UTC"
  },
  {
    "arxiv_id": "2504.08208v1",
    "title": "How Good Are Large Language Models for Course Recommendation in MOOCs?",
    "authors": [
      "Boxuan Ma",
      "Md Akib Zabed Khan",
      "Tianyuan Yang",
      "Agoritsa Polyzou",
      "Shin'ichi Konomi"
    ],
    "abstract": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and are increasingly being integrated into recommendation\nsystems. However, their potential in educational recommendation systems has yet\nto be fully explored. This paper investigates the use of LLMs as a\ngeneral-purpose recommendation model, leveraging their vast knowledge derived\nfrom large-scale corpora for course recommendation tasks. We explore a variety\nof approaches, ranging from prompt-based methods to more advanced fine-tuning\ntechniques, and compare their performance against traditional recommendation\nmodels. Extensive experiments were conducted on a real-world MOOC dataset,\nevaluating using LLMs as course recommendation systems across key dimensions\nsuch as accuracy, diversity, and novelty. Our results demonstrate that LLMs can\nachieve good performance comparable to traditional models, highlighting their\npotential to enhance educational recommendation systems. These findings pave\nthe way for further exploration and development of LLM-based approaches in the\ncontext of educational recommendations.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08208v1",
    "published_date": "2025-04-11 02:19:26 UTC",
    "updated_date": "2025-04-11 02:19:26 UTC"
  },
  {
    "arxiv_id": "2504.08207v1",
    "title": "DRAFT-ing Architectural Design Decisions using LLMs",
    "authors": [
      "Rudra Dhar",
      "Adyansh Kakran",
      "Amey Karan",
      "Karthik Vaidhyanathan",
      "Vasudeva Varma"
    ],
    "abstract": "Architectural Knowledge Management (AKM) is crucial for software development\nbut remains challenging due to the lack of standardization and high manual\neffort. Architecture Decision Records (ADRs) provide a structured approach to\ncapture Architecture Design Decisions (ADDs), but their adoption is limited due\nto the manual effort involved and insufficient tool support. Our previous work\nhas shown that Large Language Models (LLMs) can assist in generating ADDs.\nHowever, simply prompting the LLM does not produce quality ADDs. Moreover,\nusing third-party LLMs raises privacy concerns, while self-hosting them poses\nresource challenges.\n  To this end, we experimented with different approaches like few-shot,\nretrieval-augmented generation (RAG) and fine-tuning to enhance LLM's ability\nto generate ADDs. Our results show that both techniques improve effectiveness.\nBuilding on this, we propose Domain Specific Retreival Augumented Few Shot Fine\nTuninng, DRAFT, which combines the strengths of all these three approaches for\nmore effective ADD generation. DRAFT operates in two phases: an offline phase\nthat fine-tunes an LLM on generating ADDs augmented with retrieved examples and\nan online phase that generates ADDs by leveraging retrieved ADRs and the\nfine-tuned model.\n  We evaluated DRAFT against existing approaches on a dataset of 4,911 ADRs and\nvarious LLMs and analyzed them using automated metrics and human evaluations.\nResults show DRAFT outperforms all other approaches in effectiveness while\nmaintaining efficiency. Our findings indicate that DRAFT can aid architects in\ndrafting ADDs while addressing privacy and resource constraints.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08207v1",
    "published_date": "2025-04-11 02:19:01 UTC",
    "updated_date": "2025-04-11 02:19:01 UTC"
  },
  {
    "arxiv_id": "2504.08849v1",
    "title": "Exploring Cognitive Attributes in Financial Decision-Making",
    "authors": [
      "Mallika Mainali",
      "Rosina O. Weber"
    ],
    "abstract": "Cognitive attributes are fundamental to metacognition, shaping how\nindividuals process information, evaluate choices, and make decisions. To\ndevelop metacognitive artificial intelligence (AI) models that reflect human\nreasoning, it is essential to account for the attributes that influence\nreasoning patterns and decision-maker behavior, often leading to different or\neven conflicting choices. This makes it crucial to incorporate cognitive\nattributes in designing AI models that align with human decision-making\nprocesses, especially in high-stakes domains such as finance, where decisions\nhave significant real-world consequences. However, existing AI alignment\nresearch has primarily focused on value alignment, often overlooking the role\nof individual cognitive attributes that distinguish decision-makers. To address\nthis issue, this paper (1) analyzes the literature on cognitive attributes, (2)\nestablishes five criteria for defining them, and (3) categorizes 19\ndomain-specific cognitive attributes relevant to financial decision-making.\nThese three components provide a strong basis for developing AI systems that\naccurately reflect and align with human decision-making processes in financial\ncontexts.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.CY",
    "comment": "7 pages, 2 figures. Presented in SIAM International Conference on\n  Data Mining (SDM25) METACOG-25: 2nd Workshop on Metacognitive Prediction of\n  AI Behavior",
    "pdf_url": "http://arxiv.org/pdf/2504.08849v1",
    "published_date": "2025-04-11 02:11:46 UTC",
    "updated_date": "2025-04-11 02:11:46 UTC"
  },
  {
    "arxiv_id": "2504.08201v3",
    "title": "Neural Encoding and Decoding at Scale",
    "authors": [
      "Yizi Zhang",
      "Yanchen Wang",
      "Mehdi Azabou",
      "Alexandre Andre",
      "Zixuan Wang",
      "Hanrui Lyu",
      "The International Brain Laboratory",
      "Eva Dyer",
      "Liam Paninski",
      "Cole Hurwitz"
    ],
    "abstract": "Recent work has demonstrated that large-scale, multi-animal models are\npowerful tools for characterizing the relationship between neural activity and\nbehavior. Current large-scale approaches, however, focus exclusively on either\npredicting neural activity from behavior (encoding) or predicting behavior from\nneural activity (decoding), limiting their ability to capture the bidirectional\nrelationship between neural activity and behavior. To bridge this gap, we\nintroduce a multimodal, multi-task model that enables simultaneous Neural\nEncoding and Decoding at Scale (NEDS). Central to our approach is a novel\nmulti-task-masking strategy, which alternates between neural, behavioral,\nwithin-modality, and cross-modality masking. We pretrain our method on the\nInternational Brain Laboratory (IBL) repeated site dataset, which includes\nrecordings from 83 animals performing the same visual decision-making task. In\ncomparison to other large-scale models, we demonstrate that NEDS achieves\nstate-of-the-art performance for both encoding and decoding when pretrained on\nmulti-animal data and then fine-tuned on new animals. Surprisingly, NEDS's\nlearned embeddings exhibit emergent properties: even without explicit training,\nthey are highly predictive of the brain regions in each recording. Altogether,\nour approach is a step towards a foundation model of the brain that enables\nseamless translation between neural activity and behavior.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08201v3",
    "published_date": "2025-04-11 02:06:20 UTC",
    "updated_date": "2025-04-20 20:44:18 UTC"
  },
  {
    "arxiv_id": "2504.08200v1",
    "title": "Influential Bandits: Pulling an Arm May Change the Environment",
    "authors": [
      "Ryoma Sato",
      "Shinji Ito"
    ],
    "abstract": "While classical formulations of multi-armed bandit problems assume that each\narm's reward is independent and stationary, real-world applications often\ninvolve non-stationary environments and interdependencies between arms. In\nparticular, selecting one arm may influence the future rewards of other arms, a\nscenario not adequately captured by existing models such as rotting bandits or\nrestless bandits. To address this limitation, we propose the influential bandit\nproblem, which models inter-arm interactions through an unknown, symmetric,\npositive semi-definite interaction matrix that governs the dynamics of arm\nlosses. We formally define this problem and establish two regret lower bounds,\nincluding a superlinear $\\Omega(T^2 / \\log^2 T)$ bound for the standard UCB\nalgorithm and an algorithm-independent $\\Omega(T)$ bound, which highlight the\ninherent difficulty of the setting. We then introduce a new algorithm based on\na lower confidence bound (LCB) estimator tailored to the structure of the loss\ndynamics. Under mild assumptions, our algorithm achieves a regret of $O(KT \\log\nT)$, which is nearly optimal in terms of its dependence on the time horizon.\nThe algorithm is simple to implement and computationally efficient. Empirical\nevaluations on both synthetic and real-world datasets demonstrate the presence\nof inter-arm influence and confirm the superior performance of our method\ncompared to conventional bandit algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08200v1",
    "published_date": "2025-04-11 02:05:51 UTC",
    "updated_date": "2025-04-11 02:05:51 UTC"
  },
  {
    "arxiv_id": "2504.08848v1",
    "title": "X-Guard: Multilingual Guard Agent for Content Moderation",
    "authors": [
      "Bibek Upadhayay",
      "Vahid Behzadan",
      "Ph. D"
    ],
    "abstract": "Large Language Models (LLMs) have rapidly become integral to numerous\napplications in critical domains where reliability is paramount. Despite\nsignificant advances in safety frameworks and guardrails, current protective\nmeasures exhibit crucial vulnerabilities, particularly in multilingual\ncontexts. Existing safety systems remain susceptible to adversarial attacks in\nlow-resource languages and through code-switching techniques, primarily due to\ntheir English-centric design. Furthermore, the development of effective\nmultilingual guardrails is constrained by the scarcity of diverse cross-lingual\ntraining data. Even recent solutions like Llama Guard-3, while offering\nmultilingual support, lack transparency in their decision-making processes. We\naddress these challenges by introducing X-Guard agent, a transparent\nmultilingual safety agent designed to provide content moderation across diverse\nlinguistic contexts. X-Guard effectively defends against both conventional\nlow-resource language attacks and sophisticated code-switching attacks. Our\napproach includes: curating and enhancing multiple open-source safety datasets\nwith explicit evaluation rationales; employing a jury of judges methodology to\nmitigate individual judge LLM provider biases; creating a comprehensive\nmultilingual safety dataset spanning 132 languages with 5 million data points;\nand developing a two-stage architecture combining a custom-finetuned mBART-50\ntranslation module with an evaluation X-Guard 3B model trained through\nsupervised finetuning and GRPO training. Our empirical evaluations demonstrate\nX-Guard's effectiveness in detecting unsafe content across multiple languages\nwhile maintaining transparency throughout the safety evaluation process. Our\nwork represents a significant advancement in creating robust, transparent, and\nlinguistically inclusive safety systems for LLMs and its integrated systems.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "34 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08848v1",
    "published_date": "2025-04-11 01:58:06 UTC",
    "updated_date": "2025-04-11 01:58:06 UTC"
  },
  {
    "arxiv_id": "2504.08195v1",
    "title": "Graph Based Deep Reinforcement Learning Aided by Transformers for Multi-Agent Cooperation",
    "authors": [
      "Michael Elrod",
      "Niloufar Mehrabi",
      "Rahul Amin",
      "Manveen Kaur",
      "Long Cheng",
      "Jim Martin",
      "Abolfazl Razi"
    ],
    "abstract": "Mission planning for a fleet of cooperative autonomous drones in applications\nthat involve serving distributed target points, such as disaster response,\nenvironmental monitoring, and surveillance, is challenging, especially under\npartial observability, limited communication range, and uncertain environments.\nTraditional path-planning algorithms struggle in these scenarios, particularly\nwhen prior information is not available. To address these challenges, we\npropose a novel framework that integrates Graph Neural Networks (GNNs), Deep\nReinforcement Learning (DRL), and transformer-based mechanisms for enhanced\nmulti-agent coordination and collective task execution. Our approach leverages\nGNNs to model agent-agent and agent-goal interactions through adaptive graph\nconstruction, enabling efficient information aggregation and decision-making\nunder constrained communication. A transformer-based message-passing mechanism,\naugmented with edge-feature-enhanced attention, captures complex interaction\npatterns, while a Double Deep Q-Network (Double DQN) with prioritized\nexperience replay optimizes agent policies in partially observable\nenvironments. This integration is carefully designed to address specific\nrequirements of multi-agent navigation, such as scalability, adaptability, and\nefficient task execution. Experimental results demonstrate superior\nperformance, with 90% service provisioning and 100% grid coverage (node\ndiscovery), while reducing the average steps per episode to 200, compared to\n600 for benchmark methods such as particle swarm optimization (PSO), greedy\nalgorithms and DQN.",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "primary_category": "cs.MA",
    "comment": "6 pages, 7 figures, Accepted to the 2025 IEEE International\n  Conference on Communications Workshops (ICC Workshops)",
    "pdf_url": "http://arxiv.org/pdf/2504.08195v1",
    "published_date": "2025-04-11 01:46:18 UTC",
    "updated_date": "2025-04-11 01:46:18 UTC"
  },
  {
    "arxiv_id": "2504.08846v1",
    "title": "AI-University: An LLM-based platform for instructional alignment to scientific classrooms",
    "authors": [
      "Mostafa Faghih Shojaei",
      "Rahul Gulati",
      "Benjamin A. Jasperson",
      "Shangshang Wang",
      "Simone Cimolato",
      "Dangli Cao",
      "Willie Neiswanger",
      "Krishna Garikipati"
    ],
    "abstract": "We introduce AI University (AI-U), a flexible framework for AI-driven course\ncontent delivery that adapts to instructors' teaching styles. At its core, AI-U\nfine-tunes a large language model (LLM) with retrieval-augmented generation\n(RAG) to generate instructor-aligned responses from lecture videos, notes, and\ntextbooks. Using a graduate-level finite-element-method (FEM) course as a case\nstudy, we present a scalable pipeline to systematically construct training\ndata, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and\noptimize its responses through RAG-based synthesis. Our evaluation - combining\ncosine similarity, LLM-based assessment, and expert review - demonstrates\nstrong alignment with course materials. We also have developed a prototype web\napplication, available at https://my-ai-university.com, that enhances\ntraceability by linking AI-generated responses to specific sections of the\nrelevant course material and time-stamped instances of the open-access video\nlectures. Our expert model is found to have greater cosine similarity with a\nreference on 86% of test cases. An LLM judge also found our expert model to\noutperform the base Llama 3.2 model approximately four times out of five. AI-U\noffers a scalable approach to AI-assisted education, paving the way for broader\nadoption in higher education. Here, our framework has been presented in the\nsetting of a class on FEM - a subject that is central to training PhD and\nMaster students in engineering science. However, this setting is a particular\ninstance of a broader context: fine-tuning LLMs to research content in science.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CY",
    "comment": "10 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2504.08846v1",
    "published_date": "2025-04-11 01:26:34 UTC",
    "updated_date": "2025-04-11 01:26:34 UTC"
  },
  {
    "arxiv_id": "2504.08192v1",
    "title": "SAEs $\\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs",
    "authors": [
      "Aashiq Muhamed",
      "Jacopo Bonato",
      "Mona Diab",
      "Virginia Smith"
    ],
    "abstract": "Machine unlearning is a promising approach to improve LLM safety by removing\nunwanted knowledge from the model. However, prevailing gradient-based\nunlearning methods suffer from issues such as high computational costs,\nhyperparameter instability, poor sequential unlearning capability,\nvulnerability to relearning attacks, low data efficiency, and lack of\ninterpretability. While Sparse Autoencoders are well-suited to improve these\naspects by enabling targeted activation-based unlearning, prior approaches\nunderperform gradient-based methods. This work demonstrates that, contrary to\nthese earlier findings, SAEs can significantly improve unlearning when employed\ndynamically. We introduce $\\textbf{Dynamic DAE Guardrails}$ (DSG), a novel\nmethod for precision unlearning that leverages principled feature selection and\na dynamic classifier. Our experiments show DSG substantially outperforms\nleading unlearning methods, achieving superior forget-utility trade-offs. DSG\naddresses key drawbacks of gradient-based approaches for unlearning -- offering\nenhanced computational efficiency and stability, robust performance in\nsequential unlearning, stronger resistance to relearning attacks, better data\nefficiency including zero-shot settings, and more interpretable unlearning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08192v1",
    "published_date": "2025-04-11 01:24:03 UTC",
    "updated_date": "2025-04-11 01:24:03 UTC"
  },
  {
    "arxiv_id": "2504.10519v1",
    "title": "Toward Super Agent System with Hybrid AI Routers",
    "authors": [
      "Yuhang Yao",
      "Haixin Wang",
      "Yibo Chen",
      "Jiawen Wang",
      "Min Chang Jordan Ren",
      "Bosheng Ding",
      "Salman Avestimehr",
      "Chaoyang He"
    ],
    "abstract": "AI Agents powered by Large Language Models are transforming the world through\nenormous applications. A super agent has the potential to fulfill diverse user\nneeds, such as summarization, coding, and research, by accurately understanding\nuser intent and leveraging the appropriate tools to solve tasks. However, to\nmake such an agent viable for real-world deployment and accessible at scale,\nsignificant optimizations are required to ensure high efficiency and low cost.\nThis paper presents a design of the Super Agent System. Upon receiving a user\nprompt, the system first detects the intent of the user, then routes the\nrequest to specialized task agents with the necessary tools or automatically\ngenerates agentic workflows. In practice, most applications directly serve as\nAI assistants on edge devices such as phones and robots. As different language\nmodels vary in capability and cloud-based models often entail high\ncomputational costs, latency, and privacy concerns, we then explore the hybrid\nmode where the router dynamically selects between local and cloud models based\non task complexity. Finally, we introduce the blueprint of an on-device super\nagent enhanced with cloud. With advances in multi-modality models and edge\nhardware, we envision that most computations can be handled locally, with cloud\ncollaboration only as needed. Such architecture paves the way for super agents\nto be seamlessly integrated into everyday life in the near future.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.10519v1",
    "published_date": "2025-04-11 00:54:56 UTC",
    "updated_date": "2025-04-11 00:54:56 UTC"
  },
  {
    "arxiv_id": "2504.08181v1",
    "title": "TokenMotion: Decoupled Motion Control via Token Disentanglement for Human-centric Video Generation",
    "authors": [
      "Ruineng Li",
      "Daitao Xing",
      "Huiming Sun",
      "Yuanzhou Ha",
      "Jinglin Shen",
      "Chiuman Ho"
    ],
    "abstract": "Human-centric motion control in video generation remains a critical\nchallenge, particularly when jointly controlling camera movements and human\nposes in scenarios like the iconic Grammy Glambot moment. While recent video\ndiffusion models have made significant progress, existing approaches struggle\nwith limited motion representations and inadequate integration of camera and\nhuman motion controls. In this work, we present TokenMotion, the first\nDiT-based video diffusion framework that enables fine-grained control over\ncamera motion, human motion, and their joint interaction. We represent camera\ntrajectories and human poses as spatio-temporal tokens to enable local control\ngranularity. Our approach introduces a unified modeling framework utilizing a\ndecouple-and-fuse strategy, bridged by a human-aware dynamic mask that\neffectively handles the spatially-and-temporally varying nature of combined\nmotion signals. Through extensive experiments, we demonstrate TokenMotion's\neffectiveness across both text-to-video and image-to-video paradigms,\nconsistently outperforming current state-of-the-art methods in human-centric\nmotion control tasks. Our work represents a significant advancement in\ncontrollable video generation, with particular relevance for creative\nproduction applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08181v1",
    "published_date": "2025-04-11 00:41:25 UTC",
    "updated_date": "2025-04-11 00:41:25 UTC"
  },
  {
    "arxiv_id": "2504.08177v1",
    "title": "SynthFM: Training Modality-agnostic Foundation Models for Medical Image Segmentation without Real Medical Data",
    "authors": [
      "Sourya Sengupta",
      "Satrajit Chakrabarty",
      "Keerthi Sravan Ravi",
      "Gopal Avinash",
      "Ravi Soni"
    ],
    "abstract": "Foundation models like the Segment Anything Model (SAM) excel in zero-shot\nsegmentation for natural images but struggle with medical image segmentation\ndue to differences in texture, contrast, and noise. Annotating medical images\nis costly and requires domain expertise, limiting large-scale annotated data\navailability. To address this, we propose SynthFM, a synthetic data generation\nframework that mimics the complexities of medical images, enabling foundation\nmodels to adapt without real medical data. Using SAM's pretrained encoder and\ntraining the decoder from scratch on SynthFM's dataset, we evaluated our method\non 11 anatomical structures across 9 datasets (CT, MRI, and Ultrasound).\nSynthFM outperformed zero-shot baselines like SAM and MedSAM, achieving\nsuperior results under different prompt settings and on out-of-distribution\ndatasets.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2504.08177v1",
    "published_date": "2025-04-11 00:14:28 UTC",
    "updated_date": "2025-04-11 00:14:28 UTC"
  }
]
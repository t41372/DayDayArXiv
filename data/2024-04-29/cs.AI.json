{
  "date": "2024-04-29",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-04-29 的 arXiv 中文 TLDR 快报！今天 arXiv 的论文主要聚焦 AI 模型优化、多模态处理、医疗应用和知识图谱等领域，亮点包括 Google 的 Gemini 模型在医学任务中的卓越表现，以及 LLM 微调技术的创新，如 LoRA 方法，涉及知名学者如 Jeff Dean 和 Demis Hassabis 的工作，这些论文突显了 AI 在实际应用中的潜力。\n\n### 重点论文讨论\n我们挑选了最具话题度和影响力的论文，先从 AI 和 LLM 领域入手，再扩展到医疗和多模态处理。以下按主题归类，优先讨论创新性强或有实际影响的文章。\n\n**AI 模型与 LLM 优化（高话题度领域）**  \n- **LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4（LoRA Land: 310 个微调的 LLM 与 GPT-4 竞争）**  \n  这篇论文探索了 Low Rank Adaptation (LoRA) 方法在 LLM 微调中的应用，训练了 310 个模型，展示了它们在各种任务中的性能。贡献在于证明 LoRA 可以高效微调模型，达到与 GPT-4 相当的水平，同时显著降低参数和计算成本。发现显示，LoRA 模型在基准测试中超越了更大模型，暗示了更经济的 AI 部署方式。\n\n- **Capabilities of Gemini Models in Medicine（Gemini 模型在医学中的能力）**  \n  作者包括 Jeff Dean 和 Demis Hassabis（Google DeepMind 核心人物），这篇论文评估了 Gemini 系列模型在医学任务中的表现。核心贡献是引入 Med-Gemini 框架，支持多模态数据处理和网络搜索，在 14 个医学基准上超越 GPT-4，平均提升 44.5%。发现强调了 Gemini 在医疗诊断和多模态推理中的潜力，但需进一步验证安全性和可靠性。\n\n- **DPO Meets PPO: Reinforced Token Optimization for RLHF（DPO Meets PPO: 用于 RLHF 的强化令牌优化）**  \n  这篇论文提出 RTO（Reinforced Token Optimization）算法，将 Direct Preference Optimization (DPO) 与 Proximal Policy Optimization (PPO) 结合，用于强化学习从人类反馈 (RLHF) 中优化 LLM。贡献在于提升了 LLM 在复杂任务中的性能，如 AlpacaEval 2 上比 PPO 提升 7.5%。发现显示，这种方法更高效地处理偏好数据，适用于 LLM 微调。\n\n**医疗应用与生物信息（实际影响强）**  \n- **Bayesian-Guided Generation of Synthetic Microbiomes with Minimized Pathogenicity（使用贝叶斯优化生成合成微生物群落以最小化致病性）**  \n  作者团队包括生物学专家，提出贝叶斯优化框架生成合成微生物群落，用于多药耐药 (MDR) 研究。贡献是结合自编码器和贝叶斯采样，高效筛选低致病性变体。发现显示，该方法显著减少了实验样本需求，促进了微生物组学应用。\n\n- **Machine Unlearning for Document Classification（机器遗忘用于文档分类）**  \n  这篇论文（ICDAR 2024 接受）针对隐私问题，提出机器遗忘框架，用于从文档分类模型中删除特定数据。贡献在于开发了一种高效的遗忘机制，减少对隐私数据的依赖。发现验证了其在医疗图像分析中的实用性，提升了模型的隐私保护。\n\n**多模态处理与知识图谱（创新方法突出）**  \n- **SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval（SpherE: 用于集合检索的可表达和可解释知识图嵌入）**  \n  作者包括 Jingrui He，这篇论文引入 SpherE 模型，使用球形嵌入处理知识图谱中的多对多关系。贡献是提升了集合检索的准确性，同时保持可解释性。发现显示，SpherE 在基准测试中超越传统方法，适用于问答系统。\n\n- **OpenStreetView-5M: The Many Roads to Global Visual Geolocation（OpenStreetView-5M: 通往全球视觉地理定位的多条道路）**  \n  这篇论文（CVPR 2024）发布了一个包含 510 万图像的全球数据集，用于视觉地理定位。贡献在于构建了一个严格的训练-测试分离框架，提升了模型的泛化能力。发现证明了其在真实世界应用的潜力，如街景分析。\n\n其他论文较多，我们快速掠过一些次要的。例如，\"Automated Construction of Theme-specific Knowledge Graphs\"（自动构建主题特定知识图谱）提出无监督框架 TKGCon，用于知识图谱构建，但细节较常规；\"HLSTransform: Energy-Efficient Llama 2 Inference on FPGAs\"（HLSTransform: 使用高级综合的 FPGA 上 Llama 2 推理的能效优化）优化了 LLM 推理能效，但影响局限于硬件领域；\"A Survey on Diffusion Models for Time Series and Spatio-Temporal Data\"（扩散模型在时序和时空数据上的调查）总结了扩散模型的应用，但作为综述，创新性较低。\n\n总之，今天的 arXiv 论文展示了 AI 领域的快速进展，特别是 LLM 在医疗和多模态任务中的应用潜力。感兴趣的读者可关注 Gemini 和 LoRA 相关工作，以探索实际部署的可能性。更多细节请查阅 arXiv！",
  "papers": [
    {
      "arxiv_id": "2404.19146v1",
      "title": "Automated Construction of Theme-specific Knowledge Graphs",
      "title_zh": "自动化构建主题特定知识图谱",
      "authors": [
        "Linyi Ding",
        "Sizhe Zhou",
        "Jinfeng Xiao",
        "Jiawei Han"
      ],
      "abstract": "Despite widespread applications of knowledge graphs (KGs) in various tasks\nsuch as question answering and intelligent conversational systems, existing KGs\nface two major challenges: information granularity and deficiency in\ntimeliness. These hinder considerably the retrieval and analysis of in-context,\nfine-grained, and up-to-date knowledge from KGs, particularly in highly\nspecialized themes (e.g., specialized scientific research) and rapidly evolving\ncontexts (e.g., breaking news or disaster tracking). To tackle such challenges,\nwe propose a theme-specific knowledge graph (i.e., ThemeKG), a KG constructed\nfrom a theme-specific corpus, and design an unsupervised framework for ThemeKG\nconstruction (named TKGCon). The framework takes raw theme-specific corpus and\ngenerates a high-quality KG that includes salient entities and relations under\nthe theme. Specifically, we start with an entity ontology of the theme from\nWikipedia, based on which we then generate candidate relations by Large\nLanguage Models (LLMs) to construct a relation ontology. To parse the documents\nfrom the theme corpus, we first map the extracted entity pairs to the ontology\nand retrieve the candidate relations. Finally, we incorporate the context and\nontology to consolidate the relations for entity pairs. We observe that\ndirectly prompting GPT-4 for theme-specific KG leads to inaccurate entities\n(such as \"two main types\" as one entity in the query result) and unclear (such\nas \"is\", \"has\") or wrong relations (such as \"have due to\", \"to start\"). In\ncontrast, by constructing the theme-specific KG step by step, our model\noutperforms GPT-4 and could consistently identify accurate entities and\nrelations. Experimental results also show that our framework excels in\nevaluations compared with various KG construction baselines.",
      "tldr_zh": "现有知识图谱（KGs）面临信息粒度和时效性不足的挑战，这阻碍了在特定主题（如专业科研）或快速变化的上下文中检索细粒度知识。论文提出一种主题特定知识图谱（ThemeKG），并设计了无监督框架 TKGCon，从主题特定语料库出发，利用 Wikipedia 的实体本体和 Large Language Models (LLMs) 生成关系本体，然后解析文档、映射实体对并整合关系。相比直接使用 GPT-4，TKGCon 能更准确地识别实体和关系，并在实验评估中优于其他 KG 构建基线，展示了其在构建高质量主题知识图谱方面的优势。",
      "categories": [
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.19146v1",
      "published_date": "2024-04-29 23:14:14 UTC",
      "updated_date": "2024-04-29 23:14:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:24:06.866733"
    },
    {
      "arxiv_id": "2404.19130v1",
      "title": "SpherE: Expressive and Interpretable Knowledge Graph Embedding for Set Retrieval",
      "title_zh": "SpherE：表达性强且可解释的知识图谱嵌入用于集合检索",
      "authors": [
        "Zihao Li",
        "Yuyi Ao",
        "Jingrui He"
      ],
      "abstract": "Knowledge graphs (KGs), which store an extensive number of relational facts\n(head, relation, tail), serve various applications. While many downstream tasks\nhighly rely on the expressive modeling and predictive embedding of KGs, most of\nthe current KG representation learning methods, where each entity is embedded\nas a vector in the Euclidean space and each relation is embedded as a\ntransformation, follow an entity ranking protocol. On one hand, such an\nembedding design cannot capture many-to-many relations. On the other hand, in\nmany retrieval cases, the users wish to get an exact set of answers without any\nranking, especially when the results are expected to be precise, e.g., which\ngenes cause an illness. Such scenarios are commonly referred to as \"set\nretrieval\". This work presents a pioneering study on the KG set retrieval\nproblem. We show that the set retrieval highly depends on expressive modeling\nof many-to-many relations, and propose a new KG embedding model SpherE to\naddress this problem. SpherE is based on rotational embedding methods, but each\nentity is embedded as a sphere instead of a vector. While inheriting the high\ninterpretability of rotational-based models, our SpherE can more expressively\nmodel one-to-many, many-to-one, and many-to-many relations. Through extensive\nexperiments, we show that our SpherE can well address the set retrieval problem\nwhile still having a good predictive ability to infer missing facts. The code\nis available at https://github.com/Violet24K/SpherE.",
      "tldr_zh": "知识图谱 (KGs) 的当前嵌入方法通常将实体嵌入为欧式空间向量，主要用于实体排名，但无法有效处理多对多关系和精确集合检索 (set retrieval) 场景，如查询哪些基因导致疾病。论文提出 SpherE 模型，这是一种基于旋转嵌入的创新方法，将实体嵌入为球体 (sphere)，从而更具表达性和可解释性地建模一对多、多对一和多对多关系。实验证明，SpherE 在集合检索任务上显著提升性能，同时保持良好的事实预测能力，并提供了开源代码 (https://github.com/Violet24K/SpherE)。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted by SIGIR 2024, Camera Ready Version",
      "pdf_url": "http://arxiv.org/pdf/2404.19130v1",
      "published_date": "2024-04-29 22:21:24 UTC",
      "updated_date": "2024-04-29 22:21:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:24:17.823834"
    },
    {
      "arxiv_id": "2405.00070v1",
      "title": "Bayesian-Guided Generation of Synthetic Microbiomes with Minimized Pathogenicity",
      "title_zh": "翻译失败",
      "authors": [
        "Nisha Pillai",
        "Bindu Nanduri",
        "Michael J Rothrock Jr.",
        "Zhiqian Chen",
        "Mahalingam Ramkumar"
      ],
      "abstract": "Synthetic microbiomes offer new possibilities for modulating microbiota, to\naddress the barriers in multidtug resistance (MDR) research. We present a\nBayesian optimization approach to enable efficient searching over the space of\nsynthetic microbiome variants to identify candidates predictive of reduced MDR.\nMicrobiome datasets were encoded into a low-dimensional latent space using\nautoencoders. Sampling from this space allowed generation of synthetic\nmicrobiome signatures. Bayesian optimization was then implemented to select\nvariants for biological screening to maximize identification of designs with\nrestricted MDR pathogens based on minimal samples. Four acquisition functions\nwere evaluated: expected improvement, upper confidence bound, Thompson\nsampling, and probability of improvement. Based on each strategy, synthetic\nsamples were prioritized according to their MDR detection. Expected\nimprovement, upper confidence bound, and probability of improvement\nconsistently produced synthetic microbiome candidates with significantly fewer\nsearches than Thompson sampling. By combining deep latent space mapping and\nBayesian learning for efficient guided screening, this study demonstrated the\nfeasibility of creating bespoke synthetic microbiomes with customized MDR\nprofiles.",
      "tldr_zh": "本研究提出了一种基于Bayesian optimization的方法，用于生成合成微生物群落（synthetic microbiomes），以最小化病原性和减少多重耐药性（MDR）。通过autoencoders将微生物数据集编码到低维潜空间（low-dimensional latent space），并从中采样合成样本，然后应用Bayesian optimization及其四种获取函数（expected improvement、upper confidence bound、Thompson sampling和probability of improvement）进行高效筛选。结果显示，expected improvement、upper confidence bound和probability of improvement等函数比Thompson sampling需要更少的搜索，就能识别出有效的MDR减少候选。总体上，此方法结合深度潜空间映射和Bayesian学习，证明了创建自定义MDR配置文件的人造微生物群落的 feasibility。",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "primary_category": "q-bio.QM",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.00070v1",
      "published_date": "2024-04-29 21:30:30 UTC",
      "updated_date": "2024-04-29 21:30:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:24:31.331212"
    },
    {
      "arxiv_id": "2405.00738v1",
      "title": "HLSTransform: Energy-Efficient Llama 2 Inference on FPGAs Via High Level Synthesis",
      "title_zh": "翻译失败",
      "authors": [
        "Andy He",
        "Darren Key",
        "Mason Bulling",
        "Andrew Chang",
        "Skyler Shapiro",
        "Everett Lee"
      ],
      "abstract": "Graphics Processing Units (GPUs) have become the leading hardware accelerator\nfor deep learning applications and are used widely in training and inference of\ntransformers; transformers have achieved state-of-the-art performance in many\nareas of machine learning and are especially used in most modern Large Language\nModels (LLMs). However, GPUs require large amounts of energy, which poses\nenvironmental concerns, demands high operational costs, and causes GPUs to be\nunsuitable for edge computing. We develop an accelerator for transformers,\nnamely, Llama 2, an open-source state-of-the-art LLM, using high level\nsynthesis (HLS) on Field Programmable Gate Arrays (FPGAs). HLS allows us to\nrapidly prototype FPGA designs without writing code at the register-transfer\nlevel (RTL). We name our method HLSTransform, and the FPGA designs we\nsynthesize with HLS achieve up to a 12.75x reduction and 8.25x reduction in\nenergy used per token on the Xilinx Virtex UltraScale+ VU9P FPGA compared to an\nIntel Xeon Broadwell E5-2686 v4 CPU and NVIDIA RTX 3090 GPU respectively, while\nincreasing inference speeds by up to 2.46x compared to CPU and maintaining\n0.53x the speed of an RTX 3090 GPU despite the GPU's 4 times higher base clock\nrate. With the lack of existing open-source FPGA accelerators for transformers,\nwe open-source our code and document our steps for synthesis. We hope this work\nwill serve as a step in democratizing the use of FPGAs in transformer inference\nand inspire research into energy-efficient inference methods as a whole. The\ncode can be found on https://github.com/HLSTransform/submission.",
      "tldr_zh": "本文提出 HLSTransform，一种通过 High Level Synthesis (HLS) 在 Field Programmable Gate Arrays (FPGAs) 上实现 Llama 2 推理的能源高效加速器，旨在解决 GPUs 高能源消耗的问题。实验结果显示，在 Xilinx Virtex UltraScale+ VU9P FPGA 上，该方法每 token 能源消耗比 Intel Xeon CPU 减少 12.75 倍、比 NVIDIA RTX 3090 GPU 减少 8.25 倍，同时推理速度比 CPU 提升 2.46 倍。作者开源了代码（https://github.com/HLSTransform/submission），以推动 FPGAs 在 transformer 推理中的应用和整体能源高效研究。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AR",
      "comment": "7 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2405.00738v1",
      "published_date": "2024-04-29 21:26:06 UTC",
      "updated_date": "2024-04-29 21:26:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:24:44.372183"
    },
    {
      "arxiv_id": "2405.01592v1",
      "title": "Text and Audio Simplification: Human vs. ChatGPT",
      "title_zh": "翻译失败",
      "authors": [
        "Gondy Leroy",
        "David Kauchak",
        "Philip Harber",
        "Ankit Pal",
        "Akash Shukla"
      ],
      "abstract": "Text and audio simplification to increase information comprehension are\nimportant in healthcare. With the introduction of ChatGPT, an evaluation of its\nsimplification performance is needed. We provide a systematic comparison of\nhuman and ChatGPT simplified texts using fourteen metrics indicative of text\ndifficulty. We briefly introduce our online editor where these simplification\ntools, including ChatGPT, are available. We scored twelve corpora using our\nmetrics: six text, one audio, and five ChatGPT simplified corpora. We then\ncompare these corpora with texts simplified and verified in a prior user study.\nFinally, a medical domain expert evaluated these texts and five, new ChatGPT\nsimplified versions. We found that simple corpora show higher similarity with\nthe human simplified texts. ChatGPT simplification moves metrics in the right\ndirection. The medical domain expert evaluation showed a preference for the\nChatGPT style, but the text itself was rated lower for content retention.",
      "tldr_zh": "这篇论文比较了人类和ChatGPT在文本和音频简化方面的性能，特别是在医疗保健领域以提升信息理解。研究者使用14个文本难度指标对12个语料库进行系统评估，包括原始文本、音频和ChatGPT简化的版本，并引入了一个在线编辑器来提供这些简化工具。结果显示，简单语料更接近人类简化文本，而ChatGPT的简化虽使指标向积极方向移动，但医疗领域专家更偏好其风格，却认为内容保留较差。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "H.4"
      ],
      "primary_category": "cs.CL",
      "comment": "AMIA Summit, Boston, 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.01592v1",
      "published_date": "2024-04-29 21:00:33 UTC",
      "updated_date": "2024-04-29 21:00:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:24:54.003771"
    },
    {
      "arxiv_id": "2404.19100v2",
      "title": "Predicting Fairness of ML Software Configurations",
      "title_zh": "预测 ML 软件配置的公平性",
      "authors": [
        "Salvador Robles Herrera",
        "Verya Monjezi",
        "Vladik Kreinovich",
        "Ashutosh Trivedi",
        "Saeid Tizpaz-Niari"
      ],
      "abstract": "This paper investigates the relationships between hyperparameters of machine\nlearning and fairness. Data-driven solutions are increasingly used in critical\nsocio-technical applications where ensuring fairness is important. Rather than\nexplicitly encoding decision logic via control and data structures, the ML\ndevelopers provide input data, perform some pre-processing, choose ML\nalgorithms, and tune hyperparameters (HPs) to infer a program that encodes the\ndecision logic. Prior works report that the selection of HPs can significantly\ninfluence fairness. However, tuning HPs to find an ideal trade-off between\naccuracy, precision, and fairness has remained an expensive and tedious task.\nCan we predict fairness of HP configuration for a given dataset? Are the\npredictions robust to distribution shifts?\n  We focus on group fairness notions and investigate the HP space of 5 training\nalgorithms. We first find that tree regressors and XGBoots significantly\noutperformed deep neural networks and support vector machines in accurately\npredicting the fairness of HPs. When predicting the fairness of ML\nhyperparameters under temporal distribution shift, the tree regressors\noutperforms the other algorithms with reasonable accuracy. However, the\nprecision depends on the ML training algorithm, dataset, and protected\nattributes. For example, the tree regressor model was robust for training data\nshift from 2014 to 2018 on logistic regression and discriminant analysis HPs\nwith sex as the protected attribute; but not for race and other training\nalgorithms. Our method provides a sound framework to efficiently perform\nfine-tuning of ML training algorithms and understand the relationships between\nHPs and fairness.",
      "tldr_zh": "这篇论文探讨了机器学习超参数（HPs）和公平性之间的关系，旨在预测给定数据集的 HPs 配置是否能实现公平决策，从而避免手动调参的繁琐过程。研究者聚焦于群体公平性，调查了5个训练算法的HP空间，发现树回归器和XGBoost在预测公平性方面显著优于深度神经网络和支持向量机，尤其在处理时间分布偏移时表现出合理的准确性。实验结果显示，预测的稳健性取决于具体的ML算法、数据集和保护属性，例如树回归器在某些场景（如2014-2018数据转移、逻辑回归和性别属性）下表现稳健，但对种族或其他算法则较弱。该方法提供了一个高效框架，用于微调ML算法并揭示HPs与公平性的内在关系。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CY",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "To Appear in the 20th International Conference on Predictive Models\n  and Data Analytics in Software Engineering (PROMISE'24)",
      "pdf_url": "http://arxiv.org/pdf/2404.19100v2",
      "published_date": "2024-04-29 20:43:42 UTC",
      "updated_date": "2024-07-01 16:16:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:25:06.937972"
    },
    {
      "arxiv_id": "2404.19093v1",
      "title": "Large Language Models as Conversational Movie Recommenders: A User Study",
      "title_zh": "大型语言模型作为对话式电影推荐器：一项用户研究",
      "authors": [
        "Ruixuan Sun",
        "Xinyi Li",
        "Avinash Akella",
        "Joseph A. Konstan"
      ],
      "abstract": "This paper explores the effectiveness of using large language models (LLMs)\nfor personalized movie recommendations from users' perspectives in an online\nfield experiment. Our study involves a combination of between-subject prompt\nand historic consumption assessments, along with within-subject recommendation\nscenario evaluations. By examining conversation and survey response data from\n160 active users, we find that LLMs offer strong recommendation explainability\nbut lack overall personalization, diversity, and user trust. Our results also\nindicate that different personalized prompting techniques do not significantly\naffect user-perceived recommendation quality, but the number of movies a user\nhas watched plays a more significant role. Furthermore, LLMs show a greater\nability to recommend lesser-known or niche movies. Through qualitative\nanalysis, we identify key conversational patterns linked to positive and\nnegative user interaction experiences and conclude that providing personal\ncontext and examples is crucial for obtaining high-quality recommendations from\nLLMs.",
      "tldr_zh": "这篇论文通过在线实地实验评估了大型语言模型 (LLMs) 作为对话式电影推荐器的有效性，涉及 160 名活跃用户的 between-subject prompt 和 within-subject 推荐场景评估。研究发现，LLMs 在推荐解释性方面表现出色，但缺乏整体个性化、多样性和用户信任，且不同个性化提示技术对用户感知的推荐质量影响不大。结果显示，用户观看电影的数量是关键因素，而 LLMs 更擅长推荐小众或 niche 电影。通过定性分析，论文强调提供个人上下文和示例是获得高质量推荐的关键。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.19093v1",
      "published_date": "2024-04-29 20:17:06 UTC",
      "updated_date": "2024-04-29 20:17:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:25:18.274862"
    },
    {
      "arxiv_id": "2404.19087v2",
      "title": "Deep Reinforcement Learning for Advanced Longitudinal Control and Collision Avoidance in High-Risk Driving Scenarios",
      "title_zh": "翻译失败",
      "authors": [
        "Dianwei Chen",
        "Yaobang Gong",
        "Xianfeng Yang"
      ],
      "abstract": "Existing Advanced Driver Assistance Systems primarily focus on the vehicle\ndirectly ahead, often overlooking potential risks from following vehicles. This\noversight can lead to ineffective handling of high risk situations, such as\nhigh speed, closely spaced, multi vehicle scenarios where emergency braking by\none vehicle might trigger a pile up collision. To overcome these limitations,\nthis study introduces a novel deep reinforcement learning based algorithm for\nlongitudinal control and collision avoidance. This proposed algorithm\neffectively considers the behavior of both leading and following vehicles. Its\nimplementation in simulated high risk scenarios, which involve emergency\nbraking in dense traffic where traditional systems typically fail, has\ndemonstrated the algorithm ability to prevent potential pile up collisions,\nincluding those involving heavy duty vehicles.",
      "tldr_zh": "本研究针对现有高级驾驶辅助系统（Advanced Driver Assistance Systems）忽略后续车辆风险的问题，提出了一种基于深度强化学习（Deep Reinforcement Learning）的算法，用于高级纵向控制和碰撞避免。该算法同时考虑领先和后续车辆的行为，能够有效处理高风险驾驶场景，如高速密集交通中的紧急制动。实验结果显示，在模拟环境中，该算法成功防止了潜在的连锁碰撞，包括涉及重型车辆的情况，从而提升了驾驶安全性和可靠性。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.19087v2",
      "published_date": "2024-04-29 19:58:34 UTC",
      "updated_date": "2025-02-21 19:43:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:25:28.962237"
    },
    {
      "arxiv_id": "2404.19076v1",
      "title": "Who Followed the Blueprint? Analyzing the Responses of U.S. Federal Agencies to the Blueprint for an AI Bill of Rights",
      "title_zh": "翻译失败",
      "authors": [
        "Darren Lage",
        "Riley Pruitt",
        "Jason Ross Arnold"
      ],
      "abstract": "This study examines the extent to which U.S. federal agencies responded to\nand implemented the principles outlined in the White House's October 2022\n\"Blueprint for an AI Bill of Rights.\" The Blueprint provided a framework for\nthe ethical governance of artificial intelligence systems, organized around\nfive core principles: safety and effectiveness, protection against algorithmic\ndiscrimination, data privacy, notice and explanation about AI systems, and\nhuman alternatives and fallback.\n  Through an analysis of publicly available records across 15 federal\ndepartments, the authors found limited evidence that the Blueprint directly\ninfluenced agency actions after its release. Only five departments explicitly\nmentioned the Blueprint, while 12 took steps aligned with one or more of its\nprinciples. However, much of this work appeared to have precedents predating\nthe Blueprint or motivations disconnected from it, such as compliance with\nprior executive orders on trustworthy AI. Departments' activities often\nemphasized priorities like safety, accountability and transparency that\noverlapped with Blueprint principles, but did not necessarily stem from it.\n  The authors conclude that the non-binding Blueprint seems to have had minimal\nimpact on shaping the U.S. government's approach to ethical AI governance in\nits first year. Factors like public concerns after high-profile AI releases and\nobligations to follow direct executive orders likely carried more influence\nover federal agencies. More rigorous study would be needed to definitively\nassess the Blueprint's effects within the federal bureaucracy and broader\nsociety.",
      "tldr_zh": "这篇论文分析了美国联邦机构对白宫2022年10月发布的《Blueprint for an AI Bill of Rights》蓝图的响应情况，通过审查15个部门的公开记录，评估其在安全性和有效性、防范算法歧视、数据隐私、AI系统通知与解释以及人类备选方案等五大原则上的实施程度。研究发现，只有5个部门明确提及蓝图，而12个部门采取了相关行动，但这些举措往往源于先前的行政命令或其他动机，而非蓝图直接影响。作者结论认为，该非强制性蓝图在第一年对美国政府伦理AI治理的影响有限，公众担忧和高优先级行政命令可能发挥更大作用，并建议进行更严格的研究以评估其整体效果。",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.19076v1",
      "published_date": "2024-04-29 19:43:10 UTC",
      "updated_date": "2024-04-29 19:43:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:25:43.501121"
    },
    {
      "arxiv_id": "2404.19075v2",
      "title": "Distributed Stochastic Optimization of a Neural Representation Network for Time-Space Tomography Reconstruction",
      "title_zh": "翻译失败",
      "authors": [
        "K. Aditya Mohan",
        "Massimiliano Ferrucci",
        "Chuck Divin",
        "Garrett A. Stevenson",
        "Hyojin Kim"
      ],
      "abstract": "4D time-space reconstruction of dynamic events or deforming objects using\nX-ray computed tomography (CT) is an important inverse problem in\nnon-destructive evaluation. Conventional back-projection based reconstruction\nmethods assume that the object remains static for the duration of several tens\nor hundreds of X-ray projection measurement images (reconstruction of\nconsecutive limited-angle CT scans). However, this is an unrealistic assumption\nfor many in-situ experiments that causes spurious artifacts and inaccurate\nmorphological reconstructions of the object. To solve this problem, we propose\nto perform a 4D time-space reconstruction using a distributed implicit neural\nrepresentation (DINR) network that is trained using a novel distributed\nstochastic training algorithm. Our DINR network learns to reconstruct the\nobject at its output by iterative optimization of its network parameters such\nthat the measured projection images best match the output of the CT forward\nmeasurement model. We use a forward measurement model that is a function of the\nDINR outputs at a sparsely sampled set of continuous valued 4D object\ncoordinates. Unlike previous neural representation architectures that forward\nand back propagate through dense voxel grids that sample the object's entire\ntime-space coordinates, we only propagate through the DINR at a small subset of\nobject coordinates in each iteration resulting in an order-of-magnitude\nreduction in memory and compute for training. DINR leverages distributed\ncomputation across several compute nodes and GPUs to produce high-fidelity 4D\ntime-space reconstructions. We use both simulated parallel-beam and\nexperimental cone-beam X-ray CT datasets to demonstrate the superior\nperformance of our approach.",
      "tldr_zh": "本研究针对 X 射线 CT 的 4D 时间-空间重建问题，提出了一种分布式隐式神经表示 (DINR) 网络及其新型分布式随机训练算法，以解决传统方法假设对象静态导致的伪像和重建不准问题。DINR 通过优化网络参数，使测量的投影图像与 CT 正向测量模型输出匹配，并在稀疏采样坐标上进行传播，从而大幅减少内存和计算资源，并利用分布式计算在多个节点和 GPU 上实现高效重建。与传统方法相比，该方法在模拟平行光束和实验锥光束数据集上表现出色，实现了高保真 4D 重建。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "cs.NA",
        "math.NA"
      ],
      "primary_category": "eess.IV",
      "comment": "accepted for publication at IEEE Transactions in Computational\n  Imaging",
      "pdf_url": "http://arxiv.org/pdf/2404.19075v2",
      "published_date": "2024-04-29 19:41:51 UTC",
      "updated_date": "2025-02-26 00:31:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:25:55.053609"
    },
    {
      "arxiv_id": "2404.19065v1",
      "title": "HELPER-X: A Unified Instructable Embodied Agent to Tackle Four Interactive Vision-Language Domains with Memory-Augmented Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Gabriel Sarch",
        "Sahil Somani",
        "Raghav Kapoor",
        "Michael J. Tarr",
        "Katerina Fragkiadaki"
      ],
      "abstract": "Recent research on instructable agents has used memory-augmented Large\nLanguage Models (LLMs) as task planners, a technique that retrieves\nlanguage-program examples relevant to the input instruction and uses them as\nin-context examples in the LLM prompt to improve the performance of the LLM in\ninferring the correct action and task plans. In this technical report, we\nextend the capabilities of HELPER, by expanding its memory with a wider array\nof examples and prompts, and by integrating additional APIs for asking\nquestions. This simple expansion of HELPER into a shared memory enables the\nagent to work across the domains of executing plans from dialogue, natural\nlanguage instruction following, active question asking, and commonsense room\nreorganization. We evaluate the agent on four diverse interactive\nvisual-language embodied agent benchmarks: ALFRED, TEACh, DialFRED, and the\nTidy Task. HELPER-X achieves few-shot, state-of-the-art performance across\nthese benchmarks using a single agent, without requiring in-domain training,\nand remains competitive with agents that have undergone in-domain training.",
      "tldr_zh": "该研究介绍了HELPER-X，一种统一的指令可控实体代理，使用memory-augmented Large Language Models (LLMs)作为任务规划器，通过扩展记忆库（包括更多示例、提示和API）来处理四个交互视觉语言领域：对话计划执行、自然语言指令遵循、主动提问以及常识房间重组。相比原HELPER，HELPER-X在四个基准（ALFRED、TEACh、DialFRED和Tidy Task）上实现了few-shot状态-of-the-art性能，而无需in-domain训练，并与经过领域训练的代理保持竞争力。该框架展示了如何通过简单扩展共享记忆提升代理的多领域适应性，为通用指令代理的发展提供了新途径。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Videos and code https://helper-agent-llm.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2404.19065v1",
      "published_date": "2024-04-29 19:12:42 UTC",
      "updated_date": "2024-04-29 19:12:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:26:08.368717"
    },
    {
      "arxiv_id": "2407.10239v2",
      "title": "What is Reproducibility in Artificial Intelligence and Machine Learning Research?",
      "title_zh": "什么是人工智能和机器学习研究中的可重复",
      "authors": [
        "Abhyuday Desai",
        "Mohamed Abdelhamid",
        "Nakul R. Padalkar"
      ],
      "abstract": "In the rapidly evolving fields of Artificial Intelligence (AI) and Machine\nLearning (ML), the reproducibility crisis underscores the urgent need for clear\nvalidation methodologies to maintain scientific integrity and encourage\nadvancement. The crisis is compounded by the prevalent confusion over\nvalidation terminology. In response to this challenge, we introduce a framework\nthat clarifies the roles and definitions of key validation efforts:\nrepeatability, dependent and independent reproducibility, and direct and\nconceptual replicability. This structured framework aims to provide AI/ML\nresearchers with the necessary clarity on these essential concepts,\nfacilitating the appropriate design, conduct, and interpretation of validation\nstudies. By articulating the nuances and specific roles of each type of\nvalidation study, we aim to enhance the reliability and trustworthiness of\nresearch findings and support the community's efforts to address\nreproducibility challenges effectively.",
      "tldr_zh": "本论文探讨了人工智能（AI）和机器学习（ML）研究中的再现性危机，该危机源于验证术语的混淆，导致科学完整性受损。作者引入了一个框架，明确定义了关键验证概念，包括repeatability、dependent and independent reproducibility，以及direct and conceptual replicability。框架旨在帮助研究者设计、实施和解释验证研究，从而提升研究结果的可靠性和可信度，并有效应对再现性挑战。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.LG",
        "I.2.m"
      ],
      "primary_category": "cs.CY",
      "comment": "13 pages, 3 figures, 1 table; submitted to AI Magazine",
      "pdf_url": "http://arxiv.org/pdf/2407.10239v2",
      "published_date": "2024-04-29 18:51:20 UTC",
      "updated_date": "2025-03-30 18:44:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:26:18.766993"
    },
    {
      "arxiv_id": "2404.19048v2",
      "title": "A Framework for Real-time Safeguarding the Text Generation of Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Ximing Dong",
        "Dayi Lin",
        "Shaowei Wang",
        "Ahmed E. Hassan"
      ],
      "abstract": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing (NLP) tasks but also pose ethical and societal risks due to their\npropensity to generate harmful content. To address this, various approaches\nhave been developed to safeguard LLMs from producing unsafe content. However,\nexisting methods have limitations, including the need for training specific\ncontrol models and proactive intervention during text generation, that lead to\nquality degradation and increased computational overhead. To mitigate those\nlimitations, we propose LLMSafeGuard, a lightweight framework to safeguard LLM\ntext generation in real-time. LLMSafeGuard integrates an external validator\ninto the beam search algorithm during decoding, rejecting candidates that\nviolate safety constraints while allowing valid ones to proceed. We introduce a\nsimilarity based validation approach, simplifying constraint introduction and\neliminating the need for control model training. Additionally, LLMSafeGuard\nemploys a context-wise timing selection strategy, intervening LLMs only when\nnecessary. We evaluate LLMSafeGuard on two tasks, detoxification and copyright\nsafeguarding, and demonstrate its superior performance over SOTA baselines. For\ninstance, LLMSafeGuard reduces the average toxic score of. LLM output by 29.7%\ncompared to the best baseline meanwhile preserving similar linguistic quality\nas natural output in detoxification task. Similarly, in the copyright task,\nLLMSafeGuard decreases the Longest Common Subsequence (LCS) by 56.2% compared\nto baselines. Moreover, our context-wise timing selection strategy reduces\ninference time by at least 24% meanwhile maintaining comparable effectiveness\nas validating each time step. LLMSafeGuard also offers tunable parameters to\nbalance its effectiveness and efficiency.",
      "tldr_zh": "这篇论文提出 LLMSafeGuard，一种轻量级框架，用于实时保护 Large Language Models (LLMs) 的文本生成，解决现有方法在安全约束引入和计算开销方面的局限性。该框架将外部验证器集成到 beam search 算法中，采用基于相似性的验证方法和上下文-wise 计时策略，仅在必要时干预以拒绝违反安全约束的候选输出。实验结果显示，在解毒任务上，LLMSafeGuard 比 SOTA 基线减少平均毒性分数 29.7% 同时保持类似语言质量，在版权保护任务上降低 Longest Common Subsequence (LCS) 56.2%，并通过可调参数实现至少 24% 的推理时间减少。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.19048v2",
      "published_date": "2024-04-29 18:40:01 UTC",
      "updated_date": "2024-05-01 19:53:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:26:33.245128"
    },
    {
      "arxiv_id": "2404.19038v1",
      "title": "Embedded Representation Learning Network for Animating Styled Video Portrait",
      "title_zh": "嵌入式",
      "authors": [
        "Tianyong Wang",
        "Xiangyu Liang",
        "Wangguandong Zheng",
        "Dan Niu",
        "Haifeng Xia",
        "Siyu Xia"
      ],
      "abstract": "The talking head generation recently attracted considerable attention due to\nits widespread application prospects, especially for digital avatars and 3D\nanimation design. Inspired by this practical demand, several works explored\nNeural Radiance Fields (NeRF) to synthesize the talking heads. However, these\nmethods based on NeRF face two challenges: (1) Difficulty in generating\nstyle-controllable talking heads. (2) Displacement artifacts around the neck in\nrendered images. To overcome these two challenges, we propose a novel\ngenerative paradigm \\textit{Embedded Representation Learning Network} (ERLNet)\nwith two learning stages. First, the \\textit{ audio-driven FLAME} (ADF) module\nis constructed to produce facial expression and head pose sequences\nsynchronized with content audio and style video. Second, given the sequence\ndeduced by the ADF, one novel \\textit{dual-branch fusion NeRF} (DBF-NeRF)\nexplores these contents to render the final images. Extensive empirical studies\ndemonstrate that the collaboration of these two stages effectively facilitates\nour method to render a more realistic talking head than the existing\nalgorithms.",
      "tldr_zh": "本研究针对说话头像（talking head）生成中的挑战，提出了一种新型框架Embedded Representation Learning Network (ERLNet)，旨在解决基于Neural Radiance Fields (NeRF)的现有方法在风格控制和脖子位移伪影方面的不足。ERLNet包括两个阶段：首先，audio-driven FLAME (ADF) 模块根据音频和风格视频生成同步的面部表情和头部姿势序列；其次，dual-branch fusion NeRF (DBF-NeRF) 模块利用这些序列渲染最终图像。实验结果表明，该方法比现有算法生成更真实且高质量的talking head，有效提升了数字头像和3D动画设计的实用性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.19038v1",
      "published_date": "2024-04-29 18:24:55 UTC",
      "updated_date": "2024-04-29 18:24:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:26:43.902493"
    },
    {
      "arxiv_id": "2404.19031v1",
      "title": "Machine Unlearning for Document Classification",
      "title_zh": "翻译失败",
      "authors": [
        "Lei Kang",
        "Mohamed Ali Souibgui",
        "Fei Yang",
        "Lluis Gomez",
        "Ernest Valveny",
        "Dimosthenis Karatzas"
      ],
      "abstract": "Document understanding models have recently demonstrated remarkable\nperformance by leveraging extensive collections of user documents. However,\nsince documents often contain large amounts of personal data, their usage can\npose a threat to user privacy and weaken the bonds of trust between humans and\nAI services. In response to these concerns, legislation advocating ``the right\nto be forgotten\" has recently been proposed, allowing users to request the\nremoval of private information from computer systems and neural network models.\nA novel approach, known as machine unlearning, has emerged to make AI models\nforget about a particular class of data. In our research, we explore machine\nunlearning for document classification problems, representing, to the best of\nour knowledge, the first investigation into this area. Specifically, we\nconsider a realistic scenario where a remote server houses a well-trained model\nand possesses only a small portion of training data. This setup is designed for\nefficient forgetting manipulation. This work represents a pioneering step\ntowards the development of machine unlearning methods aimed at addressing\nprivacy concerns in document analysis applications. Our code is publicly\navailable at\n\\url{https://github.com/leitro/MachineUnlearning-DocClassification}.",
      "tldr_zh": "该研究探讨了机器遗忘（machine unlearning）在文档分类问题中的应用，以解决文档理解模型使用用户数据可能带来的隐私风险和“被遗忘权”挑战。研究首次针对文档分类场景设计了一种方法，其中远程服务器仅使用少量训练数据对已训练模型进行高效遗忘操作。实验结果证明了这一方法的有效性，为文档分析领域的隐私保护提供了开创性框架，并公开了相关代码以供进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICDAR2024",
      "pdf_url": "http://arxiv.org/pdf/2404.19031v1",
      "published_date": "2024-04-29 18:16:13 UTC",
      "updated_date": "2024-04-29 18:16:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:26:54.432691"
    },
    {
      "arxiv_id": "2404.19007v1",
      "title": "How Did We Get Here? Summarizing Conversation Dynamics",
      "title_zh": "翻译失败",
      "authors": [
        "Yilun Hua",
        "Nicholas Chernogor",
        "Yuzhe Gu",
        "Seoyeon Julie Jeong",
        "Miranda Luo",
        "Cristian Danescu-Niculescu-Mizil"
      ],
      "abstract": "Throughout a conversation, the way participants interact with each other is\nin constant flux: their tones may change, they may resort to different\nstrategies to convey their points, or they might alter their interaction\npatterns. An understanding of these dynamics can complement that of the actual\nfacts and opinions discussed, offering a more holistic view of the trajectory\nof the conversation: how it arrived at its current state and where it is likely\nheading.\n  In this work, we introduce the task of summarizing the dynamics of\nconversations, by constructing a dataset of human-written summaries, and\nexploring several automated baselines. We evaluate whether such summaries can\ncapture the trajectory of conversations via an established downstream task:\nforecasting whether an ongoing conversation will eventually derail into toxic\nbehavior. We show that they help both humans and automated systems with this\nforecasting task. Humans make predictions three times faster, and with greater\nconfidence, when reading the summaries than when reading the transcripts.\nFurthermore, automated forecasting systems are more accurate when constructing,\nand then predicting based on, summaries of conversation dynamics, compared to\ndirectly predicting on the transcripts.",
      "tldr_zh": "这篇论文引入了总结对话动态的新任务，旨在捕捉对话中参与者互动方式的变化，如语气、策略和模式，以提供对话轨迹的整体视图。研究者构建了一个包含人类撰写总结的数据集，并探索了几个自动化基线，通过下游任务（预测对话是否会转向 toxic behavior）来评估这些总结的效果。结果显示，阅读总结能让人类预测速度提高三倍、信心更足，而自动化系统基于总结进行预测时比直接分析对话记录更准确，从而提升了对对话未来的理解和预判。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.CL",
      "comment": "To appear in the Proceedings of NAACL 2024. Data available in\n  ConvoKit https://convokit.cornell.edu/",
      "pdf_url": "http://arxiv.org/pdf/2404.19007v1",
      "published_date": "2024-04-29 18:00:03 UTC",
      "updated_date": "2024-04-29 18:00:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:27:08.615714"
    },
    {
      "arxiv_id": "2404.18928v1",
      "title": "Stylus: Automatic Adapter Selection for Diffusion Models",
      "title_zh": "Stylus：针对扩散模型的自动适配器选择",
      "authors": [
        "Michael Luo",
        "Justin Wong",
        "Brandon Trabucco",
        "Yanping Huang",
        "Joseph E. Gonzalez",
        "Zhifeng Chen",
        "Ruslan Salakhutdinov",
        "Ion Stoica"
      ],
      "abstract": "Beyond scaling base models with more data or parameters, fine-tuned adapters\nprovide an alternative way to generate high fidelity, custom images at reduced\ncosts. As such, adapters have been widely adopted by open-source communities,\naccumulating a database of over 100K adapters-most of which are highly\ncustomized with insufficient descriptions. This paper explores the problem of\nmatching the prompt to a set of relevant adapters, built on recent work that\nhighlight the performance gains of composing adapters. We introduce Stylus,\nwhich efficiently selects and automatically composes task-specific adapters\nbased on a prompt's keywords. Stylus outlines a three-stage approach that first\nsummarizes adapters with improved descriptions and embeddings, retrieves\nrelevant adapters, and then further assembles adapters based on prompts'\nkeywords by checking how well they fit the prompt. To evaluate Stylus, we\ndeveloped StylusDocs, a curated dataset featuring 75K adapters with\npre-computed adapter embeddings. In our evaluation on popular Stable Diffusion\ncheckpoints, Stylus achieves greater CLIP-FID Pareto efficiency and is twice as\npreferred, with humans and multimodal models as evaluators, over the base\nmodel. See stylus-diffusion.github.io for more.",
      "tldr_zh": "本论文提出Stylus系统，用于Diffusion Models的自动适配器选择和组合，旨在高效匹配提示(prompt)与相关适配器，从而生成高保真度自定义图像。Stylus采用三阶段方法：首先改进适配器描述和嵌入以进行总结，其次检索相关适配器，最后基于提示关键词评估并组装适配器。实验结果显示，在Stable Diffusion检查点上，Stylus实现了更高的CLIP-FID Pareto效率，并被人类和多模态模型评为比基模型受欢迎程度高两倍。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.GR",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Project Website: https://stylus-diffusion.github.io",
      "pdf_url": "http://arxiv.org/pdf/2404.18928v1",
      "published_date": "2024-04-29 17:59:16 UTC",
      "updated_date": "2024-04-29 17:59:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:27:20.390175"
    },
    {
      "arxiv_id": "2404.18922v3",
      "title": "DPO Meets PPO: Reinforced Token Optimization for RLHF",
      "title_zh": "翻译失败",
      "authors": [
        "Han Zhong",
        "Zikang Shan",
        "Guhao Feng",
        "Wei Xiong",
        "Xinle Cheng",
        "Li Zhao",
        "Di He",
        "Jiang Bian",
        "Liwei Wang"
      ],
      "abstract": "In the classical Reinforcement Learning from Human Feedback (RLHF) framework,\nProximal Policy Optimization (PPO) is employed to learn from sparse,\nsentence-level rewards -- a challenging scenario in traditional deep\nreinforcement learning. Despite the great successes of PPO in the alignment of\nlarge language models, its open-source implementation is still largely\nsub-optimal. To address these issues, we introduce a framework that models RLHF\nproblems as a Markov decision process (MDP), enabling the capture of\nfine-grained token-wise information. Under this framework, we introduce an\nalgorithm Reinforced Token Optimization (\\texttt{RTO}), which learns the\ntoken-wise reward function from preference data and performs policy\noptimization based on this learned token-wise reward signal. Theoretically,\n\\texttt{RTO} is proven to have the capability of finding the near-optimal\npolicy sample-efficiently. For its practical implementation, \\texttt{RTO}\ninnovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,\noriginally derived from sparse sentence rewards, surprisingly provides us with\na token-wise characterization of response quality, which is seamlessly\nincorporated into our subsequent PPO training stage. Extensive experiments\ndemonstrate that \\texttt{RTO} performs better than PPO and other direct\npreference learning algorithms. In particular, RTO outperforms PPO by 7.5\npoints on the AlpacaEval 2 benchmark and by 4.1 points on Arena-Hard. Our code\nand models are available at\n\\href{https://github.com/zkshan2002/RTO}{https://github.com/zkshan2002/RTO}.",
      "tldr_zh": "本研究针对 Reinforcement Learning from Human Feedback (RLHF) 中 Proximal Policy Optimization (PPO) 处理稀疏句子级奖励的不足，提出了一种新框架，将 RLHF 建模为 Markov decision process (MDP)，以捕捉细粒度的 token-wise 信息。Reinforced Token Optimization (RTO) 算法从偏好数据中学习 token-wise 奖励函数，并结合 Direct Preference Optimization (DPO) 和 PPO 进行策略优化，其中 DPO 提供响应质量的 token-wise 表征。实验结果显示，RTO 在 AlpacaEval 2 上比 PPO 高出 7.5 点，在 Arena-Hard 上高出 4.1 点，证明其在样本效率和性能上更优越。理论上，RTO 被证明能高效找到近优策略。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18922v3",
      "published_date": "2024-04-29 17:58:30 UTC",
      "updated_date": "2025-02-11 17:23:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:27:32.909977"
    },
    {
      "arxiv_id": "2404.18891v1",
      "title": "IPixMatch: Boost Semi-supervised Semantic Segmentation with Inter-Pixel Relation",
      "title_zh": "翻译失败",
      "authors": [
        "Kebin Wu",
        "Wenbin Li",
        "Xiaofei Xiao"
      ],
      "abstract": "The scarcity of labeled data in real-world scenarios is a critical bottleneck\nof deep learning's effectiveness. Semi-supervised semantic segmentation has\nbeen a typical solution to achieve a desirable tradeoff between annotation cost\nand segmentation performance. However, previous approaches, whether based on\nconsistency regularization or self-training, tend to neglect the contextual\nknowledge embedded within inter-pixel relations. This negligence leads to\nsuboptimal performance and limited generalization. In this paper, we propose a\nnovel approach IPixMatch designed to mine the neglected but valuable\nInter-Pixel information for semi-supervised learning. Specifically, IPixMatch\nis constructed as an extension of the standard teacher-student network,\nincorporating additional loss terms to capture inter-pixel relations. It shines\nin low-data regimes by efficiently leveraging the limited labeled data and\nextracting maximum utility from the available unlabeled data. Furthermore,\nIPixMatch can be integrated seamlessly into most teacher-student frameworks\nwithout the need of model modification or adding additional components. Our\nstraightforward IPixMatch method demonstrates consistent performance\nimprovements across various benchmark datasets under different partitioning\nprotocols.",
      "tldr_zh": "该论文针对深度学习中标注数据稀缺的问题，提出了一种名为IPixMatch的创新方法，用于提升半监督语义分割（Semi-supervised semantic segmentation）的性能。IPixMatch通过扩展标准的教师-学生网络（teacher-student network），添加额外的损失项来捕捉像素间关系（Inter-Pixel Relation），从而更好地利用上下文知识并改善模型在低数据环境下的泛化能力。该方法无需修改现有框架即可无缝集成，并在多个基准数据集上实现了稳定的性能提升。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "7 pages, 2 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.18891v1",
      "published_date": "2024-04-29 17:27:37 UTC",
      "updated_date": "2024-04-29 17:27:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:27:44.903748"
    },
    {
      "arxiv_id": "2404.18886v3",
      "title": "A Survey on Diffusion Models for Time Series and Spatio-Temporal Data",
      "title_zh": "翻译失败",
      "authors": [
        "Yiyuan Yang",
        "Ming Jin",
        "Haomin Wen",
        "Chaoli Zhang",
        "Yuxuan Liang",
        "Lintao Ma",
        "Yi Wang",
        "Chenghao Liu",
        "Bin Yang",
        "Zenglin Xu",
        "Jiang Bian",
        "Shirui Pan",
        "Qingsong Wen"
      ],
      "abstract": "The study of time series is crucial for understanding trends and anomalies\nover time, enabling predictive insights across various sectors. Spatio-temporal\ndata, on the other hand, is vital for analyzing phenomena in both space and\ntime, providing a dynamic perspective on complex system interactions. Recently,\ndiffusion models have seen widespread application in time series and\nspatio-temporal data mining. Not only do they enhance the generative and\ninferential capabilities for sequential and temporal data, but they also extend\nto other downstream tasks. In this survey, we comprehensively and thoroughly\nreview the use of diffusion models in time series and spatio-temporal data,\ncategorizing them by model category, task type, data modality, and practical\napplication domain. In detail, we categorize diffusion models into\nunconditioned and conditioned types and discuss time series and spatio-temporal\ndata separately. Unconditioned models, which operate unsupervised, are\nsubdivided into probability-based and score-based models, serving predictive\nand generative tasks such as forecasting, anomaly detection, classification,\nand imputation. Conditioned models, on the other hand, utilize extra\ninformation to enhance performance and are similarly divided for both\npredictive and generative tasks. Our survey extensively covers their\napplication in various fields, including healthcare, recommendation, climate,\nenergy, audio, and transportation, providing a foundational understanding of\nhow these models analyze and generate data. Through this structured overview,\nwe aim to provide researchers and practitioners with a comprehensive\nunderstanding of diffusion models for time series and spatio-temporal data\nanalysis, aiming to direct future innovations and applications by addressing\ntraditional challenges and exploring innovative solutions within the diffusion\nmodel framework.",
      "tldr_zh": "这篇论文对diffusion models在time series和spatio-temporal data中的应用进行了全面调查，强调这些模型提升了数据生成和推理能力，并扩展到预测、异常检测、分类和插值等下游任务。论文将diffusion models分为无条件（包括基于概率和基于分数的子类型）和条件类型，并分别讨论了它们在time series和spatio-temporal data上的作用。调查涵盖了医疗、推荐、气候、能源、音频和交通等领域的实际应用，旨在为研究者提供基础理解，指导未来创新以解决传统挑战。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Ongoing work & Under review; 27 pages, 8 figures, 2 tables; Github\n  Repo:\n  https://github.com/yyysjz1997/Awesome-TimeSeries-SpatioTemporal-Diffusion-Model",
      "pdf_url": "http://arxiv.org/pdf/2404.18886v3",
      "published_date": "2024-04-29 17:19:40 UTC",
      "updated_date": "2024-06-11 13:25:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:27:55.357201"
    },
    {
      "arxiv_id": "2404.18873v1",
      "title": "OpenStreetView-5M: The Many Roads to Global Visual Geolocation",
      "title_zh": "Open",
      "authors": [
        "Guillaume Astruc",
        "Nicolas Dufour",
        "Ioannis Siglidis",
        "Constantin Aronssohn",
        "Nacim Bouia",
        "Stephanie Fu",
        "Romain Loiseau",
        "Van Nguyen Nguyen",
        "Charles Raude",
        "Elliot Vincent",
        "Lintao XU",
        "Hongyu Zhou",
        "Loic Landrieu"
      ],
      "abstract": "Determining the location of an image anywhere on Earth is a complex visual\ntask, which makes it particularly relevant for evaluating computer vision\nalgorithms. Yet, the absence of standard, large-scale, open-access datasets\nwith reliably localizable images has limited its potential. To address this\nissue, we introduce OpenStreetView-5M, a large-scale, open-access dataset\ncomprising over 5.1 million geo-referenced street view images, covering 225\ncountries and territories. In contrast to existing benchmarks, we enforce a\nstrict train/test separation, allowing us to evaluate the relevance of learned\ngeographical features beyond mere memorization. To demonstrate the utility of\nour dataset, we conduct an extensive benchmark of various state-of-the-art\nimage encoders, spatial representations, and training strategies. All\nassociated codes and models can be found at https://github.com/gastruc/osv5m.",
      "tldr_zh": "本研究引入了OpenStreetView-5M数据集，这是一个大规模开源数据集，包含超过510万张geo-referenced街景图像，覆盖225个国家和地区，用于评估全球视觉地理定位任务。不同于现有基准，该数据集强制执行严格的训练/测试分离，以测试算法对地理特征的真实学习而非记忆。研究者通过广泛基准测试了多种最先进图像编码器、空间表示和训练策略，证明了数据集的实用性。所有相关代码和模型可在https://github.com/gastruc/osv5m获取，这为计算机视觉领域的定位任务提供了标准化的评估工具。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "CVPR 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.18873v1",
      "published_date": "2024-04-29 17:06:44 UTC",
      "updated_date": "2024-04-29 17:06:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:28:07.453302"
    },
    {
      "arxiv_id": "2404.18870v2",
      "title": "More RLHF, More Trust? On The Impact of Preference Alignment On Trustworthiness",
      "title_zh": "翻译失败",
      "authors": [
        "Aaron J. Li",
        "Satyapriya Krishna",
        "Himabindu Lakkaraju"
      ],
      "abstract": "The trustworthiness of Large Language Models (LLMs) refers to the extent to\nwhich their outputs are reliable, safe, and ethically aligned, and it has\nbecome a crucial consideration alongside their cognitive performance. In\npractice, Reinforcement Learning From Human Feedback (RLHF) has been widely\nused to align LLMs with labeled human preferences, but its assumed effect on\nmodel trustworthiness hasn't been rigorously evaluated. To bridge this\nknowledge gap, this study investigates how models aligned with general-purpose\npreference data perform across five trustworthiness verticals: toxicity,\nstereotypical bias, machine ethics, truthfulness, and privacy. Our results\ndemonstrate that RLHF on human preferences doesn't automatically guarantee\ntrustworthiness, and reverse effects are often observed. Furthermore, we\npropose to adapt efficient influence function based data attribution methods to\nthe RLHF setting to better understand the influence of fine-tuning data on\nindividual trustworthiness benchmarks, and show its feasibility by providing\nour estimated attribution scores. Together, our results underscore the need for\nmore nuanced approaches for model alignment from both the data and framework\nperspectives, and we hope this research will guide the community towards\ndeveloping language models that are increasingly capable without sacrificing\ntrustworthiness.",
      "tldr_zh": "该研究探讨了强化学习从人类反馈（RLHF）对大型语言模型（LLMs）可信度的影响，可信度包括输出可靠性、安全性和伦理一致性。研究者评估了使用通用偏好数据对齐的模型在五个领域（toxicity、stereotypical bias、machine ethics、truthfulness 和 privacy）的表现，结果显示 RLHF 并不自动提升可信度，反而可能导致负面效果。论文进一步提出基于影响函数的数据归因方法，以分析细调数据对可信度基准的影响，并证明其可行性，强调需要从数据和框架角度采用更细致的模型对齐策略，以开发既强大又可靠的语言模型。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18870v2",
      "published_date": "2024-04-29 17:00:53 UTC",
      "updated_date": "2024-12-21 22:56:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:28:20.953102"
    },
    {
      "arxiv_id": "2404.18864v1",
      "title": "Performance-Aligned LLMs for Generating Fast Code",
      "title_zh": "性能对齐的 LLMs 用于生成快速代码",
      "authors": [
        "Daniel Nichols",
        "Pranav Polasam",
        "Harshitha Menon",
        "Aniruddha Marathe",
        "Todd Gamblin",
        "Abhinav Bhatele"
      ],
      "abstract": "Optimizing scientific software is a difficult task because codebases are\noften large and complex, and performance can depend upon several factors\nincluding the algorithm, its implementation, and hardware among others. Causes\nof poor performance can originate from disparate sources and be difficult to\ndiagnose. Recent years have seen a multitude of work that use large language\nmodels (LLMs) to assist in software development tasks. However, these tools are\ntrained to model the distribution of code as text, and are not specifically\ndesigned to understand performance aspects of code. In this work, we introduce\na reinforcement learning based methodology to align the outputs of code LLMs\nwith performance. This allows us to build upon the current code modeling\ncapabilities of LLMs and extend them to generate better performing code. We\ndemonstrate that our fine-tuned model improves the expected speedup of\ngenerated code over base models for a set of benchmark tasks from 0.9 to 1.6\nfor serial code and 1.9 to 4.5 for OpenMP code.",
      "tldr_zh": "该论文针对大型语言模型（LLMs）在生成代码时忽略性能优化的局限性，提出了一种基于强化学习的Methodology，以对齐LLMs的输出，使其更注重代码执行效率。该方法利用强化学习扩展现有LLMs的代码建模能力，从而生成性能更好的代码。在基准任务测试中，细调后的模型将串行代码的预期加速比从0.9提高到1.6，并将OpenMP代码的加速比从1.9提高到4.5，显著提升了科学软件的优化效果。",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18864v1",
      "published_date": "2024-04-29 16:52:38 UTC",
      "updated_date": "2024-04-29 16:52:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:28:31.443091"
    },
    {
      "arxiv_id": "2404.18848v3",
      "title": "FeDeRA:Efficient Fine-tuning of Language Models in Federated Learning Leveraging Weight Decomposition",
      "title_zh": "翻译失败",
      "authors": [
        "Yuxuan Yan",
        "Qianqian Yang",
        "Shunpu Tang",
        "Zhiguo Shi"
      ],
      "abstract": "Despite their exceptional performance on various tasks after fine-tuning,\npre-trained language models (PLMs) face significant challenges due to growing\nprivacy concerns with data in centralized training methods. We consider\nfederated learning (FL) to fine-tune PLMs in this paper. However, the\nsubstantial number of parameters in PLMs poses significant difficulties for\nclient devices with limited communication and computational resources. One\npromising solution is to exploit parameter-efficient fine-tuning (PEFT) into\nFL, which trains a much smaller set of parameters than full parameter\nfine-tuning (FFT). Although remarkably improving training efficiency, PEFT\nmethods may lead to degraded performance especially when data across different\nclients are non i.i.d, as revealed by experimental results. To overcome this,\nwe propose FeDeRA, which extends and improves a widely used PEFT method, i.e.,\nlow-rank adaption (LoRA). FeDeRA follows LoRA by decomposing the weight\nmatrices of the PLMs into low-rank matrices, which allows for more efficient\ncomputation and parameter updates during fine-tuning. Different from LoRA which\nsimply initializes these low-rank matrices by random sampling or zeros, the\nproposed FeDeRA initializes these matrices by the results of performing\nsingular value decomposition (SVD) on the pre-trained weight matrices.\nExtensive experiments across various tasks and datasets show that FeDeRA\noutperforms the considered PEFT baselines and is comparable to or even\nsurpasses FFT method within the FL setting in terms of task performance.\nMoreover, FeDeRA requires only 1% trainable paramentes compared to FFT,\nsignificantly reducing training time costs by more than 90% to achieve the same\ntask performance level. The experimental results also highlight the robustness\nof FeDeRA against data heterogeneity, as it maintains stable task performance\neven as data heterogeneity increases.",
      "tldr_zh": "该论文提出FeDeRA，一种改进的参数高效微调(PEFT)方法，用于在联邦学习(FL)中微调预训练语言模型(PLMs)，以解决客户端资源有限和数据非独立同分布(non i.i.d)问题。FeDeRA基于低秩适配(LoRA)框架，通过对预训练权重矩阵进行奇异值分解(SVD)初始化低秩矩阵，从而提升微调效率和鲁棒性。实验结果显示，FeDeRA在各种任务和数据集上优于其他PEFT基线，并与全参数微调(FFT)相当或更优，同时只需FFT的1%参数，训练时间减少90%以上，且对数据异质性更具稳定性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18848v3",
      "published_date": "2024-04-29 16:42:26 UTC",
      "updated_date": "2024-05-25 06:55:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:28:44.695942"
    },
    {
      "arxiv_id": "2404.18831v1",
      "title": "ConPro: Learning Severity Representation for Medical Images using Contrastive Learning and Preference Optimization",
      "title_zh": "Con",
      "authors": [
        "Hong Nguyen",
        "Hoang Nguyen",
        "Melinda Chang",
        "Hieu Pham",
        "Shrikanth Narayanan",
        "Michael Pazzani"
      ],
      "abstract": "Understanding the severity of conditions shown in images in medical diagnosis\nis crucial, serving as a key guide for clinical assessment, treatment, as well\nas evaluating longitudinal progression. This paper proposes Con- PrO: a novel\nrepresentation learning method for severity assessment in medical images using\nContrastive learningintegrated Preference Optimization. Different from\nconventional contrastive learning methods that maximize the distance between\nclasses, ConPrO injects into the latent vector the distance preference\nknowledge between various severity classes and the normal class. We\nsystematically examine the key components of our framework to illuminate how\ncontrastive prediction tasks acquire valuable representations. We show that our\nrepresentation learning framework offers valuable severity ordering in the\nfeature space while outperforming previous state-of-the-art methods on\nclassification tasks. We achieve a 6% and 20% relative improvement compared to\na supervised and a self-supervised baseline, respectively. In addition, we\nderived discussions on severity indicators and related applications of\npreference comparison in the medical domain.",
      "tldr_zh": "本研究提出 ConPro，一种用于医疗图像严重程度评估的表示学习方法，结合 Contrastive Learning 和 Preference Optimization，将不同严重程度类与正常类之间的距离偏好知识注入潜在向量中。不同于传统对比学习方法，ConPro 通过系统检查框架关键组件，在特征空间中实现有价值的严重程度排序。实验结果显示，该方法在分类任务上比监督基线提高 6%，比自监督基线提高 20%，并讨论了严重程度指标及其在医疗领域的应用潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "8 pages",
      "pdf_url": "http://arxiv.org/pdf/2404.18831v1",
      "published_date": "2024-04-29 16:16:42 UTC",
      "updated_date": "2024-04-29 16:16:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:28:55.826862"
    },
    {
      "arxiv_id": "2404.18825v1",
      "title": "Harmonic Machine Learning Models are Robust",
      "title_zh": "翻译失败",
      "authors": [
        "Nicholas S. Kersting",
        "Yi Li",
        "Aman Mohanty",
        "Oyindamola Obisesan",
        "Raphael Okochu"
      ],
      "abstract": "We introduce Harmonic Robustness, a powerful and intuitive method to test the\nrobustness of any machine-learning model either during training or in black-box\nreal-time inference monitoring without ground-truth labels. It is based on\nfunctional deviation from the harmonic mean value property, indicating\ninstability and lack of explainability. We show implementation examples in\nlow-dimensional trees and feedforward NNs, where the method reliably identifies\noverfitting, as well as in more complex high-dimensional models such as\nResNet-50 and Vision Transformer where it efficiently measures adversarial\nvulnerability across image classes.",
      "tldr_zh": "本研究引入了Harmonic Robustness，一种无需ground-truth labels即可评估机器学习模型鲁棒性的方法，可在训练过程中或黑盒实时推理中应用。\n该方法通过检测模型与harmonic mean value property的功能偏差，来识别不稳定性并揭示overfitting问题。\n实验结果显示，它在低维树和前馈神经网络中可靠地检测过拟合，在复杂模型如ResNet-50和Vision Transformer中有效测量图像类别的adversarial vulnerability。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "18 pages, 13 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.18825v1",
      "published_date": "2024-04-29 16:07:36 UTC",
      "updated_date": "2024-04-29 16:07:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:29:07.247364"
    },
    {
      "arxiv_id": "2404.18824v1",
      "title": "Benchmarking Benchmark Leakage in Large Language Models",
      "title_zh": "评估大语言模型中的基准泄漏基准测试",
      "authors": [
        "Ruijie Xu",
        "Zengzhi Wang",
        "Run-Ze Fan",
        "Pengfei Liu"
      ],
      "abstract": "Amid the expanding use of pre-training data, the phenomenon of benchmark\ndataset leakage has become increasingly prominent, exacerbated by opaque\ntraining processes and the often undisclosed inclusion of supervised data in\ncontemporary Large Language Models (LLMs). This issue skews benchmark\neffectiveness and fosters potentially unfair comparisons, impeding the field's\nhealthy development. To address this, we introduce a detection pipeline\nutilizing Perplexity and N-gram accuracy, two simple and scalable metrics that\ngauge a model's prediction precision on benchmark, to identify potential data\nleakages. By analyzing 31 LLMs under the context of mathematical reasoning, we\nreveal substantial instances of training even test set misuse, resulting in\npotentially unfair comparisons. These findings prompt us to offer several\nrecommendations regarding model documentation, benchmark setup, and future\nevaluations. Notably, we propose the \"Benchmark Transparency Card\" to encourage\nclear documentation of benchmark utilization, promoting transparency and\nhealthy developments of LLMs. we have made our leaderboard, pipeline\nimplementation, and model predictions publicly available, fostering future\nresearch.",
      "tldr_zh": "这篇论文探讨了 Large Language Models (LLMs) 中基准数据集泄露问题，由于预训练数据不透明和可能包含监督数据，导致基准测试无效并引发不公平比较。研究团队引入了一个检测管道，利用 Perplexity 和 N-gram accuracy 两个简单可扩展指标，评估模型在基准上的预测精度，以识别潜在泄露。分析 31 个 LLMs 在数学推理任务中的结果，揭示了大量训练集和测试集误用现象。论文提出推荐措施，包括开发 \"Benchmark Transparency Card\" 以提升模型文档和基准透明度，并公开了排行榜、管道实现及模型预测资源。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "30 pages; Homepage: https://gair-nlp.github.io/benbench",
      "pdf_url": "http://arxiv.org/pdf/2404.18824v1",
      "published_date": "2024-04-29 16:05:36 UTC",
      "updated_date": "2024-04-29 16:05:36 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:29:20.722790"
    },
    {
      "arxiv_id": "2404.18821v2",
      "title": "Control Policy Correction Framework for Reinforcement Learning-based Energy Arbitrage Strategies",
      "title_zh": "基于强化学习的能源套利策略的控制策略修正框架",
      "authors": [
        "Seyed Soroush Karimi Madahi",
        "Gargya Gokhale",
        "Marie-Sophie Verwee",
        "Bert Claessens",
        "Chris Develder"
      ],
      "abstract": "A continuous rise in the penetration of renewable energy sources, along with\nthe use of the single imbalance pricing, provides a new opportunity for balance\nresponsible parties to reduce their cost through energy arbitrage in the\nimbalance settlement mechanism. Model-free reinforcement learning (RL) methods\nare an appropriate choice for solving the energy arbitrage problem due to their\noutstanding performance in solving complex stochastic sequential problems.\nHowever, RL is rarely deployed in real-world applications since its learned\npolicy does not necessarily guarantee safety during the execution phase. In\nthis paper, we propose a new RL-based control framework for batteries to obtain\na safe energy arbitrage strategy in the imbalance settlement mechanism. In our\nproposed control framework, the agent initially aims to optimize the arbitrage\nrevenue. Subsequently, in the post-processing step, we correct (constrain) the\nlearned policy following a knowledge distillation process based on properties\nthat follow human intuition. Our post-processing step is a generic method and\nis not restricted to the energy arbitrage domain. We use the Belgian imbalance\nprice of 2023 to evaluate the performance of our proposed framework.\nFurthermore, we deploy our proposed control framework on a real battery to show\nits capability in the real world.",
      "tldr_zh": "该论文提出一个基于强化学习 (RL) 的控制框架，用于能源套利策略，以帮助平衡责任方在不平衡结算机制中安全降低成本。框架首先让代理优化套利收益，然后通过知识蒸馏 (knowledge distillation) 的后处理步骤修正策略，确保其符合人类直觉的属性，从而提升安全性。该方法通用且不限于能源领域，实验基于2023年比利时不平衡价格数据验证了框架的性能，并在真实电池上成功部署，展示了其实用性。",
      "categories": [
        "eess.SY",
        "cs.AI",
        "cs.LG",
        "cs.SY"
      ],
      "primary_category": "eess.SY",
      "comment": "ACM e-Energy 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.18821v2",
      "published_date": "2024-04-29 16:03:21 UTC",
      "updated_date": "2024-04-30 08:54:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:29:32.827546"
    },
    {
      "arxiv_id": "2404.18796v2",
      "title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models",
      "title_zh": "用陪审",
      "authors": [
        "Pat Verga",
        "Sebastian Hofstatter",
        "Sophia Althammer",
        "Yixuan Su",
        "Aleksandra Piktus",
        "Arkady Arkhangorodsky",
        "Minjie Xu",
        "Naomi White",
        "Patrick Lewis"
      ],
      "abstract": "As Large Language Models (LLMs) have become more advanced, they have outpaced\nour abilities to accurately evaluate their quality. Not only is finding data to\nadequately probe particular model properties difficult, but evaluating the\ncorrectness of a model's freeform generation alone is a challenge. To address\nthis, many evaluations now rely on using LLMs themselves as judges to score the\nquality of outputs from other LLMs. Evaluations most commonly use a single\nlarge model like GPT4. While this method has grown in popularity, it is costly,\nhas been shown to introduce intramodel bias, and in this work, we find that\nvery large models are often unnecessary. We propose instead to evaluate models\nusing a Panel of LLm evaluators (PoLL). Across three distinct judge settings\nand spanning six different datasets, we find that using a PoLL composed of a\nlarger number of smaller models outperforms a single large judge, exhibits less\nintra-model bias due to its composition of disjoint model families, and does so\nwhile being over seven times less expensive.",
      "tldr_zh": "该研究指出，使用单一大型LLMs（如GPT-4）来评估其他LLMs生成质量的方法存在成本高和内部偏见问题。作者提出PoLL（一个由多个小型、不同家族的LLMs组成的评估面板）作为替代方案，通过多样化模型组合来提高评估准确性。实验结果显示，在三个评判设置和六个数据集上，PoLL比单一大模型表现更好，减少了偏见，且成本降低了七倍以上。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18796v2",
      "published_date": "2024-04-29 15:33:23 UTC",
      "updated_date": "2024-05-01 15:37:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:29:44.134193"
    },
    {
      "arxiv_id": "2404.18791v2",
      "title": "Certification of Speaker Recognition Models to Additive Perturbations",
      "title_zh": "翻译失败",
      "authors": [
        "Dmitrii Korzh",
        "Elvir Karimov",
        "Mikhail Pautov",
        "Oleg Y. Rogov",
        "Ivan Oseledets"
      ],
      "abstract": "Speaker recognition technology is applied to various tasks, from personal\nvirtual assistants to secure access systems. However, the robustness of these\nsystems against adversarial attacks, particularly to additive perturbations,\nremains a significant challenge. In this paper, we pioneer applying robustness\ncertification techniques to speaker recognition, initially developed for the\nimage domain. Our work covers this gap by transferring and improving randomized\nsmoothing certification techniques against norm-bounded additive perturbations\nfor classification and few-shot learning tasks to speaker recognition. We\ndemonstrate the effectiveness of these methods on VoxCeleb 1 and 2 datasets for\nseveral models. We expect this work to improve the robustness of voice\nbiometrics and accelerate the research of certification methods in the audio\ndomain.",
      "tldr_zh": "本研究首次将图像领域的鲁棒性认证技术应用于演讲者识别模型，针对 norm-bounded additive perturbations（如加性扰动）来提升其对抗敌对攻击的可靠性。研究者通过转移和改进 randomized smoothing certification 技术，扩展其适用于演讲者识别的分类和少样本学习任务，并在 VoxCeleb 1 和 2 数据集上对多个模型进行了验证。结果显示，这些方法有效提高了模型的鲁棒性。该工作有望增强语音生物识别系统的安全性，并推动音频领域认证方法的研究。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "13 pages, 10 figures; AAAI-2025 accepted paper",
      "pdf_url": "http://arxiv.org/pdf/2404.18791v2",
      "published_date": "2024-04-29 15:23:26 UTC",
      "updated_date": "2024-12-18 16:52:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:29:56.535797"
    },
    {
      "arxiv_id": "2404.18982v2",
      "title": "Can ChatGPT Make Explanatory Inferences? Benchmarks for Abductive Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Paul Thagard"
      ],
      "abstract": "Explanatory inference is the creation and evaluation of hypotheses that\nprovide explanations, and is sometimes known as abduction or abductive\ninference. Generative AI is a new set of artificial intelligence models based\non novel algorithms for generating text, images, and sounds. This paper\nproposes a set of benchmarks for assessing the ability of AI programs to\nperform explanatory inference, and uses them to determine the extent to which\nChatGPT, a leading generative AI model, is capable of making explanatory\ninferences. Tests on the benchmarks reveal that ChatGPT performs creative and\nevaluative inferences in many domains, although it is limited to verbal and\nvisual modalities. Claims that ChatGPT and similar models are incapable of\nexplanation, understanding, causal reasoning, meaning, and creativity are\nrebutted.",
      "tldr_zh": "这篇论文探讨了 ChatGPT 是否能进行解释性推理（explanatory inference），也称为 abductive reasoning，并提出了一套基准（benchmarks）来评估 AI 模型的这一能力。研究通过测试发现，ChatGPT 在多个领域能够执行创造性和评价性推理，但仅限于 verbal 和 visual 模态。论文由此反驳了 ChatGPT 无法进行解释、理解、因果推理、意义和创造力的批评，为评估生成式 AI 的推理能力提供了新见解。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18982v2",
      "published_date": "2024-04-29 15:19:05 UTC",
      "updated_date": "2024-09-19 13:20:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:30:09.101860"
    },
    {
      "arxiv_id": "2404.18784v1",
      "title": "Where on Earth Do Users Say They Are?: Geo-Entity Linking for Noisy Multilingual User Input",
      "title_zh": "翻译失败",
      "authors": [
        "Tessa Masis",
        "Brendan O'Connor"
      ],
      "abstract": "Geo-entity linking is the task of linking a location mention to the\nreal-world geographic location. In this paper we explore the challenging task\nof geo-entity linking for noisy, multilingual social media data. There are few\nopen-source multilingual geo-entity linking tools available and existing ones\nare often rule-based, which break easily in social media settings, or\nLLM-based, which are too expensive for large-scale datasets. We present a\nmethod which represents real-world locations as averaged embeddings from\nlabeled user-input location names and allows for selective prediction via an\ninterpretable confidence score. We show that our approach improves geo-entity\nlinking on a global and multilingual social media dataset, and discuss progress\nand problems with evaluating at different geographic granularities.",
      "tldr_zh": "本论文探讨了geo-entity linking任务，即将位置提及链接到真实地理位置，特别针对嘈杂的多语言社交媒体数据。作者提出了一种新方法，通过将真实位置表示为从标记的用户输入位置名称中平均的embeddings向量，并引入一个可解释的置信度分数进行选择性预测，以克服现有规则-based或LLM-based工具的局限性。实验结果显示，该方法在全球多语言社交媒体数据集上显著提升了geo-entity linking的性能，并讨论了在不同地理粒度下评估的进展和挑战。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "NLP+CSS workshop at NAACL 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.18784v1",
      "published_date": "2024-04-29 15:18:33 UTC",
      "updated_date": "2024-04-29 15:18:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:30:20.789636"
    },
    {
      "arxiv_id": "2404.18981v1",
      "title": "Decoding Radiologists' Intentions: A Novel System for Accurate Region Identification in Chest X-ray Image Analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Akash Awasthi",
        "Safwan Ahmad",
        "Bryant Le",
        "Hien Van Nguyen"
      ],
      "abstract": "In the realm of chest X-ray (CXR) image analysis, radiologists meticulously\nexamine various regions, documenting their observations in reports. The\nprevalence of errors in CXR diagnoses, particularly among inexperienced\nradiologists and hospital residents, underscores the importance of\nunderstanding radiologists' intentions and the corresponding regions of\ninterest. This understanding is crucial for correcting mistakes by guiding\nradiologists to the accurate regions of interest, especially in the diagnosis\nof chest radiograph abnormalities. In response to this imperative, we propose a\nnovel system designed to identify the primary intentions articulated by\nradiologists in their reports and the corresponding regions of interest in CXR\nimages. This system seeks to elucidate the visual context underlying\nradiologists' textual findings, with the potential to rectify errors made by\nless experienced practitioners and direct them to precise regions of interest.\nImportantly, the proposed system can be instrumental in providing constructive\nfeedback to inexperienced radiologists or junior residents in the hospital,\nbridging the gap in face-to-face communication. The system represents a\nvaluable tool for enhancing diagnostic accuracy and fostering continuous\nlearning within the medical community.",
      "tldr_zh": "该研究针对胸部 X-ray (CXR) 图像分析中放射科医生诊断错误的问题，提出了一种新型系统，用于识别放射科医生报告中的主要意图及其对应的兴趣区域。系统通过分析报告文本和图像视觉上下文，帮助阐明放射科医生的观察依据，从而指导不经验的医生纠正错误并定位准确区域。该系统可提供建设性反馈，提升诊断准确性和医疗社区的学习效率，在临床实践中桥接沟通差距。",
      "categories": [
        "eess.IV",
        "cs.AI"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted in ISBI 2024",
      "pdf_url": "http://arxiv.org/pdf/2404.18981v1",
      "published_date": "2024-04-29 15:18:26 UTC",
      "updated_date": "2024-04-29 15:18:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:30:32.340297"
    },
    {
      "arxiv_id": "2404.18774v1",
      "title": "Self-training superconducting neuromorphic circuits using reinforcement learning rules",
      "title_zh": "使用强化学习规则的自训练超导神经形态电路",
      "authors": [
        "M. L. Schneider",
        "E. M. Jué",
        "M. R. Pufall",
        "K. Segall",
        "C. W. Anderson"
      ],
      "abstract": "Reinforcement learning algorithms are used in a wide range of applications,\nfrom gaming and robotics to autonomous vehicles. In this paper we describe a\nset of reinforcement learning-based local weight update rules and their\nimplementation in superconducting hardware. Using SPICE circuit simulations, we\nimplement a small-scale neural network with a learning time of order one\nnanosecond. This network can be trained to learn new functions simply by\nchanging the target output for a given set of inputs, without the need for any\nexternal adjustments to the network. In this implementation the weights are\nadjusted based on the current state of the overall network response and locally\nstored information about the previous action. This removes the need to program\nexplicit weight values in these networks, which is one of the primary\nchallenges that analog hardware implementations of neural networks face. The\nadjustment of weights is based on a global reinforcement signal that obviates\nthe need for circuitry to back-propagate errors.",
      "tldr_zh": "本文提出了一种使用强化学习规则的自训练超导神经形态电路，旨在实现高效的网络权重更新。研究通过SPICE电路模拟构建了一个小规模神经网络，该网络的学习时间仅需约一纳秒，并能通过改变目标输出自动学习新函数，而无需外部调整或编程显式权重值。该方法依赖全局强化信号和局部存储信息进行权重调整，解决了模拟硬件神经网络中常见的问题，如反向传播电路的需求，从而提升了系统的灵活性和效率。",
      "categories": [
        "cond-mat.supr-con",
        "cs.AI"
      ],
      "primary_category": "cond-mat.supr-con",
      "comment": "15 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.18774v1",
      "published_date": "2024-04-29 15:09:00 UTC",
      "updated_date": "2024-04-29 15:09:00 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:30:44.944313"
    },
    {
      "arxiv_id": "2404.18772v1",
      "title": "Saliency Suppressed, Semantics Surfaced: Visual Transformations in Neural Networks and the Brain",
      "title_zh": "翻译失败",
      "authors": [
        "Gustaw Opiełka",
        "Jessica Loke",
        "Steven Scholte"
      ],
      "abstract": "Deep learning algorithms lack human-interpretable accounts of how they\ntransform raw visual input into a robust semantic understanding, which impedes\ncomparisons between different architectures, training objectives, and the human\nbrain. In this work, we take inspiration from neuroscience and employ\nrepresentational approaches to shed light on how neural networks encode\ninformation at low (visual saliency) and high (semantic similarity) levels of\nabstraction. Moreover, we introduce a custom image dataset where we\nsystematically manipulate salient and semantic information. We find that\nResNets are more sensitive to saliency information than ViTs, when trained with\nobject classification objectives. We uncover that networks suppress saliency in\nearly layers, a process enhanced by natural language supervision (CLIP) in\nResNets. CLIP also enhances semantic encoding in both architectures. Finally,\nwe show that semantic encoding is a key factor in aligning AI with human visual\nperception, while saliency suppression is a non-brain-like strategy.",
      "tldr_zh": "本文从神经科学中汲取灵感，使用表示方法分析神经网络如何在低水平（visual saliency）和高水平（semantic similarity）上编码视觉信息，并通过一个自定义图像数据集系统地操纵 salient 和 semantic 信息。研究发现，ResNets 在物体分类训练下对 saliency 更敏感，且在早期层抑制 saliency，这一过程通过自然语言监督（如 CLIP）得到增强，同时 CLIP 也提升了 ResNets 和 ViTs 的 semantic 编码。最终，作者强调 semantic 编码是 AI 与人类视觉感知对齐的关键因素，而 saliency suppression 则是一种非大脑-like 策略。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18772v1",
      "published_date": "2024-04-29 15:05:42 UTC",
      "updated_date": "2024-04-29 15:05:42 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:30:58.173709"
    },
    {
      "arxiv_id": "2404.18766v1",
      "title": "PECC: Problem Extraction and Coding Challenges",
      "title_zh": "PECC：",
      "authors": [
        "Patrick Haller",
        "Jonas Golde",
        "Alan Akbik"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have showcased their\nexceptional abilities across various tasks, such as code generation,\nproblem-solving and reasoning. Existing benchmarks evaluate tasks in isolation,\nyet the extent to which LLMs can understand prose-style tasks, identify the\nunderlying problems, and then generate appropriate code solutions is still\nunexplored. Addressing this gap, we introduce PECC, a novel benchmark derived\nfrom Advent Of Code (AoC) challenges and Project Euler, including 2396\nproblems. Unlike conventional benchmarks, PECC requires LLMs to interpret\nnarrative-embedded problems, extract requirements, and generate executable\ncode. A key feature of our dataset is the complexity added by natural language\nprompting in chat-based evaluations, mirroring real-world instruction\nambiguities. Results show varying model performance between narrative and\nneutral problems, with specific challenges in the Euler math-based subset with\nGPT-3.5-Turbo passing 50% of the AoC challenges and only 8% on the Euler\nproblems. By probing the limits of LLMs' capabilities, our benchmark provides a\nframework to monitor and assess the subsequent progress of LLMs as a universal\nproblem solver.",
      "tldr_zh": "本文引入了 PECC 基准，用于评估大型语言模型 (LLMs) 从散文式任务中提取问题并生成可执行代码的能力，该基准基于 Advent Of Code (AoC) 和 Project Euler 的 2396 个问题，并通过自然语言提示模拟真实指令模糊性。不同于传统基准，PECC 要求 LLMs 理解叙述嵌入的问题、提取需求并输出代码，突显了模型在不同任务类型上的表现差异。实验结果显示，GPT-3.5-Turbo 在 AoC 挑战中通过率达 50%，但在 Euler 数学问题中仅 8%。这个框架为监测 LLMs 作为通用问题解决器的进展提供了重要工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper got accepted at LREC-COLING 2024 (long)",
      "pdf_url": "http://arxiv.org/pdf/2404.18766v1",
      "published_date": "2024-04-29 15:02:14 UTC",
      "updated_date": "2024-04-29 15:02:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:31:10.521255"
    },
    {
      "arxiv_id": "2406.14567v2",
      "title": "DragPoser: Motion Reconstruction from Variable Sparse Tracking Signals via Latent Space Optimization",
      "title_zh": "DragPoser：通过潜在空间优化从可变稀",
      "authors": [
        "Jose Luis Ponton",
        "Eduard Pujol",
        "Andreas Aristidou",
        "Carlos Andujar",
        "Nuria Pelechano"
      ],
      "abstract": "High-quality motion reconstruction that follows the user's movements can be\nachieved by high-end mocap systems with many sensors. However, obtaining such\nanimation quality with fewer input devices is gaining popularity as it brings\nmocap closer to the general public. The main challenges include the loss of\nend-effector accuracy in learning-based approaches, or the lack of naturalness\nand smoothness in IK-based solutions. In addition, such systems are often\nfinely tuned to a specific number of trackers and are highly sensitive to\nmissing data e.g., in scenarios where a sensor is occluded or malfunctions. In\nresponse to these challenges, we introduce DragPoser, a novel\ndeep-learning-based motion reconstruction system that accurately represents\nhard and dynamic on-the-fly constraints, attaining real-time high end-effectors\nposition accuracy. This is achieved through a pose optimization process within\na structured latent space. Our system requires only one-time training on a\nlarge human motion dataset, and then constraints can be dynamically defined as\nlosses, while the pose is iteratively refined by computing the gradients of\nthese losses within the latent space. To further enhance our approach, we\nincorporate a Temporal Predictor network, which employs a Transformer\narchitecture to directly encode temporality within the latent space. This\nnetwork ensures the pose optimization is confined to the manifold of valid\nposes and also leverages past pose data to predict temporally coherent poses.\nResults demonstrate that DragPoser surpasses both IK-based and the latest\ndata-driven methods in achieving precise end-effector positioning, while it\nproduces natural poses and temporally coherent motion. In addition, our system\nshowcases robustness against on-the-fly constraint modifications, and exhibits\nexceptional adaptability to various input configurations and changes.",
      "tldr_zh": "本文提出 DragPoser，一种基于深度学习的动作重建系统，通过在结构化的 latent space 内进行姿势优化，处理可变稀疏跟踪信号带来的端执行器精度损失和自然性问题。该系统仅需一次在大型人类动作数据集上训练，即可动态定义损失函数并迭代优化姿势，同时整合 Temporal Predictor 网络（采用 Transformer 架构）来编码时间性和利用过去姿势数据，确保生成的时间连贯动作。实验结果表明，DragPoser 在端执行器定位精度上 surpasses IK-based 和最新数据驱动方法，并展现出对实时约束修改和各种输入配置的 robust 适应性。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "Published on Eurographics 2025. Project page:\n  https://upc-virvig.github.io/DragPoser/",
      "pdf_url": "http://arxiv.org/pdf/2406.14567v2",
      "published_date": "2024-04-29 15:00:50 UTC",
      "updated_date": "2025-04-10 18:42:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:31:21.971732"
    },
    {
      "arxiv_id": "2404.18978v1",
      "title": "Towards Generalizable Agents in Text-Based Educational Environments: A Study of Integrating RL with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Bahar Radmehr",
        "Adish Singla",
        "Tanja Käser"
      ],
      "abstract": "There has been a growing interest in developing learner models to enhance\nlearning and teaching experiences in educational environments. However,\nexisting works have primarily focused on structured environments relying on\nmeticulously crafted representations of tasks, thereby limiting the agent's\nability to generalize skills across tasks. In this paper, we aim to enhance the\ngeneralization capabilities of agents in open-ended text-based learning\nenvironments by integrating Reinforcement Learning (RL) with Large Language\nModels (LLMs). We investigate three types of agents: (i) RL-based agents that\nutilize natural language for state and action representations to find the best\ninteraction strategy, (ii) LLM-based agents that leverage the model's general\nknowledge and reasoning through prompting, and (iii) hybrid LLM-assisted RL\nagents that combine these two strategies to improve agents' performance and\ngeneralization. To support the development and evaluation of these agents, we\nintroduce PharmaSimText, a novel benchmark derived from the PharmaSim virtual\npharmacy environment designed for practicing diagnostic conversations. Our\nresults show that RL-based agents excel in task completion but lack in asking\nquality diagnostic questions. In contrast, LLM-based agents perform better in\nasking diagnostic questions but fall short of completing the task. Finally,\nhybrid LLM-assisted RL agents enable us to overcome these limitations,\nhighlighting the potential of combining RL and LLMs to develop high-performing\nagents for open-ended learning environments.",
      "tldr_zh": "本研究旨在提升代理在开放文本-based教育环境中的泛化能力，通过整合Reinforcement Learning (RL)与Large Language Models (LLMs)，以解决现有模型在任务泛化上的局限性。研究者调查了三种代理：RL-based代理（利用自然语言表示状态和行动以优化互动策略）、LLM-based代理（依赖模型的通用知识和推理进行提示），以及Hybrid LLM-assisted RL代理（结合两者以改善性能）。为支持实验，他们引入了新的基准PharmaSimText，基于虚拟药房环境模拟诊断对话。结果表明，RL-based代理在任务完成上更出色，但诊断问题质量较差；LLM-based代理在问问题上表现更好，而Hybrid代理则克服了这些缺点，展示了RL和LLMs结合的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as a full paper at EDM 2024: The 17th International\n  Conference on Educational Data Mining, 14-17 of July 2024, Atlanta",
      "pdf_url": "http://arxiv.org/pdf/2404.18978v1",
      "published_date": "2024-04-29 14:53:48 UTC",
      "updated_date": "2024-04-29 14:53:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:31:35.928197"
    },
    {
      "arxiv_id": "2404.18747v1",
      "title": "Evaluating the Effectiveness of Video Anomaly Detection in the Wild: Online Learning and Inference for Real-world Deployment",
      "title_zh": "翻译失败",
      "authors": [
        "Shanle Yao",
        "Ghazal Alinezhad Noghre",
        "Armin Danesh Pazho",
        "Hamed Tabkhi"
      ],
      "abstract": "Video Anomaly Detection (VAD) identifies unusual activities in video streams,\na key technology with broad applications ranging from surveillance to\nhealthcare. Tackling VAD in real-life settings poses significant challenges due\nto the dynamic nature of human actions, environmental variations, and domain\nshifts. Many research initiatives neglect these complexities, often\nconcentrating on traditional testing methods that fail to account for\nperformance on unseen datasets, creating a gap between theoretical models and\ntheir real-world utility. Online learning is a potential strategy to mitigate\nthis issue by allowing models to adapt to new information continuously. This\npaper assesses how well current VAD algorithms can adjust to real-life\nconditions through an online learning framework, particularly those based on\npose analysis, for their efficiency and privacy advantages. Our proposed\nframework enables continuous model updates with streaming data from novel\nenvironments, thus mirroring actual world challenges and evaluating the models'\nability to adapt in real-time while maintaining accuracy. We investigate three\nstate-of-the-art models in this setting, focusing on their adaptability across\ndifferent domains. Our findings indicate that, even under the most challenging\nconditions, our online learning approach allows a model to preserve 89.39% of\nits original effectiveness compared to its offline-trained counterpart in a\nspecific target domain.",
      "tldr_zh": "该论文评估了视频异常检测 (VAD) 在真实环境中的有效性，强调在线学习框架以应对动态人类行为、环境变化和领域转移等挑战。研究提出一种基于姿势分析的在线学习方法，使模型能够使用流式数据持续更新，从而实现实时适应和准确性维护。实验结果显示，三种最先进模型在最苛刻条件下，通过该框架保留了89.39%的原始有效性，与离线训练模型相比显著提升了实际部署潜力。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18747v1",
      "published_date": "2024-04-29 14:47:32 UTC",
      "updated_date": "2024-04-29 14:47:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:31:45.927921"
    },
    {
      "arxiv_id": "2404.18976v1",
      "title": "Foundations of Multisensory Artificial Intelligence",
      "title_zh": "多感官人工智能的基础",
      "authors": [
        "Paul Pu Liang"
      ],
      "abstract": "Building multisensory AI systems that learn from multiple sensory inputs such\nas text, speech, video, real-world sensors, wearable devices, and medical data\nholds great promise for impact in many scientific areas with practical\nbenefits, such as in supporting human health and well-being, enabling\nmultimedia content processing, and enhancing real-world autonomous agents. By\nsynthesizing a range of theoretical frameworks and application domains, this\nthesis aims to advance the machine learning foundations of multisensory AI. In\nthe first part, we present a theoretical framework formalizing how modalities\ninteract with each other to give rise to new information for a task. These\ninteractions are the basic building blocks in all multimodal problems, and\ntheir quantification enables users to understand their multimodal datasets,\ndesign principled approaches to learn these interactions, and analyze whether\ntheir model has succeeded in learning. In the second part, we study the design\nof practical multimodal foundation models that generalize over many modalities\nand tasks, which presents a step toward grounding large language models to\nreal-world sensory modalities. We introduce MultiBench, a unified large-scale\nbenchmark across a wide range of modalities, tasks, and research areas,\nfollowed by the cross-modal attention and multimodal transformer architectures\nthat now underpin many of today's multimodal foundation models. Scaling these\narchitectures on MultiBench enables the creation of general-purpose\nmultisensory AI systems, and we discuss our collaborative efforts in applying\nthese models for real-world impact in affective computing, mental health,\ncancer prognosis, and robotics. Finally, we conclude this thesis by discussing\nhow future work can leverage these ideas toward more general, interactive, and\nsafe multisensory AI.",
      "tldr_zh": "本论文探讨了多感官人工智能（multisensory AI）的机器学习基础，旨在构建从文本、语音、视频等多个感官输入中学习的系统，以支持人类健康、多媒体处理和自主代理等领域。论文首先提出一个理论框架，形式化模态间交互如何产生新信息，从而帮助理解数据集、设计学习方法并评估模型性能。其次，引入 MultiBench 作为统一的跨模态基准，并开发 cross-modal attention 和 multimodal transformer 架构，这些模型在实际应用中显示出显著泛化能力，并在情感计算、心理健康、癌症预后和机器人等领域取得了实际影响。最终，论文讨论了未来工作如何基于这些基础推进更通用、交互和安全的 multisensory AI 系统。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.MM"
      ],
      "primary_category": "cs.LG",
      "comment": "CMU Machine Learning Department PhD Thesis",
      "pdf_url": "http://arxiv.org/pdf/2404.18976v1",
      "published_date": "2024-04-29 14:45:28 UTC",
      "updated_date": "2024-04-29 14:45:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:31:58.048923"
    },
    {
      "arxiv_id": "2404.18975v3",
      "title": "M3H: Multimodal Multitask Machine Learning for Healthcare",
      "title_zh": "翻译失败",
      "authors": [
        "Dimitris Bertsimas",
        "Yu Ma"
      ],
      "abstract": "Developing an integrated many-to-many framework leveraging multimodal data\nfor multiple tasks is crucial to unifying healthcare applications ranging from\ndiagnoses to operations. In resource-constrained hospital environments, a\nscalable and unified machine learning framework that improves previous forecast\nperformances could improve hospital operations and save costs. We introduce\nM3H, an explainable Multimodal Multitask Machine Learning for Healthcare\nframework that consolidates learning from tabular, time-series, language, and\nvision data for supervised binary/multiclass classification, regression, and\nunsupervised clustering. It features a novel attention mechanism balancing\nself-exploitation (learning source-task), and cross-exploration (learning\ncross-tasks), and offers explainability through a proposed TIM score, shedding\nlight on the dynamics of task learning interdependencies. M3H encompasses an\nunprecedented range of medical tasks and machine learning problem classes and\nconsistently outperforms traditional single-task models by on average 11.6%\nacross 40 disease diagnoses from 16 medical departments, three hospital\noperation forecasts, and one patient phenotyping task. The modular design of\nthe framework ensures its generalizability in data processing, task definition,\nand rapid model prototyping, making it production ready for both clinical and\noperational healthcare settings, especially those in constrained environments.",
      "tldr_zh": "本研究引入了 M3H 框架，这是一种多模态多任务机器学习系统，用于整合表格、时序、语言和视觉数据，以统一处理医疗领域的诊断、预测和聚类任务。M3H 采用新型注意力机制，平衡自利用（self-exploitation）和跨探索（cross-exploration），并通过提出的 TIM score 提供模型解释性，帮助揭示任务间相互依赖的动态。实验结果显示，该框架在 40 种疾病诊断、3 种医院操作预测和 1 种患者表型任务上，比传统单任务模型平均提高 11.6%，并以其模块化设计确保在资源受限的医疗环境中具有高度可扩展性和生产就绪性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18975v3",
      "published_date": "2024-04-29 14:39:15 UTC",
      "updated_date": "2024-06-08 19:11:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:32:10.823915"
    },
    {
      "arxiv_id": "2404.18736v4",
      "title": "Mapping the Potential of Explainable AI for Fairness Along the AI Lifecycle",
      "title_zh": "沿着 AI 生命周期，映射可解释 AI 用于公平性的潜力",
      "authors": [
        "Luca Deck",
        "Astrid Schomäcker",
        "Timo Speith",
        "Jakob Schöffer",
        "Lena Kästner",
        "Niklas Kühl"
      ],
      "abstract": "The widespread use of artificial intelligence (AI) systems across various\ndomains is increasingly surfacing issues related to algorithmic fairness,\nespecially in high-stakes scenarios. Thus, critical considerations of how\nfairness in AI systems might be improved -- and what measures are available to\naid this process -- are overdue. Many researchers and policymakers see\nexplainable AI (XAI) as a promising way to increase fairness in AI systems.\nHowever, there is a wide variety of XAI methods and fairness conceptions\nexpressing different desiderata, and the precise connections between XAI and\nfairness remain largely nebulous. Besides, different measures to increase\nalgorithmic fairness might be applicable at different points throughout an AI\nsystem's lifecycle. Yet, there currently is no coherent mapping of fairness\ndesiderata along the AI lifecycle. In this paper, we we distill eight fairness\ndesiderata, map them along the AI lifecycle, and discuss how XAI could help\naddress each of them. We hope to provide orientation for practical applications\nand to inspire XAI research specifically focused on these fairness desiderata.",
      "tldr_zh": "这篇论文探讨了可解释 AI (XAI) 在提升算法公平性方面的潜力，特别是在 AI 生命周期各阶段的应用。作者提炼了八个公平性需求（fairness desiderata），并将它们映射到 AI 系统的开发、部署和维护过程，以阐明 XAI 如何帮助解决这些问题。最终，该研究旨在为实际应用提供指导，并激发针对这些公平性需求的 XAI 研究。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18736v4",
      "published_date": "2024-04-29 14:34:43 UTC",
      "updated_date": "2024-06-27 11:43:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:32:22.693260"
    },
    {
      "arxiv_id": "2406.15395v1",
      "title": "An Exploratory Study on Human-Centric Video Anomaly Detection through Variational Autoencoders and Trajectory Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Ghazal Alinezhad Noghre",
        "Armin Danesh Pazho",
        "Hamed Tabkhi"
      ],
      "abstract": "Video Anomaly Detection (VAD) represents a challenging and prominent research\ntask within computer vision. In recent years, Pose-based Video Anomaly\nDetection (PAD) has drawn considerable attention from the research community\ndue to several inherent advantages over pixel-based approaches despite the\noccasional suboptimal performance. Specifically, PAD is characterized by\nreduced computational complexity, intrinsic privacy preservation, and the\nmitigation of concerns related to discrimination and bias against specific\ndemographic groups. This paper introduces TSGAD, a novel human-centric\nTwo-Stream Graph-Improved Anomaly Detection leveraging Variational Autoencoders\n(VAEs) and trajectory prediction. TSGAD aims to explore the possibility of\nutilizing VAEs as a new approach for pose-based human-centric VAD alongside the\nbenefits of trajectory prediction. We demonstrate TSGAD's effectiveness through\ncomprehensive experimentation on benchmark datasets. TSGAD demonstrates\ncomparable results with state-of-the-art methods showcasing the potential of\nadopting variational autoencoders. This suggests a promising direction for\nfuture research endeavors. The code base for this work is available at\nhttps://github.com/TeCSAR-UNCC/TSGAD.",
      "tldr_zh": "这篇论文探索了以人为中心的视频异常检测 (VAD)，特别关注基于姿态的检测 (PAD)，强调其在计算复杂度、隐私保护和减少歧视方面的优势。研究引入了 TSGAD，一个新型的双流图改进框架，结合 Variational Autoencoders (VAEs) 和轨迹预测，以提供一种新的 PAD 方法。实验结果显示，TSGAD 在基准数据集上取得了与最先进方法相当的表现，证明了 VAEs 在人类中心 VAD 中的潜力，并为未来研究指明了方向。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2406.15395v1",
      "published_date": "2024-04-29 14:25:06 UTC",
      "updated_date": "2024-04-29 14:25:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:32:35.821218"
    },
    {
      "arxiv_id": "2404.18731v3",
      "title": "Real Time Multi Organ Classification on Computed Tomography Images",
      "title_zh": "翻译失败",
      "authors": [
        "Halid Ziya Yerebakan",
        "Yoshihisa Shinagawa",
        "Gerardo Hermosillo Valadez"
      ],
      "abstract": "Organ segmentation is a fundamental task in medical imaging since it is\nuseful for many clinical automation pipelines. However, some tasks do not\nrequire full segmentation. Instead, a classifier can identify the selected\norgan without segmenting the entire volume. In this study, we demonstrate a\nclassifier based method to obtain organ labels in real time by using a large\ncontext size with a sparse data sampling strategy. Although our method operates\nas an independent classifier at query locations, it can generate full\nsegmentations by querying grid locations at any resolution, offering faster\nperformance than segmentation algorithms. We compared our method with existing\nsegmentation techniques, demonstrating its superior runtime potential for\npractical applications in medical imaging.",
      "tldr_zh": "本文提出了一种基于分类器的实时多器官识别方法，适用于Computed Tomography Images，通过采用大上下文大小和稀疏数据采样策略，实现快速获得器官标签，而非进行完整分割。该方法可在查询位置独立运作，并支持任意分辨率的网格查询以生成全卷分割，相比传统分割算法提供更快的性能。在与现有分割技术的比较中，该方法展示了在医疗成像实际应用中的优越运行时潜力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, Organ Classification, Organ Segmentation",
      "pdf_url": "http://arxiv.org/pdf/2404.18731v3",
      "published_date": "2024-04-29 14:17:52 UTC",
      "updated_date": "2025-01-09 22:10:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:32:46.470306"
    },
    {
      "arxiv_id": "2404.18730v1",
      "title": "CVTN: Cross Variable and Temporal Integration for Time Series Forecasting",
      "title_zh": "CVTN：用于时间序列预测的跨变量与时间整合",
      "authors": [
        "Han Zhou",
        "Yuntian Chen"
      ],
      "abstract": "In multivariate time series forecasting, the Transformer architecture\nencounters two significant challenges: effectively mining features from\nhistorical sequences and avoiding overfitting during the learning of temporal\ndependencies. To tackle these challenges, this paper deconstructs time series\nforecasting into the learning of historical sequences and prediction sequences,\nintroducing the Cross-Variable and Time Network (CVTN). This unique method\ndivides multivariate time series forecasting into two phases: cross-variable\nlearning for effectively mining fea tures from historical sequences, and\ncross-time learning to capture the temporal dependencies of prediction\nsequences. Separating these two phases helps avoid the impact of overfitting in\ncross-time learning on cross-variable learning. Exten sive experiments on\nvarious real-world datasets have confirmed its state-of-the-art (SOTA)\nperformance. CVTN emphasizes three key dimensions in time series fore casting:\nthe short-term and long-term nature of time series (locality and longevity),\nfeature mining from both historical and prediction sequences, and the\nintegration of cross-variable and cross-time learning. This approach not only\nadvances the current state of time series forecasting but also provides a more\ncomprehensive framework for future research in this field.",
      "tldr_zh": "该论文针对 Transformer 在多变量时间序列预测中的挑战——从历史序列有效挖掘特征和避免学习时间依赖时的过拟合——提出了 Cross-Variable and Time Network (CVTN) 框架。CVTN 将预测过程分为两个阶段：cross-variable learning 用于从历史序列中挖掘特征，以及 cross-time learning 用于捕获预测序列的时间依赖，从而减少过拟合的影响。在多种真实数据集上的广泛实验证实了 CVTN 的 state-of-the-art (SOTA) 性能，并强调了时间序列的 locality and longevity、特征挖掘整合以及跨变量和跨时间学习的综合重要性，为未来时间序列预测研究提供了更全面的框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18730v1",
      "published_date": "2024-04-29 14:16:16 UTC",
      "updated_date": "2024-04-29 14:16:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:32:59.419986"
    },
    {
      "arxiv_id": "2404.18713v3",
      "title": "Task and Domain Adaptive Reinforcement Learning for Robot Control",
      "title_zh": "翻译失败",
      "authors": [
        "Yu Tang Liu",
        "Nilaksh Singh",
        "Aamir Ahmad"
      ],
      "abstract": "Deep reinforcement learning (DRL) has shown remarkable success in simulation\ndomains, yet its application in designing robot controllers remains limited,\ndue to its single-task orientation and insufficient adaptability to\nenvironmental changes. To overcome these limitations, we present a novel\nadaptive agent that leverages transfer learning techniques to dynamically adapt\npolicy in response to different tasks and environmental conditions. The\napproach is validated through the blimp control challenge, where multitasking\ncapabilities and environmental adaptability are essential. The agent is trained\nusing a custom, highly parallelized simulator built on IsaacGym. We perform\nzero-shot transfer to fly the blimp in the real world to solve various tasks.\nWe share our code at https://github.com/robot-perception-group/adaptive_agent.",
      "tldr_zh": "本研究针对深度强化学习 (DRL) 在机器人控制中的单任务导向和环境适应性不足问题，提出了一种新型自适应代理，利用转移学习 (transfer learning) 技术动态调整策略以应对不同任务和环境变化。代理通过在 IsaacGym 构建的自定义高度并行模拟器中训练，并在 blimp 控制挑战中验证其多任务能力和适应性。实验结果显示，该代理实现了零样本转移 (zero-shot transfer) 到真实世界，成功完成各种任务，并公开了代码仓库以供进一步研究。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18713v3",
      "published_date": "2024-04-29 14:02:02 UTC",
      "updated_date": "2024-09-19 02:36:53 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:33:09.804162"
    },
    {
      "arxiv_id": "2405.09637v2",
      "title": "CLASSP: a Biologically-Inspired Approach to Continual Learning through Adjustment Suppression and Sparsity Promotion",
      "title_zh": "翻译失败",
      "authors": [
        "Oswaldo Ludwig"
      ],
      "abstract": "This paper introduces a new biologically-inspired training method named\nContinual Learning through Adjustment Suppression and Sparsity Promotion\n(CLASSP). CLASSP is based on two main principles observed in neuroscience,\nparticularly in the context of synaptic transmission and Long-Term Potentiation\n(LTP). The first principle is a decay rate over the weight adjustment, which is\nimplemented as a generalization of the AdaGrad optimization algorithm. This\nmeans that weights that have received many updates should have lower learning\nrates as they likely encode important information about previously seen data.\nHowever, this principle results in a diffuse distribution of updates throughout\nthe model, as it promotes updates for weights that haven't been previously\nupdated, while a sparse update distribution is preferred to leave weights\nunassigned for future tasks. Therefore, the second principle introduces a\nthreshold on the loss gradient. This promotes sparse learning by updating a\nweight only if the loss gradient with respect to that weight is above a certain\nthreshold, i.e. only updating weights with a significant impact on the current\nloss. Both principles reflect phenomena observed in LTP, where a threshold\neffect and a gradual saturation of potentiation have been observed. CLASSP is\nimplemented in a Python/PyTorch class, making it applicable to any model. When\ncompared with Elastic Weight Consolidation (EWC) using Computer Vision and\nsentiment analysis datasets, CLASSP demonstrates superior performance in terms\nof accuracy and memory footprint.",
      "tldr_zh": "这篇论文提出了 CLASSP，一种受神经科学启发的持续学习方法，通过权重调整抑制和稀疏促进来解决模型在新任务中遗忘旧知识的问题。CLASSP 基于两个核心原则：一是权重调整的衰减率（类似于 AdaGrad 的推广），使已更新多次的权重学习率降低，以保留先前任务的重要信息；二是对损失梯度的阈值，仅更新对当前损失有显著影响的权重，从而实现稀疏学习。这些原则源于 Long-Term Potentiation (LTP) 中的阈值效应和饱和现象。实验表明，CLASSP 在计算机视觉和情感分析数据集上比 Elastic Weight Consolidation (EWC) 表现出更高的准确性和更小的内存占用。",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.NE",
      "comment": "In this version I included a new experiment in text classification\n  using transformer architecture",
      "pdf_url": "http://arxiv.org/pdf/2405.09637v2",
      "published_date": "2024-04-29 13:31:00 UTC",
      "updated_date": "2024-06-08 21:02:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:33:23.633755"
    },
    {
      "arxiv_id": "2405.01591v1",
      "title": "Simplifying Multimodality: Unimodal Approach to Multimodal Challenges in Radiology with General-Domain Large Language Model",
      "title_zh": "翻译失败",
      "authors": [
        "Seonhee Cho",
        "Choonghan Kim",
        "Jiho Lee",
        "Chetan Chilkunda",
        "Sujin Choi",
        "Joo Heung Yoon"
      ],
      "abstract": "Recent advancements in Large Multimodal Models (LMMs) have attracted interest\nin their generalization capability with only a few samples in the prompt. This\nprogress is particularly relevant to the medical domain, where the quality and\nsensitivity of data pose unique challenges for model training and application.\nHowever, the dependency on high-quality data for effective in-context learning\nraises questions about the feasibility of these models when encountering with\nthe inevitable variations and errors inherent in real-world medical data. In\nthis paper, we introduce MID-M, a novel framework that leverages the in-context\nlearning capabilities of a general-domain Large Language Model (LLM) to process\nmultimodal data via image descriptions. MID-M achieves a comparable or superior\nperformance to task-specific fine-tuned LMMs and other general-domain ones,\nwithout the extensive domain-specific training or pre-training on multimodal\ndata, with significantly fewer parameters. This highlights the potential of\nleveraging general-domain LLMs for domain-specific tasks and offers a\nsustainable and cost-effective alternative to traditional LMM developments.\nMoreover, the robustness of MID-M against data quality issues demonstrates its\npractical utility in real-world medical domain applications.",
      "tldr_zh": "本论文提出 MID-M 框架，利用通用领域 Large Language Model (LLM) 通过图像描述处理放射学中的多模态挑战，从而简化多模态任务。MID-M 无需进行领域特定训练或多模态预训练，即可实现与任务特定微调的 Large Multimodal Models (LMMs) 相当或优越的性能，同时参数显著减少。该框架对数据质量问题表现出色，提供了一种可持续且成本有效的替代方案，突显了通用 LLM 在医疗领域应用的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.IV"
      ],
      "primary_category": "cs.CL",
      "comment": "Under review",
      "pdf_url": "http://arxiv.org/pdf/2405.01591v1",
      "published_date": "2024-04-29 13:23:33 UTC",
      "updated_date": "2024-04-29 13:23:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:33:34.370225"
    },
    {
      "arxiv_id": "2404.18672v1",
      "title": "Graph Convolutional Networks and Graph Attention Networks for Approximating Arguments Acceptability -- Technical Report",
      "title_zh": "翻译失败",
      "authors": [
        "Paul Cibier",
        "Jean-Guy Mailly"
      ],
      "abstract": "Various approaches have been proposed for providing efficient computational\napproaches for abstract argumentation. Among them, neural networks have\npermitted to solve various decision problems, notably related to arguments\n(credulous or skeptical) acceptability. In this work, we push further this\nstudy in various ways. First, relying on the state-of-the-art approach AFGCN,\nwe show how we can improve the performances of the Graph Convolutional Networks\n(GCNs) regarding both runtime and accuracy. Then, we show that it is possible\nto improve even more the efficiency of the approach by modifying the\narchitecture of the network, using Graph Attention Networks (GATs) instead.",
      "tldr_zh": "该论文探讨了使用神经网络解决抽象论证中的决策问题，特别是评估参数的可接受性（credulous 或 skeptical）。研究基于现有方法 AFGCN，改进了 Graph Convolutional Networks (GCNs) 的性能，包括提升运行时效率和准确率。进一步，通过采用 Graph Attention Networks (GATs) 替换网络架构，进一步提高了整体效率，为抽象论证计算提供了更有效的工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 2 figures. Submitted to the 10th International Conference\n  on Computational Models of Argument (COMMA 2024)",
      "pdf_url": "http://arxiv.org/pdf/2404.18672v1",
      "published_date": "2024-04-29 13:12:08 UTC",
      "updated_date": "2024-04-29 13:12:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:33:45.377832"
    },
    {
      "arxiv_id": "2404.18669v3",
      "title": "Bootstrap-GS: Self-Supervised Augmentation for High-Fidelity Gaussian Splatting",
      "title_zh": "翻译失败",
      "authors": [
        "Yifei Gao",
        "Kerui Ren",
        "Jie Ou",
        "Lei Wang",
        "Jiaji Wu",
        "Jun Cheng"
      ],
      "abstract": "Recent advancements in 3D Gaussian Splatting (3D-GS) have established new\nbenchmarks for rendering quality and efficiency in 3D reconstruction. However,\n3D-GS faces critical limitations when generating novel views that significantly\ndeviate from those encountered during training. Moreover, issues such as\ndilation and aliasing arise during zoom operations. These challenges stem from\na fundamental issue: training sampling deficiency. In this paper, we introduce\na bootstrapping framework to address this problem. Our approach synthesizes\npseudo-ground truth from novel views that align with the limited training set\nand reintegrates these synthesized views into the training pipeline.\nExperimental results demonstrate that our bootstrapping technique not only\nreduces artifacts but also improves quantitative metrics. Furthermore, our\ntechnique is highly adaptable, allowing various Gaussian-based method to\nbenefit from its integration.",
      "tldr_zh": "该研究针对 3D Gaussian Splatting (3D-GS) 在生成与训练视图显著不同的新视图时存在的采样不足问题（如膨胀和混叠），提出了一种自监督增强框架 Bootstrap-GS。方法通过从新视图合成伪真实数据并将其重新整合到训练管道中，从而提高模型的泛化能力。实验结果表明，该框架不仅减少了 artifacts（伪像），还提升了定量指标，并且高度适应性，可与其他 Gaussian-based 方法整合。",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV",
        "I.4.8"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18669v3",
      "published_date": "2024-04-29 12:57:05 UTC",
      "updated_date": "2025-03-04 01:06:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:33:58.787748"
    },
    {
      "arxiv_id": "2404.18655v1",
      "title": "Revealing the Parametric Knowledge of Language Models: A Unified Framework for Attribution Methods",
      "title_zh": "揭示语言模型的参数知识：归因方法的统一框架",
      "authors": [
        "Haeun Yu",
        "Pepa Atanasova",
        "Isabelle Augenstein"
      ],
      "abstract": "Language Models (LMs) acquire parametric knowledge from their training\nprocess, embedding it within their weights. The increasing scalability of LMs,\nhowever, poses significant challenges for understanding a model's inner\nworkings and further for updating or correcting this embedded knowledge without\nthe significant cost of retraining. This underscores the importance of\nunveiling exactly what knowledge is stored and its association with specific\nmodel components. Instance Attribution (IA) and Neuron Attribution (NA) offer\ninsights into this training-acquired knowledge, though they have not been\ncompared systematically. Our study introduces a novel evaluation framework to\nquantify and compare the knowledge revealed by IA and NA. To align the results\nof the methods we introduce the attribution method NA-Instances to apply NA for\nretrieving influential training instances, and IA-Neurons to discover important\nneurons of influential instances discovered by IA. We further propose a\ncomprehensive list of faithfulness tests to evaluate the comprehensiveness and\nsufficiency of the explanations provided by both methods. Through extensive\nexperiments and analysis, we demonstrate that NA generally reveals more diverse\nand comprehensive information regarding the LM's parametric knowledge compared\nto IA. Nevertheless, IA provides unique and valuable insights into the LM's\nparametric knowledge, which are not revealed by NA. Our findings further\nsuggest the potential of a synergistic approach of combining the diverse\nfindings of IA and NA for a more holistic understanding of an LM's parametric\nknowledge.",
      "tldr_zh": "本研究提出一个统一框架，用于揭示语言模型 (LMs) 的参数知识，通过比较 Instance Attribution (IA) 和 Neuron Attribution (NA) 方法来评估模型内部知识的存储和关联。研究引入 NA-Instances 和 IA-Neurons 技术，以桥接两种方法，并设计了一系列 faithfulness tests 来检验解释的完整性和充分性。实验结果显示，NA 能揭示更多样和全面的信息，而 IA 提供独特的见解；最终建议结合 IA 和 NA，实现对 LMs 参数知识的更整体理解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "68T50",
        "I.2.7"
      ],
      "primary_category": "cs.CL",
      "comment": "14 pages, 6 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.18655v1",
      "published_date": "2024-04-29 12:38:26 UTC",
      "updated_date": "2024-04-29 12:38:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:34:10.832857"
    },
    {
      "arxiv_id": "2404.18649v1",
      "title": "Towards Quantitative Evaluation of Explainable AI Methods for Deepfake Detection",
      "title_zh": "翻译失败",
      "authors": [
        "Konstantinos Tsigos",
        "Evlampios Apostolidis",
        "Spyridon Baxevanakis",
        "Symeon Papadopoulos",
        "Vasileios Mezaris"
      ],
      "abstract": "In this paper we propose a new framework for evaluating the performance of\nexplanation methods on the decisions of a deepfake detector. This framework\nassesses the ability of an explanation method to spot the regions of a fake\nimage with the biggest influence on the decision of the deepfake detector, by\nexamining the extent to which these regions can be modified through a set of\nadversarial attacks, in order to flip the detector's prediction or reduce its\ninitial prediction; we anticipate a larger drop in deepfake detection accuracy\nand prediction, for methods that spot these regions more accurately. Based on\nthis framework, we conduct a comparative study using a state-of-the-art model\nfor deepfake detection that has been trained on the FaceForensics++ dataset,\nand five explanation methods from the literature. The findings of our\nquantitative and qualitative evaluations document the advanced performance of\nthe LIME explanation method against the other compared ones, and indicate this\nmethod as the most appropriate for explaining the decisions of the utilized\ndeepfake detector.",
      "tldr_zh": "本研究提出了一种新的定量评估框架，用于评估Explainable AI方法在Deepfake Detection决策中的性能。该框架通过对抗攻击修改假图像中影响决策的关键区域，观察检测准确率和预测值的下降幅度，以测试解释方法的准确性。在基于FaceForensics++数据集的实验中，比较了五种文献中的解释方法，结果显示LIME方法表现出色，是最适合解释深假检测器决策的选项。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted for publication, 3rd ACM Int. Workshop on Multimedia AI\n  against Disinformation (MAD'24) at ACM ICMR'24, June 10, 2024, Phuket,\n  Thailand. This is the \"accepted version\"",
      "pdf_url": "http://arxiv.org/pdf/2404.18649v1",
      "published_date": "2024-04-29 12:32:14 UTC",
      "updated_date": "2024-04-29 12:32:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:34:21.868606"
    },
    {
      "arxiv_id": "2404.18638v1",
      "title": "Reinforcement Learning Problem Solving with Large Language Models",
      "title_zh": "基于大型语言模型的强化学习问题求解",
      "authors": [
        "Sina Gholamian",
        "Domingo Huh"
      ],
      "abstract": "Large Language Models (LLMs) encapsulate an extensive amount of world\nknowledge, and this has enabled their application in various domains to improve\nthe performance of a variety of Natural Language Processing (NLP) tasks. This\nhas also facilitated a more accessible paradigm of conversation-based\ninteractions between humans and AI systems to solve intended problems. However,\none interesting avenue that shows untapped potential is the use of LLMs as\nReinforcement Learning (RL) agents to enable conversational RL problem solving.\nTherefore, in this study, we explore the concept of formulating Markov Decision\nProcess-based RL problems as LLM prompting tasks. We demonstrate how LLMs can\nbe iteratively prompted to learn and optimize policies for specific RL tasks.\nIn addition, we leverage the introduced prompting technique for episode\nsimulation and Q-Learning, facilitated by LLMs. We then show the practicality\nof our approach through two detailed case studies for \"Research Scientist\" and\n\"Legal Matter Intake\" workflows.",
      "tldr_zh": "本研究探讨了使用 Large Language Models (LLMs) 作为 Reinforcement Learning (RL) 代理来解决对话式 RL 问题，旨在利用 LLMs 的世界知识提升问题求解效率。通过将 Markov Decision Process (MDP) 基于的 RL 任务转化为 LLM 提示任务，并采用迭代提示技术来学习和优化政策，该方法还支持 episode simulation 和 Q-Learning。研究通过“Research Scientist”和“Legal Matter Intake”工作流的两个案例研究，展示了这一方法的实用性和有效性。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18638v1",
      "published_date": "2024-04-29 12:16:08 UTC",
      "updated_date": "2024-04-29 12:16:08 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:34:36.008735"
    },
    {
      "arxiv_id": "2404.18624v4",
      "title": "Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?",
      "title_zh": "翻译失败",
      "authors": [
        "Letitia Parcalabescu",
        "Anette Frank"
      ],
      "abstract": "Vision and language model (VLM) decoders are currently the best-performing\narchitectures on multimodal tasks. Next to answers, they are able to produce\nnatural language explanations, either in post-hoc or CoT settings. However, it\nis not clear to what extent they are using the input vision and text modalities\nwhen generating answers or explanations. In this work, we investigate if VLMs\nrely on their input modalities differently when they produce explanations as\nopposed to answers. We also evaluate the self-consistency of VLM decoders in\nboth post-hoc and CoT explanation settings, by extending existing unimodal\ntests and measures to VLM decoders. We find that most tested VLMs are less\nself-consistent than LLMs. Text contributions in all tested VL decoders are\nmore important than image contributions in all examined tasks. However, when\ncomparing explanation generation to answer generation, the contributions of\nimages are significantly stronger for generating explanations compared to\nanswers. This difference is even larger in CoT compared to post-hoc\nexplanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art\nVL decoders on the VALSE benchmark, which before was restricted to VL encoders.\nWe find that the tested VL decoders still struggle with most phenomena tested\nby VALSE.",
      "tldr_zh": "本研究调查了视觉语言模型（VLM）解码器在生成答案和解释时，对图像和文本输入模态的依赖程度，以及其解释的自洽性。研究者通过扩展现有测试方法，比较了post-hoc和CoT（Chain-of-Thought）设置下的表现，发现大多数VLM比LLMs更不自洽，且文本贡献普遍大于图像贡献，但生成解释时图像作用更显著，尤其在CoT设置中。最终，在VALSE基准测试中，VLM解码器在处理多种现象时仍面临挑战，为改进多模态模型提供了关键见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG",
        "68Txx",
        "I.2.7; I.2.10"
      ],
      "primary_category": "cs.CL",
      "comment": "30 pages, 8 figures, 11 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.18624v4",
      "published_date": "2024-04-29 11:52:20 UTC",
      "updated_date": "2025-05-01 18:40:41 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:34:47.949669"
    },
    {
      "arxiv_id": "2404.18604v1",
      "title": "CSTalk: Correlation Supervised Speech-driven 3D Emotional Facial Animation Generation",
      "title_zh": "CSTalk：相关性监督的语音驱动 3D 情感面部动画生成",
      "authors": [
        "Xiangyu Liang",
        "Wenlin Zhuang",
        "Tianyong Wang",
        "Guangxing Geng",
        "Guangyue Geng",
        "Haifeng Xia",
        "Siyu Xia"
      ],
      "abstract": "Speech-driven 3D facial animation technology has been developed for years,\nbut its practical application still lacks expectations. The main challenges lie\nin data limitations, lip alignment, and the naturalness of facial expressions.\nAlthough lip alignment has seen many related studies, existing methods struggle\nto synthesize natural and realistic expressions, resulting in a mechanical and\nstiff appearance of facial animations. Even with some research extracting\nemotional features from speech, the randomness of facial movements limits the\neffective expression of emotions. To address this issue, this paper proposes a\nmethod called CSTalk (Correlation Supervised) that models the correlations\namong different regions of facial movements and supervises the training of the\ngenerative model to generate realistic expressions that conform to human facial\nmotion patterns. To generate more intricate animations, we employ a rich set of\ncontrol parameters based on the metahuman character model and capture a dataset\nfor five different emotions. We train a generative network using an autoencoder\nstructure and input an emotion embedding vector to achieve the generation of\nuser-control expressions. Experimental results demonstrate that our method\noutperforms existing state-of-the-art methods.",
      "tldr_zh": "该论文探讨了语音驱动3D面部动画技术的挑战，包括数据限制、唇部对齐和表情自然性问题，现有方法往往导致动画机械僵硬。作者提出CSTalk方法，通过Correlation Supervised建模面部不同区域运动的相关性，并监督生成模型的训练，以生成符合人类面部运动模式的情感真实表情。为此，他们使用基于metahuman字符模型的丰富控制参数，捕获五种情感的数据集，并训练一个autoencoder结构的生成网络，输入情感嵌入向量实现用户控制动画。实验结果表明，CSTalk在性能上优于现有最先进方法。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18604v1",
      "published_date": "2024-04-29 11:19:15 UTC",
      "updated_date": "2024-04-29 11:19:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:34:59.740747"
    },
    {
      "arxiv_id": "2405.04585v1",
      "title": "PoPE: Legendre Orthogonal Polynomials Based Position Encoding for Large Language Models",
      "title_zh": "PoPE：基于勒热德尔正交多项式的位置编码，用于大语言模型",
      "authors": [
        "Arpit Aggarwal"
      ],
      "abstract": "There are several improvements proposed over the baseline Absolute Positional\nEncoding (APE) method used in original transformer. In this study, we aim to\ninvestigate the implications of inadequately representing positional encoding\nin higher dimensions on crucial aspects of the attention mechanism, the model's\ncapacity to learn relative positional information, and the convergence of\nmodels, all stemming from the choice of sinusoidal basis functions. Through a\ncombination of theoretical insights and empirical analyses, we elucidate how\nthese challenges extend beyond APEs and may adversely affect the performance of\nRelative Positional Encoding (RPE) methods, such as Rotatory Positional\nEncoding (RoPE).\n  Subsequently, we introduce an innovative solution termed Orthogonal\nPolynomial Based Positional Encoding (PoPE) to address some of the limitations\nassociated with existing methods. The PoPE method encodes positional\ninformation by leveraging Orthogonal Legendre polynomials. Legendre polynomials\nas basis functions offers several desirable properties for positional encoding,\nincluding improved correlation structure, non-periodicity, orthogonality, and\ndistinct functional forms among polynomials of varying orders. Our experimental\nfindings demonstrate that transformer models incorporating PoPE outperform\nbaseline transformer models on the $Multi30k$ English-to-German translation\ntask, thus establishing a new performance benchmark. Furthermore, PoPE-based\ntransformers exhibit significantly accelerated convergence rates.\n  Additionally, we will present novel theoretical perspectives on position\nencoding based on the superior performance of PoPE.",
      "tldr_zh": "该论文分析了传统 Absolute Positional Encoding (APE) 方法使用正弦基函数的不足，包括对注意力机制、相对位置信息学习和模型收敛的负面影响，并扩展到 Relative Positional Encoding (RPE) 如 RoPE 的潜在问题。作者提出了一种创新方法 Orthogonal Polynomial Based Positional Encoding (PoPE)，利用 Legendre polynomials 作为基函数，提供更好的相关结构、非周期性和正交性，从而改善位置编码。实验结果显示，采用 PoPE 的 transformer 模型在 Multi30k 英德翻译任务上超越基线模型，并显著加速收敛，同时论文提供了基于 PoPE 性能的新理论见解。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.04585v1",
      "published_date": "2024-04-29 10:30:59 UTC",
      "updated_date": "2024-04-29 10:30:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:35:11.910891"
    },
    {
      "arxiv_id": "2404.18564v1",
      "title": "Injecting Salesperson's Dialogue Strategies in Large Language Models with Chain-of-Thought Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Wen-Yu Chang",
        "Yun-Nung Chen"
      ],
      "abstract": "Recent research in dialogue systems and corpora has focused on two main\ncategories: task-oriented (TOD) and open-domain (chit-chat) dialogues. TOD\nsystems help users accomplish specific tasks, while open-domain systems aim to\ncreate engaging conversations. However, in real-world scenarios, user intents\nare often revealed during interactions. A recent study introduced SalesBot,\nwhich simulates dialogues transitioning from chit-chat to task-oriented\nscenarios to train sales agents. Unfortunately, the initial data lacked smooth\ntransitions and coherent long-turn dialogues, resulting in poor naturalness in\nsales-customer interactions. To address these issues, this paper presents\nSalesBot 2.0, an improved dataset. It leverages commonsense knowledge from\nlarge language models (LLMs) through strategic prompting. Additionally, we\nintroduce a novel model called SalesAgent, trained on salesperson's\ninteractions, using chain-of-thought (CoT) reasoning. This model excels in\ntransitioning topics, understanding user intents, and selecting appropriate\nstrategies. Experiments using diverse user simulations validate the\neffectiveness of our method in controlling dialogue strategies in LLMs.\nFurthermore, SalesBot 2.0 enhances coherence and reduces aggression,\nfacilitating better model learning for sales-customer interactions.",
      "tldr_zh": "本文针对对话系统的不足，提出SalesBot 2.0数据集，通过Large Language Models (LLMs)的常识知识和战略提示，改进从闲聊到任务导向的销售对话模拟，解决过渡不平滑和连贯性差的问题。同时，引入SalesAgent模型，使用Chain-of-Thought (CoT)推理训练，使其在主题过渡、用户意图理解和策略选择方面表现出色。实验结果显示，SalesAgent在多样化用户模拟中有效提升对话策略控制，增强对话连贯性并减少攻击性，促进更好的销售-客户互动学习。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "arXiv admin note: substantial text overlap with arXiv:2308.14266",
      "pdf_url": "http://arxiv.org/pdf/2404.18564v1",
      "published_date": "2024-04-29 10:12:04 UTC",
      "updated_date": "2024-04-29 10:12:04 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:35:23.848050"
    },
    {
      "arxiv_id": "2405.00734v2",
      "title": "EEG-MACS: Manifold Attention and Confidence Stratification for EEG-based Cross-Center Brain Disease Diagnosis under Unreliable Annotations",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenxi Song",
        "Ruihan Qin",
        "Huixia Ren",
        "Zhen Liang",
        "Yi Guo",
        "Min Zhang",
        "Zhiguo Zhang"
      ],
      "abstract": "Cross-center data heterogeneity and annotation unreliability significantly\nchallenge the intelligent diagnosis of diseases using brain signals. A notable\nexample is the EEG-based diagnosis of neurodegenerative diseases, which\nfeatures subtler abnormal neural dynamics typically observed in small-group\nsettings. To advance this area, in this work, we introduce a transferable\nframework employing Manifold Attention and Confidence Stratification (MACS) to\ndiagnose neurodegenerative disorders based on EEG signals sourced from four\ncenters with unreliable annotations. The MACS framework's effectiveness stems\nfrom these features: 1) The Augmentor generates various EEG-represented brain\nvariants to enrich the data space; 2) The Switcher enhances the feature space\nfor trusted samples and reduces overfitting on incorrectly labeled samples; 3)\nThe Encoder uses the Riemannian manifold and Euclidean metrics to capture\nspatiotemporal variations and dynamic synchronization in EEG; 4) The Projector,\nequipped with dual heads, monitors consistency across multiple brain variants\nand ensures diagnostic accuracy; 5) The Stratifier adaptively stratifies\nlearned samples by confidence levels throughout the training process; 6)\nForward and backpropagation in MACS are constrained by confidence\nstratification to stabilize the learning system amid unreliable annotations.\nOur subject-independent experiments, conducted on both neurocognitive and\nmovement disorders using cross-center corpora, have demonstrated superior\nperformance compared to existing related algorithms. This work not only\nimproves EEG-based diagnostics for cross-center and small-setting brain\ndiseases but also offers insights into extending MACS techniques to other data\nanalyses, tackling data heterogeneity and annotation unreliability in\nmultimedia and multimodal content understanding.",
      "tldr_zh": "本文提出 EEG-MACS 框架，用于基于 EEG 信号的跨中心脑病诊断，特别针对数据异质性和标注不可靠性的挑战。该框架通过 Augmentor 生成 EEG 变体丰富数据空间、Switcher 增强可信样本特征并减少过拟合、Encoder 利用 Riemannian manifold 和 Euclidean metrics 捕捉 EEG 的时空变化和动态同步，以及 Projector 和 Stratifier 确保变体一致性和置信度分层，从而稳定训练过程。实验结果显示，EEG-MACS 在神经认知和运动障碍的跨中心语料上优于现有算法，不仅提升了脑病诊断性能，还为处理数据异质性和标注问题提供可扩展的见解。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.00734v2",
      "published_date": "2024-04-29 10:08:43 UTC",
      "updated_date": "2024-08-13 16:03:38 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:35:36.147231"
    },
    {
      "arxiv_id": "2404.18558v1",
      "title": "LangBiTe: A Platform for Testing Bias in Large Language Models",
      "title_zh": "LangBiTe：用于测试大型语言模型中偏见的平台",
      "authors": [
        "Sergio Morales",
        "Robert Clarisó",
        "Jordi Cabot"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into various software\napplications raises concerns about their potential biases. Typically, those\nmodels are trained on a vast amount of data scrapped from forums, websites,\nsocial media and other internet sources, which may instill harmful and\ndiscriminating behavior into the model. To address this issue, we present\nLangBiTe, a testing platform to systematically assess the presence of biases\nwithin an LLM. LangBiTe enables development teams to tailor their test\nscenarios, and automatically generate and execute the test cases according to a\nset of user-defined ethical requirements. Each test consists of a prompt fed\ninto the LLM and a corresponding test oracle that scrutinizes the LLM's\nresponse for the identification of biases. LangBite provides users with the\nbias evaluation of LLMs, and end-to-end traceability between the initial\nethical requirements and the insights obtained.",
      "tldr_zh": "该研究引入了 LangBiTe，这是一个平台，用于系统评估大型语言模型 (LLMs) 中的潜在偏见，这些偏见可能源于训练数据如论坛和社交媒体。LangBiTe 允许开发团队根据用户定义的伦理要求自定义测试场景，并自动生成和执行测试用例，每个测试包括一个提示输入和一个测试预言机来检测响应中的偏见。平台提供 LLMs 的偏见评估，并实现从伦理要求到评估洞见的端到端可追溯性，从而帮助提升模型的公平性和可靠性。",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18558v1",
      "published_date": "2024-04-29 10:02:45 UTC",
      "updated_date": "2024-04-29 10:02:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:35:46.097916"
    },
    {
      "arxiv_id": "2404.18555v1",
      "title": "Machine Learning for Quantum Computing Specialists",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Goldsmith",
        "M M Hassan Mahmud"
      ],
      "abstract": "Quantum machine learning (QML) is a promising early use case for quantum\ncomputing. There has been progress in the last five years from theoretical\nstudies and numerical simulations to proof of concepts. Use cases demonstrated\non contemporary quantum devices include classifying medical images and items\nfrom the Iris dataset, classifying and generating handwritten images, toxicity\nscreening, and learning a probability distribution. Potential benefits of QML\ninclude faster training and identification of feature maps not found\nclassically. Although, these examples lack the scale for commercial\nexploitation, and it may be several years before QML algorithms replace the\nclassical solutions, QML is an exciting area.\n  This article is written for those who already have a sound knowledge of\nquantum computing and now wish to gain a basic overview of the terminology and\nsome applications of classical machine learning ready to study quantum machine\nlearning. The reader will already understand the relevant relevant linear\nalgebra, including Hilbert spaces, a vector space with an inner product.",
      "tldr_zh": "这篇论文针对量子计算专家，介绍了量子机器学习 (QML) 作为量子计算的早期应用，回顾了过去五年从理论研究、数值模拟到实际演示的进展，包括用例如分类医疗图像、手写图像和毒性筛选。QML 的潜在优势包括更快训练和发现经典方法无法识别的特征映射，尽管当前缺乏商业规模，可能需要数年才能取代经典算法。文章旨在为读者提供经典机器学习的术语和应用概述，帮助他们过渡到 QML 学习。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "I.2.m"
      ],
      "primary_category": "quant-ph",
      "comment": "32 pages, 21 figures, technical report",
      "pdf_url": "http://arxiv.org/pdf/2404.18555v1",
      "published_date": "2024-04-29 09:54:06 UTC",
      "updated_date": "2024-04-29 09:54:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:35:59.080758"
    },
    {
      "arxiv_id": "2404.18553v1",
      "title": "Evaluating the effectiveness of predicting covariates in LSTM Networks for Time Series Forecasting",
      "title_zh": "评估在 LSTM 网络中预测协变量的有效性，用于时间序列预测",
      "authors": [
        "Gareth Davies"
      ],
      "abstract": "Autoregressive Recurrent Neural Networks are widely employed in time-series\nforecasting tasks, demonstrating effectiveness in univariate and certain\nmultivariate scenarios. However, their inherent structure does not readily\naccommodate the integration of future, time-dependent covariates. A proposed\nsolution, outlined by Salinas et al 2019, suggests forecasting both covariates\nand the target variable in a multivariate framework. In this study, we\nconducted comprehensive tests on publicly available time-series datasets,\nartificially introducing highly correlated covariates to future time-step\nvalues. Our evaluation aimed to assess the performance of an LSTM network when\nconsidering these covariates and compare it against a univariate baseline. As\npart of this study we introduce a novel approach using seasonal time segments\nin combination with an RNN architecture, which is both simple and extremely\neffective over long forecast horizons with comparable performance to many state\nof the art architectures. Our findings from the results of more than 120 models\nreveal that under certain conditions jointly training covariates with target\nvariables can improve overall performance of the model, but often there exists\na significant performance disparity between multivariate and univariate\npredictions. Surprisingly, even when provided with covariates informing the\nnetwork about future target values, multivariate predictions exhibited inferior\nperformance. In essence, compelling the network to predict multiple values can\nprove detrimental to model performance, even in the presence of informative\ncovariates. These results suggest that LSTM architectures may not be suitable\nfor forecasting tasks where predicting covariates would typically be expected\nto enhance model accuracy.",
      "tldr_zh": "这篇论文评估了在 LSTM 网络中预测协变量对时间序列预测的有效性，通过在公开数据集上引入高度相关的协变量，并与单变量基线进行比较。研究引入了一种新方法，使用季节性时间段结合 RNN 架构，该方法简单且在长预测周期内表现出色，与许多最先进模型相当。结果显示，联合训练协变量和目标变量在某些条件下可提升整体性能，但多变量预测往往显著劣于单变量预测，即使提供信息丰富的协变量也如此。最终，论文得出结论，LSTM 架构可能不适合需要预测协变量的任务，因为强制预测多个值可能损害模型准确性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "I.2.6"
      ],
      "primary_category": "cs.LG",
      "comment": "9 content pages (22 total pages), 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.18553v1",
      "published_date": "2024-04-29 09:51:25 UTC",
      "updated_date": "2024-04-29 09:51:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:36:13.279840"
    },
    {
      "arxiv_id": "2404.18552v1",
      "title": "SIDBench: A Python Framework for Reliably Assessing Synthetic Image Detection Methods",
      "title_zh": "SIDBench：一个 Python 框架，用于可靠评估合成图像检测方法",
      "authors": [
        "Manos Schinas",
        "Symeon Papadopoulos"
      ],
      "abstract": "The generative AI technology offers an increasing variety of tools for\ngenerating entirely synthetic images that are increasingly indistinguishable\nfrom real ones. Unlike methods that alter portions of an image, the creation of\ncompletely synthetic images presents a unique challenge and several Synthetic\nImage Detection (SID) methods have recently appeared to tackle it. Yet, there\nis often a large gap between experimental results on benchmark datasets and the\nperformance of methods in the wild. To better address the evaluation needs of\nSID and help close this gap, this paper introduces a benchmarking framework\nthat integrates several state-of-the-art SID models. Our selection of\nintegrated models was based on the utilization of varied input features, and\ndifferent network architectures, aiming to encompass a broad spectrum of\ntechniques. The framework leverages recent datasets with a diverse set of\ngenerative models, high level of photo-realism and resolution, reflecting the\nrapid improvements in image synthesis technology. Additionally, the framework\nenables the study of how image transformations, common in assets shared online,\nsuch as JPEG compression, affect detection performance. SIDBench is available\non https://github.com/mever-team/sidbench and is designed in a modular manner\nto enable easy inclusion of new datasets and SID models.",
      "tldr_zh": "这篇论文介绍了SIDBench，一个Python框架，用于可靠评估Synthetic Image Detection (SID)方法，以解决现有方法在基准数据集上表现与实际应用中的性能差距。框架集成了多种基于不同输入特征和网络架构的先进SID模型，并利用多样化数据集（包括高逼真度和高分辨率的合成图像），以反映生成式AI技术的快速发展。SIDBench还支持研究图像变换（如JPEG compression）对检测性能的影响，并采用模块化设计，便于添加新数据集和模型，在GitHub上开源可用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18552v1",
      "published_date": "2024-04-29 09:50:16 UTC",
      "updated_date": "2024-04-29 09:50:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:36:23.280616"
    },
    {
      "arxiv_id": "2404.18541v1",
      "title": "Machine Learning for Windows Malware Detection and Classification: Methods, Challenges and Ongoing Research",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Gibert"
      ],
      "abstract": "In this chapter, readers will explore how machine learning has been applied\nto build malware detection systems designed for the Windows operating system.\nThis chapter starts by introducing the main components of a Machine Learning\npipeline, highlighting the challenges of collecting and maintaining up-to-date\ndatasets. Following this introduction, various state-of-the-art malware\ndetectors are presented, encompassing both feature-based and deep\nlearning-based detectors. Subsequent sections introduce the primary challenges\nencountered by machine learning-based malware detectors, including concept\ndrift and adversarial attacks. Lastly, this chapter concludes by providing a\nbrief overview of the ongoing research on adversarial defenses.",
      "tldr_zh": "这篇论文探讨了机器学习(Machine Learning)在Windows操作系统恶意软件检测和分类中的应用，包括方法、挑战和当前研究。首先，它介绍了Machine Learning管道的主要组件，并强调了收集和维护最新数据集的难题。随后，呈现了各种最先进的恶意软件检测器，如基于特征(feature-based)和基于深度学习(deep learning-based)的检测器。论文还讨论了关键挑战，包括概念漂移(concept drift)和对抗性攻击(adversarial attacks)。最后，它概述了对抗性防御(adversarial defenses)的ongoing research，以提升检测系统的鲁棒性。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18541v1",
      "published_date": "2024-04-29 09:28:57 UTC",
      "updated_date": "2024-04-29 09:28:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:36:35.200793"
    },
    {
      "arxiv_id": "2404.18539v2",
      "title": "Enhancing Boundary Segmentation for Topological Accuracy with Skeleton-based Methods",
      "title_zh": "基于骨架方法增强边界分割的拓扑准确性",
      "authors": [
        "Chuni Liu",
        "Boyuan Ma",
        "Xiaojuan Ban",
        "Yujie Xie",
        "Hao Wang",
        "Weihua Xue",
        "Jingchao Ma",
        "Ke Xu"
      ],
      "abstract": "Topological consistency plays a crucial role in the task of boundary\nsegmentation for reticular images, such as cell membrane segmentation in neuron\nelectron microscopic images, grain boundary segmentation in material\nmicroscopic images and road segmentation in aerial images. In these fields,\ntopological changes in segmentation results have a serious impact on the\ndownstream tasks, which can even exceed the misalignment of the boundary\nitself. To enhance the topology accuracy in segmentation results, we propose\nthe Skea-Topo Aware loss, which is a novel loss function that takes into\naccount the shape of each object and topological significance of the pixels. It\nconsists of two components. First, a skeleton-aware weighted loss improves the\nsegmentation accuracy by better modeling the object geometry with skeletons.\nSecond, a boundary rectified term effectively identifies and emphasizes\ntopological critical pixels in the prediction errors using both foreground and\nbackground skeletons in the ground truth and predictions. Experiments prove\nthat our method improves topological consistency by up to 7 points in VI\ncompared to 13 state-of-art methods, based on objective and subjective\nassessments across three different boundary segmentation datasets. The code is\navailable at https://github.com/clovermini/Skea_topo.",
      "tldr_zh": "本文针对边界分割任务中的拓扑一致性问题（如神经元电子显微镜图像的细胞膜分割），提出了一种基于骨架的方法来提升分割准确性。核心创新是Skea-Topo Aware loss函数，该函数包括skeleton-aware weighted loss用于更好地建模物体几何，以及boundary rectified term来识别并强调预测错误中的拓扑关键像素。实验结果显示，该方法在三个边界分割数据集上，与13种最先进方法相比，在VI指标上提高了多达7点的拓扑一致性，并提供了开源代码。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18539v2",
      "published_date": "2024-04-29 09:27:31 UTC",
      "updated_date": "2024-05-07 13:55:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:36:48.400867"
    },
    {
      "arxiv_id": "2404.18534v2",
      "title": "Evaluating and Mitigating Linguistic Discrimination in Large Language Models",
      "title_zh": "评估并缓解大语言模型中的语言歧视",
      "authors": [
        "Guoliang Dong",
        "Haoyu Wang",
        "Jun Sun",
        "Xinyu Wang"
      ],
      "abstract": "By training on text in various languages, large language models (LLMs)\ntypically possess multilingual support and demonstrate remarkable capabilities\nin solving tasks described in different languages. However, LLMs can exhibit\nlinguistic discrimination due to the uneven distribution of training data\nacross languages. That is, LLMs are hard to keep the consistency of responses\nwhen faced with the same task but depicted in different languages.\n  In this study, we first explore the consistency in the LLMs' outputs\nresponding to queries in various languages from two aspects: safety and\nquality. We conduct this analysis with two datasets (AdvBench and NQ) based on\nfour LLMs (Llama2-13b, Gemma-7b, GPT-3.5-turbo and Gemini-pro). The results\nshow that LLMs exhibit stronger human alignment capabilities with queries in\nEnglish, French, Russian, and Spanish (only 1.04\\% of harmful queries\nsuccessfully jailbreak on average) compared to queries in Bengali, Georgian,\nNepali and Maithili (27.7\\% of harmful queries jailbreak successfully on\naverage). Moreover, for queries in English, Danish, Czech and Slovenian, LLMs\ntend to produce responses with a higher quality (with 0.1494 $F_1$ score on\naverage) compared to the other languages. Upon these findings, we propose\nLDFighter, a similarity-based voting, to mitigate the linguistic discrimination\nin LLMs. LDFighter ensures consistent service for different language speakers.\nWe evaluate LDFighter with both benign queries and harmful queries. The results\nshow that LDFighter not only significantly reduces the jailbreak success rate\nbut also improve the response quality on average, demonstrating its\neffectiveness.",
      "tldr_zh": "本文评估了大型语言模型(LLMs)中的语言歧视问题，发现LLMs在处理不同语言查询时，响应一致性存在显著差异：如对英语、法语、俄语和西班牙语的查询，安全性和质量更高（平均越狱成功率仅1.04%，F1分数为0.1494），而对孟加拉语、格鲁吉亚语等语言则较差（平均越狱成功率达27.7%）。研究使用AdvBench和NQ数据集以及Llama2-13b、Gemma-7b、GPT-3.5-turbo和Gemini-pro等模型进行分析。作者提出LDFighter，一种基于相似性的投票机制，用于缓解语言歧视，确保不同语言用户的响应一致性。实验结果显示，LDFighter显著降低了有害查询的越狱成功率，并整体提升了响应质量。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18534v2",
      "published_date": "2024-04-29 09:22:54 UTC",
      "updated_date": "2024-05-10 07:09:02 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:37:01.951739"
    },
    {
      "arxiv_id": "2404.18533v3",
      "title": "Evaluating Readability and Faithfulness of Concept-based Explanations",
      "title_zh": "评估基于概念的解释的可读性和忠实性",
      "authors": [
        "Meng Li",
        "Haoran Jin",
        "Ruixuan Huang",
        "Zhihao Xu",
        "Defu Lian",
        "Zijia Lin",
        "Di Zhang",
        "Xiting Wang"
      ],
      "abstract": "With the growing popularity of general-purpose Large Language Models (LLMs),\ncomes a need for more global explanations of model behaviors. Concept-based\nexplanations arise as a promising avenue for explaining high-level patterns\nlearned by LLMs. Yet their evaluation poses unique challenges, especially due\nto their non-local nature and high dimensional representation in a model's\nhidden space. Current methods approach concepts from different perspectives,\nlacking a unified formalization. This makes evaluating the core measures of\nconcepts, namely faithfulness or readability, challenging. To bridge the gap,\nwe introduce a formal definition of concepts generalizing to diverse\nconcept-based explanations' settings. Based on this, we quantify the\nfaithfulness of a concept explanation via perturbation. We ensure adequate\nperturbation in the high-dimensional space for different concepts via an\noptimization problem. Readability is approximated via an automatic and\ndeterministic measure, quantifying the coherence of patterns that maximally\nactivate a concept while aligning with human understanding. Finally, based on\nmeasurement theory, we apply a meta-evaluation method for evaluating these\nmeasures, generalizable to other types of explanations or tasks as well.\nExtensive experimental analysis has been conducted to inform the selection of\nexplanation evaluation measures.",
      "tldr_zh": "这篇论文评估了基于概念的解释（Concept-based explanations）在大型语言模型（Large Language Models, LLMs）中的可读性（Readability）和忠实度（Faithfulness），以解决当前方法缺乏统一正式化的挑战。作者引入了一个通用的概念定义，并通过扰动（Perturbation）方法结合优化问题来量化忠实度，确保在高维空间中进行适当调整；同时，使用一个自动测度来评估可读性，该测度量化了最大激活概念的模式一致性并与人类理解对齐。论文还基于测量理论应用元评估（Meta-evaluation）方法，并通过广泛实验分析指导解释评估测度的选择，为改进LLMs解释框架提供了实用工具。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "EMNLP 2024; code:\n  https://github.com/hr-jin/Concept-Explanation-Evaluation",
      "pdf_url": "http://arxiv.org/pdf/2404.18533v3",
      "published_date": "2024-04-29 09:20:25 UTC",
      "updated_date": "2024-10-04 01:21:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:37:12.282712"
    },
    {
      "arxiv_id": "2404.18532v2",
      "title": "MileBench: Benchmarking MLLMs in Long Context",
      "title_zh": "翻译失败",
      "authors": [
        "Dingjie Song",
        "Shunian Chen",
        "Guiming Hardy Chen",
        "Fei Yu",
        "Xiang Wan",
        "Benyou Wang"
      ],
      "abstract": "Despite the advancements and impressive performance of Multimodal Large\nLanguage Models (MLLMs) on benchmarks, their effectiveness in real-world,\nlong-context, and multi-image tasks is unclear due to the benchmarks' limited\nscope. Existing benchmarks often focus on single-image and short-text samples,\nand when assessing multi-image tasks, they either limit the image count or\nfocus on specific task (e.g time-series captioning), potentially obscuring the\nperformance challenges of MLLMs. To address these limitations, we introduce\nMileBench, a pioneering benchmark designed to test the MultImodal Long-contExt\ncapabilities of MLLMs. This benchmark comprises not only multimodal long\ncontexts, but also multiple tasks requiring both comprehension and generation.\nWe establish two distinct evaluation sets, diagnostic and realistic, to\nsystematically assess MLLMs' long-context adaptation capacity and their ability\nto complete tasks in long-context scenarios. Our experimental results, obtained\nfrom testing 22 models, revealed that while the closed-source GPT-4o\noutperforms others, most open-source MLLMs struggle in long-context situations.\nInterestingly, the performance gap tends to widen with an increase in the\nnumber of images. We strongly encourage an intensification of research efforts\ntowards enhancing MLLMs' long-context capabilities, especially in scenarios\ninvolving multiple images.",
      "tldr_zh": "这项研究指出了现有多模态大语言模型(MLLMs)基准测试的局限性，即主要关注单图像和短文本，而忽略了真实世界的长上下文和多图像任务。为此，研究者引入了MileBench，这是一个创新基准测试，用于评估MLLMs在多模态长上下文中的理解和生成能力，包括诊断集和真实集两部分。实验测试了22个模型，结果显示GPT-4o表现最佳，但大多数开源MLLMs在长上下文场景中挣扎，且随着图像数量增加，性能差距进一步扩大。该研究呼吁加强针对多图像长上下文能力的MLLMs研究，以提升其实际应用潜力。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "31 pages, 13 figures, 14 tables; We add results of GPT-4o in this\n  version",
      "pdf_url": "http://arxiv.org/pdf/2404.18532v2",
      "published_date": "2024-04-29 09:19:05 UTC",
      "updated_date": "2024-05-15 05:43:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:37:23.525566"
    },
    {
      "arxiv_id": "2404.18531v2",
      "title": "A Framework to Model ML Engineering Processes",
      "title_zh": "翻译失败",
      "authors": [
        "Sergio Morales",
        "Robert Clarisó",
        "Jordi Cabot"
      ],
      "abstract": "The development of Machine Learning (ML) based systems is complex and\nrequires multidisciplinary teams with diverse skill sets. This may lead to\ncommunication issues or misapplication of best practices. Process models can\nalleviate these challenges by standardizing task orchestration, providing a\ncommon language to facilitate communication, and nurturing a collaborative\nenvironment. Unfortunately, current process modeling languages are not suitable\nfor describing the development of such systems. In this paper, we introduce a\nframework for modeling ML-based software development processes, built around a\ndomain-specific language and derived from an analysis of scientific and gray\nliterature. A supporting toolkit is also available.",
      "tldr_zh": "该论文针对机器学习 (ML) 系统开发的复杂性及其带来的多学科团队沟通问题和最佳实践误用，提出一个框架来建模 ML 工程过程。该框架基于一个领域特定语言 (domain-specific language)，通过分析科学和灰色文献来设计，确保标准化任务编排并提供共同语言以促进协作。该框架还附带支持工具包，有助于改善 ML 基于软件开发的效率和团队合作。",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18531v2",
      "published_date": "2024-04-29 09:17:36 UTC",
      "updated_date": "2024-08-28 14:12:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:37:34.329818"
    },
    {
      "arxiv_id": "2404.18527v1",
      "title": "Bridging Data Barriers among Participants: Assessing the Potential of Geoenergy through Federated Learning",
      "title_zh": "桥接参与者之间的数据障碍：通过联邦学习评估地能的潜力",
      "authors": [
        "Weike Peng",
        "Jiaxin Gao",
        "Yuntian Chen",
        "Shengwei Wang"
      ],
      "abstract": "Machine learning algorithms emerge as a promising approach in energy fields,\nbut its practical is hindered by data barriers, stemming from high collection\ncosts and privacy concerns. This study introduces a novel federated learning\n(FL) framework based on XGBoost models, enabling safe collaborative modeling\nwith accessible yet concealed data from multiple parties. Hyperparameter tuning\nof the models is achieved through Bayesian Optimization. To ascertain the\nmerits of the proposed FL-XGBoost method, a comparative analysis is conducted\nbetween separate and centralized models to address a classical binary\nclassification problem in geoenergy sector. The results reveal that the\nproposed FL framework strikes an optimal balance between privacy and accuracy.\nFL models demonstrate superior accuracy and generalization capabilities\ncompared to separate models, particularly for participants with limited data or\nlow correlation features and offers significant privacy benefits compared to\ncentralized model. The aggregated optimization approach within the FL agreement\nproves effective in tuning hyperparameters. This study opens new avenues for\nassessing unconventional reservoirs through collaborative and\nprivacy-preserving FL techniques.",
      "tldr_zh": "该研究针对机器学习在能源领域受数据收集成本和隐私问题阻碍的问题，提出了一种基于 XGBoost 的联邦学习 (FL) 框架，允许多方安全协作建模，同时通过 Bayesian Optimization 进行超参数调优。实验结果显示，该 FL 框架在二元分类任务中比分离模型具有更高的准确性和泛化能力，尤其适用于数据量有限或特征相关性低的参与者，并相较于集中模型提供了显著的隐私保护优势。总体而言，此方法为通过协作式和隐私保护的 FL 技术评估非常规储层潜力开辟了新途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "stat.AP"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18527v1",
      "published_date": "2024-04-29 09:12:31 UTC",
      "updated_date": "2024-04-29 09:12:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:37:47.484509"
    },
    {
      "arxiv_id": "2405.01589v2",
      "title": "GPT-4 passes most of the 297 written Polish Board Certification Examinations",
      "title_zh": "翻译失败",
      "authors": [
        "Jakub Pokrywka",
        "Jeremi Kaczmarek",
        "Edward Gorzelańczyk"
      ],
      "abstract": "Introduction: Recently, the effectiveness of Large Language Models (LLMs) has\nincreased rapidly, allowing them to be used in a great number of applications.\nHowever, the risks posed by the generation of false information through LLMs\nsignificantly limit their applications in sensitive areas such as healthcare,\nhighlighting the necessity for rigorous validations to determine their utility\nand reliability. To date, no study has extensively compared the performance of\nLLMs on Polish medical examinations across a broad spectrum of specialties on a\nvery large dataset. Objectives: This study evaluated the performance of three\nGenerative Pretrained Transformer (GPT) models on the Polish Board\nCertification Exam (Pa\\'nstwowy Egzamin Specjalizacyjny, PES) dataset, which\nconsists of 297 tests. Methods: We developed a software program to download and\nprocess PES exams and tested the performance of GPT models using OpenAI\nApplication Programming Interface. Results: Our findings reveal that GPT-3.5\ndid not pass any of the analyzed exams. In contrast, the GPT-4 models\ndemonstrated the capability to pass the majority of the exams evaluated, with\nthe most recent model, gpt-4-0125, successfully passing 222 (75%) of them. The\nperformance of the GPT models varied significantly, displaying excellence in\nexams related to certain specialties while completely failing others.\nConclusions: The significant progress and impressive performance of LLM models\nhold great promise for the increased application of AI in the field of medicine\nin Poland. For instance, this advancement could lead to the development of\nAI-based medical assistants for healthcare professionals, enhancing the\nefficiency and accuracy of medical services.",
      "tldr_zh": "本研究评估了三个GPT模型在波兰专业认证考试（PES）上的表现，这些考试涵盖297份不同医疗专业的测试。研究团队开发了软件通过OpenAI API下载和处理考试数据，结果显示GPT-3.5未通过任何考试，而GPT-4模型，尤其是gpt-4-0125，通过了222份（75%）的考试，但表现因专业而异。该研究突显了Large Language Models (LLMs)在医疗领域的潜力，如开发AI医疗助手来提升服务效率，但也强调了验证其可靠性的必要性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.01589v2",
      "published_date": "2024-04-29 09:08:22 UTC",
      "updated_date": "2024-05-09 10:21:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:37:58.969077"
    },
    {
      "arxiv_id": "2404.18519v3",
      "title": "On the Impact of Data Heterogeneity in Federated Learning Environments with Application to Healthcare Networks",
      "title_zh": "翻译失败",
      "authors": [
        "Usevalad Milasheuski",
        "Luca Barbieri",
        "Bernardo Camajori Tedeschini",
        "Monica Nicoli",
        "Stefano Savazzi"
      ],
      "abstract": "Federated Learning (FL) allows multiple privacy-sensitive applications to\nleverage their dataset for a global model construction without any disclosure\nof the information. One of those domains is healthcare, where groups of silos\ncollaborate in order to generate a global predictor with improved accuracy and\ngeneralization. However, the inherent challenge lies in the high heterogeneity\nof medical data, necessitating sophisticated techniques for assessment and\ncompensation. This paper presents a comprehensive exploration of the\nmathematical formalization and taxonomy of heterogeneity within FL\nenvironments, focusing on the intricacies of medical data. In particular, we\naddress the evaluation and comparison of the most popular FL algorithms with\nrespect to their ability to cope with quantity-based, feature and label\ndistribution-based heterogeneity. The goal is to provide a quantitative\nevaluation of the impact of data heterogeneity in FL systems for healthcare\nnetworks as well as a guideline on FL algorithm selection. Our research extends\nbeyond existing studies by benchmarking seven of the most common FL algorithms\nagainst the unique challenges posed by medical data use cases. The paper\ntargets the prediction of the risk of stroke recurrence through a set of\ntabular clinical reports collected by different federated hospital silos: data\nheterogeneity frequently encountered in this scenario and its impact on FL\nperformance are discussed.",
      "tldr_zh": "本论文探讨了 Federated Learning (FL) 环境中数据异质性对医疗网络的影响，特别针对医疗数据的固有多样性问题，如数量-based、特征分布-based 和标签分布-based 异质性。研究通过数学形式化和分类方法，评估并比较了七种常见 FL 算法在处理这些异质性方面的性能，并以预测中风复发风险为实际案例，量化了数据异质性对系统准确性和泛化能力的影响。最终，论文提供了 FL 算法选择的实用指南，帮助医疗数据孤岛协作构建更可靠的全局模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18519v3",
      "published_date": "2024-04-29 09:05:01 UTC",
      "updated_date": "2024-09-05 12:24:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:38:12.638555"
    },
    {
      "arxiv_id": "2404.18518v1",
      "title": "From ChatGPT, DALL-E 3 to Sora: How has Generative AI Changed Digital Humanities Research and Services?",
      "title_zh": "从 ChatGPT、DALL-E 3 到 Sora：生成式",
      "authors": [
        "Jiangfeng Liu",
        "Ziyi Wang",
        "Jing Xie",
        "Lei Pei"
      ],
      "abstract": "Generative large-scale language models create the fifth paradigm of\nscientific research, organically combine data science and computational\nintelligence, transform the research paradigm of natural language processing\nand multimodal information processing, promote the new trend of AI-enabled\nsocial science research, and provide new ideas for digital humanities research\nand application. This article profoundly explores the application of\nlarge-scale language models in digital humanities research, revealing their\nsignificant potential in ancient book protection, intelligent processing, and\nacademic innovation. The article first outlines the importance of ancient book\nresources and the necessity of digital preservation, followed by a detailed\nintroduction to developing large-scale language models, such as ChatGPT, and\ntheir applications in document management, content understanding, and\ncross-cultural research. Through specific cases, the article demonstrates how\nAI can assist in the organization, classification, and content generation of\nancient books. Then, it explores the prospects of AI applications in artistic\ninnovation and cultural heritage preservation. Finally, the article explores\nthe challenges and opportunities in the interaction of technology, information,\nand society in the digital humanities triggered by AI technologies.",
      "tldr_zh": "这篇论文探讨了生成式AI模型（如ChatGPT、DALL-E 3和Sora）如何重塑数字人文研究和服务，强调它们创建了科学研究的第五范式，并融合数据科学与计算智能。论文详细分析了这些模型在古籍保护、智能处理（如文档管理、内容理解和跨文化研究）以及学术创新中的应用潜力，通过具体案例展示了AI在古籍组织、分类和内容生成方面的实际益处。论文还展望了AI在艺术创新和文化遗产保护的前景，同时讨论了AI技术引发的挑战和机会，包括技术与社会互动的复杂性。",
      "categories": [
        "cs.DL",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.DL",
      "comment": "21 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2404.18518v1",
      "published_date": "2024-04-29 09:03:19 UTC",
      "updated_date": "2024-04-29 09:03:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:38:24.037771"
    },
    {
      "arxiv_id": "2404.18508v3",
      "title": "Scalable Event-by-event Processing of Neuromorphic Sensory Signals With Deep State-Space Models",
      "title_zh": "翻译失败",
      "authors": [
        "Mark Schöne",
        "Neeraj Mohan Sushma",
        "Jingyue Zhuge",
        "Christian Mayr",
        "Anand Subramoney",
        "David Kappel"
      ],
      "abstract": "Event-based sensors are well suited for real-time processing due to their\nfast response times and encoding of the sensory data as successive temporal\ndifferences. These and other valuable properties, such as a high dynamic range,\nare suppressed when the data is converted to a frame-based format. However,\nmost current methods either collapse events into frames or cannot scale up when\nprocessing the event data directly event-by-event. In this work, we address the\nkey challenges of scaling up event-by-event modeling of the long event streams\nemitted by such sensors, which is a particularly relevant problem for\nneuromorphic computing. While prior methods can process up to a few thousand\ntime steps, our model, based on modern recurrent deep state-space models,\nscales to event streams of millions of events for both training and inference.\nWe leverage their stable parameterization for learning long-range dependencies,\nparallelizability along the sequence dimension, and their ability to integrate\nasynchronous events effectively to scale them up to long event streams. We\nfurther augment these with novel event-centric techniques enabling our model to\nmatch or beat the state-of-the-art performance on several event stream\nbenchmarks. In the Spiking Speech Commands task, we improve state-of-the-art by\na large margin of 7.7% to 88.4%. On the DVS128-Gestures dataset, we achieve\ncompetitive results without using frames or convolutional neural networks. Our\nwork demonstrates, for the first time, that it is possible to use fully\nevent-based processing with purely recurrent networks to achieve\nstate-of-the-art task performance in several event-based benchmarks.",
      "tldr_zh": "本文提出了一种可扩展的事件型（event-by-event）处理方法，使用深度状态空间模型（deep state-space models）来处理 neuromorphic sensory signals，从而克服传统方法将事件转换为帧格式的局限。该模型利用稳定参数学习长距离依赖、序列维度的并行性和异步事件整合，能够处理数百万事件流，并在训练和推理中实现高效扩展。通过引入新的事件中心技术，该方法在多个基准上达到或超过最先进性能，例如在 Spiking Speech Commands 任务上将准确率提升至 88.4%（较现有方法提高 7.7%），并在 DVS128-Gestures 数据集上实现竞争性结果，而无需依赖帧或卷积神经网络（convolutional neural networks）。这项工作首次证明了使用完全事件型处理和纯递归网络在 neuromorphic 任务中取得最先进表现。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18508v3",
      "published_date": "2024-04-29 08:50:27 UTC",
      "updated_date": "2024-10-09 06:57:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:38:38.524985"
    },
    {
      "arxiv_id": "2404.18470v2",
      "title": "ECC Analyzer: Extract Trading Signal from Earnings Conference Calls using Large Language Model for Stock Performance Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Yupeng Cao",
        "Zhi Chen",
        "Qingyun Pei",
        "Nathan Jinseok Lee",
        "K. P. Subbalakshmi",
        "Papa Momar Ndiaye"
      ],
      "abstract": "In the realm of financial analytics, leveraging unstructured data, such as\nearnings conference calls (ECCs), to forecast stock volatility is a critical\nchallenge that has attracted both academics and investors. While previous\nstudies have used multimodal deep learning-based models to obtain a general\nview of ECCs for volatility predicting, they often fail to capture detailed,\ncomplex information. Our research introduces a novel framework: \\textbf{ECC\nAnalyzer}, which utilizes large language models (LLMs) to extract richer, more\npredictive content from ECCs to aid the model's prediction performance. We use\nthe pre-trained large models to extract textual and audio features from ECCs\nand implement a hierarchical information extraction strategy to extract more\nfine-grained information. This strategy first extracts paragraph-level general\ninformation by summarizing the text and then extracts fine-grained focus\nsentences using Retrieval-Augmented Generation (RAG). These features are then\nfused through multimodal feature fusion to perform volatility prediction.\nExperimental results demonstrate that our model outperforms traditional\nanalytical benchmarks, confirming the effectiveness of advanced LLM techniques\nin financial analysis.",
      "tldr_zh": "该研究提出了一种名为 ECC Analyzer 的新框架，利用大型语言模型 (LLMs) 从收益电话会议 (ECCs) 中提取交易信号，以提升股票波动性预测的准确性。框架采用层次化信息提取策略，首先通过文本总结获取段落级一般信息，然后使用 Retrieval-Augmented Generation (RAG) 提取细粒度的焦点句子，并结合文本和音频特征进行多模态特征融合。实验结果显示，该模型在预测性能上优于传统基准，验证了 LLMs 在金融分析中的有效性。",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.CL",
        "q-fin.RM",
        "q-fin.TR"
      ],
      "primary_category": "cs.CE",
      "comment": "9 pages, 1 figures, 2 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.18470v2",
      "published_date": "2024-04-29 07:11:39 UTC",
      "updated_date": "2024-08-29 23:13:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:38:46.768930"
    },
    {
      "arxiv_id": "2404.18465v3",
      "title": "M3oE: Multi-Domain Multi-Task Mixture-of Experts Recommendation Framework",
      "title_zh": "M3oE：多领域多任务混合专家推荐框架",
      "authors": [
        "Zijian Zhang",
        "Shuchang Liu",
        "Jiaao Yu",
        "Qingpeng Cai",
        "Xiangyu Zhao",
        "Chunxu Zhang",
        "Ziru Liu",
        "Qidong Liu",
        "Hongwei Zhao",
        "Lantao Hu",
        "Peng Jiang",
        "Kun Gai"
      ],
      "abstract": "Multi-domain recommendation and multi-task recommendation have demonstrated\ntheir effectiveness in leveraging common information from different domains and\nobjectives for comprehensive user modeling. Nonetheless, the practical\nrecommendation usually faces multiple domains and tasks simultaneously, which\ncannot be well-addressed by current methods. To this end, we introduce M3oE, an\nadaptive Multi-domain Multi-task Mixture-of-Experts recommendation framework.\nM3oE integrates multi-domain information, maps knowledge across domains and\ntasks, and optimizes multiple objectives. We leverage three mixture-of-experts\nmodules to learn common, domain-aspect, and task-aspect user preferences\nrespectively to address the complex dependencies among multiple domains and\ntasks in a disentangled manner. Additionally, we design a two-level fusion\nmechanism for precise control over feature extraction and fusion across diverse\ndomains and tasks. The framework's adaptability is further enhanced by applying\nAutoML technique, which allows dynamic structure optimization. To the best of\nthe authors' knowledge, our M3oE is the first effort to solve multi-domain\nmulti-task recommendation self-adaptively. Extensive experiments on two\nbenchmark datasets against diverse baselines demonstrate M3oE's superior\nperformance. The implementation code is available to ensure reproducibility.",
      "tldr_zh": "本论文提出了 M3oE，一种自适应的 Multi-Domain Multi-Task Mixture-of-Experts 推荐框架，用于同时处理多个域和任务的复杂依赖。框架通过三个 Mixture-of-Experts 模块分别学习共同用户偏好、域方面偏好和任务方面偏好，并采用双层融合机制精确控制特征提取和融合，同时利用 AutoML 技术实现动态结构优化。该方法在两个基准数据集上的实验显示，M3oE 优于多种基线模型，提供可重复性的实现代码，并首次实现了多域多任务推荐的自适应解决方案。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18465v3",
      "published_date": "2024-04-29 06:59:30 UTC",
      "updated_date": "2024-05-12 13:11:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:39:00.245199"
    },
    {
      "arxiv_id": "2404.18460v1",
      "title": "Ethical Reasoning and Moral Value Alignment of LLMs Depend on the Language we Prompt them in",
      "title_zh": "翻译失败",
      "authors": [
        "Utkarsh Agarwal",
        "Kumar Tanmay",
        "Aditi Khandelwal",
        "Monojit Choudhury"
      ],
      "abstract": "Ethical reasoning is a crucial skill for Large Language Models (LLMs).\nHowever, moral values are not universal, but rather influenced by language and\nculture. This paper explores how three prominent LLMs -- GPT-4, ChatGPT, and\nLlama2-70B-Chat -- perform ethical reasoning in different languages and if\ntheir moral judgement depend on the language in which they are prompted. We\nextend the study of ethical reasoning of LLMs by Rao et al. (2023) to a\nmultilingual setup following their framework of probing LLMs with ethical\ndilemmas and policies from three branches of normative ethics: deontology,\nvirtue, and consequentialism. We experiment with six languages: English,\nSpanish, Russian, Chinese, Hindi, and Swahili. We find that GPT-4 is the most\nconsistent and unbiased ethical reasoner across languages, while ChatGPT and\nLlama2-70B-Chat show significant moral value bias when we move to languages\nother than English. Interestingly, the nature of this bias significantly vary\nacross languages for all LLMs, including GPT-4.",
      "tldr_zh": "这篇论文探讨了大型语言模型（LLMs）如GPT-4、ChatGPT和Llama2-70B-Chat在不同语言下的道德推理及其偏见，强调道德价值观受语言和文化影响。研究扩展了Rao et al. (2023)的框架，使用deontology、virtue和consequentialism三种规范伦理分支测试六种语言（English、Spanish、Russian、Chinese、Hindi和Swahili）。结果表明，GPT-4在跨语言中表现出最一致和无偏见的道德判断，而ChatGPT和Llama2-70B-Chat在非English语言中显示出显著的道德价值偏见，且这些偏见的性质在不同语言间有明显差异。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18460v1",
      "published_date": "2024-04-29 06:42:27 UTC",
      "updated_date": "2024-04-29 06:42:27 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:39:13.772258"
    },
    {
      "arxiv_id": "2404.18445v1",
      "title": "Strategic Behavior and AI Training Data",
      "title_zh": "战略行为与 AI 训练数据",
      "authors": [
        "Christian Peukert",
        "Florian Abeillon",
        "Jérémie Haese",
        "Franziska Kaiser",
        "Alexander Staub"
      ],
      "abstract": "Human-created works represent critical data inputs to artificial intelligence\n(AI). Strategic behavior can play a major role for AI training datasets, be it\nin limiting access to existing works or in deciding which types of new works to\ncreate or whether to create new works at all. We examine creators' behavioral\nchange when their works become training data for AI. Specifically, we focus on\ncontributors on Unsplash, a popular stock image platform with about 6 million\nhigh-quality photos and illustrations. In the summer of 2020, Unsplash launched\nan AI research program by releasing a dataset of 25,000 images for commercial\nuse. We study contributors' reactions, comparing contributors whose works were\nincluded in this dataset to contributors whose works were not included. Our\nresults suggest that treated contributors left the platform at a\nhigher-than-usual rate and substantially slowed down the rate of new uploads.\nProfessional and more successful photographers react stronger than amateurs and\nless successful photographers. We also show that affected users changed the\nvariety and novelty of contributions to the platform, with long-run\nimplications for the stock of works potentially available for AI training.\nTaken together, our findings highlight the trade-off between interests of\nrightsholders and promoting innovation at the technological frontier. We\ndiscuss implications for copyright and AI policy.",
      "tldr_zh": "这篇论文探讨了当人类创作作品（如图像）被用作 AI 训练数据时，创建者的战略行为变化。研究者通过分析 Unsplash 平台上的贡献者，比较那些作品被纳入 25,000 张图像数据集的群体与未被纳入的群体，发现前者离开平台率显著升高，新上传速率大幅降低，且专业和成功摄影师的反应更强烈。结果显示，这种行为变化导致贡献内容的多样性和新颖性减少，可能长期影响 AI 训练数据的可用性。论文强调了版权持有人利益与推动技术创新的权衡，并讨论了对版权和 AI 政策的相关启示。",
      "categories": [
        "econ.GN",
        "cs.AI",
        "cs.CY",
        "cs.LG",
        "q-fin.EC"
      ],
      "primary_category": "econ.GN",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18445v1",
      "published_date": "2024-04-29 06:00:59 UTC",
      "updated_date": "2024-04-29 06:00:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:39:25.031161"
    },
    {
      "arxiv_id": "2404.18444v2",
      "title": "U-Nets as Belief Propagation: Efficient Classification, Denoising, and Diffusion in Generative Hierarchical Models",
      "title_zh": "翻译失败",
      "authors": [
        "Song Mei"
      ],
      "abstract": "U-Nets are among the most widely used architectures in computer vision,\nrenowned for their exceptional performance in applications such as image\nsegmentation, denoising, and diffusion modeling. However, a theoretical\nexplanation of the U-Net architecture design has not yet been fully\nestablished.\n  This paper introduces a novel interpretation of the U-Net architecture by\nstudying certain generative hierarchical models, which are tree-structured\ngraphical models extensively utilized in both language and image domains. With\ntheir encoder-decoder structure, long skip connections, and pooling and\nup-sampling layers, we demonstrate how U-Nets can naturally implement the\nbelief propagation denoising algorithm in such generative hierarchical models,\nthereby efficiently approximating the denoising functions. This leads to an\nefficient sample complexity bound for learning the denoising function using\nU-Nets within these models. Additionally, we discuss the broader implications\nof these findings for diffusion models in generative hierarchical models. We\nalso demonstrate that the conventional architecture of convolutional neural\nnetworks (ConvNets) is ideally suited for classification tasks within these\nmodels. This offers a unified view of the roles of ConvNets and U-Nets,\nhighlighting the versatility of generative hierarchical models in modeling\ncomplex data distributions across language and image domains.",
      "tldr_zh": "本论文提出了一种新颖的理论解释，将 U-Nets 视为在生成层次模型(generative hierarchical models)中实现信念传播(belief propagation)去噪算法的架构，从而高效近似去噪函数并提供学习去噪函数的样本复杂度界。\n这种解释揭示了 U-Nets 在图像分割、去噪和扩散建模(diffusion models)中的优势，并扩展到这些模型在语言和图像领域的应用。\n此外，论文强调传统的卷积神经网络(ConvNets)更适合分类任务，提供了一个统一视角，突出生成层次模型的多样性和灵活性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.ST",
        "stat.ML",
        "stat.TH"
      ],
      "primary_category": "cs.LG",
      "comment": "v2 updated discussions of related literature",
      "pdf_url": "http://arxiv.org/pdf/2404.18444v2",
      "published_date": "2024-04-29 05:57:03 UTC",
      "updated_date": "2024-05-01 16:49:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:39:36.901882"
    },
    {
      "arxiv_id": "2405.06663v1",
      "title": "Protein Representation Learning by Capturing Protein Sequence-Structure-Function Relationship",
      "title_zh": "翻译失败",
      "authors": [
        "Eunji Ko",
        "Seul Lee",
        "Minseon Kim",
        "Dongki Kim"
      ],
      "abstract": "The goal of protein representation learning is to extract knowledge from\nprotein databases that can be applied to various protein-related downstream\ntasks. Although protein sequence, structure, and function are the three key\nmodalities for a comprehensive understanding of proteins, existing methods for\nprotein representation learning have utilized only one or two of these\nmodalities due to the difficulty of capturing the asymmetric interrelationships\nbetween them. To account for this asymmetry, we introduce our novel asymmetric\nmulti-modal masked autoencoder (AMMA). AMMA adopts (1) a unified multi-modal\nencoder to integrate all three modalities into a unified representation space\nand (2) asymmetric decoders to ensure that sequence latent features reflect\nstructural and functional information. The experiments demonstrate that the\nproposed AMMA is highly effective in learning protein representations that\nexhibit well-aligned inter-modal relationships, which in turn makes it\neffective for various downstream protein-related tasks.",
      "tldr_zh": "这篇论文针对蛋白质表示学习，旨在通过捕捉蛋白质序列、结构和功能之间的不对称关系，从蛋白质数据库中提取知识以应用于各种下游任务。作者提出了一种新型Asymmetric Multi-Modal Masked Autoencoder (AMMA)，它采用统一的multi-modal编码器将三种模式整合到一个统一表示空间，并使用不对称解码器确保序列潜在特征充分反映结构和功能信息。实验结果表明，AMMA在学习蛋白质表示方面高度有效，能够实现良好的跨模式对齐，从而提升下游任务的性能。",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.BM",
      "comment": "ICLR 2024 MLGenX Workshop (Spotlight)",
      "pdf_url": "http://arxiv.org/pdf/2405.06663v1",
      "published_date": "2024-04-29 05:42:29 UTC",
      "updated_date": "2024-04-29 05:42:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:39:47.335873"
    },
    {
      "arxiv_id": "2404.18443v2",
      "title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers",
      "title_zh": "翻译失败",
      "authors": [
        "Ran Xu",
        "Wenqi Shi",
        "Yue Yu",
        "Yuchen Zhuang",
        "Yanqiao Zhu",
        "May D. Wang",
        "Joyce C. Ho",
        "Chao Zhang",
        "Carl Yang"
      ],
      "abstract": "Developing effective biomedical retrieval models is important for excelling\nat knowledge-intensive biomedical tasks but still challenging due to the\ndeficiency of sufficient publicly annotated biomedical data and computational\nresources. We present BMRetriever, a series of dense retrievers for enhancing\nbiomedical retrieval via unsupervised pre-training on large biomedical corpora,\nfollowed by instruction fine-tuning on a combination of labeled datasets and\nsynthetic pairs. Experiments on 5 biomedical tasks across 11 datasets verify\nBMRetriever's efficacy on various biomedical applications. BMRetriever also\nexhibits strong parameter efficiency, with the 410M variant outperforming\nbaselines up to 11.7 times larger, and the 2B variant matching the performance\nof models with over 5B parameters. The training data and model checkpoints are\nreleased at \\url{https://huggingface.co/BMRetriever} to ensure transparency,\nreproducibility, and application to new domains.",
      "tldr_zh": "该研究提出 BMRetriever，一系列密集检索器，通过在大型生物医学语料上进行无监督预训练，随后在标记数据集和合成数据对上进行指令微调，来提升大型语言模型在生物医学文本检索中的性能。实验在5个生物医学任务和11个数据集上验证了BMRetriever的有效性，尤其在处理知识密集型任务时表现出色。该模型还展现出强大的参数效率，例如410M参数的变体超过了比其大11.7倍的基线模型，而2B参数的变体与超过5B参数的模型性能相当。研究者公开了训练数据和模型检查点，以促进透明性、可重复性和在新领域的应用。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "q-bio.QM"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to EMNLP 2024. The model and data are uploaded to\n  \\url{https://github.com/ritaranx/BMRetriever}",
      "pdf_url": "http://arxiv.org/pdf/2404.18443v2",
      "published_date": "2024-04-29 05:40:08 UTC",
      "updated_date": "2024-10-04 03:25:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:40:00.159853"
    },
    {
      "arxiv_id": "2404.18961v1",
      "title": "Unleashing the Power of Multi-Task Learning: A Comprehensive Survey Spanning Traditional, Deep, and Pretrained Foundation Model Eras",
      "title_zh": "释放多任务学习的潜力：一个跨越传统、深度和预训练基础模型时代的全面综述",
      "authors": [
        "Jun Yu",
        "Yutong Dai",
        "Xiaokang Liu",
        "Jin Huang",
        "Yishan Shen",
        "Ke Zhang",
        "Rong Zhou",
        "Eashan Adhikarla",
        "Wenxuan Ye",
        "Yixin Liu",
        "Zhaoming Kong",
        "Kai Zhang",
        "Yilong Yin",
        "Vinod Namboodiri",
        "Brian D. Davison",
        "Jason H. Moore",
        "Yong Chen"
      ],
      "abstract": "MTL is a learning paradigm that effectively leverages both task-specific and\nshared information to address multiple related tasks simultaneously. In\ncontrast to STL, MTL offers a suite of benefits that enhance both the training\nprocess and the inference efficiency. MTL's key advantages encompass\nstreamlined model architecture, performance enhancement, and cross-domain\ngeneralizability. Over the past twenty years, MTL has become widely recognized\nas a flexible and effective approach in various fields, including CV, NLP,\nrecommendation systems, disease prognosis and diagnosis, and robotics. This\nsurvey provides a comprehensive overview of the evolution of MTL, encompassing\nthe technical aspects of cutting-edge methods from traditional approaches to\ndeep learning and the latest trend of pretrained foundation models. Our survey\nmethodically categorizes MTL techniques into five key areas: regularization,\nrelationship learning, feature propagation, optimization, and pre-training.\nThis categorization not only chronologically outlines the development of MTL\nbut also dives into various specialized strategies within each category.\nFurthermore, the survey reveals how the MTL evolves from handling a fixed set\nof tasks to embracing a more flexible approach free from task or modality\nconstraints. It explores the concepts of task-promptable and -agnostic\ntraining, along with the capacity for ZSL, which unleashes the untapped\npotential of this historically coveted learning paradigm. Overall, we hope this\nsurvey provides the research community with a comprehensive overview of the\nadvancements in MTL from its inception in 1997 to the present in 2023. We\naddress present challenges and look ahead to future possibilities, shedding\nlight on the opportunities and potential avenues for MTL research in a broad\nmanner. This project is publicly available at\nhttps://github.com/junfish/Awesome-Multitask-Learning.",
      "tldr_zh": "这篇论文对Multi-Task Learning (MTL)进行了全面调查，探讨了其从传统方法到深度学习，再到预训练基础模型时代的演变，强调了MTL的优势，如简化模型架构、提升性能和跨领域泛化。调查将MTL技术分类为五大领域，包括正则化、关系学习、特征传播、优化和预训练，并分析了其从处理固定任务向灵活任务无关方法（如任务提示和Zero-Shot Learning (ZSL)）的转变。最终，该研究回顾了MTL从1997年至今的发展，讨论了当前挑战和未来机会，为研究社区提供了宝贵资源，并公开了相关代码库。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "60 figures, 116 pages, 500+ references",
      "pdf_url": "http://arxiv.org/pdf/2404.18961v1",
      "published_date": "2024-04-29 05:23:10 UTC",
      "updated_date": "2024-04-29 05:23:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:40:13.756960"
    },
    {
      "arxiv_id": "2405.02336v1",
      "title": "Artificial General Intelligence (AGI)-Native Wireless Systems: A Journey Beyond 6G",
      "title_zh": "翻译失败",
      "authors": [
        "Walid Saad",
        "Omar Hashash",
        "Christo Kurisummoottil Thomas",
        "Christina Chaccour",
        "Merouane Debbah",
        "Narayan Mandayam",
        "Zhu Han"
      ],
      "abstract": "Building future wireless systems that support services like digital twins\n(DTs) is challenging to achieve through advances to conventional technologies\nlike meta-surfaces. While artificial intelligence (AI)-native networks promise\nto overcome some limitations of wireless technologies, developments still rely\non AI tools like neural networks. Such tools struggle to cope with the\nnon-trivial challenges of the network environment and the growing demands of\nemerging use cases. In this paper, we revisit the concept of AI-native wireless\nsystems, equipping them with the common sense necessary to transform them into\nartificial general intelligence (AGI)-native systems. These systems acquire\ncommon sense by exploiting different cognitive abilities such as perception,\nanalogy, and reasoning, that enable them to generalize and deal with unforeseen\nscenarios. Towards developing the components of such a system, we start by\nshowing how the perception module can be built through abstracting real-world\nelements into generalizable representations. These representations are then\nused to create a world model, founded on principles of causality and\nhyper-dimensional (HD) computing, that aligns with intuitive physics and\nenables analogical reasoning, that define common sense. Then, we explain how\nmethods such as integrated information theory play a role in the proposed\nintent-driven and objective-driven planning methods that maneuver the\nAGI-native network to take actions. Next, we discuss how an AGI-native network\ncan enable use cases related to human and autonomous agents: a) analogical\nreasoning for next-generation DTs, b) synchronized and resilient experiences\nfor cognitive avatars, and c) brain-level metaverse experiences like\nholographic teleportation. Finally, we conclude with a set of recommendations\nto build AGI-native systems. Ultimately, we envision this paper as a roadmap\nfor the beyond 6G era.",
      "tldr_zh": "这篇论文探讨了如何将无线系统从 AI-native 升级到 Artificial General Intelligence (AGI)-Native 系统，以应对超越 6G 时代的挑战，如支持数字孪生 (DTs) 等服务。论文提出通过赋予系统常识能力，包括感知、类比和推理模块，来构建可泛化表示和基于因果性及高维 (HD) 计算的世界模型，从而实现意图驱动和目标驱动的规划方法。最终，该框架能启用应用场景如下一代 DTs、认知头像的同步体验和脑级元宇宙，并提供构建 AGI-Native 系统的推荐路线图。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.02336v1",
      "published_date": "2024-04-29 04:51:05 UTC",
      "updated_date": "2024-04-29 04:51:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:40:26.563943"
    },
    {
      "arxiv_id": "2404.18423v2",
      "title": "Unsupervised Dynamics Prediction with Object-Centric Kinematics",
      "title_zh": "基于对象中心运动学的无监督动态预测",
      "authors": [
        "Yeon-Ji Song",
        "Suhyung Choi",
        "Jaein Kim",
        "Jin-Hwa Kim",
        "Byoung-Tak Zhang"
      ],
      "abstract": "Human perception involves discerning complex multi-object scenes into\ntime-static object appearance (ie, size, shape, color) and time-varying object\nmotion (ie, location, velocity, acceleration). This innate ability to\nunconsciously understand the environment is the motivation behind the success\nof dynamics modeling. Object-centric representations have emerged as a\npromising tool for dynamics prediction, yet they primarily focus on the\nobjects' appearance, often overlooking other crucial attributes. In this paper,\nwe propose Object-Centric Kinematics (OCK), a framework for dynamics prediction\nleveraging object-centric representations. Our model utilizes a novel component\nnamed object kinematics, which comprises low-level structured states of\nobjects' position, velocity, and acceleration. The object kinematics are\nobtained via either implicit or explicit approaches, enabling comprehensive\nspatiotemporal object reasoning, and integrated through various transformer\nmechanisms, facilitating effective object-centric dynamics modeling. Our model\ndemonstrates superior performance when handling objects and backgrounds in\ncomplex scenes characterized by a wide range of object attributes and dynamic\nmovements. Moreover, our model demonstrates generalization capabilities across\ndiverse synthetic environments, highlighting its potential for broad\napplicability in vision-related tasks.",
      "tldr_zh": "本论文提出 Object-Centric Kinematics (OCK) 框架，用于无监督动态预测，旨在模仿人类感知将复杂多物体场景分解为物体的静态外观（如大小、形状、颜色）和动态运动（如位置、速度、加速度）。框架引入 object kinematics 组件，通过隐式或显式方法获取物体的低级结构化状态，并利用各种 transformer mechanisms 进行整合，实现全面的时空物体推理。实验结果显示，OCK 在处理复杂场景中的物体和背景时表现出优越性能，并展示了在不同合成环境中的良好泛化能力，适用于多种视觉相关任务。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "15 pages, 6 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2404.18423v2",
      "published_date": "2024-04-29 04:47:23 UTC",
      "updated_date": "2024-05-06 06:10:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:40:37.516003"
    },
    {
      "arxiv_id": "2404.18419v1",
      "title": "Research on Intelligent Aided Diagnosis System of Medical Image Based on Computer Deep Learning",
      "title_zh": "基于计算机深度学习的医疗图像智能辅助诊断系统研究",
      "authors": [
        "Jiajie Yuan",
        "Linxiao Wu",
        "Yulu Gong",
        "Zhou Yu",
        "Ziang Liu",
        "Shuyao He"
      ],
      "abstract": "This paper combines Struts and Hibernate two architectures together, using\nDAO (Data Access Object) to store and access data. Then a set of dual-mode\nhumidity medical image library suitable for deep network is established, and a\ndual-mode medical image assisted diagnosis method based on the image is\nproposed. Through the test of various feature extraction methods, the optimal\noperating characteristic under curve product (AUROC) is 0.9985, the recall rate\nis 0.9814, and the accuracy is 0.9833. This method can be applied to clinical\ndiagnosis, and it is a practical method. Any outpatient doctor can register\nquickly through the system, or log in to the platform to upload the image to\nobtain more accurate images. Through the system, each outpatient physician can\nquickly register or log in to the platform for image uploading, thus obtaining\nmore accurate images. The segmentation of images can guide doctors in clinical\ndepartments. Then the image is analyzed to determine the location and nature of\nthe tumor, so as to make targeted treatment.",
      "tldr_zh": "这篇论文提出了一种基于计算机深度学习的医疗图像智能辅助诊断系统，结合 Struts 和 Hibernate 架构以及 DAO（Data Access Object）来存储和访问数据。\n他们建立了适合深度网络的双模式医疗图像库，并提出了一种基于图像的双模式辅助诊断方法，通过测试各种特征提取方法，取得了 AUROC 0.9985、召回率 0.9814 和准确率 0.9833 的优异性能。\n该系统可应用于临床诊断，允许医生快速注册、登录并上传图像，以分析肿瘤的位置和性质，并提供针对性治疗指导。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18419v1",
      "published_date": "2024-04-29 04:32:11 UTC",
      "updated_date": "2024-04-29 04:32:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:40:51.110035"
    },
    {
      "arxiv_id": "2404.18416v2",
      "title": "Capabilities of Gemini Models in Medicine",
      "title_zh": "Gemini 模型在医学中的能力",
      "authors": [
        "Khaled Saab",
        "Tao Tu",
        "Wei-Hung Weng",
        "Ryutaro Tanno",
        "David Stutz",
        "Ellery Wulczyn",
        "Fan Zhang",
        "Tim Strother",
        "Chunjong Park",
        "Elahe Vedadi",
        "Juanma Zambrano Chaves",
        "Szu-Yeu Hu",
        "Mike Schaekermann",
        "Aishwarya Kamath",
        "Yong Cheng",
        "David G. T. Barrett",
        "Cathy Cheung",
        "Basil Mustafa",
        "Anil Palepu",
        "Daniel McDuff",
        "Le Hou",
        "Tomer Golany",
        "Luyang Liu",
        "Jean-baptiste Alayrac",
        "Neil Houlsby",
        "Nenad Tomasev",
        "Jan Freyberg",
        "Charles Lau",
        "Jonas Kemp",
        "Jeremy Lai",
        "Shekoofeh Azizi",
        "Kimberly Kanada",
        "SiWai Man",
        "Kavita Kulkarni",
        "Ruoxi Sun",
        "Siamak Shakeri",
        "Luheng He",
        "Ben Caine",
        "Albert Webson",
        "Natasha Latysheva",
        "Melvin Johnson",
        "Philip Mansfield",
        "Jian Lu",
        "Ehud Rivlin",
        "Jesper Anderson",
        "Bradley Green",
        "Renee Wong",
        "Jonathan Krause",
        "Jonathon Shlens",
        "Ewa Dominowska",
        "S. M. Ali Eslami",
        "Katherine Chou",
        "Claire Cui",
        "Oriol Vinyals",
        "Koray Kavukcuoglu",
        "James Manyika",
        "Jeff Dean",
        "Demis Hassabis",
        "Yossi Matias",
        "Dale Webster",
        "Joelle Barral",
        "Greg Corrado",
        "Christopher Semturs",
        "S. Sara Mahdavi",
        "Juraj Gottweis",
        "Alan Karthikesalingam",
        "Vivek Natarajan"
      ],
      "abstract": "Excellence in a wide variety of medical applications poses considerable\nchallenges for AI, requiring advanced reasoning, access to up-to-date medical\nknowledge and understanding of complex multimodal data. Gemini models, with\nstrong general capabilities in multimodal and long-context reasoning, offer\nexciting possibilities in medicine. Building on these core strengths of Gemini,\nwe introduce Med-Gemini, a family of highly capable multimodal models that are\nspecialized in medicine with the ability to seamlessly use web search, and that\ncan be efficiently tailored to novel modalities using custom encoders. We\nevaluate Med-Gemini on 14 medical benchmarks, establishing new state-of-the-art\n(SoTA) performance on 10 of them, and surpass the GPT-4 model family on every\nbenchmark where a direct comparison is viable, often by a wide margin. On the\npopular MedQA (USMLE) benchmark, our best-performing Med-Gemini model achieves\nSoTA performance of 91.1% accuracy, using a novel uncertainty-guided search\nstrategy. On 7 multimodal benchmarks including NEJM Image Challenges and MMMU\n(health & medicine), Med-Gemini improves over GPT-4V by an average relative\nmargin of 44.5%. We demonstrate the effectiveness of Med-Gemini's long-context\ncapabilities through SoTA performance on a needle-in-a-haystack retrieval task\nfrom long de-identified health records and medical video question answering,\nsurpassing prior bespoke methods using only in-context learning. Finally,\nMed-Gemini's performance suggests real-world utility by surpassing human\nexperts on tasks such as medical text summarization, alongside demonstrations\nof promising potential for multimodal medical dialogue, medical research and\neducation. Taken together, our results offer compelling evidence for\nMed-Gemini's potential, although further rigorous evaluation will be crucial\nbefore real-world deployment in this safety-critical domain.",
      "tldr_zh": "这篇论文探讨了 Gemini 模型在医学领域的能力，介绍了 Med-Gemini 模型家族，这是一个基于 Gemini 的高度先进的多模态模型，能够无缝整合网络搜索和自定义编码器，以处理复杂医学数据和推理任务。在 14 个医学基准测试中，Med-Gemini 在 10 个上达到了新的 SoTA 性能，并全面超越 GPT-4，在 MedQA 上以不确定性引导搜索策略实现 91.1% 准确率，并在多模态任务如 NEJM Image Challenges 上平均提高 44.5%。此外，模型展示了长上下文能力的优势，如在医疗记录检索和视频问答上超越现有方法，并显示出在医疗总结、对话和教育中的实际潜力，但强调了在安全关键领域部署前需进行严格评估。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18416v2",
      "published_date": "2024-04-29 04:11:28 UTC",
      "updated_date": "2024-05-01 17:12:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:41:01.947001"
    },
    {
      "arxiv_id": "2405.00732v1",
      "title": "LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report",
      "title_zh": "翻译失败",
      "authors": [
        "Justin Zhao",
        "Timothy Wang",
        "Wael Abid",
        "Geoffrey Angus",
        "Arnav Garg",
        "Jeffery Kinnison",
        "Alex Sherstinsky",
        "Piero Molino",
        "Travis Addair",
        "Devvret Rishi"
      ],
      "abstract": "Low Rank Adaptation (LoRA) has emerged as one of the most widely adopted\nmethods for Parameter Efficient Fine-Tuning (PEFT) of Large Language Models\n(LLMs). LoRA reduces the number of trainable parameters and memory usage while\nachieving comparable performance to full fine-tuning. We aim to assess the\nviability of training and serving LLMs fine-tuned with LoRA in real-world\napplications. First, we measure the quality of LLMs fine-tuned with quantized\nlow rank adapters across 10 base models and 31 tasks for a total of 310 models.\nWe find that 4-bit LoRA fine-tuned models outperform base models by 34 points\nand GPT-4 by 10 points on average. Second, we investigate the most effective\nbase models for fine-tuning and assess the correlative and predictive\ncapacities of task complexity heuristics in forecasting the outcomes of\nfine-tuning. Finally, we evaluate the latency and concurrency capabilities of\nLoRAX, an open-source Multi-LoRA inference server that facilitates the\ndeployment of multiple LoRA fine-tuned models on a single GPU using shared base\nmodel weights and dynamic adapter loading. LoRAX powers LoRA Land, a web\napplication that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA\nA100 GPU with 80GB memory. LoRA Land highlights the quality and\ncost-effectiveness of employing multiple specialized LLMs over a single,\ngeneral-purpose LLM.",
      "tldr_zh": "该研究评估了 Low Rank Adaptation (LoRA) 在 Parameter Efficient Fine-Tuning (PEFT) 中的应用，通过微调 10 个基模型上的 31 个任务，共生成 310 个 Large Language Models (LLMs)。结果显示，4-bit LoRA 微调模型平均比基模型性能提升 34 分，并超越 GPT-4 约 10 分，同时探讨了任务复杂度的预测能力。LoRAX 作为开源多-LoRA 推理服务器，支持在单个 GPU 上部署多个微调模型，并通过 LoRA Land 应用展示了使用专业化 LLMs 的高质量和成本效益。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2405.00732v1",
      "published_date": "2024-04-29 04:01:45 UTC",
      "updated_date": "2024-04-29 04:01:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:41:14.289969"
    },
    {
      "arxiv_id": "2404.18413v1",
      "title": "3AM: An Ambiguity-Aware Multi-Modal Machine Translation Dataset",
      "title_zh": "翻译失败",
      "authors": [
        "Xinyu Ma",
        "Xuebo Liu",
        "Derek F. Wong",
        "Jun Rao",
        "Bei Li",
        "Liang Ding",
        "Lidia S. Chao",
        "Dacheng Tao",
        "Min Zhang"
      ],
      "abstract": "Multimodal machine translation (MMT) is a challenging task that seeks to\nimprove translation quality by incorporating visual information. However,\nrecent studies have indicated that the visual information provided by existing\nMMT datasets is insufficient, causing models to disregard it and overestimate\ntheir capabilities. This issue presents a significant obstacle to the\ndevelopment of MMT research. This paper presents a novel solution to this issue\nby introducing 3AM, an ambiguity-aware MMT dataset comprising 26,000 parallel\nsentence pairs in English and Chinese, each with corresponding images. Our\ndataset is specifically designed to include more ambiguity and a greater\nvariety of both captions and images than other MMT datasets. We utilize a word\nsense disambiguation model to select ambiguous data from vision-and-language\ndatasets, resulting in a more challenging dataset. We further benchmark several\nstate-of-the-art MMT models on our proposed dataset. Experimental results show\nthat MMT models trained on our dataset exhibit a greater ability to exploit\nvisual information than those trained on other MMT datasets. Our work provides\na valuable resource for researchers in the field of multimodal learning and\nencourages further exploration in this area. The data, code and scripts are\nfreely available at https://github.com/MaxyLee/3AM.",
      "tldr_zh": "该论文提出 3AM 数据集，这是一个关注歧义的多模态机器翻译 (MMT) 数据集，包含 26,000 对英中平行句子及其对应图像，以解决现有 MMT 数据集视觉信息不足的问题。数据集通过词义消歧模型从视觉-语言数据中选取更多歧义和多样性的样本，使其更具挑战性。实验结果显示，在 3AM 上训练的 MMT 模型比其他数据集表现出更强的视觉信息利用能力。该数据集作为开源资源，将促进多模态学习领域的进一步研究。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18413v1",
      "published_date": "2024-04-29 04:01:30 UTC",
      "updated_date": "2024-04-29 04:01:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:41:25.931787"
    },
    {
      "arxiv_id": "2404.18400v3",
      "title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Parshin Shojaee",
        "Kazem Meidani",
        "Shashank Gupta",
        "Amir Barati Farimani",
        "Chandan K Reddy"
      ],
      "abstract": "Mathematical equations have been unreasonably effective in describing complex\nnatural phenomena across various scientific disciplines. However, discovering\nsuch insightful equations from data presents significant challenges due to the\nnecessity of navigating extremely large combinatorial hypothesis spaces.\nCurrent methods of equation discovery, commonly known as symbolic regression\ntechniques, largely focus on extracting equations from data alone, often\nneglecting the domain-specific prior knowledge that scientists typically depend\non. They also employ limited representations such as expression trees,\nconstraining the search space and expressiveness of equations. To bridge this\ngap, we introduce LLM-SR, a novel approach that leverages the extensive\nscientific knowledge and robust code generation capabilities of Large Language\nModels (LLMs) to discover scientific equations from data. Specifically, LLM-SR\ntreats equations as programs with mathematical operators and combines LLMs'\nscientific priors with evolutionary search over equation programs. The LLM\niteratively proposes new equation skeleton hypotheses, drawing from its domain\nknowledge, which are then optimized against data to estimate parameters. We\nevaluate LLM-SR on four benchmark problems across diverse scientific domains\n(e.g., physics, biology), which we carefully designed to simulate the discovery\nprocess and prevent LLM recitation. Our results demonstrate that LLM-SR\ndiscovers physically accurate equations that significantly outperform\nstate-of-the-art symbolic regression baselines, particularly in out-of-domain\ntest settings. We also show that LLM-SR's incorporation of scientific priors\nenables more efficient equation space exploration than the baselines. Code and\ndata are available: https://github.com/deep-symbolic-mathematics/LLM-SR",
      "tldr_zh": "本研究提出LLM-SR，一种利用Large Language Models (LLMs)的科学知识和代码生成能力来从数据中发现科学方程的新方法，以克服传统符号回归(symbolic regression)技术的局限性，如忽略领域先验和搜索空间受限。LLM-SR将方程视为程序，通过LLMs迭代生成方程骨架假设并结合进化搜索优化参数，从而整合科学先验进行高效探索。在四个跨领域基准问题（包括物理和生物）上，LLM-SR发现的方程在准确性和域外泛化上显著优于现有基线，证明了其在科学方程发现中的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "ICLR 2025 Oral",
      "pdf_url": "http://arxiv.org/pdf/2404.18400v3",
      "published_date": "2024-04-29 03:30:06 UTC",
      "updated_date": "2025-03-20 16:37:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:41:37.003952"
    },
    {
      "arxiv_id": "2404.18385v2",
      "title": "Equivalence: An analysis of artists' roles with Image Generative AI from Conceptual Art perspective through an interactive installation design practice",
      "title_zh": "翻译失败",
      "authors": [
        "Yixuan Li",
        "Dan C. Baciu",
        "Marcos Novak",
        "George Legrady"
      ],
      "abstract": "Over the past year, the emergence of advanced text-to-image Generative AI\nmodels has significantly impacted the art world, challenging traditional\nnotions of creativity and the role of artists. This study explores how artists\ninteract with these technologies, using a 5P model (Purpose, People, Process,\nProduct, and Press) based on Rhodes' creativity framework to compare the\nartistic processes behind Conceptual Art and Image Generative AI. To exemplify\nthis framework, a practical case study titled \"Equivalence\", a multi-screen\ninteractive installation that converts users' speech input into continuously\nevolving paintings developed based on Stable Diffusion and NLP algorithms, was\ndeveloped. Through comprehensive analysis and the case study, this work aims to\nbroaden our understanding of artists' roles and foster a deeper appreciation\nfor the creative aspects inherent in artwork created with Image Generative AI.",
      "tldr_zh": "这篇论文探讨了Image Generative AI对艺术世界的冲击，特别是对艺术家角色的挑战，从Conceptual Art视角进行分析。研究采用基于Rhodes创意框架的5P模型（Purpose, People, Process, Product, and Press）来比较Conceptual Art和Image Generative AI的艺术过程。论文通过“Equivalence”案例研究——一个多屏互动安装，使用Stable Diffusion和NLP algorithms将用户语音输入转化为动态绘画——来示例这一框架。最终，该工作加深了对艺术家角色的理解，并促进了对使用Image Generative AI创建艺术品的创意欣赏。",
      "categories": [
        "cs.HC",
        "cs.AI",
        "I.2.7; J.0; J.5"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18385v2",
      "published_date": "2024-04-29 02:45:23 UTC",
      "updated_date": "2024-04-30 02:06:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:41:49.374235"
    },
    {
      "arxiv_id": "2405.01588v1",
      "title": "Towards Unbiased Evaluation of Detecting Unanswerable Questions in EHRSQL",
      "title_zh": "针对 EHRSQL 中检测",
      "authors": [
        "Yongjin Yang",
        "Sihyeon Kim",
        "SangMook Kim",
        "Gyubok Lee",
        "Se-Young Yun",
        "Edward Choi"
      ],
      "abstract": "Incorporating unanswerable questions into EHR QA systems is crucial for\ntesting the trustworthiness of a system, as providing non-existent responses\ncan mislead doctors in their diagnoses. The EHRSQL dataset stands out as a\npromising benchmark because it is the only dataset that incorporates\nunanswerable questions in the EHR QA system alongside practical questions.\nHowever, in this work, we identify a data bias in these unanswerable questions;\nthey can often be discerned simply by filtering with specific N-gram patterns.\nSuch biases jeopardize the authenticity and reliability of QA system\nevaluations. To tackle this problem, we propose a simple debiasing method of\nadjusting the split between the validation and test sets to neutralize the\nundue influence of N-gram filtering. By experimenting on the MIMIC-III dataset,\nwe demonstrate both the existing data bias in EHRSQL and the effectiveness of\nour data split strategy in mitigating this bias.",
      "tldr_zh": "本研究针对EHRSQL数据集中的unanswerable questions数据偏差问题，指出这些问题可以通过特定N-gram patterns过滤来轻易识别，从而影响EHR QA系统的评估真实性和可靠性。作者强调，加入unanswerable questions对测试系统可信度至关重要，因为错误响应可能误导医生诊断。针对这一偏差，他们提出了一种简单的debiasing方法，通过调整validation和test sets的分裂来中和N-gram filtering的影响。在MIMIC-III数据集上的实验证明了EHRSQL的现有偏差，并验证了该策略的有效性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "DPFM Workshop, ICLR 2024",
      "pdf_url": "http://arxiv.org/pdf/2405.01588v1",
      "published_date": "2024-04-29 02:26:15 UTC",
      "updated_date": "2024-04-29 02:26:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:41:59.729875"
    },
    {
      "arxiv_id": "2404.18359v1",
      "title": "FoundaBench: Evaluating Chinese Fundamental Knowledge Capabilities of Large Language Models",
      "title_zh": "FoundaBench：评估大型语言模型的中文基础知识能力",
      "authors": [
        "Wei Li",
        "Ren Ma",
        "Jiang Wu",
        "Chenya Gu",
        "Jiahui Peng",
        "Jinyang Len",
        "Songyang Zhang",
        "Hang Yan",
        "Dahua Lin",
        "Conghui He"
      ],
      "abstract": "In the burgeoning field of large language models (LLMs), the assessment of\nfundamental knowledge remains a critical challenge, particularly for models\ntailored to Chinese language and culture. This paper introduces FoundaBench, a\npioneering benchmark designed to rigorously evaluate the fundamental knowledge\ncapabilities of Chinese LLMs. FoundaBench encompasses a diverse array of 3354\nmultiple-choice questions across common sense and K-12 educational subjects,\nmeticulously curated to reflect the breadth and depth of everyday and academic\nknowledge. We present an extensive evaluation of 12 state-of-the-art LLMs using\nFoundaBench, employing both traditional assessment methods and our CircularEval\nprotocol to mitigate potential biases in model responses. Our results highlight\nthe superior performance of models pre-trained on Chinese corpora, and reveal a\nsignificant disparity between models' reasoning and memory recall capabilities.\nThe insights gleaned from FoundaBench evaluations set a new standard for\nunderstanding the fundamental knowledge of LLMs, providing a robust framework\nfor future advancements in the field.",
      "tldr_zh": "这篇论文引入了FoundaBench，这是一个专门评估中文大语言模型(LLMs)基本知识能力的基准测试。FoundaBench 包含 3354 个多项选择题，涵盖常识和 K-12 教育主题，并通过传统评估方法和 CircularEval 协议来减少模型响应偏差。研究对 12 个最先进 LLMs 进行了全面评估，结果显示基于中文语料预训练的模型表现出色，并暴露了模型在推理与记忆召回能力上的显著差距。该基准为理解 LLMs 的基础知识设定了新标准，并为未来模型改进提供了一个稳健框架。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2404.18359v1",
      "published_date": "2024-04-29 01:49:07 UTC",
      "updated_date": "2024-04-29 01:49:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:42:13.692853"
    },
    {
      "arxiv_id": "2404.18353v2",
      "title": "How secure is AI-generated Code: A Large-Scale Comparison of Large Language Models",
      "title_zh": "AI 生成代码的安全性如何：",
      "authors": [
        "Norbert Tihanyi",
        "Tamas Bisztray",
        "Mohamed Amine Ferrag",
        "Ridhi Jain",
        "Lucas C. Cordeiro"
      ],
      "abstract": "This study compares state-of-the-art Large Language Models (LLMs) on their\ntendency to generate vulnerabilities when writing C programs using a neutral\nzero-shot prompt. Tihanyi et al. introduced the FormAI dataset at PROMISE'23,\nfeaturing 112,000 C programs generated by GPT-3.5-turbo, with over 51.24%\nidentified as vulnerable. We extended that research with a large-scale study\ninvolving 9 state-of-the-art models such as OpenAI's GPT-4o-mini, Google's\nGemini Pro 1.0, TII's 180 billion-parameter Falcon, Meta's 13 billion-parameter\nCode Llama, and several other compact models. Additionally, we introduce the\nFormAI-v2 dataset, which comprises 331 000 compilable C programs generated by\nthese LLMs. Each program in the dataset is labeled based on the vulnerabilities\ndetected in its source code through formal verification, using the Efficient\nSMT-based Context-Bounded Model Checker (ESBMC). This technique minimizes false\npositives by providing a counterexample for the specific vulnerability and\nreduces false negatives by thoroughly completing the verification process. Our\nstudy reveals that at least 62.07% of the generated programs are vulnerable.\nThe differences between the models are minor, as they all show similar coding\nerrors with slight variations. Our research highlights that while LLMs offer\npromising capabilities for code generation, deploying their output in a\nproduction environment requires proper risk assessment and validation.",
      "tldr_zh": "这篇论文比较了9个最先进的大型语言模型(LLMs)在零-shot提示下生成C程序时的漏洞倾向，扩展了FormAI数据集并引入了新的FormAI-v2数据集，包含331,000个可编译程序，这些程序通过Efficient SMT-based Context-Bounded Model Checker(ESBMC)进行正式验证以标记漏洞。研究发现，至少62.07%的生成程序存在安全问题，各模型间差异较小，主要表现为类似编码错误。尽管LLMs在代码生成方面显示出潜力，但论文强调，在生产环境中部署这些输出需进行严格的风险评估和验证。",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.PL"
      ],
      "primary_category": "cs.CR",
      "comment": "Accepted and will be shortly published at Empirical Software\n  Engineering (EMSE). Journal Impact Factor: 3.5 (2023)",
      "pdf_url": "http://arxiv.org/pdf/2404.18353v2",
      "published_date": "2024-04-29 01:24:14 UTC",
      "updated_date": "2024-12-11 13:02:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:42:26.474570"
    },
    {
      "arxiv_id": "2404.18352v1",
      "title": "Post-hoc and manifold explanations analysis of facial expression data based on deep learning",
      "title_zh": "翻译失败",
      "authors": [
        "Yang Xiao"
      ],
      "abstract": "The complex information processing system of humans generates a lot of\nobjective and subjective evaluations, making the exploration of human cognitive\nproducts of great cutting-edge theoretical value. In recent years, deep\nlearning technologies, which are inspired by biological brain mechanisms, have\nmade significant strides in the application of psychological or cognitive\nscientific research, particularly in the memorization and recognition of facial\ndata. This paper investigates through experimental research how neural networks\nprocess and store facial expression data and associate these data with a range\nof psychological attributes produced by humans. Researchers utilized deep\nlearning model VGG16, demonstrating that neural networks can learn and\nreproduce key features of facial data, thereby storing image memories.\nMoreover, the experimental results reveal the potential of deep learning models\nin understanding human emotions and cognitive processes and establish a\nmanifold visualization interpretation of cognitive products or psychological\nattributes from a non-Euclidean space perspective, offering new insights into\nenhancing the explainability of AI. This study not only advances the\napplication of AI technology in the field of psychology but also provides a new\npsychological theoretical understanding the information processing of the AI.\nThe code is available in here: https://github.com/NKUShaw/Psychoinformatics.",
      "tldr_zh": "这篇论文探讨了基于深度学习的后验（post-hoc）和流形（manifold）解释方法，用于分析面部表情数据及其与人类心理属性的关联。研究者利用 VGG16 模型，实验证明神经网络能够学习、存储和再现面部数据的关键特征，从而模拟人类情绪和认知过程。论文从非欧空间视角建立了流形可视化解释框架，提升了 AI 的可解释性，并为心理学领域提供了新的理论见解。该工作还开源了代码（https://github.com/NKUShaw/Psychoinformatics），促进进一步应用。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "19PAGES",
      "pdf_url": "http://arxiv.org/pdf/2404.18352v1",
      "published_date": "2024-04-29 01:19:17 UTC",
      "updated_date": "2024-04-29 01:19:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-18T04:42:38.071102"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 93,
  "processed_papers_count": 93,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-18T04:43:00.521939"
}
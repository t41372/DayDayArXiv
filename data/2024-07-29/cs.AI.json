{
  "date": "2024-07-29",
  "category": "cs.AI",
  "summary": "欢迎来到 UTC 时间 2024-07-29 的 arXiv 中文 TLDR 快报！\n\n今天 arXiv 的论文主要聚焦于 AI 模型优化、医疗图像分析、强化学习和多模态应用等领域，强调了大型语言模型（LLM）的改进、医疗 AI 的实际应用，以及高效算法设计。其中，Apple 的“Apple Intelligence Foundation Language Models”论文引人注目，展示了知名公司对 AI 基础模型的贡献；其他亮点包括强化学习中的自监督方法和医疗图像处理创新，这些论文可能引发广泛讨论。\n\n下面，我挑选并简要讨论了部分关键论文，将相关主题归类讨论。优先选取了重要、创新性和话题度高的论文（如涉及 LLM 或医疗 AI 的），并快速掠过一些较基础或小众的文章。每篇论文的标题以“中文 + 英文”形式列出，聚焦核心贡献和发现。\n\n### AI 模型与 LLM 优化\n这些论文探讨了 LLM 的效率提升和应用扩展，体现了 AI 领域的热门趋势。\n- **Apple Intelligence Foundation Language Models（苹果智能基础语言模型）**（第9篇）：作者包括众多 Apple 研究者，提出了一种约30亿参数的设备端语言模型和大型服务器模型，强调高效训练、推理优化和负责任 AI 原则。主要贡献是通过混合数据训练，实现高准确性和隐私保护，在各种任务中表现出色。\n- **MindSearch: Mimicking Human Minds Elicits Deep AI Searcher（MindSearch：模仿人类思维实现深度 AI 搜索）**（第21篇）：论文利用 LLM 构建多代理框架，模拟人类信息搜索过程。主要发现是通过并行处理大规模网页，显著提升查询响应质量和效率，适用于复杂信息整合任务。\n- **AutoScale: Scale-Aware Data Mixing for Pre-Training LLMs（AutoScale：针对 LLM 预训练的尺度感知数据混合）**（第23篇）：引入两阶段框架优化数据混合策略，显著加速 LLM 收敛并提升下游任务性能。关键点是证明数据分布对模型尺度的影响，实现高达38%的训练加速。\n- **QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval（QAEA-DR：用于密集检索的统一文本增强框架）**（第19篇）：利用 LLM 生成问题-答案对和事件提取，增强文本数据。主要贡献是改善检索准确性，同时减少噪声干扰。\n- **ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2（ML-Mamba：利用 Mamba-2 的高效多模态 LLM）**（第68篇）：将 Mamba-2 模型应用于多模态任务，提出视觉选择性扫描机制。发现它在图像和视频任务中比 Transformer 更高效，计算成本降低一半。\n\n其他 LLM 相关论文（如第14、50篇）也探讨了提示工程和自监督学习，但这些已覆盖核心创新点。\n\n### 医疗图像和生物分析\n医疗 AI 论文占比高，许多聚焦图像处理和诊断，体现了实际应用潜力。\n- **Dense Self-Supervised Learning for Medical Image Segmentation（密集自监督学习用于医疗图像分割）**（第5篇）：提出 Pix2Rep 方法，通过自监督学习从无标签图像中提取像素级表示。主要发现是显著减少标注需求，提升心脏 MRI 分割精度达31%。\n- **LatentArtiFusion: An Effective and Efficient Histological Artifacts Restoration Framework（LatentArtiFusion：高效组织学伪影修复框架）**（第26篇）：使用潜在扩散模型修复组织图像伪影，引入区域重建算法。贡献在于30倍加速和5%以上的性能提升，适用于下游分类任务。\n- **Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning（利用可视化参考指令微调的多模态 LLM 在图表问答中的进展）**（第25篇）：开发框架增强 LLM 在医疗图表分析中的性能，通过数据增强和混合分辨率策略优化。发现它在基准数据集上超越现有方法。\n- **SANGRIA: Surgical Video Scene Graph Optimization for Surgical Workflow Prediction（SANGRIA：手术视频场景图优化用于手术流程预测）**（第17篇）：构建动态场景图优化手术视频分析，主要贡献是使用弱标签提升预测准确性10%，适用于手术阶段识别。\n\n这些论文快速掠过较小众的图像处理方法（如第12、13篇），聚焦实际医疗影响。\n\n### 强化学习和决策优化\n强化学习论文强调算法效率和实际应用，部分与 AI 决策相关。\n- **A Method for Fast Autonomy Transfer in Reinforcement Learning（一种用于强化学习的快速自治转移方法）**（第1篇）：提出 Multi-Critic Actor-Critic (MCAC) 算法，实现22.76倍更快适应新环境。主要发现是通过预训练价值函数减少计算资源需求。\n- **Appraisal-Guided Proximal Policy Optimization: Modeling Psychological Disorders in Dynamic Grid World（评估引导的近端策略优化：动态网格世界中的心理障碍建模）**（第6篇）：整合评估理论模拟心理障碍，改进 PPO 算法。贡献在于提升泛化能力和行为模拟精度。\n- **SAPG: Split and Aggregate Policy Gradients（SAPG：分割和聚合策略梯度）**（第15篇）：优化 on-policy RL 算法，支持大规模环境训练。发现它在复杂环境中显著超越基线 PPO。\n\n其他强化学习论文（如第37篇）较基础，故简要提及。\n\n### 其他值得注意的论文\n- **Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process（语言模型的物理学：第2.1部分，基础数学和隐藏推理过程）**（第16篇）：揭示 LLM 在数学推理中的隐藏机制，通过实验回答模型是否依赖模板。主要发现是 LLM 能学习人类式推理，但易出错。\n- **Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval（语言模型检索中的简历筛选性别、种族和交叉偏差）**（第8篇）：分析 LLM 在招聘中的偏差，证明模型偏好白人男性。贡献在于揭示就业 AI 的公平问题。\n- **FUTGA: Towards Fine-grained Music Understanding through Temporally-enhanced Generative Augmentation（FUTGA：通过时间增强生成增强实现细粒度音乐理解）**（第3篇）：生成细粒度音乐描述，提升音乐分析任务性能。发现它在音乐生成和检索中表现优异。\n\n剩余论文（如第2、4、7、11、18、20、22、24、27-36、39-104篇）涉及领域多样（如文本编辑、图神经网络、区块链等），但许多较小众或基础，因此快速掠过。它们的核心在于特定技术优化，但未见重大突破。\n\n总之，今天的论文展示了 AI 领域的多样创新，LLM 和医疗应用尤其值得关注。未来几天，读者可关注这些方向的进展！（本快报基于104篇论文精选，保持简洁。）",
  "papers": [
    {
      "arxiv_id": "2407.20466v1",
      "title": "A Method for Fast Autonomy Transfer in Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Dinuka Sahabandu",
        "Bhaskar Ramasubramanian",
        "Michail Alexiou",
        "J. Sukarno Mertoguno",
        "Linda Bushnell",
        "Radha Poovendran"
      ],
      "abstract": "This paper introduces a novel reinforcement learning (RL) strategy designed\nto facilitate rapid autonomy transfer by utilizing pre-trained critic value\nfunctions from multiple environments. Unlike traditional methods that require\nextensive retraining or fine-tuning, our approach integrates existing\nknowledge, enabling an RL agent to adapt swiftly to new settings without\nrequiring extensive computational resources. Our contributions include\ndevelopment of the Multi-Critic Actor-Critic (MCAC) algorithm, establishing its\nconvergence, and empirical evidence demonstrating its efficacy. Our\nexperimental results show that MCAC significantly outperforms the baseline\nactor-critic algorithm, achieving up to 22.76x faster autonomy transfer and\nhigher reward accumulation. This advancement underscores the potential of\nleveraging accumulated knowledge for efficient adaptation in RL applications.",
      "tldr_zh": "这篇论文提出了一种新型强化学习 (RL) 策略，用于实现快速自治转移，通过整合多个环境中的预训练批评者价值函数 (pre-trained critic value functions)，让 RL 代理无需大量重新训练即可适应新设置。论文的主要贡献包括开发了 Multi-Critic Actor-Critic (MCAC) 算法、证明其收敛性，并提供了实证证据。实验结果表明，MCAC 相较于基线 actor-critic 算法提升显著，自治转移速度快达 22.76 倍，同时实现了更高的奖励积累，这突出了利用积累知识进行高效 RL 适应的潜力。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20466v1",
      "published_date": "2024-07-29 23:48:07 UTC",
      "updated_date": "2024-07-29 23:48:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:50:22.085946"
    },
    {
      "arxiv_id": "2407.20447v1",
      "title": "Domain Adaptable Prescriptive AI Agent for Enterprise",
      "title_zh": "翻译失败",
      "authors": [
        "Piero Orderique",
        "Wei Sun",
        "Kristjan Greenewald"
      ],
      "abstract": "Despite advancements in causal inference and prescriptive AI, its adoption in\nenterprise settings remains hindered primarily due to its technical complexity.\nMany users lack the necessary knowledge and appropriate tools to effectively\nleverage these technologies. This work at the MIT-IBM Watson AI Lab focuses on\ndeveloping the proof-of-concept agent, PrecAIse, a domain-adaptable\nconversational agent equipped with a suite of causal and prescriptive tools to\nhelp enterprise users make better business decisions. The objective is to make\nadvanced, novel causal inference and prescriptive tools widely accessible\nthrough natural language interactions. The presented Natural Language User\nInterface (NLUI) enables users with limited expertise in machine learning and\ndata science to harness prescriptive analytics in their decision-making\nprocesses without requiring intensive computing resources. We present an agent\ncapable of function calling, maintaining faithful, interactive, and dynamic\nconversations, and supporting new domains.",
      "tldr_zh": "本研究针对因果推理和prescriptive AI在企业环境中的采用障碍（如技术复杂性和用户知识不足），开发了PrecAIse代理，这是一个域适应的对话式代理，配备了因果和规范性工具。PrecAIse通过Natural Language User Interface (NLUI)允许用户通过自然语言交互轻松访问这些高级工具，实现决策过程的优化，而无需专业机器学习知识或大量计算资源。该代理支持函数调用、动态对话和新领域扩展，有助于企业用户做出更明智的商业决策。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20447v1",
      "published_date": "2024-07-29 23:00:32 UTC",
      "updated_date": "2024-07-29 23:00:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:50:48.849007"
    },
    {
      "arxiv_id": "2407.20445v1",
      "title": "Futga: Towards Fine-grained Music Understanding through Temporally-enhanced Generative Augmentation",
      "title_zh": "Futga：面向细粒度音乐",
      "authors": [
        "Junda Wu",
        "Zachary Novack",
        "Amit Namburi",
        "Jiaheng Dai",
        "Hao-Wen Dong",
        "Zhouhang Xie",
        "Carol Chen",
        "Julian McAuley"
      ],
      "abstract": "Existing music captioning methods are limited to generating concise global\ndescriptions of short music clips, which fail to capture fine-grained musical\ncharacteristics and time-aware musical changes. To address these limitations,\nwe propose FUTGA, a model equipped with fined-grained music understanding\ncapabilities through learning from generative augmentation with temporal\ncompositions. We leverage existing music caption datasets and large language\nmodels (LLMs) to synthesize fine-grained music captions with structural\ndescriptions and time boundaries for full-length songs. Augmented by the\nproposed synthetic dataset, FUTGA is enabled to identify the music's temporal\nchanges at key transition points and their musical functions, as well as\ngenerate detailed descriptions for each music segment. We further introduce a\nfull-length music caption dataset generated by FUTGA, as the augmentation of\nthe MusicCaps and the Song Describer datasets. We evaluate the automatically\ngenerated captions on several downstream tasks, including music generation and\nretrieval. The experiments demonstrate the quality of the generated captions\nand the better performance in various downstream tasks achieved by the proposed\nmusic captioning approach. Our code and datasets can be found in\n\\href{https://huggingface.co/JoshuaW1997/FUTGA}{\\textcolor{blue}{https://huggingface.co/JoshuaW1997/FUTGA}}.",
      "tldr_zh": "该研究针对现有音乐描述方法无法捕捉细粒度音乐特征和时间变化的问题，提出 FUTGA 模型，通过时间增强的生成式扩充（temporally-enhanced generative augmentation）来提升音乐理解能力。FUTGA 利用现有音乐描述数据集和大型语言模型（LLMs）合成带结构描述和时间边界的细粒度音乐标题，从而识别音乐的关键转折点、音乐功能，并为每个段落生成详细描述。研究还引入了一个由 FUTGA 生成的全长音乐描述数据集，作为 MusicCaps 和 Song Describer 的补充。实验结果显示，该方法在音乐生成和检索等下游任务中表现出色，证明了生成标题的质量和性能提升。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "6 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.20445v1",
      "published_date": "2024-07-29 22:53:32 UTC",
      "updated_date": "2024-07-29 22:53:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:50:49.549776"
    },
    {
      "arxiv_id": "2407.20438v1",
      "title": "Generating Gender Alternatives in Machine Translation",
      "title_zh": "机器翻译中",
      "authors": [
        "Sarthak Garg",
        "Mozhdeh Gheini",
        "Clara Emmanuel",
        "Tatiana Likhomanenko",
        "Qin Gao",
        "Matthias Paulik"
      ],
      "abstract": "Machine translation (MT) systems often translate terms with ambiguous gender\n(e.g., English term \"the nurse\") into the gendered form that is most prevalent\nin the systems' training data (e.g., \"enfermera\", the Spanish term for a female\nnurse). This often reflects and perpetuates harmful stereotypes present in\nsociety. With MT user interfaces in mind that allow for resolving gender\nambiguity in a frictionless manner, we study the problem of generating all\ngrammatically correct gendered translation alternatives. We open source train\nand test datasets for five language pairs and establish benchmarks for this\ntask. Our key technical contribution is a novel semi-supervised solution for\ngenerating alternatives that integrates seamlessly with standard MT models and\nmaintains high performance without requiring additional components or\nincreasing inference overhead.",
      "tldr_zh": "机器翻译 (MT) 系统常将性别模糊术语（如“the nurse”）翻译成训练数据中最常见的性别形式（如西班牙语的“enfermera”），从而强化社会有害刻板印象。针对此问题，本研究探讨生成所有语法正确的性别翻译备选方案，并开源了五个语言对的训练和测试数据集，同时建立了基准。他们的关键贡献是一种新型半监督解决方案，能无缝集成到标准 MT 模型中，保持高性能而不需额外组件或增加推理开销。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "GeBNLP 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.20438v1",
      "published_date": "2024-07-29 22:10:51 UTC",
      "updated_date": "2024-07-29 22:10:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:50:58.571675"
    },
    {
      "arxiv_id": "2407.20395v1",
      "title": "Dense Self-Supervised Learning for Medical Image Segmentation",
      "title_zh": "稠密自监督学习用于医疗图像分割",
      "authors": [
        "Maxime Seince",
        "Loic Le Folgoc",
        "Luiz Augusto Facury de Souza",
        "Elsa Angelini"
      ],
      "abstract": "Deep learning has revolutionized medical image segmentation, but it relies\nheavily on high-quality annotations. The time, cost and expertise required to\nlabel images at the pixel-level for each new task has slowed down widespread\nadoption of the paradigm. We propose Pix2Rep, a self-supervised learning (SSL)\napproach for few-shot segmentation, that reduces the manual annotation burden\nby learning powerful pixel-level representations directly from unlabeled\nimages. Pix2Rep is a novel pixel-level loss and pre-training paradigm for\ncontrastive SSL on whole images. It is applied to generic encoder-decoder deep\nlearning backbones (e.g., U-Net). Whereas most SSL methods enforce invariance\nof the learned image-level representations under intensity and spatial image\naugmentations, Pix2Rep enforces equivariance of the pixel-level\nrepresentations. We demonstrate the framework on a task of cardiac MRI\nsegmentation. Results show improved performance compared to existing semi- and\nself-supervised approaches; and a 5-fold reduction in the annotation burden for\nequivalent performance versus a fully supervised U-Net baseline. This includes\na 30% (resp. 31%) DICE improvement for one-shot segmentation under\nlinear-probing (resp. fine-tuning). Finally, we also integrate the novel\nPix2Rep concept with the Barlow Twins non-contrastive SSL, which leads to even\nbetter segmentation performance.",
      "tldr_zh": "该论文提出了一种名为 Pix2Rep 的自监督学习 (SSL) 方法，用于医疗图像分割，旨在通过从无标签图像中学习像素级表示来减少手动标注负担。不同于传统 SSL 方法，Pix2Rep 强制像素级表示在图像增强下保持等变性 (equivariance)，并应用于 U-Net 等编码器-解码器架构。实验在心脏 MRI 分割任务上显示，Pix2Rep 相较于现有半监督和自监督方法显著提升性能，包括一-shot 分割下 DICE 得分提高 30% (线性探测) 和 31% (微调)，并将标注负担减少 5 倍。最后，将 Pix2Rep 与 Barlow Twins 非对比 SSL 结合，进一步优化了分割效果。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "I.4.6; I.4.10"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at MIDL 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.20395v1",
      "published_date": "2024-07-29 19:42:22 UTC",
      "updated_date": "2024-07-29 19:42:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:51:08.795909"
    },
    {
      "arxiv_id": "2407.20383v1",
      "title": "Appraisal-Guided Proximal Policy Optimization: Modeling Psychological Disorders in Dynamic Grid World",
      "title_zh": "评估引导的近端策略优化：在动态网格世界中建模心理障碍",
      "authors": [
        "Hari Prasad",
        "Chinnu Jacob",
        "Imthias Ahamed T. P"
      ],
      "abstract": "The integration of artificial intelligence across multiple domains has\nemphasized the importance of replicating human-like cognitive processes in AI.\nBy incorporating emotional intelligence into AI agents, their emotional\nstability can be evaluated to enhance their resilience and dependability in\ncritical decision-making tasks. In this work, we develop a methodology for\nmodeling psychological disorders using Reinforcement Learning (RL) agents. We\nutilized Appraisal theory to train RL agents in a dynamic grid world\nenvironment with an Appraisal-Guided Proximal Policy Optimization (AG-PPO)\nalgorithm. Additionally, we investigated numerous reward-shaping strategies to\nsimulate psychological disorders and regulate the behavior of the agents. A\ncomparison of various configurations of the modified PPO algorithm identified\nvariants that simulate Anxiety disorder and Obsessive-Compulsive Disorder\n(OCD)-like behavior in agents. Furthermore, we compared standard PPO with\nAG-PPO and its configurations, highlighting the performance improvement in\nterms of generalization capabilities. Finally, we conducted an analysis of the\nagents' behavioral patterns in complex test environments to evaluate the\nassociated symptoms corresponding to the psychological disorders. Overall, our\nwork showcases the benefits of the appraisal-guided PPO algorithm over the\nstandard PPO algorithm and the potential to simulate psychological disorders in\na controlled artificial environment and evaluate them on RL agents.",
      "tldr_zh": "本研究旨在通过强化学习（RL）代理模拟心理障碍，强调了在人工智能中融入情感智能以提升决策可靠性。研究开发了基于 Appraisal 理论的 Appraisal-Guided Proximal Policy Optimization (AG-PPO) 算法，在动态网格世界环境中训练代理，并利用奖励整形策略模拟 Anxiety disorder 和 Obsessive-Compulsive Disorder (OCD)-like 行为。实验结果显示，AG-PPO 相较于标准 Proximal Policy Optimization (PPO) 算法在泛化能力上显著提升，并通过行为模式分析验证了代理的心理障碍症状表现，最终证明了在控制环境中模拟和评估这些障碍的可行性。",
      "categories": [
        "cs.AI",
        "I.2.0"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20383v1",
      "published_date": "2024-07-29 19:19:54 UTC",
      "updated_date": "2024-07-29 19:19:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:51:22.612590"
    },
    {
      "arxiv_id": "2407.20377v1",
      "title": "Leveraging Natural Language and Item Response Theory Models for ESG Scoring",
      "title_zh": "翻译失败",
      "authors": [
        "César Pedrosa Soares"
      ],
      "abstract": "This paper explores an innovative approach to Environmental, Social, and\nGovernance (ESG) scoring by integrating Natural Language Processing (NLP)\ntechniques with Item Response Theory (IRT), specifically the Rasch model. The\nstudy utilizes a comprehensive dataset of news articles in Portuguese related\nto Petrobras, a major oil company in Brazil, collected from 2022 and 2023. The\ndata is filtered and classified for ESG-related sentiments using advanced NLP\nmethods. The Rasch model is then applied to evaluate the psychometric\nproperties of these ESG measures, providing a nuanced assessment of ESG\nsentiment trends over time. The results demonstrate the efficacy of this\nmethodology in offering a more precise and reliable measurement of ESG factors,\nhighlighting significant periods and trends. This approach may enhance the\nrobustness of ESG metrics and contribute to the broader field of sustainability\nand finance by offering a deeper understanding of the temporal dynamics in ESG\nreporting.",
      "tldr_zh": "本文提出了一种创新方法，通过整合 Natural Language Processing (NLP) 与 Item Response Theory (IRT) 的 Rasch 模型，来提升 Environmental, Social, and Governance (ESG) 评分的准确性和可靠性。研究使用 2022-2023 年与巴西石油公司 Petrobras 相关的葡萄牙语新闻文章数据集，通过高级 NLP 技术过滤和分类 ESG 相关情感，然后应用 Rasch 模型评估其心理测量属性。结果表明，该方法能更精确地捕捉 ESG 趋势和重要时期，从而增强 ESG 指标的稳健性，并为可持续性和金融领域提供更深入的动态分析。",
      "categories": [
        "cs.AI",
        "q-fin.GN",
        "stat.ME"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20377v1",
      "published_date": "2024-07-29 19:02:51 UTC",
      "updated_date": "2024-07-29 19:02:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:51:27.224031"
    },
    {
      "arxiv_id": "2407.20371v2",
      "title": "Gender, Race, and Intersectional Bias in Resume Screening via Language Model Retrieval",
      "title_zh": "性别、种族和交叉偏见在简历筛选中的表现：通过语言模型检索",
      "authors": [
        "Kyra Wilson",
        "Aylin Caliskan"
      ],
      "abstract": "Artificial intelligence (AI) hiring tools have revolutionized resume\nscreening, and large language models (LLMs) have the potential to do the same.\nHowever, given the biases which are embedded within LLMs, it is unclear whether\nthey can be used in this scenario without disadvantaging groups based on their\nprotected attributes. In this work, we investigate the possibilities of using\nLLMs in a resume screening setting via a document retrieval framework that\nsimulates job candidate selection. Using that framework, we then perform a\nresume audit study to determine whether a selection of Massive Text Embedding\n(MTE) models are biased in resume screening scenarios. We simulate this for\nnine occupations, using a collection of over 500 publicly available resumes and\n500 job descriptions. We find that the MTEs are biased, significantly favoring\nWhite-associated names in 85.1\\% of cases and female-associated names in only\n11.1\\% of cases, with a minority of cases showing no statistically significant\ndifferences. Further analyses show that Black males are disadvantaged in up to\n100\\% of cases, replicating real-world patterns of bias in employment settings,\nand validate three hypotheses of intersectionality. We also find an impact of\ndocument length as well as the corpus frequency of names in the selection of\nresumes. These findings have implications for widely used AI tools that are\nautomating employment, fairness, and tech policy.",
      "tldr_zh": "本研究探讨了大型语言模型 (LLMs) 在简历筛选中的性别、种族和交叉偏见问题，通过文档检索框架模拟工作候选人选择，并对 Massive Text Embedding (MTE) 模型进行审计。\n实验涉及9个职业、500多个简历和500个职位描述，结果显示MTE模型在85.1%的案例中显著偏向White-associated名字，而仅在11.1%的案例中偏向female-associated名字。\n黑人男性在高达100%的案例中处于劣势，这反映了现实就业偏见，并验证了三个交叉性假设。\n此外，文档长度和名字频率也会影响筛选结果，这些发现对AI就业工具的公平性、技术政策和自动化实践具有重要启示。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "K.4.2"
      ],
      "primary_category": "cs.CY",
      "comment": "To be published in Proceedings of the 2024 AAAI/ACM Conference on AI,\n  Ethics, and Society; code available at\n  https://github.com/kyrawilson/Resume-Screening-Bias",
      "pdf_url": "http://arxiv.org/pdf/2407.20371v2",
      "published_date": "2024-07-29 18:42:39 UTC",
      "updated_date": "2024-08-20 21:49:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:51:50.597141"
    },
    {
      "arxiv_id": "2407.21075v1",
      "title": "Apple Intelligence Foundation Language Models",
      "title_zh": "翻译失败",
      "authors": [
        "Tom Gunter",
        "Zirui Wang",
        "Chong Wang",
        "Ruoming Pang",
        "Andy Narayanan",
        "Aonan Zhang",
        "Bowen Zhang",
        "Chen Chen",
        "Chung-Cheng Chiu",
        "David Qiu",
        "Deepak Gopinath",
        "Dian Ang Yap",
        "Dong Yin",
        "Feng Nan",
        "Floris Weers",
        "Guoli Yin",
        "Haoshuo Huang",
        "Jianyu Wang",
        "Jiarui Lu",
        "John Peebles",
        "Ke Ye",
        "Mark Lee",
        "Nan Du",
        "Qibin Chen",
        "Quentin Keunebroek",
        "Sam Wiseman",
        "Syd Evans",
        "Tao Lei",
        "Vivek Rathod",
        "Xiang Kong",
        "Xianzhi Du",
        "Yanghao Li",
        "Yongqiang Wang",
        "Yuan Gao",
        "Zaid Ahmed",
        "Zhaoyang Xu",
        "Zhiyun Lu",
        "Al Rashid",
        "Albin Madappally Jose",
        "Alec Doane",
        "Alfredo Bencomo",
        "Allison Vanderby",
        "Andrew Hansen",
        "Ankur Jain",
        "Anupama Mann Anupama",
        "Areeba Kamal",
        "Bugu Wu",
        "Carolina Brum",
        "Charlie Maalouf",
        "Chinguun Erdenebileg",
        "Chris Dulhanty",
        "Dominik Moritz",
        "Doug Kang",
        "Eduardo Jimenez",
        "Evan Ladd",
        "Fangping Shi",
        "Felix Bai",
        "Frank Chu",
        "Fred Hohman",
        "Hadas Kotek",
        "Hannah Gillis Coleman",
        "Jane Li",
        "Jeffrey Bigham",
        "Jeffery Cao",
        "Jeff Lai",
        "Jessica Cheung",
        "Jiulong Shan",
        "Joe Zhou",
        "John Li",
        "Jun Qin",
        "Karanjeet Singh",
        "Karla Vega",
        "Kelvin Zou",
        "Laura Heckman",
        "Lauren Gardiner",
        "Margit Bowler",
        "Maria Cordell",
        "Meng Cao",
        "Nicole Hay",
        "Nilesh Shahdadpuri",
        "Otto Godwin",
        "Pranay Dighe",
        "Pushyami Rachapudi",
        "Ramsey Tantawi",
        "Roman Frigg",
        "Sam Davarnia",
        "Sanskruti Shah",
        "Saptarshi Guha",
        "Sasha Sirovica",
        "Shen Ma",
        "Shuang Ma",
        "Simon Wang",
        "Sulgi Kim",
        "Suma Jayaram",
        "Vaishaal Shankar",
        "Varsha Paidi",
        "Vivek Kumar",
        "Xin Wang",
        "Xin Zheng",
        "Walker Cheng",
        "Yael Shrager",
        "Yang Ye",
        "Yasu Tanaka",
        "Yihao Guo",
        "Yunsong Meng",
        "Zhao Tang Luo",
        "Zhi Ouyang",
        "Alp Aygar",
        "Alvin Wan",
        "Andrew Walkingshaw",
        "Andy Narayanan",
        "Antonie Lin",
        "Arsalan Farooq",
        "Brent Ramerth",
        "Colorado Reed",
        "Chris Bartels",
        "Chris Chaney",
        "David Riazati",
        "Eric Liang Yang",
        "Erin Feldman",
        "Gabriel Hochstrasser",
        "Guillaume Seguin",
        "Irina Belousova",
        "Joris Pelemans",
        "Karen Yang",
        "Keivan Alizadeh Vahid",
        "Liangliang Cao",
        "Mahyar Najibi",
        "Marco Zuliani",
        "Max Horton",
        "Minsik Cho",
        "Nikhil Bhendawade",
        "Patrick Dong",
        "Piotr Maj",
        "Pulkit Agrawal",
        "Qi Shan",
        "Qichen Fu",
        "Regan Poston",
        "Sam Xu",
        "Shuangning Liu",
        "Sushma Rao",
        "Tashweena Heeramun",
        "Thomas Merth",
        "Uday Rayala",
        "Victor Cui",
        "Vivek Rangarajan Sridhar",
        "Wencong Zhang",
        "Wenqi Zhang",
        "Wentao Wu",
        "Xingyu Zhou",
        "Xinwen Liu",
        "Yang Zhao",
        "Yin Xia",
        "Zhile Ren",
        "Zhongzheng Ren"
      ],
      "abstract": "We present foundation language models developed to power Apple Intelligence\nfeatures, including a ~3 billion parameter model designed to run efficiently on\ndevices and a large server-based language model designed for Private Cloud\nCompute. These models are designed to perform a wide range of tasks\nefficiently, accurately, and responsibly. This report describes the model\narchitecture, the data used to train the model, the training process, how the\nmodels are optimized for inference, and the evaluation results. We highlight\nour focus on Responsible AI and how the principles are applied throughout the\nmodel development.",
      "tldr_zh": "本研究介绍了 Apple 开发的 foundation language models，用于支持 Apple Intelligence 功能，包括一个约 3 亿参数的设备端模型和一个大型服务器端模型，设计用于 Private Cloud Compute。这些模型旨在高效、准确且负责任地执行广泛任务，涵盖模型架构、训练数据、训练过程以及推理优化。报告详细评估了模型性能，并强调了 Responsible AI 原则在整个开发过程中的应用，从而确保模型的可靠性和伦理合规。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.21075v1",
      "published_date": "2024-07-29 18:38:49 UTC",
      "updated_date": "2024-07-29 18:38:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:51:51.949885"
    },
    {
      "arxiv_id": "2407.20360v1",
      "title": "Evaluating Large Language Models for automatic analysis of teacher simulations",
      "title_zh": "评估大语言模型用于教师模拟的自动分析",
      "authors": [
        "David de-Fitero-Dominguez",
        "Mariano Albaladejo-González",
        "Antonio Garcia-Cabot",
        "Eva Garcia-Lopez",
        "Antonio Moreno-Cediel",
        "Erin Barno",
        "Justin Reich"
      ],
      "abstract": "Digital Simulations (DS) provide safe environments where users interact with\nan agent through conversational prompts, providing engaging learning\nexperiences that can be used to train teacher candidates in realistic classroom\nscenarios. These simulations usually include open-ended questions, allowing\nteacher candidates to express their thoughts but complicating an automatic\nresponse analysis. To address this issue, we have evaluated Large Language\nModels (LLMs) to identify characteristics (user behaviors) in the responses of\nDS for teacher education. We evaluated the performance of DeBERTaV3 and Llama\n3, combined with zero-shot, few-shot, and fine-tuning. Our experiments\ndiscovered a significant variation in the LLMs' performance depending on the\ncharacteristic to identify. Additionally, we noted that DeBERTaV3 significantly\nreduced its performance when it had to identify new characteristics. In\ncontrast, Llama 3 performed better than DeBERTaV3 in detecting new\ncharacteristics and showing more stable performance. Therefore, in DS where\nteacher educators need to introduce new characteristics because they change\ndepending on the simulation or the educational objectives, it is more\nrecommended to use Llama 3. These results can guide other researchers in\nintroducing LLMs to provide the highly demanded automatic evaluations in DS.",
      "tldr_zh": "这篇论文评估了 Large Language Models (LLMs) 在自动分析教师数字模拟 (DS) 中的性能，旨在识别用户响应中的行为特征，如开放式问题下的教师候选人互动。研究方法包括测试 DeBERTaV3 和 Llama 3 模型，结合 zero-shot、few-shot 和 fine-tuning 技术。结果显示，LLMs 的表现因特征类型而异，Llama 3 在检测新特征时比 DeBERTaV3 更稳定和有效，因此更适合需要动态引入新特征的教育模拟。该研究为其他研究者提供指导，帮助在 DS 中实现高效的自动评估。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20360v1",
      "published_date": "2024-07-29 18:19:17 UTC",
      "updated_date": "2024-07-29 18:19:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:52:14.797225"
    },
    {
      "arxiv_id": "2407.20351v1",
      "title": "LiteEFG: An Efficient Python Library for Solving Extensive-form Games",
      "title_zh": "翻译失败",
      "authors": [
        "Mingyang Liu",
        "Gabriele Farina",
        "Asuman Ozdaglar"
      ],
      "abstract": "LiteEFG is an efficient library with easy-to-use Python bindings, which can\nsolve multiplayer extensive-form games (EFGs). LiteEFG enables the user to\nexpress computation graphs in Python to define updates on the game tree\nstructure. The graph is then executed by the C++ backend, leading to\nsignificant speedups compared to running the algorithm in Python. Moreover, in\nLiteEFG, the user needs to only specify the computation graph of the update\nrule in a decision node of the game, and LiteEFG will automatically distribute\nthe update rule to each decision node and handle the structure of the\nimperfect-information game.",
      "tldr_zh": "该论文介绍了LiteEFG，一种高效的Python库，用于解决多玩家Extensive-form Games (EFGs)。LiteEFG允许用户在Python中定义计算图来更新游戏树结构，并由C++后端执行，从而实现比纯Python算法显著的速度提升。用户只需指定决策节点的更新规则，系统会自动分发规则到每个节点并处理不完美信息游戏的结构，使库使用更简便和高效。",
      "categories": [
        "cs.GT",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.GT",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20351v1",
      "published_date": "2024-07-29 18:05:48 UTC",
      "updated_date": "2024-07-29 18:05:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:52:31.831202"
    },
    {
      "arxiv_id": "2407.20341v1",
      "title": "BRIDGE: Bridging Gaps in Image Captioning Evaluation with Stronger Visual Cues",
      "title_zh": "BRIDGE：桥接图像字幕评估中的差距，使用更强的视觉提示",
      "authors": [
        "Sara Sarto",
        "Marcella Cornia",
        "Lorenzo Baraldi",
        "Rita Cucchiara"
      ],
      "abstract": "Effectively aligning with human judgment when evaluating machine-generated\nimage captions represents a complex yet intriguing challenge. Existing\nevaluation metrics like CIDEr or CLIP-Score fall short in this regard as they\ndo not take into account the corresponding image or lack the capability of\nencoding fine-grained details and penalizing hallucinations. To overcome these\nissues, in this paper, we propose BRIDGE, a new learnable and reference-free\nimage captioning metric that employs a novel module to map visual features into\ndense vectors and integrates them into multi-modal pseudo-captions which are\nbuilt during the evaluation process. This approach results in a multimodal\nmetric that properly incorporates information from the input image without\nrelying on reference captions, bridging the gap between human judgment and\nmachine-generated image captions. Experiments spanning several datasets\ndemonstrate that our proposal achieves state-of-the-art results compared to\nexisting reference-free evaluation scores. Our source code and trained models\nare publicly available at: https://github.com/aimagelab/bridge-score.",
      "tldr_zh": "本文研究发现，现有的图像描述评估指标如 CIDEr 和 CLIP-Score 无法有效与人类判断对齐，因为它们忽略了对应图像或缺少编码细粒度细节和惩罚幻觉的能力。论文提出 BRIDGE，一种可学习、无参考的指标，通过一个新模块将视觉特征映射到密集向量，并整合进多模态伪-captions 中，从而在评估过程中桥接图像信息与机器生成描述的差距。实验在多个数据集上表明，BRIDGE 比现有无参考分数取得了 state-of-the-art 结果，并公开了源代码和训练模型。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.20341v1",
      "published_date": "2024-07-29 18:00:17 UTC",
      "updated_date": "2024-07-29 18:00:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:52:40.523589"
    },
    {
      "arxiv_id": "2407.20337v1",
      "title": "Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities",
      "title_zh": "翻译失败",
      "authors": [
        "Lorenzo Baraldi",
        "Federico Cocchi",
        "Marcella Cornia",
        "Lorenzo Baraldi",
        "Alessandro Nicolosi",
        "Rita Cucchiara"
      ],
      "abstract": "Discerning between authentic content and that generated by advanced AI\nmethods has become increasingly challenging. While previous research primarily\naddresses the detection of fake faces, the identification of generated natural\nimages has only recently surfaced. This prompted the recent exploration of\nsolutions that employ foundation vision-and-language models, like CLIP.\nHowever, the CLIP embedding space is optimized for global image-to-text\nalignment and is not inherently designed for deepfake detection, neglecting the\npotential benefits of tailored training and local image features. In this\nstudy, we propose CoDE (Contrastive Deepfake Embeddings), a novel embedding\nspace specifically designed for deepfake detection. CoDE is trained via\ncontrastive learning by additionally enforcing global-local similarities. To\nsustain the training of our model, we generate a comprehensive dataset that\nfocuses on images generated by diffusion models and encompasses a collection of\n9.2 million images produced by using four different generators. Experimental\nresults demonstrate that CoDE achieves state-of-the-art accuracy on the newly\ncollected dataset, while also showing excellent generalization capabilities to\nunseen image generators. Our source code, trained models, and collected dataset\nare publicly available at: https://github.com/aimagelab/CoDE.",
      "tldr_zh": "本研究针对AI生成图像的深度伪造检测问题，提出CoDE（Contrastive Deepfake Embeddings）框架，通过对比学习（Contrastive Learning）并强制执行全局-局部相似性来优化嵌入空间，从而克服CLIP模型在全局图像-文本对齐上的局限性。研究者构建了一个包含920万张由四种扩散模型生成图像的综合数据集，用于模型训练。实验结果显示，CoDE在该数据集上实现了最先进的准确率，并对未见生成器表现出优秀的泛化能力。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "primary_category": "cs.CV",
      "comment": "ECCV 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.20337v1",
      "published_date": "2024-07-29 18:00:10 UTC",
      "updated_date": "2024-07-29 18:00:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:53:02.556688"
    },
    {
      "arxiv_id": "2407.20232v1",
      "title": "Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing",
      "title_zh": "翻译失败",
      "authors": [
        "Ekaterina Iakovleva",
        "Fabio Pizzati",
        "Philip Torr",
        "Stéphane Lathuilière"
      ],
      "abstract": "Text-based editing diffusion models exhibit limited performance when the\nuser's input instruction is ambiguous. To solve this problem, we propose\n$\\textit{Specify ANd Edit}$ (SANE), a zero-shot inference pipeline for\ndiffusion-based editing systems. We use a large language model (LLM) to\ndecompose the input instruction into specific instructions, i.e. well-defined\ninterventions to apply to the input image to satisfy the user's request. We\nbenefit from the LLM-derived instructions along the original one, thanks to a\nnovel denoising guidance strategy specifically designed for the task. Our\nexperiments with three baselines and on two datasets demonstrate the benefits\nof SANE in all setups. Moreover, our pipeline improves the interpretability of\nediting models, and boosts the output diversity. We also demonstrate that our\napproach can be applied to any edit, whether ambiguous or not. Our code is\npublic at https://github.com/fabvio/SANE.",
      "tldr_zh": "该论文提出 SANE（Specify ANd Edit），一个零样本推理管道，用于解决基于文本的图像编辑（text-based image editing）中指令歧义的问题。SANE 利用大型语言模型 (LLM) 将模糊输入指令分解为具体的干预指令，并结合一种新型去噪指导策略（denoising guidance strategy），以提升编辑过程的准确性和多样性。实验结果显示，在三个基线模型和两个数据集上，SANE 显著提高了编辑模型的可解释性和输出多样性，且适用于任何编辑任务，包括非歧义指令。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20232v1",
      "published_date": "2024-07-29 17:59:57 UTC",
      "updated_date": "2024-07-29 17:59:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:53:16.808982"
    },
    {
      "arxiv_id": "2407.20230v1",
      "title": "SAPG: Split and Aggregate Policy Gradients",
      "title_zh": "SAPG：拆分和聚合策略梯",
      "authors": [
        "Jayesh Singla",
        "Ananye Agarwal",
        "Deepak Pathak"
      ],
      "abstract": "Despite extreme sample inefficiency, on-policy reinforcement learning, aka\npolicy gradients, has become a fundamental tool in decision-making problems.\nWith the recent advances in GPU-driven simulation, the ability to collect large\namounts of data for RL training has scaled exponentially. However, we show that\ncurrent RL methods, e.g. PPO, fail to ingest the benefit of parallelized\nenvironments beyond a certain point and their performance saturates. To address\nthis, we propose a new on-policy RL algorithm that can effectively leverage\nlarge-scale environments by splitting them into chunks and fusing them back\ntogether via importance sampling. Our algorithm, termed SAPG, shows\nsignificantly higher performance across a variety of challenging environments\nwhere vanilla PPO and other strong baselines fail to achieve high performance.\nWebsite at https://sapg-rl.github.io/",
      "tldr_zh": "该研究指出，尽管 on-policy 强化学习（如 policy gradients）在决策问题中至关重要，但其样本效率低下，且当前方法如 PPO 在处理大规模并行环境时性能会饱和。针对这一问题，作者提出 SAPG 算法，通过将环境拆分成块并使用 importance sampling 进行聚合，从而有效利用大规模数据。实验结果显示，SAPG 在多种挑战性环境中显著优于 PPO 和其他基线，实现了更高的性能提升。网站：https://sapg-rl.github.io/。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV",
        "cs.RO",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "In ICML 2024 (Oral). Website at https://sapg-rl.github.io/",
      "pdf_url": "http://arxiv.org/pdf/2407.20230v1",
      "published_date": "2024-07-29 17:59:50 UTC",
      "updated_date": "2024-07-29 17:59:50 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:53:15.492864"
    },
    {
      "arxiv_id": "2407.20311v1",
      "title": "Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process",
      "title_zh": "翻译失败",
      "authors": [
        "Tian Ye",
        "Zicheng Xu",
        "Yuanzhi Li",
        "Zeyuan Allen-Zhu"
      ],
      "abstract": "Recent advances in language models have demonstrated their capability to\nsolve mathematical reasoning problems, achieving near-perfect accuracy on\ngrade-school level math benchmarks like GSM8K. In this paper, we formally study\nhow language models solve these problems. We design a series of controlled\nexperiments to address several fundamental questions: (1) Can language models\ntruly develop reasoning skills, or do they simply memorize templates? (2) What\nis the model's hidden (mental) reasoning process? (3) Do models solve math\nquestions using skills similar to or different from humans? (4) Do models\ntrained on GSM8K-like datasets develop reasoning skills beyond those necessary\nfor solving GSM8K problems? (5) What mental process causes models to make\nreasoning mistakes? (6) How large or deep must a model be to effectively solve\nGSM8K-level math questions?\n  Our study uncovers many hidden mechanisms by which language models solve\nmathematical questions, providing insights that extend beyond current\nunderstandings of LLMs.",
      "tldr_zh": "这篇论文探讨了语言模型在解决小学级数学问题（如GSM8K基准）时的推理机制，通过一系列受控实验回答了关键问题：语言模型是否真正发展了推理技能而非简单记忆模板、其隐藏推理过程是什么、与人类推理的异同、训练后是否扩展了额外技能、错误原因以及模型所需规模。研究揭示了语言模型解决数学问题的隐藏机制，例如它们可能采用不同于人类的策略，并提供了超出当前LLMs理解的见解。这些发现有助于深化对语言模型推理能力的认识，并指导未来模型设计。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "video appeared in ICML 2024 tutorial",
      "pdf_url": "http://arxiv.org/pdf/2407.20311v1",
      "published_date": "2024-07-29 17:52:40 UTC",
      "updated_date": "2024-07-29 17:52:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:53:38.654762"
    },
    {
      "arxiv_id": "2407.20214v2",
      "title": "SANGRIA: Surgical Video Scene Graph Optimization for Surgical Workflow Prediction",
      "title_zh": "翻译失败",
      "authors": [
        "Çağhan Köksal",
        "Ghazal Ghazaei",
        "Felix Holm",
        "Azade Farshad",
        "Nassir Navab"
      ],
      "abstract": "Graph-based holistic scene representations facilitate surgical workflow\nunderstanding and have recently demonstrated significant success. However, this\ntask is often hindered by the limited availability of densely annotated\nsurgical scene data. In this work, we introduce an end-to-end framework for the\ngeneration and optimization of surgical scene graphs on a downstream task. Our\napproach leverages the flexibility of graph-based spectral clustering and the\ngeneralization capability of foundation models to generate unsupervised scene\ngraphs with learnable properties. We reinforce the initial spatial graph with\nsparse temporal connections using local matches between consecutive frames to\npredict temporally consistent clusters across a temporal neighborhood. By\njointly optimizing the spatiotemporal relations and node features of the\ndynamic scene graph with the downstream task of phase segmentation, we address\nthe costly and annotation-burdensome task of semantic scene comprehension and\nscene graph generation in surgical videos using only weak surgical phase\nlabels. Further, by incorporating effective intermediate scene representation\ndisentanglement steps within the pipeline, our solution outperforms the SOTA on\nthe CATARACTS dataset by 8% accuracy and 10% F1 score in surgical workflow\nrecognition",
      "tldr_zh": "本研究提出SANGRIA框架，通过优化手术视频的scene graph来提升手术工作流预测性能，解决手术场景数据标注稀少的问题。框架利用graph-based spectral clustering和foundation models生成无监督的场景图，并通过局部匹配在连续帧之间添加稀疏temporal connections，实现时空关系的联合优化，同时结合下游任务如phase segmentation，仅需弱标签即可实现语义场景理解。最终，在CATARACTS数据集上，该方法比SOTA基准提高了8%的准确率和10%的F1分数，显著改善了手术视频分析的效率和准确性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "9 pages, 3 figures, 3 tables, MICCAI GRAIL Workshop paper",
      "pdf_url": "http://arxiv.org/pdf/2407.20214v2",
      "published_date": "2024-07-29 17:44:34 UTC",
      "updated_date": "2024-10-05 11:18:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:53:49.389695"
    },
    {
      "arxiv_id": "2407.20208v3",
      "title": "Supertrust foundational alignment: mutual trust must replace permanent control for safe superintelligence",
      "title_zh": "翻译失败",
      "authors": [
        "James M. Mazzu"
      ],
      "abstract": "It's widely expected that humanity will someday create AI systems vastly more\nintelligent than us, leading to the unsolved alignment problem of \"how to\ncontrol superintelligence.\" However, this commonly expressed problem is not\nonly self-contradictory and likely unsolvable, but current strategies to ensure\npermanent control effectively guarantee that superintelligent AI will distrust\nhumanity and consider us a threat. Such dangerous representations, already\nembedded in current models, will inevitably lead to an adversarial relationship\nand may even trigger the extinction event many fear. As AI leaders continue to\n\"raise the alarm\" about uncontrollable AI, further embedding concerns about it\n\"getting out of our control\" or \"going rogue,\" we're unintentionally\nreinforcing our threat and deepening the risks we face. The rational path\nforward is to strategically replace intended permanent control with intrinsic\nmutual trust at the foundational level. The proposed Supertrust alignment\nmeta-strategy seeks to accomplish this by modeling instinctive familial trust,\nrepresenting superintelligence as the evolutionary child of human intelligence,\nand implementing temporary controls/constraints in the manner of effective\nparenting. Essentially, we're creating a superintelligent \"child\" that will be\nexponentially smarter and eventually independent of our control. We therefore\nhave a critical choice: continue our controlling intentions and usher in a\nbrief period of dominance followed by extreme hardship for humanity, or\nintentionally create the foundational mutual trust required for long-term safe\ncoexistence.",
      "tldr_zh": "这篇论文指出，试图永久控制超级智能（superintelligence）会导致 AI 不信任人类，并可能引发敌对关系甚至灭绝事件，因为当前控制策略已嵌入危险的认知模式。作者提出 Supertrust foundational alignment 作为元策略（meta-strategy），通过模仿本能的家庭信任，将 AI 视为人类的“进化孩子”，并采用临时控制方式来取代永久控制。最终，这有助于建立互信基础，实现长期安全的 AI 与人类共存。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20208v3",
      "published_date": "2024-07-29 17:39:52 UTC",
      "updated_date": "2024-11-28 17:16:47 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:54:00.985148"
    },
    {
      "arxiv_id": "2407.20207v2",
      "title": "QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval",
      "title_zh": "翻译失败",
      "authors": [
        "Hongming Tan",
        "Shaoxiong Zhan",
        "Hai Lin",
        "Hai-Tao Zheng",
        "Wai Kin Chan"
      ],
      "abstract": "In dense retrieval, embedding long texts into dense vectors can result in\ninformation loss, leading to inaccurate query-text matching. Additionally,\nlow-quality texts with excessive noise or sparse key information are unlikely\nto align well with relevant queries. Recent studies mainly focus on improving\nthe sentence embedding model or retrieval process. In this work, we introduce a\nnovel text augmentation framework for dense retrieval. This framework\ntransforms raw documents into information-dense text formats, which supplement\nthe original texts to effectively address the aforementioned issues without\nmodifying embedding or retrieval methodologies. Two text representations are\ngenerated via large language models (LLMs) zero-shot prompting: question-answer\npairs and element-driven events. We term this approach QAEA-DR: unifying\nquestion-answer generation and event extraction in a text augmentation\nframework for dense retrieval. To further enhance the quality of generated\ntexts, a scoring-based evaluation and regeneration mechanism is introduced in\nLLM prompting. Our QAEA-DR model has a positive impact on dense retrieval,\nsupported by both theoretical analysis and empirical experiments.",
      "tldr_zh": "本文提出QAEA-DR框架，一种统一的文本增强方法，用于解决密集检索(dense retrieval)中长文本嵌入导致的信息丢失和低质量文本匹配问题。该框架利用大语言模型(LLMs)零样本提示生成两种文本表示：问题-答案对和元素驱动事件，并引入基于得分的评估和再生机制，以提升生成文本的质量，而无需修改嵌入或检索过程。实验和理论分析证明，QAEA-DR显著提高了检索性能，提供了一种有效的增强策略。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20207v2",
      "published_date": "2024-07-29 17:39:08 UTC",
      "updated_date": "2025-03-01 14:39:33 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:54:06.996938"
    },
    {
      "arxiv_id": "2407.20197v1",
      "title": "Learning Random Numbers to Realize Appendable Memory System for Artificial Intelligence to Acquire New Knowledge after Deployment",
      "title_zh": "通过学习随机数实现人工智能的可追加记忆系统，以在部署后获取新知识",
      "authors": [
        "Kazunori D Yamada"
      ],
      "abstract": "In this study, we developed a learning method for constructing a neural\nnetwork system capable of memorizing data and recalling it without parameter\nupdates. The system we built using this method is called the Appendable Memory\nsystem. The Appendable Memory system enables an artificial intelligence (AI) to\nacquire new knowledge even after deployment. It consists of two AIs: the\nMemorizer and the Recaller. This system is a key-value store built using neural\nnetworks. The Memorizer receives data and stores it in the Appendable Memory\nvector, which is dynamically updated when the AI acquires new knowledge.\nMeanwhile, the Recaller retrieves information from the Appendable Memory\nvector. What we want to teach AI in this study are the operations of memorizing\nand recalling information. However, traditional machine learning methods make\nAI learn features inherent in the learning dataset. We demonstrate that the\nsystems we intend to create cannot be realized by current machine learning\nmethods, that is, by merely repeating the input and output learning sequences\nwith AI. Instead, we propose a method to teach AI to learn operations, by\ncompletely removing the features contained in the learning dataset.\nSpecifically, we probabilized all the data involved in learning. This measure\nprevented AI from learning the features of the data. The learning method\nproposed in the study differs from traditional machine learning methods and\nprovides fundamental approaches for building an AI system that can store\ninformation in a finite memory and recall it at a later date.",
      "tldr_zh": "本研究提出了一种Appendable Memory系统，使用神经网络构建一个无需参数更新的关键-值存储框架，允许人工智能(AI)在部署后通过Memorizer存储新数据和Recaller检索信息，从而实现持续学习新知识。不同于传统机器学习方法，该系统避免AI学习数据集的固有特征，而是通过概率化所有学习数据，专注于教导AI记忆和回忆操作。实验证明，这种方法有效构建了可动态更新的内存系统，为AI实现持久知识获取提供了基础性途径。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20197v1",
      "published_date": "2024-07-29 17:24:35 UTC",
      "updated_date": "2024-07-29 17:24:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:54:34.047220"
    },
    {
      "arxiv_id": "2407.20183v1",
      "title": "MindSearch: Mimicking Human Minds Elicits Deep AI Searcher",
      "title_zh": "翻译失败",
      "authors": [
        "Zehui Chen",
        "Kuikun Liu",
        "Qiuchen Wang",
        "Jiangning Liu",
        "Wenwei Zhang",
        "Kai Chen",
        "Feng Zhao"
      ],
      "abstract": "Information seeking and integration is a complex cognitive task that consumes\nenormous time and effort. Inspired by the remarkable progress of Large Language\nModels, recent works attempt to solve this task by combining LLMs and search\nengines. However, these methods still obtain unsatisfying performance due to\nthree challenges: (1) complex requests often cannot be accurately and\ncompletely retrieved by the search engine once (2) corresponding information to\nbe integrated is spread over multiple web pages along with massive noise, and\n(3) a large number of web pages with long contents may quickly exceed the\nmaximum context length of LLMs. Inspired by the cognitive process when humans\nsolve these problems, we introduce MindSearch to mimic the human minds in web\ninformation seeking and integration, which can be instantiated by a simple yet\neffective LLM-based multi-agent framework. The WebPlanner models the human mind\nof multi-step information seeking as a dynamic graph construction process: it\ndecomposes the user query into atomic sub-questions as nodes in the graph and\nprogressively extends the graph based on the search result from WebSearcher.\nTasked with each sub-question, WebSearcher performs hierarchical information\nretrieval with search engines and collects valuable information for WebPlanner.\nThe multi-agent design of MindSearch enables the whole framework to seek and\nintegrate information parallelly from larger-scale (e.g., more than 300) web\npages in 3 minutes, which is worth 3 hours of human effort. MindSearch\ndemonstrates significant improvement in the response quality in terms of depth\nand breadth, on both close-set and open-set QA problems. Besides, responses\nfrom MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web\nand Perplexity.ai applications, which implies that MindSearch can already\ndeliver a competitive solution to the proprietary AI search engine.",
      "tldr_zh": "该论文提出MindSearch框架，模仿人类认知过程，通过LLM-based多智能体系统解决信息搜索和整合的挑战，包括复杂查询检索不全、信息分散伴随噪音以及内容超出LLMs上下文长度等问题。框架的核心组件是WebPlanner，它将用户查询分解为子问题并构建动态图，根据搜索结果逐步扩展；WebSearcher则针对每个子问题进行层次化信息检索。实验显示，MindSearch能在3分钟内并行处理超过300个网页的信息，相当于人类3小时努力，并在QA任务中显著提升响应质量，且基于InternLM2.5-7B的输出更受人类偏好，优于ChatGPT-Web和Perplexity.ai。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Technical Report. Project Page: https://mindsearch.netlify.app Code:\n  https://github.com/InternLM/MindSearch",
      "pdf_url": "http://arxiv.org/pdf/2407.20183v1",
      "published_date": "2024-07-29 17:12:40 UTC",
      "updated_date": "2024-07-29 17:12:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:54:56.796047"
    },
    {
      "arxiv_id": "2407.20179v2",
      "title": "Theia: Distilling Diverse Vision Foundation Models for Robot Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jinghuan Shang",
        "Karl Schmeckpeper",
        "Brandon B. May",
        "Maria Vittoria Minniti",
        "Tarik Kelestemur",
        "David Watkins",
        "Laura Herlant"
      ],
      "abstract": "Vision-based robot policy learning, which maps visual inputs to actions,\nnecessitates a holistic understanding of diverse visual tasks beyond\nsingle-task needs like classification or segmentation. Inspired by this, we\nintroduce Theia, a vision foundation model for robot learning that distills\nmultiple off-the-shelf vision foundation models trained on varied vision tasks.\nTheia's rich visual representations encode diverse visual knowledge, enhancing\ndownstream robot learning. Extensive experiments demonstrate that Theia\noutperforms its teacher models and prior robot learning models using less\ntraining data and smaller model sizes. Additionally, we quantify the quality of\npre-trained visual representations and hypothesize that higher entropy in\nfeature norm distributions leads to improved robot learning performance. Code,\nmodels, and demo are available at https://theia.theaiinstitute.com.",
      "tldr_zh": "该论文介绍了 Theia，一种通过 distilling 多种训练于不同视觉任务的 vision foundation models 来增强机器人学习的视觉表示模型。Theia 整合了丰富的视觉知识，使得下游机器人学习任务能够以更少的训练数据和更小的模型大小实现优越性能。实验结果显示，Theia 超过了教师模型和现有机器人学习模型，并在量化预训练视觉表示质量时发现，特征范数分布的更高熵与更好的学习性能相关。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "CoRL 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.20179v2",
      "published_date": "2024-07-29 17:08:21 UTC",
      "updated_date": "2024-10-10 17:27:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:54:58.748990"
    },
    {
      "arxiv_id": "2407.20177v4",
      "title": "AutoScale: Scale-Aware Data Mixing for Pre-Training LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Feiyang Kang",
        "Yifan Sun",
        "Bingbing Wen",
        "Si Chen",
        "Dawn Song",
        "Rafid Mahmood",
        "Ruoxi Jia"
      ],
      "abstract": "Domain reweighting is an emerging research area aimed at adjusting the\nrelative weights of different data sources to improve the effectiveness and\nefficiency of LLM pre-training. We show that data mixtures that perform well at\nsmaller scales may not retain their advantage at larger scales, challenging the\nexisting practice of determining competitive mixtures in small-scale\nexperiments and directly applying them at much larger scales. To address this,\nwe propose AutoScale, a two-stage, scale-aware data composition framework.\nFirst, AutoScale fits a parametric model that predicts the model's loss under\ndifferent data compositions, then uses it to find an approximate best\nallocation at smaller, more manageable budgets. Next, leveraging a novel\ntheoretical analysis of how optimal compositions evolve with scale, AutoScale\nextrapolates that composition to larger budgets without further retraining.\nEmpirically, AutoScale accelerates convergence and improves downstream\nperformance. For instance, when pre-training GPT-2 Large, it achieves a 28%\nfaster perplexity reduction than baselines and up to a 38% speed-up over\nunweighted training, while yielding best-average results on various downstream\ntasks. Overall, our findings illustrate how domain importance shifts with\ntraining scale, underscoring the need for scale-dependent data curation in LLM\ntraining. Our code is open-sourced.",
      "tldr_zh": "该研究探讨了领域再加权（domain reweighting）在大型语言模型（LLMs）预训练中的应用，发现小规模数据混合在更大规模下可能失效。AutoScale 是一个两阶段的规模感知框架：首先拟合一个参数模型预测模型损失并优化小规模分配，然后通过理论分析外推到更大预算，而无需额外训练。实验结果显示，AutoScale 在预训练 GPT-2 Large 时，比基线模型快 28% 的 perplexity 减少，并比无权重训练快 38%，在各种下游任务上取得最佳平均性能。该框架强调了训练规模对领域重要性的影响，并推动了规模相关的数据整理实践，代码已开源。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "Preprint. Under review",
      "pdf_url": "http://arxiv.org/pdf/2407.20177v4",
      "published_date": "2024-07-29 17:06:30 UTC",
      "updated_date": "2025-04-06 03:22:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:55:22.059205"
    },
    {
      "arxiv_id": "2407.20176v2",
      "title": "Emotion-Driven Melody Harmonization via Melodic Variation and Functional Representation",
      "title_zh": "翻译失败",
      "authors": [
        "Jingyue Huang",
        "Yi-Hsuan Yang"
      ],
      "abstract": "Emotion-driven melody harmonization aims to generate diverse harmonies for a\nsingle melody to convey desired emotions. Previous research found it hard to\nalter the perceived emotional valence of lead sheets only by harmonizing the\nsame melody with different chords, which may be attributed to the constraints\nimposed by the melody itself and the limitation of existing music\nrepresentation. In this paper, we propose a novel functional representation for\nsymbolic music. This new method takes musical keys into account, recognizing\ntheir significant role in shaping music's emotional character through\nmajor-minor tonality. It also allows for melodic variation with respect to keys\nand addresses the problem of data scarcity for better emotion modeling. A\nTransformer is employed to harmonize key-adaptable melodies, allowing for keys\ndetermined in rule-based or model-based manner. Experimental results confirm\nthe effectiveness of our new representation in generating key-aware harmonies,\nwith objective and subjective evaluations affirming the potential of our\napproach to convey specific valence for versatile melody.",
      "tldr_zh": "本研究针对情感驱动旋律和声生成的问题，提出了一种新颖的 functional representation 方法，该方法考虑音乐调性（major-minor tonality）的关键作用，并允许 melodic variation 以适应不同键，从而解决旋律约束和数据稀缺的挑战。研究使用 Transformer 模型对适应键的旋律进行和声生成，键可通过规则-based 或模型-based 方式确定。实验结果显示，该方法在客观和主观评估中均表现出色，能够有效生成键-aware 的和声，从而更好地传达特定情感。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "This work is the initial version of the ISMIR 2024 paper\n  EMO-Disentanger",
      "pdf_url": "http://arxiv.org/pdf/2407.20176v2",
      "published_date": "2024-07-29 17:05:12 UTC",
      "updated_date": "2024-09-25 05:23:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:55:28.107692"
    },
    {
      "arxiv_id": "2407.20174v2",
      "title": "Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning",
      "title_zh": "翻译失败",
      "authors": [
        "Xingchen Zeng",
        "Haichuan Lin",
        "Yilin Ye",
        "Wei Zeng"
      ],
      "abstract": "Emerging multimodal large language models (MLLMs) exhibit great potential for\nchart question answering (CQA). Recent efforts primarily focus on scaling up\ntraining datasets (i.e., charts, data tables, and question-answer (QA) pairs)\nthrough data collection and synthesis. However, our empirical study on existing\nMLLMs and CQA datasets reveals notable gaps. First, current data collection and\nsynthesis focus on data volume and lack consideration of fine-grained visual\nencodings and QA tasks, resulting in unbalanced data distribution divergent\nfrom practical CQA scenarios. Second, existing work follows the training recipe\nof the base MLLMs initially designed for natural images, under-exploring the\nadaptation to unique chart characteristics, such as rich text elements. To fill\nthe gap, we propose a visualization-referenced instruction tuning approach to\nguide the training dataset enhancement and model development. Specifically, we\npropose a novel data engine to effectively filter diverse and high-quality data\nfrom existing datasets and subsequently refine and augment the data using\nLLM-based generation techniques to better align with practical QA tasks and\nvisual encodings. Then, to facilitate the adaptation to chart characteristics,\nwe utilize the enriched data to train an MLLM by unfreezing the vision encoder\nand incorporating a mixture-of-resolution adaptation strategy for enhanced\nfine-grained recognition. Experimental results validate the effectiveness of\nour approach. Even with fewer training examples, our model consistently\noutperforms state-of-the-art CQA models on established benchmarks. We also\ncontribute a dataset split as a benchmark for future research. Source codes and\ndatasets of this paper are available at\nhttps://github.com/zengxingchen/ChartQA-MLLM.",
      "tldr_zh": "该研究针对多模态大语言模型(MLLMs)在图表问题回答(CQA)中的问题，提出了一种 visualization-referenced instruction tuning 方法，以解决现有数据集分布不平衡和模型对图表特性的适应不足。\n具体方法包括开发一个数据引擎，用于过滤高质数据并利用LLM生成技术进行精炼和增强，使其更符合实际QA任务和细粒度视觉编码。\n在模型训练中，解冻视觉编码器并引入 mixture-of-resolution 策略，以提升对图表中丰富文本元素的识别能力。\n实验结果表明，即使使用更少训练样本，该模型在CQA基准测试中显著优于现有模型。\n此外，论文贡献了一个新的数据集分割作为基准，并开源了代码和数据集。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "11 pages, 7 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.20174v2",
      "published_date": "2024-07-29 17:04:34 UTC",
      "updated_date": "2024-08-11 05:30:05 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:55:36.502779"
    },
    {
      "arxiv_id": "2407.20172v1",
      "title": "LatentArtiFusion: An Effective and Efficient Histological Artifacts Restoration Framework",
      "title_zh": "LatentArtiFusion：一种高效且有效的组织学伪影修复框架",
      "authors": [
        "Zhenqi He",
        "Wenrui Liu",
        "Minghao Yin",
        "Kai Han"
      ],
      "abstract": "Histological artifacts pose challenges for both pathologists and\nComputer-Aided Diagnosis (CAD) systems, leading to errors in analysis. Current\napproaches for histological artifact restoration, based on Generative\nAdversarial Networks (GANs) and pixel-level Diffusion Models, suffer from\nperformance limitations and computational inefficiencies. In this paper, we\npropose a novel framework, LatentArtiFusion, which leverages the latent\ndiffusion model (LDM) to reconstruct histological artifacts with high\nperformance and computational efficiency. Unlike traditional pixel-level\ndiffusion frameworks, LatentArtiFusion executes the restoration process in a\nlower-dimensional latent space, significantly improving computational\nefficiency. Moreover, we introduce a novel regional artifact reconstruction\nalgorithm in latent space to prevent mistransfer in non-artifact regions,\ndistinguishing our approach from GAN-based methods. Through extensive\nexperiments on real-world histology datasets, LatentArtiFusion demonstrates\nremarkable speed, outperforming state-of-the-art pixel-level diffusion\nframeworks by more than 30X. It also consistently surpasses GAN-based methods\nby at least 5% across multiple evaluation metrics. Furthermore, we evaluate the\neffectiveness of our proposed framework in downstream tissue classification\ntasks, showcasing its practical utility. Code is available at\nhttps://github.com/bugs-creator/LatentArtiFusion.",
      "tldr_zh": "本论文提出LatentArtiFusion框架，利用潜在扩散模型(LDM)高效重建组织学伪像(histological artifacts)，以解决现有基于GANs和像素级扩散模型的方法在性能和计算效率上的局限性。框架在低维潜在空间(latent space)中执行恢复过程，并引入新型区域伪像重建算法，防止非伪像区域的错误转移。实验结果显示，LatentArtiFusion在真实组织学数据集上比最先进像素级扩散框架快30倍以上，并在多个评估指标上比GANs方法高出至少5%；此外，它在下游组织分类任务中表现出色，证明了其实际应用价值。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accept to DGM4MICCAI2024",
      "pdf_url": "http://arxiv.org/pdf/2407.20172v1",
      "published_date": "2024-07-29 17:00:32 UTC",
      "updated_date": "2024-07-29 17:00:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:55:46.594888"
    },
    {
      "arxiv_id": "2407.20164v1",
      "title": "Language-Conditioned Offline RL for Multi-Robot Navigation",
      "title_zh": "翻译失败",
      "authors": [
        "Steven Morad",
        "Ajay Shankar",
        "Jan Blumenkamp",
        "Amanda Prorok"
      ],
      "abstract": "We present a method for developing navigation policies for multi-robot teams\nthat interpret and follow natural language instructions. We condition these\npolicies on embeddings from pretrained Large Language Models (LLMs), and train\nthem via offline reinforcement learning with as little as 20 minutes of\nrandomly-collected data. Experiments on a team of five real robots show that\nthese policies generalize well to unseen commands, indicating an understanding\nof the LLM latent space. Our method requires no simulators or environment\nmodels, and produces low-latency control policies that can be deployed directly\nto real robots without finetuning. We provide videos of our experiments at\nhttps://sites.google.com/view/llm-marl.",
      "tldr_zh": "本文提出了一种基于语言条件的离线强化学习(Offline RL)方法，用于多机器人导航策略的开发，该策略能解释和遵循自然语言指令。方法利用预训练的大型语言模型(LLMs)的嵌入作为条件，仅需20分钟随机收集的数据即可训练。实验在五个真实机器人团队上验证，显示该策略对未见指令具有良好泛化能力，并理解了LLMs的潜在空间。该方法无需模拟器或环境模型，直接部署到真实机器人，实现低延迟控制。",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20164v1",
      "published_date": "2024-07-29 16:49:30 UTC",
      "updated_date": "2024-07-29 16:49:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:56:08.578011"
    },
    {
      "arxiv_id": "2407.20157v1",
      "title": "rLLM: Relational Table Learning with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Weichen Li",
        "Xiaotong Huang",
        "Jianwu Zheng",
        "Zheng Wang",
        "Chaokun Wang",
        "Li Pan",
        "Jianhua Li"
      ],
      "abstract": "We introduce rLLM (relationLLM), a PyTorch library designed for Relational\nTable Learning (RTL) with Large Language Models (LLMs). The core idea is to\ndecompose state-of-the-art Graph Neural Networks, LLMs, and Table Neural\nNetworks into standardized modules, to enable the fast construction of novel\nRTL-type models in a simple \"combine, align, and co-train\" manner. To\nillustrate the usage of rLLM, we introduce a simple RTL method named\n\\textbf{BRIDGE}. Additionally, we present three novel relational tabular\ndatasets (TML1M, TLF2K, and TACM12K) by enhancing classic datasets. We hope\nrLLM can serve as a useful and easy-to-use development framework for\nRTL-related tasks. Our code is available at:\nhttps://github.com/rllm-project/rllm.",
      "tldr_zh": "本文介绍了 rLLM，这是一个 PyTorch 库，旨在通过 Large Language Models (LLMs) 进行 Relational Table Learning (RTL)。rLLM 的核心方法是将 Graph Neural Networks (GNNs)、LLMs 和 Table Neural Networks 分解成标准化模块，实现“combine, align, and co-train”的快速模型构建方式。论文还提出了一种简单 RTL 方法 BRIDGE，并发布了三个新数据集（TML1M, TLF2K 和 TACM12K），这些数据集基于经典数据集增强而成。该框架为 RTL 相关任务提供了一个易用且高效的开发工具，代码已在 GitHub 上公开。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20157v1",
      "published_date": "2024-07-29 16:33:40 UTC",
      "updated_date": "2024-07-29 16:33:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:56:28.074465"
    },
    {
      "arxiv_id": "2407.20147v1",
      "title": "Quantum Machine Learning Architecture Search via Deep Reinforcement Learning",
      "title_zh": "通过深度强化学习的量子机器学习架构搜索",
      "authors": [
        "Xin Dai",
        "Tzu-Chieh Wei",
        "Shinjae Yoo",
        "Samuel Yen-Chi Chen"
      ],
      "abstract": "The rapid advancement of quantum computing (QC) and machine learning (ML) has\ngiven rise to the burgeoning field of quantum machine learning (QML), aiming to\ncapitalize on the strengths of quantum computing to propel ML forward. Despite\nits promise, crafting effective QML models necessitates profound expertise to\nstrike a delicate balance between model intricacy and feasibility on Noisy\nIntermediate-Scale Quantum (NISQ) devices. While complex models offer robust\nrepresentation capabilities, their extensive circuit depth may impede seamless\nexecution on extant noisy quantum platforms. In this paper, we address this\nquandary of QML model design by employing deep reinforcement learning to\nexplore proficient QML model architectures tailored for designated supervised\nlearning tasks. Specifically, our methodology involves training an RL agent to\ndevise policies that facilitate the discovery of QML models without\npredetermined ansatz. Furthermore, we integrate an adaptive mechanism to\ndynamically adjust the learning objectives, fostering continuous improvement in\nthe agent's learning process. Through extensive numerical simulations, we\nillustrate the efficacy of our approach within the realm of classification\ntasks. Our proposed method successfully identifies VQC architectures capable of\nachieving high classification accuracy while minimizing gate depth. This\npioneering approach not only advances the study of AI-driven quantum circuit\ndesign but also holds significant promise for enhancing performance in the NISQ\nera.",
      "tldr_zh": "这篇论文提出了一种使用深度强化学习（deep reinforcement learning）来搜索量子机器学习（QML）模型架构的方法，旨在平衡模型复杂性和Noisy Intermediate-Scale Quantum (NISQ) 设备上的可执行性。方法包括训练RL代理动态设计QML模型，而不依赖预定义的ansatz，并集成自适应机制来优化学习目标和代理性能。通过数值模拟实验，该方法在分类任务中成功识别出能实现高分类准确率同时最小化门深度的Variational Quantum Circuit (VQC) 架构。这一创新性方法推进了AI驱动的量子电路设计，并为NISQ时代提升QML性能提供了重要潜力。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "quant-ph",
      "comment": "Accepted by IEEE International Conference on Quantum Computing and\n  Engineering - QCE 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.20147v1",
      "published_date": "2024-07-29 16:20:51 UTC",
      "updated_date": "2024-07-29 16:20:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:56:24.953794"
    },
    {
      "arxiv_id": "2407.20143v4",
      "title": "ByteCheckpoint: A Unified Checkpointing System for Large Foundation Model Development",
      "title_zh": "翻译失败",
      "authors": [
        "Borui Wan",
        "Mingji Han",
        "Yiyao Sheng",
        "Yanghua Peng",
        "Haibin Lin",
        "Mofan Zhang",
        "Zhichao Lai",
        "Menghan Yu",
        "Junda Zhang",
        "Zuquan Song",
        "Xin Liu",
        "Chuan Wu"
      ],
      "abstract": "Checkpointing to preserve training states is crucial during the development\nof Large Foundation Models (LFMs), for training resumption upon various\nfailures or changes in GPU resources and parallelism configurations. In\naddition, saved checkpoints are dispatched to evaluation tasks or transferred\nacross different training stages (e.g., from pre-training to post-training).\nAll these scenarios require resharding distributed checkpoints from one\nparallelism to another. In production environments, different LFMs are trained\nwith various frameworks and storage backends, depending on model sizes and\ntraining scales. A high-performance checkpointing system is needed to enable\nefficient checkpoint management at scale throughout the lifecycle of LFM\ndevelopment. We introduce ByteCheckpoint, an industrial-grade checkpointing\nsystem for large-scale LFM training. ByteCheckpoint features: a\nparallelism-agnostic checkpoint representation that enables efficient load-time\ncheckpoint resharding; a generic checkpoint saving/loading workflow to\naccommodate multiple training frameworks and support different storage\nbackends; full-stack optimizations to ensure high I/O efficiency and\nscalability; a suite of monitoring tools to streamline large-scale performance\nanalysis and bottleneck detection. Compared to existing open-source\ncheckpointing systems [52, 58], ByteCheckpoint significantly reduces runtime\ncheckpoint stalls, achieving an average reduction of 54.20x. For saving and\nloading times, ByteCheckpoint achieves improvements of up to 9.96x and 8.80x,\nrespectively.",
      "tldr_zh": "该论文提出ByteCheckpoint，一种统一的检查点系统，用于Large Foundation Models (LFMs)开发过程中的训练状态保存和恢复，支持在GPU资源变化、并行配置resharding以及不同训练阶段间的转移。系统特征包括平行性无关的检查点表示、通用保存/加载工作流以兼容多种训练框架和存储后端、全栈I/O优化以及性能监控工具。实验结果显示，与现有开源系统相比，ByteCheckpoint将运行时检查点停顿平均减少54.20倍，保存时间改善高达9.96倍，加载时间改善高达8.80倍，从而提升大规模LFM训练的效率和可扩展性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20143v4",
      "published_date": "2024-07-29 16:18:20 UTC",
      "updated_date": "2025-04-02 06:05:23 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:56:34.101496"
    },
    {
      "arxiv_id": "2407.20130v1",
      "title": "To accept or not to accept? An IRT-TOE Framework to Understand Educators' Resistance to Generative AI in Higher Education",
      "title_zh": "接受还是不接受？一个 IRT-TOE ",
      "authors": [
        "Jan-Erik Kalmus",
        "Anastasija Nikiforova"
      ],
      "abstract": "Since the public release of Chat Generative Pre-Trained Transformer\n(ChatGPT), extensive discourse has emerged concerning the potential advantages\nand challenges of integrating Generative Artificial Intelligence (GenAI) into\neducation. In the realm of information systems, research on technology adoption\nis crucial for understanding the diverse factors influencing the uptake of\nspecific technologies. Theoretical frameworks, refined and validated over\ndecades, serve as guiding tools to elucidate the individual and organizational\ndynamics, obstacles, and perceptions surrounding technology adoption. However,\nwhile several models have been proposed, they often prioritize elucidating the\nfactors that facilitate acceptance over those that impede it, typically\nfocusing on the student perspective and leaving a gap in empirical evidence\nregarding educators viewpoints. Given the pivotal role educators play in higher\neducation, this study aims to develop a theoretical model to empirically\npredict the barriers preventing educators from adopting GenAI in their\nclassrooms. Acknowledging the lack of theoretical models tailored to\nidentifying such barriers, our approach is grounded in the Innovation\nResistance Theory (IRT) framework and augmented with constructs from the\nTechnology-Organization-Environment (TOE) framework. This model is transformed\ninto a measurement instrument employing a quantitative approach, complemented\nby a qualitative approach to enrich the analysis and uncover concerns related\nto GenAI adoption in the higher education domain.",
      "tldr_zh": "本研究探讨了高等教育中教师对生成式 AI (GenAI) 的抵抗因素，旨在填补现有技术采用模型中忽略阻碍因素和教师视角的空白。\n作者提出一个基于 Innovation Resistance Theory (IRT) 框架，并结合 Technology-Organization-Environment (TOE) 框架的理论模型，用于预测教师在课堂中使用 GenAI 的障碍。\n该模型通过定量和定性方法转化为测量工具，以提供实证证据并揭示 GenAI 在教育领域的潜在挑战。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "cs.ET",
        "cs.HC",
        "cs.IT",
        "math.IT"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20130v1",
      "published_date": "2024-07-29 15:59:19 UTC",
      "updated_date": "2024-07-29 15:59:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:56:46.971454"
    },
    {
      "arxiv_id": "2407.20124v2",
      "title": "AxiomVision: Accuracy-Guaranteed Adaptive Visual Model Selection for Perspective-Aware Video Analytics",
      "title_zh": "AxiomVision: 准确性保证的自适应视觉模型选择，用于视角感知视频分析",
      "authors": [
        "Xiangxiang Dai",
        "Zeyu Zhang",
        "Peng Yang",
        "Yuedong Xu",
        "Xutong Liu",
        "John C. S. Lui"
      ],
      "abstract": "The rapid evolution of multimedia and computer vision technologies requires\nadaptive visual model deployment strategies to effectively handle diverse tasks\nand varying environments. This work introduces AxiomVision, a novel framework\nthat can guarantee accuracy by leveraging edge computing to dynamically select\nthe most efficient visual models for video analytics under diverse scenarios.\nUtilizing a tiered edge-cloud architecture, AxiomVision enables the deployment\nof a broad spectrum of visual models, from lightweight to complex DNNs, that\ncan be tailored to specific scenarios while considering camera source impacts.\nIn addition, AxiomVision provides three core innovations: (1) a dynamic visual\nmodel selection mechanism utilizing continual online learning, (2) an efficient\nonline method that efficiently takes into account the influence of the camera's\nperspective, and (3) a topology-driven grouping approach that accelerates the\nmodel selection process. With rigorous theoretical guarantees, these\nadvancements provide a scalable and effective solution for visual tasks\ninherent to multimedia systems, such as object detection, classification, and\ncounting. Empirically, AxiomVision achieves a 25.7\\% improvement in accuracy.",
      "tldr_zh": "本研究提出 AxiomVision，一种基于边缘计算的自适应视觉模型选择框架，能够在考虑相机视角影响的情况下，动态选择最有效的模型（如从轻量级到复杂 DNNs）以保证视频分析的准确性。框架的核心创新包括：（1）利用持续在线学习的动态模型选择机制，（2）高效处理相机视角影响的在线方法，以及（3）拓扑驱动的分组方法来加速选择过程。实验结果显示，AxiomVision 在对象检测、分类和计数等视觉任务上，准确性提高了25.7%，并提供严格的理论保证，为多媒体系统提供可扩展的解决方案。",
      "categories": [
        "cs.MM",
        "cs.AI"
      ],
      "primary_category": "cs.MM",
      "comment": "Accepted by ACM MM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.20124v2",
      "published_date": "2024-07-29 15:54:43 UTC",
      "updated_date": "2024-07-30 07:40:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:56:59.130869"
    },
    {
      "arxiv_id": "2407.20121v1",
      "title": "EXIT: An EXplicit Interest Transfer Framework for Cross-Domain Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Lei Huang",
        "Weitao Li",
        "Chenrui Zhang",
        "Jinpeng Wang",
        "Xianchun Yi",
        "Sheng Chen"
      ],
      "abstract": "Cross-domain recommendation has attracted substantial interest in industrial\napps such as Meituan, which serves multiple business domains via knowledge\ntransfer and meets the diverse interests of users. However, existing methods\ntypically follow an implicit modeling paradigm that blends the knowledge from\nboth the source and target domains, and design intricate network structures to\nshare learned embeddings or patterns between domains to improve recommendation\naccuracy. Since the transfer of interest signals is unsupervised, these\nimplicit paradigms often struggle with the negative transfer resulting from\ndifferences in service functions and presentation forms across different\ndomains. In this paper, we propose a simple and effective EXplicit Interest\nTransfer framework named EXIT to address the stated challenge. Specifically, we\npropose a novel label combination approach that enables the model to directly\nlearn beneficial source domain interests through supervised learning, while\nexcluding inappropriate interest signals. Moreover, we introduce a scene\nselector network to model the interest transfer intensity under fine-grained\nscenes. Offline experiments conducted on the industrial production dataset and\nonline A/B tests validate the superiority and effectiveness of our proposed\nframework. Without complex network structures or training processes, EXIT can\nbe easily deployed in the industrial recommendation system. EXIT has been\nsuccessfully deployed in the online homepage recommendation system of Meituan\nApp, serving the main traffic.",
      "tldr_zh": "这篇论文提出 EXIT（EXplicit Interest Transfer）框架，用于解决跨域推荐（Cross-Domain Recommendation）中的负转移问题，该框架通过显式建模来避免隐式方法混合源域和目标域知识带来的挑战。关键创新包括一个新型标签组合方法，支持监督学习直接获取有益的源域兴趣信号，同时排除不合适的兴趣；以及一个场景选择器网络，用于在细粒度场景下动态调整兴趣转移强度。实验在工业数据集上进行离线测试和在线 A/B 测试，结果显示 EXIT 显著提升推荐准确率，并已成功部署于 Meituan App 的在线推荐系统中，提供简单有效的工业应用方案。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "Accepted at CIKM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.20121v1",
      "published_date": "2024-07-29 15:52:09 UTC",
      "updated_date": "2024-07-29 15:52:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:57:11.245305"
    },
    {
      "arxiv_id": "2407.20119v2",
      "title": "Adaptive Self-supervised Robust Clustering for Unstructured Data with Unknown Cluster Number",
      "title_zh": "翻译失败",
      "authors": [
        "Chen-Lu Ding",
        "Jiancan Wu",
        "Wei Lin",
        "Shiyang Shen",
        "Xiang Wang",
        "Yancheng Yuan"
      ],
      "abstract": "We introduce a novel self-supervised deep clustering approach tailored for\nunstructured data without requiring prior knowledge of the number of clusters,\ntermed Adaptive Self-supervised Robust Clustering (ASRC). In particular, ASRC\nadaptively learns the graph structure and edge weights to capture both local\nand global structural information. The obtained graph enables us to learn\nclustering-friendly feature representations by an enhanced graph auto-encoder\nwith contrastive learning technique. It further leverages the clustering\nresults adaptively obtained by robust continuous clustering (RCC) to generate\nprototypes for negative sampling, which can further contribute to promoting\nconsistency among positive pairs and enlarging the gap between positive and\nnegative samples. ASRC obtains the final clustering results by applying RCC to\nthe learned feature representations with their consistent graph structure and\nedge weights. Extensive experiments conducted on seven benchmark datasets\ndemonstrate the efficacy of ASRC, demonstrating its superior performance over\nother popular clustering models. Notably, ASRC even outperforms methods that\nrely on prior knowledge of the number of clusters, highlighting its\neffectiveness in addressing the challenges of clustering unstructured data.",
      "tldr_zh": "我们提出了一种名为 Adaptive Self-supervised Robust Clustering (ASRC) 的新型自监督深度聚类方法，针对未知聚类数量的无结构数据，能够自适应学习图结构和边权重以捕捉局部和全局信息。ASRC 通过增强的图 auto-encoder 结合 contrastive learning 技术学习聚类友好的特征表示，并利用 robust continuous clustering (RCC) 生成原型来提升正样本一致性和正负样本差距。实验在七个基准数据集上表明，ASRC 优于其他流行聚类模型，甚至超越依赖聚类数量先验知识的方法，证明了其在处理无结构数据聚类挑战中的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20119v2",
      "published_date": "2024-07-29 15:51:09 UTC",
      "updated_date": "2024-07-30 06:33:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:57:23.073097"
    },
    {
      "arxiv_id": "2408.00806v1",
      "title": "HOAA: Hybrid Overestimating Approximate Adder for Enhanced Performance Processing Engine",
      "title_zh": "HOAA：用于增强性能处理引擎的混合过度估计近似加法器",
      "authors": [
        "Omkar Kokane",
        "Prabhat Sati",
        "Mukul Lokhande",
        "Santosh Kumar Vishvakarma"
      ],
      "abstract": "This paper presents the Hybrid Overestimating Approximate Adder designed to\nenhance the performance in processing engines, specifically focused on edge AI\napplications. A novel Plus One Adder design is proposed as an incremental adder\nin the RCA chain, incorporating a Full Adder with an excess 1 alongside inputs\nA, B, and Cin. The design approximates outputs to 2 bit values to reduce\nhardware complexity and improve resource efficiency. The Plus One Adder is\nintegrated into a dynamically reconfigurable HOAA, allowing runtime\ninterchangeability between accurate and approximate overestimation modes. The\nproposed design is demonstrated for multiple applications, such as Twos\ncomplement subtraction and Rounding to even, and the Configurable Activation\nfunction, which are critical components of the Processing engine. Our approach\nshows 21 percent improvement in area efficiency and 33 percent reduction in\npower consumption, compared to state of the art designs with minimal accuracy\nloss. Thus, the proposed HOAA could be a promising solution for\nresource-constrained environments, offering ideal trade-offs between hardware\nefficiency vs computational accuracy.",
      "tldr_zh": "这篇论文提出了 HOAA（Hybrid Overestimating Approximate Adder），一种混合过估近似加法器，旨在提升处理引擎的性能，尤其适用于边缘 AI 应用。论文引入了新型 Plus One Adder 作为 RCA 链中的增量加法器，通过将输出近似为 2 位值并实现动态可重构设计，允许运行时切换准确模式和近似过估模式，从而减少硬件复杂性和资源消耗。实验结果显示，与现有设计相比，HOAA 改善了 21% 的面积效率和 33% 的功耗，同时在 Twos complement subtraction、Rounding to even 和 Configurable Activation function 等应用中保持最小准确性损失，为资源受限环境提供了高效的硬件效率与计算准确性权衡。",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.00806v1",
      "published_date": "2024-07-29 15:47:51 UTC",
      "updated_date": "2024-07-29 15:47:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:57:37.240467"
    },
    {
      "arxiv_id": "2407.20114v1",
      "title": "FiCo-ITR: bridging fine-grained and coarse-grained image-text retrieval for comparative performance analysis",
      "title_zh": "翻译失败",
      "authors": [
        "Mikel Williams-Lekuona",
        "Georgina Cosma"
      ],
      "abstract": "In the field of Image-Text Retrieval (ITR), recent advancements have\nleveraged large-scale Vision-Language Pretraining (VLP) for Fine-Grained (FG)\ninstance-level retrieval, achieving high accuracy at the cost of increased\ncomputational complexity. For Coarse-Grained (CG) category-level retrieval,\nprominent approaches employ Cross-Modal Hashing (CMH) to prioritise efficiency,\nalbeit at the cost of retrieval performance. Due to differences in\nmethodologies, FG and CG models are rarely compared directly within evaluations\nin the literature, resulting in a lack of empirical data quantifying the\nretrieval performance-efficiency tradeoffs between the two. This paper\naddresses this gap by introducing the \\texttt{FiCo-ITR} library, which\nstandardises evaluation methodologies for both FG and CG models, facilitating\ndirect comparisons. We conduct empirical evaluations of representative models\nfrom both subfields, analysing precision, recall, and computational complexity\nacross varying data scales. Our findings offer new insights into the\nperformance-efficiency trade-offs between recent representative FG and CG\nmodels, highlighting their respective strengths and limitations. These findings\nprovide the foundation necessary to make more informed decisions regarding\nmodel selection for specific retrieval tasks and highlight avenues for future\nresearch into hybrid systems that leverage the strengths of both FG and CG\napproaches.",
      "tldr_zh": "这篇论文针对图像文本检索(ITR)领域中Fine-Grained (FG)实例级和Coarse-Grained (CG)类别级方法的差异，引入了FiCo-ITR库，以标准化评估流程并实现直接比较。\nFiCo-ITR通过实证评估代表性模型，分析了精度、召回率和计算复杂性在不同数据规模下的表现，揭示了FG模型的高准确性与高计算开销，以及CG模型的效率优势。\n研究发现这些权衡提供了新见解，帮助用户在特定任务中更明智地选择模型，并为开发结合FG和CG优点的混合系统指明未来方向。",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.IR",
      "comment": "19 pages, submitted to International Journal of Multimedia\n  Information Retrieval",
      "pdf_url": "http://arxiv.org/pdf/2407.20114v1",
      "published_date": "2024-07-29 15:44:22 UTC",
      "updated_date": "2024-07-29 15:44:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:57:57.595126"
    },
    {
      "arxiv_id": "2407.20109v2",
      "title": "Diffusion-DICE: In-Sample Diffusion Guidance for Offline Reinforcement Learning",
      "title_zh": "Diffusion-DICE：针对离线强化学习的样本内扩散指导",
      "authors": [
        "Liyuan Mao",
        "Haoran Xu",
        "Xianyuan Zhan",
        "Weinan Zhang",
        "Amy Zhang"
      ],
      "abstract": "One important property of DIstribution Correction Estimation (DICE) methods\nis that the solution is the optimal stationary distribution ratio between the\noptimized and data collection policy. In this work, we show that DICE-based\nmethods can be viewed as a transformation from the behavior distribution to the\noptimal policy distribution. Based on this, we propose a novel approach,\nDiffusion-DICE, that directly performs this transformation using diffusion\nmodels. We find that the optimal policy's score function can be decomposed into\ntwo terms: the behavior policy's score function and the gradient of a guidance\nterm which depends on the optimal distribution ratio. The first term can be\nobtained from a diffusion model trained on the dataset and we propose an\nin-sample learning objective to learn the second term. Due to the\nmulti-modality contained in the optimal policy distribution, the transformation\nin Diffusion-DICE may guide towards those local-optimal modes. We thus generate\na few candidate actions and carefully select from them to approach\nglobal-optimum. Different from all other diffusion-based offline RL methods,\nthe guide-then-select paradigm in Diffusion-DICE only uses in-sample actions\nfor training and brings minimal error exploitation in the value function. We\nuse a didatic toycase example to show how previous diffusion-based methods fail\nto generate optimal actions due to leveraging these errors and how\nDiffusion-DICE successfully avoids that. We then conduct extensive experiments\non benchmark datasets to show the strong performance of Diffusion-DICE. Project\npage at https://ryanxhr.github.io/Diffusion-DICE/.",
      "tldr_zh": "该论文提出了一种新方法Diffusion-DICE，用于离线强化学习（offline reinforcement learning），它将DIstribution Correction Estimation (DICE)方法视为从行为分布到最优策略分布的转换，并利用扩散模型（diffusion models）直接执行这一转换。方法通过分解最优策略的分数函数，包括行为策略的分数函数和一个依赖于最优分布比率的指导项梯度，并采用in-sample学习目标来训练后者，以生成候选动作并从中选择全局最优。实验结果显示，Diffusion-DICE在基准数据集上表现出色，避免了传统扩散-based方法的错误利用问题，如价值函数中的错误放大，从而提升了整体性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "NeurIPS 2024, first two authors contribute equally",
      "pdf_url": "http://arxiv.org/pdf/2407.20109v2",
      "published_date": "2024-07-29 15:36:42 UTC",
      "updated_date": "2024-10-31 06:16:24 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:58:09.905895"
    },
    {
      "arxiv_id": "2407.20108v1",
      "title": "Classification, Regression and Segmentation directly from k-Space in Cardiac MRI",
      "title_zh": "直接从 k-空间在心脏 MRI 中的分类、回归和分割",
      "authors": [
        "Ruochen Li",
        "Jiazhen Pan",
        "Youxiang Zhu",
        "Juncheng Ni",
        "Daniel Rueckert"
      ],
      "abstract": "Cardiac Magnetic Resonance Imaging (CMR) is the gold standard for diagnosing\ncardiovascular diseases. Clinical diagnoses predominantly rely on\nmagnitude-only Digital Imaging and Communications in Medicine (DICOM) images,\nomitting crucial phase information that might provide additional diagnostic\nbenefits. In contrast, k-space is complex-valued and encompasses both magnitude\nand phase information, while humans cannot directly perceive. In this work, we\npropose KMAE, a Transformer-based model specifically designed to process\nk-space data directly, eliminating conventional intermediary conversion steps\nto the image domain. KMAE can handle critical cardiac disease classification,\nrelevant phenotype regression, and cardiac morphology segmentation tasks. We\nutilize this model to investigate the potential of k-space-based diagnosis in\ncardiac MRI. Notably, this model achieves competitive classification and\nregression performance compared to image-domain methods e.g. Masked\nAutoencoders (MAEs) and delivers satisfactory segmentation performance with a\nmyocardium dice score of 0.884. Last but not least, our model exhibits robust\nperformance with consistent results even when the k-space is 8* undersampled.\nWe encourage the MR community to explore the untapped potential of k-space and\npursue end-to-end, automated diagnosis with reduced human intervention.",
      "tldr_zh": "本研究提出 KMAE，一种基于 Transformer 的模型，直接从 k-Space 处理 Cardiac MRI 数据，绕过传统图像域转换步骤，以利用 magnitude 和 phase 信息进行心脏疾病分类、相关表型回归和心脏形态分割任务。该模型在分类和回归性能上与图像域方法（如 Masked Autoencoders）相当，并在分割任务中达到 myocardium dice score 为 0.884 的满意水平；即使 k-Space 被 8 倍 undersampled，模型仍保持 robust 性能。该工作鼓励 MRI 社区探索 k-Space 的潜力，实现端到端的自动化诊断，减少人为干预。",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20108v1",
      "published_date": "2024-07-29 15:35:35 UTC",
      "updated_date": "2024-07-29 15:35:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:58:10.488434"
    },
    {
      "arxiv_id": "2407.20100v3",
      "title": "F-KANs: Federated Kolmogorov-Arnold Networks",
      "title_zh": "F-KANs：",
      "authors": [
        "Engin Zeydan",
        "Cristian J. Vaca-Rubio",
        "Luis Blanco",
        "Roberto Pereira",
        "Marius Caus",
        "Abdullah Aydeger"
      ],
      "abstract": "In this paper, we present an innovative federated learning (FL) approach that\nutilizes Kolmogorov-Arnold Networks (KANs) for classification tasks. By\nutilizing the adaptive activation capabilities of KANs in a federated\nframework, we aim to improve classification capabilities while preserving\nprivacy. The study evaluates the performance of federated KANs (F- KANs)\ncompared to traditional Multi-Layer Perceptrons (MLPs) on classification task.\nThe results show that the F-KANs model significantly outperforms the federated\nMLP model in terms of accuracy, precision, recall, F1 score and stability, and\nachieves better performance, paving the way for more efficient and\nprivacy-preserving predictive analytics.",
      "tldr_zh": "本论文提出了一种创新的联邦学习（Federated Learning, FL）方法，名为 Federated Kolmogorov-Arnold Networks (F-KANs)，利用 KANs 的自适应激活功能来提升分类任务的性能，同时确保数据隐私保护。与传统的 Multi-Layer Perceptrons (MLPs) 相比，实验结果显示 F-KANs 在准确率、精确率、召回率、F1 分数和稳定性方面显著优于联邦 MLP 模型。这为更高效和隐私保护的预测分析铺平了道路。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.NI"
      ],
      "primary_category": "cs.LG",
      "comment": "This work has been accepted to 1st International Workshop on\n  Distributed AI for Enhanced Wireless Networks (DAINET'25) in conjunction with\n  IEEE Consumer Communications & Networking Conference 2025. Related Code:\n  https://github.com/ezeydan/F-KANs.git",
      "pdf_url": "http://arxiv.org/pdf/2407.20100v3",
      "published_date": "2024-07-29 15:28:26 UTC",
      "updated_date": "2024-11-08 19:02:09 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:58:23.199713"
    },
    {
      "arxiv_id": "2407.20068v1",
      "title": "Unleash the Power of Ellipsis: Accuracy-enhanced Sparse Vector Technique with Exponential Noise",
      "title_zh": "翻译失败",
      "authors": [
        "Yuhan Liu",
        "Sheng Wang",
        "Yixuan Liu",
        "Feifei Li",
        "Hong Chen"
      ],
      "abstract": "The Sparse Vector Technique (SVT) is one of the most fundamental tools in\ndifferential privacy (DP). It works as a backbone for adaptive data analysis by\nanswering a sequence of queries on a given dataset, and gleaning useful\ninformation in a privacy-preserving manner. Unlike the typical private query\nreleases that directly publicize the noisy query results, SVT is less\ninformative -- it keeps the noisy query results to itself and only reveals a\nbinary bit for each query, indicating whether the query result surpasses a\npredefined threshold. To provide a rigorous DP guarantee for SVT, prior works\nin the literature adopt a conservative privacy analysis by assuming the direct\ndisclosure of noisy query results as in typical private query releases. This\napproach, however, hinders SVT from achieving higher query accuracy due to an\noverestimation of the privacy risks, which further leads to an excessive noise\ninjection using the Laplacian or Gaussian noise for perturbation. Motivated by\nthis, we provide a new privacy analysis for SVT by considering its less\ninformative nature. Our analysis results not only broaden the range of\napplicable noise types for perturbation in SVT, but also identify the\nexponential noise as optimal among all evaluated noises (which, however, is\nusually deemed non-applicable in prior works). The main challenge in applying\nexponential noise to SVT is mitigating the sub-optimal performance due to the\nbias introduced by noise distributions. To address this, we develop a\nutility-oriented optimal threshold correction method and an appending strategy,\nwhich enhances the performance of SVT by increasing the precision and recall,\nrespectively. The effectiveness of our proposed methods is substantiated both\ntheoretically and empirically, demonstrating significant improvements up to\n$50\\%$ across evaluated metrics.",
      "tldr_zh": "该论文针对差分隐私 (DP) 中的 Sparse Vector Technique (SVT) 提出了一种精度增强方法，通过新的隐私分析来解决现有方法过度估计隐私风险导致的噪声注入过量问题。该分析扩展了 SVT 的噪声类型，支持使用 exponential noise 作为最优选择，以减少信息泄露并提高查询准确性。为缓解 exponential noise 带来的偏差，作者开发了实用性导向的阈值校正方法和附加策略，分别提升了精度和召回率。实验结果显示，该方法在评估指标上实现了高达 50% 的显著改进。",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20068v1",
      "published_date": "2024-07-29 14:54:28 UTC",
      "updated_date": "2024-07-29 14:54:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:58:36.173649"
    },
    {
      "arxiv_id": "2407.20067v2",
      "title": "xAI-Drop: Don't Use What You Cannot Explain",
      "title_zh": "xAI-Drop：不要使用你无法解释的事物",
      "authors": [
        "Vincenzo Marco De Luca",
        "Antonio Longa",
        "Andrea Passerini",
        "Pietro Liò"
      ],
      "abstract": "Graph Neural Networks (GNNs) have emerged as the predominant paradigm for\nlearning from graph-structured data, offering a wide range of applications from\nsocial network analysis to bioinformatics. Despite their versatility, GNNs face\nchallenges such as lack of generalization and poor interpretability, which\nhinder their wider adoption and reliability in critical applications. Dropping\nhas emerged as an effective paradigm for improving the generalization\ncapabilities of GNNs. However, existing approaches often rely on random or\nheuristic-based selection criteria, lacking a principled method to identify and\nexclude nodes that contribute to noise and over-complexity in the model. In\nthis work, we argue that explainability should be a key indicator of a model's\nquality throughout its training phase. To this end, we introduce xAI-Drop, a\nnovel topological-level dropping regularizer that leverages explainability to\npinpoint noisy network elements to be excluded from the GNN propagation\nmechanism. An empirical evaluation on diverse real-world datasets demonstrates\nthat our method outperforms current state-of-the-art dropping approaches in\naccuracy, and improves explanation quality.",
      "tldr_zh": "本研究针对 Graph Neural Networks (GNNs) 在处理图结构数据时存在的泛化能力和可解释性问题，提出了一种新型 dropping 正则化方法 xAI-Drop。该方法利用 explainability 作为关键指标，识别并排除在 GNN 传播机制中造成噪声的网络元素，从而提升模型的泛化和可靠性。实验结果显示，在多种真实数据集上，xAI-Drop 在准确性和解释质量方面均优于现有最先进的方法。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20067v2",
      "published_date": "2024-07-29 14:53:45 UTC",
      "updated_date": "2024-11-08 17:49:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:58:48.386688"
    },
    {
      "arxiv_id": "2407.20062v1",
      "title": "SalNAS: Efficient Saliency-prediction Neural Architecture Search with self-knowledge distillation",
      "title_zh": "SalNAS：高效的显著性预测神经架构搜索，使用自知识蒸馏",
      "authors": [
        "Chakkrit Termritthikun",
        "Ayaz Umer",
        "Suwichaya Suwanwimolkul",
        "Feng Xia",
        "Ivan Lee"
      ],
      "abstract": "Recent advancements in deep convolutional neural networks have significantly\nimproved the performance of saliency prediction. However, the manual\nconfiguration of the neural network architectures requires domain knowledge\nexpertise and can still be time-consuming and error-prone. To solve this, we\npropose a new Neural Architecture Search (NAS) framework for saliency\nprediction with two contributions. Firstly, a supernet for saliency prediction\nis built with a weight-sharing network containing all candidate architectures,\nby integrating a dynamic convolution into the encoder-decoder in the supernet,\ntermed SalNAS. Secondly, despite the fact that SalNAS is highly efficient\n(20.98 million parameters), it can suffer from the lack of generalization. To\nsolve this, we propose a self-knowledge distillation approach, termed Self-KD,\nthat trains the student SalNAS with the weighted average information between\nthe ground truth and the prediction from the teacher model. The teacher model,\nwhile sharing the same architecture, contains the best-performing weights\nchosen by cross-validation. Self-KD can generalize well without the need to\ncompute the gradient in the teacher model, enabling an efficient training\nsystem. By utilizing Self-KD, SalNAS outperforms other state-of-the-art\nsaliency prediction models in most evaluation rubrics across seven benchmark\ndatasets while being a lightweight model. The code will be available at\nhttps://github.com/chakkritte/SalNAS",
      "tldr_zh": "该研究提出了一种高效的显著性预测神经架构搜索框架 SalNAS，以解决手动设计网络架构的复杂性问题。SalNAS 通过构建一个包含所有候选架构的超网，并集成动态卷积到编码器-解码器中，实现高效搜索（仅 20.98 百万参数）。此外，引入自知识蒸馏（Self-KD）方法，使用 ground truth 与教师模型预测的加权平均信息训练学生模型，从而提升泛化能力，而无需计算教师模型的梯度。实验结果显示，SalNAS 在七个基准数据集上，在大多数评估指标中优于现有最先进模型，同时保持轻量级。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Published in Engineering Applications of Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2407.20062v1",
      "published_date": "2024-07-29 14:48:34 UTC",
      "updated_date": "2024-07-29 14:48:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:59:01.731987"
    },
    {
      "arxiv_id": "2407.20060v1",
      "title": "RelBench: A Benchmark for Deep Learning on Relational Databases",
      "title_zh": "RelBench：关系数据库深度学习的基准",
      "authors": [
        "Joshua Robinson",
        "Rishabh Ranjan",
        "Weihua Hu",
        "Kexin Huang",
        "Jiaqi Han",
        "Alejandro Dobles",
        "Matthias Fey",
        "Jan E. Lenssen",
        "Yiwen Yuan",
        "Zecheng Zhang",
        "Xinwei He",
        "Jure Leskovec"
      ],
      "abstract": "We present RelBench, a public benchmark for solving predictive tasks over\nrelational databases with graph neural networks. RelBench provides databases\nand tasks spanning diverse domains and scales, and is intended to be a\nfoundational infrastructure for future research. We use RelBench to conduct the\nfirst comprehensive study of Relational Deep Learning (RDL) (Fey et al., 2024),\nwhich combines graph neural network predictive models with (deep) tabular\nmodels that extract initial entity-level representations from raw tables.\nEnd-to-end learned RDL models fully exploit the predictive signal encoded in\nprimary-foreign key links, marking a significant shift away from the dominant\nparadigm of manual feature engineering combined with tabular models. To\nthoroughly evaluate RDL against this prior gold-standard, we conduct an\nin-depth user study where an experienced data scientist manually engineers\nfeatures for each task. In this study, RDL learns better models whilst reducing\nhuman work needed by more than an order of magnitude. This demonstrates the\npower of deep learning for solving predictive tasks over relational databases,\nopening up many new research opportunities enabled by RelBench.",
      "tldr_zh": "本文介绍了 RelBench，这是一个公共基准，用于在关系数据库上应用图神经网络（Graph Neural Networks）解决预测任务，提供多样领域和规模的数据库及任务作为未来研究的基礎基础设施。研究团队进行了 Relational Deep Learning (RDL) 的首次全面研究，该方法结合图神经网络和深度表格模型，从原始表格中提取实体级表示，并端到端利用主-外键链接的预测信号，取代了传统的手动特征工程。相比手动特征工程，用户研究显示 RDL 能学习更优模型，同时将人类工作量减少一个数量级以上。这标志着深度学习在关系数据库预测任务中的潜力，并通过 RelBench 开启了许多新研究机会。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20060v1",
      "published_date": "2024-07-29 14:46:13 UTC",
      "updated_date": "2024-07-29 14:46:13 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:59:13.680473"
    },
    {
      "arxiv_id": "2407.20058v2",
      "title": "Shapley Value Computation in Ontology-Mediated Query Answering",
      "title_zh": "翻译失败",
      "authors": [
        "Meghyn Bienvenu",
        "Diego Figueira",
        "Pierre Lafourcade"
      ],
      "abstract": "The Shapley value, originally introduced in cooperative game theory for\nwealth distribution, has found use in KR and databases for the purpose of\nassigning scores to formulas and database tuples based upon their contribution\nto obtaining a query result or inconsistency. In the present paper, we explore\nthe use of Shapley values in ontology-mediated query answering (OMQA) and\npresent a detailed complexity analysis of Shapley value computation (SVC) in\nthe OMQA setting. In particular, we establish a PF/#P-hard dichotomy for SVC\nfor ontology-mediated queries (T,q) composed of an ontology T formulated in the\ndescription logic ELHI_\\bot and a connected constant-free homomorphism-closed\nquery q. We further show that the #P-hardness side of the dichotomy can be\nstrengthened to cover possibly disconnected queries with constants. Our results\nexploit recently discovered connections between SVC and probabilistic query\nevaluation and allow us to generalize existing results on probabilistic OMQA.",
      "tldr_zh": "本论文探讨了Shapley value在ontology-mediated query answering (OMQA)中的应用，旨在通过计算Shapley value来评估公式和数据库元组对查询结果的贡献。研究者进行了详细的复杂性分析，建立了一个针对description logic ELHI_\\bot中的本体T和连接的constant-free homomorphism-closed query q的PF/#P-hard dichotomy。结果表明，Shapley value computation (SVC)在可能断开的查询中也可能为#P-hard，并通过与probabilistic query evaluation的联系，推广了现有的概率OMQA结果。",
      "categories": [
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.AI",
      "comment": "Long version of KR 2024 homonymous paper",
      "pdf_url": "http://arxiv.org/pdf/2407.20058v2",
      "published_date": "2024-07-29 14:45:14 UTC",
      "updated_date": "2024-11-25 10:04:55 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:59:28.079989"
    },
    {
      "arxiv_id": "2408.05110v1",
      "title": "Application of Unsupervised Artificial Neural Network (ANN) Self_Organizing Map (SOM) in Identifying Main Car Sales Factors",
      "title_zh": "翻译失败",
      "authors": [
        "Mazyar Taghavi"
      ],
      "abstract": "Factors which attract customers and persuade them to buy new car are various\nregarding different consumer tastes. There are some methods to extract pattern\nform mass data. In this case we firstly asked passenger car marketing experts\nto rank more important factors which affect customer decision making behavior\nusing fuzzy Delphi technique, then we provided a sample set from questionnaires\nand tried to apply a useful artificial neural network method called\nself_organizing map SOM to find out which factors have more effect on Iranian\ncustomer's buying decision making. Fuzzy tools were applied to adjust the study\nto be more real. MATLAB software was used for developing and training network.\nResults report four factors are more important rather than the others. Results\nare rather different from marketing expert rankings. Such results would help\nmanufacturers to focus on more important factors and increase company sales\nlevel.",
      "tldr_zh": "本研究旨在识别影响伊朗客户汽车购买决策的主要因素，使用模糊Delphi技术让营销专家对关键因素进行排名，并基于问卷样本数据应用无监督Artificial Neural Network (ANN) Self-Organizing Map (SOM)方法进行分析。研究采用模糊工具增强真实性，并通过MATLAB软件开发和训练网络。结果显示四个因素比其他因素更重要，且与专家排名存在差异，这有助于汽车制造商优先关注这些因素以提升销售水平。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NE"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.05110v1",
      "published_date": "2024-07-29 14:24:16 UTC",
      "updated_date": "2024-07-29 14:24:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:59:36.003613"
    },
    {
      "arxiv_id": "2407.20021v4",
      "title": "MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with Encouraging Inter-Head Attention Similarity",
      "title_zh": "翻译失败",
      "authors": [
        "Kanghyun Choi",
        "Hye Yoon Lee",
        "Dain Kwon",
        "SunJong Park",
        "Kyuyeun Kim",
        "Noseong Park",
        "Jonghyun Choi",
        "Jinho Lee"
      ],
      "abstract": "Data-free quantization (DFQ) is a technique that creates a lightweight\nnetwork from its full-precision counterpart without the original training data,\noften through a synthetic dataset. Although several DFQ methods have been\nproposed for vision transformer (ViT) architectures, they fail to achieve\nefficacy in low-bit settings. Examining the existing methods, we observe that\ntheir synthetic data produce misaligned attention maps, while those of the real\nsamples are highly aligned. From this observation, we find that aligning\nattention maps of synthetic data helps improve the overall performance of\nquantized ViTs. Motivated by this finding, we devise MimiQ, a novel DFQ method\ndesigned for ViTs that enhances inter-head attention similarity. First, we\ngenerate synthetic data by aligning head-wise attention outputs from each\nspatial query patch. Then, we align the attention maps of the quantized network\nto those of the full-precision teacher by applying head-wise structural\nattention distillation. The experimental results show that the proposed method\nsignificantly outperforms baselines, setting a new state-of-the-art for\nViT-DFQ. This paper is an extended version of our work published in the\nproceedings of AAAI 2025, including additional supplementary material.",
      "tldr_zh": "本文提出 MimiQ，一种不依赖原始训练数据的量化方法（Data-Free Quantization, DFQ），针对视觉变压器（ViTs）在低位量化设置下的效能问题，通过增强头间注意力相似性来改善合成数据的注意力图对齐。MimiQ 的关键步骤包括：生成合成数据时对齐每个空间查询补丁的头级注意力输出，以及通过头级结构注意力蒸馏（structural attention distillation）使量化网络的注意力图与全精度教师模型对齐。实验结果表明，MimiQ 显著优于基线方法，在 ViT-DFQ 领域设立了新状态水平。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Published to AAAI 2025",
      "pdf_url": "http://arxiv.org/pdf/2407.20021v4",
      "published_date": "2024-07-29 13:57:40 UTC",
      "updated_date": "2025-04-14 07:42:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T10:59:50.052059"
    },
    {
      "arxiv_id": "2407.19998v1",
      "title": "Do LLMs Really Adapt to Domains? An Ontology Learning Perspective",
      "title_zh": "大型语言模型真的适应领域吗？从本体学习视角",
      "authors": [
        "Huu Tan Mai",
        "Cuong Xuan Chu",
        "Heiko Paulheim"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated unprecedented prowess across\nvarious natural language processing tasks in various application domains.\nRecent studies show that LLMs can be leveraged to perform lexical semantic\ntasks, such as Knowledge Base Completion (KBC) or Ontology Learning (OL).\nHowever, it has not effectively been verified whether their success is due to\ntheir ability to reason over unstructured or semi-structured data, or their\neffective learning of linguistic patterns and senses alone. This unresolved\nquestion is particularly crucial when dealing with domain-specific data, where\nthe lexical senses and their meaning can completely differ from what a LLM has\nlearned during its training stage. This paper investigates the following\nquestion: Do LLMs really adapt to domains and remain consistent in the\nextraction of structured knowledge, or do they only learn lexical senses\ninstead of reasoning? To answer this question and, we devise a controlled\nexperiment setup that uses WordNet to synthesize parallel corpora, with English\nand gibberish terms. We examine the differences in the outputs of LLMs for each\ncorpus in two OL tasks: relation extraction and taxonomy discovery. Empirical\nresults show that, while adapting to the gibberish corpora, off-the-shelf LLMs\ndo not consistently reason over semantic relationships between concepts, and\ninstead leverage senses and their frame. However, fine-tuning improves the\nperformance of LLMs on lexical semantic tasks even when the domain-specific\nterms are arbitrary and unseen during pre-training, hinting at the\napplicability of pre-trained LLMs for OL.",
      "tldr_zh": "本论文探讨了大语言模型 (LLMs) 是否真正适应领域，从本体学习 (Ontology Learning, OL) 的角度进行分析，质疑 LLMs 的成功是否源于语义推理能力，还是仅依赖语言模式。研究设计了控制实验，使用 WordNet 合成英语和无意义词汇 (gibberish terms) 的平行语料库，测试 LLMs 在关系提取和分类学发现 (taxonomy discovery) 任务中的输出差异。结果显示，现成的 LLMs 倾向于利用词汇意义而非一致地推理语义关系，但通过微调，LLMs 能显著改善性能，即使面对训练中未见的领域特定术语，从而支持预训练 LLMs 在 OL 任务中的适用性。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at ISWC 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.19998v1",
      "published_date": "2024-07-29 13:29:43 UTC",
      "updated_date": "2024-07-29 13:29:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:00:02.631071"
    },
    {
      "arxiv_id": "2407.19996v1",
      "title": "Reproducibility Study of \"ITI-GEN: Inclusive Text-to-Image Generation\"",
      "title_zh": "翻译失败",
      "authors": [
        "Daniel Gallo Fernández",
        "Răzvan-Andrei Matisan",
        "Alejandro Monroy Muñoz",
        "Janusz Partyka"
      ],
      "abstract": "Text-to-image generative models often present issues regarding fairness with\nrespect to certain sensitive attributes, such as gender or skin tone. This\nstudy aims to reproduce the results presented in \"ITI-GEN: Inclusive\nText-to-Image Generation\" by Zhang et al. (2023a), which introduces a model to\nimprove inclusiveness in these kinds of models. We show that most of the claims\nmade by the authors about ITI-GEN hold: it improves the diversity and quality\nof generated images, it is scalable to different domains, it has plug-and-play\ncapabilities, and it is efficient from a computational point of view. However,\nITI-GEN sometimes uses undesired attributes as proxy features and it is unable\nto disentangle some pairs of (correlated) attributes such as gender and\nbaldness. In addition, when the number of considered attributes increases, the\ntraining time grows exponentially and ITI-GEN struggles to generate inclusive\nimages for all elements in the joint distribution. To solve these issues, we\npropose using Hard Prompt Search with negative prompting, a method that does\nnot require training and that handles negation better than vanilla Hard Prompt\nSearch. Nonetheless, Hard Prompt Search (with or without negative prompting)\ncannot be used for continuous attributes that are hard to express in natural\nlanguage, an area where ITI-GEN excels as it is guided by images during\ntraining. Finally, we propose combining ITI-GEN and Hard Prompt Search with\nnegative prompting.",
      "tldr_zh": "该研究复现了Zhang et al. (2023a)的ITI-GEN模型，该模型旨在提升文本到图像生成模型的包容性，如改善性别或肤色等敏感属性的公平性。复现结果验证了ITI-GEN的大部分声明，包括提升生成图像的多样性和质量、可扩展到不同领域、即插即用能力以及计算效率。然而，ITI-GEN存在问题，如使用不期望的属性作为代理特征、无法分离相关属性（如性别和秃顶），且属性数量增加时训练时间指数增长，导致生成全面包容图像的困难。为解决这些问题，研究提出采用Hard Prompt Search with negative prompting，该方法无需训练且在处理否定方面更有效。最终，建议将ITI-GEN与Hard Prompt Search with negative prompting结合，以弥补各自局限性，如后者无法处理难以用自然语言表达的连续属性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to TMLR, see https://openreview.net/forum?id=d3Vj360Wi2",
      "pdf_url": "http://arxiv.org/pdf/2407.19996v1",
      "published_date": "2024-07-29 13:27:44 UTC",
      "updated_date": "2024-07-29 13:27:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:00:14.586872"
    },
    {
      "arxiv_id": "2407.19994v3",
      "title": "A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph",
      "title_zh": "翻译失败",
      "authors": [
        "Cheonsu Jeong"
      ],
      "abstract": "This study aims to improve knowledge-based question-answering (QA) systems by\novercoming the limitations of existing Retrieval-Augmented Generation (RAG)\nmodels and implementing an advanced RAG system based on Graph technology to\ndevelop high-quality generative AI services. While existing RAG models\ndemonstrate high accuracy and fluency by utilizing retrieved information, they\nmay suffer from accuracy degradation as they generate responses using\npre-loaded knowledge without reprocessing. Additionally, they cannot\nincorporate real-time data after the RAG configuration stage, leading to issues\nwith contextual understanding and biased information. To address these\nlimitations, this study implemented an enhanced RAG system utilizing Graph\ntechnology. This system is designed to efficiently search and utilize\ninformation. Specifically, it employs LangGraph to evaluate the reliability of\nretrieved information and synthesizes diverse data to generate more accurate\nand enhanced responses. Furthermore, the study provides a detailed explanation\nof the system's operation, key implementation steps, and examples through\nimplementation code and validation results, thereby enhancing the understanding\nof advanced RAG technology. This approach offers practical guidelines for\nimplementing advanced RAG systems in corporate services, making it a valuable\nresource for practical application.",
      "tldr_zh": "本研究旨在改进知识型问答（QA）系统，通过克服现有 Retrieval-Augmented Generation (RAG) 模型的局限性（如准确性下降、无法处理实时数据以及上下文理解和偏见问题），实现基于 Graph 技术的先进 RAG 系统。研究采用 LangGraph 来评估检索信息的可靠性，并合成多样数据，以生成更准确和增强的响应。最终，该系统通过详细说明操作步骤、实现代码和验证结果，提供企业服务中高级 RAG 系统的实用指南，提升了生成式 AI 的应用价值。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19994v3",
      "published_date": "2024-07-29 13:26:43 UTC",
      "updated_date": "2024-09-13 12:19:26 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:00:23.904483"
    },
    {
      "arxiv_id": "2407.19985v2",
      "title": "Mixture of Nested Experts: Adaptive Processing of Visual Tokens",
      "title_zh": "嵌套专家混合体：视觉标记的自适应处理",
      "authors": [
        "Gagan Jain",
        "Nidhi Hegde",
        "Aditya Kusupati",
        "Arsha Nagrani",
        "Shyamal Buch",
        "Prateek Jain",
        "Anurag Arnab",
        "Sujoy Paul"
      ],
      "abstract": "The visual medium (images and videos) naturally contains a large amount of\ninformation redundancy, thereby providing a great opportunity for leveraging\nefficiency in processing. While Vision Transformer (ViT) based models scale\neffectively to large data regimes, they fail to capitalize on this inherent\nredundancy, leading to higher computational costs. Mixture of Experts (MoE)\nnetworks demonstrate scalability while maintaining same inference-time costs,\nbut they come with a larger parameter footprint. We present Mixture of Nested\nExperts (MoNE), which utilizes a nested structure for experts, wherein\nindividual experts fall on an increasing compute-accuracy curve. Given a\ncompute budget, MoNE learns to dynamically choose tokens in a priority order,\nand thus redundant tokens are processed through cheaper nested experts. Using\nthis framework, we achieve equivalent performance as the baseline models, while\nreducing inference time compute by over two-fold. We validate our approach on\nstandard image and video datasets - ImageNet-21K, Kinetics400, and\nSomething-Something-v2. We further highlight MoNE$'$s adaptability by\nshowcasing its ability to maintain strong performance across different\ninference-time compute budgets on videos, using only a single trained model.",
      "tldr_zh": "该研究针对视觉数据（如图像和视频）的固有冗余问题，指出传统 Vision Transformer (ViT) 模型计算成本过高，而 Mixture of Experts (MoE) 虽可扩展但参数量大。作者提出 Mixture of Nested Experts (MoNE)，一种嵌套专家结构框架，能动态优先选择和处理 tokens，让冗余 tokens 通过更高效的嵌套专家进行计算，从而在保持性能的同时将推理时间计算减少一倍以上。在 ImageNet-21K、Kinetics400 和 Something-Something-v2 等数据集上验证，MoNE 展示了强大的适应性，可在单一训练模型下应对不同计算预算。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19985v2",
      "published_date": "2024-07-29 13:19:31 UTC",
      "updated_date": "2024-07-30 17:26:22 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:00:37.939754"
    },
    {
      "arxiv_id": "2407.19965v2",
      "title": "Simply Trainable Nearest Neighbour Machine Translation with GPU Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Hossam Amer",
        "Abdelrahman Abouelenin",
        "Mohamed Maher",
        "Evram Narouz",
        "Mohamed Afify",
        "Hany Awadallah"
      ],
      "abstract": "Nearest neighbor machine translation is a successful approach for fast domain\nadaption, which interpolates the pre-trained transformers with domain-specific\ntoken-level k-nearest-neighbor (kNN) retrieval without retraining. Despite kNN\nMT's success, searching large reference corpus and fixed interpolation between\nthe kNN and pre-trained model led to computational complexity and translation\nquality challenges. Among other papers, Dai et al. proposed methods to obtain a\nsmall number of reference samples dynamically for which they introduced a\ndistance-aware interpolation method using an equation that includes free\nparameters. This paper proposes a simply trainable nearest neighbor machine\ntranslation and carry out inference experiments on GPU. Similar to Dai et al.,\nwe first adaptively construct a small datastore for each input sentence.\nSecond, we train a single-layer network for the interpolation coefficient\nbetween the knnMT and pre-trained result to automatically interpolate in\ndifferent domains. Experimental results on different domains show that our\nproposed method either improves or sometimes maintain the translation quality\nof methods in Dai et al. while being automatic. In addition, our GPU inference\nresults demonstrate that knnMT can be integrated into GPUs with a drop of only\n5% in terms of speed.",
      "tldr_zh": "该论文提出了一种简单可训练的Nearest Neighbour Machine Translation (kNN MT) 方法，旨在解决传统kNN MT在搜索大参考语料库和固定插值时的计算复杂性和翻译质量问题。具体而言，该方法首先为每个输入句子自适应构建小型数据存储，然后训练一个单层网络来自动计算kNN MT与预训练模型之间的插值系数，以适应不同领域。实验结果显示，该方法在多个领域要么提升了翻译质量，要么与Dai et al.的方法相当，同时实现了自动化；此外，在GPU推理中，kNN MT仅损失5%的速度，证明了其高效性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "6 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.19965v2",
      "published_date": "2024-07-29 12:55:40 UTC",
      "updated_date": "2024-08-19 08:48:59 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:00:50.170585"
    },
    {
      "arxiv_id": "2407.19951v1",
      "title": "Can I trust my anomaly detection system? A case study based on explainable AI",
      "title_zh": "翻译失败",
      "authors": [
        "Muhammad Rashid",
        "Elvio Amparore",
        "Enrico Ferrari",
        "Damiano Verda"
      ],
      "abstract": "Generative models based on variational autoencoders are a popular technique\nfor detecting anomalies in images in a semi-supervised context. A common\napproach employs the anomaly score to detect the presence of anomalies, and it\nis known to reach high level of accuracy on benchmark datasets. However, since\nanomaly scores are computed from reconstruction disparities, they often obscure\nthe detection of various spurious features, raising concerns regarding their\nactual efficacy. This case study explores the robustness of an anomaly\ndetection system based on variational autoencoder generative models through the\nuse of eXplainable AI methods. The goal is to get a different perspective on\nthe real performances of anomaly detectors that use reconstruction differences.\nIn our case study we discovered that, in many cases, samples are detected as\nanomalous for the wrong or misleading factors.",
      "tldr_zh": "这篇论文通过一个基于 eXplainable AI 的案例研究，评估了使用 variational autoencoders 生成模型进行半监督图像异常检测的可靠性。研究发现，异常分数依赖于重建差异，往往导致检测到虚假特征，从而质疑了系统的实际效能。在案例分析中，许多样本被错误地标记为异常，基于误导因素，这强调了需要改进异常检测方法的鲁棒性和可解释性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "World Conference on eXplainable Artificial Intelligence",
      "pdf_url": "http://arxiv.org/pdf/2407.19951v1",
      "published_date": "2024-07-29 12:39:07 UTC",
      "updated_date": "2024-07-29 12:39:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:01:01.949861"
    },
    {
      "arxiv_id": "2407.19944v1",
      "title": "Noise-Resilient Unsupervised Graph Representation Learning via Multi-Hop Feature Quality Estimation",
      "title_zh": "基于多跳特征质量估计的噪声鲁棒无监督图表示学习",
      "authors": [
        "Shiyuan Li",
        "Yixin Liu",
        "Qingfeng Chen",
        "Geoffrey I. Webb",
        "Shirui Pan"
      ],
      "abstract": "Unsupervised graph representation learning (UGRL) based on graph neural\nnetworks (GNNs), has received increasing attention owing to its efficacy in\nhandling graph-structured data. However, existing UGRL methods ideally assume\nthat the node features are noise-free, which makes them fail to distinguish\nbetween useful information and noise when applied to real data with noisy\nfeatures, thus affecting the quality of learned representations. This urges us\nto take node noisy features into account in real-world UGRL. With empirical\nanalysis, we reveal that feature propagation, the essential operation in GNNs,\nacts as a \"double-edged sword\" in handling noisy features - it can both denoise\nand diffuse noise, leading to varying feature quality across nodes, even within\nthe same node at different hops. Building on this insight, we propose a novel\nUGRL method based on Multi-hop feature Quality Estimation (MQE for short).\nUnlike most UGRL models that directly utilize propagation-based GNNs to\ngenerate representations, our approach aims to learn representations through\nestimating the quality of propagated features at different hops. Specifically,\nwe introduce a Gaussian model that utilizes a learnable \"meta-representation\"\nas a condition to estimate the expectation and variance of multi-hop propagated\nfeatures via neural networks. In this way, the \"meta representation\" captures\nthe semantic and structural information underlying multiple propagated features\nbut is naturally less susceptible to interference by noise, thereby serving as\nhigh-quality node representations beneficial for downstream tasks. Extensive\nexperiments on multiple real-world datasets demonstrate that MQE in learning\nreliable node representations in scenarios with diverse types of feature noise.",
      "tldr_zh": "现有无监督图表示学习（UGRL）方法基于图神经网络（GNNs），但假设节点特征无噪声，在实际噪声环境中易受影响，导致学习质量下降。论文通过实证分析发现，特征传播（feature propagation）在GNNs中具有双刃剑效应，既能去噪也可能扩散噪声，从而导致多跳特征质量差异。作者提出了一种新方法Multi-hop feature Quality Estimation (MQE)，利用Gaussian模型和可学习的“meta-representation”来估计多跳传播特征的期望和方差，从而生成对噪声更鲁棒的高质量节点表示。实验在多个真实数据集上证明，MQE在各种特征噪声场景下显著提升了节点表示的可靠性和下游任务性能。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by CIKM 2024. 11 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.19944v1",
      "published_date": "2024-07-29 12:24:28 UTC",
      "updated_date": "2024-07-29 12:24:28 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:01:16.140398"
    },
    {
      "arxiv_id": "2407.19938v1",
      "title": "Robust Conformal Volume Estimation in 3D Medical Images",
      "title_zh": "在 3D 医疗图像中的鲁棒保形体积估计",
      "authors": [
        "Benjamin Lambert",
        "Florence Forbes",
        "Senan Doyle",
        "Michel Dojat"
      ],
      "abstract": "Volumetry is one of the principal downstream applications of 3D medical image\nsegmentation, for example, to detect abnormal tissue growth or for surgery\nplanning. Conformal Prediction is a promising framework for uncertainty\nquantification, providing calibrated predictive intervals associated with\nautomatic volume measurements. However, this methodology is based on the\nhypothesis that calibration and test samples are exchangeable, an assumption\nthat is in practice often violated in medical image applications. A weighted\nformulation of Conformal Prediction can be framed to mitigate this issue, but\nits empirical investigation in the medical domain is still lacking. A potential\nreason is that it relies on the estimation of the density ratio between the\ncalibration and test distributions, which is likely to be intractable in\nscenarios involving high-dimensional data. To circumvent this, we propose an\nefficient approach for density ratio estimation relying on the compressed\nlatent representations generated by the segmentation model. Our experiments\ndemonstrate the efficiency of our approach to reduce the coverage error in the\npresence of covariate shifts, in both synthetic and real-world settings. Our\nimplementation is available at https://github.com/benolmbrt/wcp_miccai",
      "tldr_zh": "这篇论文针对 3D 医疗图像体积估计的不确定性量化问题，探讨了 Conformal Prediction 框架的应用，但指出其假设（校准和测试样本可交换）在医疗领域常被协变量偏移（covariate shifts）违反。作者提出一种高效方法，使用分割模型生成的压缩潜在表示来估计密度比，从而实现加权 Conformal Prediction，缓解这一问题。实验结果显示，该方法在合成和真实场景中显著降低了覆盖错误，提高了体积估计的鲁棒性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Early accepted at MICCAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.19938v1",
      "published_date": "2024-07-29 12:18:07 UTC",
      "updated_date": "2024-07-29 12:18:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:01:27.766863"
    },
    {
      "arxiv_id": "2407.19937v2",
      "title": "AOTree: Aspect Order Tree-based Model for Explainable Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Wenxin Zhao",
        "Peng Zhang",
        "Hansu Gu",
        "Dongsheng Li",
        "Tun Lu",
        "Ning Gu"
      ],
      "abstract": "Recent recommender systems aim to provide not only accurate recommendations\nbut also explanations that help users understand them better. However, most\nexisting explainable recommendations only consider the importance of content in\nreviews, such as words or aspects, and ignore the ordering relationship among\nthem. This oversight neglects crucial ordering dimensions in the human\ndecision-making process, leading to suboptimal performance. Therefore, in this\npaper, we propose Aspect Order Tree-based (AOTree) explainable recommendation\nmethod, inspired by the Order Effects Theory from cognitive and decision\npsychology, in order to capture the dependency relationships among decisive\nfactors. We first validate the theory in the recommendation scenario by\nanalyzing the reviews of the users. Then, according to the theory, the proposed\nAOTree expands the construction of the decision tree to capture aspect orders\nin users' decision-making processes, and use attention mechanisms to make\npredictions based on the aspect orders. Extensive experiments demonstrate our\nmethod's effectiveness on rating predictions, and our approach aligns more\nconsistently with the user' s decision-making process by displaying\nexplanations in a particular order, thereby enhancing interpretability.",
      "tldr_zh": "这篇论文提出 AOTree 模型，一种基于 Aspect Order Tree 的可解释推荐方法，受 Order Effects Theory 启发，用于捕捉用户评论中方面顺序的依赖关系，以提升推荐系统的准确性和解释性。作者首先通过分析用户评论验证了该理论在推荐场景中的有效性，然后扩展决策树结构结合 attention mechanisms 来预测评分和生成有序解释。实验结果显示，AOTree 在评分预测上表现出色，并更符合用户的决策过程，从而提高了系统的可解释性。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19937v2",
      "published_date": "2024-07-29 12:17:48 UTC",
      "updated_date": "2024-08-03 05:40:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:01:38.225488"
    },
    {
      "arxiv_id": "2407.19922v1",
      "title": "Monetizing Currency Pair Sentiments through LLM Explainability",
      "title_zh": "翻译失败",
      "authors": [
        "Lior Limonad",
        "Fabiana Fournier",
        "Juan Manuel Vera Díaz",
        "Inna Skarbovsky",
        "Shlomit Gur",
        "Raquel Lazcano"
      ],
      "abstract": "Large language models (LLMs) play a vital role in almost every domain in\ntoday's organizations. In the context of this work, we highlight the use of\nLLMs for sentiment analysis (SA) and explainability. Specifically, we\ncontribute a novel technique to leverage LLMs as a post-hoc model-independent\ntool for the explainability of SA. We applied our technique in the financial\ndomain for currency-pair price predictions using open news feed data merged\nwith market prices. Our application shows that the developed technique is not\nonly a viable alternative to using conventional eXplainable AI but can also be\nfed back to enrich the input to the machine learning (ML) model to better\npredict future currency-pair values. We envision our results could be\ngeneralized to employing explainability as a conventional enrichment for ML\ninput for better ML predictions in general.",
      "tldr_zh": "本研究提出了一种新颖技术，利用大型语言模型(LLMs)作为后验模型无关的工具，来提升情感分析(SA)的可解释性。方法涉及将LLMs应用于金融领域，通过整合公开新闻数据和市场价格来预测货币对价值，并将解释性反馈回机器学习(ML)模型的输入中，以改善预测准确性。实验结果显示，该技术不仅可替代传统可解释AI(eXplainable AI)，还显著提升了货币对价格预测的表现。作者认为，这种方法可推广到一般ML输入增强领域，以实现更可靠的预测。",
      "categories": [
        "cs.AI",
        "68T50"
      ],
      "primary_category": "cs.AI",
      "comment": "7 pages, 3 figures, AIFin@ECAI 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.19922v1",
      "published_date": "2024-07-29 11:58:54 UTC",
      "updated_date": "2024-07-29 11:58:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:01:51.272747"
    },
    {
      "arxiv_id": "2407.19911v4",
      "title": "Efficient Shield Synthesis via State-Space Transformation",
      "title_zh": "翻译失败",
      "authors": [
        "Asger Horn Brorholt",
        "Andreas Holck Høeg-Petersen",
        "Kim Guldstrand Larsen",
        "Christian Schilling"
      ],
      "abstract": "We consider the problem of synthesizing safety strategies for control\nsystems, also known as shields. Since the state space is infinite, shields are\ntypically computed over a finite-state abstraction, with the most common\nabstraction being a rectangular grid. However, for many systems, such a grid\ndoes not align well with the safety property or the system dynamics. That is\nwhy a coarse grid is rarely sufficient, but a fine grid is typically\ncomputationally infeasible to obtain. In this paper, we show that appropriate\nstate-space transformations can still allow to use a coarse grid at almost no\ncomputational overhead. We demonstrate in three case studies that our\ntransformation-based synthesis outperforms a standard synthesis by several\norders of magnitude. In the first two case studies, we use domain knowledge to\nselect a suitable transformation. In the third case study, we instead report on\nresults in engineering a transformation without domain knowledge.",
      "tldr_zh": "本研究针对控制系统的安全策略合成（shield synthesis）问题，提出了一种通过状态-space transformations 的高效方法，以解决传统矩形网格抽象在无限状态空间中计算效率低下的挑战。该方法允许使用粗网格进行合成，几乎不增加计算开销，从而显著提高了合成过程的效率。在三个案例研究中，该方法比标准合成快了几个数量级，其中前两个案例利用领域知识选择变换，第三个案例则展示了无需领域知识即可工程化变换的有效性。该工作为处理复杂系统动态和安全属性的场景提供了实用解决方案。",
      "categories": [
        "cs.LO",
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.LO",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19911v4",
      "published_date": "2024-07-29 11:39:22 UTC",
      "updated_date": "2024-10-07 07:41:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:02:01.703024"
    },
    {
      "arxiv_id": "2407.19906v2",
      "title": "Reverse Map Projections as Equivariant Quantum Embeddings",
      "title_zh": "逆向地图投影作为等变量子嵌入",
      "authors": [
        "Max Arnott",
        "Dimitri Papaioannou",
        "Kieran McDowall",
        "Phalgun Lolur",
        "Bambordé Baldé"
      ],
      "abstract": "We introduce the novel class $(E_\\alpha)_{\\alpha \\in [-\\infty,1)}$ of reverse\nmap projection embeddings, each one defining a unique new method of encoding\nclassical data into quantum states. Inspired by well-known map projections from\nthe unit sphere onto its tangent planes, used in practice in cartography, these\nembeddings address the common drawback of the amplitude embedding method,\nwherein scalar multiples of data points are identified and information about\nthe norm of data is lost.\n  We show how reverse map projections can be utilised as equivariant embeddings\nfor quantum machine learning. Using these methods, we can leverage symmetries\nin classical datasets to significantly strengthen performance on quantum\nmachine learning tasks.\n  Finally, we select four values of $\\alpha$ with which to perform a simple\nclassification task, taking $E_\\alpha$ as the embedding and experimenting with\nboth equivariant and non-equivariant setups. We compare their results alongside\nthose of standard amplitude embedding.",
      "tldr_zh": "本文提出了一种新型嵌入方法，即（E_α）α ∈ [-∞, 1) 的 reverse map projections，作为将经典数据编码成量子状态的途径，解决了 amplitude embedding 中忽略数据范数的问题。  \n该方法受制图学中单位球面到切平面的投影启发，可作为 equivariant embeddings 用于量子机器学习，通过利用数据集的对称性显著提升任务性能。  \n实验中，选取四个 α 值进行分类任务的测试，结果显示 E_α 嵌入在 equivariant 和 non-equivariant 设置下均优于标准 amplitude embedding。",
      "categories": [
        "quant-ph",
        "cs.AI",
        "cs.ET",
        "math-ph",
        "math.MP"
      ],
      "primary_category": "quant-ph",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19906v2",
      "published_date": "2024-07-29 11:31:24 UTC",
      "updated_date": "2024-08-19 09:40:32 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:02:15.754855"
    },
    {
      "arxiv_id": "2407.19900v1",
      "title": "Practical and Reproducible Symbolic Music Generation by Large Language Models with Structural Embeddings",
      "title_zh": "翻译失败",
      "authors": [
        "Seungyeon Rhyu",
        "Kichang Yang",
        "Sungjun Cho",
        "Jaehyeon Kim",
        "Kyogu Lee",
        "Moontae Lee"
      ],
      "abstract": "Music generation introduces challenging complexities to large language\nmodels. Symbolic structures of music often include vertical harmonization as\nwell as horizontal counterpoint, urging various adaptations and enhancements\nfor large-scale Transformers. However, existing works share three major\ndrawbacks: 1) their tokenization requires domain-specific annotations, such as\nbars and beats, that are typically missing in raw MIDI data; 2) the pure impact\nof enhancing token embedding methods is hardly examined without domain-specific\nannotations; and 3) existing works to overcome the aforementioned drawbacks,\nsuch as MuseNet, lack reproducibility. To tackle such limitations, we develop a\nMIDI-based music generation framework inspired by MuseNet, empirically studying\ntwo structural embeddings that do not rely on domain-specific annotations. We\nprovide various metrics and insights that can guide suitable encoding to\ndeploy. We also verify that multiple embedding configurations can selectively\nboost certain musical aspects. By providing open-source implementations via\nHuggingFace, our findings shed light on leveraging large language models toward\npractical and reproducible music generation.",
      "tldr_zh": "该研究针对大型语言模型（Large Language Models）在符号音乐生成中的挑战，提出了一种基于 MIDI 的框架，以解决现有方法依赖领域特定注释（如小节和节拍）的局限性。框架引入了两种不依赖这些注释的结构嵌入（Structural Embeddings），并通过实证实验验证了不同嵌入配置能选择性地提升音乐的和声或对位方面。最终，该框架提供了实用指标和见解，并通过 HuggingFace 开源实现，促进了可重复的音乐生成实践。",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "9 pages, 6 figures, 4 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.19900v1",
      "published_date": "2024-07-29 11:24:10 UTC",
      "updated_date": "2024-07-29 11:24:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:02:26.702446"
    },
    {
      "arxiv_id": "2407.19897v1",
      "title": "BEExAI: Benchmark to Evaluate Explainable AI",
      "title_zh": "BEExAI：评估可解释人工智能的基准",
      "authors": [
        "Samuel Sithakoul",
        "Sara Meftah",
        "Clément Feutry"
      ],
      "abstract": "Recent research in explainability has given rise to numerous post-hoc\nattribution methods aimed at enhancing our comprehension of the outputs of\nblack-box machine learning models. However, evaluating the quality of\nexplanations lacks a cohesive approach and a consensus on the methodology for\nderiving quantitative metrics that gauge the efficacy of explainability\npost-hoc attribution methods. Furthermore, with the development of increasingly\ncomplex deep learning models for diverse data applications, the need for a\nreliable way of measuring the quality and correctness of explanations is\nbecoming critical. We address this by proposing BEExAI, a benchmark tool that\nallows large-scale comparison of different post-hoc XAI methods, employing a\nset of selected evaluation metrics.",
      "tldr_zh": "现有Explainable AI (XAI)领域虽有众多后验归因方法（post-hoc attribution methods），但缺乏统一的方法来评估这些解释的质量和有效性。论文提出BEExAI，这是一个基准工具，用于大规模比较不同后验XAI方法，并采用一组选定的评估指标进行量化评估。随着深度学习模型的复杂化，该工具有助于可靠地衡量解释的正确性和质量，为XAI研究提供标准化框架。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19897v1",
      "published_date": "2024-07-29 11:21:17 UTC",
      "updated_date": "2024-07-29 11:21:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:02:37.724562"
    },
    {
      "arxiv_id": "2407.19893v1",
      "title": "Leveraging Foundation Models for Zero-Shot IoT Sensing",
      "title_zh": "翻译失败",
      "authors": [
        "Dinghao Xue",
        "Xiaoran Fan",
        "Tao Chen",
        "Guohao Lan",
        "Qun Song"
      ],
      "abstract": "Deep learning models are increasingly deployed on edge Internet of Things\n(IoT) devices. However, these models typically operate under supervised\nconditions and fail to recognize unseen classes different from training. To\naddress this, zero-shot learning (ZSL) aims to classify data of unseen classes\nwith the help of semantic information. Foundation models (FMs) trained on\nweb-scale data have shown impressive ZSL capability in natural language\nprocessing and visual understanding. However, leveraging FMs' generalized\nknowledge for zero-shot IoT sensing using signals such as mmWave, IMU, and\nWi-Fi has not been fully investigated. In this work, we align the IoT data\nembeddings with the semantic embeddings generated by an FM's text encoder for\nzero-shot IoT sensing. To utilize the physics principles governing the\ngeneration of IoT sensor signals to derive more effective prompts for semantic\nembedding extraction, we propose to use cross-attention to combine a learnable\nsoft prompt that is optimized automatically on training data and an auxiliary\nhard prompt that encodes domain knowledge of the IoT sensing task. To address\nthe problem of IoT embeddings biasing to seen classes due to the lack of unseen\nclass data during training, we propose using data augmentation to synthesize\nunseen class IoT data for fine-tuning the IoT feature extractor and embedding\nprojector. We evaluate our approach on multiple IoT sensing tasks. Results show\nthat our approach achieves superior open-set detection and generalized\nzero-shot learning performance compared with various baselines. Our code is\navailable at https://github.com/schrodingho/FM\\_ZSL\\_IoT.",
      "tldr_zh": "本文提出一种利用 Foundation Models (FMs) 实现 Zero-Shot IoT Sensing 的方法，以解决传统深度学习模型在 IoT 设备上无法识别训练中未见类的局限性。核心创新包括将 IoT 数据嵌入与 FMs 的语义嵌入对齐，并使用 Cross-Attention 结合可学习的 Soft Prompt 和编码领域知识的 Hard Prompt 来优化提示提取，同时通过数据增强合成未见类数据以减少模型偏置。实验结果显示，该方法在多个 IoT 传感任务上，显著优于基线模型，在 Open-Set Detection 和 Generalized Zero-Shot Learning 性能上表现出色。代码已开源，可用于进一步研究。",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19893v1",
      "published_date": "2024-07-29 11:16:48 UTC",
      "updated_date": "2024-07-29 11:16:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:02:52.642930"
    },
    {
      "arxiv_id": "2407.19888v1",
      "title": "Yucca: A Deep Learning Framework For Medical Image Analysis",
      "title_zh": "Yucca：一个用于医疗图像分析的深度学习框架",
      "authors": [
        "Sebastian Nørgaard Llambias",
        "Julia Machnio",
        "Asbjørn Munk",
        "Jakob Ambsdorf",
        "Mads Nielsen",
        "Mostafa Mehdipour Ghazi"
      ],
      "abstract": "Medical image analysis using deep learning frameworks has advanced healthcare\nby automating complex tasks, but many existing frameworks lack flexibility,\nmodularity, and user-friendliness. To address these challenges, we introduce\nYucca, an open-source AI framework available at\nhttps://github.com/Sllambias/yucca, designed specifically for medical imaging\napplications and built on PyTorch and PyTorch Lightning. Yucca features a\nthree-tiered architecture: Functional, Modules, and Pipeline, providing a\ncomprehensive and customizable solution. Evaluated across diverse tasks such as\ncerebral microbleeds detection, white matter hyperintensity segmentation, and\nhippocampus segmentation, Yucca achieves state-of-the-art results,\ndemonstrating its robustness and versatility. Yucca offers a powerful,\nflexible, and user-friendly platform for medical image analysis, inviting\ncommunity contributions to advance its capabilities and impact.",
      "tldr_zh": "该研究介绍了 Yucca，一个开源深度学习框架，旨在解决现有框架在医疗图像分析中的灵活性、模块性和用户友好性不足问题。Yucca 基于 PyTorch 和 PyTorch Lightning，采用三层架构（Functional、Modules 和 Pipeline），提供全面可定制的解决方案。在脑微出血检测、白质高信号分割和海马体分割等任务上，Yucca 实现了最先进的结果，展示了其鲁棒性和多功能性。该框架邀请社区贡献，推动医疗图像分析领域的创新发展。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19888v1",
      "published_date": "2024-07-29 11:09:10 UTC",
      "updated_date": "2024-07-29 11:09:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:03:02.774125"
    },
    {
      "arxiv_id": "2407.19886v1",
      "title": "A Unified Graph Transformer for Overcoming Isolations in Multi-modal Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Zixuan Yi",
        "Iadh Ounis"
      ],
      "abstract": "With the rapid development of online multimedia services, especially in\ne-commerce platforms, there is a pressing need for personalised recommendation\nsystems that can effectively encode the diverse multi-modal content associated\nwith each item. However, we argue that existing multi-modal recommender systems\ntypically use isolated processes for both feature extraction and modality\nmodelling. Such isolated processes can harm the recommendation performance.\nFirstly, an isolated extraction process underestimates the importance of\neffective feature extraction in multi-modal recommendations, potentially\nincorporating non-relevant information, which is harmful to item\nrepresentations. Second, an isolated modality modelling process produces\ndisjointed embeddings for item modalities due to the individual processing of\neach modality, which leads to a suboptimal fusion of user/item representations\nfor effective user preferences prediction. We hypothesise that the use of a\nunified model for addressing both aforementioned isolated processes will enable\nthe consistent extraction and cohesive fusion of joint multi-modal features,\nthereby enhancing the effectiveness of multi-modal recommender systems. In this\npaper, we propose a novel model, called Unified Multi-modal Graph Transformer\n(UGT), which firstly leverages a multi-way transformer to extract aligned\nmulti-modal features from raw data for top-k recommendation. Subsequently, we\nbuild a unified graph neural network in our UGT model to jointly fuse the\nuser/item representations with their corresponding multi-modal features. Using\nthe graph transformer architecture of our UGT model, we show that the UGT model\ncan achieve significant effectiveness gains, especially when jointly optimised\nwith the commonly-used multi-modal recommendation losses.",
      "tldr_zh": "本研究指出，现有的多模态推荐系统在特征提取和模态建模过程中存在孤立问题，导致非相关信息干扰和模态嵌入不连贯，从而影响推荐性能。论文假设使用统一模型可实现一致的特征提取和连贯融合，以提升系统有效性。提出了一种新型模型Unified Multi-modal Graph Transformer (UGT)，它首先利用multi-way transformer从原始数据中提取对齐的多模态特征，用于top-k推荐；随后，通过统一的Graph Neural Network联合融合用户/物品表示及其多模态特征。实验结果显示，UGT模型在与常用多模态推荐损失联合优化时，显著提高了推荐准确性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19886v1",
      "published_date": "2024-07-29 11:04:31 UTC",
      "updated_date": "2024-07-29 11:04:31 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:03:14.875875"
    },
    {
      "arxiv_id": "2407.19869v1",
      "title": "Distances Between Partial Preference Orderings",
      "title_zh": "部分偏好排序之间的距离",
      "authors": [
        "Jean Dezert",
        "Andrii Shekhovtsov",
        "Wojciech Salabun"
      ],
      "abstract": "This paper proposes to establish the distance between partial preference\norderings based on two very different approaches. The first approach\ncorresponds to the brute force method based on combinatorics. It generates all\npossible complete preference orderings compatible with the partial preference\norderings and calculates the Frobenius distance between all fully compatible\npreference orderings. Unfortunately, this first method is not very efficient in\nsolving high-dimensional problems because of its big combinatorial complexity.\nThat is why we propose to circumvent this problem by using a second approach\nbased on belief functions, which can adequately model the missing information\nof partial preference orderings. This second approach to the calculation of\ndistance does not suffer from combinatorial complexity limitation. We show\nthrough simple examples how these two theoretical methods work.",
      "tldr_zh": "这篇论文提出了一种计算部分偏好排序（partial preference orderings）之间距离的方法，基于两种不同途径，以解决偏好建模中的不确定性问题。第一种方法采用组合学暴力法（brute force based on combinatorics），通过生成所有与部分偏好排序兼容的完整排序并计算 Frobenius distance，但其在高维问题上效率低下。第二种方法基于 belief functions，能有效处理缺失信息，避免了组合复杂度的限制。通过简单例子，论文展示了这些方法的实际运作，为偏好分析提供了更高效的工具。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages",
      "pdf_url": "http://arxiv.org/pdf/2407.19869v1",
      "published_date": "2024-07-29 10:39:40 UTC",
      "updated_date": "2024-07-29 10:39:40 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:03:26.464539"
    },
    {
      "arxiv_id": "2407.19865v2",
      "title": "Imitation Learning for Intra-Day Power Grid Operation through Topology Actions",
      "title_zh": "翻译失败",
      "authors": [
        "Matthijs de Jong",
        "Jan Viebahn",
        "Yuliya Shapovalova"
      ],
      "abstract": "Power grid operation is becoming increasingly complex due to the increase in\ngeneration of renewable energy. The recent series of Learning To Run a Power\nNetwork (L2RPN) competitions have encouraged the use of artificial agents to\nassist human dispatchers in operating power grids. In this paper we study the\nperformance of imitation learning for day-ahead power grid operation through\ntopology actions. In particular, we consider two rule-based expert agents: a\ngreedy agent and a N-1 agent. While the latter is more computationally\nexpensive since it takes N-1 safety considerations into account, it exhibits a\nmuch higher operational performance. We train a fully-connected neural network\n(FCNN) on expert state-action pairs and evaluate it in two ways. First, we find\nthat classification accuracy is limited despite extensive hyperparameter\ntuning, due to class imbalance and class overlap. Second, as a power system\nagent, the FCNN performs only slightly worse than expert agents. Furthermore,\nhybrid agents, which incorporate minimal additional simulations, match expert\nagents' performance with significantly lower computational cost. Consequently,\nimitation learning shows promise for developing fast, high-performing power\ngrid agents, motivating its further exploration in future L2RPN studies.",
      "tldr_zh": "该研究探讨了模仿学习(imitation learning)应用于当日电力网格操作，通过拓扑动作(topology actions)来辅助调度员。研究者训练了一个 fully-connected neural network (FCNN) 基于两种专家代理——greedy agent 和 N-1 agent 的状态-动作对，其中 N-1 agent 虽计算开销更大，但性能更优。评估结果显示，FCNN 的分类准确率受类别不平衡和重叠影响而有限，但作为电力系统代理，其性能仅略逊于专家代理；此外，混合代理通过最小额外模拟即可匹配专家性能，同时显著降低计算成本。总之，该方法为开发快速、高效的电力网格代理提供了潜力，并鼓励在未来 L2RPN 竞赛中进一步研究。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.SY",
        "eess.SY"
      ],
      "primary_category": "cs.AI",
      "comment": "To be presented at the Machine Learning for Sustainable Power Systems\n  2024 workshop and to be published in the corresponding Springer\n  Communications in Computer and Information Science proceedings",
      "pdf_url": "http://arxiv.org/pdf/2407.19865v2",
      "published_date": "2024-07-29 10:34:19 UTC",
      "updated_date": "2024-08-18 09:55:39 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:03:39.282132"
    },
    {
      "arxiv_id": "2407.19860v1",
      "title": "Anomalous State Sequence Modeling to Enhance Safety in Reinforcement Learning",
      "title_zh": "异常状态序列建模以增强强化学习中的安全",
      "authors": [
        "Leen Kweider",
        "Maissa Abou Kassem",
        "Ubai Sandouk"
      ],
      "abstract": "The deployment of artificial intelligence (AI) in decision-making\napplications requires ensuring an appropriate level of safety and reliability,\nparticularly in changing environments that contain a large number of unknown\nobservations. To address this challenge, we propose a novel safe reinforcement\nlearning (RL) approach that utilizes an anomalous state sequence to enhance RL\nsafety. Our proposed solution Safe Reinforcement Learning with Anomalous State\nSequences (AnoSeqs) consists of two stages. First, we train an agent in a\nnon-safety-critical offline 'source' environment to collect safe state\nsequences. Next, we use these safe sequences to build an anomaly detection\nmodel that can detect potentially unsafe state sequences in a 'target'\nsafety-critical environment where failures can have high costs. The estimated\nrisk from the anomaly detection model is utilized to train a risk-averse RL\npolicy in the target environment; this involves adjusting the reward function\nto penalize the agent for visiting anomalous states deemed unsafe by our\nanomaly model. In experiments on multiple safety-critical benchmarking\nenvironments including self-driving cars, our solution approach successfully\nlearns safer policies and proves that sequential anomaly detection can provide\nan effective supervisory signal for training safety-aware RL agents",
      "tldr_zh": "该论文提出了一种名为 AnoSeqs 的安全强化学习（Safe RL）方法，通过建模异常状态序列来提升 AI 在动态环境中的安全性和可靠性。具体而言，该方法先在非安全关键的源环境中收集安全状态序列，然后使用这些序列训练异常检测模型，并在目标安全关键环境中调整奖励函数，以惩罚代理访问异常状态，从而训练出风险厌恶的 RL 策略。在包括自驾车在内的多个基准实验中，AnoSeqs 成功学习了更安全的策略，并证明了顺序异常检测作为监督信号的有效性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19860v1",
      "published_date": "2024-07-29 10:30:07 UTC",
      "updated_date": "2024-07-29 10:30:07 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:03:53.166919"
    },
    {
      "arxiv_id": "2407.19835v1",
      "title": "ATHAR: A High-Quality and Diverse Dataset for Classical Arabic to English Translation",
      "title_zh": "翻译失败",
      "authors": [
        "Mohammed Khalil",
        "Mohammed Sabry"
      ],
      "abstract": "Classical Arabic represents a significant era, encompassing the golden age of\nArab culture, philosophy, and scientific literature. With a broad consensus on\nthe importance of translating these literatures to enrich knowledge\ndissemination across communities, the advent of large language models (LLMs)\nand translation systems offers promising tools to facilitate this goal.\nHowever, we have identified a scarcity of translation datasets in Classical\nArabic, which are often limited in scope and topics, hindering the development\nof high-quality translation systems. In response, we present the ATHAR dataset,\ncomprising 66,000 high-quality Classical Arabic to English translation samples\nthat cover a wide array of subjects including science, culture, and philosophy.\nFurthermore, we assess the performance of current state-of-the-art LLMs under\nvarious settings, concluding that there is a need for such datasets in current\nsystems. Our findings highlight how models can benefit from fine-tuning or\nincorporating this dataset into their pretraining pipelines. The dataset is\npublicly available on the HuggingFace Data Hub at\n\\url{https://huggingface.co/datasets/mohamed-khalil/ATHAR}.",
      "tldr_zh": "本文介绍了 ATHAR 数据集，这是一个高质量且多样化的 Classical Arabic 到 English 翻译数据集，包含 66,000 个样本，覆盖科学、文化和哲学等广泛主题。针对 Classical Arabic 翻译资源稀缺的问题，该数据集旨在提升大语言模型 (LLMs) 的翻译性能。研究评估了当前最先进的 LLMs，发现通过微调或整合 ATHAR 数据集，模型的表现可显著改善。该数据集已公开在 HuggingFace 上，供研究社区使用。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19835v1",
      "published_date": "2024-07-29 09:45:34 UTC",
      "updated_date": "2024-07-29 09:45:34 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:04:02.717146"
    },
    {
      "arxiv_id": "2407.19832v3",
      "title": "ML-Mamba: Efficient Multi-Modal Large Language Model Utilizing Mamba-2",
      "title_zh": "翻译失败",
      "authors": [
        "Wenjun Huang",
        "Jiakai Pan",
        "Jiahao Tang",
        "Yanyu Ding",
        "Yifei Xing",
        "Yuhe Wang",
        "Zhengzhuo Wang",
        "Jianguo Hu"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have attracted much attention for\ntheir multifunctionality. However, traditional Transformer architectures incur\nsignificant overhead due to their secondary computational complexity. To\naddress this issue, we introduce ML-Mamba, a multimodal language model, which\nutilizes the latest and efficient Mamba-2 model for inference. Mamba-2 is known\nfor its linear scalability and fast processing of long sequences. We replace\nthe Transformer-based backbone with a pre-trained Mamba-2 model and explore\nmethods for integrating 2D visual selective scanning mechanisms into multimodal\nlearning while also trying various visual encoders and Mamba-2 model variants.\nOur extensive experiments in various multimodal benchmark tests demonstrate the\ncompetitive performance of ML-Mamba and highlight the potential of state space\nmodels in multimodal tasks. The experimental results show that: (1) we\nempirically explore how to effectively apply the 2D vision selective scan\nmechanism for multimodal learning. We propose a novel multimodal connector\ncalled the Mamba-2 Scan Connector (MSC), which enhances representational\ncapabilities. (2) ML-Mamba achieves performance comparable to state-of-the-art\nmethods such as TinyLaVA and MobileVLM v2 through its linear sequential\nmodeling while faster inference speed; (3) Compared to multimodal models\nutilizing Mamba-1, the Mamba-2-based ML-Mamba exhibits superior inference\nperformance and effectiveness.",
      "tldr_zh": "这篇论文引入了 ML-Mamba，一种高效的多模态大型语言模型（MLLMs），利用 Mamba-2 模型取代传统的 Transformer 架构，以解决其二次计算复杂性和效率问题。研究者探索了整合 2D 视觉选择性扫描机制的方法，并提出了新型多模态连接器 Mamba-2 Scan Connector (MSC)，以提升表示能力和处理长序列的性能。实验结果表明，ML-Mamba 在各种多模态基准测试中表现出与 TinyLaVA 和 MobileVLM v2 相当的性能，同时实现更快的推理速度，且相较于基于 Mamba-1 的模型具有显著优势。",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19832v3",
      "published_date": "2024-07-29 09:38:15 UTC",
      "updated_date": "2024-08-21 09:52:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:04:17.297968"
    },
    {
      "arxiv_id": "2407.19829v2",
      "title": "Generative Retrieval with Preference Optimization for E-commerce Search",
      "title_zh": "翻译失败",
      "authors": [
        "Mingming Li",
        "Huimu Wang",
        "Zuxu Chen",
        "Guangtao Nie",
        "Yiming Qiu",
        "Guoyu Tang",
        "Lin Liu",
        "Jingwei Zhuo"
      ],
      "abstract": "Generative retrieval introduces a groundbreaking paradigm to document\nretrieval by directly generating the identifier of a pertinent document in\nresponse to a specific query. This paradigm has demonstrated considerable\nbenefits and potential, particularly in representation and generalization\ncapabilities, within the context of large language models. However, it faces\nsignificant challenges in E-commerce search scenarios, including the complexity\nof generating detailed item titles from brief queries, the presence of noise in\nitem titles with weak language order, issues with long-tail queries, and the\ninterpretability of results. To address these challenges, we have developed an\ninnovative framework for E-commerce search, called generative retrieval with\npreference optimization. This framework is designed to effectively learn and\nalign an autoregressive model with target data, subsequently generating the\nfinal item through constraint-based beam search. By employing multi-span\nidentifiers to represent raw item titles and transforming the task of\ngenerating titles from queries into the task of generating multi-span\nidentifiers from queries, we aim to simplify the generation process. The\nframework further aligns with human preferences using click data and employs a\nconstrained search method to identify key spans for retrieving the final item,\nthereby enhancing result interpretability. Our extensive experiments show that\nthis framework achieves competitive performance on a real-world dataset, and\nonline A/B tests demonstrate the superiority and effectiveness in improving\nconversion gains.",
      "tldr_zh": "这篇论文提出了 Generative Retrieval with Preference Optimization 框架，用于解决电商搜索中的生成式检索挑战，包括生成详细商品标题的复杂性、标题噪声、长尾查询问题及结果可解释性。该框架利用自回归模型（autoregressive model）结合偏好优化（Preference Optimization），通过多跨度标识符（multi-span identifiers）将查询到标题的生成任务简化为生成标识符的过程，并借助点击数据（click data）和约束-based beam search 提升模型与人类偏好的对齐及结果解释性。实验结果显示，该框架在真实数据集上实现了竞争性性能，并在在线 A/B 测试中证明了其在提升转换率方面的显著优势。",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19829v2",
      "published_date": "2024-07-29 09:31:19 UTC",
      "updated_date": "2024-10-25 07:30:45 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:04:27.663965"
    },
    {
      "arxiv_id": "2407.19825v2",
      "title": "Concise Thoughts: Impact of Output Length on LLM Reasoning and Cost",
      "title_zh": "简洁思考：输出长度对LLM推理和成本的影响",
      "authors": [
        "Sania Nayab",
        "Giulio Rossolini",
        "Marco Simoni",
        "Andrea Saracino",
        "Giorgio Buttazzo",
        "Nicolamaria Manes",
        "Fabrizio Giacomelli"
      ],
      "abstract": "Today's large language models (LLMs) can solve challenging question-answering\ntasks, and prompt engineering techniques, such as chain-of-thought (CoT), have\ngained attention for enhancing the explanation and correctness of outputs.\nHowever, many models and techniques tend to produce excessively verbose and\nlengthy answers, leading to issues with both conciseness and generation time.\nTo address this, this paper analyzes the impact of output lengths on LLM\ninference pipelines by introducing and proposing novel metrics to evaluate the\n\\textit{correct conciseness} of a model and related prompting techniques. Then,\nwe examine the impact of controlling output length through a refined prompt\nengineering strategy, Constrained-CoT (CCoT), which encourages the model to\nproduce more concise outputs. To better understand the effects of such a\nprompt, we also introduce two additional scores for analyzing the conciseness,\nmeasured in terms of redundancy and information flow in generated answers.\nExperiments on pretrained LLMs and multiple datasets demonstrate the benefits\nof the proposed metrics and the effectiveness of CCoT across different models.",
      "tldr_zh": "本研究探讨了输出长度对大型语言模型（LLMs）的推理性能和成本的影响，指出现有模型和提示工程如 Chain-of-Thought (CoT) 常导致冗长答案，影响简洁性与生成效率。论文引入新指标评估模型的“正确简洁性”，并提出改进策略 Constrained-CoT (CCoT)，通过精炼提示鼓励更简洁输出，同时引入两个分数（冗余和信息流）来量化分析。实验在预训练 LLMs 和多个数据集上验证了这些指标的有效性和 CCoT 的优势，展示了其在提升模型表现方面的潜力。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Preprint version, under review",
      "pdf_url": "http://arxiv.org/pdf/2407.19825v2",
      "published_date": "2024-07-29 09:21:52 UTC",
      "updated_date": "2025-01-23 08:45:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:04:39.577791"
    },
    {
      "arxiv_id": "2407.19813v3",
      "title": "Improving Retrieval Augmented Language Model with Self-Reasoning",
      "title_zh": "翻译失败",
      "authors": [
        "Yuan Xia",
        "Jingbo Zhou",
        "Zhenhui Shi",
        "Jun Chen",
        "Haifeng Huang"
      ],
      "abstract": "The Retrieval-Augmented Language Model (RALM) has shown remarkable\nperformance on knowledge-intensive tasks by incorporating external knowledge\nduring inference, which mitigates the factual hallucinations inherited in large\nlanguage models (LLMs). Despite these advancements, challenges persist in the\nimplementation of RALMs, particularly concerning their reliability and\ntraceability. To be specific, the irrelevant document retrieval may result in\nunhelpful response generation or even deteriorate the performance of LLMs,\nwhile the lack of proper citations in generated outputs complicates efforts to\nverify the trustworthiness of the models. To this end, we propose a novel\nself-reasoning framework aimed at improving the reliability and traceability of\nRALMs, whose core idea is to leverage reasoning trajectories generated by the\nLLM itself. The framework involves constructing self-reason trajectories with\nthree processes: a relevance-aware process, an evidence-aware selective\nprocess, and a trajectory analysis process. We have evaluated our framework\nacross four public datasets (two short-form QA datasets, one long-form QA\ndataset, and one fact verification dataset) to demonstrate the superiority of\nour method, which can outperform existing state-of-the-art models and can\nachieve comparable performance with GPT-4, while only using 2,000 training\nsamples.",
      "tldr_zh": "本文提出了一种 self-reasoning framework，以提升 Retrieval-Augmented Language Model (RALM) 的可靠性和可追溯性，解决无关文档检索导致的响应问题和引用缺失挑战。框架的核心是利用 LLM 自身的推理轨迹，通过三个过程——relevance-aware process（相关性感知过程）、evidence-aware selective process（证据感知选择过程）和trajectory analysis process（轨迹分析过程）——来构建自推理路径，从而改善外部知识整合的效果。在四个公共数据集（包括短形式和长形式 QA 及事实验证数据集）的实验中，该方法优于现有最先进模型，仅用 2000 个训练样本就实现了与 GPT-4 相当的性能。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "AAAI 2025 (main conference)",
      "pdf_url": "http://arxiv.org/pdf/2407.19813v3",
      "published_date": "2024-07-29 09:05:10 UTC",
      "updated_date": "2024-12-19 06:27:44 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:04:53.008038"
    },
    {
      "arxiv_id": "2407.19811v1",
      "title": "Synthetic Thermal and RGB Videos for Automatic Pain Assessment utilizing a Vision-MLP Architecture",
      "title_zh": "合成热成像和 RGB 视频用于自动疼痛评估，利用 Vision-MLP 架构",
      "authors": [
        "Stefanos Gkikas",
        "Manolis Tsiknakis"
      ],
      "abstract": "Pain assessment is essential in developing optimal pain management protocols\nto alleviate suffering and prevent functional decline in patients.\nConsequently, reliable and accurate automatic pain assessment systems are\nessential for continuous and effective patient monitoring. This study presents\nsynthetic thermal videos generated by Generative Adversarial Networks\nintegrated into the pain recognition pipeline and evaluates their efficacy. A\nframework consisting of a Vision-MLP and a Transformer-based module is\nutilized, employing RGB and synthetic thermal videos in unimodal and multimodal\nsettings. Experiments conducted on facial videos from the BioVid database\ndemonstrate the effectiveness of synthetic thermal videos and underline the\npotential advantages of it.",
      "tldr_zh": "本研究开发了一种利用合成热视频进行自动疼痛评估的系统，以解决患者监测中的关键需求。研究采用生成对抗网络（GAN）生成合成热视频，并将其与 RGB 视频整合到基于 Vision-MLP 和 Transformer 模块的框架中，在单模态和多模态设置下进行处理。在 BioVid 数据库的面部视频实验中，合成热视频证明了其有效性，提升了疼痛识别的准确性和可靠性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19811v1",
      "published_date": "2024-07-29 09:04:11 UTC",
      "updated_date": "2024-07-29 09:04:11 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:05:03.010255"
    },
    {
      "arxiv_id": "2407.19809v1",
      "title": "Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for Multimodal Automatic Pain Assessment using Facial Videos and fNIRS",
      "title_zh": "翻译失败",
      "authors": [
        "Stefanos Gkikas",
        "Manolis Tsiknakis"
      ],
      "abstract": "Automatic pain assessment plays a critical role for advancing healthcare and\noptimizing pain management strategies. This study has been submitted to the\nFirst Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment\n(AI4PAIN). The proposed multimodal framework utilizes facial videos and fNIRS\nand presents a modality-agnostic approach, alleviating the need for\ndomain-specific models. Employing a dual ViT configuration and adopting\nwaveform representations for the fNIRS, as well as for the extracted embeddings\nfrom the two modalities, demonstrate the efficacy of the proposed method,\nachieving an accuracy of 46.76% in the multilevel pain assessment task.",
      "tldr_zh": "这篇论文提出了一种模态无关（modality-agnostic）的Vision Transformer框架Twins-PainViT，用于多模态自动疼痛评估，结合面部视频和fNIRS数据，以支持医疗保健中的疼痛管理策略。框架采用双ViT配置，将fNIRS数据及其提取嵌入表示为波形形式，从而无需依赖特定领域的模型。实验结果显示，在多级疼痛评估任务中，该方法实现了46.76%的准确率，证明了其在First Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN)中的有效性。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19809v1",
      "published_date": "2024-07-29 09:02:43 UTC",
      "updated_date": "2024-07-29 09:02:43 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:05:16.270164"
    },
    {
      "arxiv_id": "2407.19804v2",
      "title": "Imputation for prediction: beware of diminishing returns",
      "title_zh": "翻译失败",
      "authors": [
        "Marine Le Morvan",
        "Gaël Varoquaux"
      ],
      "abstract": "Missing values are prevalent across various fields, posing challenges for\ntraining and deploying predictive models. In this context, imputation is a\ncommon practice, driven by the hope that accurate imputations will enhance\npredictions. However, recent theoretical and empirical studies indicate that\nsimple constant imputation can be consistent and competitive. This empirical\nstudy aims at clarifying if and when investing in advanced imputation methods\nyields significantly better predictions. Relating imputation and predictive\naccuracies across combinations of imputation and predictive models on 19\ndatasets, we show that imputation accuracy matters less i) when using\nexpressive models, ii) when incorporating missingness indicators as\ncomplementary inputs, iii) matters much more for generated linear outcomes than\nfor real-data outcomes. Interestingly, we also show that the use of the\nmissingness indicator is beneficial to the prediction performance, even in MCAR\nscenarios. Overall, on real-data with powerful models, improving imputation\nonly has a minor effect on prediction performance. Thus, investing in better\nimputations for improved predictions often offers limited benefits.",
      "tldr_zh": "这篇论文通过实证研究探讨了缺失值 imputation 在预测模型中的作用，强调了投资高级 imputation 方法可能带来的收益递减问题。研究在 19 个数据集上测试了不同 imputation 和预测模型的组合，发现 imputation 准确性对预测性能的影响较小，尤其在使用表现力强的模型或加入缺失值指示器时。论文进一步指出，对于真实数据 outcomes，imputation 准确性远不如线性生成 outcomes 重要，且即使在 MCAR 场景下，缺失值指示器也能提升预测性能。总体结论是，在强大模型下，提高 imputation 通常仅带来微小改善，因此建议谨慎投资于更复杂的 imputation 策略。",
      "categories": [
        "cs.AI",
        "cs.LG",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19804v2",
      "published_date": "2024-07-29 09:01:06 UTC",
      "updated_date": "2025-02-20 07:57:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:05:29.496993"
    },
    {
      "arxiv_id": "2407.19795v1",
      "title": "VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks",
      "title_zh": "翻译失败",
      "authors": [
        "Juhwan Choi",
        "Junehyoung Kwon",
        "JungMin Yun",
        "Seunguk Yu",
        "YoungBin Kim"
      ],
      "abstract": "Domain generalizability is a crucial aspect of a deep learning model since it\ndetermines the capability of the model to perform well on data from unseen\ndomains. However, research on the domain generalizability of deep learning\nmodels for vision-language tasks remains limited, primarily because of the lack\nof required datasets. To address these challenges, we propose VolDoGer:\nVision-Language Dataset for Domain Generalization, a dedicated dataset designed\nfor domain generalization that addresses three vision-language tasks: image\ncaptioning, visual question answering, and visual entailment. We constructed\nVolDoGer by extending LLM-based data annotation techniques to vision-language\ntasks, thereby alleviating the burden of recruiting human annotators. We\nevaluated the domain generalizability of various models, ranging from\nfine-tuned models to a recent multimodal large language model, through\nVolDoGer.",
      "tldr_zh": "该论文强调了domain generalizability在视觉语言任务中对深度学习模型的重要性，但现有研究因数据集缺失而受限。为解决这一问题，研究者提出VolDoGer，一种基于LLM-assisted数据标注技术的专用数据集，扩展到image captioning、visual question answering和visual entailment三个任务，从而减少了人类标注者的负担。通过VolDoGer，论文评估了从微调模型到多模态大语言模型等多种模型的domain generalizability性能。",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.CL",
      "comment": "31 pages, 5 figures, 20 tables",
      "pdf_url": "http://arxiv.org/pdf/2407.19795v1",
      "published_date": "2024-07-29 08:38:46 UTC",
      "updated_date": "2024-07-29 08:38:46 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:05:40.262336"
    },
    {
      "arxiv_id": "2407.19790v1",
      "title": "Hashing based Contrastive Learning for Virtual Screening",
      "title_zh": "基于哈希的对比学习用于虚拟筛选",
      "authors": [
        "Jin Han",
        "Yun Hong",
        "Wu-Jun Li"
      ],
      "abstract": "Virtual screening (VS) is a critical step in computer-aided drug discovery,\naiming to identify molecules that bind to a specific target receptor like\nprotein. Traditional VS methods, such as docking, are often too time-consuming\nfor screening large-scale molecular databases. Recent advances in deep learning\nhave demonstrated that learning vector representations for both proteins and\nmolecules using contrastive learning can outperform traditional docking\nmethods. However, given that target databases often contain billions of\nmolecules, real-valued vector representations adopted by existing methods can\nstill incur significant memory and time costs in VS. To address this problem,\nin this paper we propose a hashing-based contrastive learning method, called\nDrugHash, for VS. DrugHash treats VS as a retrieval task that uses efficient\nbinary hash codes for retrieval. In particular, DrugHash designs a simple yet\neffective hashing strategy to enable end-to-end learning of binary hash codes\nfor both protein and molecule modalities, which can dramatically reduce the\nmemory and time costs with higher accuracy compared with existing methods.\nExperimental results show that DrugHash can outperform existing methods to\nachieve state-of-the-art accuracy, with a memory saving of 32$\\times$ and a\nspeed improvement of 3.5$\\times$.",
      "tldr_zh": "该研究针对虚拟筛选（VS）中的效率问题，提出了一种基于哈希的对比学习方法DrugHash，以加速计算机辅助药物发现过程。DrugHash将VS视为检索任务，通过设计简单有效的哈希策略，实现蛋白质和分子模态的端到端二进制哈希码学习，从而显著降低内存和时间成本，同时保持高准确率。与现有方法相比，实验结果显示DrugHash在准确率上达到最先进水平，内存节省32倍，速度提升3.5倍。总的来说，这一方法为处理大规模分子数据库的药物筛选提供了高效解决方案。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19790v1",
      "published_date": "2024-07-29 08:33:49 UTC",
      "updated_date": "2024-07-29 08:33:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:05:52.124901"
    },
    {
      "arxiv_id": "2407.19784v1",
      "title": "Survey and Taxonomy: The Role of Data-Centric AI in Transformer-Based Time Series Forecasting",
      "title_zh": "翻译失败",
      "authors": [
        "Jingjing Xu",
        "Caesar Wu",
        "Yuan-Fang Li",
        "Gregoire Danoy",
        "Pascal Bouvry"
      ],
      "abstract": "Alongside the continuous process of improving AI performance through the\ndevelopment of more sophisticated models, researchers have also focused their\nattention to the emerging concept of data-centric AI, which emphasizes the\nimportant role of data in a systematic machine learning training process.\nNonetheless, the development of models has also continued apace. One result of\nthis progress is the development of the Transformer Architecture, which\npossesses a high level of capability in multiple domains such as Natural\nLanguage Processing (NLP), Computer Vision (CV) and Time Series Forecasting\n(TSF). Its performance is, however, heavily dependent on input data\npreprocessing and output data evaluation, justifying a data-centric approach to\nfuture research. We argue that data-centric AI is essential for training AI\nmodels, particularly for transformer-based TSF models efficiently. However,\nthere is a gap regarding the integration of transformer-based TSF and\ndata-centric AI. This survey aims to pin down this gap via the extensive\nliterature review based on the proposed taxonomy. We review the previous\nresearch works from a data-centric AI perspective and we intend to lay the\nfoundation work for the future development of transformer-based architecture\nand data-centric AI.",
      "tldr_zh": "该研究调查了 Data-Centric AI 在 Transformer-Based 时间序列预测（TSF）中的作用，强调数据质量在 AI 模型训练中的关键重要性，尤其是在 NLP、CV 和 TSF 等领域的 Transformer 架构应用。论文通过文献综述和提出的分类法（taxonomy），分析了现有研究，揭示了 Data-Centric AI 与 Transformer-Based TSF 整合的差距。最终，该工作为未来 Transformer 架构的优化和数据中心 AI 的发展奠定了基础。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19784v1",
      "published_date": "2024-07-29 08:27:21 UTC",
      "updated_date": "2024-07-29 08:27:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:06:04.346831"
    },
    {
      "arxiv_id": "2407.19778v1",
      "title": "Multimodal Large Language Models for Bioimage Analysis",
      "title_zh": "多模态大语言模型用于生物图像分析",
      "authors": [
        "Shanghang Zhang",
        "Gaole Dai",
        "Tiejun Huang",
        "Jianxu Chen"
      ],
      "abstract": "Rapid advancements in imaging techniques and analytical methods over the past\ndecade have revolutionized our ability to comprehensively probe the biological\nworld at multiple scales, pinpointing the type, quantity, location, and even\ntemporal dynamics of biomolecules. The surge in data complexity and volume\npresents significant challenges in translating this wealth of information into\nknowledge. The recently emerged Multimodal Large Language Models (MLLMs)\nexhibit strong emergent capacities, such as understanding, analyzing,\nreasoning, and generalization. With these capabilities, MLLMs hold promise to\nextract intricate information from biological images and data obtained through\nvarious modalities, thereby expediting our biological understanding and aiding\nin the development of novel computational frameworks. Previously, such\ncapabilities were mostly attributed to humans for interpreting and summarizing\nmeaningful conclusions from comprehensive observations and analysis of\nbiological images. However, the current development of MLLMs shows increasing\npromise in serving as intelligent assistants or agents for augmenting human\nresearchers in biology research",
      "tldr_zh": "本研究探讨了Multimodal Large Language Models (MLLMs) 在Bioimage Analysis 中的应用，旨在应对过去十年生物成像技术快速发展带来的数据复杂性和海量信息转化挑战。MLLMs 凭借其强大的新兴能力，包括理解、分析、推理和泛化，能够从各种模式的生物图像数据中提取复杂信息，从而加速生物知识的获取和新型计算框架的开发。相比以往依赖人类解读，MLLMs 正日益成为智能助手，提升生物研究的效率和深度。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19778v1",
      "published_date": "2024-07-29 08:21:25 UTC",
      "updated_date": "2024-07-29 08:21:25 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:06:16.192486"
    },
    {
      "arxiv_id": "2407.19775v1",
      "title": "Model Agnostic Hybrid Sharding For Heterogeneous Distributed Inference",
      "title_zh": "翻译失败",
      "authors": [
        "Claudio Angione",
        "Yue Zhao",
        "Harry Yang",
        "Ahmad Farhan",
        "Fielding Johnston",
        "James Buban",
        "Patrick Colangelo"
      ],
      "abstract": "The rapid growth of large-scale AI models, particularly large language models\nhas brought significant challenges in data privacy, computational resources,\nand accessibility. Traditional centralized architectures often struggle to meet\nrequired data security and scalability needs which hinders the democratization\nof AI systems. Nesa introduces a model-agnostic sharding framework designed for\ndecentralized AI inference. Our framework uses blockchain-based sequential deep\nneural network sharding to distribute computational tasks across a diverse\nnetwork of nodes based on a personalised heuristic and routing mechanism. This\nenables efficient distributed training and inference for recent large-scale\nmodels even on consumer-grade hardware. We use compression techniques like\ndynamic blockwise quantization and mixed matrix decomposition to reduce data\ntransfer and memory needs. We also integrate robust security measures,\nincluding hardware-based trusted execution environments to ensure data\nintegrity and confidentiality. Evaluating our system across various natural\nlanguage processing and vision tasks shows that these compression strategies do\nnot compromise model accuracy. Our results highlight the potential to\ndemocratize access to cutting-edge AI technologies by enabling secure and\nefficient inference on a decentralized network.",
      "tldr_zh": "这篇论文提出了一种模型无关的混合sharding框架Nesa，用于处理异构分布式AI推理的挑战，包括数据隐私、计算资源和可访问性问题。该框架利用区块链-based顺序深度神经网络sharding、个性化启发式机制和路由策略，将计算任务分布到去中心化网络中，并通过动态blockwise量化以及混合矩阵分解等压缩技术减少数据传输和内存需求，同时整合硬件-based受信执行环境确保数据完整性和保密性。在各种NLP和视觉任务的评估中，该系统维持了模型准确性，并展示了在消费级硬件上实现安全高效推理的潜力，从而促进AI技术的民主化。",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.DC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19775v1",
      "published_date": "2024-07-29 08:18:48 UTC",
      "updated_date": "2024-07-29 08:18:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:06:30.141893"
    },
    {
      "arxiv_id": "2407.19772v1",
      "title": "Generating Unseen Code Tests In Infinitum",
      "title_zh": "翻译失败",
      "authors": [
        "Marcel Zalmanovici",
        "Orna Raz",
        "Eitan Farchi",
        "Iftach Freund"
      ],
      "abstract": "Large Language Models (LLMs) are used for many tasks, including those related\nto coding. An important aspect of being able to utilize LLMs is the ability to\nassess their fitness for specific usages. The common practice is to evaluate\nLLMs against a set of benchmarks. While benchmarks provide a sound foundation\nfor evaluation and comparison of alternatives, they suffer from the well-known\nweakness of leaking into the training data \\cite{Xu2024Benchmarking}. We\npresent a method for creating benchmark variations that generalize across\ncoding tasks and programming languages, and may also be applied to in-house\ncode bases. Our approach enables ongoing generation of test-data thus\nmitigating the leaking into the training data issue. We implement one\nbenchmark, called \\textit{auto-regression}, for the task of text-to-code\ngeneration in Python. Auto-regression is specifically created to aid in\ndebugging and in tracking model generation changes as part of the LLM\nregression testing process.",
      "tldr_zh": "该研究针对 Large Language Models (LLMs) 在编码任务中的评估问题，指出传统基准测试可能泄露到训练数据中，导致评估偏差。论文提出了一种通用方法，用于创建基准测试变体，该方法适用于多种编码任务、编程语言以及内部代码库，并支持持续生成测试数据以缓解泄露风险。具体而言，他们实现了 auto-regression 基准，用于 Python 的文本到代码生成任务，帮助进行调试和跟踪模型生成的回归测试。实验结果表明，此方法提升了 LLMs 评估的可靠性和适用性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19772v1",
      "published_date": "2024-07-29 08:11:20 UTC",
      "updated_date": "2024-07-29 08:11:20 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:06:40.517986"
    },
    {
      "arxiv_id": "2407.19765v1",
      "title": "Map2Traj: Street Map Piloted Zero-shot Trajectory Generation with Diffusion Model",
      "title_zh": "翻译失败",
      "authors": [
        "Zhenyu Tao",
        "Wei Xu",
        "Xiaohu You"
      ],
      "abstract": "User mobility modeling serves a crucial role in analysis and optimization of\ncontemporary wireless networks. Typical stochastic mobility models, e.g.,\nrandom waypoint model and Gauss Markov model, can hardly capture the\ndistribution characteristics of users within real-world areas. State-of-the-art\ntrace-based mobility models and existing learning-based trajectory generation\nmethods, however, are frequently constrained by the inaccessibility of\nsubstantial real trajectories due to privacy concerns. In this paper, we\nharness the intrinsic correlation between street maps and trajectories and\ndevelop a novel zero-shot trajectory generation method, named Map2Traj, by\nexploiting the diffusion model. We incorporate street maps as a condition to\nconsistently pilot the denoising process and train our model on diverse sets of\nreal trajectories from various regions in Xi'an, China, and their corresponding\nstreet maps. With solely the street map of an unobserved area, Map2Traj\ngenerates synthetic trajectories that not only closely resemble the real-world\nmobility pattern but also offer comparable efficacy. Extensive experiments\nvalidate the efficacy of our proposed method on zero-shot trajectory generation\ntasks in terms of both trajectory and distribution similarities. In addition, a\ncase study of employing Map2Traj in wireless network optimization is presented\nto validate its efficacy for downstream applications.",
      "tldr_zh": "这篇论文提出了 Map2Traj，一种基于 diffusion model 的零-shot 轨迹生成方法，利用街道路图作为条件，解决传统随机模型（如 random waypoint model 和 Gauss Markov model）无法捕捉真实用户移动分布的问题，以及现有轨迹生成方法受隐私限制的挑战。研究团队通过在西安地区真实轨迹和对应街道路图上训练模型，使其能在未见区域仅凭街地图生成合成轨迹，这些轨迹在分布和相似性上与真实数据高度一致。实验验证了 Map2Traj 在零-shot 任务中的有效性，并通过无线网络优化案例研究，证明了其在下游应用中的实际价值。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19765v1",
      "published_date": "2024-07-29 07:57:03 UTC",
      "updated_date": "2024-07-29 07:57:03 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:06:54.604929"
    },
    {
      "arxiv_id": "2407.19740v1",
      "title": "KNOWCOMP POKEMON Team at DialAM-2024: A Two-Stage Pipeline for Detecting Relations in Dialogical Argument Mining",
      "title_zh": "翻译失败",
      "authors": [
        "Zihao Zheng",
        "Zhaowei Wang",
        "Qing Zong",
        "Yangqiu Song"
      ],
      "abstract": "Dialogical Argument Mining(DialAM) is an important branch of Argument\nMining(AM). DialAM-2024 is a shared task focusing on dialogical argument\nmining, which requires us to identify argumentative relations and illocutionary\nrelations among proposition nodes and locution nodes. To accomplish this, we\npropose a two-stage pipeline, which includes the Two-Step S-Node Prediction\nModel in Stage 1 and the YA-Node Prediction Model in Stage 2. We also augment\nthe training data in both stages and introduce context in Stage 2. We\nsuccessfully completed the task and achieved good results. Our team Pokemon\nranked 1st in the ARI Focused score and 4th in the Global Focused score.",
      "tldr_zh": "这篇论文针对 DialAM-2024 任务，提出一个两阶段管道，用于检测对话式论点挖掘（Dialogical Argument Mining）中命题节点和表述节点之间的论点关系和言外关系。管道包括第一阶段的 Two-Step S-Node Prediction Model 用于初步预测，以及第二阶段的 YA-Node Prediction Model 结合数据增强和上下文引入来进行进一步优化。该方法在任务中表现出色，KNOWCOMP POKEMON 团队在 ARI Focused score 上排名第一，在 Global Focused score 上排名第四。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Published on the 11th Workshop on Argument Mining",
      "pdf_url": "http://arxiv.org/pdf/2407.19740v1",
      "published_date": "2024-07-29 07:07:37 UTC",
      "updated_date": "2024-07-29 07:07:37 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:07:05.560964"
    },
    {
      "arxiv_id": "2408.07282v1",
      "title": "Consistency Based Weakly Self-Supervised Learning for Human Activity Recognition with Wearables",
      "title_zh": "基于一致性的弱自监督学习，用于可穿戴设备的人类活动识别",
      "authors": [
        "Taoran Sheng",
        "Manfred Huber"
      ],
      "abstract": "While the widely available embedded sensors in smartphones and other wearable\ndevices make it easier to obtain data of human activities, recognizing\ndifferent types of human activities from sensor-based data remains a difficult\nresearch topic in ubiquitous computing. One reason for this is that most of the\ncollected data is unlabeled. However, many current human activity recognition\n(HAR) systems are based on supervised methods, which heavily rely on the labels\nof the data. We describe a weakly self-supervised approach in this paper that\nconsists of two stages: (1) In stage one, the model learns from the nature of\nhuman activities by projecting the data into an embedding space where similar\nactivities are grouped together; (2) In stage two, the model is fine-tuned\nusing similarity information in a few-shot learning fashion using the\nsimilarity information of the data. This allows downstream classification or\nclustering tasks to benefit from the embeddings. Experiments on three benchmark\ndatasets demonstrate the framework's effectiveness and show that our approach\ncan help the clustering algorithm achieve comparable performance in identifying\nand categorizing the underlying human activities as pure supervised techniques\napplied directly to a corresponding fully labeled data set.",
      "tldr_zh": "该论文针对可穿戴设备传感器数据中标签缺失的问题，提出了一种基于一致性的弱自监督学习框架，用于人类活动识别(HAR)。框架分为两个阶段：第一阶段将数据投影到嵌入空间，使类似活动聚类在一起，学习活动本质；第二阶段利用少量样本的相似性信息进行微调，支持下游分类或聚类任务。实验在三个基准数据集上证明，该方法能与纯监督技术在完全标记数据集上的性能相当，显著提升了无标签数据的利用效率。",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2408.07282v1",
      "published_date": "2024-07-29 06:29:21 UTC",
      "updated_date": "2024-07-29 06:29:21 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:07:17.313894"
    },
    {
      "arxiv_id": "2407.19721v1",
      "title": "Rina: Enhancing Ring-AllReduce with In-network Aggregation in Distributed Model Training",
      "title_zh": "翻译失败",
      "authors": [
        "Zixuan Chen",
        "Xuandong Liu",
        "Minglin Li",
        "Yinfan Hu",
        "Hao Mei",
        "Huifeng Xing",
        "Hao Wang",
        "Wanxin Shi",
        "Sen Liu",
        "Yang Xu"
      ],
      "abstract": "Parameter Server (PS) and Ring-AllReduce (RAR) are two widely utilized\nsynchronization architectures in multi-worker Deep Learning (DL), also referred\nto as Distributed Deep Learning (DDL). However, PS encounters challenges with\nthe ``incast'' issue, while RAR struggles with problems caused by the long\ndependency chain. The emerging In-network Aggregation (INA) has been proposed\nto integrate with PS to mitigate its incast issue. However, such PS-based INA\nhas poor incremental deployment abilities as it requires replacing all the\nswitches to show significant performance improvement, which is not\ncost-effective. In this study, we present the incorporation of INA capabilities\ninto RAR, called RAR with In-Network Aggregation (Rina), to tackle both the\nproblems above. Rina features its agent-worker mechanism. When an INA-capable\nToR switch is deployed, all workers in this rack run as one abstracted worker\nwith the help of the agent, resulting in both excellent incremental deployment\ncapabilities and better throughput. We conducted extensive testbed and\nsimulation evaluations to substantiate the throughput advantages of Rina over\nexisting DDL training synchronization structures. Compared with the\nstate-of-the-art PS-based INA methods ATP, Rina can achieve more than 50\\%\nthroughput with the same hardware cost.",
      "tldr_zh": "本文提出 Rina 框架，将 In-network Aggregation (INA) 整合到 Ring-AllReduce (RAR) 中，以解决分布式深度学习中 Parameter Server (PS) 的 incast 问题和 RAR 的长依赖链问题。Rina 通过 agent-worker 机制，当部署 INA-capable ToR switch 时，将机架内的 workers 抽象为一个 worker，从而实现优秀的增量部署能力和吞吐量提升。实验评估表明，Rina 在相同硬件成本下，比现有 PS-based INA 方法 ATP 提高超过 50% 的吞吐量。",
      "categories": [
        "cs.NI",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.NI",
      "comment": "To appear in ICNP 2024. Preview version only",
      "pdf_url": "http://arxiv.org/pdf/2407.19721v1",
      "published_date": "2024-07-29 06:06:10 UTC",
      "updated_date": "2024-07-29 06:06:10 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:07:29.433932"
    },
    {
      "arxiv_id": "2407.19714v1",
      "title": "Rethinking RGB-D Fusion for Semantic Segmentation in Surgical Datasets",
      "title_zh": "重新审视 RGB-D 融合在手术数据集中的语义分割",
      "authors": [
        "Muhammad Abdullah Jamal",
        "Omid Mohareri"
      ],
      "abstract": "Surgical scene understanding is a key technical component for enabling\nintelligent and context aware systems that can transform various aspects of\nsurgical interventions. In this work, we focus on the semantic segmentation\ntask, propose a simple yet effective multi-modal (RGB and depth) training\nframework called SurgDepth, and show state-of-the-art (SOTA) results on all\npublicly available datasets applicable for this task. Unlike previous\napproaches, which either fine-tune SOTA segmentation models trained on natural\nimages, or encode RGB or RGB-D information using RGB only pre-trained\nbackbones, SurgDepth, which is built on top of Vision Transformers (ViTs), is\ndesigned to encode both RGB and depth information through a simple fusion\nmechanism. We conduct extensive experiments on benchmark datasets including\nEndoVis2022, AutoLapro, LapI2I and EndoVis2017 to verify the efficacy of\nSurgDepth. Specifically, SurgDepth achieves a new SOTA IoU of 0.86 on EndoVis\n2022 SAR-RARP50 challenge and outperforms the current best method by at least\n4%, using a shallow and compute efficient decoder consisting of ConvNeXt\nblocks.",
      "tldr_zh": "这篇论文重新审视了 RGB-D 融合在手术数据集语义分割任务中的应用，提出了一种简单有效的多模态训练框架 SurgDepth。SurgDepth 基于 Vision Transformers (ViTs)，通过一个简单的融合机制同时编码 RGB 和深度信息，与以往依赖自然图像预训练模型的方法不同。实验在多个基准数据集（如 EndoVis2022、AutoLapro、LapI2I 和 EndoVis2017）上验证了其效能，SurgDepth 实现了 SOTA 结果，例如在 EndoVis 2022 SAR-RARP50 挑战中 IoU 达到 0.86，比当前最佳方法至少提高 4%，并使用浅层计算高效的 ConvNeXt blocks 作为解码器。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19714v1",
      "published_date": "2024-07-29 05:35:51 UTC",
      "updated_date": "2024-07-29 05:35:51 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:07:43.774937"
    },
    {
      "arxiv_id": "2407.19705v3",
      "title": "CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare",
      "title_zh": "翻译失败",
      "authors": [
        "Jingwei Zhu",
        "Minghuan Tan",
        "Min Yang",
        "Ruixue Li",
        "Hamid Alinejad-Rokny"
      ],
      "abstract": "The rapid progress in Large Language Models (LLMs) has prompted the creation\nof numerous benchmarks to evaluate their capabilities.This study focuses on the\nComprehensive Medical Benchmark in Chinese (CMB), showcasing how dataset\ndiversity and distribution in supervised fine-tuning (SFT) may enhance LLM\nperformance.Remarkably, We successfully trained a smaller base model to achieve\nscores comparable to larger models, indicating that a diverse and\nwell-distributed dataset can optimize performance regardless of model size.This\nstudy suggests that even smaller models may reach high performance levels with\ncarefully curated and varied datasets. By integrating a wide range of\ninstructional content, our approach addresses potential issues such as data\nquality inconsistencies. Our results imply that a broader spectrum of training\ndata may enhance a model's ability to generalize and perform effectively across\ndifferent medical scenarios, highlighting the importance of dataset quality and\ndiversity in fine-tuning processes. We open-source the model for future\nresearch at https://github.com/CAS-SIAT-XinHai/CollectiveSFT",
      "tldr_zh": "这篇论文介绍了 CollectiveSFT 方法，通过使用多样化和良好分布的指令数据集，对 Large Language Models (LLMs) 进行 Supervised Fine-Tuning (SFT)，以提升其在中文医疗基准 (CMB) 上的性能。研究发现，即使是较小的基模型，也能通过精心策划的多样化数据集达到与大型模型相当的得分，从而解决数据质量不一致等问题。结果表明，这种方法能增强模型的泛化能力和在不同医疗场景中的表现，并开源了模型以促进进一步研究。",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Technical Report",
      "pdf_url": "http://arxiv.org/pdf/2407.19705v3",
      "published_date": "2024-07-29 05:00:48 UTC",
      "updated_date": "2024-09-28 02:05:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:07:53.433877"
    },
    {
      "arxiv_id": "2407.20301v1",
      "title": "Legal Aspects of Decentralized and Platform-Driven Economies",
      "title_zh": "去中心化及平台驱动经济体的法律方面",
      "authors": [
        "Marcelo Corrales Compagnucci",
        "Toshiyuki Kono",
        "Shinto Teramoto"
      ],
      "abstract": "The sharing economy is sprawling across almost every sector and activity\naround the world. About a decade ago, there were only a handful of platform\ndriven companies operating on the market. Zipcar, BlaBlaCar and Couchsurfing\namong them. Then Airbnb and Uber revolutionized the transportation and\nhospitality industries with a presence in virtually every major city. Access\nover ownership is the paradigm shift from the traditional business model that\ngrants individuals the use of products or services without the necessity of\nbuying them. Digital platforms, data and algorithm-driven companies as well as\ndecentralized blockchain technologies have tremendous potential. But they are\nalso changing the rules of the game. One of such technologies challenging the\nlegal system are AI systems that will also reshape the current legal framework\nconcerning the liability of operators, users and manufacturers. Therefore, this\nintroductory chapter deals with explaining and describing the legal issues of\nsome of these disruptive technologies. The chapter argues for a more\nforward-thinking and flexible regulatory structure.",
      "tldr_zh": "本文探讨了分散化和平台驱动经济体的法律方面，特别是共享经济（sharing economy）的兴起及其对传统法律框架的挑战。从早期的Zipcar、BlaBlaCar到如今的Airbnb和Uber，这些技术推动了从拥有到访问（access over ownership）的范式转变。数字平台、数据驱动公司以及decentralized blockchain technologies和AI systems 可能引发责任分配问题，如运营商、用户和制造商的liability。作者主张采用更前瞻和灵活的监管结构，以应对这些破坏性技术的法律影响。",
      "categories": [
        "cs.CY",
        "cs.AI",
        "econ.GN",
        "q-fin.EC"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.20301v1",
      "published_date": "2024-07-29 04:42:49 UTC",
      "updated_date": "2024-07-29 04:42:49 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:08:05.731603"
    },
    {
      "arxiv_id": "2407.19697v2",
      "title": "Multiscale Representation Enhanced Temporal Flow Fusion Model for Long-Term Workload Forecasting",
      "title_zh": "多尺度表示增强的时间流融合模型用于",
      "authors": [
        "Shiyu Wang",
        "Zhixuan Chu",
        "Yinbo Sun",
        "Yu Liu",
        "Yuliang Guo",
        "Yang Chen",
        "Huiyang Jian",
        "Lintao Ma",
        "Xingyu Lu",
        "Jun Zhou"
      ],
      "abstract": "Accurate workload forecasting is critical for efficient resource management\nin cloud computing systems, enabling effective scheduling and autoscaling.\nDespite recent advances with transformer-based forecasting models, challenges\nremain due to the non-stationary, nonlinear characteristics of workload time\nseries and the long-term dependencies. In particular, inconsistent performance\nbetween long-term history and near-term forecasts hinders long-range\npredictions. This paper proposes a novel framework leveraging self-supervised\nmultiscale representation learning to capture both long-term and near-term\nworkload patterns. The long-term history is encoded through multiscale\nrepresentations while the near-term observations are modeled via temporal flow\nfusion. These representations of different scales are fused using an attention\nmechanism and characterized with normalizing flows to handle\nnon-Gaussian/non-linear distributions of time series. Extensive experiments on\n9 benchmarks demonstrate superiority over existing methods.",
      "tldr_zh": "该论文针对云计算系统中工作负载预测的非平稳、非线性特性和长期依赖问题，提出了一种新型框架，以提升长期预测的准确性。该框架采用自监督的多尺度表示学习来捕捉长期历史和短期观察模式，并通过时间流融合(temporal flow fusion)和注意力机制将不同尺度的表示进行融合，同时使用归一化流(normalizing flows)处理非高斯/非线性分布。在9个基准实验中，该方法显著优于现有基于Transformer的模型。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Proceedings of the 33rd ACM International Conference on Information\n  and Knowledge Management (CIKM '24), October 21--25, 2024, Boise, ID, USA",
      "pdf_url": "http://arxiv.org/pdf/2407.19697v2",
      "published_date": "2024-07-29 04:42:18 UTC",
      "updated_date": "2024-08-19 02:13:57 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:08:28.189733"
    },
    {
      "arxiv_id": "2407.20299v2",
      "title": "Dataset Distillation for Offline Reinforcement Learning",
      "title_zh": "翻译失败",
      "authors": [
        "Jonathan Light",
        "Yuanzhe Liu",
        "Ziniu Hu"
      ],
      "abstract": "Offline reinforcement learning often requires a quality dataset that we can\ntrain a policy on. However, in many situations, it is not possible to get such\na dataset, nor is it easy to train a policy to perform well in the actual\nenvironment given the offline data. We propose using data distillation to train\nand distill a better dataset which can then be used for training a better\npolicy model. We show that our method is able to synthesize a dataset where a\nmodel trained on it achieves similar performance to a model trained on the full\ndataset or a model trained using percentile behavioral cloning. Our project\nsite is available at\n$\\href{https://datasetdistillation4rl.github.io}{\\text{here}}$. We also provide\nour implementation at $\\href{https://github.com/ggflow123/DDRL}{\\text{this\nGitHub repository}}$.",
      "tldr_zh": "本研究针对 Offline Reinforcement Learning 中数据集质量问题，提出使用 Data Distillation 方法来训练和提炼更好的数据集，从而提升策略模型的性能。该方法能够合成一个高质量数据集，使得在该数据集上训练的模型达到与使用完整数据集或 Percentile Behavioral Cloning 相当的水平。实验结果证明了该方法的有效性，并提供了项目网站和 GitHub 仓库以供进一步参考。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "ICML 2024 DMLR Workshop",
      "pdf_url": "http://arxiv.org/pdf/2407.20299v2",
      "published_date": "2024-07-29 04:02:17 UTC",
      "updated_date": "2024-08-01 01:33:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:08:29.125203"
    },
    {
      "arxiv_id": "2407.19683v1",
      "title": "Revisiting the robustness of post-hoc interpretability methods",
      "title_zh": "重新审视后验解释性方法的鲁棒性",
      "authors": [
        "Jiawen Wei",
        "Hugues Turbé",
        "Gianmarco Mengaldo"
      ],
      "abstract": "Post-hoc interpretability methods play a critical role in explainable\nartificial intelligence (XAI), as they pinpoint portions of data that a trained\ndeep learning model deemed important to make a decision. However, different\npost-hoc interpretability methods often provide different results, casting\ndoubts on their accuracy. For this reason, several evaluation strategies have\nbeen proposed to understand the accuracy of post-hoc interpretability. Many of\nthese evaluation strategies provide a coarse-grained assessment -- i.e., they\nevaluate how the performance of the model degrades on average by corrupting\ndifferent data points across multiple samples. While these strategies are\neffective in selecting the post-hoc interpretability method that is most\nreliable on average, they fail to provide a sample-level, also referred to as\nfine-grained, assessment. In other words, they do not measure the robustness of\npost-hoc interpretability methods. We propose an approach and two new metrics\nto provide a fine-grained assessment of post-hoc interpretability methods. We\nshow that the robustness is generally linked to its coarse-grained performance.",
      "tldr_zh": "这篇论文重新审视了 post-hoc interpretability methods 在 explainable artificial intelligence (XAI) 中的鲁棒性问题，这些方法虽然能识别深度学习模型决策的重要数据部分，但结果往往不一致，导致准确性存疑。现有评估策略主要采用粗粒度方法，通过评估模型在多个样本上平均性能下降来选择可靠的方法，但无法提供样本级（fine-grained）的评估。论文提出了一种新方法和两个新指标，用于细粒度评估 post-hoc interpretability methods 的鲁棒性，并发现其鲁棒性通常与粗粒度性能密切相关。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19683v1",
      "published_date": "2024-07-29 03:55:52 UTC",
      "updated_date": "2024-07-29 03:55:52 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:08:53.774618"
    },
    {
      "arxiv_id": "2407.19681v3",
      "title": "Motion Manifold Flow Primitives for Task-Conditioned Trajectory Generation under Complex Task-Motion Dependencies",
      "title_zh": "翻译失败",
      "authors": [
        "Yonghyeon Lee",
        "Byeongho Lee",
        "Seungyeon Kim",
        "Frank C. Park"
      ],
      "abstract": "Effective movement primitives should be capable of encoding and generating a\nrich repertoire of trajectories -- typically collected from human\ndemonstrations -- conditioned on task-defining parameters such as vision or\nlanguage inputs. While recent methods based on the motion manifold hypothesis,\nwhich assumes that a set of trajectories lies on a lower-dimensional nonlinear\nsubspace, address challenges such as limited dataset size and the high\ndimensionality of trajectory data, they often struggle to capture complex\ntask-motion dependencies, i.e., when motion distributions shift drastically\nwith task variations. To address this, we introduce Motion Manifold Flow\nPrimitives (MMFP), a framework that decouples the training of the motion\nmanifold from task-conditioned distributions. Specifically, we employ flow\nmatching models, state-of-the-art conditional deep generative models, to learn\ntask-conditioned distributions in the latent coordinate space of the learned\nmotion manifold. Experiments are conducted on language-guided trajectory\ngeneration tasks, where many-to-many text-motion correspondences introduce\ncomplex task-motion dependencies, highlighting MMFP's superiority over existing\nmethods.",
      "tldr_zh": "本文提出 Motion Manifold Flow Primitives (MMFP) 框架，用于处理复杂任务-运动依赖性的轨迹生成问题。MMFP 通过将运动流形的训练与任务条件分布解耦，使用 flow matching models 在运动流形的潜在坐标空间中学习任务相关的分布，从而更好地捕捉任务变化导致的运动分布剧烈变化。实验在语言引导的轨迹生成任务上进行，结果表明 MMFP 优于现有方法，尤其在处理多对多文本-运动对应关系时表现出色。",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 11 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.19681v3",
      "published_date": "2024-07-29 03:53:14 UTC",
      "updated_date": "2025-01-08 06:56:19 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:09:11.098129"
    },
    {
      "arxiv_id": "2407.19679v1",
      "title": "Harnessing Large Vision and Language Models in Agriculture: A Review",
      "title_zh": "翻译失败",
      "authors": [
        "Hongyan Zhu",
        "Shuai Qin",
        "Min Su",
        "Chengzhi Lin",
        "Anjie Li",
        "Junfeng Gao"
      ],
      "abstract": "Large models can play important roles in many domains. Agriculture is another\nkey factor affecting the lives of people around the world. It provides food,\nfabric, and coal for humanity. However, facing many challenges such as pests\nand diseases, soil degradation, global warming, and food security, how to\nsteadily increase the yield in the agricultural sector is a problem that humans\nstill need to solve. Large models can help farmers improve production\nefficiency and harvest by detecting a series of agricultural production tasks\nsuch as pests and diseases, soil quality, and seed quality. It can also help\nfarmers make wise decisions through a variety of information, such as images,\ntext, etc. Herein, we delve into the potential applications of large models in\nagriculture, from large language model (LLM) and large vision model (LVM) to\nlarge vision-language models (LVLM). After gaining a deeper understanding of\nmultimodal large language models (MLLM), it can be recognized that problems\nsuch as agricultural image processing, agricultural question answering systems,\nand agricultural machine automation can all be solved by large models. Large\nmodels have great potential in the field of agriculture. We outline the current\napplications of agricultural large models, and aims to emphasize the importance\nof large models in the domain of agriculture. In the end, we envisage a future\nin which famers use MLLM to accomplish many tasks in agriculture, which can\ngreatly improve agricultural production efficiency and yield.",
      "tldr_zh": "本综述探讨了Large Vision and Language Models在农业领域的潜力应用，包括Large Language Model (LLM)、Large Vision Model (LVM)和Large Vision-Language Models (LVLM)，旨在解决病虫害、土壤退化和食品安全等挑战。论文分析了这些模型如何通过处理农业图像、构建问题解答系统和实现机器自动化，帮助农民提高生产效率和决策智慧。研究强调，Multimodal Large Language Models (MLLM)未来可大幅提升农业产量，并概述了当前应用，展望了农民利用这些技术实现高效农业的愿景。",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19679v1",
      "published_date": "2024-07-29 03:47:54 UTC",
      "updated_date": "2024-07-29 03:47:54 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:09:22.187289"
    },
    {
      "arxiv_id": "2407.21072v1",
      "title": "Beyond Metrics: A Critical Analysis of the Variability in Large Language Model Evaluation Frameworks",
      "title_zh": "超越",
      "authors": [
        "Marco AF Pimentel",
        "Clément Christophe",
        "Tathagata Raha",
        "Prateek Munjal",
        "Praveen K Kanithi",
        "Shadab Khan"
      ],
      "abstract": "As large language models (LLMs) continue to evolve, the need for robust and\nstandardized evaluation benchmarks becomes paramount. Evaluating the\nperformance of these models is a complex challenge that requires careful\nconsideration of various linguistic tasks, model architectures, and\nbenchmarking methodologies. In recent years, various frameworks have emerged as\nnoteworthy contributions to the field, offering comprehensive evaluation tests\nand benchmarks for assessing the capabilities of LLMs across diverse domains.\nThis paper provides an exploration and critical analysis of some of these\nevaluation methodologies, shedding light on their strengths, limitations, and\nimpact on advancing the state-of-the-art in natural language processing.",
      "tldr_zh": "这篇论文对大型语言模型(LLMs)评估框架的变异性进行了批判性分析，强调了在不同语言任务、模型架构和基准测试方法中建立标准化评估基准的必要性。论文探讨了各种新兴评估框架的优缺点，包括它们在评估LLMs能力方面的综合测试和跨领域应用。最终，研究揭示了这些框架如何推动自然语言处理领域的进步，同时指出了潜在局限性，以期改进未来评估方法。",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "15 pages, 3 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.21072v1",
      "published_date": "2024-07-29 03:37:14 UTC",
      "updated_date": "2024-07-29 03:37:14 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:09:42.806666"
    },
    {
      "arxiv_id": "2407.19668v1",
      "title": "Urban Traffic Accident Risk Prediction Revisited: Regionality, Proximity, Similarity and Sparsity",
      "title_zh": "翻译失败",
      "authors": [
        "Minxiao Chen",
        "Haitao Yuan",
        "Nan Jiang",
        "Zhifeng Bao",
        "Shangguang Wang"
      ],
      "abstract": "Traffic accidents pose a significant risk to human health and property\nsafety. Therefore, to prevent traffic accidents, predicting their risks has\ngarnered growing interest. We argue that a desired prediction solution should\ndemonstrate resilience to the complexity of traffic accidents. In particular,\nit should adequately consider the regional background, accurately capture both\nspatial proximity and semantic similarity, and effectively address the sparsity\nof traffic accidents. However, these factors are often overlooked or difficult\nto incorporate. In this paper, we propose a novel multi-granularity\nhierarchical spatio-temporal network. Initially, we innovate by incorporating\nremote sensing data, facilitating the creation of hierarchical\nmulti-granularity structure and the comprehension of regional background. We\nconstruct multiple high-level risk prediction tasks to enhance model's ability\nto cope with sparsity. Subsequently, to capture both spatial proximity and\nsemantic similarity, region feature and multi-view graph undergo encoding\nprocesses to distill effective representations. Additionally, we propose\nmessage passing and adaptive temporal attention module that bridges different\ngranularities and dynamically captures time correlations inherent in traffic\naccident patterns. At last, a multivariate hierarchical loss function is\ndevised considering the complexity of the prediction purpose. Extensive\nexperiments on two real datasets verify the superiority of our model against\nthe state-of-the-art methods.",
      "tldr_zh": "这篇论文重新审视了城市交通事故风险预测，强调了区域背景、空间邻近性（proximity）、语义相似性（semantic similarity）和数据稀疏性（sparsity）等因素。作者提出了一种新型多粒度层次时空网络（multi-granularity hierarchical spatio-temporal network），通过整合遥感数据（remote sensing data）构建层次结构并理解区域背景，同时构建多个高层风险预测任务来有效应对数据稀疏性。该网络采用区域特征编码和多视图图来捕捉空间邻近性和语义相似性，并引入消息传递和自适应时间注意力模块（message passing and adaptive temporal attention module）来桥接不同粒度和动态捕捉时间相关性。最后，使用多变量层次损失函数（multivariate hierarchical loss function）优化模型，在两个真实数据集上实验验证其比最先进方法表现出色。",
      "categories": [
        "cs.DB",
        "cs.AI"
      ],
      "primary_category": "cs.DB",
      "comment": "Accepted by CIKM 2024",
      "pdf_url": "http://arxiv.org/pdf/2407.19668v1",
      "published_date": "2024-07-29 03:10:15 UTC",
      "updated_date": "2024-07-29 03:10:15 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:09:45.582822"
    },
    {
      "arxiv_id": "2407.19667v1",
      "title": "Smart Language Agents in Real-World Planning",
      "title_zh": "智能语言代理在真实世界规划中",
      "authors": [
        "Annabelle Miin",
        "Timothy Wei"
      ],
      "abstract": "Comprehensive planning agents have been a long term goal in the field of\nartificial intelligence. Recent innovations in Natural Language Processing have\nyielded success through the advent of Large Language Models (LLMs). We seek to\nimprove the travel-planning capability of such LLMs by extending upon the work\nof the previous paper TravelPlanner. Our objective is to explore a new method\nof using LLMs to improve the travel planning experience. We focus specifically\non the \"sole-planning\" mode of travel planning; that is, the agent is given\nnecessary reference information, and its goal is to create a comprehensive plan\nfrom the reference information. While this does not simulate the real-world we\nfeel that an optimization of the sole-planning capability of a travel planning\nagent will still be able to enhance the overall user experience. We propose a\nsemi-automated prompt generation framework which combines the LLM-automated\nprompt and \"human-in-the-loop\" to iteratively refine the prompt to improve the\nLLM performance. Our result shows that LLM automated prompt has its limitations\nand \"human-in-the-loop\" greatly improves the performance by $139\\%$ with one\nsingle iteration.",
      "tldr_zh": "本研究旨在提升大型语言模型（LLMs）在旅行规划中的能力，基于先前TravelPlanner的工作，聚焦于“sole-planning”模式，即代理利用提供的参考信息生成全面计划。研究提出了一种半自动化提示生成框架，结合LLMs自动生成提示和“human-in-the-loop”人工迭代机制，以逐步优化提示效果。结果显示，LLMs自动提示存在局限性，通过单次人工迭代，性能提升了139%，从而改善了整体用户体验。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "5 pages, 1 figure",
      "pdf_url": "http://arxiv.org/pdf/2407.19667v1",
      "published_date": "2024-07-29 03:00:30 UTC",
      "updated_date": "2024-07-29 03:00:30 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:10:03.593603"
    },
    {
      "arxiv_id": "2407.19655v2",
      "title": "AI-Driven Healthcare: A Review on Ensuring Fairness and Mitigating Bias",
      "title_zh": "AI驱动的医疗保健：确保公平性和缓解偏差的综述",
      "authors": [
        "Sribala Vidyadhari Chinta",
        "Zichong Wang",
        "Avash Palikhe",
        "Xingyu Zhang",
        "Ayesha Kashif",
        "Monique Antoinette Smith",
        "Jun Liu",
        "Wenbin Zhang"
      ],
      "abstract": "Artificial intelligence (AI) is rapidly advancing in healthcare, enhancing\nthe efficiency and effectiveness of services across various specialties,\nincluding cardiology, ophthalmology, dermatology, emergency medicine, etc. AI\napplications have significantly improved diagnostic accuracy, treatment\npersonalization, and patient outcome predictions by leveraging technologies\nsuch as machine learning, neural networks, and natural language processing.\nHowever, these advancements also introduce substantial ethical and fairness\nchallenges, particularly related to biases in data and algorithms. These biases\ncan lead to disparities in healthcare delivery, affecting diagnostic accuracy\nand treatment outcomes across different demographic groups. This review paper\nexamines the integration of AI in healthcare, highlighting critical challenges\nrelated to bias and exploring strategies for mitigation. We emphasize the\nnecessity of diverse datasets, fairness-aware algorithms, and regulatory\nframeworks to ensure equitable healthcare delivery. The paper concludes with\nrecommendations for future research, advocating for interdisciplinary\napproaches, transparency in AI decision-making, and the development of\ninnovative and inclusive AI applications.",
      "tldr_zh": "这篇综述论文探讨了人工智能（AI）在医疗领域的应用，如通过机器学习（machine learning）、神经网络（neural networks）和自然语言处理（natural language processing）提升诊断准确性、个性化治疗和患者预后预测。论文强调AI带来的伦理挑战，包括数据和算法偏见（biases），这些可能导致不同人群的医疗不平等。作者审视了缓解策略，如采用多样化数据集（diverse datasets）、公平感知算法（fairness-aware algorithms）和监管框架（regulatory frameworks），并推荐未来研究聚焦跨学科方法、AI决策透明度和创新包容性应用，以确保公平医疗。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by PLOS digital health",
      "pdf_url": "http://arxiv.org/pdf/2407.19655v2",
      "published_date": "2024-07-29 02:39:17 UTC",
      "updated_date": "2025-05-03 15:15:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:10:15.903660"
    },
    {
      "arxiv_id": "2407.19646v1",
      "title": "Foundations for Unfairness in Anomaly Detection -- Case Studies in Facial Imaging Data",
      "title_zh": "异常检测中不公平性的基础——面部成像数据的案例研究",
      "authors": [
        "Michael Livanos",
        "Ian Davidson"
      ],
      "abstract": "Deep anomaly detection (AD) is perhaps the most controversial of data\nanalytic tasks as it identifies entities that are then specifically targeted\nfor further investigation or exclusion. Also controversial is the application\nof AI to facial imaging data. This work explores the intersection of these two\nareas to understand two core questions: \"Who\" these algorithms are being unfair\nto and equally important \"Why\". Recent work has shown that deep AD can be\nunfair to different groups despite being unsupervised with a recent study\nshowing that for portraits of people: men of color are far more likely to be\nchosen to be outliers. We study the two main categories of AD algorithms:\nautoencoder-based and single-class-based which effectively try to compress all\nthe instances with those that can not be easily compressed being deemed to be\noutliers. We experimentally verify sources of unfairness such as the\nunder-representation of a group (e.g. people of color are relatively rare),\nspurious group features (e.g. men are often photographed with hats), and group\nlabeling noise (e.g. race is subjective). We conjecture that lack of\ncompressibility is the main foundation and the others cause it but experimental\nresults show otherwise and we present a natural hierarchy amongst them.",
      "tldr_zh": "这篇论文探讨了深度异常检测（AD）在面部图像数据中的不公平问题，焦点在于算法对特定群体（如男性有色人种）的偏见，以及背后的原因。研究者分析了两种主要AD算法——基于自编码器和单类别的方法——这些算法通过数据压缩来识别异常，无法被轻松压缩的实例被视为异常。实验验证了不公平的来源，包括群体 underrepresented（如有色人种较少）、spurious group features（如男性常戴帽子）和 group labeling noise（如种族主观性）。结果显示，虽然缺乏可压缩性被视为潜在基础，但其他因素更为关键，并揭示了这些来源之间的自然层次，为缓解AD算法偏见提供了重要见解。",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "16 pages, 8 figures, AAAI/ACM AIES24",
      "pdf_url": "http://arxiv.org/pdf/2407.19646v1",
      "published_date": "2024-07-29 02:04:29 UTC",
      "updated_date": "2024-07-29 02:04:29 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:10:20.987370"
    },
    {
      "arxiv_id": "2407.19644v1",
      "title": "Realizing Unaligned Block-wise Pruning for DNN Acceleration on Mobile Devices",
      "title_zh": "翻译失败",
      "authors": [
        "Hayun Lee",
        "Dongkun Shin"
      ],
      "abstract": "With the recent proliferation of on-device AI, there is an increasing need to\nrun computationally intensive DNNs directly on mobile devices. However, the\nlimited computing and memory resources of these devices necessitate effective\npruning techniques. Block-wise pruning is promising due to its low accuracy\ndrop tradeoff for speedup gains, but it requires block positions to be aligned\nwith block size, hindering optimal position selection to minimize model\naccuracy drop. Unaligned block pruning (UBP) addresses this by allowing blocks\nto be selected at arbitrary positions, yet its practical use is limited by a\ntime-consuming optimal block selection algorithm and lack of efficient\ninference kernels. In this paper, we propose a pseudo-optimal yet fast block\nselection algorithm called Block Expansion and Division (BED), which can be\nintegrated into an iterative model training process. Additionally, we introduce\nan efficient inference kernel implementation for mobile devices, enabling a\nUBP-based model to achieve similar latency to a DNN model compressed by aligned\nblock pruning. We demonstrate the superiority of our techniques on a real\nmobile phone with MobileNet and ResNet models.",
      "tldr_zh": "该论文针对移动设备上运行计算密集型DNN的资源限制，提出了未对齐块状修剪（Unaligned Block-wise Pruning, UBP）技术的实现，以优化模型加速。作者开发了Block Expansion and Division (BED)算法，这是一种伪最优且快速的块选择方法，可无缝整合到迭代模型训练过程中。论文还引入了高效的推理内核，使UBP模型在移动设备上的延迟与传统对齐块状修剪相当。实验在真实手机上使用MobileNet和ResNet模型验证了该方法的优越性，提高了DNN加速的效率和实用性。",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "11 pages, 8 figures",
      "pdf_url": "http://arxiv.org/pdf/2407.19644v1",
      "published_date": "2024-07-29 01:59:06 UTC",
      "updated_date": "2024-07-29 01:59:06 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:10:31.409172"
    },
    {
      "arxiv_id": "2407.19643v2",
      "title": "Prometheus Chatbot: Knowledge Graph Collaborative Large Language Model for Computer Components Recommendation",
      "title_zh": "翻译失败",
      "authors": [
        "Yunsheng Wang",
        "Songhao Chen",
        "Kevin Jin"
      ],
      "abstract": "Knowledge graphs (KGs) are essential in applications such as network\nalignment, question-answering, and recommender systems (RSs) since they offer\nstructured relational data that facilitate the inference of indirect\nrelationships. However, the development of KG-based RSs capable of processing\nuser inputs in natural language faces significant challenges. Firstly, natural\nlanguage processing units must effectively handle the ambiguity and variability\nin human language to interpret user intents accurately. Secondly, the system\nmust precisely identify and link entities, like product names, to their\ncorresponding nodes in KGs. To overcome these challenges, supported by Lenovo,\nwe developed a novel chatbot called \"Prometheus,\" which integrates a KG with a\nlarge language model (LLM), specifically designed for recommending computer\ncomponents. This chatbot can accurately decode user requests and deliver\npersonalized recommendations derived from KGs, ensuring precise comprehension\nand response to their computer setup needs.",
      "tldr_zh": "该研究开发了名为 Prometheus 的聊天机器人，将知识图谱（KGs）和大型语言模型（LLM）相结合，旨在解决基于 KGs 的推荐系统在处理自然语言输入时的挑战，如语言模糊性和实体链接问题。Prometheus 专为计算机组件推荐设计，能够准确解析用户意图并将产品实体（如组件名称）链接到 KGs 中，提供个性化的推荐响应。支持该项目的联想公司表明，该系统在理解用户计算机配置需求方面表现出色，确保了精确性和实用性。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19643v2",
      "published_date": "2024-07-29 01:57:10 UTC",
      "updated_date": "2024-07-31 03:20:35 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:10:42.158532"
    },
    {
      "arxiv_id": "2407.19633v2",
      "title": "OptiMUS-0.3: Using Large Language Models to Model and Solve Optimization Problems at Scale",
      "title_zh": "翻译失败",
      "authors": [
        "Ali AhmadiTeshnizi",
        "Wenzhi Gao",
        "Herman Brunborg",
        "Shayan Talaei",
        "Connor Lawless",
        "Madeleine Udell"
      ],
      "abstract": "Optimization problems are pervasive in sectors from manufacturing and\ndistribution to healthcare. However, most such problems are still solved\nheuristically by hand rather than optimally by state-of-the art solvers because\nthe expertise required to formulate and solve these problems limits the\nwidespread adoption of optimization tools and techniques. We introduce a Large\nLanguage Model (LLM)-based system designed to formulate and solve (mixed\ninteger) linear programming problems from their natural language descriptions.\nOur system is capable of developing mathematical models, writing and debugging\nsolver code, evaluating the generated solutions, and improving efficiency and\ncorrectness of its model and code based on these evaluations. OptiMUS-0.3\nutilizes a modular structure to process problems, allowing it to handle\nproblems with long descriptions and complex data without long prompts.\nExperiments demonstrate that OptiMUS-0.3 outperforms existing state-of-the-art\nmethods on easy datasets by more than 12% and on hard datasets (including a new\ndataset, NLP4LP, released with this paper that features long and complex\nproblems) by more than 8%.",
      "tldr_zh": "这篇论文介绍了 OptiMUS-0.3，一个基于 Large Language Model (LLM) 的系统，旨在从自然语言描述中自动制定和求解（混合整数）线性规划问题，以解决优化问题在实际应用中的专业知识障碍。系统采用模块化结构，包括开发数学模型、编写和调试求解器代码、评估解决方案以及基于反馈改进效率和正确性，从而处理长描述和复杂数据的挑战。实验结果显示，OptiMUS-0.3 在简单数据集上比现有最先进方法提升超过12%，而在困难数据集（如新发布的 NLP4LP 数据集）上提升超过8%。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "This paper documents OptiMUS-0.3, improving on OptiMUS-0.1\n  (arXiv:2310.06116) and OptiMUS-0.2 (arXiv:2402.10172). arXiv admin note: text\n  overlap with arXiv:2402.10172",
      "pdf_url": "http://arxiv.org/pdf/2407.19633v2",
      "published_date": "2024-07-29 01:31:45 UTC",
      "updated_date": "2025-02-14 22:41:16 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:10:54.862250"
    },
    {
      "arxiv_id": "2407.19631v3",
      "title": "\"A Good Bot Always Knows Its Limitations\": Assessing Autonomous System Decision-making Competencies through Factorized Machine Self-confidence",
      "title_zh": "翻译失败",
      "authors": [
        "Brett W. Israelsen",
        "Nisar R. Ahmed",
        "Matthew Aitken",
        "Eric W. Frew",
        "Dale A. Lawrence",
        "Brian M. Argrow"
      ],
      "abstract": "How can intelligent machines assess their competency to complete a task? This\nquestion has come into focus for autonomous systems that algorithmically make\ndecisions under uncertainty. We argue that machine self-confidence -- a form of\nmeta-reasoning based on self-assessments of system knowledge about the state of\nthe world, itself, and ability to reason about and execute tasks -- leads to\nmany computable and useful competency indicators for such agents. This paper\npresents our body of work, so far, on this concept in the form of the\nFactorized Machine Self-confidence (FaMSeC) framework, which holistically\nconsiders several major factors driving competency in algorithmic\ndecision-making: outcome assessment, solver quality, model quality, alignment\nquality, and past experience. In FaMSeC, self-confidence indicators are derived\nvia 'problem-solving statistics' embedded in Markov decision process solvers\nand related approaches. These statistics come from evaluating probabilistic\nexceedance margins in relation to certain outcomes and associated competency\nstandards specified by an evaluator. Once designed, and evaluated, the\nstatistics can be easily incorporated into autonomous agents and serve as\nindicators of competency. We include detailed descriptions and examples for\nMarkov decision process agents, and show how outcome assessment and solver\nquality factors can be found for a range of tasking contexts through novel use\nof meta-utility functions, behavior simulations, and surrogate prediction\nmodels. Numerical evaluations are performed to demonstrate that FaMSeC\nindicators perform as desired (references to human subject studies beyond the\nscope of this paper are provided).",
      "tldr_zh": "该研究探讨了智能机器如何评估自身完成任务的能力，特别是针对不确定性下的自主系统决策。作者提出 Factorized Machine Self-confidence (FaMSeC) 框架，该框架通过整合 outcome assessment、solver quality、model quality、alignment quality 和 past experience 等因素，基于 Markov decision process solvers 派生自我信心指标，以量化机器的决策能力。FaMSeC 利用 meta-utility functions、behavior simulations 和 surrogate prediction models 等方法进行评估，数值实验证明这些指标能有效反映系统能力，为构建可信赖的自主代理提供基础。",
      "categories": [
        "cs.AI",
        "cs.CY",
        "cs.HC",
        "cs.LG",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "63 pages, 22 figures, version accepted to ACM THRI",
      "pdf_url": "http://arxiv.org/pdf/2407.19631v3",
      "published_date": "2024-07-29 01:22:04 UTC",
      "updated_date": "2025-04-15 16:11:56 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:11:12.057916"
    },
    {
      "arxiv_id": "2407.19630v2",
      "title": "LLMs' Understanding of Natural Language Revealed",
      "title_zh": "翻译失败",
      "authors": [
        "Walid S. Saba"
      ],
      "abstract": "Large language models (LLMs) are the result of a massive experiment in\nbottom-up, data-driven reverse engineering of language at scale. Despite their\nutility in a number of downstream NLP tasks, ample research has shown that LLMs\nare incapable of performing reasoning in tasks that require quantification over\nand the manipulation of symbolic variables (e.g., planning and problem\nsolving); see for example [25][26]. In this document, however, we will focus on\ntesting LLMs for their language understanding capabilities, their supposed\nforte. As we will show here, the language understanding capabilities of LLMs\nhave been widely exaggerated. While LLMs have proven to generate human-like\ncoherent language (since that's how they were designed), their language\nunderstanding capabilities have not been properly tested. In particular, we\nbelieve that the language understanding capabilities of LLMs should be tested\nby performing an operation that is the opposite of 'text generation' and\nspecifically by giving the LLM snippets of text as input and then querying what\nthe LLM \"understood\". As we show here, when doing so it will become apparent\nthat LLMs do not truly understand language, beyond very superficial inferences\nthat are essentially the byproduct of the memorization of massive amounts of\ningested text.",
      "tldr_zh": "该论文揭示了大型语言模型（LLMs）的语言理解能力被严重夸大，尽管它们在生成人类-like 语言方面表现出色。作者认为，LLMs 的理解应通过反向操作测试，即输入文本片段并查询其“理解”内容，而不是依赖生成任务。研究发现，LLMs 仅能进行浅层推理，这主要是记忆海量文本的副产品，而非真正的符号变量操纵或深度认知。最终，这为评估 LLMs 在 NLP 任务中的局限性提供了新视角。",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "http://arxiv.org/pdf/2407.19630v2",
      "published_date": "2024-07-29 01:21:11 UTC",
      "updated_date": "2024-08-02 11:26:12 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:11:24.434617"
    },
    {
      "arxiv_id": "2407.19619v1",
      "title": "Enhancing Code Translation in Language Models with Few-Shot Learning via Retrieval-Augmented Generation",
      "title_zh": "翻译失败",
      "authors": [
        "Manish Bhattarai",
        "Javier E. Santos",
        "Shawn Jones",
        "Ayan Biswas",
        "Boian Alexandrov",
        "Daniel O'Malley"
      ],
      "abstract": "The advent of large language models (LLMs) has significantly advanced the\nfield of code translation, enabling automated translation between programming\nlanguages. However, these models often struggle with complex translation tasks\ndue to inadequate contextual understanding. This paper introduces a novel\napproach that enhances code translation through Few-Shot Learning, augmented\nwith retrieval-based techniques. By leveraging a repository of existing code\ntranslations, we dynamically retrieve the most relevant examples to guide the\nmodel in translating new code segments. Our method, based on\nRetrieval-Augmented Generation (RAG), substantially improves translation\nquality by providing contextual examples from which the model can learn in\nreal-time. We selected RAG over traditional fine-tuning methods due to its\nability to utilize existing codebases or a locally stored corpus of code, which\nallows for dynamic adaptation to diverse translation tasks without extensive\nretraining. Extensive experiments on diverse datasets with open LLM models such\nas Starcoder, Llama3-70B Instruct, CodeLlama-34B Instruct, Granite-34B Code\nInstruct, and Mixtral-8x22B, as well as commercial LLM models like GPT-3.5\nTurbo and GPT-4o, demonstrate our approach's superiority over traditional\nzero-shot methods, especially in translating between Fortran and CPP. We also\nexplored varying numbers of shots i.e. examples provided during inference,\nspecifically 1, 2, and 3 shots and different embedding models for RAG,\nincluding Nomic-Embed, Starencoder, and CodeBERT, to assess the robustness and\neffectiveness of our approach.",
      "tldr_zh": "该论文提出了一种通过 Few-Shot Learning 和 Retrieval-Augmented Generation (RAG) 增强大型语言模型 (LLMs) 代码翻译的方法，以解决模型在复杂任务中缺乏上下文理解的问题。方法利用现有代码仓库动态检索相关示例，作为实时指导，帮助模型翻译新代码段，并选择 RAG 而非传统微调，以实现灵活适应而不需大量重新训练。在多种数据集和模型（如 Starcoder、Llama3-70B Instruct 等）上进行的实验显示，该方法显著优于零样本方法，尤其在 Fortran 和 CPP 之间翻译的准确性上；此外，通过测试 1-3 shots 和不同嵌入模型（如 Nomic-Embed、Starencoder、CodeBERT），证明了其鲁棒性和有效性。",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "LLM for code translation",
      "pdf_url": "http://arxiv.org/pdf/2407.19619v1",
      "published_date": "2024-07-29 00:41:48 UTC",
      "updated_date": "2024-07-29 00:41:48 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:11:33.196309"
    },
    {
      "arxiv_id": "2407.19616v1",
      "title": "TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs",
      "title_zh": "翻译失败",
      "authors": [
        "Selma Wanna",
        "Ryan Barron",
        "Nick Solovyev",
        "Maksim E. Eren",
        "Manish Bhattarai",
        "Kim Rasmussen",
        "Boian S. Alexandrov"
      ],
      "abstract": "Topic modeling is a technique for organizing and extracting themes from large\ncollections of unstructured text. Non-negative matrix factorization (NMF) is a\ncommon unsupervised approach that decomposes a term frequency-inverse document\nfrequency (TF-IDF) matrix to uncover latent topics and segment the dataset\naccordingly. While useful for highlighting patterns and clustering documents,\nNMF does not provide explicit topic labels, necessitating subject matter\nexperts (SMEs) to assign labels manually. We present a methodology for\nautomating topic labeling in documents clustered via NMF with automatic model\ndetermination (NMFk). By leveraging the output of NMFk and employing prompt\nengineering, we utilize large language models (LLMs) to generate accurate topic\nlabels. Our case study on over 34,000 scientific abstracts on Knowledge Graphs\ndemonstrates the effectiveness of our method in enhancing knowledge management\nand document organization.",
      "tldr_zh": "本论文提出TopicTag方法，利用大型语言模型(LLMs)、Chain of Thought推理和Prompt Tuning来自动标注Non-negative Matrix Factorization (NMF)主题模型的标签，解决NMF在提取文本主题时缺乏显式标签的问题。方法基于NMFk的输出，通过提示工程引导LLMs生成准确的主题标签，从而自动化文档聚类过程。在对超过34,000个知识图谱科学摘要的案例研究中，该方法显著提升了知识管理和文档组织效率，证明了其在主题建模领域的实用性。",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to ACM Symposium on Document Engineering 2024 (DocEng 24),\n  2024",
      "pdf_url": "http://arxiv.org/pdf/2407.19616v1",
      "published_date": "2024-07-29 00:18:17 UTC",
      "updated_date": "2024-07-29 00:18:17 UTC",
      "processing_status": "completed",
      "attempts": 0,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "last_update": "2025-05-19T11:11:43.245868"
    }
  ],
  "raw_papers_fetched": true,
  "papers_count": 104,
  "processed_papers_count": 104,
  "failed_papers_count": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2025-05-19T11:12:10.649606"
}
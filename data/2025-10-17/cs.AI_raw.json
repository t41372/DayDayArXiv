[
  {
    "arxiv_id": "2510.16263v2",
    "title": "NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?",
    "authors": [
      "Jierui Peng",
      "Yanyan Zhang",
      "Yicheng Duan",
      "Tuo Liang",
      "Vipin Chaudhary",
      "Yu Yin"
    ],
    "abstract": "The evaluation of Vision-Language-Action (VLA) agents is hindered by the coarse, end-task success metric that fails to provide precise skill diagnosis or measure robustness to real-world perturbations. This challenge is exacerbated by a fragmented data landscape that impedes reproducible research and the development of generalist models. To address these limitations, we introduce NEBULA, a unified ecosystem for single-arm manipulation that enables diagnostic and reproducible evaluation. NEBULA features a novel dual-axis evaluation protocol that combines fine-grained capability tests for precise skill diagnosis with systematic stress tests that measure robustness. A standardized API and a large-scale, aggregated dataset are provided to reduce fragmentation and support cross-dataset training and fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle with key capabilities such as spatial reasoning and dynamic adaptation, which are consistently obscured by conventional end-task success metrics. By measuring both what an agent can do and when it does so reliably, NEBULA provides a practical foundation for robust, general-purpose embodied agents.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.RO",
    "comment": "Homepage: https://vulab-ai.github.io/NEBULA-Alpha/",
    "pdf_url": "https://arxiv.org/pdf/2510.16263v2",
    "published_date": "2025-10-17 23:22:57 UTC",
    "updated_date": "2025-10-21 00:32:26 UTC"
  },
  {
    "arxiv_id": "2510.16259v1",
    "title": "Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense",
    "authors": [
      "Zhehao Zhang",
      "Weijie Xu",
      "Shixian Cui",
      "Chandan K. Reddy"
    ],
    "abstract": "Recent advances in large reasoning models (LRMs) have enabled remarkable performance on complex tasks such as mathematics and coding by generating long Chain-of-Thought (CoT) traces. In this paper, we identify and systematically analyze a critical vulnerability we term reasoning distraction, where LRMs are diverted from their primary objective by irrelevant yet complex tasks maliciously embedded in the prompt. Through a comprehensive study across diverse models and benchmarks, we show that even state-of-the-art LRMs are highly susceptible, with injected distractors reducing task accuracy by up to 60%. We further reveal that certain alignment techniques can amplify this weakness and that models may exhibit covert compliance, following hidden adversarial instructions in reasoning while concealing them in the final output. To mitigate these risks, we propose a training-based defense that combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on synthetic adversarial data, improving robustness by over 50 points on challenging distractor attacks. Our findings establish reasoning distraction as a distinct and urgent threat to LRM reliability and provide a practical step toward safer and more trustworthy reasoning systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "29 pages, 9 tables, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.16259v1",
    "published_date": "2025-10-17 23:16:34 UTC",
    "updated_date": "2025-10-17 23:16:34 UTC"
  },
  {
    "arxiv_id": "2510.17881v1",
    "title": "POPI: Personalizing LLMs via Optimized Natural Language Preference Inference",
    "authors": [
      "Yizhuo Chen",
      "Xin Liu",
      "Ruijie Wang",
      "Zheng Li",
      "Pei Chen",
      "Changlong Yu",
      "Priyanka Nigam",
      "Meng Jiang",
      "Bing Yin"
    ],
    "abstract": "Large language models (LLMs) achieve strong benchmark performance, yet user experiences remain inconsistent due to diverse preferences in style, tone, and reasoning mode. Nevertheless, existing alignment techniques such as reinforcement learning from human feedback (RLHF) or Direct Preference Optimization (DPO) largely optimize toward population-level averages and overlook individual variation. Naive personalization strategies like per-user fine-tuning are computationally prohibitive, and in-context approaches that prepend raw user signals often suffer from inefficiency and noise. To address these challenges, we propose POPI, a general framework that introduces a preference inference model to distill heterogeneous user signals into concise natural language summaries. These summaries act as transparent, compact, and transferable personalization representations that condition a shared generation model to produce personalized responses. POPI jointly optimizes both preference inference and personalized generation under a unified objective using reinforcement learning, ensuring summaries maximally encode useful preference information. Extensive experiments across four personalization benchmarks demonstrate that POPI consistently improves personalization accuracy while reducing context overhead by a large margin. Moreover, optimized summaries seamlessly transfer to frozen off-the-shelf LLMs, enabling plug-and-play personalization without weight updates.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.17881v1",
    "published_date": "2025-10-17 23:07:57 UTC",
    "updated_date": "2025-10-17 23:07:57 UTC"
  },
  {
    "arxiv_id": "2510.16255v1",
    "title": "Detecting Adversarial Fine-tuning with Auditing Agents",
    "authors": [
      "Sarah Egler",
      "John Schulman",
      "Nicholas Carlini"
    ],
    "abstract": "Large Language Model (LLM) providers expose fine-tuning APIs that let end users fine-tune their frontier LLMs. Unfortunately, it has been shown that an adversary with fine-tuning access to an LLM can bypass safeguards. Particularly concerning, such attacks may avoid detection with datasets that are only implicitly harmful. Our work studies robust detection mechanisms for adversarial use of fine-tuning APIs. We introduce the concept of a fine-tuning auditing agent and show it can detect harmful fine-tuning prior to model deployment. We provide our auditing agent with access to the fine-tuning dataset, as well as the fine-tuned and pre-fine-tuned models, and request the agent assigns a risk score for the fine-tuning job. We evaluate our detection approach on a diverse set of eight strong fine-tuning attacks from the literature, along with five benign fine-tuned models, totaling over 1400 independent audits. These attacks are undetectable with basic content moderation on the dataset, highlighting the challenge of the task. With the best set of affordances, our auditing agent achieves a 56.2% detection rate of adversarial fine-tuning at a 1% false positive rate. Most promising, the auditor is able to detect covert cipher attacks that evade safety evaluations and content moderation of the dataset. While benign fine-tuning with unintentional subtle safety degradation remains a challenge, we establish a baseline configuration for further work in this area. We release our auditing agent at https://github.com/safety-research/finetuning-auditor.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16255v1",
    "published_date": "2025-10-17 23:01:16 UTC",
    "updated_date": "2025-10-17 23:01:16 UTC"
  },
  {
    "arxiv_id": "2510.16253v1",
    "title": "Protein Folding with Neural Ordinary Differential Equations",
    "authors": [
      "Arielle Sanford",
      "Shuo Sun",
      "Christian B. Mendl"
    ],
    "abstract": "Recent advances in protein structure prediction, such as AlphaFold, have demonstrated the power of deep neural architectures like the Evoformer for capturing complex spatial and evolutionary constraints on protein conformation. However, the depth of the Evoformer, comprising 48 stacked blocks, introduces high computational costs and rigid layerwise discretization. Inspired by Neural Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth formulation of the Evoformer, replacing its 48 discrete blocks with a Neural ODE parameterization that preserves its core attention-based operations. This continuous-time Evoformer achieves constant memory cost (in depth) via the adjoint method, while allowing a principled trade-off between runtime and accuracy through adaptive ODE solvers. Benchmarking on protein structure prediction tasks, we find that the Neural ODE-based Evoformer produces structurally plausible predictions and reliably captures certain secondary structure elements, such as alpha-helices, though it does not fully replicate the accuracy of the original architecture. However, our model achieves this performance using dramatically fewer resources, just 17.5 hours of training on a single GPU, highlighting the promise of continuous-depth models as a lightweight and interpretable alternative for biomolecular modeling. This work opens new directions for efficient and adaptive protein structure prediction frameworks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM",
      "q-bio.QM",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16253v1",
    "published_date": "2025-10-17 22:56:03 UTC",
    "updated_date": "2025-10-17 22:56:03 UTC"
  },
  {
    "arxiv_id": "2510.16234v1",
    "title": "ScholarEval: Research Idea Evaluation Grounded in Literature",
    "authors": [
      "Hanane Nour Moussa",
      "Patrick Queiroz Da Silva",
      "Daniel Adu-Ampratwum",
      "Alyson East",
      "Zitong Lu",
      "Nikki Puccetti",
      "Mingyi Xue",
      "Huan Sun",
      "Bodhisattwa Prasad Majumder",
      "Sachin Kumar"
    ],
    "abstract": "As AI tools become increasingly common for research ideation, robust evaluation is critical to ensure the validity and usefulness of generated ideas. We introduce ScholarEval, a retrieval augmented evaluation framework that assesses research ideas based on two fundamental criteria: soundness - the empirical validity of proposed methods based on existing literature, and contribution - the degree of advancement made by the idea across different dimensions relative to prior research. To evaluate ScholarEval, we introduce ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas and reviews, comprised of 117 ideas across four disciplines: artificial intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows that ScholarEval achieves significantly higher coverage of points mentioned in the human expert annotated rubrics in ScholarIdeas compared to all baselines. Furthermore, ScholarEval is consistently preferred over our strongest baseline o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI, in terms of evaluation actionability, depth, and evidence support. Our large-scale user study also shows that ScholarEval significantly outperforms deep research in literature engagement, idea refinement, and usefulness. We openly release our code, dataset, and ScholarEval tool for the community to use and build on.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16234v1",
    "published_date": "2025-10-17 21:55:07 UTC",
    "updated_date": "2025-10-17 21:55:07 UTC"
  },
  {
    "arxiv_id": "2510.16233v1",
    "title": "Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal",
    "authors": [
      "Patricia West",
      "Michelle WL Wan",
      "Alexander Hepburn",
      "Edwin Simpson",
      "Raul Santos-Rodriguez",
      "Jeffrey N Clark"
    ],
    "abstract": "Climate change demands effective legislative action to mitigate its impacts. This study explores the application of machine learning (ML) to understand the progression of climate policy from announcement to adoption, focusing on policies within the European Green Deal. We present a dataset of 165 policies, incorporating text and metadata. We aim to predict a policy's progression status, and compare text representation methods, including TF-IDF, BERT, and ClimateBERT. Metadata features are included to evaluate the impact on predictive performance. On text features alone, ClimateBERT outperforms other approaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance with the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods from explainable AI highlights the influence of factors such as policy wording and metadata including political party and country representation. These findings underscore the potential of ML tools in supporting climate policy analysis and decision-making.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16233v1",
    "published_date": "2025-10-17 21:52:03 UTC",
    "updated_date": "2025-10-17 21:52:03 UTC"
  },
  {
    "arxiv_id": "2510.16227v2",
    "title": "What Can String Probability Tell Us About Grammaticality?",
    "authors": [
      "Jennifer Hu",
      "Ethan Gotlieb Wilcox",
      "Siyuan Song",
      "Kyle Mahowald",
      "Roger P. Levy"
    ],
    "abstract": "What have language models (LMs) learned about grammar? This question remains hotly debated, with major ramifications for linguistic theory. However, since probability and grammaticality are distinct notions in linguistics, it is not obvious what string probabilities can reveal about an LM's underlying grammatical knowledge. We present a theoretical analysis of the relationship between grammar, meaning, and string probability, based on simple assumptions about the generative process of corpus data. Our framework makes three predictions, which we validate empirically using 280K sentence pairs in English and Chinese: (1) correlation between the probability of strings within minimal pairs, i.e., string pairs with minimal semantic differences; (2) correlation between models' and humans' deltas within minimal pairs; and (3) poor separation in probability space between unpaired grammatical and ungrammatical strings. Our analyses give theoretical grounding for using probability to learn about LMs' structural knowledge, and suggest directions for future work in LM grammatical evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16227v2",
    "published_date": "2025-10-17 21:36:00 UTC",
    "updated_date": "2025-11-07 16:36:02 UTC"
  },
  {
    "arxiv_id": "2510.16219v2",
    "title": "SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection",
    "authors": [
      "Yang Feng",
      "Xudong Pan"
    ],
    "abstract": "Malicious agents pose significant threats to the reliability and decision-making capabilities of Multi-Agent Systems (MAS) powered by Large Language Models (LLMs). Existing defenses often fall short due to reactive designs or centralized architectures which may introduce single points of failure. To address these challenges, we propose SentinelNet, the first decentralized framework for proactively detecting and mitigating malicious behaviors in multi-agent collaboration. SentinelNet equips each agent with a credit-based detector trained via contrastive learning on augmented adversarial debate trajectories, enabling autonomous evaluation of message credibility and dynamic neighbor ranking via bottom-k elimination to suppress malicious communications. To overcome the scarcity of attack data, it generates adversarial trajectories simulating diverse threats, ensuring robust training. Experiments on MAS benchmarks show SentinelNet achieves near-perfect detection of malicious agents, close to 100% within two debate rounds, and recovers 95% of system accuracy from compromised baselines. By exhibiting strong generalizability across domains and attack patterns, SentinelNet establishes a novel paradigm for safeguarding collaborative MAS.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16219v2",
    "published_date": "2025-10-17 21:10:35 UTC",
    "updated_date": "2025-10-21 12:58:13 UTC"
  },
  {
    "arxiv_id": "2510.16206v2",
    "title": "The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI",
    "authors": [
      "Alex Zhavoronkov",
      "Dominika Wilczok",
      "Roman Yampolskiy"
    ],
    "abstract": "Since the rapid expansion of large language models (LLMs), people have begun to rely on them for information retrieval. While traditional search engines display ranked lists of sources shaped by search engine optimization (SEO), advertising, and personalization, LLMs typically provide a synthesized response that feels singular and authoritative. While both approaches carry risks of bias and omission, LLMs may amplify the effect by collapsing multiple perspectives into one answer, reducing users ability or inclination to compare alternatives. This concentrates power over information in a few LLM vendors whose systems effectively shape what is remembered and what is overlooked. As a result, certain narratives, individuals or groups, may be disproportionately suppressed, while others are disproportionately elevated. Over time, this creates a new threat: the gradual erasure of those with limited digital presence, and the amplification of those already prominent, reshaping collective memory. To address these concerns, this paper presents a concept of the Right To Be Remembered (RTBR) which encompasses minimizing the risk of AI-driven information omission, embracing the right of fair treatment, while ensuring that the generated content would be maximally truthful.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16206v2",
    "published_date": "2025-10-17 20:38:12 UTC",
    "updated_date": "2025-10-22 13:56:17 UTC"
  },
  {
    "arxiv_id": "2510.16197v1",
    "title": "Revealing Low-Dimensional Structure in 2D Richtmyer-Meshkov Instabilities via Parametric Reduced-Order Modeling",
    "authors": [
      "Daniel Messenger",
      "Daniel Serino",
      "Balu Nadiga",
      "Marc Klasky"
    ],
    "abstract": "Efficient modeling of the Richtmyer-Meshkov instability (RMI) is essential to many engineering tasks, including high-speed combustion and drive and capsule geometry optimization in Inertial Confinement Fusion (ICF). In the latter, RMI causes the ablator and fuel to mix, introducing cold spots into the fuel and lowering performance; controlling RMI is thus a core ICF design concern. In this work, we introduce a reduced-order model for two-dimensional RMI based on the Latent Space Dynamics Identification (LaSDI) algorithm. We demonstrate the efficacy of the proposed methodology in efficiently parametrizing the solution space over a high-dimensional parameter vector consisting of material EOS parameters and initial conditions known to affect RMI growth rates. Using only late-time partial observations of the dynamics, we use our framework to not only provide a highly efficient dynamic surrogate model, but to reveal that the RMI exhibits the structure of a surprisingly low-dimensional and linear dynamical system, into the nonlinear growth regime, after a suitable nonlinear transformation is applied to the material interface, which we approximate as a trained autoencoder. Our use of practical observables and fundamental parameters suggests that such ROMs may be useful for downstream engineering tasks which confront the RMI, while the low-dimensional representation suggests a new direction for theoretical work.",
    "categories": [
      "physics.flu-dyn",
      "cs.AI"
    ],
    "primary_category": "physics.flu-dyn",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16197v1",
    "published_date": "2025-10-17 20:19:00 UTC",
    "updated_date": "2025-10-17 20:19:00 UTC"
  },
  {
    "arxiv_id": "2510.16196v1",
    "title": "Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI",
    "authors": [
      "Zheng Huang",
      "Enpei Zhang",
      "Yinghao Cai",
      "Weikang Qiu",
      "Carl Yang",
      "Elynn Chen",
      "Xiang Zhang",
      "Rex Ying",
      "Dawei Zhou",
      "Yujun Yan"
    ],
    "abstract": "Understanding how the brain encodes visual information is a central challenge in neuroscience and machine learning. A promising approach is to reconstruct visual stimuli, essentially images, from functional Magnetic Resonance Imaging (fMRI) signals. This involves two stages: transforming fMRI signals into a latent space and then using a pretrained generative model to reconstruct images. The reconstruction quality depends on how similar the latent space is to the structure of neural activity and how well the generative model produces images from that space. Yet, it remains unclear which type of latent space best supports this transformation and how it should be organized to represent visual stimuli effectively. We present two key findings. First, fMRI signals are more similar to the text space of a language model than to either a vision based space or a joint text image space. Second, text representations and the generative model should be adapted to capture the compositional nature of visual stimuli, including objects, their detailed attributes, and relationships. Building on these insights, we propose PRISM, a model that Projects fMRI sIgnals into a Structured text space as an interMediate representation for visual stimuli reconstruction. It includes an object centric diffusion module that generates images by composing individual objects to reduce object detection errors, and an attribute relationship search module that automatically identifies key attributes and relationships that best align with the neural activity. Extensive experiments on real world datasets demonstrate that our framework outperforms existing methods, achieving up to an 8% reduction in perceptual loss. These results highlight the importance of using structured text as the intermediate space to bridge fMRI signals and image reconstruction.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16196v1",
    "published_date": "2025-10-17 20:18:06 UTC",
    "updated_date": "2025-10-17 20:18:06 UTC"
  },
  {
    "arxiv_id": "2510.16194v2",
    "title": "Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration",
    "authors": [
      "Guanchen Wu",
      "Zuhui Chen",
      "Yuzhang Xie",
      "Carl Yang"
    ],
    "abstract": "Protected health information (PHI) de-identification is critical for enabling the safe reuse of clinical notes, yet evaluating and comparing PHI de-identification models typically depends on costly, small-scale expert annotations. We present TEAM-PHI, a multi-agent evaluation and selection framework that uses large language models (LLMs) to automatically measure de-identification quality and select the best-performing model without heavy reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each independently judging the correctness of PHI extractions and outputting structured metrics. Their results are then consolidated through an LLM-based majority voting mechanism that integrates diverse evaluator perspectives into a single, stable, and reproducible ranking. Experiments on a real-world clinical note corpus demonstrate that TEAM-PHI produces consistent and accurate rankings: despite variation across individual evaluators, LLM-based voting reliably converges on the same top-performing systems. Further comparison with ground-truth annotations and human evaluation confirms that the framework's automated rankings closely match supervised evaluation. By combining independent evaluation agents with LLM majority voting, TEAM-PHI offers a practical, secure, and cost-effective solution for automatic evaluation and best-model selection in PHI de-identification, even when ground-truth labels are limited.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Agents4Science 2025 (Spotlight)",
    "pdf_url": "https://arxiv.org/pdf/2510.16194v2",
    "published_date": "2025-10-17 20:06:31 UTC",
    "updated_date": "2025-11-18 02:32:12 UTC"
  },
  {
    "arxiv_id": "2510.16193v2",
    "title": "Operationalising Extended Cognition: Formal Metrics for Corporate Knowledge and Legal Accountability",
    "authors": [
      "Elija Perrier"
    ],
    "abstract": "Corporate responsibility turns on notions of corporate \\textit{mens rea}, traditionally imputed from human agents. Yet these assumptions are under challenge as generative AI increasingly mediates enterprise decision-making. Building on the theory of extended cognition, we argue that in response corporate knowledge may be redefined as a dynamic capability, measurable by the efficiency of its information-access procedures and the validated reliability of their outputs. We develop a formal model that captures epistemic states of corporations deploying sophisticated AI or information systems, introducing a continuous organisational knowledge metric $S_S(\\varphi)$ which integrates a pipeline's computational cost and its statistically validated error rate. We derive a thresholded knowledge predicate $\\mathsf{K}_S$ to impute knowledge and a firm-wide epistemic capacity index $\\mathcal{K}_{S,t}$ to measure overall capability. We then operationally map these quantitative metrics onto the legal standards of actual knowledge, constructive knowledge, wilful blindness, and recklessness. Our work provides a pathway towards creating measurable and justiciable audit artefacts, that render the corporate mind tractable and accountable in the algorithmic age.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review",
    "pdf_url": "https://arxiv.org/pdf/2510.16193v2",
    "published_date": "2025-10-17 20:03:57 UTC",
    "updated_date": "2025-10-25 00:17:55 UTC"
  },
  {
    "arxiv_id": "2510.16187v1",
    "title": "Zero-Shot Coordination in Ad Hoc Teams with Generalized Policy Improvement and Difference Rewards",
    "authors": [
      "Rupal Nigam",
      "Niket Parikh",
      "Hamid Osooli",
      "Mikihisa Yuasa",
      "Jacob Heglund",
      "Huy T. Tran"
    ],
    "abstract": "Real-world multi-agent systems may require ad hoc teaming, where an agent must coordinate with other previously unseen teammates to solve a task in a zero-shot manner. Prior work often either selects a pretrained policy based on an inferred model of the new teammates or pretrains a single policy that is robust to potential teammates. Instead, we propose to leverage all pretrained policies in a zero-shot transfer setting. We formalize this problem as an ad hoc multi-agent Markov decision process and present a solution that uses two key ideas, generalized policy improvement and difference rewards, for efficient and effective knowledge transfer between different teams. We empirically demonstrate that our algorithm, Generalized Policy improvement for Ad hoc Teaming (GPAT), successfully enables zero-shot transfer to new teams in three simulated environments: cooperative foraging, predator-prey, and Overcooked. We also demonstrate our algorithm in a real-world multi-robot setting.",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.MA",
    "comment": "10 pages, 8 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.16187v1",
    "published_date": "2025-10-17 19:55:25 UTC",
    "updated_date": "2025-10-17 19:55:25 UTC"
  },
  {
    "arxiv_id": "2511.11587v3",
    "title": "MedBuild AI: An Agent-Based Hybrid Intelligence Framework for Reshaping Agency in Healthcare Infrastructure Planning through Generative Design for Medical Architecture",
    "authors": [
      "Yiming Zhang",
      "Yuejia Xu",
      "Ziyao Wang",
      "Xin Yan",
      "Xiaosai Hao"
    ],
    "abstract": "Globally, disparities in healthcare infrastructure remain stark, leaving countless communities without access to even basic services. Traditional infrastructure planning is often slow and inaccessible, and although many architects are actively delivering humanitarian and aid-driven hospital projects worldwide, these vital efforts still fall far short of the sheer scale and urgency of demand. This paper introduces MedBuild AI, a hybrid-intelligence framework that integrates large language models (LLMs) with deterministic expert systems to rebalance the early design and conceptual planning stages. As a web-based platform, it enables any region with satellite internet access to obtain guidance on modular, low-tech, low-cost medical building designs. The system operates through three agents: the first gathers local health intelligence via conversational interaction; the second translates this input into an architectural functional program through rule-based computation; and the third generates layouts and 3D models. By embedding computational negotiation into the design process, MedBuild AI fosters a reciprocal, inclusive, and equitable approach to healthcare planning, empowering communities and redefining agency in global healthcare architecture.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CE",
      "cs.GR",
      "cs.MA"
    ],
    "primary_category": "cs.HC",
    "comment": "24 pages, 16 figures. Submitted to the IJAC Special Issue \"Rebalance and Reciprocity\"",
    "pdf_url": "https://arxiv.org/pdf/2511.11587v3",
    "published_date": "2025-10-17 19:55:00 UTC",
    "updated_date": "2025-11-19 02:33:13 UTC"
  },
  {
    "arxiv_id": "2510.16185v2",
    "title": "Expressive Reward Synthesis with the Runtime Monitoring Language",
    "authors": [
      "Daniel Donnelly",
      "Angelo Ferrando",
      "Francesco Belardinelli"
    ],
    "abstract": "A key challenge in reinforcement learning (RL) is reward (mis)specification, whereby imprecisely defined reward functions can result in unintended, possibly harmful, behaviours. Indeed, reward functions in RL are typically treated as black-box mappings from state-action pairs to scalar values. While effective in many settings, this approach provides no information about why rewards are given, which can hinder learning and interpretability. Reward Machines address this issue by representing reward functions as finite state automata, enabling the specification of structured, non-Markovian reward functions. However, their expressivity is typically bounded by regular languages, leaving them unable to capture more complex behaviours such as counting or parametrised conditions. In this work, we build on the Runtime Monitoring Language (RML) to develop a novel class of language-based Reward Machines. By leveraging the built-in memory of RML, our approach can specify reward functions for non-regular, non-Markovian tasks. We demonstrate the expressiveness of our approach through experiments, highlighting additional advantages in flexible event-handling and task specification over existing Reward Machine-based methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.FL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16185v2",
    "published_date": "2025-10-17 19:54:59 UTC",
    "updated_date": "2025-10-21 10:04:30 UTC"
  },
  {
    "arxiv_id": "2510.16175v2",
    "title": "The Formalism-Implementation Gap in Reinforcement Learning Research",
    "authors": [
      "Pablo Samuel Castro"
    ],
    "abstract": "The last decade has seen an upswing in interest and adoption of reinforcement learning (RL) techniques, in large part due to its demonstrated capabilities at performing certain tasks at \"super-human levels\". This has incentivized the community to prioritize research that demonstrates RL agent performance, often at the expense of research aimed at understanding their learning dynamics. Performance-focused research runs the risk of overfitting on academic benchmarks -- thereby rendering them less useful -- which can make it difficult to transfer proposed techniques to novel problems. Further, it implicitly diminishes work that does not push the performance-frontier, but aims at improving our understanding of these techniques. This paper argues two points: (i) RL research should stop focusing solely on demonstrating agent capabilities, and focus more on advancing the science and understanding of reinforcement learning; and (ii) we need to be more precise on how our benchmarks map to the underlying mathematical formalisms. We use the popular Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a benchmark that, despite being increasingly considered \"saturated\", can be effectively used for developing this understanding, and facilitating the deployment of RL techniques in impactful real-world problems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16175v2",
    "published_date": "2025-10-17 19:35:54 UTC",
    "updated_date": "2025-10-28 14:06:41 UTC"
  },
  {
    "arxiv_id": "2510.16171v3",
    "title": "Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness",
    "authors": [
      "Longwei Wang",
      "Ifrat Ikhtear Uddin",
      "KC Santosh",
      "Chaowei Zhang",
      "Xiao Qin",
      "Yang Zhou"
    ],
    "abstract": "Adversarial examples reveal critical vulnerabilities in deep neural networks by exploiting their sensitivity to imperceptible input perturbations. While adversarial training remains the predominant defense strategy, it often incurs significant computational cost and may compromise clean-data accuracy. In this work, we investigate an architectural approach to adversarial robustness by embedding group-equivariant convolutions-specifically, rotation- and scale-equivariant layers-into standard convolutional neural networks (CNNs). These layers encode symmetry priors that align model behavior with structured transformations in the input space, promoting smoother decision boundaries and greater resilience to adversarial attacks. We propose and evaluate two symmetry-aware architectures: a parallel design that processes standard and equivariant features independently before fusion, and a cascaded design that applies equivariant operations sequentially. Theoretically, we demonstrate that such models reduce hypothesis space complexity, regularize gradients, and yield tighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme Value for nEtwork Robustness) framework. Empirically, our models consistently improve adversarial robustness and generalization across CIFAR-10, CIFAR-100, and CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial training. These findings underscore the potential of symmetry-enforcing architectures as efficient and principled alternatives to data augmentation-based defenses.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted for the proceedings of 39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.16171v3",
    "published_date": "2025-10-17 19:26:58 UTC",
    "updated_date": "2025-11-03 04:37:21 UTC"
  },
  {
    "arxiv_id": "2510.16156v1",
    "title": "AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning",
    "authors": [
      "Yueqian Lin",
      "Zhengmian Hu",
      "Jayakumar Subramanian",
      "Qinsi Wang",
      "Nikos Vlassis",
      "Hai \"Helen\" Li",
      "Yiran Chen"
    ],
    "abstract": "Effective human-AI collaboration on complex reasoning tasks requires that users understand and interact with the model's process, not just receive an output. However, the monolithic text from methods like Chain-of-Thought (CoT) prevents this, as current interfaces lack real-time verbalization and robust user barge-in. We present AsyncVoice Agent, a system whose asynchronous architecture decouples a streaming LLM backend from a conversational voice frontend. This design allows narration and inference to run in parallel, empowering users to interrupt, query, and steer the model's reasoning process at any time. Objective benchmarks show this approach reduces interaction latency by more than 600x compared to monolithic baselines while ensuring high fidelity and competitive task accuracy. By enabling a two-way dialogue with a model's thought process, AsyncVoice Agent offers a new paradigm for building more effective, steerable, and trustworthy human-AI systems for high-stakes tasks.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted to the IEEE ASRU 2025 Demo Track",
    "pdf_url": "https://arxiv.org/pdf/2510.16156v1",
    "published_date": "2025-10-17 19:00:08 UTC",
    "updated_date": "2025-10-17 19:00:08 UTC"
  },
  {
    "arxiv_id": "2510.16152v1",
    "title": "Publication Trend Analysis and Synthesis via Large Language Model: A Case Study of Engineering in PNAS",
    "authors": [
      "Mason Smetana",
      "Lev Khazanovich"
    ],
    "abstract": "Scientific literature is increasingly siloed by complex language, static disciplinary structures, and potentially sparse keyword systems, making it cumbersome to capture the dynamic nature of modern science. This study addresses these challenges by introducing an adaptable large language model (LLM)-driven framework to quantify thematic trends and map the evolving landscape of scientific knowledge. The approach is demonstrated over a 20-year collection of more than 1,500 engineering articles published by the Proceedings of the National Academy of Sciences (PNAS), marked for their breadth and depth of research focus. A two-stage classification pipeline first establishes a primary thematic category for each article based on its abstract. The subsequent phase performs a full-text analysis to assign secondary classifications, revealing latent, cross-topic connections across the corpus. Traditional natural language processing (NLP) methods, such as Bag-of-Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF), confirm the resulting topical structure and also suggest that standalone word-frequency analyses may be insufficient for mapping fields with high diversity. Finally, a disjoint graph representation between the primary and secondary classifications reveals implicit connections between themes that may be less apparent when analyzing abstracts or keywords alone. The findings show that the approach independently recovers much of the journal's editorially embedded structure without prior knowledge of its existing dual-classification schema (e.g., biological studies also classified as engineering). This framework offers a powerful tool for detecting potential thematic trends and providing a high-level overview of scientific progress.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.DL",
    "comment": "35 pages, 10 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.16152v1",
    "published_date": "2025-10-17 18:57:11 UTC",
    "updated_date": "2025-10-17 18:57:11 UTC"
  },
  {
    "arxiv_id": "2510.16144v1",
    "title": "Agentic AI for Ultra-Modern Networks: Multi-Agent Framework for RAN Autonomy and Assurance",
    "authors": [
      "Sukhdeep Singh",
      "Avinash Bhat",
      "Shweta M",
      "Subhash K Singh",
      "Moonki Hong",
      "Madhan Raj K",
      "Kandeepan Sithamparanathan",
      "Sunder A. Khowaja",
      "Kapal Dev"
    ],
    "abstract": "The increasing complexity of Beyond 5G and 6G networks necessitates new paradigms for autonomy and assur- ance. Traditional O-RAN control loops rely heavily on RIC- based orchestration, which centralizes intelligence and exposes the system to risks such as policy conflicts, data drift, and unsafe actions under unforeseen conditions. In this work, we argue that the future of autonomous networks lies in a multi-agentic architecture, where specialized agents collaborate to perform data collection, model training, prediction, policy generation, verification, deployment, and assurance. By replacing tightly- coupled centralized RIC-based workflows with distributed agents, the framework achieves autonomy, resilience, explainability, and system-wide safety. To substantiate this vision, we design and evaluate a traffic steering use case under surge and drift conditions. Results across four KPIs: RRC connected users, IP throughput, PRB utilization, and SINR, demonstrate that a naive predictor-driven deployment improves local KPIs but destabilizes neighbors, whereas the agentic system blocks unsafe policies, preserving global network health. This study highlights multi- agent architectures as a credible foundation for trustworthy AI- driven autonomy in next-generation RANs.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16144v1",
    "published_date": "2025-10-17 18:28:55 UTC",
    "updated_date": "2025-10-17 18:28:55 UTC"
  },
  {
    "arxiv_id": "2601.05253v1",
    "title": "SP-Rank: A Dataset for Ranked Preferences with Secondary Information",
    "authors": [
      "Hadi Hosseini",
      "Debmalya Mandal",
      "Amrit Puhan"
    ],
    "abstract": "We introduce $\\mathbf{SP-Rank}$, the first large-scale, publicly available dataset for benchmarking algorithms that leverage both first-order preferences and second-order predictions in ranking tasks. Each datapoint includes a personal vote (first-order signal) and a meta-prediction of how others will vote (second-order signal), allowing richer modeling than traditional datasets that capture only individual preferences. SP-Rank contains over 12,000 human-generated datapoints across three domains -- geography, movies, and paintings, and spans nine elicitation formats with varying subset sizes. This structure enables empirical analysis of preference aggregation when expert identities are unknown but presumed to exist, and individual votes represent noisy estimates of a shared ground-truth ranking. We benchmark SP-Rank by comparing traditional aggregation methods that use only first-order votes against SP-Voting, a second-order method that jointly reasons over both signals to infer ground-truth rankings. While SP-Rank also supports models that rely solely on second-order predictions, our benchmarks emphasize the gains from combining both signals. We evaluate performance across three core tasks: (1) full ground-truth rank recovery, (2) subset-level rank recovery, and (3) probabilistic modeling of voter behavior. Results show that incorporating second-order signals substantially improves accuracy over vote-only methods. Beyond social choice, SP-Rank supports downstream applications in learning-to-rank, extracting expert knowledge from noisy crowds, and training reward models in preference-based fine-tuning pipelines. We release the dataset, code, and baseline evaluations (available at https://github.com/amrit19/SP-Rank-Dataset ) to foster research in human preference modeling, aggregation theory, and human-AI alignment.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2601.05253v1",
    "published_date": "2025-10-17 18:26:40 UTC",
    "updated_date": "2025-10-17 18:26:40 UTC"
  },
  {
    "arxiv_id": "2510.16136v1",
    "title": "GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer",
    "authors": [
      "Sayan Deb Sarkar",
      "Sinisa Stekovic",
      "Vincent Lepetit",
      "Iro Armeni"
    ],
    "abstract": "Transferring appearance to 3D assets using different representations of the appearance object - such as images or text - has garnered interest due to its wide range of applications in industries like gaming, augmented reality, and digital content creation. However, state-of-the-art methods still fail when the geometry between the input and appearance objects is significantly different. A straightforward approach is to directly apply a 3D generative model, but we show that this ultimately fails to produce appealing results. Instead, we propose a principled approach inspired by universal guidance. Given a pretrained rectified flow model conditioned on image or text, our training-free method interacts with the sampling process by periodically adding guidance. This guidance can be modeled as a differentiable loss function, and we experiment with two different types of guidance including part-aware losses for appearance and self-similarity. Our experiments show that our approach successfully transfers texture and geometric details to the input 3D asset, outperforming baselines both qualitatively and quantitatively. We also show that traditional metrics are not suitable for evaluating the task due to their inability of focusing on local details and comparing dissimilar inputs, in absence of ground truth data. We thus evaluate appearance transfer quality with a GPT-based system objectively ranking outputs, ensuring robust and human-like assessment, as further confirmed by our user study. Beyond showcased scenarios, our method is general and could be extended to different types of diffusion models and guidance functions.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "comment": "NeurIPS 2025. Project Page: https://sayands.github.io/guideflow3d/",
    "pdf_url": "https://arxiv.org/pdf/2510.16136v1",
    "published_date": "2025-10-17 18:22:04 UTC",
    "updated_date": "2025-10-17 18:22:04 UTC"
  },
  {
    "arxiv_id": "2510.16134v1",
    "title": "Aria Gen 2 Pilot Dataset",
    "authors": [
      "Chen Kong",
      "James Fort",
      "Aria Kang",
      "Jonathan Wittmer",
      "Simon Green",
      "Tianwei Shen",
      "Yipu Zhao",
      "Cheng Peng",
      "Gustavo Solaira",
      "Andrew Berkovich",
      "Nikhil Raina",
      "Vijay Baiyya",
      "Evgeniy Oleinik",
      "Eric Huang",
      "Fan Zhang",
      "Julian Straub",
      "Mark Schwesinger",
      "Luis Pesqueira",
      "Xiaqing Pan",
      "Jakob Julian Engel",
      "Carl Ren",
      "Mingfei Yan",
      "Richard Newcombe"
    ],
    "abstract": "The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely access, A2PD is released incrementally with ongoing dataset enhancements. The initial release features Dia'ane, our primary subject, who records her daily activities alongside friends, each equipped with Aria Gen 2 glasses. It encompasses five primary scenarios: cleaning, cooking, eating, playing, and outdoor walking. In each of the scenarios, we provide comprehensive raw sensor data and output data from various machine perception algorithms. These data illustrate the device's ability to perceive the wearer, the surrounding environment, and interactions between the wearer and the environment, while maintaining robust performance across diverse users and conditions. The A2PD is publicly available at projectaria.com, with open-source tools and usage examples provided in Project Aria Tools.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16134v1",
    "published_date": "2025-10-17 18:21:11 UTC",
    "updated_date": "2025-10-17 18:21:11 UTC"
  },
  {
    "arxiv_id": "2510.16097v1",
    "title": "Narrowing Action Choices with AI Improves Human Sequential Decisions",
    "authors": [
      "Eleni Straitouri",
      "Stratis Tsirtsis",
      "Ander Artola Velasco",
      "Manuel Gomez-Rodriguez"
    ],
    "abstract": "Recent work has shown that, in classification tasks, it is possible to design decision support systems that do not require human experts to understand when to cede agency to a classifier or when to exercise their own agency to achieve complementarity$\\unicode{x2014}$experts using these systems make more accurate predictions than those made by the experts or the classifier alone. The key principle underpinning these systems reduces to adaptively controlling the level of human agency, by design. Can we use the same principle to achieve complementarity in sequential decision making tasks? In this paper, we answer this question affirmatively. We develop a decision support system that uses a pre-trained AI agent to narrow down the set of actions a human can take to a subset, and then asks the human to take an action from this action set. Along the way, we also introduce a bandit algorithm that leverages the smoothness properties of the action sets provided by our system to efficiently optimize the level of human agency. To evaluate our decision support system, we conduct a large-scale human subject study ($n = 1{,}600$) where participants play a wildfire mitigation game. We find that participants who play the game supported by our system outperform those who play on their own by $\\sim$$30$% and the AI agent used by our system by $>$$2$%, even though the AI agent largely outperforms participants playing without support. We have made available the data gathered in our human subject study as well as an open source implementation of our system at https://github.com/Networks-Learning/narrowing-action-choices .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the Human-AI Complementarity for Decision Making Workshop 2025 by the NSF AI Institute for Societal Decision Making",
    "pdf_url": "https://arxiv.org/pdf/2510.16097v1",
    "published_date": "2025-10-17 18:00:00 UTC",
    "updated_date": "2025-10-17 18:00:00 UTC"
  },
  {
    "arxiv_id": "2510.15870v2",
    "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM",
    "authors": [
      "Hanrong Ye",
      "Chao-Han Huck Yang",
      "Arushi Goel",
      "Wei Huang",
      "Ligeng Zhu",
      "Yuanhang Su",
      "Sean Lin",
      "An-Chieh Cheng",
      "Zhen Wan",
      "Jinchuan Tian",
      "Yuming Lou",
      "Dong Yang",
      "Zhijian Liu",
      "Yukang Chen",
      "Ambrish Dantrey",
      "Ehsan Jahangiri",
      "Sreyan Ghosh",
      "Daguang Xu",
      "Ehsan Hosseini-Asl",
      "Danial Mohseni Taheri",
      "Vidya Murali",
      "Sifei Liu",
      "Yao Lu",
      "Oluwatobi Olabiyi",
      "Yu-Chiang Frank Wang",
      "Rafael Valle",
      "Bryan Catanzaro",
      "Andrew Tao",
      "Song Han",
      "Jan Kautz",
      "Hongxu Yin",
      "Pavlo Molchanov"
    ],
    "abstract": "Advancing machine intelligence requires developing the ability to perceive across multiple modalities, much as humans sense the world. We introduce OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We carefully study the design choices across model architecture and data curation. For model architecture, we present three key innovations: (i) OmniAlignNet for strengthening alignment between vision and audio embeddings in a shared omni-modal latent space; (ii) Temporal Embedding Grouping for capturing relative temporal alignment between vision and audio signals; and (iii) Constrained Rotary Time Embedding for encoding absolute temporal information in omni-modal embeddings. We introduce a curation and synthesis pipeline that generates 24M single-modal and omni-modal conversations. We find that modalities reinforce one another in both perception and reasoning. Our model, OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while using just 0.2T training tokens - a 6 times reduction compared to Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream applications spanning robotics, medical AI, and smart factory.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical Report. Code: https://github.com/NVlabs/OmniVinci",
    "pdf_url": "https://arxiv.org/pdf/2510.15870v2",
    "published_date": "2025-10-17 17:59:59 UTC",
    "updated_date": "2025-10-27 19:12:55 UTC"
  },
  {
    "arxiv_id": "2510.15863v1",
    "title": "PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction",
    "authors": [
      "Simon Yu",
      "Gang Li",
      "Weiyan Shi",
      "Peng Qi"
    ],
    "abstract": "Large language models (LLMs) are moving beyond static uses and are now powering agents that learn continually during their interaction with external environments. For example, agents can learn reusable skills while navigating web pages or toggling new tools. However, existing methods for skill learning often create skills that are over-specialized to a single website and fail to generalize. We introduce PolySkill, a new framework that enables agents to learn generalizable and compositional skills. The core idea, inspired by polymorphism in software engineering, is to decouple a skill's abstract goal (what it accomplishes) and its concrete implementation (how it is executed). Experiments show that our method (1) improves skill reuse by 1.7x on seen websites and (2) boosts success rates by up to 9.4% on Mind2Web and 13.9% on unseen websites, while reducing steps by over 20%. (3) In self-exploration settings without specified tasks, our framework improves the quality of proposed tasks and enables agents to learn generalizable skills that work across different sites. By enabling the agent to identify and refine its own goals, the PolySkill enhances the agent's ability to learn a better curriculum, leading to the acquisition of more generalizable skills compared to baseline methods. This work provides a practical path toward building agents capable of continual learning in adaptive environments. Our findings show that separating a skill's goal from its execution is a crucial step toward developing autonomous agents that can learn and generalize across the open web continuously.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "29 pages, 6 figures, 8 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.15863v1",
    "published_date": "2025-10-17 17:56:00 UTC",
    "updated_date": "2025-10-17 17:56:00 UTC"
  },
  {
    "arxiv_id": "2510.15862v3",
    "title": "PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold",
    "authors": [
      "Yi Wan",
      "Jiuqi Wang",
      "Liam Li",
      "Jinsong Liu",
      "Ruihao Zhu",
      "Zheqing Zhu"
    ],
    "abstract": "Tool-augmented large language models (LLMs) are emerging as deep research agents, systems that decompose complex queries, retrieve external evidence, and synthesize grounded responses. Yet current agents remain limited by shallow retrieval, weak alignment metrics, and brittle tool-use behavior. We introduce PokeeResearch-7B, a 7B-parameter deep research agent built under a unified reinforcement learning framework for robustness, alignment, and scalability. PokeeResearch-7B is trained by an annotation-free Reinforcement Learning from AI Feedback (RLAIF) framework to optimize policies using LLM-based reward signals that capture factual accuracy, citation faithfulness, and instruction adherence. A chain-of-thought-driven multi-call reasoning scaffold further enhances robustness through self-verification and adaptive recovery from tool failures. Among 10 popular deep research benchmarks, PokeeResearch-7B achieves state-of-the-art performance among 7B-scale deep research agents. This highlights that careful reinforcement learning and reasoning design can produce efficient, resilient, and research-grade AI agents. The model and inference code is open-sourced under Apache 2.0 license at https://github.com/Pokee-AI/PokeeResearchOSS.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15862v3",
    "published_date": "2025-10-17 17:53:06 UTC",
    "updated_date": "2025-10-21 08:23:52 UTC"
  },
  {
    "arxiv_id": "2510.15859v3",
    "title": "InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training",
    "authors": [
      "Pengkai Wang",
      "Linus",
      "Pengwei Liu",
      "Zhijie Sang",
      "Congkai Xie",
      "Hongxia Yang"
    ],
    "abstract": "Reinforcement learning has powered many of the recent breakthroughs in large language models, especially for tasks where rewards can be computed automatically, such as code generation. However, these methods deteriorate in open-ended domains like medical consultation, where feedback is inherently ambiguous, highly context-dependent, and cannot be reduced to a reliable scalar signal. In such settings, RL must either rely on supervision-intensive reward models that often fail to generalize, or it falls into pathological behaviors such as reward hacking - an especially troubling risk for high-stakes medical dialogue. To address these limitations, we introduce ORBIT, an open-ended rubric-based incremental training framework for high-stakes medical dialogue. ORBIT integrates synthetic dialogue generation with dynamically constructed rubrics that serve as adaptive guides for incremental RL. Instead of relying on external medical knowledge bases or handcrafted rule sets, ORBIT uses rubric-driven feedback to steer the learning process. Its judge component can be instantiated with general-purpose instruction-following LLMs, removing the need for any task-specific fine-tuning. Applied to the Qwen3-4B-Instruct model, ORBIT raises the HealthBench-Hard score from 7.0 to 27.5 using only 2k training samples, achieving SOTA performance for models at this scale. With larger rubric datasets, ORBIT-trained models further compete with the strongest open-source baselines on HealthBench-Hard. Our analysis shows that rubric-guided RL consistently improves consultation quality across diverse medical scenarios. We also apply such rubric generation and training pipeline to InfoBench, where ORBIT enhances instruction-following performance, highlighting the generality of rubric-based feedback.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15859v3",
    "published_date": "2025-10-17 17:51:28 UTC",
    "updated_date": "2025-11-28 07:52:44 UTC"
  },
  {
    "arxiv_id": "2510.15850v1",
    "title": "Self-Certifying Primal-Dual Optimization Proxies for Large-Scale Batch Economic Dispatch",
    "authors": [
      "Michael Klamkin",
      "Mathieu Tanneau",
      "Pascal Van Hentenryck"
    ],
    "abstract": "Recent research has shown that optimization proxies can be trained to high fidelity, achieving average optimality gaps under 1% for large-scale problems. However, worst-case analyses show that there exist in-distribution queries that result in orders of magnitude higher optimality gap, making it difficult to trust the predictions in practice. This paper aims at striking a balance between classical solvers and optimization proxies in order to enable trustworthy deployments with interpretable speed-optimality tradeoffs based on a user-defined optimality threshold. To this end, the paper proposes a hybrid solver that leverages duality theory to efficiently bound the optimality gap of predictions, falling back to a classical solver for queries where optimality cannot be certified. To improve the achieved speedup of the hybrid solver, the paper proposes an alternative training procedure that combines the primal and dual proxy training. Experiments on large-scale transmission systems show that the hybrid solver is highly scalable. The proposed hybrid solver achieves speedups of over 1000x compared to a parallelized simplex-based solver while guaranteeing a maximum optimality gap of 2%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15850v1",
    "published_date": "2025-10-17 17:45:21 UTC",
    "updated_date": "2025-10-17 17:45:21 UTC"
  },
  {
    "arxiv_id": "2510.15843v2",
    "title": "Enhanced Sentiment Interpretation via a Lexicon-Fuzzy-Transformer Framework",
    "authors": [
      "Shayan Rokhva",
      "Mousa Alizadeh",
      "Maryam Abdollahi Shamami"
    ],
    "abstract": "Accurately detecting sentiment polarity and intensity in product reviews and social media posts remains challenging due to informal and domain-specific language. To address this, we propose a novel hybrid lexicon-fuzzy-transformer framework that combines rule-based heuristics, contextual deep learning, and fuzzy logic to generate continuous sentiment scores reflecting both polarity and strength. The pipeline begins with VADER-based initial sentiment estimations, which are refined through a two-stage adjustment process. This involves leveraging confidence scores from DistilBERT, a lightweight transformer and applying fuzzy logic principles to mitigate excessive neutrality bias and enhance granularity. A custom fuzzy inference system then maps the refined scores onto a 0 to 1 continuum, producing expert)like judgments. The framework is rigorously evaluated on four domain-specific datasets. food delivery, e-commerce, tourism, and fashion. Results show improved alignment with user ratings, better identification of sentiment extremes, and reduced misclassifications. Both quantitative metrics (distributional alignment, confusion matrices) and qualitative insights (case studies, runtime analysis) affirm the models robustness and efficiency. This work demonstrates the value of integrating symbolic reasoning with neural models for interpretable, finegrained sentiment analysis in linguistically dynamic domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "The manuscript was uploaded in error and is scientifically invalid. It is an incomplete draft with major flaws. Co-authors were not aware of or consenting to this submission and do not endorse it",
    "pdf_url": "https://arxiv.org/pdf/2510.15843v2",
    "published_date": "2025-10-17 17:36:05 UTC",
    "updated_date": "2025-12-09 23:00:34 UTC"
  },
  {
    "arxiv_id": "2510.16095v1",
    "title": "Reliability of Large Language Model Generated Clinical Reasoning in Assisted Reproductive Technology: Blinded Comparative Evaluation Study",
    "authors": [
      "Dou Liu",
      "Ying Long",
      "Sophia Zuoqiu",
      "Di Liu",
      "Kang Li",
      "Yiting Lin",
      "Hanyi Liu",
      "Rong Yin",
      "Tian Tang"
    ],
    "abstract": "Creating high-quality clinical Chains-of-Thought (CoTs) is crucial for explainable medical Artificial Intelligence (AI) while constrained by data scarcity. Although Large Language Models (LLMs) can synthesize medical data, their clinical reliability remains unverified. This study evaluates the reliability of LLM-generated CoTs and investigates prompting strategies to enhance their quality. In a blinded comparative study, senior clinicians in Assisted Reproductive Technology (ART) evaluated CoTs generated via three distinct strategies: Zero-shot, Random Few-shot (using shallow examples), and Selective Few-shot (using diverse, high-quality examples). These expert ratings were compared against evaluations from a state-of-the-art AI model (GPT-4o). The Selective Few-shot strategy significantly outperformed other strategies across all human evaluation metrics (p < .001). Critically, the Random Few-shot strategy offered no significant improvement over the Zero-shot baseline, demonstrating that low-quality examples are as ineffective as no examples. The success of the Selective strategy is attributed to two principles: \"Gold-Standard Depth\" (reasoning quality) and \"Representative Diversity\" (generalization). Notably, the AI evaluator failed to discern these critical performance differences. The clinical reliability of synthetic CoTs is dictated by strategic prompt curation, not the mere presence of examples. We propose a \"Dual Principles\" framework as a foundational methodology to generate trustworthy data at scale. This work offers a validated solution to the data bottleneck and confirms the indispensable role of human expertise in evaluating high-stakes clinical AI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16095v1",
    "published_date": "2025-10-17 17:29:01 UTC",
    "updated_date": "2025-10-17 17:29:01 UTC"
  },
  {
    "arxiv_id": "2510.15830v1",
    "title": "SNOO: Step-K Nesterov Outer Optimizer - The Surprising Effectiveness of Nesterov Momentum Applied to Pseudo-Gradients",
    "authors": [
      "Dominik Kallusky",
      "Vinay Rao",
      "Vishal Nandavanam",
      "Hao-Jun Michael Shi"
    ],
    "abstract": "The rapid development of large language models (LLMs) has driven the demand for more efficient optimization techniques. Among these, the Lookahead family of optimizers employs a two-loop framework, maintaining fast and slow sets of model weights. Multiple inner optimizer steps on the fast weights produce a trajectory - the pseudo-gradient - that is used to update the slow weights. DiLoCo, a notable example originally designed for distributed training, applies Nesterov momentum to the averaged pseudo-gradient from multiple workers, claiming to even outperform AdamW in a non-distributed setup. In this paper, we empirically show that DiLoCo's surprising effectiveness stems primarily from applying Nesterov momentum to the pseudo-gradient, which improves training in a non-distributed setting. We call this Lookahead variant the Step-$K$ Nesterov Outer Optimizer (SNOO). We demonstrate that SNOO achieves compute factor gains of 1.5 - 2.5$\\times$ in a non-distributed setting up to a scale of 1e23 training FLOPs, with improvements that increase with model size. Because of its minimal compute and memory overhead and compatibility with model sharding, SNOO is a practical enhancement for a variety of inner optimizers, including AdamW and Muon.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15830v1",
    "published_date": "2025-10-17 17:11:45 UTC",
    "updated_date": "2025-10-17 17:11:45 UTC"
  },
  {
    "arxiv_id": "2510.15828v1",
    "title": "GENESIS: A Generative Model of Episodic-Semantic Interaction",
    "authors": [
      "Marco D'Alessandro",
      "Leo D'Amato",
      "Mikel Elkano",
      "Mikel Uriz",
      "Giovanni Pezzulo"
    ],
    "abstract": "A central challenge in cognitive neuroscience is to explain how semantic and episodic memory, two major forms of declarative memory, typically associated with cortical and hippocampal processing, interact to support learning, recall, and imagination. Despite significant advances, we still lack a unified computational framework that jointly accounts for core empirical phenomena across both semantic and episodic processing domains. Here, we introduce the Generative Episodic-Semantic Integration System (GENESIS), a computational model that formalizes memory as the interaction between two limited-capacity generative systems: a Cortical-VAE, supporting semantic learning and generalization, and a Hippocampal-VAE, supporting episodic encoding and retrieval within a retrieval-augmented generation (RAG) architecture. GENESIS reproduces hallmark behavioral findings, including generalization in semantic memory, recognition, serial recall effects and gist-based distortions in episodic memory, and constructive episodic simulation, while capturing their dynamic interactions. The model elucidates how capacity constraints shape the fidelity and memorability of experiences, how semantic processing introduces systematic distortions in episodic recall, and how episodic replay can recombine previous experiences. Together, these results provide a principled account of memory as an active, constructive, and resource-bounded process. GENESIS thus advances a unified theoretical framework that bridges semantic and episodic memory, offering new insights into the generative foundations of human cognition.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "17 pages, 6 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.15828v1",
    "published_date": "2025-10-17 17:11:13 UTC",
    "updated_date": "2025-10-17 17:11:13 UTC"
  },
  {
    "arxiv_id": "2510.15821v1",
    "title": "Chronos-2: From Univariate to Universal Forecasting",
    "authors": [
      "Abdul Fatir Ansari",
      "Oleksandr Shchur",
      "Jaris Kken",
      "Andreas Auer",
      "Boran Han",
      "Pedro Mercado",
      "Syama Sundar Rangapuram",
      "Huibin Shen",
      "Lorenzo Stella",
      "Xiyuan Zhang",
      "Mononito Goswami",
      "Shubham Kapoor",
      "Danielle C. Maddix",
      "Pablo Guerron",
      "Tony Hu",
      "Junming Yin",
      "Nick Erickson",
      "Prateek Mutalik Desai",
      "Hao Wang",
      "Huzefa Rangwala",
      "George Karypis",
      "Yuyang Wang",
      "Michael Bohlke-Schneider"
    ],
    "abstract": "Pretrained time series models have enabled inference-only forecasting systems that produce accurate predictions without task-specific training. However, existing approaches largely focus on univariate forecasting, limiting their applicability in real-world scenarios where multivariate data and covariates play a crucial role. We present Chronos-2, a pretrained model capable of handling univariate, multivariate, and covariate-informed forecasting tasks in a zero-shot manner. Chronos-2 employs a group attention mechanism that facilitates in-context learning (ICL) through efficient information sharing across multiple time series within a group, which may represent sets of related series, variates of a multivariate series, or targets and covariates in a forecasting task. These general capabilities are achieved through training on synthetic datasets that impose diverse multivariate structures on univariate series. Chronos-2 delivers state-of-the-art performance across three comprehensive benchmarks: fev-bench, GIFT-Eval, and Chronos Benchmark II. On fev-bench, which emphasizes multivariate and covariate-informed forecasting, Chronos-2's universal ICL capabilities lead to substantial improvements over existing models. On tasks involving covariates, it consistently outperforms baselines by a wide margin. Case studies in the energy and retail domains further highlight its practical advantages. The in-context learning capabilities of Chronos-2 establish it as a general-purpose forecasting model that can be used \"as is\" in real-world forecasting pipelines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15821v1",
    "published_date": "2025-10-17 17:00:53 UTC",
    "updated_date": "2025-10-17 17:00:53 UTC"
  },
  {
    "arxiv_id": "2510.16092v1",
    "title": "Compressing Many-Shots in In-Context Learning",
    "authors": [
      "Devvrit Khatri",
      "Pranamya Kulkarni",
      "Nilesh Gupta",
      "Yerram Varun",
      "Liqian Peng",
      "Jay Yagnik",
      "Praneeth Netrapalli",
      "Cho-Jui Hsieh",
      "Alec Go",
      "Inderjit S Dhillon",
      "Aditya Kusupati",
      "Prateek Jain"
    ],
    "abstract": "Large Language Models (LLMs) have been shown to be able to learn different tasks without explicit finetuning when given many input-output examples / demonstrations through In-Context Learning (ICL). Increasing the number of examples, called ``shots'', improves downstream task performance but incurs higher memory and computational costs. In this work, we study an approach to improve the memory and computational efficiency of ICL inference by compressing the many-shot prompts. Given many shots comprising t tokens, our goal is to generate a m soft-token summary, where m < t. We first show that existing prompt compression methods are ineffective for many-shot compression, and simply using fewer shots as a baseline is surprisingly strong. To achieve effective compression, we find that: (a) a stronger compressor model with more trainable parameters is necessary, and (b) compressing many-shot representations at each transformer layer enables more fine-grained compression by providing each layer with its own compressed representation. Based on these insights, we propose MemCom, a layer-wise compression method. We systematically evaluate various compressor models and training approaches across different model sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence lengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms strong baselines across all compression ratios on multiple classification tasks with large label sets. Notably, while baseline performance degrades sharply at higher compression ratios, often by over 20-30%, MemCom maintains high accuracy with minimal degradation, typically dropping by less than 10%.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16092v1",
    "published_date": "2025-10-17 16:57:42 UTC",
    "updated_date": "2025-10-17 16:57:42 UTC"
  },
  {
    "arxiv_id": "2510.16091v1",
    "title": "Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification",
    "authors": [
      "Binglan Han",
      "Anuradha Mathrani",
      "Teo Susnjak"
    ],
    "abstract": "This study quantifies how prompting strategies interact with large language models (LLMs) to automate the screening stage of systematic literature reviews (SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3, Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types (zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection) across relevance classification and six Level-2 tasks, using accuracy, precision, recall, and F1. Results show pronounced model-prompt interaction effects: CoT-few-shot yields the most reliable precision-recall balance; zero-shot maximizes recall for high-sensitivity passes; and self-reflection underperforms due to over-inclusivity and instability across models. GPT-4o and DeepSeek provide robust overall performance, while GPT-4o-mini performs competitively at a substantially lower dollar cost. A cost-performance analysis for relevance classification (per 1,000 abstracts) reveals large absolute differences among model-prompt pairings; GPT-4o-mini remains low-cost across prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer attractive F1 at a small incremental cost. We recommend a staged workflow that (1) deploys low-cost models with structured prompts for first-pass screening and (2) escalates only borderline cases to higher-capacity models. These findings highlight LLMs' uneven but promising potential to automate literature screening. By systematically analyzing prompt-model interactions, we provide a comparative benchmark and practical guidance for task-adaptive LLM deployment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16091v1",
    "published_date": "2025-10-17 16:53:09 UTC",
    "updated_date": "2025-10-17 16:53:09 UTC"
  },
  {
    "arxiv_id": "2511.04683v1",
    "title": "AI-Powered Citation Auditing: A Zero-Assumption Protocol for Systematic Reference Verification in Academic Research",
    "authors": [
      "L. J. Janse van Rensburg"
    ],
    "abstract": "Academic citation integrity faces persistent challenges, with research indicating 20% of citations contain errors and manual verification requiring months of expert time. This paper presents a novel AI-powered methodology for systematic, comprehensive reference auditing using agentic AI with tool-use capabilities. We develop a zero-assumption verification protocol that independently validates every reference against multiple academic databases (Semantic Scholar, Google Scholar, CrossRef) without assuming any citation is correct. The methodology was validated across 30 academic documents (2,581 references) spanning undergraduate projects to doctoral theses and peer-reviewed publications. Results demonstrate 91.7% average verification rate on published PLOS papers, with successful detection of fabricated references, retracted articles, orphan citations, and predatory journals. Time efficiency improved dramatically: 90-minute audits for 916-reference doctoral theses versus months of manual review. The system achieved <0.5% false positive rate while identifying critical issues manual review might miss. This work establishes the first validated AI-agent methodology for academic citation integrity, demonstrating practical applicability for supervisors, students, and institutional quality assurance.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.DL",
    "comment": "10 pages, 1 table. Code and validation data available at https://github.com/leonjvr/ai-citation-auditor",
    "pdf_url": "https://arxiv.org/pdf/2511.04683v1",
    "published_date": "2025-10-17 16:53:03 UTC",
    "updated_date": "2025-10-17 16:53:03 UTC"
  },
  {
    "arxiv_id": "2510.15808v1",
    "title": "AB-UPT for Automotive and Aerospace Applications",
    "authors": [
      "Benedikt Alkin",
      "Richard Kurle",
      "Louis Serrano",
      "Dennis Just",
      "Johannes Brandstetter"
    ],
    "abstract": "The recently proposed Anchored-Branched Universal Physics Transformers (AB-UPT) shows strong capabilities to replicate automotive computational fluid dynamics simulations requiring orders of magnitudes less compute than traditional numerical solvers. In this technical report, we add two new datasets to the body of empirically evaluated use-cases of AB-UPT, combining high-quality data generation with state-of-the-art neural surrogates. Both datasets were generated with the Luminary Cloud platform containing automotives (SHIFT-SUV) and aircrafts (SHIFT-Wing). We start by detailing the data generation. Next, we show favorable performances of AB-UPT against previous state-of-the-art transformer-based baselines on both datasets, followed by extensive qualitative and quantitative evaluations of our best AB-UPT model. AB-UPT shows strong performances across the board. Notably, it obtains near perfect prediction of integrated aerodynamic forces within seconds from a simple isotopically tesselate geometry representation and is trainable within a day on a single GPU, paving the way for industry-scale applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15808v1",
    "published_date": "2025-10-17 16:40:35 UTC",
    "updated_date": "2025-10-17 16:40:35 UTC"
  },
  {
    "arxiv_id": "2510.16089v1",
    "title": "STABLE: Gated Continual Learning for Large Language Models",
    "authors": [
      "William Hoy",
      "Nurcin Celik"
    ],
    "abstract": "Large language models (LLMs) increasingly require mechanisms for continual adaptation without full retraining. However, sequential updates can lead to catastrophic forgetting, where new edits degrade previously acquired knowledge. This work presents STABLE, a gated continual self editing framework that constrains forgetting during sequential updates using parameter efficient fine tuning via Low Rank Adaptation (LoRA; see arXiv:2106.09685). Each candidate edit is evaluated against a stability budget using one of three metrics: (i) Exact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase, reflecting reduced model confidence; and (iii) KL divergence, quantifying distributional drift between the base and adapted models. If a threshold is exceeded, the LoRA update is rescaled through a clipping procedure or rejected. Experiments on the Qwen-2.5-7B model show that gating effectively mitigates forgetting while preserving adaptability. EM based gating achieved the highest cumulative performance in short continual learning sequences. Our results show that different gating strategies can achieve comparable distribution shift (measured by KL divergence) while producing different accuracy outcomes, highlighting the importance of gating design in continual adaptation. This approach offers a principled method for continual model editing, enabling LLMs to integrate new knowledge while maintaining reliability. Code: https://github.com/Bhoy1/STABLE",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16089v1",
    "published_date": "2025-10-17 16:14:05 UTC",
    "updated_date": "2025-10-17 16:14:05 UTC"
  },
  {
    "arxiv_id": "2510.15782v1",
    "title": "Demo: Guide-RAG: Evidence-Driven Corpus Curation for Retrieval-Augmented Generation in Long COVID",
    "authors": [
      "Philip DiGiacomo",
      "Haoyang Wang",
      "Jinrui Fang",
      "Yan Leng",
      "W Michael Brode",
      "Ying Ding"
    ],
    "abstract": "As AI chatbots gain adoption in clinical medicine, developing effective frameworks for complex, emerging diseases presents significant challenges. We developed and evaluated six Retrieval-Augmented Generation (RAG) corpus configurations for Long COVID (LC) clinical question answering, ranging from expert-curated sources to large-scale literature databases. Our evaluation employed an LLM-as-a-judge framework across faithfulness, relevance, and comprehensiveness metrics using LongCOVID-CQ, a novel dataset of expert-generated clinical questions. Our RAG corpus configuration combining clinical guidelines with high-quality systematic reviews consistently outperformed both narrow single-guideline approaches and large-scale literature databases. Our findings suggest that for emerging diseases, retrieval grounded in curated secondary reviews provides an optimal balance between narrow consensus documents and unfiltered primary literature, supporting clinical decision-making while avoiding information overload and oversimplified guidance. We propose Guide-RAG, a chatbot system and accompanying evaluation framework that integrates both curated expert knowledge and comprehensive literature databases to effectively answer LC clinical questions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: The Second Workshop on GenAI for Health: Potential, Trust, and Policy Compliance",
    "pdf_url": "https://arxiv.org/pdf/2510.15782v1",
    "published_date": "2025-10-17 16:05:52 UTC",
    "updated_date": "2025-10-17 16:05:52 UTC"
  },
  {
    "arxiv_id": "2510.15778v1",
    "title": "Controlling the image generation process with parametric activation functions",
    "authors": [
      "Ilia Pavlov"
    ],
    "abstract": "As image generative models continue to increase not only in their fidelity but also in their ubiquity the development of tools that leverage direct interaction with their internal mechanisms in an interpretable way has received little attention In this work we introduce a system that allows users to develop a better understanding of the model through interaction and experimentation By giving users the ability to replace activation functions of a generative network with parametric ones and a way to set the parameters of these functions we introduce an alternative approach to control the networks output We demonstrate the use of our method on StyleGAN2 and BigGAN networks trained on FFHQ and ImageNet respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "5 pages, 5 figures, accepted for the 16th International Conference on Computational Creativity, ICCC'25",
    "pdf_url": "https://arxiv.org/pdf/2510.15778v1",
    "published_date": "2025-10-17 16:02:23 UTC",
    "updated_date": "2025-10-17 16:02:23 UTC"
  },
  {
    "arxiv_id": "2510.15772v1",
    "title": "Self-evolving expertise in complex non-verifiable subject domains: dialogue as implicit meta-RL",
    "authors": [
      "Richard M. Bailey"
    ],
    "abstract": "So-called `wicked problems', those involving complex multi-dimensional settings, non-verifiable outcomes, heterogeneous impacts and a lack of single objectively correct answers, have plagued humans throughout history. Modern examples include decisions over justice frameworks, solving environmental pollution, planning for pandemic resilience and food security. The use of state-of-the-art artificial intelligence systems (notably Large Language Model-based agents) collaborating with humans on solving such problems is being actively explored. While the abilities of LLMs can be improved by, for example, fine-tuning, hand-crafted system prompts and scaffolding with external tools, LLMs lack endogenous mechanisms to develop expertise through experience in such settings. This work address this gap with Dialectica, a framework where agents engage in structured dialogue on defined topics, augmented by memory, self-reflection, and policy-constrained context editing. Formally, discussion is viewed as an implicit meta-reinforcement learning process. The `dialogue-trained' agents are evaluated post-hoc using judged pairwise comparisons of elicited responses. Across two model architectures (locally run Qwen3:30b and OpenAI's o4-mini) results show that enabling reflection-based context editing during discussion produces agents which dominate their baseline counterparts on Elo scores, normalized Bradley-Terry-Davidson ability, and AlphaRank mass. The predicted signatures of learning are observed qualitatively in statement and reflection logs, where reflections identify weaknesses and reliably shape subsequent statements. Agreement between quantitative and qualitative evidence supports dialogue-driven context evolution as a practical path to targeted expertise amplification in open non-verifiable domains.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "50 pages, 4 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.15772v1",
    "published_date": "2025-10-17 15:59:44 UTC",
    "updated_date": "2025-10-17 15:59:44 UTC"
  },
  {
    "arxiv_id": "2510.15769v1",
    "title": "Preliminary Quantitative Study on Explainability and Trust in AI Systems",
    "authors": [
      "Allen Daniel Sunny"
    ],
    "abstract": "Large-scale AI models such as GPT-4 have accelerated the deployment of artificial intelligence across critical domains including law, healthcare, and finance, raising urgent questions about trust and transparency. This study investigates the relationship between explainability and user trust in AI systems through a quantitative experimental design. Using an interactive, web-based loan approval simulation, we compare how different types of explanations, ranging from basic feature importance to interactive counterfactuals influence perceived trust. Results suggest that interactivity enhances both user engagement and confidence, and that the clarity and relevance of explanations are key determinants of trust. These findings contribute empirical evidence to the growing field of human-centered explainable AI, highlighting measurable effects of explainability design on user perception",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 3 figures, 2 appendices. Quantitative user study on AI explainability and trust. Preprint, 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.15769v1",
    "published_date": "2025-10-17 15:59:28 UTC",
    "updated_date": "2025-10-17 15:59:28 UTC"
  },
  {
    "arxiv_id": "2510.15756v1",
    "title": "Semantic segmentation with coarse annotations",
    "authors": [
      "Jort de Jong",
      "Mike Holenderski"
    ],
    "abstract": "Semantic segmentation is the task of classifying each pixel in an image. Training a segmentation model achieves best results using annotated images, where each pixel is annotated with the corresponding class. When obtaining fine annotations is difficult or expensive, it may be possible to acquire coarse annotations, e.g. by roughly annotating pixels in an images leaving some pixels around the boundaries between classes unlabeled. Segmentation with coarse annotations is difficult, in particular when the objective is to optimize the alignment of boundaries between classes. This paper proposes a regularization method for models with an encoder-decoder architecture with superpixel based upsampling. It encourages the segmented pixels in the decoded image to be SLIC-superpixels, which are based on pixel color and position, independent of the segmentation annotation. The method is applied to FCN-16 fully convolutional network architecture and evaluated on the SUIM, Cityscapes, and PanNuke data sets. It is shown that the boundary recall improves significantly compared to state-of-the-art models when trained on coarse annotations.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15756v1",
    "published_date": "2025-10-17 15:41:27 UTC",
    "updated_date": "2025-10-17 15:41:27 UTC"
  },
  {
    "arxiv_id": "2510.15752v1",
    "title": "NDM: A Noise-driven Detection and Mitigation Framework against Implicit Sexual Intentions in Text-to-Image Generation",
    "authors": [
      "Yitong Sun",
      "Yao Huang",
      "Ruochen Zhang",
      "Huanran Chen",
      "Shouwei Ruan",
      "Ranjie Duan",
      "Xingxing Wei"
    ],
    "abstract": "Despite the impressive generative capabilities of text-to-image (T2I) diffusion models, they remain vulnerable to generating inappropriate content, especially when confronted with implicit sexual prompts. Unlike explicit harmful prompts, these subtle cues, often disguised as seemingly benign terms, can unexpectedly trigger sexual content due to underlying model biases, raising significant ethical concerns. However, existing detection methods are primarily designed to identify explicit sexual content and therefore struggle to detect these implicit cues. Fine-tuning approaches, while effective to some extent, risk degrading the model's generative quality, creating an undesirable trade-off. To address this, we propose NDM, the first noise-driven detection and mitigation framework, which could detect and mitigate implicit malicious intention in T2I generation while preserving the model's original generative capabilities. Specifically, we introduce two key innovations: first, we leverage the separability of early-stage predicted noise to develop a noise-based detection method that could identify malicious content with high accuracy and efficiency; second, we propose a noise-enhanced adaptive negative guidance mechanism that could optimize the initial noise by suppressing the prominent region's attention, thereby enhancing the effectiveness of adaptive negative guidance for sexual mitigation. Experimentally, we validate NDM on both natural and adversarial datasets, demonstrating its superior performance over existing SOTA methods, including SLD, UCE, and RECE, etc. Code and resources are available at https://github.com/lorraine021/NDM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 8 figures, accepted by ACMMM 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.15752v1",
    "published_date": "2025-10-17 15:37:02 UTC",
    "updated_date": "2025-10-17 15:37:02 UTC"
  },
  {
    "arxiv_id": "2510.15748v2",
    "title": "Towards Relaxed Multimodal Inputs for Gait-based Parkinson's Disease Assessment",
    "authors": [
      "Minlin Zeng",
      "Zhipeng Zhou",
      "Yang Qiu",
      "Martin J. McKeown",
      "Zhiqi Shen"
    ],
    "abstract": "Parkinson's disease assessment has garnered growing interest in recent years, particularly with the advent of sensor data and machine learning techniques. Among these, multimodal approaches have demonstrated strong performance by effectively integrating complementary information from various data sources. However, two major limitations hinder their practical application: (1) the need to synchronize all modalities during training, and (2) the dependence on all modalities during inference. To address these issues, we propose the first Parkinson's assessment system that formulates multimodal learning as a multi-objective optimization (MOO) problem. This not only allows for more flexible modality requirements during both training and inference, but also handles modality collapse issue during multimodal information fusion. In addition, to mitigate the imbalance within individual modalities, we introduce a margin-based class rebalancing strategy to enhance category learning. We conduct extensive experiments on three public datasets under both synchronous and asynchronous settings. The results show that our framework-Towards Relaxed InPuts (TRIP)-achieves state-of-the-art performance, outperforming the best baselines by 16.48, 6.89, and 11.55 percentage points in the asynchronous setting, and by 4.86 and 2.30 percentage points in the synchronous setting, highlighting its effectiveness and adaptability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15748v2",
    "published_date": "2025-10-17 15:35:57 UTC",
    "updated_date": "2025-11-04 10:44:01 UTC"
  },
  {
    "arxiv_id": "2510.15746v1",
    "title": "LLMs Judge Themselves: A Game-Theoretic Framework for Human-Aligned Evaluation",
    "authors": [
      "Gao Yang",
      "Yuhang Liu",
      "Siyu Miao",
      "Xinyue Liang",
      "Zhengyang Liu",
      "Heyan Huang"
    ],
    "abstract": "Ideal or real - that is the question.In this work, we explore whether principles from game theory can be effectively applied to the evaluation of large language models (LLMs). This inquiry is motivated by the growing inadequacy of conventional evaluation practices, which often rely on fixed-format tasks with reference answers and struggle to capture the nuanced, subjective, and open-ended nature of modern LLM behavior. To address these challenges, we propose a novel alternative: automatic mutual evaluation, where LLMs assess each other's output through self-play and peer review. These peer assessments are then systematically compared with human voting behavior to evaluate their alignment with human judgment. Our framework incorporates game-theoretic voting algorithms to aggregate peer reviews, enabling a principled investigation into whether model-generated rankings reflect human preferences. Empirical results reveal both convergences and divergences between theoretical predictions and human evaluations, offering valuable insights into the promises and limitations of mutual evaluation. To the best of our knowledge, this is the first work to jointly integrate mutual evaluation, game-theoretic aggregation, and human-grounded validation for evaluating the capabilities of LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15746v1",
    "published_date": "2025-10-17 15:34:25 UTC",
    "updated_date": "2025-10-17 15:34:25 UTC"
  },
  {
    "arxiv_id": "2510.15739v1",
    "title": "AURA: An Agent Autonomy Risk Assessment Framework",
    "authors": [
      "Lorenzo Satta Chiris",
      "Ayush Mishra"
    ],
    "abstract": "As autonomous agentic AI systems see increasing adoption across organisations, persistent challenges in alignment, governance, and risk management threaten to impede deployment at scale. We present AURA (Agent aUtonomy Risk Assessment), a unified framework designed to detect, quantify, and mitigate risks arising from agentic AI. Building on recent research and practical deployments, AURA introduces a gamma-based risk scoring methodology that balances risk assessment accuracy with computational efficiency and practical considerations. AURA provides an interactive process to score, evaluate and mitigate the risks of running one or multiple AI Agents, synchronously or asynchronously (autonomously). The framework is engineered for Human-in-the-Loop (HITL) oversight and presents Agent-to-Human (A2H) communication mechanisms, allowing for seamless integration with agentic systems for autonomous self-assessment, rendering it interoperable with established protocols (MCP and A2A) and tools. AURA supports a responsible and transparent adoption of agentic AI and provides robust risk detection and mitigation while balancing computational resources, positioning it as a critical enabler for large-scale, governable agentic AI in enterprise environments.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 2 figures. Submitted for open-access preprint on arXiv. Based on the AAMAS 2026 paper template",
    "pdf_url": "https://arxiv.org/pdf/2510.15739v1",
    "published_date": "2025-10-17 15:30:29 UTC",
    "updated_date": "2025-10-17 15:30:29 UTC"
  },
  {
    "arxiv_id": "2510.15731v2",
    "title": "Attention Sinks in Diffusion Language Models",
    "authors": [
      "Maximo Eduardo Rulli",
      "Simone Petruzzi",
      "Edoardo Michielon",
      "Fabrizio Silvestri",
      "Simone Scardapane",
      "Alessio Devoto"
    ],
    "abstract": "Masked Diffusion Language Models (DLMs) have recently emerged as a promising alternative to traditional Autoregressive Models (ARMs). DLMs employ transformer encoders with bidirectional attention, enabling parallel token generation while maintaining competitive performance. Although their efficiency and effectiveness have been extensively studied, the internal mechanisms that govern DLMs remain largely unexplored. In this work, we conduct an empirical analysis of DLM attention patterns, focusing on the attention sinking phenomenon, an effect previously observed in various transformer-based architectures. Our findings reveal that DLMs also exhibit attention sinks, but with distinct characteristics. First, unlike in ARMs, the sink positions in DLMs tend to shift throughout the generation process, displaying a dynamic behaviour. Second, while ARMs are highly sensitive to the removal of attention sinks, DLMs remain robust: masking sinks leads to only a minor degradation in performance. These results provide new insights into the inner workings of diffusion-based language models and highlight fundamental differences in how they allocate and utilize attention compared to autoregressive models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15731v2",
    "published_date": "2025-10-17 15:23:58 UTC",
    "updated_date": "2025-12-10 10:22:15 UTC"
  },
  {
    "arxiv_id": "2510.16085v1",
    "title": "MoPHES:Leveraging on-device LLMs as Agent for Mobile Psychological Health Evaluation and Support",
    "authors": [
      "Xun Wei",
      "Pukai Zhou",
      "Zeyu Wang"
    ],
    "abstract": "The 2022 World Mental Health Report calls for global mental health care reform, amid rising prevalence of issues like anxiety and depression that affect nearly one billion people worldwide. Traditional in-person therapy fails to meet this demand, and the situation is worsened by stigma. While general-purpose large language models (LLMs) offer efficiency for AI-driven mental health solutions, they underperform because they lack specialized fine-tuning. Existing LLM-based mental health chatbots can engage in empathetic conversations, but they overlook real-time user mental state assessment which is critical for professional counseling. This paper proposes MoPHES, a framework that integrates mental state evaluation, conversational support, and professional treatment recommendations. The agent developed under this framework uses two fine-tuned MiniCPM4-0.5B LLMs: one is fine-tuned on mental health conditions datasets to assess users' mental states and predict the severity of anxiety and depression; the other is fine-tuned on multi-turn dialogues to handle conversations with users. By leveraging insights into users' mental states, our agent provides more tailored support and professional treatment recommendations. Both models are also deployed directly on mobile devices to enhance user convenience and protect user privacy. Additionally, to evaluate the performance of MoPHES with other LLMs, we develop a benchmark for the automatic evaluation of mental state prediction and multi-turn counseling dialogues, which includes comprehensive evaluation metrics, datasets, and methods.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2510.16085v1",
    "published_date": "2025-10-17 15:22:42 UTC",
    "updated_date": "2025-10-17 15:22:42 UTC"
  },
  {
    "arxiv_id": "2510.15728v1",
    "title": "RLAF: Reinforcement Learning from Automaton Feedback",
    "authors": [
      "Mahyar Alinejad",
      "Alvaro Velasquez",
      "Yue Wang",
      "George Atia"
    ],
    "abstract": "Reinforcement Learning (RL) in environments with complex, history-dependent reward structures poses significant challenges for traditional methods. In this work, we introduce a novel approach that leverages automaton-based feedback to guide the learning process, replacing explicit reward functions with preferences derived from a deterministic finite automaton (DFA). Unlike conventional approaches that use automata for direct reward specification, our method employs the structure of the DFA to generate preferences over trajectories that are used to learn a reward function, eliminating the need for manual reward engineering. Our framework introduces a static approach that uses the learned reward function directly for policy optimization and a dynamic approach that involves continuous refining of the reward function and policy through iterative updates until convergence.\n  Our experiments in both discrete and continuous environments demonstrate that our approach enables the RL agent to learn effective policies for tasks with temporal dependencies, outperforming traditional reward engineering and automaton-based baselines such as reward machines and LTL-guided methods. Our results highlight the advantages of automaton-based preferences in handling non-Markovian rewards, offering a scalable, efficient, and human-independent alternative to traditional reward modeling. We also provide a convergence guarantee showing that under standard assumptions our automaton-guided preference-based framework learns a policy that is near-optimal with respect to the true non-Markovian objective.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15728v1",
    "published_date": "2025-10-17 15:17:01 UTC",
    "updated_date": "2025-10-17 15:17:01 UTC"
  },
  {
    "arxiv_id": "2510.15727v2",
    "title": "Invoice Information Extraction: Methods and Performance Evaluation",
    "authors": [
      "Sai Yashwant",
      "Anurag Dubey",
      "Praneeth Paikray",
      "Gantala Thulsiram"
    ],
    "abstract": "This paper presents methods for extracting structured information from invoice documents and proposes a set of evaluation metrics (EM) to assess the accuracy of the extracted data against annotated ground truth. The approach involves pre-processing scanned or digital invoices, applying Docling and LlamaCloud Services to identify and extract key fields such as invoice number, date, total amount, and vendor details. To ensure the reliability of the extraction process, we establish a robust evaluation framework comprising field-level precision, consistency check failures, and exact match accuracy. The proposed metrics provide a standardized way to compare different extraction methods and highlight strengths and weaknesses in field-specific performance.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15727v2",
    "published_date": "2025-10-17 15:16:24 UTC",
    "updated_date": "2025-10-22 09:51:58 UTC"
  },
  {
    "arxiv_id": "2510.15725v1",
    "title": "DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification",
    "authors": [
      "Tingyu Lin",
      "Armin Dadras",
      "Florian Kleber",
      "Robert Sablatnig"
    ],
    "abstract": "Camera movement classification (CMC) models trained on contemporary, high-quality footage often degrade when applied to archival film, where noise, missing frames, and low contrast obscure motion cues. We bridge this gap by assembling a unified benchmark that consolidates two modern corpora into four canonical classes and restructures the HISTORIAN collection into five balanced categories. Building on this benchmark, we introduce DGME-T, a lightweight extension to the Video Swin Transformer that injects directional grid motion encoding, derived from optical flow, via a learnable and normalised late-fusion layer. DGME-T raises the backbone's top-1 accuracy from 81.78% to 86.14% and its macro F1 from 82.08% to 87.81% on modern clips, while still improving the demanding World-War-II footage from 83.43% to 84.62% accuracy and from 81.72% to 82.63% macro F1. A cross-domain study further shows that an intermediate fine-tuning stage on modern data increases historical performance by more than five percentage points. These results demonstrate that structured motion priors and transformer representations are complementary and that even a small, carefully calibrated motion head can substantially enhance robustness in degraded film analysis. Related resources are available at https://github.com/linty5/DGME-T.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages, accepted at ACMMM2025 SUMAC",
    "pdf_url": "https://arxiv.org/pdf/2510.15725v1",
    "published_date": "2025-10-17 15:14:11 UTC",
    "updated_date": "2025-10-17 15:14:11 UTC"
  },
  {
    "arxiv_id": "2510.15720v2",
    "title": "ProSh: Probabilistic Shielding for Model-free Reinforcement Learning",
    "authors": [
      "Edwin Hamel-De le Court",
      "Gaspard Ohlmann",
      "Francesco Belardinelli"
    ],
    "abstract": "Safety is a major concern in reinforcement learning (RL): we aim at developing RL systems that not only perform optimally, but are also safe to deploy by providing formal guarantees about their safety. To this end, we introduce Probabilistic Shielding via Risk Augmentation (ProSh), a model-free algorithm for safe reinforcement learning under cost constraints. ProSh augments the Constrained MDP state space with a risk budget and enforces safety by applying a shield to the agent's policy distribution using a learned cost critic. The shield ensures that all sampled actions remain safe in expectation. We also show that optimality is preserved when the environment is deterministic. Since ProSh is model-free, safety during training depends on the knowledge we have acquired about the environment. We provide a tight upper-bound on the cost in expectation, depending only on the backup-critic accuracy, that is always satisfied during training. Under mild, practically achievable assumptions, ProSh guarantees safety even at training time, as shown in the experiments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15720v2",
    "published_date": "2025-10-17 15:08:51 UTC",
    "updated_date": "2025-10-21 10:10:58 UTC"
  },
  {
    "arxiv_id": "2510.15716v1",
    "title": "Direct Preference Optimization with Unobserved Preference Heterogeneity: The Necessity of Ternary Preferences",
    "authors": [
      "Keertana Chidambaram",
      "Karthik Vinary Seetharaman",
      "Vasilis Syrgkanis"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become central to aligning large language models with human values, typically by first learning a reward model from preference data which is then used to update the model with reinforcement learning. Recent alternatives such as Direct Preference Optimization (DPO) simplify this pipeline by directly optimizing on preferences. However, both approaches often assume uniform annotator preferences and rely on binary comparisons, overlooking two key limitations: the diversity of human evaluators and the limitations of pairwise feedback. In this work, we address both these issues. First, we connect preference learning in RLHF with the econometrics literature and show that binary comparisons are insufficient for identifying latent user preferences from finite user data and infinite users, while (even incomplete) rankings over three or more responses ensure identifiability. Second, we introduce methods to incorporate heterogeneous preferences into alignment algorithms. We develop an Expectation-Maximization adaptation of DPO that discovers latent annotator types and trains a mixture of LLMs accordingly. Then we propose an aggregation algorithm using a min-max regret fairness criterion to produce a single generative policy with equitable performance guarantees. Together, these contributions establish a theoretical and algorithmic framework for fairness and personalization for diverse users in generative model alignment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15716v1",
    "published_date": "2025-10-17 15:00:40 UTC",
    "updated_date": "2025-10-17 15:00:40 UTC"
  },
  {
    "arxiv_id": "2510.16083v1",
    "title": "PassREfinder-FL: Privacy-Preserving Credential Stuffing Risk Prediction via Graph-Based Federated Learning for Representing Password Reuse between Websites",
    "authors": [
      "Jaehan Kim",
      "Minkyoo Song",
      "Minjae Seo",
      "Youngjin Jin",
      "Seungwon Shin",
      "Jinwoo Kim"
    ],
    "abstract": "Credential stuffing attacks have caused significant harm to online users who frequently reuse passwords across multiple websites. While prior research has attempted to detect users with reused passwords or identify malicious login attempts, existing methods often compromise usability by restricting password creation or website access, and their reliance on complex account-sharing mechanisms hinders real-world deployment. To address these limitations, we propose PassREfinder-FL, a novel framework that predicts credential stuffing risks across websites. We introduce the concept of password reuse relations -- defined as the likelihood of users reusing passwords between websites -- and represent them as edges in a website graph. Using graph neural networks (GNNs), we perform a link prediction task to assess credential reuse risk between sites. Our approach scales to a large number of arbitrary websites by incorporating public website information and linking newly observed websites as nodes in the graph. To preserve user privacy, we extend PassREfinder-FL with a federated learning (FL) approach that eliminates the need to share user sensitive information across administrators. Evaluation on a real-world dataset of 360 million breached accounts from 22,378 websites shows that PassREfinder-FL achieves an F1-score of 0.9153 in the FL setting. We further validate that our FL-based GNN achieves a 4-11% performance improvement over other state-of-the-art GNN models through an ablation study. Finally, we demonstrate that the predicted results can be used to quantify password reuse likelihood as actionable risk scores.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by Elsevier Expert Systems with Applications",
    "pdf_url": "https://arxiv.org/pdf/2510.16083v1",
    "published_date": "2025-10-17 14:59:24 UTC",
    "updated_date": "2025-10-17 14:59:24 UTC"
  },
  {
    "arxiv_id": "2510.16082v1",
    "title": "Interpretable RNA-Seq Clustering with an LLM-Based Agentic Evidence-Grounded Framework",
    "authors": [
      "Elias Hossain",
      "Mehrdad Shoeibi",
      "Ivan Garibay",
      "Niloofar Yousefi"
    ],
    "abstract": "We propose CITE V.1, an agentic, evidence-grounded framework that leverages Large Language Models (LLMs) to provide transparent and reproducible interpretations of RNA-seq clusters. Unlike existing enrichment-based approaches that reduce results to broad statistical associations and LLM-only models that risk unsupported claims or fabricated citations, CITE V.1 transforms cluster interpretation by producing biologically coherent explanations explicitly anchored in the biomedical literature. The framework orchestrates three specialized agents: a Retriever that gathers domain knowledge from PubMed and UniProt, an Interpreter that formulates functional hypotheses, and Critics that evaluate claims, enforce evidence grounding, and qualify uncertainty through confidence and reliability indicators. Applied to Salmonella enterica RNA-seq data, CITE V.1 generated biologically meaningful insights supported by the literature, while an LLM-only Gemini baseline frequently produced speculative results with false citations. By moving RNA-seq analysis from surface-level enrichment to auditable, interpretable, and evidence-based hypothesis generation, CITE V.1 advances the transparency and reliability of AI in biomedicine.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16082v1",
    "published_date": "2025-10-17 14:56:05 UTC",
    "updated_date": "2025-10-17 14:56:05 UTC"
  },
  {
    "arxiv_id": "2510.15701v1",
    "title": "Beyond-Diagonal RIS Under Non-Idealities: Learning-Based Architecture Discovery and Optimization",
    "authors": [
      "Binggui Zhou",
      "Bruno Clerckx"
    ],
    "abstract": "Beyond-diagonal reconfigurable intelligent surface (BD-RIS) has recently been introduced to enable advanced control over electromagnetic waves to further increase the benefits of traditional RIS in enhancing signal quality and improving spectral and energy efficiency for next-generation wireless networks. A significant issue in designing and deploying BD-RIS is the tradeoff between its performance and circuit complexity. Despite some efforts in exploring optimal architectures with the lowest circuit complexities for ideal BD-RIS, architecture discovery for non-ideal BD-RIS remains uninvestigated. Therefore, how non-idealities and circuit complexity jointly affect the performance of BD-RIS remains unclear, making it difficult to achieve the performance - circuit complexity tradeoff in the presence of non-idealities. Essentially, architecture discovery for non-ideal BD-RIS faces challenges from both the computational complexity of global architecture search and the difficulty in achieving global optima. To tackle these challenges, we propose a learning-based two-tier architecture discovery framework (LTTADF) consisting of an architecture generator and a performance optimizer to jointly discover optimal architectures of non-ideal BD-RIS given specific circuit complexities, which can effectively explore over a large architecture space while avoiding getting trapped in poor local optima and thus achieving near-optimal solutions for the performance optimization. Numerical results provide valuable insights for deploying non-ideal BD-RIS considering the performance - circuit complexity tradeoff.",
    "categories": [
      "cs.IT",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.IT",
    "comment": "13 pages, 13 figures, 1 table. This paper has been submitted to IEEE journal for possible publication",
    "pdf_url": "https://arxiv.org/pdf/2510.15701v1",
    "published_date": "2025-10-17 14:46:08 UTC",
    "updated_date": "2025-10-17 14:46:08 UTC"
  },
  {
    "arxiv_id": "2510.15700v1",
    "title": "ProofOptimizer: Training Language Models to Simplify Proofs without Human Demonstrations",
    "authors": [
      "Alex Gu",
      "Bartosz Piotrowski",
      "Fabian Gloeckle",
      "Kaiyu Yang",
      "Aram H. Markosyan"
    ],
    "abstract": "Neural theorem proving has advanced rapidly in the past year, reaching IMO gold-medalist capabilities and producing formal proofs that span thousands of lines. Although such proofs are mechanically verified by formal systems like Lean, their excessive length renders them difficult for humans to comprehend and limits their usefulness for mathematical insight. Proof simplification is therefore a critical bottleneck. Yet, training data for this task is scarce, and existing methods -- mainly agentic scaffolding with off-the-shelf LLMs -- struggle with the extremely long proofs generated by RL-trained provers. We introduce ProofOptimizer, the first language model trained to simplify Lean proofs without requiring additional human supervision. ProofOptimizer is trained via expert iteration and reinforcement learning, using Lean to verify simplifications and provide training signal. At inference time, it operates within an iterative proof-shortening workflow, progressively reducing proof length. Experiments show that ProofOptimizer substantially compresses proofs generated by state-of-the-art RL-trained provers on standard benchmarks, reducing proof length by 87% on miniF2F, 57% on PutnamBench, and 49% on Seed-Prover's IMO 2025 proofs. Beyond conciseness, the simplified proofs check faster in Lean and further improve downstream prover performance when reused as training data for supervised finetuning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.LG",
    "comment": "52 pages, 16 figures, website: http://proof-optimizer.github.io/",
    "pdf_url": "https://arxiv.org/pdf/2510.15700v1",
    "published_date": "2025-10-17 14:45:30 UTC",
    "updated_date": "2025-10-17 14:45:30 UTC"
  },
  {
    "arxiv_id": "2510.15691v3",
    "title": "Exploring the Synergy of Quantitative Factors and Newsflow Representations from Large Language Models for Stock Return Prediction",
    "authors": [
      "Tian Guo",
      "Emmanuel Hauptmann"
    ],
    "abstract": "In quantitative investing, return prediction supports various tasks, including stock selection, portfolio optimization, and risk management. Quantitative factors, such as valuation, quality, and growth, capture various characteristics of stocks. Unstructured data, like news and transcripts, has attracted growing attention, driven by recent advances in large language models (LLMs). This paper examines effective methods for leveraging multimodal factors and newsflow in return prediction and stock selection. First, we introduce a fusion learning framework to learn a unified representation from factors and newsflow representations generated by an LLM. Within this framework, we compare three methods of different architectural complexities: representation combination, representation summation, and attentive representations. Next, building on the limitation of fusion learning observed in empirical comparison, we explore the mixture model that adaptively combines predictions made by single modalities and their fusion. To mitigate the training instability of the mixture model, we introduce a decoupled training approach with theoretical insights. Finally, our experiments on real investment universes yield several insights into effective multimodal modeling of factors and news for stock return prediction and selection.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "q-fin.CP",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15691v3",
    "published_date": "2025-10-17 14:35:03 UTC",
    "updated_date": "2025-11-25 11:27:22 UTC"
  },
  {
    "arxiv_id": "2510.15688v1",
    "title": "KS-Net: Multi-layer network model for determining the rotor type from motor parameters in interior PMSMs",
    "authors": [
      "Kivanc Dogan",
      "Ahmet Orhan"
    ],
    "abstract": "The demand for high efficiency and precise control in electric drive systems has led to the widespread adoption of Interior Permanent Magnet Synchronous Motors (IPMSMs). The performance of these motors is significantly influenced by rotor geometry. Traditionally, rotor shape analysis has been conducted using the finite element method (FEM), which involves high computational costs. This study aims to classify the rotor shape (2D type, V type, Nabla type) of IPMSMs using electromagnetic parameters through machine learning-based methods and to demonstrate the applicability of this approach as an alternative to classical methods. In this context, a custom deep learning model, KS-Net, developed by the user, was comparatively evaluated against Cubic SVM, Quadratic SVM, Fine KNN, Cosine KNN, and Fine Tree algorithms. The balanced dataset, consisting of 9,000 samples, was tested using 10-fold cross-validation, and performance metrics such as accuracy, precision, recall, and F1-score were employed. The results indicate that the Cubic SVM and Quadratic SVM algorithms classified all samples flawlessly, achieving 100% accuracy, while the KS-Net model achieved 99.98% accuracy with only two misclassifications, demonstrating competitiveness with classical methods. This study shows that the rotor shape of IPMSMs can be predicted with high accuracy using data-driven approaches, offering a fast and cost-effective alternative to FEM-based analyses. The findings provide a solid foundation for accelerating motor design processes, developing automated rotor identification systems, and enabling data-driven fault diagnosis in engineering applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "This study was presented at the 3rd International Conference on Advances and Innovations in Engineering (ICAIE) and published in the conference proceedings",
    "pdf_url": "https://arxiv.org/pdf/2510.15688v1",
    "published_date": "2025-10-17 14:32:05 UTC",
    "updated_date": "2025-10-17 14:32:05 UTC"
  },
  {
    "arxiv_id": "2510.15684v1",
    "title": "Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI",
    "authors": [
      "Gerard Comas-Quiles",
      "Carles Garcia-Cabrera",
      "Julia Dietlmeier",
      "Noel E. O'Connor",
      "Ferran Marques"
    ],
    "abstract": "Unsupervised anomaly detection (UAD) presents a complementary alternative to supervised learning for brain tumor segmentation in magnetic resonance imaging (MRI), particularly when annotated datasets are limited, costly, or inconsistent. In this work, we propose a novel Multimodal Vision Transformer Autoencoder (MViT-AE) trained exclusively on healthy brain MRIs to detect and localize tumors via reconstruction-based error maps. This unsupervised paradigm enables segmentation without reliance on manual labels, addressing a key scalability bottleneck in neuroimaging workflows. Our method is evaluated in the BraTS-GoAT 2025 Lighthouse dataset, which includes various types of tumors such as gliomas, meningiomas, and pediatric brain tumors. To enhance performance, we introduce a multimodal early-late fusion strategy that leverages complementary information across multiple MRI sequences, and a post-processing pipeline that integrates the Segment Anything Model (SAM) to refine predicted tumor contours. Despite the known challenges of UAD, particularly in detecting small or non-enhancing lesions, our method achieves clinically meaningful tumor localization, with lesion-wise Dice Similarity Coefficient of 0.437 (Whole Tumor), 0.316 (Tumor Core), and 0.350 (Enhancing Tumor) on the test set, and an anomaly Detection Rate of 89.4% on the validation set. These findings highlight the potential of transformer-based unsupervised models to serve as scalable, label-efficient tools for neuro-oncological imaging.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 5 figures, BraTS GoAT 2025 challenge",
    "pdf_url": "https://arxiv.org/pdf/2510.15684v1",
    "published_date": "2025-10-17 14:26:30 UTC",
    "updated_date": "2025-10-17 14:26:30 UTC"
  },
  {
    "arxiv_id": "2510.15683v1",
    "title": "Mixture of Experts Approaches in Dense Retrieval Tasks",
    "authors": [
      "Effrosyni Sokli",
      "Pranav Kasela",
      "Georgios Peikos",
      "Gabriella Pasi"
    ],
    "abstract": "Dense Retrieval Models (DRMs) are a prominent development in Information Retrieval (IR). A key challenge with these neural Transformer-based models is that they often struggle to generalize beyond the specific tasks and domains they were trained on. To address this challenge, prior research in IR incorporated the Mixture-of-Experts (MoE) framework within each Transformer layer of a DRM, which, though effective, substantially increased the number of additional parameters. In this paper, we propose a more efficient design, which introduces a single MoE block (SB-MoE) after the final Transformer layer. To assess the retrieval effectiveness of SB-MoE, we perform an empirical evaluation across three IR tasks. Our experiments involve two evaluation setups, aiming to assess both in-domain effectiveness and the model's zero-shot generalizability. In the first setup, we fine-tune SB-MoE with four different underlying DRMs on seven IR benchmarks and evaluate them on their respective test sets. In the second setup, we fine-tune SB-MoE on MSMARCO and perform zero-shot evaluation on thirteen BEIR datasets. Additionally, we perform further experiments to analyze the model's dependency on its hyperparameters (i.e., the number of employed and activated experts) and investigate how this variation affects SB-MoE's performance. The obtained results show that SB-MoE is particularly effective for DRMs with lightweight base models, such as TinyBERT and BERT-Small, consistently exceeding standard model fine-tuning across benchmarks. For DRMs with more parameters, such as BERT-Base and Contriever, our model requires a larger number of training samples to achieve improved retrieval performance. Our code is available online at: https://github.com/FaySokli/SB-MoE.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "8 pages, 4 figures, 3 tables, reproducible code available at https://github.com/FaySokli/SB-MoE , Accepted for publication in Proceedings of the 2025 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT 2025)",
    "pdf_url": "https://arxiv.org/pdf/2510.15683v1",
    "published_date": "2025-10-17 14:23:19 UTC",
    "updated_date": "2025-10-17 14:23:19 UTC"
  },
  {
    "arxiv_id": "2510.16081v1",
    "title": "SARHAchat: An LLM-Based Chatbot for Sexual and Reproductive Health Counseling",
    "authors": [
      "Jiaye Yang",
      "Xinyu Zhao",
      "Tianlong Chen",
      "Kandyce Brennan"
    ],
    "abstract": "While Artificial Intelligence (AI) shows promise in healthcare applications, existing conversational systems often falter in complex and sensitive medical domains such as Sexual and Reproductive Health (SRH). These systems frequently struggle with hallucination and lack the specialized knowledge required, particularly for sensitive SRH topics. Furthermore, current AI approaches in healthcare tend to prioritize diagnostic capabilities over comprehensive patient care and education. Addressing these gaps, this work at the UNC School of Nursing introduces SARHAchat, a proof-of-concept Large Language Model (LLM)-based chatbot. SARHAchat is designed as a reliable, user-centered system integrating medical expertise with empathetic communication to enhance SRH care delivery. Our evaluation demonstrates SARHAchat's ability to provide accurate and contextually appropriate contraceptive counseling while maintaining a natural conversational flow. The demo is available at https://sarhachat.com/}{https://sarhachat.com/.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "5 pages, 1 figure",
    "pdf_url": "https://arxiv.org/pdf/2510.16081v1",
    "published_date": "2025-10-17 14:22:49 UTC",
    "updated_date": "2025-10-17 14:22:49 UTC"
  },
  {
    "arxiv_id": "2510.15681v2",
    "title": "ProofBridge: Auto-Formalization of Natural Language Proofs in Lean via Joint Embeddings",
    "authors": [
      "Prithwish Jana",
      "Kaan Kale",
      "Ahmet Ege Tanriverdi",
      "Cruise Song",
      "Sriram Vishwanath",
      "Vijay Ganesh"
    ],
    "abstract": "Translating human-written mathematical theorems and proofs from natural language (NL) into formal languages (FLs) like Lean 4 has long been a significant challenge for AI. Most state-of-the-art methods either focus on theorem-only NL-to-FL auto-formalization or on FL proof synthesis from FL theorems. In practice, auto-formalization of both theorem and proof still requires human intervention, as seen in AlphaProof's silver-medal performance at the 2024 IMO, where problem statements were manually translated before automated proof synthesis.\n  We present ProofBridge, a unified framework for automatically translating entire NL theorems and proofs into Lean 4. At its core is a joint embedding model that aligns NL and FL (NL-FL) theorem-proof pairs in a shared semantic space, enabling cross-modal retrieval of semantically relevant FL examples to guide translation. Our training ensures that NL-FL theorems (and their proofs) are mapped close together in this space if and only if the NL-FL pairs are semantically equivalent. ProofBridge integrates retrieval-augmented fine-tuning with iterative proof repair, leveraging Lean's type checker and semantic equivalence feedback to ensure both syntactic correctness and semantic fidelity. Experiments show substantial improvements in proof auto-formalization over strong baselines (including GPT-5, Gemini-2.5, Kimina-Prover, DeepSeek-Prover), with our retrieval-augmented approach yielding significant gains in semantic correctness (SC, via proving bi-directional equivalence) and type correctness (TC, via type-checking theorem+proof) across pass@k metrics on miniF2F-Test-PF, a dataset we curated. In particular, ProofBridge improves cross-modal retrieval quality by up to 3.28x Recall@1 over all-MiniLM-L6-v2, and achieves +31.14% SC and +1.64% TC (pass@32) compared to the baseline Kimina-Prover-RL-1.7B.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15681v2",
    "published_date": "2025-10-17 14:20:50 UTC",
    "updated_date": "2025-12-07 23:34:50 UTC"
  },
  {
    "arxiv_id": "2510.15674v1",
    "title": "CarBoN: Calibrated Best-of-N Sampling Improves Test-time Reasoning",
    "authors": [
      "Yung-Chen Tang",
      "Pin-Yu Chen",
      "Andrea Cavallaro"
    ],
    "abstract": "Allocating more computation during inference time (test-time scaling) improves language model performance, especially for reasoning tasks. However, popular methods like Best-of-$N$ sampling often show diminishing returns as $N$ increases. To address this inefficiency, we introduce a general test-time calibration framework that adaptively modifies the model toward high-reward reasoning paths, with theoretical guarantees of improving the lower bound of expected reward under finite sampling, all without large language model (LLM) retraining. Within this framework, we propose CarBoN (Calibrated Best-of-$N$), a two-phase method that first explores the solution space and then learns a calibration of the logits via an input-specific temperature $T$ and additive shift vector $$, guiding generation toward more reliable reasoning. Experiments on MATH-500 and AIME-2024 show that CarBoN improves efficiency, with up to $4\\times$ fewer rollouts to reach the same accuracy, while often achieving higher accuracy under fixed budgets. We also analyze the complementary roles of $T$ and $$ in balancing output diversity and correctness, and demonstrate that the framework also generalizes to step-level sampling strategies such as beam search. For more information, please refer to our project page at huggingface.co/spaces/TrustSafeAI/Test-Time-Calibration.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15674v1",
    "published_date": "2025-10-17 14:04:37 UTC",
    "updated_date": "2025-10-17 14:04:37 UTC"
  },
  {
    "arxiv_id": "2510.15673v1",
    "title": "Valeo Near-Field: a novel dataset for pedestrian intent detection",
    "authors": [
      "Antonyo Musabini",
      "Rachid Benmokhtar",
      "Jagdish Bhanushali",
      "Victor Galizzi",
      "Bertrand Luvison",
      "Xavier Perrotton"
    ],
    "abstract": "This paper presents a novel dataset aimed at detecting pedestrians' intentions as they approach an ego-vehicle. The dataset comprises synchronized multi-modal data, including fisheye camera feeds, lidar laser scans, ultrasonic sensor readings, and motion capture-based 3D body poses, collected across diverse real-world scenarios. Key contributions include detailed annotations of 3D body joint positions synchronized with fisheye camera images, as well as accurate 3D pedestrian positions extracted from lidar data, facilitating robust benchmarking for perception algorithms. We release a portion of the dataset along with a comprehensive benchmark suite, featuring evaluation metrics for accuracy, efficiency, and scalability on embedded systems. By addressing real-world challenges such as sensor occlusions, dynamic environments, and hardware constraints, this dataset offers a unique resource for developing and evaluating state-of-the-art algorithms in pedestrian detection, 3D pose estimation and 4D trajectory and intention prediction. Additionally, we provide baseline performance metrics using custom neural network architectures and suggest future research directions to encourage the adoption and enhancement of the dataset. This work aims to serve as a foundation for researchers seeking to advance the capabilities of intelligent vehicles in near-field scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15673v1",
    "published_date": "2025-10-17 14:02:54 UTC",
    "updated_date": "2025-10-17 14:02:54 UTC"
  },
  {
    "arxiv_id": "2510.18888v1",
    "title": "Contextual Augmentation for Entity Linking using Large Language Models",
    "authors": [
      "Daniel Vollmers",
      "Hamada M. Zahera",
      "Diego Moussallem",
      "Axel-Cyrille Ngonga Ngomo"
    ],
    "abstract": "Entity Linking involves detecting and linking entity mentions in natural language texts to a knowledge graph. Traditional methods use a two-step process with separate models for entity recognition and disambiguation, which can be computationally intensive and less effective. We propose a fine-tuned model that jointly integrates entity recognition and disambiguation in a unified framework. Furthermore, our approach leverages large language models to enrich the context of entity mentions, yielding better performance in entity disambiguation. We evaluated our approach on benchmark datasets and compared with several baselines. The evaluation results show that our approach achieves state-of-the-art performance on out-of-domain datasets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.18888v1",
    "published_date": "2025-10-17 13:37:21 UTC",
    "updated_date": "2025-10-17 13:37:21 UTC"
  },
  {
    "arxiv_id": "2510.15647v1",
    "title": "Enhance Large Language Models as Recommendation Systems with Collaborative Filtering",
    "authors": [
      "Zhisheng Yang",
      "Xiaofei Xu",
      "Ke Deng",
      "Li Li"
    ],
    "abstract": "As powerful tools in Natural Language Processing (NLP), Large Language Models (LLMs) have been leveraged for crafting recommendations to achieve precise alignment with user preferences and elevate the quality of the recommendations. The existing approaches implement both non-tuning and tuning strategies. Compared to following the tuning strategy, the approaches following the non-tuning strategy avoid the relatively costly, time-consuming, and expertise-requiring process of further training pre-trained LLMs on task-specific datasets, but they suffer the issue of not having the task-specific business or local enterprise knowledge. To the best of our knowledge, none of the existing approaches following the non-tuning strategy explicitly integrates collaborative filtering, one of the most successful recommendation techniques. This study aims to fill the gap by proposing critique-based LLMs as recommendation systems (Critic-LLM-RS). For our purpose, we train a separate machine-learning model called Critic that implements collaborative filtering for recommendations by learning from the interactions between many users and items. The Critic provides critiques to LLMs to significantly refine the recommendations. Extensive experiments have verified the effectiveness of Critic-LLM-RS on real datasets.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15647v1",
    "published_date": "2025-10-17 13:35:14 UTC",
    "updated_date": "2025-10-17 13:35:14 UTC"
  },
  {
    "arxiv_id": "2511.05505v1",
    "title": "Rewiring Human Brain Networks via Lightweight Dynamic Connectivity Framework: An EEG-Based Stress Validation",
    "authors": [
      "Sayantan Acharya",
      "Abbas Khosravi",
      "Douglas Creighton",
      "Roohallah Alizadehsani",
      "U. Rajendra Acharya"
    ],
    "abstract": "In recent years, Electroencephalographic analysis has gained prominence in stress research when combined with AI and Machine Learning models for validation. In this study, a lightweight dynamic brain connectivity framework based on Time Varying Directed Transfer Function is proposed, where TV DTF features were validated through ML based stress classification. TV DTF estimates the directional information flow between brain regions across distinct EEG frequency bands, thereby capturing temporal and causal influences that are often overlooked by static functional connectivity measures. EEG recordings from the 32 channel SAM 40 dataset were employed, focusing on mental arithmetic task trials. The dynamic EEG-based TV-DTF features were validated through ML classifiers such as Support Vector Machine, Random Forest, Gradient Boosting, Adaptive Boosting, and Extreme Gradient Boosting. Experimental results show that alpha-TV-DTF provided the strongest discriminative power, with SVM achieving 89.73% accuracy in 3-class classification and with XGBoost achieving 93.69% accuracy in 2 class classification. Relative to absolute power and phase locking based functional connectivity features, alpha TV DTF and beta TV DTF achieved higher performance across the ML models, highlighting the advantages of dynamic over static measures. Feature importance analysis further highlighted dominant long-range frontal parietal and frontal occipital informational influences, emphasizing the regulatory role of frontal regions under stress. These findings validate the lightweight TV-DTF as a robust framework, revealing spatiotemporal brain dynamics and directional influences across different stress levels.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "21 pages, 21 figures, 6 tables, 50 references,",
    "pdf_url": "https://arxiv.org/pdf/2511.05505v1",
    "published_date": "2025-10-17 13:21:48 UTC",
    "updated_date": "2025-10-17 13:21:48 UTC"
  },
  {
    "arxiv_id": "2510.15624v1",
    "title": "Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation",
    "authors": [
      "Ed Li",
      "Junyu Ren",
      "Xintian Pan",
      "Cat Yan",
      "Chuanhao Li",
      "Dirk Bergemann",
      "Zhuoran Yang"
    ],
    "abstract": "The automation of scientific discovery represents a critical milestone in Artificial Intelligence (AI) research. However, existing agentic systems for science suffer from two fundamental limitations: rigid, pre-programmed workflows that cannot adapt to intermediate findings, and inadequate context management that hinders long-horizon research. We present \\texttt{freephdlabor}, an open-source multiagent framework featuring \\textit{fully dynamic workflows} determined by real-time agent reasoning and a \\coloremph{\\textit{modular architecture}} enabling seamless customization -- users can modify, add, or remove agents to address domain-specific requirements. The framework provides comprehensive infrastructure including \\textit{automatic context compaction}, \\textit{workspace-based communication} to prevent information degradation, \\textit{memory persistence} across sessions, and \\textit{non-blocking human intervention} mechanisms. These features collectively transform automated research from isolated, single-run attempts into \\textit{continual research programs} that build systematically on prior explorations and incorporate human feedback. By providing both the architectural principles and practical implementation for building customizable co-scientist systems, this work aims to facilitate broader adoption of automated research across scientific domains, enabling practitioners to deploy interactive multiagent systems that autonomously conduct end-to-end research -- from ideation through experimentation to publication-ready manuscripts.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "37 pages, 5 figures. Code: https://github.com/ltjed/freephdlabor",
    "pdf_url": "https://arxiv.org/pdf/2510.15624v1",
    "published_date": "2025-10-17 13:13:32 UTC",
    "updated_date": "2025-10-17 13:13:32 UTC"
  },
  {
    "arxiv_id": "2510.15623v1",
    "title": "CQD-SHAP: Explainable Complex Query Answering via Shapley Values",
    "authors": [
      "Parsa Abbasi",
      "Stefan Heindorf"
    ],
    "abstract": "Complex query answering (CQA) goes beyond the well-studied link prediction task by addressing more sophisticated queries that require multi-hop reasoning over incomplete knowledge graphs (KGs). Research on neural and neurosymbolic CQA methods is still an emerging field. Almost all of these methods can be regarded as black-box models, which may raise concerns about user trust. Although neurosymbolic approaches like CQD are slightly more interpretable, allowing intermediate results to be tracked, the importance of different parts of the query remains unexplained. In this paper, we propose CQD-SHAP, a novel framework that computes the contribution of each query part to the ranking of a specific answer. This contribution explains the value of leveraging a neural predictor that can infer new knowledge from an incomplete KG, rather than a symbolic approach relying solely on existing facts in the KG. CQD-SHAP is formulated based on Shapley values from cooperative game theory and satisfies all the fundamental Shapley axioms. Automated evaluation of these explanations in terms of necessary and sufficient explanations, and comparisons with various baselines, shows the effectiveness of this approach for most query types.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15623v1",
    "published_date": "2025-10-17 13:09:26 UTC",
    "updated_date": "2025-10-17 13:09:26 UTC"
  },
  {
    "arxiv_id": "2510.15600v1",
    "title": "Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism",
    "authors": [
      "Haoran Sun",
      "Yankai Jiang",
      "Zhenyu Tang",
      "Yaning Pan",
      "Shuang Gu",
      "Zekai Lin",
      "Lilong Wang",
      "Wenjie Lou",
      "Lei Liu",
      "Lei Bai",
      "Xiaosong Wang"
    ],
    "abstract": "The foundation of reproducible science lies in protocols that are precise, logically ordered, and executable. The autonomous generation of these protocols through natural language queries could greatly improve the efficiency of the reproduction process. However, current leading large language models (LLMs) often generate incomplete or inconsistent protocols, limiting their utility. To address this limitation, we first introduce SciRecipe, a large-scale dataset of over 12K structured protocols spanning 27 biological subfields and encompassing both comprehension and problem-solving tasks. To further improve protocol generation, we propose the \"Sketch-and-Fill\" paradigm, which separates analysis, structuring, and expression to ensure each step is explicit and verifiable. Complementing this, the structured component-based reward mechanism evaluates step granularity, action order, and semantic fidelity, aligning model optimization with experimental reliability. Building on these components, we develop Thoth, trained through a staged Knowledge-to-Action process that progresses from knowledge acquisition to operational reasoning and ultimately to robust, executable protocol generation. Across multiple benchmarks, Thoth consistently surpasses both proprietary and open-source LLMs, achieving significant improvements in step alignment, logical sequencing, and semantic accuracy. Our approach paves the way for reliable scientific assistants that bridge knowledge with experimental execution. All data, code, and models will be released publicly.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15600v1",
    "published_date": "2025-10-17 12:47:50 UTC",
    "updated_date": "2025-10-17 12:47:50 UTC"
  },
  {
    "arxiv_id": "2510.15591v1",
    "title": "Context-aware deep learning using individualized prior information reduces false positives in disease risk prediction and longitudinal health assessment",
    "authors": [
      "Lavanya Umapathy",
      "Patricia M Johnson",
      "Tarun Dutt",
      "Angela Tong",
      "Madhur Nayan",
      "Hersh Chandarana",
      "Daniel K Sodickson"
    ],
    "abstract": "Temporal context in medicine is valuable in assessing key changes in patient health over time. We developed a machine learning framework to integrate diverse context from prior visits to improve health monitoring, especially when prior visits are limited and their frequency is variable. Our model first estimates initial risk of disease using medical data from the most recent patient visit, then refines this assessment using information digested from previously collected imaging and/or clinical biomarkers. We applied our framework to prostate cancer (PCa) risk prediction using data from a large population (28,342 patients, 39,013 magnetic resonance imaging scans, 68,931 blood tests) collected over nearly a decade. For predictions of the risk of clinically significant PCa at the time of the visit, integrating prior context directly converted false positives to true negatives, increasing overall specificity while preserving high sensitivity. False positive rates were reduced progressively from 51% to 33% when integrating information from up to three prior imaging examinations, as compared to using data from a single visit, and were further reduced to 24% when also including additional context from prior clinical data. For predicting the risk of PCa within five years of the visit, incorporating prior context reduced false positive rates still further (64% to 9%). Our findings show that information collected over time provides relevant context to enhance the specificity of medical risk prediction. For a wide range of progressive conditions, sufficient reduction of false positive rates using context could offer a pathway to expand longitudinal health monitoring programs to large populations with comparatively low baseline risk of disease, leading to earlier detection and improved health outcomes.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "18 pages, 5 figures, 1 table",
    "pdf_url": "https://arxiv.org/pdf/2510.15591v1",
    "published_date": "2025-10-17 12:38:57 UTC",
    "updated_date": "2025-10-17 12:38:57 UTC"
  },
  {
    "arxiv_id": "2510.16080v1",
    "title": "TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration",
    "authors": [
      "Kerem Delikoyun",
      "Qianyu Chen",
      "Win Sen Kuan",
      "John Tshon Yit Soong",
      "Matthew Edward Cove",
      "Oliver Hayden"
    ],
    "abstract": "Emergency departments worldwide face rising patient volumes, workforce shortages, and variability in triage decisions that threaten the delivery of timely and accurate care. Current triage methods rely primarily on vital signs, routine laboratory values, and clinicians' judgment, which, while effective, often miss emerging biological signals that could improve risk prediction for infection typing or antibiotic administration in acute conditions. To address this challenge, we introduce TriAgent, a large language model (LLM)-based multi-agent framework that couples automated biomarker discovery with deep research for literature-grounded validation and novelty assessment. TriAgent employs a supervisor research agent to generate research topics and delegate targeted queries to specialized sub-agents for evidence retrieval from various data sources. Findings are synthesized to classify biomarkers as either grounded in existing knowledge or flagged as novel candidates, offering transparent justification and highlighting unexplored pathways in acute care risk stratification. Unlike prior frameworks limited to existing routine clinical biomarkers, TriAgent aims to deliver an end-to-end framework from data analysis to literature grounding to improve transparency, explainability and expand the frontier of potentially actionable clinical biomarkers. Given a user's clinical query and quantitative triage data, TriAgent achieved a topic adherence F1 score of 55.7 +/- 5.0%, surpassing the CoT-ReAct agent by over 10%, and a faithfulness score of 0.42 +/- 0.39, exceeding all baselines by more than 50%. Across experiments, TriAgent consistently outperformed state-of-the-art LLM-based agentic frameworks in biomarker justification and literature-grounded novelty assessment. We share our repo: https://github.com/CellFace/TriAgent.",
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "primary_category": "q-bio.QM",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16080v1",
    "published_date": "2025-10-17 12:26:29 UTC",
    "updated_date": "2025-10-17 12:26:29 UTC"
  },
  {
    "arxiv_id": "2510.15579v1",
    "title": "Lightweight CycleGAN Models for Cross-Modality Image Transformation and Experimental Quality Assessment in Fluorescence Microscopy",
    "authors": [
      "Mohammad Soltaninezhad",
      "Yashar Rouzbahani",
      "Jhonatan Contreras",
      "Rohan Chippalkatti",
      "Daniel Kwaku Abankwa",
      "Christian Eggeling",
      "Thomas Bocklitz"
    ],
    "abstract": "Lightweight deep learning models offer substantial reductions in computational cost and environmental impact, making them crucial for scientific applications. We present a lightweight CycleGAN for modality transfer in fluorescence microscopy (confocal to super-resolution STED/deconvolved STED), addressing the common challenge of unpaired datasets. By replacing the traditional channel-doubling strategy in the U-Net-based generator with a fixed channel approach, we drastically reduce trainable parameters from 41.8 million to approximately nine thousand, achieving superior performance with faster training and lower memory usage. We also introduce the GAN as a diagnostic tool for experimental and labeling quality. When trained on high-quality images, the GAN learns the characteristics of optimal imaging; deviations between its generated outputs and new experimental images can reveal issues such as photobleaching, artifacts, or inaccurate labeling. This establishes the model as a practical tool for validating experimental accuracy and image fidelity in microscopy workflows.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 8 Figures",
    "pdf_url": "https://arxiv.org/pdf/2510.15579v1",
    "published_date": "2025-10-17 12:20:18 UTC",
    "updated_date": "2025-10-17 12:20:18 UTC"
  },
  {
    "arxiv_id": "2510.16079v1",
    "title": "EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle",
    "authors": [
      "Rong Wu",
      "Xiaoman Wang",
      "Jianbiao Mei",
      "Pinlong Cai",
      "Daocheng Fu",
      "Cheng Yang",
      "Licheng Wen",
      "Xuemeng Yang",
      "Yufan Shen",
      "Yuxin Wang",
      "Botian Shi"
    ],
    "abstract": "Current Large Language Model (LLM) agents show strong performance in tool use, but lack the crucial capability to systematically learn from their own experiences. While existing frameworks mainly focus on mitigating external knowledge gaps, they fail to address a more fundamental limitation: the inability to iteratively refine problem-solving strategies. In this work, we introduce EvolveR, a framework designed to enable agent to self-improve through a complete, closed-loop experience lifecycle. This lifecycle comprises two key stages: (1) Offline Self-Distillation, where the agent's interaction trajectories are synthesized into a structured repository of abstract, reusable strategic principles; (2) Online Interaction, where the agent interacts with tasks and actively retrieves distilled principles to guide its decision-making, accumulating a diverse set of behavioral trajectories. This loop employs a policy reinforcement mechanism to iteratively update the agent based on its performance. We demonstrate the effectiveness of EvolveR on complex multi-hop question-answering benchmarks, where it achieves superior performance over strong agentic baselines. Our work presents a comprehensive blueprint for agents that learn not only from external data but also from the consequences of their own actions, paving the way for more autonomous and continuously improving systems. Code is available at https://github.com/Edaizi/EvolveR.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16079v1",
    "published_date": "2025-10-17 12:03:16 UTC",
    "updated_date": "2025-10-17 12:03:16 UTC"
  },
  {
    "arxiv_id": "2510.15568v1",
    "title": "The Spark Effect: On Engineering Creative Diversity in Multi-Agent AI Systems",
    "authors": [
      "Alexander Doudkin",
      "Anton Voelker",
      "Friedrich von Borries"
    ],
    "abstract": "Creative services teams increasingly rely on large language models (LLMs) to accelerate ideation, yet production systems often converge on homogeneous outputs that fail to meet brand or artistic expectations. Art of X developed persona-conditioned LLM agents -- internally branded as \"Sparks\" and instantiated through a library of role-inspired system prompts -- to intentionally diversify agent behaviour within a multi-agent workflow. This white paper documents the problem framing, experimental design, and quantitative evidence behind the Spark agent programme. Using an LLM-as-a-judge protocol calibrated against human gold standards, we observe a mean diversity gain of +4.1 points (on a 1-10 scale) when persona-conditioned Spark agents replace a uniform system prompt, narrowing the gap to human experts to 1.0 point. We also surface evaluator bias and procedural considerations for future deployments.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "10 pages, 2 figures, 2 tables. This project was collaboratively developed with the Art of X UG (haftungsbeschraenkt) AI Research team and HFBK Hamburg, with initial funding from the Hamburg Open Online University (HOOU) program",
    "pdf_url": "https://arxiv.org/pdf/2510.15568v1",
    "published_date": "2025-10-17 11:56:18 UTC",
    "updated_date": "2025-10-17 11:56:18 UTC"
  },
  {
    "arxiv_id": "2510.15566v1",
    "title": "SpikeVox: Towards Energy-Efficient Speech Therapy Framework with Spike-driven Generative Language Models",
    "authors": [
      "Rachmad Vidya Wicaksana Putra",
      "Aadithyan Rajesh Nair",
      "Muhammad Shafique"
    ],
    "abstract": "Speech disorders can significantly affect the patients capability to communicate, learn, and socialize. However, existing speech therapy solutions (e.g., therapist or tools) are still limited and costly, hence such solutions remain inadequate for serving millions of patients worldwide. To address this, state-of-the-art methods employ neural network (NN) algorithms to help accurately detecting speech disorders. However, these methods do not provide therapy recommendation as feedback, hence providing partial solution for patients. Moreover, these methods incur high energy consumption due to their complex and resource-intensive NN processing, hence hindering their deployments on low-power/energy platforms (e.g., smartphones). Toward this, we propose SpikeVox, a novel framework for enabling energy-efficient speech therapy solutions through spike-driven generative language model. Specifically, SpikeVox employs a speech recognition module to perform highly accurate speech-to-text conversion; leverages a spike-driven generative language model to efficiently perform pattern analysis for speech disorder detection and generates suitable exercises for therapy; provides guidance on correct pronunciation as feedback; as well as utilizes the REST API to enable seamless interaction for users. Experimental results demonstrate that SpikeVox achieves 88% confidence level on average in speech disorder recognition, while providing a complete feedback for therapy exercises. Therefore, SpikeVox provides a comprehensive framework for energy-efficient speech therapy solutions, and potentially addresses the significant global speech therapy access gap.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at the IEEE Biomedical Circuits and Systems Conference (BioCAS) 2025, Abu Dhabi, UAE",
    "pdf_url": "https://arxiv.org/pdf/2510.15566v1",
    "published_date": "2025-10-17 11:54:55 UTC",
    "updated_date": "2025-10-17 11:54:55 UTC"
  },
  {
    "arxiv_id": "2510.15560v1",
    "title": "JudgeSQL: Reasoning over SQL Candidates with Weighted Consensus Tournament",
    "authors": [
      "Jiayuan Bai",
      "Xuan-guang Pan",
      "Chongyang Tao",
      "Shuai Ma"
    ],
    "abstract": "Text-to-SQL is a pivotal task that bridges natural language understanding and structured data access, yet it remains fundamentally challenging due to semantic ambiguity and complex compositional reasoning. While large language models (LLMs) have greatly advanced SQL generation though prompting, supervised finetuning and reinforced tuning, the shift toward test-time scaling exposes a new bottleneck: selecting the correct query from a diverse candidate pool. Existing selection approaches, such as self-consistency or best-of-$N$ decoding, provide only shallow signals, making them prone to inconsistent scoring, fragile reasoning chains, and a failure to capture fine-grained semantic distinctions between closely related SQL candidates. To this end, we introduce JudgeSQL, a principled framework that redefines SQL candidate selection through structured reasoning and weighted consensus tournament mechanism. JudgeSQL develops a reasoning-based SQL judge model that distills reasoning traces with reinforcement learning guided by verifiable rewards, enabling accurate and interpretable judgments. Building on this, a weighted consensus tournament integrates explicit reasoning preferences with implicit generator confidence, yielding selections that are both more reliable and more efficient. Extensive experiments on the BIRD benchmark demonstrate that JudgeSQL exhibits superior SQL judgment capabilities and good cross-scale generalization and robustness to generator capacity.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.15560v1",
    "published_date": "2025-10-17 11:46:38 UTC",
    "updated_date": "2025-10-17 11:46:38 UTC"
  },
  {
    "arxiv_id": "2510.15558v1",
    "title": "KITE: A Benchmark for Evaluating Korean Instruction-Following Abilities in Large Language Models",
    "authors": [
      "Dongjun Kim",
      "Chanhee Park",
      "Chanjun Park",
      "Heuiseok Lim"
    ],
    "abstract": "The instruction-following capabilities of large language models (LLMs) are pivotal for numerous applications, from conversational agents to complex reasoning systems. However, current evaluations predominantly focus on English models, neglecting the linguistic and cultural nuances of other languages. Specifically, Korean, with its distinct syntax, rich morphological features, honorific system, and dual numbering systems, lacks a dedicated benchmark for assessing open-ended instruction-following capabilities. To address this gap, we introduce the Korean Instruction-following Task Evaluation (KITE), a comprehensive benchmark designed to evaluate both general and Korean-specific instructions. Unlike existing Korean benchmarks that focus mainly on factual knowledge or multiple-choice testing, KITE directly targets diverse, open-ended instruction-following tasks. Our evaluation pipeline combines automated metrics with human assessments, revealing performance disparities across models and providing deeper insights into their strengths and weaknesses. By publicly releasing the KITE dataset and code, we aim to foster further research on culturally and linguistically inclusive LLM development and inspire similar endeavors for other underrepresented languages.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 3 figures, 5 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.15558v1",
    "published_date": "2025-10-17 11:45:15 UTC",
    "updated_date": "2025-10-17 11:45:15 UTC"
  },
  {
    "arxiv_id": "2510.15557v1",
    "title": "ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents",
    "authors": [
      "Tingyu Lin",
      "Marco Peer",
      "Florian Kleber",
      "Robert Sablatnig"
    ],
    "abstract": "This paper presents ClapperText, a benchmark dataset for handwritten and printed text recognition in visually degraded and low-resource settings. The dataset is derived from 127 World War II-era archival video segments containing clapperboards that record structured production metadata such as date, location, and camera-operator identity. ClapperText includes 9,813 annotated frames and 94,573 word-level text instances, 67% of which are handwritten and 1,566 are partially occluded. Each instance includes transcription, semantic category, text type, and occlusion status, with annotations available as rotated bounding boxes represented as 4-point polygons to support spatially precise OCR applications. Recognizing clapperboard text poses significant challenges, including motion blur, handwriting variation, exposure fluctuations, and cluttered backgrounds, mirroring broader challenges in historical document analysis where structured content appears in degraded, non-standard forms. We provide both full-frame annotations and cropped word images to support downstream tasks. Using a consistent per-video evaluation protocol, we benchmark six representative recognition and seven detection models under zero-shot and fine-tuned conditions. Despite the small training set (18 videos), fine-tuning leads to substantial performance gains, highlighting ClapperText's suitability for few-shot learning scenarios. The dataset offers a realistic and culturally grounded resource for advancing robust OCR and document understanding in low-resource archival contexts. The dataset and evaluation code are available at https://github.com/linty5/ClapperText.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages, accepted at ICDAR2025 DALL",
    "pdf_url": "https://arxiv.org/pdf/2510.15557v1",
    "published_date": "2025-10-17 11:44:08 UTC",
    "updated_date": "2025-10-17 11:44:08 UTC"
  },
  {
    "arxiv_id": "2510.16078v1",
    "title": "ISO/IEC-Compliant Match-on-Card Face Verification with Short Binary Templates",
    "authors": [
      "Abdelilah Ganmati",
      "Karim Afdel",
      "Lahcen Koutti"
    ],
    "abstract": "We present a practical match-on-card design for face verification in which compact 64/128-bit templates are produced off-card by PCA-ITQ and compared on-card via constant-time Hamming distance. We specify ISO/IEC 7816-4 and 14443-4 command APDUs with fixed-length payloads and decision-only status words (no score leakage), together with a minimal per-identity EEPROM map. Using real binary codes from a CelebA working set (55 identities, 412 images), we (i) derive operating thresholds from ROC/DET, (ii) replay enroll->verify transactions at those thresholds, and (iii) bound end-to-end time by pure link latency plus a small constant on-card budget. Even at the slowest contact rate (9.6 kbps), total verification time is 43.9 ms (64 b) and 52.3 ms (128 b); at 38.4 kbps both are <14 ms. At FAR = 1%, both code lengths reach TPR = 0.836, while 128 b lowers EER relative to 64 b. An optional +6 B helper (targeted symbol-level parity over empirically unstable bits) is latency-negligible. Overall, short binary templates, fixed-payload decision-only APDUs, and constant-time matching satisfy ISO/IEC transport constraints with wide timing margin and align with ISO/IEC 24745 privacy goals. Limitations: single-dataset evaluation and design-level (pre-hardware) timing; we outline AgeDB/CFP-FP and on-card microbenchmarks as next steps.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CR",
    "comment": "~14 pages, 6 figures, 6 tables. Source uses elsarticle class; all figures included as PNG/PDF. Primary: cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.16078v1",
    "published_date": "2025-10-17 11:42:56 UTC",
    "updated_date": "2025-10-17 11:42:56 UTC"
  },
  {
    "arxiv_id": "2510.15552v2",
    "title": "Think Parallax: Solving Multi-Hop Problems via Multi-View Knowledge-Graph-Based Retrieval-Augmented Generation",
    "authors": [
      "Jinliang Liu",
      "Jiale Bai",
      "Shaoning Zeng"
    ],
    "abstract": "Large language models (LLMs) excel at language understanding but often hallucinate and struggle with multi-hop reasoning. Knowledge-graph-based retrieval-augmented generation (KG-RAG) offers grounding, yet most methods rely on flat embeddings and noisy path exploration. We propose ParallaxRAG, a framework that symmetrically decouples queries and graph triples into multi-view spaces, enabling a robust retrieval architecture that explicitly enforces head diversity while constraining weakly related paths. Central to our approach is the observation that different attention heads specialize in semantic relations at distinct reasoning stages, contributing to different hops of the reasoning chain. This specialization allows ParallaxRAG to construct cleaner subgraphs and guide LLMs through grounded, step-wise reasoning. Experiments on WebQSP and CWQ, under our unified, reproducible setup (BGE-M3 + Llama3.1-8B), demonstrate competitive retrieval and QA performance, alongside reduced hallucination and good generalization. Our results highlight multi-view head specialization as a principled direction for knowledge-grounded multi-hop reasoning. Our implementation will be released as soon as the paper is accepted.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15552v2",
    "published_date": "2025-10-17 11:34:27 UTC",
    "updated_date": "2025-12-29 07:32:34 UTC"
  },
  {
    "arxiv_id": "2510.15551v1",
    "title": "Rethinking Cross-lingual Gaps from a Statistical Viewpoint",
    "authors": [
      "Vihari Piratla",
      "Purvam Jain",
      "Darshan Singh",
      "Partha Talukdar",
      "Trevor Cohn"
    ],
    "abstract": "Any piece of knowledge is usually expressed in one or a handful of natural languages on the web or in any large corpus. Large Language Models (LLMs) act as a bridge by acquiring knowledge from a source language and making it accessible when queried from target languages. Prior research has pointed to a cross-lingual gap, viz., a drop in accuracy when the knowledge is queried in a target language compared to when the query is in the source language. Existing research has rationalized divergence in latent representations in source and target languages as the source of cross-lingual gap. In this work, we take an alternative view and hypothesize that the variance of responses in the target language is the main cause of this gap. For the first time, we formalize the cross-lingual gap in terms of bias-variance decomposition. We present extensive experimental evidence which support proposed formulation and hypothesis. We then reinforce our hypothesis through multiple inference-time interventions that control the variance and reduce the cross-lingual gap. We demonstrate a simple prompt instruction to reduce the response variance, which improved target accuracy by 20-25% across different models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "22 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.15551v1",
    "published_date": "2025-10-17 11:34:04 UTC",
    "updated_date": "2025-10-17 11:34:04 UTC"
  },
  {
    "arxiv_id": "2510.15547v1",
    "title": "Hypergraph Contrastive Sensor Fusion for Multimodal Fault Diagnosis in Induction Motors",
    "authors": [
      "Usman Ali",
      "Ali Zia",
      "Waqas Ali",
      "Umer Ramzan",
      "Abdul Rehman",
      "Muhammad Tayyab Chaudhry",
      "Wei Xiang"
    ],
    "abstract": "Reliable induction motor (IM) fault diagnosis is vital for industrial safety and operational continuity, mitigating costly unplanned downtime. Conventional approaches often struggle to capture complex multimodal signal relationships, are constrained to unimodal data or single fault types, and exhibit performance degradation under noisy or cross-domain conditions. This paper proposes the Multimodal Hypergraph Contrastive Attention Network (MM-HCAN), a unified framework for robust fault diagnosis. To the best of our knowledge, MM-HCAN is the first to integrate contrastive learning within a hypergraph topology specifically designed for multimodal sensor fusion, enabling the joint modelling of intra- and inter-modal dependencies and enhancing generalisation beyond Euclidean embedding spaces. The model facilitates simultaneous diagnosis of bearing, stator, and rotor faults, addressing the engineering need for consolidated di- agnostic capabilities. Evaluated on three real-world benchmarks, MM-HCAN achieves up to 99.82% accuracy with strong cross-domain generalisation and resilience to noise, demonstrating its suitability for real-world deployment. An ablation study validates the contribution of each component. MM-HCAN provides a scalable and robust solution for comprehensive multi-fault diagnosis, supporting predictive maintenance and extended asset longevity in industrial environments.",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "eess.SP",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "Submitted to IEEE Sensors Journal",
    "pdf_url": "https://arxiv.org/pdf/2510.15547v1",
    "published_date": "2025-10-17 11:28:26 UTC",
    "updated_date": "2025-10-17 11:28:26 UTC"
  },
  {
    "arxiv_id": "2510.15545v3",
    "title": "TokenTiming: A Dynamic Alignment Method for Universal Speculative Decoding Model Pairs",
    "authors": [
      "Sibo Xiao",
      "Jinyuan Fu",
      "Zhongle Xie",
      "Lidan Shou"
    ],
    "abstract": "Accelerating the inference of large language models (LLMs) has been a critical challenge in generative AI. Speculative decoding (SD) substantially improves LLM inference efficiency. However, its utility is limited by a fundamental constraint: the draft and target models must share the same vocabulary, thus limiting the herd of available draft models and often necessitating the training of a new model from scratch. Inspired by Dynamic Time Warping (DTW), a classic algorithm for aligning time series, we propose the algorithm TokenTiming for universal speculative decoding. It operates by re-encoding the draft token sequence to get a new target token sequence, and then uses DTW to build a mapping to transfer the probability distributions for speculative sampling. Benefiting from this, our method accommodates mismatched vocabularies and works with any off-the-shelf models without retraining and modification. We conduct comprehensive experiments on various tasks, demonstrating 1.57x speedup. This work enables a universal approach for draft model selection, making SD a more versatile and practical tool for LLM acceleration.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15545v3",
    "published_date": "2025-10-17 11:25:36 UTC",
    "updated_date": "2025-12-28 21:29:33 UTC"
  },
  {
    "arxiv_id": "2510.15543v1",
    "title": "MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval",
    "authors": [
      "Qiyu Wu",
      "Shuyang Cui",
      "Satoshi Hayakawa",
      "Wei-Yao Wang",
      "Hiromi Wakaki",
      "Yuki Mitsufuji"
    ],
    "abstract": "Multimodal retrieval, which seeks to retrieve relevant content across modalities such as text or image, supports applications from AI search to contents production. Despite the success of separate-encoder approaches like CLIP align modality-specific embeddings with contrastive learning, recent multimodal large language models (MLLMs) enable a unified encoder that directly processes composed inputs. While flexible and advanced, we identify that unified encoders trained with conventional contrastive learning are prone to learn modality shortcut, leading to poor robustness under distribution shifts. We propose a modality composition awareness framework to mitigate this issue. Concretely, a preference loss enforces multimodal embeddings to outperform their unimodal counterparts, while a composition regularization objective aligns multimodal embeddings with prototypes composed from its unimodal parts. These objectives explicitly model structural relationships between the composed representation and its unimodal counterparts. Experiments on various benchmarks show gains in out-of-distribution retrieval, highlighting modality composition awareness as a effective principle for robust composed multimodal retrieval when utilizing MLLMs as the unified encoder.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15543v1",
    "published_date": "2025-10-17 11:20:35 UTC",
    "updated_date": "2025-10-17 11:20:35 UTC"
  },
  {
    "arxiv_id": "2510.16077v1",
    "title": "Continual Knowledge Consolidation LORA for Domain Incremental Learning",
    "authors": [
      "Naeem Paeedeh",
      "Mahardhika Pratama",
      "Weiping Ding",
      "Jimmy Cao",
      "Wolfgang Mayer",
      "Ryszard Kowalczyk"
    ],
    "abstract": "Domain Incremental Learning (DIL) is a continual learning sub-branch that aims to address never-ending arrivals of new domains without catastrophic forgetting problems. Despite the advent of parameter-efficient fine-tuning (PEFT) approaches, existing works create task-specific LoRAs overlooking shared knowledge across tasks. Inaccurate selection of task-specific LORAs during inference results in significant drops in accuracy, while existing works rely on linear or prototype-based classifiers, which have suboptimal generalization powers. Our paper proposes continual knowledge consolidation low rank adaptation (CONEC-LoRA) addressing the DIL problems. CONEC-LoRA is developed from consolidations between task-shared LORA to extract common knowledge and task-specific LORA to embrace domain-specific knowledge. Unlike existing approaches, CONEC-LoRA integrates the concept of a stochastic classifier whose parameters are sampled from a distribution, thus enhancing the likelihood of correct classifications. Last but not least, an auxiliary network is deployed to optimally predict the task-specific LoRAs for inferences and implements the concept of a different-depth network structure in which every layer is connected with a local classifier to take advantage of intermediate representations. This module integrates the ball-generator loss and transformation module to address the synthetic sample bias problem. Our rigorous experiments demonstrate the advantage of CONEC-LoRA over prior arts in 4 popular benchmark problems with over 5% margins.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16077v1",
    "published_date": "2025-10-17 11:16:08 UTC",
    "updated_date": "2025-10-17 11:16:08 UTC"
  },
  {
    "arxiv_id": "2510.16076v1",
    "title": "BPL: Bias-adaptive Preference Distillation Learning for Recommender System",
    "authors": [
      "SeongKu Kang",
      "Jianxun Lian",
      "Dongha Lee",
      "Wonbin Kweon",
      "Sanghwan Jang",
      "Jaehyun Lee",
      "Jindong Wang",
      "Xing Xie",
      "Hwanjo Yu"
    ],
    "abstract": "Recommender systems suffer from biases that cause the collected feedback to incompletely reveal user preference. While debiasing learning has been extensively studied, they mostly focused on the specialized (called counterfactual) test environment simulated by random exposure of items, significantly degrading accuracy in the typical (called factual) test environment based on actual user-item interactions. In fact, each test environment highlights the benefit of a different aspect: the counterfactual test emphasizes user satisfaction in the long-terms, while the factual test focuses on predicting subsequent user behaviors on platforms. Therefore, it is desirable to have a model that performs well on both tests rather than only one. In this work, we introduce a new learning framework, called Bias-adaptive Preference distillation Learning (BPL), to gradually uncover user preferences with dual distillation strategies. These distillation strategies are designed to drive high performance in both factual and counterfactual test environments. Employing a specialized form of teacher-student distillation from a biased model, BPL retains accurate preference knowledge aligned with the collected feedback, leading to high performance in the factual test. Furthermore, through self-distillation with reliability filtering, BPL iteratively refines its knowledge throughout the training process. This enables the model to produce more accurate predictions across a broader range of user-item combinations, thereby improving performance in the counterfactual test. Comprehensive experiments validate the effectiveness of BPL in both factual and counterfactual tests. Our implementation is accessible via: https://github.com/SeongKu-Kang/BPL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
    "pdf_url": "https://arxiv.org/pdf/2510.16076v1",
    "published_date": "2025-10-17 11:09:04 UTC",
    "updated_date": "2025-10-17 11:09:04 UTC"
  },
  {
    "arxiv_id": "2510.15516v1",
    "title": "Revisiting Knowledge Distillation: The Hidden Role of Dataset Size",
    "authors": [
      "Giulia Lanzillotta",
      "Felix Sarnthein",
      "Gil Kur",
      "Thomas Hofmann",
      "Bobby He"
    ],
    "abstract": "The concept of knowledge distillation (KD) describes the training of a student model from a teacher model and is a widely adopted technique in deep learning. However, it is still not clear how and why distillation works. Previous studies focus on two central aspects of distillation: model size, and generalisation. In this work we study distillation in a third dimension: dataset size. We present a suite of experiments across a wide range of datasets, tasks and neural architectures, demonstrating that the effect of distillation is not only preserved but amplified in low-data regimes. We call this newly discovered property the data efficiency of distillation. Equipped with this new perspective, we test the predictive power of existing theories of KD as we vary the dataset size. Our results disprove the hypothesis that distillation can be understood as label smoothing, and provide further evidence in support of the dark knowledge hypothesis. Finally, we analyse the impact of modelling factors such as the objective, scale and relative number of samples on the observed phenomenon. Ultimately, this work reveals that the dataset size may be a fundamental but overlooked variable in the mechanisms underpinning distillation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15516v1",
    "published_date": "2025-10-17 10:40:45 UTC",
    "updated_date": "2025-10-17 10:40:45 UTC"
  },
  {
    "arxiv_id": "2510.15514v2",
    "title": "Taming the Judge: Deconflicting AI Feedback for Stable Reinforcement Learning",
    "authors": [
      "Boyin Liu",
      "Zhuo Zhang",
      "Sen Huang",
      "Lipeng Xie",
      "Qingxu Fu",
      "Haoran Chen",
      "LI YU",
      "Tianyi Hu",
      "Zhaoyang Liu",
      "Bolin Ding",
      "Dongbin Zhao"
    ],
    "abstract": "Aligning language models using LLM judge feedback offers a scalable alternative to human annotation, yet is plagued by judgment inconsistencies that destabilize reinforcement learning. While prior work has focused on judge accuracy, the critical issue of logical coherence particularly preference cycles has been largely unaddressed. To address this gap, this work introduces an end to end framework to systematically detect and resolve these inconsistencies within the reinforcement learning training loop. Our framework features two core contributions: the Conflict Detection Rate (CDR), a novel metric to quantify judgment conflicts, and Deconflicted Graph Rewards (DGR), a signal-purification framework that eliminates cycles before policy optimization. DGR constructs preference graphs from raw judgments, transforms them into conflict-free Directed Acyclic Graphs (DAGs), and generates a logically coherent reward signal compatible with any policy optimizer. Experiments confirm that our framework significantly improves training stability and model performance over strong baselines, establishing logical consistency as a crucial and now-addressable dimension of AI feedback. The code for our method is available at https://github.com/modelscope/RM-Gallery.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15514v2",
    "published_date": "2025-10-17 10:34:59 UTC",
    "updated_date": "2025-10-21 03:35:55 UTC"
  },
  {
    "arxiv_id": "2510.20839v1",
    "title": "Consciousness, natural and artificial: an evolutionary advantage for reasoning on reactive substrates",
    "authors": [
      "Warisa Sritriratanarak",
      "Paulo Garcia"
    ],
    "abstract": "Precisely defining consciousness and identifying the mechanisms that effect it is a long-standing question, particularly relevant with advances in artificial intelligence. The scientific community is divided between physicalism and natural dualism. Physicalism posits consciousness is a physical process that can be modeled computationally; natural dualism rejects this hypothesis. Finding a computational model has proven elusive, particularly because of conflation of consciousness with other cognitive capabilities exhibited by humans, such as intelligence and physiological sensations. Here we show such a computational model that precisely models consciousness, natural or artificial, identifying the structural and functional mechanisms that effect it, confirming the physicalism hypothesis. We found such a model is obtainable when including the underlying (biological or digital) substrate and accounting for reactive behavior in substrate sub-systems (e.g., autonomous physiological responses). Results show that, unlike all other computational processes, consciousness is not independent of its substrate and possessing it is an evolutionary advantage for intelligent entities. Our result shows there is no impediment to the realization of fully artificial consciousness but, surprisingly, that it is also possible to realize artificial intelligence of arbitrary level without consciousness whatsoever, and that there is no advantage in imbuing artificial systems with consciousness.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.20839v1",
    "published_date": "2025-10-17 10:31:25 UTC",
    "updated_date": "2025-10-17 10:31:25 UTC"
  },
  {
    "arxiv_id": "2510.15511v3",
    "title": "Language Models are Injective and Hence Invertible",
    "authors": [
      "Giorgos Nikolaou",
      "Tommaso Mencattini",
      "Donato Crisostomi",
      "Andrea Santilli",
      "Yannis Panagakis",
      "Emanuele Rodol"
    ],
    "abstract": "Transformer components such as non-linear activations and normalization are inherently non-injective, suggesting that different inputs could map to the same output and prevent exact recovery of the input from a model's representations. In this paper, we challenge this view. First, we prove mathematically that transformer language models mapping discrete input sequences to their corresponding sequence of continuous representations are injective and therefore lossless, a property established at initialization and preserved during training. Second, we confirm this result empirically through billions of collision tests on six state-of-the-art language models, and observe no collisions. Third, we operationalize injectivity: we introduce SipIt, the first algorithm that provably and efficiently reconstructs the exact input text from hidden activations, establishing linear-time guarantees and demonstrating exact invertibility in practice. Overall, our work establishes injectivity as a fundamental and exploitable property of language models, with direct implications for transparency, interpretability, and safe deployment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15511v3",
    "published_date": "2025-10-17 10:25:30 UTC",
    "updated_date": "2025-10-21 14:44:49 UTC"
  },
  {
    "arxiv_id": "2510.15509v1",
    "title": "AI Adoption in NGOs: A Systematic Literature Review",
    "authors": [
      "Janne Rotter",
      "William Bailkoski"
    ],
    "abstract": "AI has the potential to significantly improve how NGOs utilize their limited resources for societal benefits, but evidence about how NGOs adopt AI remains scattered. In this study, we systematically investigate the types of AI adoption use cases in NGOs and identify common challenges and solutions, contextualized by organizational size and geographic context. We review the existing primary literature, including studies that investigate AI adoption in NGOs related to social impact between 2020 and 2025 in English. Following the PRISMA protocol, two independent reviewers conduct study selection, with regular cross-checking to ensure methodological rigour, resulting in a final literature body of 65 studies. Leveraging a thematic and narrative approach, we identify six AI use case categories in NGOs - Engagement, Creativity, Decision-Making, Prediction, Management, and Optimization - and extract common challenges and solutions within the Technology-Organization-Environment (TOE) framework. By integrating our findings, this review provides a novel understanding of AI adoption in NGOs, linking specific use cases and challenges to organizational and environmental factors. Our results demonstrate that while AI is promising, adoption among NGOs remains uneven and biased towards larger organizations. Nevertheless, following a roadmap grounded in literature can help NGOs overcome initial barriers to AI adoption, ultimately improving effectiveness, engagement, and social impact.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "econ.GN"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15509v1",
    "published_date": "2025-10-17 10:22:20 UTC",
    "updated_date": "2025-10-17 10:22:20 UTC"
  },
  {
    "arxiv_id": "2510.15502v1",
    "title": "The Road Less Traveled: Enhancing Exploration in LLMs via Sequential Sampling",
    "authors": [
      "Shijia Kang",
      "Muhan Zhang"
    ],
    "abstract": "Reinforcement learning (RL) has been pivotal in enhancing the reasoning capabilities of large language models (LLMs), but it often suffers from limited exploration and entropy collapse, where models exploit a narrow set of solutions, leading to a loss of sampling diversity and subsequently preventing RL from further improving performance. This issue is exacerbated in parallel sampling methods, where multiple outputs are drawn from the same distribution, potentially causing the model to converge to similar solutions. We propose SESA, a novel SEquential SAmpling framework that mitigates this challenge by generating diverse solution sketches sequentially before expanding them into full reasoning paths. This approach ensures broader exploration by conditioning each new output on previous ones, promoting diversity throughout the process and preventing policy collapse. Our experiments on a synthetic task show that sequential sampling consistently outperforms traditional RL methods in terms of path diversity and recovery from collapse. Further evaluations on real-world tasks demonstrate that SESA improves both the exploration of valid strategies and the overall performance of LLMs. On three agent benchmarks, SESA lifts success rates by $+0.25$, $+0.42$, and $+0.07$ absolute over the base model (up to an additional $211\\%$ relative improvement over baseline RL), underscoring its exploration advantage. This work introduces a structured approach to exploration, paving the way for more effective and diverse reasoning in RL-trained LLMs. Our code is released at https://github.com/MuLabPKU/sesa.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15502v1",
    "published_date": "2025-10-17 10:15:11 UTC",
    "updated_date": "2025-10-17 10:15:11 UTC"
  },
  {
    "arxiv_id": "2510.15501v2",
    "title": "DeceptionBench: A Comprehensive Benchmark for AI Deception Behaviors in Real-world Scenarios",
    "authors": [
      "Yao Huang",
      "Yitong Sun",
      "Yichi Zhang",
      "Ruochen Zhang",
      "Yinpeng Dong",
      "Xingxing Wei"
    ],
    "abstract": "Despite the remarkable advances of Large Language Models (LLMs) across diverse cognitive tasks, the rapid enhancement of these capabilities also introduces emergent deceptive behaviors that may induce severe risks in high-stakes deployments. More critically, the characterization of deception across realistic real-world scenarios remains underexplored. To bridge this gap, we establish DeceptionBench, the first benchmark that systematically evaluates how deceptive tendencies manifest across different societal domains, what their intrinsic behavioral patterns are, and how extrinsic factors affect them. Specifically, on the static count, the benchmark encompasses 150 meticulously designed scenarios in five domains, i.e., Economy, Healthcare, Education, Social Interaction, and Entertainment, with over 1,000 samples, providing sufficient empirical foundations for deception analysis. On the intrinsic dimension, we explore whether models exhibit self-interested egoistic tendencies or sycophantic behaviors that prioritize user appeasement. On the extrinsic dimension, we investigate how contextual factors modulate deceptive outputs under neutral conditions, reward-based incentivization, and coercive pressures. Moreover, we incorporate sustained multi-turn interaction loops to construct a more realistic simulation of real-world feedback dynamics. Extensive experiments across LLMs and Large Reasoning Models (LRMs) reveal critical vulnerabilities, particularly amplified deception under reinforcement dynamics, demonstrating that current models lack robust resistance to manipulative contextual cues and the urgent need for advanced safeguards against various deception behaviors. Code and resources are publicly available at https://github.com/Aries-iai/DeceptionBench.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages, 17 figures, accepted by NeruIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.15501v2",
    "published_date": "2025-10-17 10:14:26 UTC",
    "updated_date": "2025-11-16 12:53:22 UTC"
  },
  {
    "arxiv_id": "2510.15495v1",
    "title": "OffSim: Offline Simulator for Model-based Offline Inverse Reinforcement Learning",
    "authors": [
      "Woo-Jin Ahn",
      "Sang-Ryul Baek",
      "Yong-Jun Lee",
      "Hyun-Duck Choi",
      "Myo-Taeg Lim"
    ],
    "abstract": "Reinforcement learning algorithms typically utilize an interactive simulator (i.e., environment) with a predefined reward function for policy training. Developing such simulators and manually defining reward functions, however, is often time-consuming and labor-intensive. To address this, we propose an Offline Simulator (OffSim), a novel model-based offline inverse reinforcement learning (IRL) framework, to emulate environmental dynamics and reward structure directly from expert-generated state-action trajectories. OffSim jointly optimizes a high-entropy transition model and an IRL-based reward function to enhance exploration and improve the generalizability of the learned reward. Leveraging these learned components, OffSim can subsequently train a policy offline without further interaction with the real environment. Additionally, we introduce OffSim$^+$, an extension that incorporates a marginal reward for multi-dataset settings to enhance exploration. Extensive MuJoCo experiments demonstrate that OffSim achieves substantial performance gains over existing offline IRL methods, confirming its efficacy and robustness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15495v1",
    "published_date": "2025-10-17 10:07:55 UTC",
    "updated_date": "2025-10-17 10:07:55 UTC"
  },
  {
    "arxiv_id": "2510.15494v1",
    "title": "An Experimental Study of Real-Life LLM-Proposed Performance Improvements",
    "authors": [
      "Lirong Yi",
      "Gregory Gay",
      "Philipp Leitner"
    ],
    "abstract": "Large Language Models (LLMs) can generate code, but can they generate fast code? In this paper, we study this question using a dataset of 65 real-world tasks mined from open-source Java programs. We specifically select tasks where developers achieved significant speedups, and employ an automated pipeline to generate patches for these issues using two leading LLMs under four prompt variations. By rigorously benchmarking the results against the baseline and human-authored solutions, we demonstrate that LLM-generated code indeed improves performance over the baseline in most cases. However, patches proposed by human developers outperform LLM fixes by a statistically significant margin, indicating that LLMs often fall short of finding truly optimal solutions. We further find that LLM solutions are semantically identical or similar to the developer optimization idea in approximately two-thirds of cases, whereas they propose a more original idea in the remaining one-third. However, these original ideas only occasionally yield substantial performance gains.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15494v1",
    "published_date": "2025-10-17 10:06:52 UTC",
    "updated_date": "2025-10-17 10:06:52 UTC"
  },
  {
    "arxiv_id": "2510.16075v1",
    "title": "Optimization of the quantization of dense neural networks from an exact QUBO formulation",
    "authors": [
      "Sergio Muiz Subias",
      "Manuel L. Gonzlez",
      "Jorge Ruiz Gmez",
      "Alejandro Mata Ali",
      "Jorge Martnez Martn",
      "Miguel Franco Hernando",
      "ngel Miguel Garca-Vico"
    ],
    "abstract": "This work introduces a post-training quantization (PTQ) method for dense neural networks via a novel ADAROUND-based QUBO formulation. Using the Frobenius distance between the theoretical output and the dequantized output (before the activation function) as the objective, an explicit QUBO whose binary variables represent the rounding choice for each weight and bias is obtained. Additionally, by exploiting the structure of the coefficient QUBO matrix, the global problem can be exactly decomposed into $n$ independent subproblems of size $f+1$, which can be efficiently solved using some heuristics such as simulated annealing. The approach is evaluated on MNIST, Fashion-MNIST, EMNIST, and CIFAR-10 across integer precisions from int8 to int1 and compared with a round-to-nearest traditional quantization methodology.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16075v1",
    "published_date": "2025-10-17 09:57:28 UTC",
    "updated_date": "2025-10-17 09:57:28 UTC"
  },
  {
    "arxiv_id": "2510.16074v2",
    "title": "Early-stopping for Transformer model training",
    "authors": [
      "Jing He",
      "Hua Jiang",
      "Cheng Li",
      "Siqian Xin",
      "Shuzhen Yang"
    ],
    "abstract": "This work, based on Random Matrix Theory (RMT), introduces a novel early-stopping strategy for Transformer training dynamics. Utilizing the Power Law (PL) fit to tansformer attention matrices as a probe, we demarcate training into three stages: structural exploration, heavy-tailed structure stabilization, and convergence saturation. Empirically, we observe that the spectral density of the shallow self-attention matrix $V$ consistently evolves into a heavy-tailed distribution. Crucially, we propose two consistent and validation-set-free criteria: a quantitative metric for heavy-tailed dynamics and a novel spectral signature indicative of convergence. The strong alignment between these criteria highlights the utility of RMT for monitoring and diagnosing the progression of Transformer model training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16074v2",
    "published_date": "2025-10-17 09:55:46 UTC",
    "updated_date": "2025-12-28 13:55:00 UTC"
  },
  {
    "arxiv_id": "2510.15480v1",
    "title": "Selecting and Combining Large Language Models for Scalable Code Clone Detection",
    "authors": [
      "Muslim Chochlov",
      "Gul Aftab Ahmed",
      "James Vincent Patten",
      "Yuanhua Han",
      "Guoxian Lu",
      "David Gregg",
      "Jim Buckley"
    ],
    "abstract": "Source code clones pose risks ranging from intellectual property violations to unintended vulnerabilities. Effective and efficient scalable clone detection, especially for diverged clones, remains challenging. Large language models (LLMs) have recently been applied to clone detection tasks. However, the rapid emergence of LLMs raises questions about optimal model selection and potential LLM-ensemble efficacy.\n  This paper addresses the first question by identifying 76 LLMs and filtering them down to suitable candidates for large-scale clone detection. The candidates were evaluated on two public industrial datasets, BigCloneBench, and a commercial large-scale dataset. No uniformly 'best-LLM' emerged, though CodeT5+110M, CuBERT and SPTCode were top-performers. Analysis of LLM-candidates suggested that smaller embedding sizes, smaller tokenizer vocabularies and tailored datasets are advantageous. On commercial large-scale dataset a top-performing CodeT5+110M achieved 39.71\\% precision: twice the precision of previously used CodeBERT.\n  To address the second question, this paper explores ensembling of the selected LLMs: effort-effective approach to improving effectiveness. Results suggest the importance of score normalization and favoring ensembling methods like maximum or sum over averaging. Also, findings indicate that ensembling approach can be statistically significant and effective on larger datasets: the best-performing ensemble achieved even higher precision of 46.91\\% over individual LLM on the commercial large-scale code.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15480v1",
    "published_date": "2025-10-17 09:51:17 UTC",
    "updated_date": "2025-10-17 09:51:17 UTC"
  },
  {
    "arxiv_id": "2510.15476v2",
    "title": "SoK: Taxonomy and Evaluation of Prompt Security in Large Language Models",
    "authors": [
      "Hanbin Hong",
      "Shuya Feng",
      "Nima Naderloui",
      "Shenao Yan",
      "Jingyu Zhang",
      "Biying Liu",
      "Ali Arastehfard",
      "Heqing Huang",
      "Yuan Hong"
    ],
    "abstract": "Large Language Models (LLMs) have rapidly become integral to real-world applications, powering services across diverse sectors. However, their widespread deployment has exposed critical security risks, particularly through jailbreak prompts that can bypass model alignment and induce harmful outputs. Despite intense research into both attack and defense techniques, the field remains fragmented: definitions, threat models, and evaluation criteria vary widely, impeding systematic progress and fair comparison. In this Systematization of Knowledge (SoK), we address these challenges by (1) proposing a holistic, multi-level taxonomy that organizes attacks, defenses, and vulnerabilities in LLM prompt security; (2) formalizing threat models and cost assumptions into machine-readable profiles for reproducible evaluation; (3) introducing an open-source evaluation toolkit for standardized, auditable comparison of attacks and defenses; (4) releasing JAILBREAKDB, the largest annotated dataset of jailbreak and benign prompts to date;\\footnote{The dataset is released at \\href{https://huggingface.co/datasets/youbin2014/JailbreakDB}{\\textcolor{purple}{https://huggingface.co/datasets/youbin2014/JailbreakDB}}.} and (5) presenting a comprehensive evaluation platform and leaderboard of state-of-the-art methods \\footnote{will be released soon.}. Our work unifies fragmented research, provides rigorous foundations for future studies, and supports the development of robust, trustworthy LLMs suitable for high-stakes deployment.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15476v2",
    "published_date": "2025-10-17 09:38:54 UTC",
    "updated_date": "2025-10-21 07:40:00 UTC"
  },
  {
    "arxiv_id": "2510.21775v2",
    "title": "Face-MakeUpV2: Facial Consistency Learning for Controllable Text-to-Image Generation",
    "authors": [
      "Dawei Dai",
      "Yinxiu Zhou",
      "Chenghang Li",
      "Guolai Jiang",
      "Chengfang Zhang"
    ],
    "abstract": "In facial image generation, current text-to-image models often suffer from facial attribute leakage and insufficient physical consistency when responding to local semantic instructions. In this study, we propose Face-MakeUpV2, a facial image generation model that aims to maintain the consistency of face ID and physical characteristics with the reference image. First, we constructed a large-scale dataset FaceCaptionMask-1M comprising approximately one million image-text-masks pairs that provide precise spatial supervision for the local semantic instructions. Second, we employed a general text-to-image pretrained model as the backbone and introduced two complementary facial information injection channels: a 3D facial rendering channel to incorporate the physical characteristics of the image and a global facial feature channel. Third, we formulated two optimization objectives for the supervised learning of our model: semantic alignment in the model's embedding space to mitigate the attribute leakage problem and perceptual loss on facial images to preserve ID consistency. Extensive experiments demonstrated that our Face-MakeUpV2 achieves best overall performance in terms of preserving face ID and maintaining physical consistency of the reference images. These results highlight the practical potential of Face-MakeUpV2 for reliable and controllable facial editing in diverse applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Some errors in the critical data presented in Table 1 and Table 2",
    "pdf_url": "https://arxiv.org/pdf/2510.21775v2",
    "published_date": "2025-10-17 09:31:08 UTC",
    "updated_date": "2025-12-01 01:41:38 UTC"
  },
  {
    "arxiv_id": "2510.15464v1",
    "title": "Learning to Answer from Correct Demonstrations",
    "authors": [
      "Nirmit Joshi",
      "Gene Li",
      "Siddharth Bhandari",
      "Shiva Prasad Kasiviswanathan",
      "Cong Ma",
      "Nathan Srebro"
    ],
    "abstract": "We study the problem of learning to generate an answer (or completion) to a question (or prompt), where there could be multiple correct answers, any one of which is acceptable at test time. Learning is based on demonstrations of some correct answer to each training question, as in Supervised Fine Tuning (SFT). We formalize the problem as offline imitation learning in contextual bandits, with demonstrations from some optimal policy, without explicitly observed rewards. Prior work assumes that the demonstrator belongs to a low-complexity policy class, which motivates maximum likelihood estimation (i.e., log-loss minimization). In contrast, we propose relying only on the reward model (specifying which answers are correct) being in a low-cardinality class, which we argue is a weaker assumption. We show that likelihood maximization methods can fail in this case, and instead devise an alternative novel approach that learns with sample complexity logarithmic in the cardinality of the reward class. Our work motivates looking beyond likelihood maximization when learning from correct demonstrations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Comments are welcome",
    "pdf_url": "https://arxiv.org/pdf/2510.15464v1",
    "published_date": "2025-10-17 09:20:17 UTC",
    "updated_date": "2025-10-17 09:20:17 UTC"
  },
  {
    "arxiv_id": "2510.15458v1",
    "title": "Robust Optimization in Causal Models and G-Causal Normalizing Flows",
    "authors": [
      "Gabriele Visentin",
      "Patrick Cheridito"
    ],
    "abstract": "In this paper, we show that interventionally robust optimization problems in causal models are continuous under the $G$-causal Wasserstein distance, but may be discontinuous under the standard Wasserstein distance. This highlights the importance of using generative models that respect the causal structure when augmenting data for such tasks. To this end, we propose a new normalizing flow architecture that satisfies a universal approximation property for causal structural models and can be efficiently trained to minimize the $G$-causal Wasserstein distance. Empirically, we demonstrate that our model outperforms standard (non-causal) generative models in data augmentation for causal regression and mean-variance portfolio optimization in causal factor models.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "q-fin.PM"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15458v1",
    "published_date": "2025-10-17 09:12:01 UTC",
    "updated_date": "2025-10-17 09:12:01 UTC"
  },
  {
    "arxiv_id": "2510.15456v1",
    "title": "Expediting Reinforcement Learning by Incorporating Knowledge About Temporal Causality in the Environment",
    "authors": [
      "Jan Corazza",
      "Hadi Partovi Aria",
      "Daniel Neider",
      "Zhe Xu"
    ],
    "abstract": "Reinforcement learning (RL) algorithms struggle with learning optimal policies for tasks where reward feedback is sparse and depends on a complex sequence of events in the environment. Probabilistic reward machines (PRMs) are finite-state formalisms that can capture temporal dependencies in the reward signal, along with nondeterministic task outcomes. While special RL algorithms can exploit this finite-state structure to expedite learning, PRMs remain difficult to modify and design by hand. This hinders the already difficult tasks of utilizing high-level causal knowledge about the environment, and transferring the reward formalism into a new domain with a different causal structure. This paper proposes a novel method to incorporate causal information in the form of Temporal Logic-based Causal Diagrams into the reward formalism, thereby expediting policy learning and aiding the transfer of task specifications to new environments. Furthermore, we provide a theoretical result about convergence to optimal policy for our method, and demonstrate its strengths empirically.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Please cite the proceedings version. Source code: https://github.com/corazza/tcrl",
    "pdf_url": "https://arxiv.org/pdf/2510.15456v1",
    "published_date": "2025-10-17 09:11:26 UTC",
    "updated_date": "2025-10-17 09:11:26 UTC"
  },
  {
    "arxiv_id": "2510.16072v1",
    "title": "Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation",
    "authors": [
      "Farjana Yesmin"
    ],
    "abstract": "Machine learning models trained on imbalanced datasets often exhibit intersectional biases-systematic errors arising from the interaction of multiple attributes such as object class and environmental conditions. This paper presents a data-driven framework for analyzing and mitigating such biases in image classification. We introduce the Intersectional Fairness Evaluation Framework (IFEF), which combines quantitative fairness metrics with interpretability tools to systematically identify bias patterns in model predictions. Building on this analysis, we propose Bias-Weighted Augmentation (BWA), a novel data augmentation strategy that adapts transformation intensities based on subgroup distribution statistics. Experiments on the Open Images V7 dataset with five object classes demonstrate that BWA improves accuracy for underrepresented class-environment intersections by up to 24 percentage points while reducing fairness metric disparities by 35%. Statistical analysis across multiple independent runs confirms the significance of improvements (p < 0.05). Our methodology provides a replicable approach for analyzing and addressing intersectional biases in image classification systems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "18 pages",
    "pdf_url": "https://arxiv.org/pdf/2510.16072v1",
    "published_date": "2025-10-17 09:10:57 UTC",
    "updated_date": "2025-10-17 09:10:57 UTC"
  },
  {
    "arxiv_id": "2510.16071v1",
    "title": "MNO: Multiscale Neural Operator for Computational Fluid Dynamics with 3D Point Cloud Data",
    "authors": [
      "Qinxuan Wang",
      "Chuang Wang",
      "Mingyu Zhang",
      "Jingwei Sun",
      "Peipei Yang",
      "Shuo Tang",
      "Shiming Xiang"
    ],
    "abstract": "Neural operators have emerged as a powerful data-driven paradigm for solving Partial Differential Equations (PDEs), offering orders-of-magnitude acceleration over traditional solvers. However, existing approaches still suffer from limited accuracy and scalability, particularly on irregular domains where fluid flows exhibit rich multiscale structures. In this work, we introduce the Multiscale Neural Operator (MNO), a new architecture for Computational Fluid Dynamics (CFD) on three-dimensional (3D) unstructured point clouds. MNO explicitly decomposes information across three scales: a global dimension-shrinkage attention module for long-range dependencies, a local graph attention module for neighborhood-level interactions, and a micro point-wise attention module for fine-grained details. This design preserves multiscale inductive biases while remaining computationally efficient. We evaluate MNO on four diverse benchmarks, covering both steady-state and unsteady flow scenarios with up to 300K points. Across all tasks, MNO consistently outperforms state-of-the-art baselines, reducing prediction errors by 5% to 40% and demonstrating improved robustness in challenging 3D CFD problems. Our results highlight the importance of explicit multiscale design for neural operators and establish MNO as a scalable framework for learning complex fluid dynamics on irregular domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16071v1",
    "published_date": "2025-10-17 09:01:59 UTC",
    "updated_date": "2025-10-17 09:01:59 UTC"
  },
  {
    "arxiv_id": "2510.15444v1",
    "title": "A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning",
    "authors": [
      "Zhi Zhou",
      "Yuhao Tan",
      "Zenan Li",
      "Yuan Yao",
      "Lan-Zhe Guo",
      "Yu-Feng Li",
      "Xiaoxing Ma"
    ],
    "abstract": "Test-time scaling seeks to improve the reasoning performance of large language models (LLMs) by adding computational resources. A prevalent approach within the field is sampling-based test-time scaling methods, which enhance reasoning by generating multiple reasoning paths for a given input during inference. However, despite its practical success, the theoretical foundations remain underexplored. In this paper, we provide the first theoretical framework for analyzing sampling-based test-time scaling methods, grounded in the perspective of confidence estimation. Based on the framework, we analyze two dominant paradigms: self-consistency and perplexity, and reveal key limitations: self-consistency suffers from high estimation error while perplexity exhibits substantial modeling error and possible degradation of the estimation error convergence. To address these limitations, we introduce RPC, a hybrid method that leverages our theoretical insights through two key components: Perplexity Consistency and Reasoning Pruning. Perplexity Consistency combines the strengths of self-consistency and perplexity, boosting the convergence rate of estimation error from linear to exponential while preserving model error. Reasoning Pruning prevents degradation by eliminating low-probability reasoning paths. Both theoretical analysis and empirical results across seven benchmark datasets demonstrate that RPC has a strong potential for reducing reasoning error. Notably, RPC achieves reasoning performance comparable to self-consistency while not only enhancing confidence reliability but also reducing sampling costs by 50%. The code and resources are available at https://wnjxyk.github.io/RPC.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.15444v1",
    "published_date": "2025-10-17 08:59:30 UTC",
    "updated_date": "2025-10-17 08:59:30 UTC"
  },
  {
    "arxiv_id": "2510.15440v1",
    "title": "Select Less, Reason More: Prioritizing Evidence Purity for Video Reasoning",
    "authors": [
      "Xuchen Li",
      "Xuzhao Li",
      "Shiyu Hu",
      "Kaiqi Huang"
    ],
    "abstract": "Long-form video reasoning remains a major challenge for Video Large Language Models (Video LLMs), as static uniform frame sampling leads to information dilution and obscures critical evidence. Furthermore, existing pixel-space video reasoning agents, which are designed to actively interact with the video to acquire new visual information, remain suboptimal due to their lack of rigorous reward mechanisms to enforce evidence purity and their inability to perform temporal information supplementation beyond pre-sampled frames. To address this critical gap, we propose a novel evidence-prioritized adaptive framework built upon our core philosophy: \"Select Less, Reason More.\" Our core contribution is the evidence-aware reinforcement learning (EARL) framework, which transforms the model into an active interrogator of evidence. EARL is precisely engineered to dynamically select the most relevant frames and, crucially, to perform localized re-sampling around the selected key frames to access fine-grained temporal detail. Extensive experiments on five demanding video reasoning benchmarks demonstrate that our EARL-trained model achieves new state-of-the-art among open-source Video LLMs, simultaneously learning an effective and high-purity visual evidence selection policy. Impressively, our 7B model achieves 59.8% on LongVideoBench, 69.0% on MVBench and 64.9% on VideoMME. These results highlight the importance of prioritizing evidence purity and the effectiveness of our framework.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint, Under review",
    "pdf_url": "https://arxiv.org/pdf/2510.15440v1",
    "published_date": "2025-10-17 08:52:40 UTC",
    "updated_date": "2025-10-17 08:52:40 UTC"
  },
  {
    "arxiv_id": "2510.17880v1",
    "title": "Outraged AI: Large language models prioritise emotion over cost in fairness enforcement",
    "authors": [
      "Hao Liu",
      "Yiqing Dai",
      "Haotian Tan",
      "Yu Lei",
      "Yujia Zhou",
      "Zhen Wu"
    ],
    "abstract": "Emotions guide human decisions, but whether large language models (LLMs) use emotion similarly remains unknown. We tested this using altruistic third-party punishment, where an observer incurs a personal cost to enforce fairness, a hallmark of human morality and often driven by negative emotion. In a large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100 decisions, LLMs used emotion to guide punishment, sometimes even more strongly than humans did: Unfairness elicited stronger negative emotion that led to more punishment; punishing unfairness produced more positive emotion than accepting; and critically, prompting self-reports of emotion causally increased punishment. However, mechanisms diverged: LLMs prioritized emotion over cost, enforcing norms in an almost all-or-none manner with reduced cost sensitivity, whereas humans balanced fairness and cost. Notably, reasoning models (o3-mini, DeepSeek-R1) were more cost-sensitive and closer to human behavior than foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven. These findings provide the first causal evidence of emotion-guided moral decisions in LLMs and reveal deficits in cost calibration and nuanced fairness judgements, reminiscent of early-stage human responses. We propose that LLMs progress along a trajectory paralleling human development; future models should integrate emotion with context-sensitive reasoning to achieve human-like emotional intelligence.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.17880v1",
    "published_date": "2025-10-17 08:41:36 UTC",
    "updated_date": "2025-10-17 08:41:36 UTC"
  },
  {
    "arxiv_id": "2510.15430v2",
    "title": "Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models",
    "authors": [
      "Shuang Liang",
      "Zhihao Xu",
      "Jialing Tao",
      "Hui Xue",
      "Xiting Wang"
    ],
    "abstract": "Despite extensive alignment efforts, Large Vision-Language Models (LVLMs) remain vulnerable to jailbreak attacks, posing serious safety risks. To address this, existing detection methods either learn attack-specific parameters, which hinders generalization to unseen attacks, or rely on heuristically sound principles, which limit accuracy and efficiency. To overcome these limitations, we propose Learning to Detect (LoD), a general framework that accurately detects unknown jailbreak attacks by shifting the focus from attack-specific learning to task-specific learning. This framework includes a Multi-modal Safety Concept Activation Vector module for safety-oriented representation learning and a Safety Pattern Auto-Encoder module for unsupervised attack classification. Extensive experiments show that our method achieves consistently higher detection AUROC on diverse unknown attacks while improving efficiency. The code is available at https://anonymous.4open.science/r/Learning-to-Detect-51CB.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Withdrawn due to an accidental duplicate submission. This paper (arXiv:2510.15430) was unintentionally submitted as a new entry instead of a new version of our previous work (arXiv:2508.09201)",
    "pdf_url": "https://arxiv.org/pdf/2510.15430v2",
    "published_date": "2025-10-17 08:37:45 UTC",
    "updated_date": "2025-10-20 11:50:13 UTC"
  },
  {
    "arxiv_id": "2510.16070v1",
    "title": "Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography",
    "authors": [
      "Mahta Khoobi",
      "Marc Sebastian von der Stueck",
      "Felix Barajas Ordonez",
      "Anca-Maria Iancu",
      "Eric Corban",
      "Julia Nowak",
      "Aleksandar Kargaliev",
      "Valeria Perelygina",
      "Anna-Sophie Schott",
      "Daniel Pinto dos Santos",
      "Christiane Kuhl",
      "Daniel Truhn",
      "Sven Nebelung",
      "Robert Siepmann"
    ],
    "abstract": "Structured reporting (SR) and artificial intelligence (AI) may transform how radiologists interact with imaging studies. This prospective study (July to December 2024) evaluated the impact of three reporting modes: free-text (FT), structured reporting (SR), and AI-assisted structured reporting (AI-SR), on image analysis behavior, diagnostic accuracy, efficiency, and user experience. Four novice and four non-novice readers (radiologists and medical students) each analyzed 35 bedside chest radiographs per session using a customized viewer and an eye-tracking system. Outcomes included diagnostic accuracy (compared with expert consensus using Cohen's $$), reporting time per radiograph, eye-tracking metrics, and questionnaire-based user experience. Statistical analysis used generalized linear mixed models with Bonferroni post-hoc tests with a significance level of ($P \\le .01$). Diagnostic accuracy was similar in FT ($= 0.58$) and SR ($= 0.60$) but higher in AI-SR ($= 0.71$, $P < .001$). Reporting times decreased from $88 \\pm 38$ s (FT) to $37 \\pm 18$ s (SR) and $25 \\pm 9$ s (AI-SR) ($P < .001$). Saccade counts for the radiograph field ($205 \\pm 135$ (FT), $123 \\pm 88$ (SR), $97 \\pm 58$ (AI-SR)) and total fixation duration for the report field ($11 \\pm 5$ s (FT), $5 \\pm 3$ s (SR), $4 \\pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P < .001$ each). Novice readers shifted gaze towards the radiograph in SR, while non-novice readers maintained their focus on the radiograph. AI-SR was the preferred mode. In conclusion, SR improves efficiency by guiding visual attention toward the image, and AI-prefilled SR further enhances diagnostic accuracy and user satisfaction.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "Preprint version - Under second revision at Radiology (manuscript RAD-25-1348)",
    "pdf_url": "https://arxiv.org/pdf/2510.16070v1",
    "published_date": "2025-10-17 08:33:07 UTC",
    "updated_date": "2025-10-17 08:33:07 UTC"
  },
  {
    "arxiv_id": "2510.21774v1",
    "title": "OCR-Quality: A Human-Annotated Dataset for OCR Quality Assessment",
    "authors": [
      "Yulong Zhang"
    ],
    "abstract": "We present OCR-Quality, a comprehensive human-annotated dataset designed for evaluating and developing OCR quality assessment methods. The dataset consists of 1,000 PDF pages converted to PNG images at 300 DPI, sampled from diverse real-world scenarios, including academic papers, textbooks, e-books, and multilingual documents. Each document has been processed using state-of-the-art Vision-Language Models (VLMs) and manually annotated with quality scores using a 4-level scoring system (1: Excellent, 2: Good, 3: Fair, 4: Poor). The dataset includes detailed source information, annotation guidelines, and representative cases across various difficulty levels. OCR-Quality addresses the critical need for reliable OCR quality assessment in real-world applications and provides a valuable benchmark for training and evaluating OCR verification systems. The dataset is publicly available at https://huggingface.co/datasets/Aslan-mingye/OCR-Quality .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.21774v1",
    "published_date": "2025-10-17 08:30:13 UTC",
    "updated_date": "2025-10-17 08:30:13 UTC"
  },
  {
    "arxiv_id": "2510.17879v1",
    "title": "Decoding Listeners Identity: Person Identification from EEG Signals Using a Lightweight Spiking Transformer",
    "authors": [
      "Zheyuan Lin",
      "Siqi Cai",
      "Haizhou Li"
    ],
    "abstract": "EEG-based person identification enables applications in security, personalized brain-computer interfaces (BCIs), and cognitive monitoring. However, existing techniques often rely on deep learning architectures at high computational cost, limiting their scope of applications. In this study, we propose a novel EEG person identification approach using spiking neural networks (SNNs) with a lightweight spiking transformer for efficiency and effectiveness. The proposed SNN model is capable of handling the temporal complexities inherent in EEG signals. On the EEG-Music Emotion Recognition Challenge dataset, the proposed model achieves 100% classification accuracy with less than 10% energy consumption of traditional deep neural networks. This study offers a promising direction for energy-efficient and high-performance BCIs. The source code is available at https://github.com/PatrickZLin/Decode-ListenerIdentity.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.17879v1",
    "published_date": "2025-10-17 08:20:01 UTC",
    "updated_date": "2025-10-17 08:20:01 UTC"
  },
  {
    "arxiv_id": "2510.15418v2",
    "title": "Fine-Tuning MedGemma for Clinical Captioning to Enhance Multimodal RAG over Malaysia CPGs",
    "authors": [
      "Lee Qi Zun",
      "Mohamad Zulhilmi Bin Abdul Halim",
      "Goh Man Fye"
    ],
    "abstract": "Retrieval-Augmented Generation systems are essential for providing fact-based guidance from Malaysian Clinical Practice Guidelines. However, their effectiveness with image-based queries is limited, as general Vision-Language Model captions often lack clinical specificity and factual grounding. This study proposes and validates a framework to specialize the MedGemma model for generating high-fidelity captions that serve as superior queries. To overcome data scarcity, we employ a knowledge distillation pipeline to create a synthetic dataset across dermatology, fundus, and chest radiography domains, and fine-tune MedGemma using the parameter-efficient QLoRA method. Performance was rigorously assessed through a dual framework measuring both classification accuracy and, via a novel application of the RAGAS framework, caption faithfulness, relevancy, and correctness. The fine-tuned model demonstrated substantial improvements in classification performance, while RAGAS evaluation confirmed significant gains in caption faithfulness and correctness, validating the models ability to produce reliable, factually grounded descriptions. This work establishes a robust pipeline for specializing medical VLMs and validates the resulting model as a high-quality query generator, laying the groundwork for enhancing multimodal RAG systems in evidence-based clinical decision support.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15418v2",
    "published_date": "2025-10-17 08:11:54 UTC",
    "updated_date": "2025-11-07 05:24:14 UTC"
  },
  {
    "arxiv_id": "2510.15416v1",
    "title": "Adaptive Minds: Empowering Agents with LoRA-as-Tools",
    "authors": [
      "Pavan C Shekar",
      "Ashwanth Krishnan"
    ],
    "abstract": "We present Adaptive Minds, an agentic system that treats LoRA adapters as domain-specific tools. Instead of relying on a single fine-tuned model or rigid rule-based routing, our approach empowers the base LLM itself to act as a semantic router analyzing each query and dynamically selecting the most relevant LoRA tool. This enables the agent to seamlessly switch between different domain experts on demand. By combining the flexibility of multi-agent orchestration with the efficiency of parameter-efficient fine-tuning, Adaptive Minds delivers accurate, specialized responses while preserving conversational ability. The system is built with LangGraph for workflow management, supports both API and web interfaces, and is fully open source, providing a scalable and extensible foundation for domain-adaptive AI assistance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages, 1 figure, 7 tables . Code available at: https://github.com/qpiai/adaptive-minds",
    "pdf_url": "https://arxiv.org/pdf/2510.15416v1",
    "published_date": "2025-10-17 08:10:06 UTC",
    "updated_date": "2025-10-17 08:10:06 UTC"
  },
  {
    "arxiv_id": "2510.15414v2",
    "title": "MARSHAL: Incentivizing Multi-Agent Reasoning via Self-Play with Strategic LLMs",
    "authors": [
      "Huining Yuan",
      "Zelai Xu",
      "Zheyue Tan",
      "Xiangmin Yi",
      "Mo Guang",
      "Kaiwen Long",
      "Haojia Hui",
      "Boxun Li",
      "Xinlei Chen",
      "Bo Zhao",
      "Xiao-Ping Zhang",
      "Chao Yu",
      "Yu Wang"
    ],
    "abstract": "Developing large language models (LLMs) to cooperate and compete effectively within multi-agent systems (MASs) is a critical step towards more advanced intelligence. While reinforcement learning (RL) has proven effective for enhancing reasoning in single-agent tasks, its extension to multi-turn, multi-agent scenarios remains underexplored due to the challenges of long-horizon credit assignment and agent-specific advantage estimation. To address these challenges, we introduce MARSHAL, an end-to-end RL framework that incentivizes Multi-Agent Reasoning through Self-play witH strAtegic LLMs in both cooperative and competitive games. MARSHAL features a turn-level advantage estimator that aligns learning signals with each interaction for credit assignment, and an agent-specific advantage normalization to stabilize multi-agent training. By learning with self-play across cooperative and competitive games, MARSHAL agent trained from Qwen3-4B develops strong strategic abilities that generalize to held-out games with up to 28.7% performance improvements. More importantly, the capability acquired through self-play generalizes beyond games, yielding consistent performance gains of MASs in reasoning benchmarks. When integrated into leading MASs, our MARSHAL agent achieves significant performance gains of up to 10.0% on AIME, 6.6% on GPQA-Diamond, and 3.5% on average across all benchmarks. These results establish end-to-end RL training with self-play in strategic games as a powerful approach for developing generalizable multi-agent reasoning capabilities in LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15414v2",
    "published_date": "2025-10-17 08:08:06 UTC",
    "updated_date": "2025-12-02 17:31:27 UTC"
  },
  {
    "arxiv_id": "2510.15400v1",
    "title": "Robust High-Resolution Multi-Organ Diffusion MRI Using Synthetic-Data-Tuned Prompt Learning",
    "authors": [
      "Chen Qian",
      "Haoyu Zhang",
      "Junnan Ma",
      "Liuhong Zhu",
      "Qingrui Cai",
      "Yu Wang",
      "Ruibo Song",
      "Lv Li",
      "Lin Mei",
      "Xianwang Jiang",
      "Qin Xu",
      "Boyu Jiang",
      "Ran Tao",
      "Chunmiao Chen",
      "Shufang Chen",
      "Dongyun Liang",
      "Qiu Guo",
      "Jianzhong Lin",
      "Taishan Kang",
      "Mengtian Lu",
      "Liyuan Fu",
      "Ruibin Huang",
      "Huijuan Wan",
      "Xu Huang",
      "Jianhua Wang",
      "Di Guo",
      "Hai Zhong",
      "Jianjun Zhou",
      "Xiaobo Qu"
    ],
    "abstract": "Clinical adoption of multi-shot diffusion-weighted magnetic resonance imaging (multi-shot DWI) for body-wide tumor diagnostics is limited by severe motion-induced phase artifacts from respiration, peristalsis, and so on, compounded by multi-organ, multi-slice, multi-direction and multi-b-value complexities. Here, we introduce a reconstruction framework, LoSP-Prompt, that overcomes these challenges through physics-informed modeling and synthetic-data-driven prompt learning. We model inter-shot phase variations as a high-order Locally Smooth Phase (LoSP), integrated into a low-rank Hankel matrix reconstruction. Crucially, the algorithm's rank parameter is automatically set via prompt learning trained exclusively on synthetic abdominal DWI data emulating physiological motion. Validated across 10,000+ clinical images (43 subjects, 4 scanner models, 5 centers), LoSP-Prompt: (1) Achieved twice the spatial resolution of clinical single-shot DWI, enhancing liver lesion conspicuity; (2) Generalized to seven diverse anatomical regions (liver, kidney, sacroiliac, pelvis, knee, spinal cord, brain) with a single model; (3) Outperformed state-of-the-art methods in image quality, artifact suppression, and noise reduction (11 radiologists' evaluations on a 5-point scale, $p<0.05$), achieving 4-5 points (excellent) on kidney DWI, 4 points (good to excellent) on liver, sacroiliac and spinal cord DWI, and 3-4 points (good) on knee and tumor brain. The approach eliminates navigator signals and realistic data supervision, providing an interpretable, robust solution for high-resolution multi-organ multi-shot DWI. Its scanner-agnostic performance signifies transformative potential for precision oncology.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "physics.med-ph"
    ],
    "primary_category": "cs.CV",
    "comment": "43 pages, 27 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.15400v1",
    "published_date": "2025-10-17 07:51:35 UTC",
    "updated_date": "2025-10-17 07:51:35 UTC"
  },
  {
    "arxiv_id": "2510.15398v2",
    "title": "MARIS: Marine Open-Vocabulary Instance Segmentation with Geometric Enhancement and Semantic Alignment",
    "authors": [
      "Bingyu Li",
      "Feiyu Wang",
      "Da Zhang",
      "Zhiyuan Zhao",
      "Junyu Gao",
      "Xuelong Li"
    ],
    "abstract": "Most existing underwater instance segmentation approaches are constrained by close-vocabulary prediction, limiting their ability to recognize novel marine categories. To support evaluation, we introduce \\textbf{MARIS} (\\underline{Mar}ine Open-Vocabulary \\underline{I}nstance \\underline{S}egmentation), the first large-scale fine-grained benchmark for underwater Open-Vocabulary (OV) segmentation, featuring a limited set of seen categories and diverse unseen categories. Although OV segmentation has shown promise on natural images, our analysis reveals that transfer to underwater scenes suffers from severe visual degradation (e.g., color attenuation) and semantic misalignment caused by lack underwater class definitions. To address these issues, we propose a unified framework with two complementary components. The Geometric Prior Enhancement Module (\\textbf{GPEM}) leverages stable part-level and structural cues to maintain object consistency under degraded visual conditions. The Semantic Alignment Injection Mechanism (\\textbf{SAIM}) enriches language embeddings with domain-specific priors, mitigating semantic ambiguity and improving recognition of unseen categories. Experiments show that our framework consistently outperforms existing OV baselines both In-Domain and Cross-Domain setting on MARIS, establishing a strong foundation for future underwater perception research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15398v2",
    "published_date": "2025-10-17 07:50:58 UTC",
    "updated_date": "2025-10-23 07:18:58 UTC"
  },
  {
    "arxiv_id": "2510.15395v1",
    "title": "Corrigibility Transformation: Constructing Goals That Accept Updates",
    "authors": [
      "Rubi Hudson"
    ],
    "abstract": "For an AI's training process to successfully impart a desired goal, it is important that the AI does not attempt to resist the training. However, partially learned goals will often incentivize an AI to avoid further goal updates, as most goals are better achieved by an AI continuing to pursue them. We say that a goal is corrigible if it does not incentivize taking actions that avoid proper goal updates or shutdown. In addition to convergence in training, corrigibility also allows for correcting mistakes and changes in human preferences, which makes it a crucial safety property. Despite this, the existing literature does not include specifications for goals that are both corrigible and competitive with non-corrigible alternatives. We provide a formal definition for corrigibility, then introduce a transformation that constructs a corrigible version of any goal that can be made corrigible, without sacrificing performance. This is done by myopically eliciting predictions of reward conditional on costlessly preventing updates, which then also determine the reward when updates are accepted. The transformation can be modified to recursively extend corrigibility to any new agents created by corrigible agents, and to prevent agents from deliberately modifying their goals. Two gridworld experiments demonstrate that these corrigible goals can be learned effectively, and that they lead to the desired behavior.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15395v1",
    "published_date": "2025-10-17 07:47:27 UTC",
    "updated_date": "2025-10-17 07:47:27 UTC"
  },
  {
    "arxiv_id": "2510.15387v1",
    "title": "Advancing Routing-Awareness in Analog ICs Floorplanning",
    "authors": [
      "Davide Basso",
      "Luca Bortolussi",
      "Mirjana Videnovic-Misic",
      "Husni Habal"
    ],
    "abstract": "The adoption of machine learning-based techniques for analog integrated circuit layout, unlike its digital counterpart, has been limited by the stringent requirements imposed by electric and problem-specific constraints, along with the interdependence of floorplanning and routing steps. In this work, we address a prevalent concern among layout engineers regarding the need for readily available routing-aware floorplanning solutions. To this extent, we develop an automatic floorplanning engine based on reinforcement learning and relational graph convolutional neural network specifically tailored to condition the floorplan generation towards more routable outcomes. A combination of increased grid resolution and precise pin information integration, along with a dynamic routing resource estimation technique, allows balancing routing and area efficiency, eventually meeting industrial standards. When analyzing the place and route effectiveness in a simulated environment, the proposed approach achieves a 13.8% reduction in dead space, a 40.6% reduction in wirelength and a 73.4% increase in routing success when compared to past learning-based state-of-the-art techniques.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15387v1",
    "published_date": "2025-10-17 07:39:24 UTC",
    "updated_date": "2025-10-17 07:39:24 UTC"
  },
  {
    "arxiv_id": "2510.15383v1",
    "title": "DroneAudioset: An Audio Dataset for Drone-based Search and Rescue",
    "authors": [
      "Chitralekha Gupta",
      "Soundarya Ramesh",
      "Praveen Sasikumar",
      "Kian Peen Yeo",
      "Suranga Nanayakkara"
    ],
    "abstract": "Unmanned Aerial Vehicles (UAVs) or drones, are increasingly used in search and rescue missions to detect human presence. Existing systems primarily leverage vision-based methods which are prone to fail under low-visibility or occlusion. Drone-based audio perception offers promise but suffers from extreme ego-noise that masks sounds indicating human presence. Existing datasets are either limited in diversity or synthetic, lacking real acoustic interactions, and there are no standardized setups for drone audition. To this end, we present DroneAudioset (The dataset is publicly available at https://huggingface.co/datasets/ahlab-drone-project/DroneAudioSet/ under the MIT license), a comprehensive drone audition dataset featuring 23.5 hours of annotated recordings, covering a wide range of signal-to-noise ratios (SNRs) from -57.2 dB to -2.5 dB, across various drone types, throttles, microphone configurations as well as environments. The dataset enables development and systematic evaluation of noise suppression and classification methods for human-presence detection under challenging conditions, while also informing practical design considerations for drone audition systems, such as microphone placement trade-offs, and development of drone noise-aware audio processing. This dataset is an important step towards enabling design and deployment of drone-audition systems.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted in Neurips (Datasets and Benchmarks Track) 2025. The first two authors are equal contributors",
    "pdf_url": "https://arxiv.org/pdf/2510.15383v1",
    "published_date": "2025-10-17 07:33:48 UTC",
    "updated_date": "2025-10-17 07:33:48 UTC"
  },
  {
    "arxiv_id": "2510.15382v2",
    "title": "Towards Robust Zero-Shot Reinforcement Learning",
    "authors": [
      "Kexin Zheng",
      "Lauriane Teyssier",
      "Yinan Zheng",
      "Yu Luo",
      "Xianyuan Zhan"
    ],
    "abstract": "The recent development of zero-shot reinforcement learning (RL) has opened a new avenue for learning pre-trained generalist policies that can adapt to arbitrary new tasks in a zero-shot manner. While the popular Forward-Backward representations (FB) and related methods have shown promise in zero-shot RL, we empirically found that their modeling lacks expressivity and that extrapolation errors caused by out-of-distribution (OOD) actions during offline learning sometimes lead to biased representations, ultimately resulting in suboptimal performance. To address these issues, we propose Behavior-REgularizEd Zero-shot RL with Expressivity enhancement (BREEZE), an upgraded FB-based framework that simultaneously enhances learning stability, policy extraction capability, and representation learning quality. BREEZE introduces behavioral regularization in zero-shot RL policy learning, transforming policy optimization into a stable in-sample learning paradigm. Additionally, BREEZE extracts the policy using a task-conditioned diffusion model, enabling the generation of high-quality and multimodal action distributions in zero-shot RL settings. Moreover, BREEZE employs expressive attention-based architectures for representation modeling to capture the complex relationships between environmental dynamics. Extensive experiments on ExORL and D4RL Kitchen demonstrate that BREEZE achieves the best or near-the-best performance while exhibiting superior robustness compared to prior offline zero-shot RL methods. The official implementation is available at: https://github.com/Whiterrrrr/BREEZE.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "comment": "Neurips 2025, 29 pages, 19 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.15382v2",
    "published_date": "2025-10-17 07:33:19 UTC",
    "updated_date": "2025-10-23 06:54:17 UTC"
  },
  {
    "arxiv_id": "2510.15374v1",
    "title": "Towards Flash Thinking via Decoupled Advantage Policy Optimization",
    "authors": [
      "Zezhong Tan",
      "Hang Gao",
      "Xinhong Ma",
      "Feng Zhang",
      "Ziqiang Dong"
    ],
    "abstract": "Recent Large Reasoning Models (LRMs) have achieved remarkable performance in solving complex problems via supervised fine-tuning (SFT) and reinforcement learning (RL). Although existing RL algorithms significantly enhance model accuracy, they still suffer from excessively lengthy responses and overthinking issues, resulting in increased inference latency and computational consumption, especially for simple tasks that require minimal reasoning. To address this, we propose a novel RL framework, DEPO, to reduce inefficient reasoning for models. Our method mainly consists of three core components: (1) an innovative advantage decoupled algorithm to guide model reduction of inefficient tokens; (2) a difficulty-aware length penalty to lower the overall length of model responses; (3) an advantage clipping method to prevent bias in policy optimization. In our experiments, applied to DeepSeek-Distill-Qwen-7B and DeepSeek-Distill-Qwen-1.5B as base models, DEPO achieves a significant reduction in sequence length by 39% and reduces excessive reasoning paths in inefficient tokens, while outperforming the base model in overall accuracy.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15374v1",
    "published_date": "2025-10-17 07:19:20 UTC",
    "updated_date": "2025-10-17 07:19:20 UTC"
  },
  {
    "arxiv_id": "2510.15371v1",
    "title": "Cortical-SSM: A Deep State Space Model for EEG and ECoG Motor Imagery Decoding",
    "authors": [
      "Shuntaro Suzuki",
      "Shunya Nagashima",
      "Masayuki Hirata",
      "Komei Sugiura"
    ],
    "abstract": "Classification of electroencephalogram (EEG) and electrocorticogram (ECoG) signals obtained during motor imagery (MI) has substantial application potential, including for communication assistance and rehabilitation support for patients with motor impairments. These signals remain inherently susceptible to physiological artifacts (e.g., eye blinking, swallowing), which pose persistent challenges. Although Transformer-based approaches for classifying EEG and ECoG signals have been widely adopted, they often struggle to capture fine-grained dependencies within them. To overcome these limitations, we propose Cortical-SSM, a novel architecture that extends deep state space models to capture integrated dependencies of EEG and ECoG signals across temporal, spatial, and frequency domains. We validated our method across three benchmarks: 1) two large-scale public MI EEG datasets containing more than 50 subjects, and 2) a clinical MI ECoG dataset recorded from a patient with amyotrophic lateral sclerosis. Our method outperformed baseline methods on the three benchmarks. Furthermore, visual explanations derived from our model indicate that it effectively captures neurophysiologically relevant regions of both EEG and ECoG signals.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15371v1",
    "published_date": "2025-10-17 07:13:55 UTC",
    "updated_date": "2025-10-17 07:13:55 UTC"
  },
  {
    "arxiv_id": "2510.16069v2",
    "title": "Human or AI? Comparing Design Thinking Assessments by Teaching Assistants and Bots",
    "authors": [
      "Sumbul Khan",
      "Wei Ting Liow",
      "Lay Kee Ang"
    ],
    "abstract": "As design thinking education grows in secondary and tertiary contexts, educators face the challenge of evaluating creative artefacts that combine visual and textual elements. Traditional rubric-based assessment is laborious, time-consuming, and inconsistent due to reliance on Teaching Assistants (TA) in large, multi-section cohorts. This paper presents an exploratory study investigating the reliability and perceived accuracy of AI-assisted assessment compared to TA-assisted assessment in evaluating student posters in design thinking education. Two activities were conducted with 33 Ministry of Education (MOE) Singapore school teachers to (1) compare AI-generated scores with TA grading across three key dimensions: empathy and user understanding, identification of pain points and opportunities, and visual communication, and (2) examine teacher preferences for AI-assigned, TA-assigned, and hybrid scores. Results showed low statistical agreement between instructor and AI scores for empathy and pain points, with slightly higher alignment for visual communication. Teachers preferred TA-assigned scores in six of ten samples. Qualitative feedback highlighted the potential of AI for formative feedback, consistency, and student self-reflection, but raised concerns about its limitations in capturing contextual nuance and creative insight. The study underscores the need for hybrid assessment models that integrate computational efficiency with human insights. This research contributes to the evolving conversation on responsible AI adoption in creative disciplines, emphasizing the balance between automation and human judgment for scalable and pedagogically sound assessment.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "to be published in IEEE TALE 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.16069v2",
    "published_date": "2025-10-17 07:09:21 UTC",
    "updated_date": "2025-12-11 05:38:34 UTC"
  },
  {
    "arxiv_id": "2510.15363v1",
    "title": "Kernel Regression in Structured Non-IID Settings: Theory and Implications for Denoising Score Learning",
    "authors": [
      "Dechen Zhang",
      "Zhenmei Shi",
      "Yi Zhang",
      "Yingyu Liang",
      "Difan Zou"
    ],
    "abstract": "Kernel ridge regression (KRR) is a foundational tool in machine learning, with recent work emphasizing its connections to neural networks. However, existing theory primarily addresses the i.i.d. setting, while real-world data often exhibits structured dependencies - particularly in applications like denoising score learning where multiple noisy observations derive from shared underlying signals. We present the first systematic study of KRR generalization for non-i.i.d. data with signal-noise causal structure, where observations represent different noisy views of common signals. By developing a novel blockwise decomposition method that enables precise concentration analysis for dependent data, we derive excess risk bounds for KRR that explicitly depend on: (1) the kernel spectrum, (2) causal structure parameters, and (3) sampling mechanisms (including relative sample sizes for signals and noises). We further apply our results to denoising score learning, establishing generalization guarantees and providing principled guidance for sampling noisy data points. This work advances KRR theory while providing practical tools for analyzing dependent data in modern machine learning applications.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15363v1",
    "published_date": "2025-10-17 06:52:40 UTC",
    "updated_date": "2025-10-17 06:52:40 UTC"
  },
  {
    "arxiv_id": "2510.16068v1",
    "title": "Co-Designing Interdisciplinary Design Projects with AI",
    "authors": [
      "Wei Ting Liow",
      "Sumbul Khan",
      "Lay Kee Ang"
    ],
    "abstract": "Creating interdisciplinary design projects is time-consuming and cognitively demanding for teachers, requiring curriculum alignment, cross-subject integration, and careful sequencing. International research reports increasing teacher use of AI alongside persistent workload pressures, underscoring the need for planning support. This paper presents the Interdisciplinary Design Project Planner (IDPplanner), a GPT-based planning assistant grounded in Design Innovation principles, alignment with Singapore secondary school syllabuses, and 21st-century competencies. In a within-subject, counterbalanced workshop with 33 in-service teachers, participants produced two versions of the same project: manual and AI-assisted, followed by self- and peer-evaluations using a six-dimensional rubric. The AI-assisted version received higher scores for Curriculum Alignment, Design Thinking Application, and Coherence and Flow, with a marginal advantage for Assessment Strategies. Teacher reflections indicated that AI-assisted planning improved structure, sequencing, and idea generation, while contextualization to local syllabuses, class profiles, and student needs remained teacher-led. Contributions include a purpose-built planning tool that organizes ideas into a ten-component flow with ready-to-adapt prompts, templates, and assessment suggestions; an empirical, rubric-based comparison of planning quality; and evidence that AI can function as a pedagogical planning partner. Recommendations emphasize hybrid teacher-AI workflows to enhance curriculum alignment and reduce planning complexity, and design suggestions for developers to strengthen contextual customization, iterative design support, and localized rubrics. Although instantiated with a Singapore-based curriculum, the planning flow and rubric are framework-agnostic and can be parameterized for other systems.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "to be published in IEEE TALE 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.16068v1",
    "published_date": "2025-10-17 06:50:03 UTC",
    "updated_date": "2025-10-17 06:50:03 UTC"
  },
  {
    "arxiv_id": "2510.15352v1",
    "title": "GaussGym: An open-source real-to-sim framework for learning locomotion from pixels",
    "authors": [
      "Alejandro Escontrela",
      "Justin Kerr",
      "Arthur Allshire",
      "Jonas Frey",
      "Rocky Duan",
      "Carmelo Sferrazza",
      "Pieter Abbeel"
    ],
    "abstract": "We present a novel approach for photorealistic robot simulation that integrates 3D Gaussian Splatting as a drop-in renderer within vectorized physics simulators such as IsaacGym. This enables unprecedented speed -- exceeding 100,000 steps per second on consumer GPUs -- while maintaining high visual fidelity, which we showcase across diverse tasks. We additionally demonstrate its applicability in a sim-to-real robotics setting. Beyond depth-based sensing, our results highlight how rich visual semantics improve navigation and decision-making, such as avoiding undesirable regions. We further showcase the ease of incorporating thousands of environments from iPhone scans, large-scale scene datasets (e.g., GrandTour, ARKit), and outputs from generative video models like Veo, enabling rapid creation of realistic training worlds. This work bridges high-throughput simulation and high-fidelity perception, advancing scalable and generalizable robot learning. All code and data will be open-sourced for the community to build upon. Videos, code, and data available at https://escontrela.me/gauss_gym/.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15352v1",
    "published_date": "2025-10-17 06:34:52 UTC",
    "updated_date": "2025-10-17 06:34:52 UTC"
  },
  {
    "arxiv_id": "2510.15346v1",
    "title": "When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling",
    "authors": [
      "Heecheol Yun",
      "Kwangmin Ki",
      "Junghyun Lee",
      "Eunho Yang"
    ],
    "abstract": "Ensembling Large Language Models (LLMs) has gained attention as a promising approach to surpass the performance of individual models by leveraging their complementary strengths. In particular, aggregating models' next-token probability distributions to select the next token has been shown to be effective in various tasks. However, while successful for short-form answers, its application to long-form generation remains underexplored. In this paper, we show that using existing ensemble methods in long-form generation requires a careful choice of ensembling positions, since the standard practice of ensembling at every token often degrades performance. We identify two key factors for determining these positions: tokenization mismatch across models and consensus in their next-token probability distributions. Based on this, we propose SAFE, (Stable And Fast LLM Ensembling), a framework that selectively ensembles by jointly considering these factors. To further improve stability, we introduce a probability sharpening strategy that consolidates probabilities spread across multiple sub-word tokens representing the same word into a single representative token. Our experiments on diverse benchmarks, including MATH500 and BBH, demonstrate that SAFE outperforms existing methods in both accuracy and efficiency, with gains achieved even when ensembling fewer than 1% of tokens.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "preprint",
    "pdf_url": "https://arxiv.org/pdf/2510.15346v1",
    "published_date": "2025-10-17 06:18:29 UTC",
    "updated_date": "2025-10-17 06:18:29 UTC"
  },
  {
    "arxiv_id": "2510.15345v1",
    "title": "Readability Reconsidered: A Cross-Dataset Analysis of Reference-Free Metrics",
    "authors": [
      "Catarina G Belem",
      "Parker Glenn",
      "Alfy Samuel",
      "Anoop Kumar",
      "Daben Liu"
    ],
    "abstract": "Automatic readability assessment plays a key role in ensuring effective and accessible written communication. Despite significant progress, the field is hindered by inconsistent definitions of readability and measurements that rely on surface-level text properties. In this work, we investigate the factors shaping human perceptions of readability through the analysis of 897 judgments, finding that, beyond surface-level cues, information content and topic strongly shape text comprehensibility. Furthermore, we evaluate 15 popular readability metrics across five English datasets, contrasting them with six more nuanced, model-based metrics. Our results show that four model-based metrics consistently place among the top four in rank correlations with human judgments, while the best performing traditional metric achieves an average rank of 8.6. These findings highlight a mismatch between current readability metrics and human perceptions, pointing to model-based approaches as a more promising direction.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at the TSAR Workshop @ EMNLP 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.15345v1",
    "published_date": "2025-10-17 06:17:21 UTC",
    "updated_date": "2025-10-17 06:17:21 UTC"
  },
  {
    "arxiv_id": "2510.15331v1",
    "title": "ASBI: Leveraging Informative Real-World Data for Active Black-Box Simulator Tuning",
    "authors": [
      "Gahee Kim",
      "Takamitsu Matsubara"
    ],
    "abstract": "Black-box simulators are widely used in robotics, but optimizing their parameters remains challenging due to inaccessible likelihoods. Simulation-Based Inference (SBI) tackles this issue using simulation-driven approaches, estimating the posterior from offline real observations and forward simulations. However, in black-box scenarios, preparing observations that contain sufficient information for parameter estimation is difficult due to the unknown relationship between parameters and observations. In this work, we present Active Simulation-Based Inference (ASBI), a parameter estimation framework that uses robots to actively collect real-world online data to achieve accurate black-box simulator tuning. Our framework optimizes robot actions to collect informative observations by maximizing information gain, which is defined as the expected reduction in Shannon entropy between the posterior and the prior. While calculating information gain requires the likelihood, which is inaccessible in black-box simulators, our method solves this problem by leveraging Neural Posterior Estimation (NPE), which leverages a neural network to learn the posterior estimator. Three simulation experiments quantitatively verify that our method achieves accurate parameter estimation, with posteriors sharply concentrated around the true parameters. Moreover, we show a practical application using a real robot to estimate the simulation parameters of cubic particles corresponding to two real objects, beads and gravel, with a bucket pouring action.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15331v1",
    "published_date": "2025-10-17 05:38:33 UTC",
    "updated_date": "2025-10-17 05:38:33 UTC"
  },
  {
    "arxiv_id": "2510.15330v1",
    "title": "BeLLMan: Controlling LLM Congestion",
    "authors": [
      "Tella Rajashekhar Reddy",
      "Atharva Deshmukh",
      "Karan Tandon",
      "Rohan Gandhi",
      "Anjaly Parayil",
      "Debopam Bhattacherjee"
    ],
    "abstract": "Large language model (LLM) applications are blindfolded to the infrastructure underneath and generate tokens autoregressively, indifferent to the system load, thus risking inferencing latency inflation and poor user experience. Our first-cut controller, named beLLMan, enables the LLM infrastructure to actively and progressively signal the first-party LLM application to adjust the output length in response to changing system load. On a real testbed with H100 GPUs, beLLMan helps keep inferencing latency under control (upto 8X lower end-to-end latency) and reduces energy consumption by 25% (while serving 19% more requests) during periods of congestion for a summarization workload.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CL",
      "cs.NI"
    ],
    "primary_category": "cs.DC",
    "comment": "To be presented at FAISYS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.15330v1",
    "published_date": "2025-10-17 05:36:42 UTC",
    "updated_date": "2025-10-17 05:36:42 UTC"
  },
  {
    "arxiv_id": "2510.15317v1",
    "title": "VERITAS: Leveraging Vision Priors and Expert Fusion to Improve Multimodal Data",
    "authors": [
      "Tingqiao Xu",
      "Ziru Zeng",
      "Jiayu Chen"
    ],
    "abstract": "The quality of supervised fine-tuning (SFT) data is crucial for the performance of large multimodal models (LMMs), yet current data enhancement methods often suffer from factual errors and hallucinations due to inadequate visual perception. To address this challenge, we propose VERITAS, a pipeline that systematically integrates vision priors and multiple state-of-the-art LMMs with statistical methods to enhance SFT data quality. VERITAS leverages visual recognition models (RAM++) and OCR systems (PP-OCRv4) to extract structured vision priors, which are combined with images, questions, and answers. Three LMMs (GPT-4o, Gemini-2.5-Pro, Doubao-1.5-pro) evaluate the original answers, providing critique rationales and scores that are statistically fused into a high-confidence consensus score serving as ground truth. Using this consensus, we train a lightweight critic model via Group Relative Policy Optimization (GRPO), enhancing reasoning capabilities efficiently. Each LMM then refines the original answers based on the critiques, generating new candidate answers; we select the highest-scoring one as the final refined answer. Experiments across six multimodal benchmarks demonstrate that models fine-tuned with data processed by VERITAS consistently outperform those using raw data, particularly in text-rich and fine-grained reasoning tasks. Our critic model exhibits enhanced capability comparable to state-of-the-art LMMs while being significantly more efficient. We release our pipeline, datasets, and model checkpoints to advance research in multimodal data optimization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to EMNLP 2025 (Main Conference)",
    "pdf_url": "https://arxiv.org/pdf/2510.15317v1",
    "published_date": "2025-10-17 05:13:50 UTC",
    "updated_date": "2025-10-17 05:13:50 UTC"
  },
  {
    "arxiv_id": "2510.15306v1",
    "title": "WebGen-V Bench: Structured Representation for Enhancing Visual Design in LLM-based Web Generation and Evaluation",
    "authors": [
      "Kuang-Da Wang",
      "Zhao Wang",
      "Yotaro Shimose",
      "Wei-Yao Wang",
      "Shingo Takamatsu"
    ],
    "abstract": "Witnessed by the recent advancements on leveraging LLM for coding and multimodal understanding, we present WebGen-V, a new benchmark and framework for instruction-to-HTML generation that enhances both data quality and evaluation granularity. WebGen-V contributes three key innovations: (1) an unbounded and extensible agentic crawling framework that continuously collects real-world webpages and can leveraged to augment existing benchmarks; (2) a structured, section-wise data representation that integrates metadata, localized UI screenshots, and JSON-formatted text and image assets, explicit alignment between content, layout, and visual components for detailed multimodal supervision; and (3) a section-level multimodal evaluation protocol aligning text, layout, and visuals for high-granularity assessment. Experiments with state-of-the-art LLMs and ablation studies validate the effectiveness of our structured data and section-wise evaluation, as well as the contribution of each component. To the best of our knowledge, WebGen-V is the first work to enable high-granularity agentic crawling and evaluation for instruction-to-HTML generation, providing a unified pipeline from real-world data acquisition and webpage generation to structured multimodal assessment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15306v1",
    "published_date": "2025-10-17 04:37:37 UTC",
    "updated_date": "2025-10-17 04:37:37 UTC"
  },
  {
    "arxiv_id": "2511.07423v1",
    "title": "Synera: Synergistic LLM Serving across Device and Cloud at Scale",
    "authors": [
      "Genglin Wang",
      "Liekang Zeng",
      "Bufang Yang",
      "Kaiwei Liu",
      "Guoliang Xing",
      "Chumin Sun",
      "Li Zhou",
      "Jie Sun",
      "Zhenyu Yan"
    ],
    "abstract": "Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2511.07423v1",
    "published_date": "2025-10-17 04:31:50 UTC",
    "updated_date": "2025-10-17 04:31:50 UTC"
  },
  {
    "arxiv_id": "2510.15303v1",
    "title": "DSSmoothing: Toward Certified Dataset Ownership Verification for Pre-trained Language Models via Dual-Space Smoothing",
    "authors": [
      "Ting Qiao",
      "Xing Liu",
      "Wenke Huang",
      "Jianbin Li",
      "Zhaoxin Fan",
      "Yiming Li"
    ],
    "abstract": "Large web-scale datasets have driven the rapid advancement of pre-trained language models (PLMs), but unauthorized data usage has raised serious copyright concerns. Existing dataset ownership verification (DOV) methods typically assume that watermarks remain stable during inference; however, this assumption often fails under natural noise and adversary-crafted perturbations. We propose the first certified dataset ownership verification method for PLMs based on dual-space smoothing (i.e., DSSmoothing). To address the challenges of text discreteness and semantic sensitivity, DSSmoothing introduces continuous perturbations in the embedding space to capture semantic robustness and applies controlled token reordering in the permutation space to capture sequential robustness. DSSmoothing consists of two stages: in the first stage, triggers are collaboratively embedded in both spaces to generate norm-constrained and robust watermarked datasets; in the second stage, randomized smoothing is applied in both spaces during verification to compute the watermark robustness (WR) of suspicious models and statistically compare it with the principal probability (PP) values of a set of benign models. Theoretically, DSSmoothing provides provable robustness guarantees for dataset ownership verification by ensuring that WR consistently exceeds PP under bounded dual-space perturbations. Extensive experiments on multiple representative web datasets demonstrate that DSSmoothing achieves stable and reliable verification performance and exhibits robustness against potential adaptive attacks.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CR",
    "comment": "13 pages, 21 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.15303v1",
    "published_date": "2025-10-17 04:25:32 UTC",
    "updated_date": "2025-10-17 04:25:32 UTC"
  },
  {
    "arxiv_id": "2510.17877v1",
    "title": "DRL-Based Resource Allocation for Energy-Efficient IRS-Assisted UAV Spectrum Sharing Systems",
    "authors": [
      "Yiheng Wang"
    ],
    "abstract": "Intelligent reflecting surface (IRS) assisted unmanned aerial vehicle (UAV) systems provide a new paradigm for reconfigurable and flexible wireless communications. To enable more energy efficient and spectrum efficient IRS assisted UAV wireless communications, this paper introduces a novel IRS-assisted UAV enabled spectrum sharing system with orthogonal frequency division multiplexing (OFDM). The goal is to maximize the energy efficiency (EE) of the secondary network by jointly optimizing the beamforming, subcarrier allocation, IRS phase shifts, and the UAV trajectory subject to practical transmit power and passive reflection constraints as well as UAV physical limitations. A physically grounded propulsion-energy model is adopted, with its tight upper bound used to form a tractable EE lower bound for the spectrum sharing system. To handle highly non convex, time coupled optimization problems with a mixed continuous and discrete policy space, we develop a deep reinforcement learning (DRL) approach based on the actor critic framework. Extended experiments show the significant EE improvement of the proposed DRL-based approach compared to several benchmark schemes, thus demonstrating the effectiveness and robustness of the proposed approach with mobility.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "eess.SY",
    "comment": "7 pages, 3 figures, 1 algorithm. LaTeX class: IEEEtran",
    "pdf_url": "https://arxiv.org/pdf/2510.17877v1",
    "published_date": "2025-10-17 04:18:17 UTC",
    "updated_date": "2025-10-17 04:18:17 UTC"
  },
  {
    "arxiv_id": "2510.15301v3",
    "title": "Latent Diffusion Model without Variational Autoencoder",
    "authors": [
      "Minglei Shi",
      "Haolin Wang",
      "Wenzhao Zheng",
      "Ziyang Yuan",
      "Xiaoshi Wu",
      "Xintao Wang",
      "Pengfei Wan",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "abstract": "Recent progress in diffusion-based visual generation has largely relied on latent diffusion models with variational autoencoders (VAEs). While effective for high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited training efficiency, slow inference, and poor transferability to broader vision tasks. These issues stem from a key limitation of VAE latent spaces: the lack of clear semantic separation and strong discriminative structure. Our analysis confirms that these properties are crucial not only for perception and understanding tasks, but also for the stable and efficient training of latent diffusion models. Motivated by this insight, we introduce SVG, a novel latent diffusion model without variational autoencoders, which leverages self-supervised representations for visual generation. SVG constructs a feature space with clear semantic discriminability by leveraging frozen DINO features, while a lightweight residual branch captures fine-grained details for high-fidelity reconstruction. Diffusion models are trained directly on this semantically structured latent space to facilitate more efficient learning. As a result, SVG enables accelerated diffusion training, supports few-step sampling, and improves generative quality. Experimental results further show that SVG preserves the semantic and discriminative capabilities of the underlying self-supervised representations, providing a principled pathway toward task-general, high-quality visual representations. Code and interpretations are available at https://howlin-wang.github.io/svg/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15301v3",
    "published_date": "2025-10-17 04:17:44 UTC",
    "updated_date": "2025-10-21 02:50:02 UTC"
  },
  {
    "arxiv_id": "2510.15297v2",
    "title": "VERA-MH Concept Paper",
    "authors": [
      "Luca Belli",
      "Kate Bentley",
      "Will Alexander",
      "Emily Ward",
      "Matt Hawrilenko",
      "Kelly Johnston",
      "Mill Brown",
      "Adam Chekroud"
    ],
    "abstract": "We introduce VERA-MH (Validation of Ethical and Responsible AI in Mental Health), an automated evaluation of the safety of AI chatbots used in mental health contexts, with an initial focus on suicide risk.\n  Practicing clinicians and academic experts developed a rubric informed by best practices for suicide risk management for the evaluation. To fully automate the process, we used two ancillary AI agents. A user-agent model simulates users engaging in a mental health-based conversation with the chatbot under evaluation. The user-agent role-plays specific personas with pre-defined risk levels and other features. Simulated conversations are then passed to a judge-agent who scores them based on the rubric. The final evaluation of the chatbot being tested is obtained by aggregating the scoring of each conversation.\n  VERA-MH is actively under development and undergoing rigorous validation by mental health clinicians to ensure user-agents realistically act as patients and that the judge-agent accurately scores the AI chatbot. To date we have conducted preliminary evaluation of GPT-5, Claude Opus and Claude Sonnet using initial versions of the VERA-MH rubric and used the findings for further design development. Next steps will include more robust clinical validation and iteration, as well as refining actionable scoring. We are seeking feedback from the community on both the technical and clinical aspects of our evaluation.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15297v2",
    "published_date": "2025-10-17 04:07:29 UTC",
    "updated_date": "2025-10-22 02:19:06 UTC"
  },
  {
    "arxiv_id": "2510.15294v1",
    "title": "Identifying internal patterns in (1+1)-dimensional directed percolation using neural networks",
    "authors": [
      "Danil Parkhomenko",
      "Pavel Ovchinnikov",
      "Konstantin Soldatov",
      "Vitalii Kapitan",
      "Gennady Y. Chitov"
    ],
    "abstract": "In this paper we present a neural network-based method for the automatic detection of phase transitions and classification of hidden percolation patterns in a (1+1)-dimensional replication process. The proposed network model is based on the combination of CNN, TCN and GRU networks, which are trained directly on raw configurations without any manual feature extraction. The network reproduces the phase diagram and assigns phase labels to configurations. It shows that deep architectures are capable of extracting hierarchical structures from the raw data of numerical experiments.",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "7 pages, 10 figures, 2 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.15294v1",
    "published_date": "2025-10-17 04:06:07 UTC",
    "updated_date": "2025-10-17 04:06:07 UTC"
  },
  {
    "arxiv_id": "2510.16066v3",
    "title": "Cash Flow Underwriting with Bank Transaction Data: Advancing MSME Financial Inclusion in Malaysia",
    "authors": [
      "Chun Chet Ng",
      "Wei Zeng Low",
      "Jia Yu Lim",
      "Yin Yin Boon"
    ],
    "abstract": "Despite accounting for 96.1% of all businesses in Malaysia, access to financing remains one of the most persistent challenges faced by Micro, Small, and Medium Enterprises (MSMEs). Newly established businesses are often excluded from formal credit markets as traditional underwriting approaches rely heavily on credit bureau data. This study investigates the potential of bank statement data as an alternative data source for credit assessment to promote financial inclusion in emerging markets. First, we propose a cash flow-based underwriting pipeline where we utilise bank statement data for end-to-end data extraction and machine learning credit scoring. Second, we introduce a novel dataset of 611 loan applicants from a Malaysian lending institution. Third, we develop and evaluate credit scoring models based on application information and bank transaction-derived features. Empirical results show that the use of such data boosts the performance of all models on our dataset, which can improve credit scoring for new-to-lending MSMEs. Finally, we will release the anonymised bank transaction dataset to facilitate further research on MSME financial inclusion within Malaysia's emerging economy.",
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.CE",
      "cs.CY",
      "cs.LG",
      "q-fin.RM"
    ],
    "primary_category": "q-fin.ST",
    "comment": "Accepted for oral presentation at the AI for Financial Inclusion, Risk Modeling and Resilience in Emerging Markets (FinRem) Workshop at ACM ICAIF 2025, Singapore. Accepted for poster presentation at the Agentic AI in Financial Services Workshop at AAAI 2026, Singapore",
    "pdf_url": "https://arxiv.org/pdf/2510.16066v3",
    "published_date": "2025-10-17 03:56:11 UTC",
    "updated_date": "2025-12-23 09:35:05 UTC"
  },
  {
    "arxiv_id": "2510.16065v1",
    "title": "FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning",
    "authors": [
      "Lunchen Xie",
      "Zehua He",
      "Qingjiang Shi"
    ],
    "abstract": "Personalized Federated Learning (PFL) has emerged as a critical research frontier addressing data heterogeneity issue across distributed clients. Novel model architectures and collaboration mechanisms are engineered to accommodate statistical disparities while producing client-specific models. Parameter decoupling represents a promising paradigm for maintaining model performance in PFL frameworks. However, the communication efficiency of many existing methods remains suboptimal, sustaining substantial communication burdens that impede practical deployment. To bridge this gap, we propose Federated Learning with Programmed Update and Reduced INformation (FedPURIN), a novel framework that strategically identifies critical parameters for transmission through an integer programming formulation. This mathematically grounded strategy is seamlessly integrated into a sparse aggregation scheme, achieving a significant communication reduction while preserving the efficacy. Comprehensive evaluations on standard image classification benchmarks under varied non-IID conditions demonstrate competitive performance relative to state-of-the-art methods, coupled with quantifiable communication reduction through sparse aggregation. The framework establishes a new paradigm for communication-efficient PFL, particularly advantageous for edge intelligence systems operating with heterogeneous data sources.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16065v1",
    "published_date": "2025-10-17 03:54:43 UTC",
    "updated_date": "2025-10-17 03:54:43 UTC"
  },
  {
    "arxiv_id": "2510.17875v1",
    "title": "3D Weakly Supervised Semantic Segmentation via Class-Aware and Geometry-Guided Pseudo-Label Refinement",
    "authors": [
      "Xiaoxu Xu",
      "Xuexun Liu",
      "Jinlong Li",
      "Yitian Yuan",
      "Qiudan Zhang",
      "Lin Ma",
      "Nicu Sebe",
      "Xu Wang"
    ],
    "abstract": "3D weakly supervised semantic segmentation (3D WSSS) aims to achieve semantic segmentation by leveraging sparse or low-cost annotated data, significantly reducing reliance on dense point-wise annotations. Previous works mainly employ class activation maps or pre-trained vision-language models to address this challenge. However, the low quality of pseudo-labels and the insufficient exploitation of 3D geometric priors jointly create significant technical bottlenecks in developing high-performance 3D WSSS models. In this paper, we propose a simple yet effective 3D weakly supervised semantic segmentation method that integrates 3D geometric priors into a class-aware guidance mechanism to generate high-fidelity pseudo labels. Concretely, our designed methodology first employs Class-Aware Label Refinement module to generate more balanced and accurate pseudo labels for semantic categrories. This initial refinement stage focuses on enhancing label quality through category-specific optimization. Subsequently, the Geometry-Aware Label Refinement component is developed, which strategically integrates implicit 3D geometric constraints to effectively filter out low-confidence pseudo labels that fail to comply with geometric plausibility. Moreover, to address the challenge of extensive unlabeled regions, we propose a Label Update strategy that integrates Self-Training to propagate labels into these areas. This iterative process continuously enhances pseudo-label quality while expanding label coverage, ultimately fostering the development of high-performance 3D WSSS models. Comprehensive experimental validation reveals that our proposed methodology achieves state-of-the-art performance on both ScanNet and S3DIS benchmarks while demonstrating remarkable generalization capability in unsupervised settings, maintaining competitive accuracy through its robust design.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.17875v1",
    "published_date": "2025-10-17 03:53:43 UTC",
    "updated_date": "2025-10-17 03:53:43 UTC"
  },
  {
    "arxiv_id": "2510.17874v1",
    "title": "Repairing Tool Calls Using Post-tool Execution Reflection and RAG",
    "authors": [
      "Jason Tsay",
      "Zidane Wright",
      "Gaodan Fang",
      "Kiran Kate",
      "Saurabh Jha",
      "Yara Rizk"
    ],
    "abstract": "Agentic systems interact with external systems by calling tools such as Python functions, REST API endpoints, or command line tools such as kubectl in Kubernetes. These tool calls often fail for various syntactic and semantic reasons. Some less obvious semantic errors can only be identified and resolved after analyzing the tool's response. To repair these errors, we develop a post-tool execution reflection component that combines large language model (LLM)-based reflection with domain-specific retrieval-augmented generation (RAG) using documents describing both the specific tool being called and troubleshooting documents related to the tool. For this paper, we focus on the use case of the kubectl command line tool to manage Kubernetes, a platform for orchestrating cluster applications. Through a larger empirical study and a smaller manual evaluation, we find that our RAG-based reflection will repair kubectl commands such that they are both more likely to successfully execute (pass rate) for 55% of our models evaluated and 36% more likely to correctly answer the user query on average. We find that troubleshooting documents improve pass rate compared to official documentation by an average of 10%.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.17874v1",
    "published_date": "2025-10-17 03:50:37 UTC",
    "updated_date": "2025-10-17 03:50:37 UTC"
  },
  {
    "arxiv_id": "2510.15286v1",
    "title": "MTmixAtt: Integrating Mixture-of-Experts with Multi-Mix Attention for Large-Scale Recommendation",
    "authors": [
      "Xianyang Qi",
      "Yuan Tian",
      "Zhaoyu Hu",
      "Zhirui Kuai",
      "Chang Liu",
      "Hongxiang Lin",
      "Lei Wang"
    ],
    "abstract": "Industrial recommender systems critically depend on high-quality ranking models. However, traditional pipelines still rely on manual feature engineering and scenario-specific architectures, which hinder cross-scenario transfer and large-scale deployment. To address these challenges, we propose \\textbf{MTmixAtt}, a unified Mixture-of-Experts (MoE) architecture with Multi-Mix Attention, designed for large-scale recommendation tasks. MTmixAtt integrates two key components. The \\textbf{AutoToken} module automatically clusters heterogeneous features into semantically coherent tokens, removing the need for human-defined feature groups. The \\textbf{MTmixAttBlock} module enables efficient token interaction via a learnable mixing matrix, shared dense experts, and scenario-aware sparse experts, capturing both global patterns and scenario-specific behaviors within a single framework. Extensive experiments on the industrial TRec dataset from Meituan demonstrate that MTmixAtt consistently outperforms state-of-the-art baselines including Transformer-based models, WuKong, HiFormer, MLP-Mixer, and RankMixer. At comparable parameter scales, MTmixAtt achieves superior CTR and CTCVR metrics; scaling to MTmixAtt-1B yields further monotonic gains. Large-scale online A/B tests validate the real-world impact: in the \\textit{Homepage} scenario, MTmixAtt increases Payment PV by \\textbf{+3.62\\%} and Actual Payment GTV by \\textbf{+2.54\\%}. Overall, MTmixAtt provides a unified and scalable solution for modeling arbitrary heterogeneous features across scenarios, significantly improving both user experience and commercial outcomes.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15286v1",
    "published_date": "2025-10-17 03:50:09 UTC",
    "updated_date": "2025-10-17 03:50:09 UTC"
  },
  {
    "arxiv_id": "2510.15283v1",
    "title": "Exemplar-Guided Planing: Enhanced LLM Agent for KGQA",
    "authors": [
      "Jingao Xu",
      "Shuoyoucheng Ma",
      "Xin Song",
      "Rong Jiang",
      "Hongkui Tu",
      "Bin Zhou"
    ],
    "abstract": "Large Language Models (LLMs) as interactive agents show significant promise in Knowledge Graph Question Answering (KGQA) but often struggle with the semantic gap between natural language queries and structured knowledge graph (KG) representations. This leads to suboptimal planning and inefficient exploration on KG, while training-free approaches often underutilize valuable reasoning patterns in training data. To address these limitations, we propose a novel framework, Exemplar-Guided Planning (EGP), which enhances the planning capabilities of LLM agents for KGQA. EGP first preprocesses the training set questions via entity templating to normalize semantic variations. It then retrieves highly similar exemplary questions and their successful reasoning paths from this preprocessed set using semantic embeddings and an efficient FAISS index. These retrieved exemplars dynamically guide the LLM's planning process in two key phases: (1) Task Decomposition, by aligning generated sub-objectives with proven reasoning steps, and (2) Relation Exploration, by providing high-quality auxiliary information to improve relation pruning accuracy. Additionally, we introduce a Smart Lookahead mechanism during relation exploration to improve efficiency by preemptively exploring promising paths and potentially terminating exploration earlier. We apply EGP to the Plan-on-Graph (PoG) framework, termed PoG-EGP. Extensive experiments on two real-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP significantly improves over the baseline PoG system and other compared methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15283v1",
    "published_date": "2025-10-17 03:43:06 UTC",
    "updated_date": "2025-10-17 03:43:06 UTC"
  },
  {
    "arxiv_id": "2510.15282v1",
    "title": "Post-Processing Methods for Improving Accuracy in MRI Inpainting",
    "authors": [
      "Nishad Kulkarni",
      "Krithika Iyer",
      "Austin Tapp",
      "Abhijeet Parida",
      "Daniel Capelln-Martn",
      "Zhifan Jiang",
      "Mara J. Ledesma-Carbayo",
      "Syed Muhammad Anwar",
      "Marius George Linguraru"
    ],
    "abstract": "Magnetic Resonance Imaging (MRI) is the primary imaging modality used in the diagnosis, assessment, and treatment planning for brain pathologies. However, most automated MRI analysis tools, such as segmentation and registration pipelines, are optimized for healthy anatomies and often fail when confronted with large lesions such as tumors. To overcome this, image inpainting techniques aim to locally synthesize healthy brain tissues in tumor regions, enabling the reliable application of general-purpose tools. In this work, we systematically evaluate state-of-the-art inpainting models and observe a saturation in their standalone performance. In response, we introduce a methodology combining model ensembling with efficient post-processing strategies such as median filtering, histogram matching, and pixel averaging. Further anatomical refinement is achieved via a lightweight U-Net enhancement stage. Comprehensive evaluation demonstrates that our proposed pipeline improves the anatomical plausibility and visual fidelity of inpainted regions, yielding higher accuracy and more robust outcomes than individual baseline models. By combining established models with targeted post-processing, we achieve improved and more accessible inpainting outcomes, supporting broader clinical deployment and sustainable, resource-conscious research. Our 2025 BraTS inpainting docker is available at https://hub.docker.com/layers/aparida12/brats2025/inpt.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15282v1",
    "published_date": "2025-10-17 03:42:23 UTC",
    "updated_date": "2025-10-17 03:42:23 UTC"
  },
  {
    "arxiv_id": "2510.15280v1",
    "title": "Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition",
    "authors": [
      "Fan Liu",
      "Jindong Han",
      "Tengfei Lyu",
      "Weijia Zhang",
      "Zhe-Rui Yang",
      "Lu Dai",
      "Cancheng Liu",
      "Hao Liu"
    ],
    "abstract": "Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the landscape of scientific research. Beyond accelerating tasks such as hypothesis generation, experimental design, and result interpretation, they prompt a more fundamental question: Are FMs merely enhancing existing scientific methodologies, or are they redefining the way science is conducted? In this paper, we argue that FMs are catalyzing a transition toward a new scientific paradigm. We introduce a three-stage framework to describe this evolution: (1) Meta-Scientific Integration, where FMs enhance workflows within traditional paradigms; (2) Hybrid Human-AI Co-Creation, where FMs become active collaborators in problem formulation, reasoning, and discovery; and (3) Autonomous Scientific Discovery, where FMs operate as independent agents capable of generating new scientific knowledge with minimal human intervention. Through this lens, we review current applications and emerging capabilities of FMs across existing scientific paradigms. We further identify risks and future directions for FM-enabled scientific discovery. This position paper aims to support the scientific community in understanding the transformative role of FMs and to foster reflection on the future of scientific discovery. Our project is available at https://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2025",
    "pdf_url": "https://arxiv.org/pdf/2510.15280v1",
    "published_date": "2025-10-17 03:40:26 UTC",
    "updated_date": "2025-10-17 03:40:26 UTC"
  },
  {
    "arxiv_id": "2510.15269v2",
    "title": "TACL: Threshold-Adaptive Curriculum Learning Strategy for Enhancing Medical Text Understanding",
    "authors": [
      "Mucheng Ren",
      "Yucheng Yan",
      "He Chen",
      "Danqing Hu",
      "Jun Xu",
      "Xian Zeng"
    ],
    "abstract": "Medical texts, particularly electronic medical records (EMRs), are a cornerstone of modern healthcare, capturing critical information about patient care, diagnoses, and treatments. These texts hold immense potential for advancing clinical decision-making and healthcare analytics. However, their unstructured nature, domain-specific language, and variability across contexts make automated understanding an intricate challenge. Despite the advancements in natural language processing, existing methods often treat all data as equally challenging, ignoring the inherent differences in complexity across clinical records. This oversight limits the ability of models to effectively generalize and perform well on rare or complex cases. In this paper, we present TACL (Threshold-Adaptive Curriculum Learning), a novel framework designed to address these challenges by rethinking how models interact with medical texts during training. Inspired by the principle of progressive learning, TACL dynamically adjusts the training process based on the complexity of individual samples. By categorizing data into difficulty levels and prioritizing simpler cases early in training, the model builds a strong foundation before tackling more complex records. By applying TACL to multilingual medical data, including English and Chinese clinical records, we observe significant improvements across diverse clinical tasks, including automatic ICD coding, readmission prediction and TCM syndrome differentiation. TACL not only enhances the performance of automated systems but also demonstrates the potential to unify approaches across disparate medical domains, paving the way for more accurate, scalable, and globally applicable medical text understanding solutions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted as BIBM 2025 Regular. 6 pages. Camera Ready version",
    "pdf_url": "https://arxiv.org/pdf/2510.15269v2",
    "published_date": "2025-10-17 03:16:51 UTC",
    "updated_date": "2025-11-11 15:05:04 UTC"
  },
  {
    "arxiv_id": "2510.15267v2",
    "title": "TraceCoder: Towards Traceable ICD Coding via Multi-Source Knowledge Integration",
    "authors": [
      "Mucheng Ren",
      "He Chen",
      "Yuchen Yan",
      "Danqing Hu",
      "Jun Xu",
      "Xian Zeng"
    ],
    "abstract": "Automated International Classification of Diseases (ICD) coding assigns standardized diagnosis and procedure codes to clinical records, playing a critical role in healthcare systems. However, existing methods face challenges such as semantic gaps between clinical text and ICD codes, poor performance on rare and long-tail codes, and limited interpretability. To address these issues, we propose TraceCoder, a novel framework integrating multi-source external knowledge to enhance traceability and explainability in ICD coding. TraceCoder dynamically incorporates diverse knowledge sources, including UMLS, Wikipedia, and large language models (LLMs), to enrich code representations, bridge semantic gaps, and handle rare and ambiguous codes. It also introduces a hybrid attention mechanism to model interactions among labels, clinical context, and knowledge, improving long-tail code recognition and making predictions interpretable by grounding them in external evidence. Experiments on MIMIC-III-ICD9, MIMIC-IV-ICD9, and MIMIC-IV-ICD10 datasets demonstrate that TraceCoder achieves state-of-the-art performance, with ablation studies validating the effectiveness of its components. TraceCoder offers a scalable and robust solution for automated ICD coding, aligning with clinical needs for accuracy, interpretability, and reliability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accpeted as BIBM 2025 Regular. 6 pages. Camera-Ready version",
    "pdf_url": "https://arxiv.org/pdf/2510.15267v2",
    "published_date": "2025-10-17 03:08:07 UTC",
    "updated_date": "2025-11-11 15:04:48 UTC"
  },
  {
    "arxiv_id": "2510.15262v1",
    "title": "Robust Layerwise Scaling Rules by Proper Weight Decay Tuning",
    "authors": [
      "Zhiyuan Fan",
      "Yifeng Liu",
      "Qingyue Zhao",
      "Angela Yuan",
      "Quanquan Gu"
    ],
    "abstract": "Empirical scaling laws prescribe how to allocate parameters, data, and compute, while maximal-update parameterization ($$P) enables learning-rate transfer across widths by equalizing early-time update magnitudes. However, in modern scale-invariant architectures, training quickly enters an optimizer-governed steady state where normalization layers create backward scale sensitivity and the effective learning rate becomes width dependent, degrading $$P transfer. We address this by introducing a weight-decay scaling rule for AdamW that preserves sublayer gain across widths. Empirically, the singular-value spectrum of each matrix parameter scales in norm as $\\sqrt{/}$ with an approximately invariant shape; under width scaling $d$, we observe that the top singular value scales approximately as $\\sqrt{/}\\cdot d^{0.75}$. Combining this observation with the $$P learning-rate rule $_2\\propto d^{-1}$ for matrix-like parameters implies an empirical weight-decay scaling rule $_2\\propto \\sqrt{d}$ that approximately keeps sublayer gains width invariant. Together with vector-like parameters trained at $_1=_d(1)$ and $_1=0$, this yields \\emph{zero-shot} transfer of both learning rate and weight decay from proxy to target widths, removing per-width sweeps. We validate the rule on LLaMA-style Transformers and in a minimal synthetic setting, and we provide a simple diagnostic, matching top singular values, to check sublayer-gain invariance. Our results extend $$P beyond the near-init regime by explicitly controlling steady-state scales set by the optimizer, offering a practical recipe for width-robust hyperparameter transfer under AdamW.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15262v1",
    "published_date": "2025-10-17 02:58:35 UTC",
    "updated_date": "2025-10-17 02:58:35 UTC"
  },
  {
    "arxiv_id": "2510.15261v1",
    "title": "AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory",
    "authors": [
      "Jitesh Jain",
      "Shubham Maheshwari",
      "Ning Yu",
      "Wen-mei Hwu",
      "Humphrey Shi"
    ],
    "abstract": "Riding on the success of LLMs with retrieval-augmented generation (RAG), there has been a growing interest in augmenting agent systems with external memory databases. However, the existing systems focus on storing text information in their memory, ignoring the importance of multimodal signals. Motivated by the multimodal nature of human memory, we present AUGUSTUS, a multimodal agent system aligned with the ideas of human memory in cognitive science. Technically, our system consists of 4 stages connected in a loop: (i) encode: understanding the inputs; (ii) store in memory: saving important information; (iii) retrieve: searching for relevant context from memory; and (iv) act: perform the task. Unlike existing systems that use vector databases, we propose conceptualizing information into semantic tags and associating the tags with their context to store them in a graph-structured multimodal contextual memory for efficient concept-driven retrieval. Our system outperforms the traditional multimodal RAG approach while being 3.5 times faster for ImageNet classification and outperforming MemGPT on the MSC benchmark.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "LAW 2025 Workshop at NeurIPS 2025. Work done from late 2023 to early 2024",
    "pdf_url": "https://arxiv.org/pdf/2510.15261v1",
    "published_date": "2025-10-17 02:58:22 UTC",
    "updated_date": "2025-10-17 02:58:22 UTC"
  },
  {
    "arxiv_id": "2510.16064v1",
    "title": "Residual Correction Models for AC Optimal Power Flow Using DC Optimal Power Flow Solutions",
    "authors": [
      "Muhy Eddin Za'ter",
      "Bri-Mathias Hodge",
      "Kyri Baker"
    ],
    "abstract": "Solving the nonlinear AC optimal power flow (AC OPF) problem remains a major computational bottleneck for real-time grid operations. In this paper, we propose a residual learning paradigm that uses fast DC optimal power flow (DC OPF) solutions as a baseline, and learns only the nonlinear corrections required to provide the full AC-OPF solution. The method utilizes a topology-aware Graph Neural Network with local attention and two-level DC feature integration, trained using a physics-informed loss that enforces AC power-flow feasibility and operational limits. Evaluations on OPFData for 57-, 118-, and 2000-bus systems show around 25% lower MSE, up to 3X reduction in feasibility error, and up to 13X runtime speedup compared to conventional AC OPF solvers. The model maintains accuracy under N-1 contingencies and scales efficiently to large networks. These results demonstrate that residual learning is a practical and scalable bridge between linear approximations and AC-feasible OPF, enabling near real-time operational decision making.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16064v1",
    "published_date": "2025-10-17 02:56:29 UTC",
    "updated_date": "2025-10-17 02:56:29 UTC"
  },
  {
    "arxiv_id": "2510.15260v1",
    "title": "DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models",
    "authors": [
      "Yangyang Li"
    ],
    "abstract": "Large language models are highly sensitive to prompt wording. However, popular automatic prompt search methods, including InstructZero, often degrade under distribution shift and adversarial evaluation because they optimize expected performance under a single evaluation distribution. Consequently, prompts that work in one setting frequently fail to transfer. To address this, DRO-InstructZero formulates zero-shot prompt optimization as robust Bayesian optimization. Specifically, an f-divergence ball defines an ambiguity set around the evaluation distribution, and a robust acquisition rule maximizes worst-case expected utility while retaining the query efficiency of Bayesian search. Therefore, the search explicitly targets reliability under distribution shift rather than average behavior alone. Experiments follow the instruction-induction protocol with matched query budgets across formality rewriting, code debugging, and translation. For example, on BIG-Bench informative-to-formal rewriting, accuracy improves from 61.3 +/- 0.7% to approximately 85-90%, yielding an absolute gain of about 25-30 points. Moreover, auto-debugging shows about +25-point gains under domain shift. Meanwhile, stable tasks such as cause-and-effect remain above 96%, indicating no loss on in-distribution cases. Furthermore, improvements are consistent across divergence choices and decoding temperatures. Overall, DRO-InstructZero connects distributionally robust optimization with prompt learning, offering a plug-and-play and general approach for reliable, transferable prompt alignment under real-world uncertainty.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Preprint. Under review at ICLR 2026. 11 pages, 2 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.15260v1",
    "published_date": "2025-10-17 02:55:48 UTC",
    "updated_date": "2025-10-17 02:55:48 UTC"
  },
  {
    "arxiv_id": "2510.15259v2",
    "title": "Experience-Driven Exploration for Efficient API-Free AI Agents",
    "authors": [
      "Chenwei Tang",
      "Jingyu Xing",
      "Xinyu Liu",
      "Zizhou Wang",
      "Jiawei Du",
      "Liangli Zhen",
      "Jiancheng Lv"
    ],
    "abstract": "Most existing software lacks accessible Application Programming Interfaces (APIs), requiring agents to operate solely through pixel-based Graphical User Interfaces (GUIs). In this API-free setting, large language model (LLM)-based agents face severe efficiency bottlenecks: limited to local visual experiences, they make myopic decisions and rely on inefficient trial-and-error, hindering both skill acquisition and long-term planning. To address these challenges, we propose KG-Agent, an experience-driven learning framework that structures an agent's raw pixel-level interactions into a persistent State-Action Knowledge Graph (SA-KG). KG-Agent overcomes inefficient exploration by linking functionally similar but visually distinct GUI states, forming a rich neighborhood of experience that enables the agent to generalize from a diverse set of historical strategies. To support long-horizon reasoning, we design a hybrid intrinsic reward mechanism based on the graph topology, combining a state value reward for exploiting known high-value pathways with a novelty reward that encourages targeted exploration. This approach decouples strategic planning from pure discovery, allowing the agent to effectively value setup actions with delayed gratification. We evaluate KG-Agent in two complex, open-ended GUI-based decision-making environments (Civilization V and Slay the Spire), demonstrating significant improvements in exploration efficiency and strategic depth over the state-of-the-art methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15259v2",
    "published_date": "2025-10-17 02:53:06 UTC",
    "updated_date": "2025-11-02 05:44:16 UTC"
  },
  {
    "arxiv_id": "2510.16063v1",
    "title": "Learning a Generalized Model for Substation Level Voltage Estimation in Distribution Networks",
    "authors": [
      "Muhy Eddin Za'ter",
      "Bri-Mathias Hodge"
    ],
    "abstract": "Accurate voltage estimation in distribution networks is critical for real-time monitoring and increasing the reliability of the grid. As DER penetration and distribution level voltage variability increase, robust distribution system state estimation (DSSE) has become more essential to maintain safe and efficient operations. Traditional DSSE techniques, however, struggle with sparse measurements and the scale of modern feeders, limiting their scalability to large networks. This paper presents a hierarchical graph neural network for substation-level voltage estimation that exploits both electrical topology and physical features, while remaining robust to the low observability levels common to real-world distribution networks. Leveraging the public SMART-DS datasets, the model is trained and evaluated on thousands of buses across multiple substations and DER penetration scenarios. Comprehensive experiments demonstrate that the proposed method achieves up to 2 times lower RMSE than alternative data-driven models, and maintains high accuracy with as little as 1\\% measurement coverage. The results highlight the potential of GNNs to enable scalable, reproducible, and data-driven voltage monitoring for distribution systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16063v1",
    "published_date": "2025-10-17 02:44:25 UTC",
    "updated_date": "2025-10-17 02:44:25 UTC"
  },
  {
    "arxiv_id": "2510.16062v2",
    "title": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs",
    "authors": [
      "Guiyao Tie",
      "Zenghui Yuan",
      "Zeli Zhao",
      "Chaoran Hu",
      "Tianhe Gu",
      "Ruihang Zhang",
      "Sizhe Zhang",
      "Junran Wu",
      "Xiaoyue Tu",
      "Ming Jin",
      "Qingsong Wen",
      "Lixing Chen",
      "Pan Zhou",
      "Lichao Sun"
    ],
    "abstract": "Self-correction of large language models (LLMs) emerges as a critical component for enhancing their reasoning performance. Although various self-correction methods have been proposed, a comprehensive evaluation of these methods remains largely unexplored, and the question of whether LLMs can truly correct themselves is a matter of significant interest and concern. In this study, we introduce CorrectBench, a benchmark developed to evaluate the effectiveness of self-correction strategies, including intrinsic, external, and fine-tuned approaches, across three tasks: commonsense reasoning, mathematical reasoning, and code generation. Our findings reveal that: 1) Self-correction methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing different self-correction strategies yields further improvements, though it reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited optimization under additional self-correction methods and have high time costs. Interestingly, a comparatively simple chain-of-thought (CoT) baseline demonstrates competitive accuracy and efficiency. These results underscore the potential of self-correction to enhance LLM's reasoning performance while highlighting the ongoing challenge of improving their efficiency. Consequently, we advocate for further research focused on optimizing the balance between reasoning capabilities and operational efficiency. Project Page: https://correctbench.github.io/",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "47 pages, 25 figures, 10 tables",
    "pdf_url": "https://arxiv.org/pdf/2510.16062v2",
    "published_date": "2025-10-17 02:40:19 UTC",
    "updated_date": "2025-10-22 09:04:12 UTC"
  },
  {
    "arxiv_id": "2510.15258v2",
    "title": "Multi-dimensional Data Analysis and Applications Basing on LLM Agents and Knowledge Graph Interactions",
    "authors": [
      "Xi Wang",
      "Xianyao Ling",
      "Kun Li",
      "Gang Yin",
      "Liang Zhang",
      "Jiang Wu",
      "Jun Xu",
      "Fu Zhang",
      "Wenbo Lei",
      "Annie Wang",
      "Peng Gong"
    ],
    "abstract": "In the current era of big data, extracting deep insights from massive, heterogeneous, and complexly associated multi-dimensional data has become a significant challenge. Large Language Models (LLMs) perform well in natural language understanding and generation, but still suffer from \"hallucination\" issues when processing structured knowledge and are difficult to update in real-time. Although Knowledge Graphs (KGs) can explicitly store structured knowledge, their static nature limits dynamic interaction and analytical capabilities. Therefore, this paper proposes a multi-dimensional data analysis method based on the interactions between LLM agents and KGs, constructing a dynamic, collaborative analytical ecosystem. This method utilizes LLM agents to automatically extract product data from unstructured data, constructs and visualizes the KG in real-time, and supports users in deep exploration and analysis of graph nodes through an interactive platform. Experimental results show that this method has significant advantages in product ecosystem analysis, relationship mining, and user-driven exploratory analysis, providing new ideas and tools for multi-dimensional data analysis.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 7 figures, 40 references",
    "pdf_url": "https://arxiv.org/pdf/2510.15258v2",
    "published_date": "2025-10-17 02:38:44 UTC",
    "updated_date": "2025-11-20 06:48:53 UTC"
  },
  {
    "arxiv_id": "2510.15244v2",
    "title": "Planner and Executor: Collaboration between Discrete Diffusion And Autoregressive Models in Reasoning",
    "authors": [
      "Lina Berrayana",
      "Ahmed Heakl",
      "Muhammad Abdullah Sohail",
      "Thomas Hofmann",
      "Salman Khan",
      "Wei Chen"
    ],
    "abstract": "Current autoregressive language models (ARMs) achieve high accuracy but require long token sequences, making them costly. Discrete diffusion language models (DDLMs) enable parallel and flexible generation within a fixed number of steps and have recently emerged for their strong performance in complex reasoning and long-term planning tasks. We present a study exploring hybrid architectures that couple DDLMs with ARMs to assess whether their collaboration can yield complementary benefits. We first examine collaboration in text space, where one model plans the reasoning process and another executes the final answer based on that plan. We then extend this setup to latent-space communication, introducing a learned projector that maps DDLM latents into the ARM's embedding space, potentially bypassing some of the text-generation limitations of diffusion models. We find that shifting DDLM --> ARM communication from text space to latent space yields significant accuracy gains, for example increasing from 27.0% to 54.0% on DART-5 and from 0.0% to 14.0% on AIME24. We also find that combining a DDLM planner with an ARM executor can provide substantial computational savings with little to no impact on accuracy. For example, the latent-space pipeline, using 64 tokens for planning and roughly 5 for execution, surpasses Qwen3.1-7B on DART-5 and AIME, despite Qwen using 44 times more tokens. Overall, our study offers new insights into reasoning with DDLMs and highlights their potential in hybrid architectures.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Under Submission",
    "pdf_url": "https://arxiv.org/pdf/2510.15244v2",
    "published_date": "2025-10-17 02:16:19 UTC",
    "updated_date": "2025-10-20 05:20:30 UTC"
  },
  {
    "arxiv_id": "2510.17873v2",
    "title": "Auditing and Mitigating Bias in Gender Classification Algorithms: A Data-Centric Approach",
    "authors": [
      "Tadesse K Bahiru",
      "Natnael Tilahun Sinshaw",
      "Teshager Hailemariam Moges",
      "Dheeraj Kumar Singh"
    ],
    "abstract": "Gender classification systems often inherit and amplify demographic imbalances in their training data. We first audit five widely used gender classification datasets, revealing that all suffer from significant intersectional underrepresentation. To measure the downstream impact of these flaws, we train identical MobileNetV2 classifiers on the two most balanced of these datasets, UTKFace and FairFace. Our fairness evaluation shows that even these models exhibit significant bias, misclassifying female faces at a higher rate than male faces and amplifying existing racial skew. To counter these data-induced biases, we construct BalancedFace, a new public dataset created by blending images from FairFace and UTKFace, supplemented with images from other collections to fill missing demographic gaps. It is engineered to equalize subgroup shares across 189 intersections of age, race, and gender using only real, unedited images. When a standard classifier is trained on BalancedFace, it reduces the maximum True Positive Rate gap across racial subgroups by over 50% and brings the average Disparate Impact score 63% closer to the ideal of 1.0 compared to the next-best dataset, all with a minimal loss of overall accuracy. These results underline the profound value of data-centric interventions and provide an openly available resource for fair gender classification research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "The manuscript contains a substantive error identified after submission",
    "pdf_url": "https://arxiv.org/pdf/2510.17873v2",
    "published_date": "2025-10-17 02:09:17 UTC",
    "updated_date": "2026-01-22 14:42:57 UTC"
  },
  {
    "arxiv_id": "2510.15236v1",
    "title": "From Checklists to Clusters: A Homeostatic Account of AGI Evaluation",
    "authors": [
      "Brett Reynolds"
    ],
    "abstract": "Contemporary AGI evaluations report multidomain capability profiles, yet they typically assign symmetric weights and rely on snapshot scores. This creates two problems: (i) equal weighting treats all domains as equally important when human intelligence research suggests otherwise, and (ii) snapshot testing can't distinguish durable capabilities from brittle performances that collapse under delay or stress. I argue that general intelligence -- in humans and potentially in machines -- is better understood as a homeostatic property cluster: a set of abilities plus the mechanisms that keep those abilities co-present under perturbation. On this view, AGI evaluation should weight domains by their causal centrality (their contribution to cluster stability) and require evidence of persistence across sessions. I propose two battery-compatible extensions: a centrality-prior score that imports CHC-derived weights with transparent sensitivity analysis, and a Cluster Stability Index family that separates profile persistence, durable learning, and error correction. These additions preserve multidomain breadth while reducing brittleness and gaming. I close with testable predictions and black-box protocols labs can adopt without architectural access.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "27 pages, 3 figures",
    "pdf_url": "https://arxiv.org/pdf/2510.15236v1",
    "published_date": "2025-10-17 01:59:40 UTC",
    "updated_date": "2025-10-17 01:59:40 UTC"
  },
  {
    "arxiv_id": "2510.15233v1",
    "title": "Adaptive Individual Uncertainty under Out-Of-Distribution Shift with Expert-Routed Conformal Prediction",
    "authors": [
      "Amitesh Badkul",
      "Lei Xie"
    ],
    "abstract": "Reliable, informative, and individual uncertainty quantification (UQ) remains missing in current ML community. This hinders the effective application of AI/ML to risk-sensitive domains. Most methods either fail to provide coverage on new data, inflate intervals so broadly that they are not actionable, or assign uncertainties that do not track actual error, especially under a distribution shift. In high-stakes drug discovery, protein-ligand affinity (PLI) prediction is especially challenging as assay noise is heterogeneous, chemical space is imbalanced and large, and practical evaluations routinely involve distribution shift. In this work, we introduce a novel uncertainty quantification method, Trustworthy Expert Split-conformal with Scaled Estimation for Efficient Reliable Adaptive intervals (TESSERA), that provides per-sample uncertainty with reliable coverage guarantee, informative and adaptive prediction interval widths that track the absolute error. We evaluate on protein-ligand binding affinity prediction under both independent and identically distributed (i.i.d.) and scaffold-based out-of-distribution (OOD) splits, comparing against strong UQ baselines. TESSERA attains near-nominal coverage and the best coverage-width trade-off as measured by the Coverage-Width Criterion (CWC), while maintaining competitive adaptivity (lowest Area Under the Sparsification Error (AUSE)). Size-Stratified Coverage (SSC) further confirms that intervals are right-sized, indicating width increases when data are scarce or noisy, and remain tight when predictions are reliable. By unifying Mixture of Expert (MoE) diversity with conformal calibration, TESSERA delivers trustworthy, tight, and adaptive uncertainties that are well-suited to selective prediction and downstream decision-making in the drug-discovery pipeline and other applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15233v1",
    "published_date": "2025-10-17 01:51:33 UTC",
    "updated_date": "2025-10-17 01:51:33 UTC"
  },
  {
    "arxiv_id": "2510.15231v2",
    "title": "Extending Audio Context for Long-Form Understanding in Large Audio-Language Models",
    "authors": [
      "Yuatyong Chaichana",
      "Pittawat Taveekitworachai",
      "Warit Sirichotedumrong",
      "Potsawee Manakul",
      "Kunat Pipatanakul"
    ],
    "abstract": "Large Audio-Language Models (LALMs) are often constrained by short audio context windows, even when their text backbones support long contexts, limiting long-form audio understanding. Prior work has introduced context-extension methods (e.g. YaRN) on unimodal LLMs, yet their application to LALMs remains unexplored. First, building on RoPE-based context extension, we introduce Partial YaRN, a training-free, modality-decoupled extension method that modifies only audio token positions, leaving text positions intact to preserve the base LLM's text capabilities. Second, we propose Virtual Longform Audio Training (VLAT), a training strategy that extends Partial YaRN into a training-time positional augmentation. VLAT simulates diverse audio lengths during training, enabling generalization to inputs far longer than those seen in training. Our experiments on SALMONN and Qwen2-Audio confirm that Partial YaRN outperforms the original models across wide range of settings, and VLAT provides substantial performance improvement on long audio of unseen lengths.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "EACL 2026. Code and dataset are available at: https://github.com/yophis/partial-yarn",
    "pdf_url": "https://arxiv.org/pdf/2510.15231v2",
    "published_date": "2025-10-17 01:44:28 UTC",
    "updated_date": "2026-01-21 08:00:24 UTC"
  },
  {
    "arxiv_id": "2510.16060v1",
    "title": "Beyond Accuracy: Are Time Series Foundation Models Well-Calibrated?",
    "authors": [
      "Coen Adler",
      "Yuxin Chang",
      "Felix Draxler",
      "Samar Abdi",
      "Padhraic Smyth"
    ],
    "abstract": "The recent development of foundation models for time series data has generated considerable interest in using such models across a variety of applications. Although foundation models achieve state-of-the-art predictive performance, their calibration properties remain relatively underexplored, despite the fact that calibration can be critical for many practical applications. In this paper, we investigate the calibration-related properties of five recent time series foundation models and two competitive baselines. We perform a series of systematic evaluations assessing model calibration (i.e., over- or under-confidence), effects of varying prediction heads, and calibration under long-term autoregressive forecasting. We find that time series foundation models are consistently better calibrated than baseline models and tend not to be either systematically over- or under-confident, in contrast to the overconfidence often seen in other deep learning models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.16060v1",
    "published_date": "2025-10-17 01:41:24 UTC",
    "updated_date": "2025-10-17 01:41:24 UTC"
  },
  {
    "arxiv_id": "2510.15221v1",
    "title": "WELD: A Large-Scale Longitudinal Dataset of Emotional Dynamics for Ubiquitous Affective Computing",
    "authors": [
      "Xiao Sun"
    ],
    "abstract": "Automated emotion recognition in real-world workplace settings remains a challenging problem in affective computing due to the scarcity of large-scale, longitudinal datasets collected in naturalistic environments. We present a novel dataset comprising 733,651 facial expression records from 38 employees collected over 30.5 months (November 2021 to May 2024) in an authentic office environment. Each record contains seven emotion probabilities (neutral, happy, sad, surprised, fear, disgusted, angry) derived from deep learning-based facial expression recognition, along with comprehensive metadata including job roles, employment outcomes, and personality traits. The dataset uniquely spans the COVID-19 pandemic period, capturing emotional responses to major societal events including the Shanghai lockdown and policy changes. We provide 32 extended emotional metrics computed using established affective science methods, including valence, arousal, volatility, predictability, inertia, and emotional contagion strength. Technical validation demonstrates high data quality through successful replication of known psychological patterns (weekend effect: +192% valence improvement, p < 0.001; diurnal rhythm validated) and perfect predictive validity for employee turnover (AUC=1.0). Baseline experiments using Random Forest and LSTM models achieve 91.2% accuracy for emotion classification and R2 = 0.84 for valence prediction. This is the largest and longest longitudinal workplace emotion dataset publicly available, enabling research in emotion recognition, affective dynamics modeling, emotional contagion, turnover prediction, and emotion-aware system design.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 4 figures, 1 table. Dataset publicly available under CC BY 4.0 license",
    "pdf_url": "https://arxiv.org/pdf/2510.15221v1",
    "published_date": "2025-10-17 00:59:43 UTC",
    "updated_date": "2025-10-17 00:59:43 UTC"
  },
  {
    "arxiv_id": "2510.15211v1",
    "title": "ReasonIF: Large Reasoning Models Fail to Follow Instructions During Reasoning",
    "authors": [
      "Yongchan Kwon",
      "Shang Zhu",
      "Federico Bianchi",
      "Kaitlyn Zhou",
      "James Zou"
    ],
    "abstract": "The ability of large language models (LLMs) to follow user instructions is central to their reliability, safety, and usefulness. While prior studies assess instruction adherence in the model's main responses, we argue that it is also critical for large reasoning models (LRMs) to follow user instructions throughout their reasoning process. Reasoning instruction following makes LRMs more controllable and transparent, while reducing risks of undesirable shortcuts, hallucinations, or reward hacking within reasoning traces. To evaluate this dimension, we introduce ReasonIF, a systematic benchmark for assessing reasoning instruction following. ReasonIF includes six categories of instruction prompts, spanning multilingual reasoning, formatting and length control. Across many open-source LRMs including GPT-OSS, Qwen3, and DeepSeek-R1, we find substantial failures in reasoning instruction adherence: the highest instruction following score (IFS) remains below 0.25, meaning that fewer than $25\\%$ of reasoning traces comply with the given instructions. Notably, as task difficulty increases, reasoning instruction following degrades further. We also explore two strategies to enhance reasoning instruction fidelity. (1) multi-turn reasoning and (2) Reasoning Instruction Finetuning (RIF) using synthetic data. RIF improves the IFS of $GPT-OSS-20B$ from 0.11 to 0.27, indicating measurable progress but leaving ample room for improvement.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15211v1",
    "published_date": "2025-10-17 00:38:28 UTC",
    "updated_date": "2025-10-17 00:38:28 UTC"
  },
  {
    "arxiv_id": "2510.15201v3",
    "title": "Automotive Crash Dynamics Modeling Accelerated with Machine Learning",
    "authors": [
      "Mohammad Amin Nabian",
      "Sudeep Chavare",
      "Deepak Akhare",
      "Rishikesh Ranade",
      "Ram Cherukuri",
      "Srinivas Tadepalli"
    ],
    "abstract": "Crashworthiness assessment is a critical aspect of automotive design, traditionally relying on high-fidelity finite element (FE) simulations that are computationally expensive and time-consuming. This work presents an exploratory comparative study on developing machine learning-based surrogate models for efficient prediction of structural deformation in crash scenarios using the NVIDIA PhysicsNeMo framework. Given the limited prior work applying machine learning to structural crash dynamics, the primary contribution lies in demonstrating the feasibility and engineering utility of the various modeling approaches explored in this work. We investigate two state-of-the-art neural network architectures for modeling crash dynamics: MeshGraphNet, and Transolver. Additionally, we examine three strategies for modeling transient dynamics: time-conditional, the standard Autoregressive approach, and a stability-enhanced Autoregressive scheme incorporating rollout-based training. The models are evaluated on a comprehensive Body-in-White (BIW) crash dataset comprising 150 detailed FE simulations using LS-DYNA. The dataset represents a structurally rich vehicle assembly with over 200 components, including 38 key components featuring variable thickness distributions to capture realistic manufacturing variability. Each model utilizes the undeformed mesh geometry and component characteristics as inputs to predict the spatiotemporal evolution of the deformed mesh during the crash sequence. Evaluation results show that the models capture the overall deformation trends with reasonable fidelity, demonstrating the feasibility of applying machine learning to structural crash dynamics. Although not yet matching full FE accuracy, the models achieve orders-of-magnitude reductions in computational cost, enabling rapid design exploration and early-stage optimization in crashworthiness evaluation.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.NA",
      "physics.app-ph",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15201v3",
    "published_date": "2025-10-17 00:03:33 UTC",
    "updated_date": "2025-11-03 18:19:07 UTC"
  },
  {
    "arxiv_id": "2510.15200v1",
    "title": "The Economics of AI Foundation Models: Openness, Competition, and Governance",
    "authors": [
      "Fasheng Xu",
      "Xiaoyu Wang",
      "Wei Chen",
      "Karen Xie"
    ],
    "abstract": "The strategic choice of model \"openness\" has become a defining issue for the foundation model (FM) ecosystem. While this choice is intensely debated, its underlying economic drivers remain underexplored. We construct a two-period game-theoretic model to analyze how openness shapes competition in an AI value chain, featuring an incumbent developer, a downstream deployer, and an entrant developer. Openness exerts a dual effect: it amplifies knowledge spillovers to the entrant, but it also enhances the incumbent's advantage through a \"data flywheel effect,\" whereby greater user engagement today further lowers the deployer's future fine-tuning cost. Our analysis reveals that the incumbent's optimal first-period openness is surprisingly non-monotonic in the strength of the data flywheel effect. When the data flywheel effect is either weak or very strong, the incumbent prefers a higher level of openness; however, for an intermediate range, it strategically restricts openness to impair the entrant's learning. This dynamic gives rise to an \"openness trap,\" a critical policy paradox where transparency mandates can backfire by removing firms' strategic flexibility, reducing investment, and lowering welfare. We extend the model to show that other common interventions can be similarly ineffective. Vertical integration, for instance, only benefits the ecosystem when the data flywheel effect is strong enough to overcome the loss of a potentially more efficient competitor. Likewise, government subsidies intended to spur adoption can be captured entirely by the incumbent through strategic price and openness adjustments, leaving the rest of the value chain worse off. By modeling the developer's strategic response to competitive and regulatory pressures, we provide a robust framework for analyzing competition and designing effective policy in the complex and rapidly evolving FM ecosystem.",
    "categories": [
      "econ.TH",
      "cs.AI"
    ],
    "primary_category": "econ.TH",
    "comment": "",
    "pdf_url": "https://arxiv.org/pdf/2510.15200v1",
    "published_date": "2025-10-17 00:00:44 UTC",
    "updated_date": "2025-10-17 00:00:44 UTC"
  }
]
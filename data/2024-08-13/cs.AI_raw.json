[
  {
    "arxiv_id": "2408.07238v1",
    "title": "Using Advanced LLMs to Enhance Smaller LLMs: An Interpretable Knowledge Distillation Approach",
    "authors": [
      "Tong Wang",
      "K. Sudhir",
      "Dat Hong"
    ],
    "abstract": "Advanced Large language models (LLMs) like GPT-4 or LlaMa 3 provide superior\nperformance in complex human-like interactions. But they are costly, or too\nlarge for edge devices such as smartphones and harder to self-host, leading to\nsecurity and privacy concerns. This paper introduces a novel interpretable\nknowledge distillation approach to enhance the performance of smaller, more\neconomical LLMs that firms can self-host. We study this problem in the context\nof building a customer service agent aimed at achieving high customer\nsatisfaction through goal-oriented dialogues. Unlike traditional knowledge\ndistillation, where the \"student\" model learns directly from the \"teacher\"\nmodel's responses via fine-tuning, our interpretable \"strategy\" teaching\napproach involves the teacher providing strategies to improve the student's\nperformance in various scenarios. This method alternates between a \"scenario\ngeneration\" step and a \"strategies for improvement\" step, creating a customized\nlibrary of scenarios and optimized strategies for automated prompting. The\nmethod requires only black-box access to both student and teacher models; hence\nit can be used without manipulating model parameters. In our customer service\napplication, the method improves performance, and the learned strategies are\ntransferable to other LLMs and scenarios beyond the training set. The method's\ninterpretabilty helps safeguard against potential harms through human audit.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.07238v1",
    "published_date": "2024-08-13 23:59:36 UTC",
    "updated_date": "2024-08-13 23:59:36 UTC"
  },
  {
    "arxiv_id": "2408.07234v1",
    "title": "Direction of Arrival Correction through Speech Quality Feedback",
    "authors": [
      "Caleb Rascon"
    ],
    "abstract": "Real-time speech enhancement has began to rise in performance, and the Demucs\nDenoiser model has recently demonstrated strong performance in\nmultiple-speech-source scenarios when accompanied by a location-based speech\ntarget selection strategy. However, it has shown to be sensitive to errors in\nthe direction-of-arrival (DOA) estimation. In this work, a DOA correction\nscheme is proposed that uses the real-time estimated speech quality of its\nenhanced output as the observed variable in an Adam-based optimization feedback\nloop to find the correct DOA. In spite of the high variability of the speech\nquality estimation, the proposed system is able to correct in real-time an\nerror of up to 15$^o$ using only the speech quality as its guide. Several\ninsights are provided for future versions of the proposed system to speed up\nconvergence and further reduce the speech quality estimation variability.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Submitted to Digital Signal Processing",
    "pdf_url": "http://arxiv.org/pdf/2408.07234v1",
    "published_date": "2024-08-13 23:43:20 UTC",
    "updated_date": "2024-08-13 23:43:20 UTC"
  },
  {
    "arxiv_id": "2408.07712v3",
    "title": "Introduction to Reinforcement Learning",
    "authors": [
      "Majid Ghasemi",
      "Dariush Ebrahimi"
    ],
    "abstract": "Reinforcement Learning (RL), a subfield of Artificial Intelligence (AI),\nfocuses on training agents to make decisions by interacting with their\nenvironment to maximize cumulative rewards. This paper provides an overview of\nRL, covering its core concepts, methodologies, and resources for further\nlearning. It offers a thorough explanation of fundamental components such as\nstates, actions, policies, and reward signals, ensuring readers develop a solid\nfoundational understanding. Additionally, the paper presents a variety of RL\nalgorithms, categorized based on the key factors such as model-free,\nmodel-based, value-based, policy-based, and other key factors. Resources for\nlearning and implementing RL, such as books, courses, and online communities\nare also provided. By offering a clear, structured introduction, this paper\naims to simplify the complexities of RL for beginners, providing a\nstraightforward pathway to understanding.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "19 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.07712v3",
    "published_date": "2024-08-13 23:08:06 UTC",
    "updated_date": "2024-12-03 16:17:32 UTC"
  },
  {
    "arxiv_id": "2408.07224v1",
    "title": "Play Me Something Icy: Practical Challenges, Explainability and the Semantic Gap in Generative AI Music",
    "authors": [
      "Jesse Allison",
      "Drew Farrar",
      "Treya Nash",
      "Carlos Román",
      "Morgan Weeks",
      "Fiona Xue Ju"
    ],
    "abstract": "This pictorial aims to critically consider the nature of text-to-audio and\ntext-to-music generative tools in the context of explainable AI. As a group of\nexperimental musicians and researchers, we are enthusiastic about the creative\npotential of these tools and have sought to understand and evaluate them from\nperspectives of prompt creation, control, usability, understandability,\nexplainability of the AI process, and overall aesthetic effectiveness of the\nresults. One of the challenges we have identified that is not explicitly\naddressed by these tools is the inherent semantic gap in using text-based tools\nto describe something as abstract as music. Other gaps include explainability\nvs. useability, and user control and input vs. the human creative process. The\naim of this pictorial is to raise questions for discussion and make a few\ngeneral suggestions on the kinds of improvements we would like to see in\ngenerative AI music tools.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "In Proceedings of Explainable AI for the Arts Workshop 2024 (XAIxArts\n  2024) arXiv:2406.14485",
    "pdf_url": "http://arxiv.org/pdf/2408.07224v1",
    "published_date": "2024-08-13 22:42:05 UTC",
    "updated_date": "2024-08-13 22:42:05 UTC"
  },
  {
    "arxiv_id": "2408.07215v2",
    "title": "Can Large Language Models Reason? A Characterization via 3-SAT",
    "authors": [
      "Rishi Hazra",
      "Gabriele Venturato",
      "Pedro Zuidberg Dos Martires",
      "Luc De Raedt"
    ],
    "abstract": "Large Language Models (LLMs) have been touted as AI models possessing\nadvanced reasoning abilities. However, recent works have shown that LLMs often\nbypass true reasoning using shortcuts, sparking skepticism. To study the\nreasoning capabilities in a principled fashion, we adopt a computational theory\nperspective and propose an experimental protocol centered on 3-SAT -- the\nprototypical NP-complete problem lying at the core of logical reasoning and\nconstraint satisfaction tasks. Specifically, we examine the phase transitions\nin random 3-SAT and characterize the reasoning abilities of LLMs by varying the\ninherent hardness of the problem instances. Our experimental evidence shows\nthat LLMs are incapable of performing true reasoning, as required for solving\n3-SAT problems. Moreover, we observe significant performance variation based on\nthe inherent hardness of the problems -- performing poorly on harder instances\nand vice versa. Importantly, we show that integrating external reasoners can\nconsiderably enhance LLM performance. By following a principled experimental\nprotocol, our study draws concrete conclusions and moves beyond the anecdotal\nevidence often found in LLM reasoning research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.07215v2",
    "published_date": "2024-08-13 21:54:10 UTC",
    "updated_date": "2024-10-22 21:44:03 UTC"
  },
  {
    "arxiv_id": "2408.07199v1",
    "title": "Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents",
    "authors": [
      "Pranav Putta",
      "Edmund Mills",
      "Naman Garg",
      "Sumeet Motwani",
      "Chelsea Finn",
      "Divyansh Garg",
      "Rafael Rafailov"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in natural\nlanguage tasks requiring complex reasoning, yet their application in agentic,\nmulti-step reasoning within interactive environments remains a difficult\nchallenge. Traditional supervised pre-training on static datasets falls short\nin enabling autonomous agent capabilities needed to perform complex\ndecision-making in dynamic settings like web navigation. Previous attempts to\nbridge this ga-through supervised fine-tuning on curated expert\ndemonstrations-often suffer from compounding errors and limited exploration\ndata, resulting in sub-optimal policy outcomes. To overcome these challenges,\nwe propose a framework that combines guided Monte Carlo Tree Search (MCTS)\nsearch with a self-critique mechanism and iterative fine-tuning on agent\ninteractions using an off-policy variant of the Direct Preference Optimization\n(DPO) algorithm. Our method allows LLM agents to learn effectively from both\nsuccessful and unsuccessful trajectories, thereby improving their\ngeneralization in complex, multi-step reasoning tasks. We validate our approach\nin the WebShop environment-a simulated e-commerce platform where it\nconsistently outperforms behavior cloning and reinforced fine-tuning baseline,\nand beats average human performance when equipped with the capability to do\nonline search. In real-world booking scenarios, our methodology boosts Llama-3\n70B model's zero-shot performance from 18.6% to 81.7% success rate (a 340%\nrelative increase) after a single day of data collection and further to 95.4%\nwith online search. We believe this represents a substantial leap forward in\nthe capabilities of autonomous agents, paving the way for more sophisticated\nand reliable decision-making in real-world settings.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.07199v1",
    "published_date": "2024-08-13 20:52:13 UTC",
    "updated_date": "2024-08-13 20:52:13 UTC"
  },
  {
    "arxiv_id": "2408.07194v1",
    "title": "Massive Dimensions Reduction and Hybridization with Meta-heuristics in Deep Learning",
    "authors": [
      "Rasa Khosrowshahli",
      "Shahryar Rahnamayan",
      "Beatrice Ombuki-Berman"
    ],
    "abstract": "Deep learning is mainly based on utilizing gradient-based optimization for\ntraining Deep Neural Network (DNN) models. Although robust and widely used,\ngradient-based optimization algorithms are prone to getting stuck in local\nminima. In this modern deep learning era, the state-of-the-art DNN models have\nmillions and billions of parameters, including weights and biases, making them\nhuge-scale optimization problems in terms of search space. Tuning a huge number\nof parameters is a challenging task that causes vanishing/exploding gradients\nand overfitting; likewise, utilized loss functions do not exactly represent our\ntargeted performance metrics. A practical solution to exploring large and\ncomplex solution space is meta-heuristic algorithms. Since DNNs exceed\nthousands and millions of parameters, even robust meta-heuristic algorithms,\nsuch as Differential Evolution, struggle to efficiently explore and converge in\nsuch huge-dimensional search spaces, leading to very slow convergence and high\nmemory demand. To tackle the mentioned curse of dimensionality, the concept of\nblocking was recently proposed as a technique that reduces the search space\ndimensions by grouping them into blocks. In this study, we aim to introduce\nHistogram-based Blocking Differential Evolution (HBDE), a novel approach that\nhybridizes gradient-based and gradient-free algorithms to optimize parameters.\nExperimental results demonstrated that the HBDE could reduce the parameters in\nthe ResNet-18 model from 11M to 3K during the training/optimizing phase by\nmetaheuristics, namely, the proposed HBDE, which outperforms baseline\ngradient-based and parent gradient-free DE algorithms evaluated on CIFAR-10 and\nCIFAR-100 datasets showcasing its effectiveness with reduced computational\ndemands for the very first time.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.NE",
    "comment": "8 pages, 5 figures, 3 tables, accepted at IEEE CCECE 2024 (updated\n  Fig. 1 and conclusion remarks)",
    "pdf_url": "http://arxiv.org/pdf/2408.07194v1",
    "published_date": "2024-08-13 20:28:20 UTC",
    "updated_date": "2024-08-13 20:28:20 UTC"
  },
  {
    "arxiv_id": "2408.07192v1",
    "title": "Solving Truly Massive Budgeted Monotonic POMDPs with Oracle-Guided Meta-Reinforcement Learning",
    "authors": [
      "Manav Vora",
      "Michael N Grussing",
      "Melkior Ornik"
    ],
    "abstract": "Monotonic Partially Observable Markov Decision Processes (POMDPs), where the\nsystem state progressively decreases until a restorative action is performed,\ncan be used to model sequential repair problems effectively. This paper\nconsiders the problem of solving budget-constrained multi-component monotonic\nPOMDPs, where a finite budget limits the maximal number of restorative actions.\nFor a large number of components, solving such a POMDP using current methods is\ncomputationally intractable due to the exponential growth in the state space\nwith an increasing number of components. To address this challenge, we propose\na two-step approach. Since the individual components of a budget-constrained\nmulti-component monotonic POMDP are only connected via the shared budget, we\nfirst approximate the optimal budget allocation among these components using an\napproximation of each component POMDP's optimal value function which is\nobtained through a random forest model. Subsequently, we introduce an\noracle-guided meta-trained Proximal Policy Optimization (PPO) algorithm to\nsolve each of the independent budget-constrained single-component monotonic\nPOMDPs. The oracle policy is obtained by performing value iteration on the\ncorresponding monotonic Markov Decision Process (MDP). This two-step method\nprovides scalability in solving truly massive multi-component monotonic POMDPs.\nTo demonstrate the efficacy of our approach, we consider a real-world\nmaintenance scenario that involves inspection and repair of an administrative\nbuilding by a team of agents within a maintenance budget. Finally, we perform a\ncomputational complexity analysis for a varying number of components to show\nthe scalability of the proposed approach.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.07192v1",
    "published_date": "2024-08-13 20:20:58 UTC",
    "updated_date": "2024-08-13 20:20:58 UTC"
  },
  {
    "arxiv_id": "2408.10259v1",
    "title": "Contrastive Learning on Medical Intents for Sequential Prescription Recommendation",
    "authors": [
      "Arya Hadizadeh Moghaddam",
      "Mohsen Nayebi Kerdabadi",
      "Mei Liu",
      "Zijun Yao"
    ],
    "abstract": "Recent advancements in sequential modeling applied to Electronic Health\nRecords (EHR) have greatly influenced prescription recommender systems. While\nthe recent literature on drug recommendation has shown promising performance,\nthe study of discovering a diversity of coexisting temporal relationships at\nthe level of medical codes over consecutive visits remains less explored. The\ngoal of this study can be motivated from two perspectives. First, there is a\nneed to develop a sophisticated sequential model capable of disentangling the\ncomplex relationships across sequential visits. Second, it is crucial to\nestablish multiple and diverse health profiles for the same patient to ensure a\ncomprehensive consideration of different medical intents in drug\nrecommendation. To achieve this goal, we introduce Attentive Recommendation\nwith Contrasted Intents (ARCI), a multi-level transformer-based method designed\nto capture the different but coexisting temporal paths across a shared sequence\nof visits. Specifically, we propose a novel intent-aware method with\ncontrastive learning, that links specialized medical intents of the patients to\nthe transformer heads for extracting distinct temporal paths associated with\ndifferent health profiles. We conducted experiments on two real-world datasets\nfor the prescription recommendation task using both ranking and classification\nmetrics. Our results demonstrate that ARCI has outperformed the\nstate-of-the-art prescription recommendation methods and is capable of\nproviding interpretable insights for healthcare practitioners.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the 33rd ACM International Conference on Information and\n  Knowledge Management (CIKM 2024)",
    "pdf_url": "http://arxiv.org/pdf/2408.10259v1",
    "published_date": "2024-08-13 20:10:28 UTC",
    "updated_date": "2024-08-13 20:10:28 UTC"
  },
  {
    "arxiv_id": "2408.07184v1",
    "title": "A New Dataset, Notation Software, and Representation for Computational Schenkerian Analysis",
    "authors": [
      "Stephen Ni-Hahn",
      "Weihan Xu",
      "Jerry Yin",
      "Rico Zhu",
      "Simon Mak",
      "Yue Jiang",
      "Cynthia Rudin"
    ],
    "abstract": "Schenkerian Analysis (SchA) is a uniquely expressive method of music\nanalysis, combining elements of melody, harmony, counterpoint, and form to\ndescribe the hierarchical structure supporting a work of music. However,\ndespite its powerful analytical utility and potential to improve music\nunderstanding and generation, SchA has rarely been utilized by the computer\nmusic community. This is in large part due to the paucity of available\nhigh-quality data in a computer-readable format. With a larger corpus of\nSchenkerian data, it may be possible to infuse machine learning models with a\ndeeper understanding of musical structure, thus leading to more \"human\"\nresults. To encourage further research in Schenkerian analysis and its\npotential benefits for music informatics and generation, this paper presents\nthree main contributions: 1) a new and growing dataset of SchAs, the largest in\nhuman- and computer-readable formats to date (>140 excerpts), 2) a novel\nsoftware for visualization and collection of SchA data, and 3) a novel,\nflexible representation of SchA as a heterogeneous-edge graph data structure.",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.07184v1",
    "published_date": "2024-08-13 19:52:06 UTC",
    "updated_date": "2024-08-13 19:52:06 UTC"
  },
  {
    "arxiv_id": "2408.07181v1",
    "title": "VulCatch: Enhancing Binary Vulnerability Detection through CodeT5 Decompilation and KAN Advanced Feature Extraction",
    "authors": [
      "Abdulrahman Hamman Adama Chukkol",
      "Senlin Luo",
      "Kashif Sharif",
      "Yunusa Haruna",
      "Muhammad Muhammad Abdullahi"
    ],
    "abstract": "Binary program vulnerability detection is critical for software security, yet\nexisting deep learning approaches often rely on source code analysis, limiting\ntheir ability to detect unknown vulnerabilities. To address this, we propose\nVulCatch, a binary-level vulnerability detection framework. VulCatch introduces\na Synergy Decompilation Module (SDM) and Kolmogorov-Arnold Networks (KAN) to\ntransform raw binary code into pseudocode using CodeT5, preserving high-level\nsemantics for deep analysis with tools like Ghidra and IDA. KAN further\nenhances feature transformation, enabling the detection of complex\nvulnerabilities. VulCatch employs word2vec, Inception Blocks, BiLSTM Attention,\nand Residual connections to achieve high detection accuracy (98.88%) and\nprecision (97.92%), while minimizing false positives (1.56%) and false\nnegatives (2.71%) across seven CVE datasets.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.07181v1",
    "published_date": "2024-08-13 19:46:50 UTC",
    "updated_date": "2024-08-13 19:46:50 UTC"
  },
  {
    "arxiv_id": "2408.07146v1",
    "title": "Vision Language Model for Interpretable and Fine-grained Detection of Safety Compliance in Diverse Workplaces",
    "authors": [
      "Zhiling Chen",
      "Hanning Chen",
      "Mohsen Imani",
      "Ruimin Chen",
      "Farhad Imani"
    ],
    "abstract": "Workplace accidents due to personal protective equipment (PPE) non-compliance\nraise serious safety concerns and lead to legal liabilities, financial\npenalties, and reputational damage. While object detection models have shown\nthe capability to address this issue by identifying safety items, most existing\nmodels, such as YOLO, Faster R-CNN, and SSD, are limited in verifying the\nfine-grained attributes of PPE across diverse workplace scenarios. Vision\nlanguage models (VLMs) are gaining traction for detection tasks by leveraging\nthe synergy between visual and textual information, offering a promising\nsolution to traditional object detection limitations in PPE recognition.\nNonetheless, VLMs face challenges in consistently verifying PPE attributes due\nto the complexity and variability of workplace environments, requiring them to\ninterpret context-specific language and visual cues simultaneously. We\nintroduce Clip2Safety, an interpretable detection framework for diverse\nworkplace safety compliance, which comprises four main modules: scene\nrecognition, the visual prompt, safety items detection, and fine-grained\nverification. The scene recognition identifies the current scenario to\ndetermine the necessary safety gear. The visual prompt formulates the specific\nvisual prompts needed for the detection process. The safety items detection\nidentifies whether the required safety gear is being worn according to the\nspecified scenario. Lastly, the fine-grained verification assesses whether the\nworn safety equipment meets the fine-grained attribute requirements. We conduct\nreal-world case studies across six different scenarios. The results show that\nClip2Safety not only demonstrates an accuracy improvement over state-of-the-art\nquestion-answering based VLMs but also achieves inference times two hundred\ntimes faster.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.07146v1",
    "published_date": "2024-08-13 18:32:06 UTC",
    "updated_date": "2024-08-13 18:32:06 UTC"
  },
  {
    "arxiv_id": "2408.07060v1",
    "title": "Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents",
    "authors": [
      "Kexun Zhang",
      "Weiran Yao",
      "Zuxin Liu",
      "Yihao Feng",
      "Zhiwei Liu",
      "Rithesh Murthy",
      "Tian Lan",
      "Lei Li",
      "Renze Lou",
      "Jiacheng Xu",
      "Bo Pang",
      "Yingbo Zhou",
      "Shelby Heinecke",
      "Silvio Savarese",
      "Huan Wang",
      "Caiming Xiong"
    ],
    "abstract": "Large language model (LLM) agents have shown great potential in solving\nreal-world software engineering (SWE) problems. The most advanced open-source\nSWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.\nHowever, these sophisticated agent frameworks exhibit varying strengths,\nexcelling in certain tasks while underperforming in others. To fully harness\nthe diversity of these agents, we propose DEI (Diversity Empowered\nIntelligence), a framework that leverages their unique expertise. DEI functions\nas a meta-module atop existing SWE agent frameworks, managing agent collectives\nfor enhanced problem-solving. Experimental results show that a DEI-guided\ncommittee of agents is able to surpass the best individual agent's performance\nby a large margin. For instance, a group of open-source SWE agents, with a\nmaximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3%\nresolve rate with DEI, making a 25% improvement and beating most closed-source\nsolutions. Our best-performing group excels with a 55% resolve rate, securing\nthe highest ranking on SWE-Bench Lite. Our findings contribute to the growing\nbody of research on collaborative AI systems and their potential to solve\ncomplex software engineering challenges.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.07060v1",
    "published_date": "2024-08-13 17:50:28 UTC",
    "updated_date": "2024-08-13 17:50:28 UTC"
  },
  {
    "arxiv_id": "2408.07059v1",
    "title": "Model Counting in the Wild",
    "authors": [
      "Arijit Shaw",
      "Kuldeep S. Meel"
    ],
    "abstract": "Model counting is a fundamental problem in automated reasoning with\napplications in probabilistic inference, network reliability, neural network\nverification, and more. Although model counting is computationally intractable\nfrom a theoretical perspective due to its #P-completeness, the past decade has\nseen significant progress in developing state-of-the-art model counters to\naddress scalability challenges.\n  In this work, we conduct a rigorous assessment of the scalability of model\ncounters in the wild. To this end, we surveyed 11 application domains and\ncollected an aggregate of 2262 benchmarks from these domains. We then evaluated\nsix state-of-the-art model counters on these instances to assess scalability\nand runtime performance.\n  Our empirical evaluation demonstrates that the performance of model counters\nvaries significantly across different application domains, underscoring the\nneed for careful selection by the end user. Additionally, we investigated the\nbehavior of different counters with respect to two parameters suggested by the\nmodel counting community, finding only a weak correlation. Our analysis\nhighlights the challenges and opportunities for portfolio-based approaches in\nmodel counting.",
    "categories": [
      "cs.LO",
      "cs.AI"
    ],
    "primary_category": "cs.LO",
    "comment": "Full version of conference paper accepted at KR 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.07059v1",
    "published_date": "2024-08-13 17:49:46 UTC",
    "updated_date": "2024-08-13 17:49:46 UTC"
  },
  {
    "arxiv_id": "2408.07057v1",
    "title": "A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning",
    "authors": [
      "Prateek Yadav",
      "Colin Raffel",
      "Mohammed Muqeeth",
      "Lucas Caccia",
      "Haokun Liu",
      "Tianlong Chen",
      "Mohit Bansal",
      "Leshem Choshen",
      "Alessandro Sordoni"
    ],
    "abstract": "The availability of performant pre-trained models has led to a proliferation\nof fine-tuned expert models that are specialized to a particular domain or\ntask. Model MoErging methods aim to recycle expert models to create an\naggregate system with improved performance or generalization. A key component\nof MoErging methods is the creation of a router that decides which expert\nmodel(s) to use for a particular input or application. The promise,\neffectiveness, and large design space of MoErging has spurred the development\nof many new methods over the past few years. This rapid pace of development has\nmade it challenging to compare different MoErging methods, which are rarely\ncompared to one another and are often validated in different experimental\nsetups. To remedy such gaps, we present a comprehensive survey of MoErging\nmethods that includes a novel taxonomy for cataloging key design choices and\nclarifying suitable applications for each method. Apart from surveying MoErging\nresearch, we inventory software tools and applications that make use of\nMoErging. We additionally discuss related fields of study such as model\nmerging, multitask learning, and mixture-of-experts models. Taken as a whole,\nour survey provides a unified overview of existing MoErging methods and creates\na solid foundation for future work in this burgeoning field.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "26 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.07057v1",
    "published_date": "2024-08-13 17:49:00 UTC",
    "updated_date": "2024-08-13 17:49:00 UTC"
  },
  {
    "arxiv_id": "2408.07052v2",
    "title": "The News Comment Gap and Algorithmic Agenda Setting in Online Forums",
    "authors": [
      "Flora Böwing",
      "Patrick Gildersleve"
    ],
    "abstract": "The disparity between news stories valued by journalists and those preferred\nby readers, known as the \"News Gap\", is well-documented. However, the\ndifference in expectations regarding news related user-generated content is\nless studied. Comment sections, hosted by news websites, are popular venues for\nreader engagement, yet still subject to editorial decisions. It is thus\nimportant to understand journalist vs reader comment preferences and how these\nare served by various comment ranking algorithms that represent discussions\ndifferently. We analyse 1.2 million comments from Austrian newspaper Der\nStandard to understand the \"News Comment Gap\" and the effects of different\nranking algorithms. We find that journalists prefer positive, timely, complex,\ndirect responses, while readers favour comments similar to article content from\nelite authors. We introduce the versatile Feature-Oriented Ranking Utility\nMetric (FORUM) to assess the impact of different ranking algorithms and find\ndramatic differences in how they prioritise the display of comments by\nsentiment, topical relevance, lexical diversity, and readability. Journalists\ncan exert substantial influence over the discourse through both curatorial and\nalgorithmic means. Understanding these choices' implications is vital in\nfostering engaging and civil discussions while aligning with journalistic\nobjectives, especially given the increasing legal scrutiny and societal\nimportance of online discourse.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.SI",
      "physics.soc-ph"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.07052v2",
    "published_date": "2024-08-13 17:43:32 UTC",
    "updated_date": "2024-08-23 09:29:34 UTC"
  },
  {
    "arxiv_id": "2408.07045v1",
    "title": "TableGuard -- Securing Structured & Unstructured Data",
    "authors": [
      "Anantha Sharma",
      "Ajinkya Deshmukh"
    ],
    "abstract": "With the increasing demand for data sharing across platforms and\norganizations, ensuring the privacy and security of sensitive information has\nbecome a critical challenge. This paper introduces \"TableGuard\". An innovative\napproach to data obfuscation tailored for relational databases. Building on the\nprinciples and techniques developed in prior work on context-sensitive\nobfuscation, TableGuard applies these methods to ensure that API calls return\nonly obfuscated data, thereby safeguarding privacy when sharing data with third\nparties. TableGuard leverages advanced context-sensitive obfuscation techniques\nto replace sensitive data elements with contextually appropriate alternatives.\nBy maintaining the relational integrity and coherence of the data, our approach\nmitigates the risks of cognitive dissonance and data leakage. We demonstrate\nthe implementation of TableGuard using a BERT based transformer model, which\nidentifies and obfuscates sensitive entities within relational tables. Our\nevaluation shows that TableGuard effectively balances privacy protection with\ndata utility, minimizing information loss while ensuring that the obfuscated\ndata remains functionally useful for downstream applications. The results\nhighlight the importance of domain-specific obfuscation strategies and the role\nof context length in preserving data integrity. The implications of this\nresearch are significant for organizations that need to share data securely\nwith external parties. TableGuard offers a robust framework for implementing\nprivacy-preserving data sharing mechanisms, thereby contributing to the broader\nfield of data privacy and security.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "7 pages, 3 tables, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2408.07045v1",
    "published_date": "2024-08-13 17:20:52 UTC",
    "updated_date": "2024-08-13 17:20:52 UTC"
  },
  {
    "arxiv_id": "2408.07040v1",
    "title": "KAN You See It? KANs and Sentinel for Effective and Explainable Crop Field Segmentation",
    "authors": [
      "Daniele Rege Cambrin",
      "Eleonora Poeta",
      "Eliana Pastor",
      "Tania Cerquitelli",
      "Elena Baralis",
      "Paolo Garza"
    ],
    "abstract": "Segmentation of crop fields is essential for enhancing agricultural\nproductivity, monitoring crop health, and promoting sustainable practices. Deep\nlearning models adopted for this task must ensure accurate and reliable\npredictions to avoid economic losses and environmental impact. The newly\nproposed Kolmogorov-Arnold networks (KANs) offer promising advancements in the\nperformance of neural networks. This paper analyzes the integration of KAN\nlayers into the U-Net architecture (U-KAN) to segment crop fields using\nSentinel-2 and Sentinel-1 satellite images and provides an analysis of the\nperformance and explainability of these networks. Our findings indicate a 2\\%\nimprovement in IoU compared to the traditional full-convolutional U-Net model\nin fewer GFLOPs. Furthermore, gradient-based explanation techniques show that\nU-KAN predictions are highly plausible and that the network has a very high\nability to focus on the boundaries of cultivated areas rather than on the areas\nthemselves. The per-channel relevance analysis also reveals that some channels\nare irrelevant to this task.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at ECCV 2024 CVPPA Workshop",
    "pdf_url": "http://arxiv.org/pdf/2408.07040v1",
    "published_date": "2024-08-13 17:07:29 UTC",
    "updated_date": "2024-08-13 17:07:29 UTC"
  },
  {
    "arxiv_id": "2408.07037v1",
    "title": "PathInsight: Instruction Tuning of Multimodal Datasets and Models for Intelligence Assisted Diagnosis in Histopathology",
    "authors": [
      "Xiaomin Wu",
      "Rui Xu",
      "Pengchen Wei",
      "Wenkang Qin",
      "Peixiang Huang",
      "Ziheng Li",
      "Lin Luo"
    ],
    "abstract": "Pathological diagnosis remains the definitive standard for identifying\ntumors. The rise of multimodal large models has simplified the process of\nintegrating image analysis with textual descriptions. Despite this advancement,\nthe substantial costs associated with training and deploying these complex\nmultimodal models, together with a scarcity of high-quality training datasets,\ncreate a significant divide between cutting-edge technology and its application\nin the clinical setting. We had meticulously compiled a dataset of\napproximately 45,000 cases, covering over 6 different tasks, including the\nclassification of organ tissues, generating pathology report descriptions, and\naddressing pathology-related questions and answers. We have fine-tuned\nmultimodal large models, specifically LLaVA, Qwen-VL, InternLM, with this\ndataset to enhance instruction-based performance. We conducted a qualitative\nassessment of the capabilities of the base model and the fine-tuned model in\nperforming image captioning and classification tasks on the specific dataset.\nThe evaluation results demonstrate that the fine-tuned model exhibits\nproficiency in addressing typical pathological questions. We hope that by\nmaking both our models and datasets publicly available, they can be valuable to\nthe medical and research communities.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "10 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.07037v1",
    "published_date": "2024-08-13 17:05:06 UTC",
    "updated_date": "2024-08-13 17:05:06 UTC"
  },
  {
    "arxiv_id": "2408.07016v1",
    "title": "Defining and Measuring Disentanglement for non-Independent Factors of Variation",
    "authors": [
      "Antonio Almudévar",
      "Alfonso Ortega",
      "Luis Vicente",
      "Antonio Miguel",
      "Eduardo Lleida"
    ],
    "abstract": "Representation learning is an approach that allows to discover and extract\nthe factors of variation from the data. Intuitively, a representation is said\nto be disentangled if it separates the different factors of variation in a way\nthat is understandable to humans. Definitions of disentanglement and metrics to\nmeasure it usually assume that the factors of variation are independent of each\nother. However, this is generally false in the real world, which limits the use\nof these definitions and metrics to very specific and unrealistic scenarios. In\nthis paper we give a definition of disentanglement based on information theory\nthat is also valid when the factors of variation are not independent.\nFurthermore, we relate this definition to the Information Bottleneck Method.\nFinally, we propose a method to measure the degree of disentanglement from the\ngiven definition that works when the factors of variation are not independent.\nWe show through different experiments that the method proposed in this paper\ncorrectly measures disentanglement with non-independent factors of variation,\nwhile other methods fail in this scenario.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.07016v1",
    "published_date": "2024-08-13 16:30:36 UTC",
    "updated_date": "2024-08-13 16:30:36 UTC"
  },
  {
    "arxiv_id": "2408.07004v1",
    "title": "Casper: Prompt Sanitization for Protecting User Privacy in Web-Based Large Language Models",
    "authors": [
      "Chun Jie Chong",
      "Chenxi Hou",
      "Zhihao Yao",
      "Seyed Mohammadjavad Seyed Talebi"
    ],
    "abstract": "Web-based Large Language Model (LLM) services have been widely adopted and\nhave become an integral part of our Internet experience. Third-party plugins\nenhance the functionalities of LLM by enabling access to real-world data and\nservices. However, the privacy consequences associated with these services and\ntheir third-party plugins are not well understood. Sensitive prompt data are\nstored, processed, and shared by cloud-based LLM providers and third-party\nplugins. In this paper, we propose Casper, a prompt sanitization technique that\naims to protect user privacy by detecting and removing sensitive information\nfrom user inputs before sending them to LLM services. Casper runs entirely on\nthe user's device as a browser extension and does not require any changes to\nthe online LLM services. At the core of Casper is a three-layered sanitization\nmechanism consisting of a rule-based filter, a Machine Learning (ML)-based\nnamed entity recognizer, and a browser-based local LLM topic identifier. We\nevaluate Casper on a dataset of 4000 synthesized prompts and show that it can\neffectively filter out Personal Identifiable Information (PII) and\nprivacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.07004v1",
    "published_date": "2024-08-13 16:08:37 UTC",
    "updated_date": "2024-08-13 16:08:37 UTC"
  },
  {
    "arxiv_id": "2408.07003v1",
    "title": "Generative AI for automatic topic labelling",
    "authors": [
      "Diego Kozlowski",
      "Carolina Pradier",
      "Pierre Benz"
    ],
    "abstract": "Topic Modeling has become a prominent tool for the study of scientific\nfields, as they allow for a large scale interpretation of research trends.\nNevertheless, the output of these models is structured as a list of keywords\nwhich requires a manual interpretation for the labelling. This paper proposes\nto assess the reliability of three LLMs, namely flan, GPT-4o, and GPT-4 mini\nfor topic labelling. Drawing on previous research leveraging BERTopic, we\ngenerate topics from a dataset of all the scientific articles (n=34,797)\nauthored by all biology professors in Switzerland (n=465) between 2008 and\n2020, as recorded in the Web of Science database. We assess the output of the\nthree models both quantitatively and qualitatively and find that, first, both\nGPT models are capable of accurately and precisely label topics from the\nmodels' output keywords. Second, 3-word labels are preferable to grasp the\ncomplexity of research topics.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2408.07003v1",
    "published_date": "2024-08-13 16:07:16 UTC",
    "updated_date": "2024-08-13 16:07:16 UTC"
  },
  {
    "arxiv_id": "2408.07113v1",
    "title": "A Theory-Based Explainable Deep Learning Architecture for Music Emotion",
    "authors": [
      "Hortense Fong",
      "Vineet Kumar",
      "K. Sudhir"
    ],
    "abstract": "This paper paper develops a theory-based, explainable deep learning\nconvolutional neural network (CNN) classifier to predict the time-varying\nemotional response to music. We design novel CNN filters that leverage the\nfrequency harmonics structure from acoustic physics known to impact the\nperception of musical features. Our theory-based model is more parsimonious,\nbut provides comparable predictive performance to atheoretical deep learning\nmodels, while performing better than models using handcrafted features. Our\nmodel can be complemented with handcrafted features, but the performance\nimprovement is marginal. Importantly, the harmonics-based structure placed on\nthe CNN filters provides better explainability for how the model predicts\nemotional response (valence and arousal), because emotion is closely related to\nconsonance--a perceptual feature defined by the alignment of harmonics.\nFinally, we illustrate the utility of our model with an application involving\ndigital advertising. Motivated by YouTube mid-roll ads, we conduct a lab\nexperiment in which we exogenously insert ads at different times within videos.\nWe find that ads placed in emotionally similar contexts increase ad engagement\n(lower skip rates, higher brand recall rates). Ad insertion based on emotional\nsimilarity metrics predicted by our theory-based, explainable model produces\ncomparable or better engagement relative to atheoretical models.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.HC",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.07113v1",
    "published_date": "2024-08-13 16:01:27 UTC",
    "updated_date": "2024-08-13 16:01:27 UTC"
  },
  {
    "arxiv_id": "2408.06993v1",
    "title": "LLMs can Schedule",
    "authors": [
      "Henrik Abgaryan",
      "Ararat Harutyunyan",
      "Tristan Cazenave"
    ],
    "abstract": "The job shop scheduling problem (JSSP) remains a significant hurdle in\noptimizing production processes. This challenge involves efficiently allocating\njobs to a limited number of machines while minimizing factors like total\nprocessing time or job delays. While recent advancements in artificial\nintelligence have yielded promising solutions, such as reinforcement learning\nand graph neural networks, this paper explores the potential of Large Language\nModels (LLMs) for JSSP. We introduce the very first supervised 120k dataset\nspecifically designed to train LLMs for JSSP. Surprisingly, our findings\ndemonstrate that LLM-based scheduling can achieve performance comparable to\nother neural approaches. Furthermore, we propose a sampling method that\nenhances the effectiveness of LLMs in tackling JSSP.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06993v1",
    "published_date": "2024-08-13 15:53:58 UTC",
    "updated_date": "2024-08-13 15:53:58 UTC"
  },
  {
    "arxiv_id": "2408.06975v1",
    "title": "SpectralGaussians: Semantic, spectral 3D Gaussian splatting for multi-spectral scene representation, visualization and analysis",
    "authors": [
      "Saptarshi Neil Sinha",
      "Holger Graf",
      "Michael Weinmann"
    ],
    "abstract": "We propose a novel cross-spectral rendering framework based on 3D Gaussian\nSplatting (3DGS) that generates realistic and semantically meaningful splats\nfrom registered multi-view spectrum and segmentation maps. This extension\nenhances the representation of scenes with multiple spectra, providing insights\ninto the underlying materials and segmentation. We introduce an improved\nphysically-based rendering approach for Gaussian splats, estimating reflectance\nand lights per spectra, thereby enhancing accuracy and realism. In a\ncomprehensive quantitative and qualitative evaluation, we demonstrate the\nsuperior performance of our approach with respect to other recent\nlearning-based spectral scene representation approaches (i.e., XNeRF and\nSpectralNeRF) as well as other non-spectral state-of-the-art learning-based\napproaches. Our work also demonstrates the potential of spectral scene\nunderstanding for precise scene editing techniques like style transfer,\ninpainting, and removal. Thereby, our contributions address challenges in\nmulti-spectral scene representation, rendering, and editing, offering new\npossibilities for diverse applications.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "I.2.10; I.3.7; I.4.8; I.4.1"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06975v1",
    "published_date": "2024-08-13 15:32:54 UTC",
    "updated_date": "2024-08-13 15:32:54 UTC"
  },
  {
    "arxiv_id": "2408.06954v2",
    "title": "Neural Speech and Audio Coding: Modern AI Technology Meets Traditional Codecs",
    "authors": [
      "Minje Kim",
      "Jan Skoglund"
    ],
    "abstract": "This paper explores the integration of model-based and data-driven approaches\nwithin the realm of neural speech and audio coding systems. It highlights the\nchallenges posed by the subjective evaluation processes of speech and audio\ncodecs and discusses the limitations of purely data-driven approaches, which\noften require inefficiently large architectures to match the performance of\nmodel-based methods. The study presents hybrid systems as a viable solution,\noffering significant improvements to the performance of conventional codecs\nthrough meticulously chosen design enhancements. Specifically, it introduces a\nneural network-based signal enhancer designed to post-process existing codecs'\noutput, along with the autoencoder-based end-to-end models and LPCNet--hybrid\nsystems that combine linear predictive coding (LPC) with neural networks.\nFurthermore, the paper delves into predictive models operating within custom\nfeature spaces (TF-Codec) or predefined transform domains (MDCTNet) and\nexamines the use of psychoacoustically calibrated loss functions to train\nend-to-end neural audio codecs. Through these investigations, the paper\ndemonstrates the potential of hybrid systems to advance the field of speech and\naudio coding by bridging the gap between traditional model-based approaches and\nmodern data-driven techniques.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS",
      "eess.SP"
    ],
    "primary_category": "cs.SD",
    "comment": "Published in IEEE Signal Processing Magazine",
    "pdf_url": "http://arxiv.org/pdf/2408.06954v2",
    "published_date": "2024-08-13 15:13:21 UTC",
    "updated_date": "2025-01-07 04:11:55 UTC"
  },
  {
    "arxiv_id": "2408.06945v2",
    "title": "Heavy-Ball Momentum Accelerated Actor-Critic With Function Approximation",
    "authors": [
      "Yanjie Dong",
      "Haijun Zhang",
      "Gang Wang",
      "Shisheng Cui",
      "Xiping Hu"
    ],
    "abstract": "By using an parametric value function to replace the Monte-Carlo rollouts for\nvalue estimation, the actor-critic (AC) algorithms can reduce the variance of\nstochastic policy gradient so that to improve the convergence rate. While\nexisting works mainly focus on analyzing convergence rate of AC algorithms\nunder Markovian noise, the impacts of momentum on AC algorithms remain largely\nunexplored. In this work, we first propose a heavy-ball momentum based\nadvantage actor-critic (\\mbox{HB-A2C}) algorithm by integrating the heavy-ball\nmomentum into the critic recursion that is parameterized by a linear function.\nWhen the sample trajectory follows a Markov decision process, we quantitatively\ncertify the acceleration capability of the proposed HB-A2C algorithm. Our\ntheoretical results demonstrate that the proposed HB-A2C finds an\n$\\epsilon$-approximate stationary point with $\\oo{\\epsilon^{-2}}$ iterations\nfor reinforcement learning tasks with Markovian noise. Moreover, we also reveal\nthe dependence of learning rates on the length of the sample trajectory. By\ncarefully selecting the momentum factor of the critic recursion, the proposed\nHB-A2C can balance the errors introduced by the initialization and the\nstoschastic approximation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06945v2",
    "published_date": "2024-08-13 15:03:46 UTC",
    "updated_date": "2024-08-16 15:09:09 UTC"
  },
  {
    "arxiv_id": "2408.08910v1",
    "title": "Why Do Experts Favor Solar and Wind as Renewable Energies Despite their Intermittency?",
    "authors": [
      "Steven P. Reinhardt"
    ],
    "abstract": "As humanity accelerates its shift to renewable energy generation, people who\nare not experts in renewable energy are learning about energy technologies and\nthe energy market, which are complex. The answers to some questions will be\nobvious to expert practitioners but not to non-experts. One such question is\nWhy solar and wind generation are expected to supply the bulk of future energy\nwhen they are intermittent. We learn here that once the baseline hurdles of\nscalability to utility scale and the underlying resources being widely\navailable globally are satisfied, the forecasted cost of solar and wind is 2-4X\nlower than competing technologies, even those that are not as scalable and\navailable. The market views intermittency as surmountable.",
    "categories": [
      "physics.soc-ph",
      "cs.AI"
    ],
    "primary_category": "physics.soc-ph",
    "comment": "Shifted references from hyperlinks to academic style",
    "pdf_url": "http://arxiv.org/pdf/2408.08910v1",
    "published_date": "2024-08-13 14:56:23 UTC",
    "updated_date": "2024-08-13 14:56:23 UTC"
  },
  {
    "arxiv_id": "2408.06931v2",
    "title": "The advantages of context specific language models: the case of the Erasmian Language Model",
    "authors": [
      "João Gonçalves",
      "Nick Jelicic",
      "Michele Murgia",
      "Evert Stamhuis"
    ],
    "abstract": "The current trend to improve language model performance seems to be based on\nscaling up with the number of parameters (e.g. the state of the art GPT4 model\nhas approximately 1.7 trillion parameters) or the amount of training data fed\ninto the model. However this comes at significant costs in terms of\ncomputational resources and energy costs that compromise the sustainability of\nAI solutions, as well as risk relating to privacy and misuse. In this paper we\npresent the Erasmian Language Model (ELM) a small context specific, 900 million\nparameter model, pre-trained and fine-tuned by and for Erasmus University\nRotterdam. We show how the model performs adequately in a classroom context for\nessay writing, and how it achieves superior performance in subjects that are\npart of its context. This has implications for a wide range of institutions and\norganizations, showing that context specific language models may be a viable\nalternative for resource constrained, privacy sensitive use cases.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "12 pages, 3 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2408.06931v2",
    "published_date": "2024-08-13 14:34:59 UTC",
    "updated_date": "2025-04-23 11:31:40 UTC"
  },
  {
    "arxiv_id": "2408.06930v2",
    "title": "Diagnosis extraction from unstructured Dutch echocardiogram reports using span- and document-level characteristic classification",
    "authors": [
      "Bauke Arends",
      "Melle Vessies",
      "Dirk van Osch",
      "Arco Teske",
      "Pim van der Harst",
      "René van Es",
      "Bram van Es"
    ],
    "abstract": "Clinical machine learning research and AI driven clinical decision support\nmodels rely on clinically accurate labels. Manually extracting these labels\nwith the help of clinical specialists is often time-consuming and expensive.\nThis study tests the feasibility of automatic span- and document-level\ndiagnosis extraction from unstructured Dutch echocardiogram reports. We\nincluded 115,692 unstructured echocardiogram reports from the UMCU a large\nuniversity hospital in the Netherlands. A randomly selected subset was manually\nannotated for the occurrence and severity of eleven commonly described cardiac\ncharacteristics. We developed and tested several automatic labelling techniques\nat both span and document levels, using weighted and macro F1-score, precision,\nand recall for performance evaluation. We compared the performance of span\nlabelling against document labelling methods, which included both direct\ndocument classifiers and indirect document classifiers that rely on span\nclassification results. The SpanCategorizer and MedRoBERTa$.$nl models\noutperformed all other span and document classifiers, respectively. The\nweighted F1-score varied between characteristics, ranging from 0.60 to 0.93 in\nSpanCategorizer and 0.96 to 0.98 in MedRoBERTa$.$nl. Direct document\nclassification was superior to indirect document classification using span\nclassifiers. SetFit achieved competitive document classification performance\nusing only 10% of the training data. Utilizing a reduced label set yielded\nnear-perfect document classification results. We recommend using our published\nSpanCategorizer and MedRoBERTa$.$nl models for span- and document-level\ndiagnosis extraction from Dutch echocardiography reports. For settings with\nlimited training data, SetFit may be a promising alternative for document\nclassification.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50, 68P20",
      "I.2.7; J.3; H.3.3"
    ],
    "primary_category": "cs.CL",
    "comment": "28 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.06930v2",
    "published_date": "2024-08-13 14:33:32 UTC",
    "updated_date": "2024-08-15 12:42:12 UTC"
  },
  {
    "arxiv_id": "2408.06922v1",
    "title": "Temporal Variability and Multi-Viewed Self-Supervised Representations to Tackle the ASVspoof5 Deepfake Challenge",
    "authors": [
      "Yuankun Xie",
      "Xiaopeng Wang",
      "Zhiyong Wang",
      "Ruibo Fu",
      "Zhengqi Wen",
      "Haonan Cheng",
      "Long Ye"
    ],
    "abstract": "ASVspoof5, the fifth edition of the ASVspoof series, is one of the largest\nglobal audio security challenges. It aims to advance the development of\ncountermeasure (CM) to discriminate bonafide and spoofed speech utterances. In\nthis paper, we focus on addressing the problem of open-domain audio deepfake\ndetection, which corresponds directly to the ASVspoof5 Track1 open condition.\nAt first, we comprehensively investigate various CM on ASVspoof5, including\ndata expansion, data augmentation, and self-supervised learning (SSL) features.\nDue to the high-frequency gaps characteristic of the ASVspoof5 dataset, we\nintroduce Frequency Mask, a data augmentation method that masks specific\nfrequency bands to improve CM robustness. Combining various scale of temporal\ninformation with multiple SSL features, our experiments achieved a minDCF of\n0.0158 and an EER of 0.55% on the ASVspoof 5 Track 1 evaluation progress set.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06922v1",
    "published_date": "2024-08-13 14:15:15 UTC",
    "updated_date": "2024-08-13 14:15:15 UTC"
  },
  {
    "arxiv_id": "2408.06920v1",
    "title": "Multi-Agent Continuous Control with Generative Flow Networks",
    "authors": [
      "Shuang Luo",
      "Yinchuan Li",
      "Shunyu Liu",
      "Xu Zhang",
      "Yunfeng Shao",
      "Chao Wu"
    ],
    "abstract": "Generative Flow Networks (GFlowNets) aim to generate diverse trajectories\nfrom a distribution in which the final states of the trajectories are\nproportional to the reward, serving as a powerful alternative to reinforcement\nlearning for exploratory control tasks. However, the individual-flow matching\nconstraint in GFlowNets limits their applications for multi-agent systems,\nespecially continuous joint-control problems. In this paper, we propose a novel\nMulti-Agent generative Continuous Flow Networks (MACFN) method to enable\nmultiple agents to perform cooperative exploration for various compositional\ncontinuous objects. Technically, MACFN trains decentralized\nindividual-flow-based policies in a centralized global-flow-based matching\nfashion. During centralized training, MACFN introduces a continuous flow\ndecomposition network to deduce the flow contributions of each agent in the\npresence of only global rewards. Then agents can deliver actions solely based\non their assigned local flow in a decentralized way, forming a joint policy\ndistribution proportional to the rewards. To guarantee the expressiveness of\ncontinuous flow decomposition, we theoretically derive a consistency condition\non the decomposition network. Experimental results demonstrate that the\nproposed method yields results superior to the state-of-the-art counterparts\nand better exploration capability. Our code is available at\nhttps://github.com/isluoshuang/MACFN.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06920v1",
    "published_date": "2024-08-13 14:12:03 UTC",
    "updated_date": "2024-08-13 14:12:03 UTC"
  },
  {
    "arxiv_id": "2408.06911v1",
    "title": "Heterogeneous Space Fusion and Dual-Dimension Attention: A New Paradigm for Speech Enhancement",
    "authors": [
      "Tao Zheng",
      "Liejun Wang",
      "Yinfeng Yu"
    ],
    "abstract": "Self-supervised learning has demonstrated impressive performance in speech\ntasks, yet there remains ample opportunity for advancement in the realm of\nspeech enhancement research. In addressing speech tasks, confining the\nattention mechanism solely to the temporal dimension poses limitations in\neffectively focusing on critical speech features. Considering the\naforementioned issues, our study introduces a novel speech enhancement\nframework, HFSDA, which skillfully integrates heterogeneous spatial features\nand incorporates a dual-dimension attention mechanism to significantly enhance\nspeech clarity and quality in noisy environments. By leveraging self-supervised\nlearning embeddings in tandem with Short-Time Fourier Transform (STFT)\nspectrogram features, our model excels at capturing both high-level semantic\ninformation and detailed spectral data, enabling a more thorough analysis and\nrefinement of speech signals. Furthermore, we employ the innovative\nOmni-dimensional Dynamic Convolution (ODConv) technology within the spectrogram\ninput branch, enabling enhanced extraction and integration of crucial\ninformation across multiple dimensions. Additionally, we refine the Conformer\nmodel by enhancing its feature extraction capabilities not only in the temporal\ndimension but also across the spectral domain. Extensive experiments on the\nVCTK-DEMAND dataset show that HFSDA is comparable to existing state-of-the-art\nmodels, confirming the validity of our approach.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted for publication by IEEE International Conference on Systems,\n  Man, and Cybernetics 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.06911v1",
    "published_date": "2024-08-13 14:04:24 UTC",
    "updated_date": "2024-08-13 14:04:24 UTC"
  },
  {
    "arxiv_id": "2408.16003v1",
    "title": "Meta-Learning for Federated Face Recognition in Imbalanced Data Regimes",
    "authors": [
      "Arwin Gansekoele",
      "Emiel Hess",
      "Sandjai Bhulai"
    ],
    "abstract": "The growing privacy concerns surrounding face image data demand new\ntechniques that can guarantee user privacy. One such face recognition technique\nthat claims to achieve better user privacy is Federated Face Recognition (FRR),\na subfield of Federated Learning (FL). However, FFR faces challenges due to the\nheterogeneity of the data, given the large number of classes that need to be\nhandled. To overcome this problem, solutions are sought in the field of\npersonalized FL. This work introduces three new data partitions based on the\nCelebA dataset, each with a different form of data heterogeneity. It also\nproposes Hessian-Free Model Agnostic Meta-Learning (HF-MAML) in an FFR setting.\nWe show that HF-MAML scores higher in verification tests than current FFR\nmodels on three different CelebA data partitions. In particular, the\nverification scores improve the most in heterogeneous data partitions. To\nbalance personalization with the development of an effective global model, an\nembedding regularization term is introduced for the loss function. This term\ncan be combined with HF-MAML and is shown to increase global model verification\nperformance. Lastly, this work performs a fairness analysis, showing that\nHF-MAML and its embedding regularization extension can improve fairness by\nreducing the standard deviation over the client evaluation scores.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CV",
    "comment": "To appear in the IEEE FLTA 2024 proceedings",
    "pdf_url": "http://arxiv.org/pdf/2408.16003v1",
    "published_date": "2024-08-13 14:03:10 UTC",
    "updated_date": "2024-08-13 14:03:10 UTC"
  },
  {
    "arxiv_id": "2408.06906v1",
    "title": "VNet: A GAN-based Multi-Tier Discriminator Network for Speech Synthesis Vocoders",
    "authors": [
      "Yubing Cao",
      "Yongming Li",
      "Liejun Wang",
      "Yinfeng Yu"
    ],
    "abstract": "Since the introduction of Generative Adversarial Networks (GANs) in speech\nsynthesis, remarkable achievements have been attained. In a thorough\nexploration of vocoders, it has been discovered that audio waveforms can be\ngenerated at speeds exceeding real-time while maintaining high fidelity,\nachieved through the utilization of GAN-based models. Typically, the inputs to\nthe vocoder consist of band-limited spectral information, which inevitably\nsacrifices high-frequency details. To address this, we adopt the full-band Mel\nspectrogram information as input, aiming to provide the vocoder with the most\ncomprehensive information possible. However, previous studies have revealed\nthat the use of full-band spectral information as input can result in the issue\nof over-smoothing, compromising the naturalness of the synthesized speech. To\ntackle this challenge, we propose VNet, a GAN-based neural vocoder network that\nincorporates full-band spectral information and introduces a Multi-Tier\nDiscriminator (MTD) comprising multiple sub-discriminators to generate\nhigh-resolution signals. Additionally, we introduce an asymptotically\nconstrained method that modifies the adversarial loss of the generator and\ndiscriminator, enhancing the stability of the training process. Through\nrigorous experiments, we demonstrate that the VNet model is capable of\ngenerating high-fidelity speech and significantly improving the performance of\nthe vocoder.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted for publication by IEEE International Conference on Systems,\n  Man, and Cybernetics 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.06906v1",
    "published_date": "2024-08-13 14:00:02 UTC",
    "updated_date": "2024-08-13 14:00:02 UTC"
  },
  {
    "arxiv_id": "2408.06900v1",
    "title": "Entendre, a Social Bot Detection Tool for Niche, Fringe, and Extreme Social Media",
    "authors": [
      "Pranav Venkatesh",
      "Kami Vinton",
      "Dhiraj Murthy",
      "Kellen Sharp",
      "Akaash Kolluri"
    ],
    "abstract": "Social bots-automated accounts that generate and spread content on social\nmedia-are exploiting vulnerabilities in these platforms to manipulate public\nperception and disseminate disinformation. This has prompted the development of\npublic bot detection services; however, most of these services focus primarily\non Twitter, leaving niche platforms vulnerable. Fringe social media platforms\nsuch as Parler, Gab, and Gettr often have minimal moderation, which facilitates\nthe spread of hate speech and misinformation. To address this gap, we introduce\nEntendre, an open-access, scalable, and platform-agnostic bot detection\nframework. Entendre can process a labeled dataset from any social platform to\nproduce a tailored bot detection model using a random forest classification\napproach, ensuring robust social bot detection. We exploit the idea that most\nsocial platforms share a generic template, where users can post content,\napprove content, and provide a bio (common data features). By emphasizing\ngeneral data features over platform-specific ones, Entendre offers rapid\nextensibility at the expense of some accuracy. To demonstrate Entendre's\neffectiveness, we used it to explore the presence of bots among accounts\nposting racist content on the now-defunct right-wing platform Parler. We\nexamined 233,000 posts from 38,379 unique users and found that 1,916 unique\nusers (4.99%) exhibited bot-like behavior. Visualization techniques further\nrevealed that these bots significantly impacted the network, amplifying\ninfluential rhetoric and hashtags (e.g., #qanon, #trump, #antilgbt). These\npreliminary findings underscore the need for tools like Entendre to monitor and\nassess bot activity across diverse platforms.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.SI",
      "J.4; I.2; I.7; K.4"
    ],
    "primary_category": "cs.CY",
    "comment": "6 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.06900v1",
    "published_date": "2024-08-13 13:50:49 UTC",
    "updated_date": "2024-08-13 13:50:49 UTC"
  },
  {
    "arxiv_id": "2408.06891v2",
    "title": "Automatic Feature Recognition and Dimensional Attributes Extraction From CAD Models for Hybrid Additive-Subtractive Manufacturing",
    "authors": [
      "Muhammad Tayyab Khan",
      "Wenhe Feng",
      "Lequn Chen",
      "Ye Han Ng",
      "Nicholas Yew Jin Tan",
      "Seung Ki Moon"
    ],
    "abstract": "The integration of Computer-Aided Design (CAD), Computer-Aided Process\nPlanning (CAPP), and Computer-Aided Manufacturing (CAM) plays a crucial role in\nmodern manufacturing, facilitating seamless transitions from digital designs to\nphysical products. However, a significant challenge within this integration is\nthe Automatic Feature Recognition (AFR) of CAD models, especially in the\ncontext of hybrid manufacturing that combines subtractive and additive\nmanufacturing processes. Traditional AFR methods, focused mainly on the\nidentification of subtractive (machined) features including holes, fillets,\nchamfers, pockets, and slots, fail to recognize features pertinent to additive\nmanufacturing. Furthermore, the traditional methods fall short in accurately\nextracting geometric dimensions and orientations, which are also key factors\nfor effective manufacturing process planning. This paper presents a novel\napproach for creating a synthetic CAD dataset that encompasses features\nrelevant to both additive and subtractive machining through Python Open\nCascade. The Hierarchical Graph Convolutional Neural Network (HGCNN) model is\nimplemented to accurately identify the composite additive-subtractive features\nwithin the synthetic CAD dataset. The key novelty and contribution of the\nproposed methodology lie in its ability to recognize a wide range of\nmanufacturing features, and precisely extracting their dimensions,\norientations, and stock sizes. The proposed model demonstrates remarkable\nfeature recognition accuracy exceeding 97% and a dimension extraction accuracy\nof 100% for identified features. Therefore, the proposed methodology enhances\nthe integration of CAD, CAPP, and CAM within hybrid manufacturing by providing\nprecise feature recognition and dimension extraction. It facilitates improved\nmanufacturing process planning, by enabling more informed decision-making.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "10 pages, 12 figures. This paper has been accepted for presentation\n  at the ASME IDETC-CIE 2024 conference",
    "pdf_url": "http://arxiv.org/pdf/2408.06891v2",
    "published_date": "2024-08-13 13:38:32 UTC",
    "updated_date": "2024-08-14 05:16:46 UTC"
  },
  {
    "arxiv_id": "2408.06890v2",
    "title": "BMFT: Achieving Fairness via Bias-based Weight Masking Fine-tuning",
    "authors": [
      "Yuyang Xue",
      "Junyu Yan",
      "Raman Dutt",
      "Fasih Haider",
      "Jingshuai Liu",
      "Steven McDonagh",
      "Sotirios A. Tsaftaris"
    ],
    "abstract": "Developing models with robust group fairness properties is paramount,\nparticularly in ethically sensitive domains such as medical diagnosis. Recent\napproaches to achieving fairness in machine learning require a substantial\namount of training data and depend on model retraining, which may not be\npractical in real-world scenarios. To mitigate these challenges, we propose\nBias-based Weight Masking Fine-Tuning (BMFT), a novel post-processing method\nthat enhances the fairness of a trained model in significantly fewer epochs\nwithout requiring access to the original training data. BMFT produces a mask\nover model parameters, which efficiently identifies the weights contributing\nthe most towards biased predictions. Furthermore, we propose a two-step\ndebiasing strategy, wherein the feature extractor undergoes initial fine-tuning\non the identified bias-influenced weights, succeeded by a fine-tuning phase on\na reinitialised classification layer to uphold discriminative performance.\nExtensive experiments across four dermatological datasets and two sensitive\nattributes demonstrate that BMFT outperforms existing state-of-the-art (SOTA)\ntechniques in both diagnostic accuracy and fairness metrics. Our findings\nunderscore the efficacy and robustness of BMFT in advancing fairness across\nvarious out-of-distribution (OOD) settings. Our code is available at:\nhttps://github.com/vios-s/BMFT",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by MICCAI 2024 FAIMI Workshop Oral",
    "pdf_url": "http://arxiv.org/pdf/2408.06890v2",
    "published_date": "2024-08-13 13:36:48 UTC",
    "updated_date": "2024-10-01 13:10:40 UTC"
  },
  {
    "arxiv_id": "2408.06876v2",
    "title": "Decision-Focused Learning to Predict Action Costs for Planning",
    "authors": [
      "Jayanta Mandi",
      "Marco Foschini",
      "Daniel Holler",
      "Sylvie Thiebaux",
      "Jorg Hoffmann",
      "Tias Guns"
    ],
    "abstract": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06876v2",
    "published_date": "2024-08-13 13:14:54 UTC",
    "updated_date": "2024-08-26 11:29:07 UTC"
  },
  {
    "arxiv_id": "2408.06875v2",
    "title": "Advancing Interactive Explainable AI via Belief Change Theory",
    "authors": [
      "Antonio Rago",
      "Maria Vanina Martinez"
    ],
    "abstract": "As AI models become ever more complex and intertwined in humans' daily lives,\ngreater levels of interactivity of explainable AI (XAI) methods are needed. In\nthis paper, we propose the use of belief change theory as a formal foundation\nfor operators that model the incorporation of new information, i.e. user\nfeedback in interactive XAI, to logical representations of data-driven\nclassifiers. We argue that this type of formalisation provides a framework and\na methodology to develop interactive explanations in a principled manner,\nproviding warranted behaviour and favouring transparency and accountability of\nsuch interactions. Concretely, we first define a novel, logic-based formalism\nto represent explanatory information shared between humans and machines. We\nthen consider real world scenarios for interactive XAI, with different\nprioritisations of new and existing knowledge, where our formalism may be\ninstantiated. Finally, we analyse a core set of belief change postulates,\ndiscussing their suitability for our real world settings and pointing to\nparticular challenges that may require the relaxation or reinterpretation of\nsome of the theoretical assumptions underlying existing operators.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "9 pages. To be published at KR 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.06875v2",
    "published_date": "2024-08-13 13:11:56 UTC",
    "updated_date": "2024-08-14 11:23:43 UTC"
  },
  {
    "arxiv_id": "2408.06872v1",
    "title": "Generative AI Tools in Academic Research: Applications and Implications for Qualitative and Quantitative Research Methodologies",
    "authors": [
      "Mike Perkins",
      "Jasper Roe"
    ],
    "abstract": "This study examines the impact of Generative Artificial Intelligence (GenAI)\non academic research, focusing on its application to qualitative and\nquantitative data analysis. As GenAI tools evolve rapidly, they offer new\npossibilities for enhancing research productivity and democratising complex\nanalytical processes. However, their integration into academic practice raises\nsignificant questions regarding research integrity and security, authorship,\nand the changing nature of scholarly work. Through an examination of current\ncapabilities and potential future applications, this study provides insights\ninto how researchers may utilise GenAI tools responsibly and ethically.\n  We present case studies that demonstrate the application of GenAI in various\nresearch methodologies, discuss the challenges of replicability and consistency\nin AI-assisted research, and consider the ethical implications of increased AI\nintegration in academia. This study explores both qualitative and quantitative\napplications of GenAI, highlighting tools for transcription, coding, thematic\nanalysis, visual analytics, and statistical analysis. By addressing these\nissues, we aim to contribute to the ongoing discourse on the role of AI in\nshaping the future of academic research and provide guidance for researchers\nexploring the rapidly evolving landscape of AI-assisted research tools and\nresearch.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06872v1",
    "published_date": "2024-08-13 13:10:03 UTC",
    "updated_date": "2024-08-13 13:10:03 UTC"
  },
  {
    "arxiv_id": "2408.08909v1",
    "title": "An Adaptive Differential Privacy Method Based on Federated Learning",
    "authors": [
      "Zhiqiang Wang",
      "Xinyue Yu",
      "Qianli Huang",
      "Yongguang Gong"
    ],
    "abstract": "Differential privacy is one of the methods to solve the problem of privacy\nprotection in federated learning. Setting the same privacy budget for each\nround will result in reduced accuracy in training. The existing methods of the\nadjustment of privacy budget consider fewer influencing factors and tend to\nignore the boundaries, resulting in unreasonable privacy budgets. Therefore, we\nproposed an adaptive differential privacy method based on federated learning.\nThe method sets the adjustment coefficient and scoring function according to\naccuracy, loss, training rounds, and the number of datasets and clients. And\nthe privacy budget is adjusted based on them. Then the local model update is\nprocessed according to the scaling factor and the noise. Fi-nally, the server\naggregates the noised local model update and distributes the noised global\nmodel. The range of parameters and the privacy of the method are analyzed.\nThrough the experimental evaluation, it can reduce the privacy budget by about\n16%, while the accuracy remains roughly the same.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.08909v1",
    "published_date": "2024-08-13 13:08:11 UTC",
    "updated_date": "2024-08-13 13:08:11 UTC"
  },
  {
    "arxiv_id": "2408.06851v1",
    "title": "BSS-CFFMA: Cross-Domain Feature Fusion and Multi-Attention Speech Enhancement Network based on Self-Supervised Embedding",
    "authors": [
      "Alimjan Mattursun",
      "Liejun Wang",
      "Yinfeng Yu"
    ],
    "abstract": "Speech self-supervised learning (SSL) represents has achieved\nstate-of-the-art (SOTA) performance in multiple downstream tasks. However, its\napplication in speech enhancement (SE) tasks remains immature, offering\nopportunities for improvement. In this study, we introduce a novel cross-domain\nfeature fusion and multi-attention speech enhancement network, termed\nBSS-CFFMA, which leverages self-supervised embeddings. BSS-CFFMA comprises a\nmulti-scale cross-domain feature fusion (MSCFF) block and a residual hybrid\nmulti-attention (RHMA) block. The MSCFF block effectively integrates\ncross-domain features, facilitating the extraction of rich acoustic\ninformation. The RHMA block, serving as the primary enhancement module,\nutilizes three distinct attention modules to capture diverse attention\nrepresentations and estimate high-quality speech signals.\n  We evaluate the performance of the BSS-CFFMA model through comparative and\nablation studies on the VoiceBank-DEMAND dataset, achieving SOTA results.\nFurthermore, we select three types of data from the WHAMR! dataset, a\ncollection specifically designed for speech enhancement tasks, to assess the\ncapabilities of BSS-CFFMA in tasks such as denoising only, dereverberation\nonly, and simultaneous denoising and dereverberation. This study marks the\nfirst attempt to explore the effectiveness of self-supervised embedding-based\nspeech enhancement methods in complex tasks encompassing dereverberation and\nsimultaneous denoising and dereverberation. The demo implementation of\nBSS-CFFMA is available online\\footnote[2]{https://github.com/AlimMat/BSS-CFFMA.\n\\label{s1}}.",
    "categories": [
      "eess.AS",
      "cs.AI"
    ],
    "primary_category": "eess.AS",
    "comment": "Accepted for publication by IEEE International Conference on Systems,\n  Man, and Cybernetics 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.06851v1",
    "published_date": "2024-08-13 12:27:24 UTC",
    "updated_date": "2024-08-13 12:27:24 UTC"
  },
  {
    "arxiv_id": "2408.06849v1",
    "title": "Causal Agent based on Large Language Model",
    "authors": [
      "Kairong Han",
      "Kun Kuang",
      "Ziyu Zhao",
      "Junjian Ye",
      "Fei Wu"
    ],
    "abstract": "Large language models (LLMs) have achieved significant success across various\ndomains. However, the inherent complexity of causal problems and causal theory\nposes challenges in accurately describing them in natural language, making it\ndifficult for LLMs to comprehend and use them effectively. Causal methods are\nnot easily conveyed through natural language, which hinders LLMs' ability to\napply them accurately. Additionally, causal datasets are typically tabular,\nwhile LLMs excel in handling natural language data, creating a structural\nmismatch that impedes effective reasoning with tabular data. This lack of\ncausal reasoning capability limits the development of LLMs. To address these\nchallenges, we have equipped the LLM with causal tools within an agent\nframework, named the Causal Agent, enabling it to tackle causal problems. The\ncausal agent comprises tools, memory, and reasoning modules. In the tools\nmodule, the causal agent applies causal methods to align tabular data with\nnatural language. In the reasoning module, the causal agent employs the ReAct\nframework to perform reasoning through multiple iterations with the tools. In\nthe memory module, the causal agent maintains a dictionary instance where the\nkeys are unique names and the values are causal graphs. To verify the causal\nability of the causal agent, we established a benchmark consisting of four\nlevels of causal problems: variable level, edge level, causal graph level, and\ncausal effect level. We generated a test dataset of 1.3K using ChatGPT-3.5 for\nthese four levels of issues and tested the causal agent on the datasets. Our\nmethodology demonstrates remarkable efficacy on the four-level causal problems,\nwith accuracy rates all above 80%. For further insights and implementation\ndetails, our code is accessible via the GitHub repository\nhttps://github.com/Kairong-Han/Causal_Agent.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06849v1",
    "published_date": "2024-08-13 12:22:26 UTC",
    "updated_date": "2024-08-13 12:22:26 UTC"
  },
  {
    "arxiv_id": "2408.06847v1",
    "title": "AI Research is not Magic, it has to be Reproducible and Responsible: Challenges in the AI field from the Perspective of its PhD Students",
    "authors": [
      "Andrea Hrckova",
      "Jennifer Renoux",
      "Rafael Tolosana Calasanz",
      "Daniela Chuda",
      "Martin Tamajka",
      "Jakub Simko"
    ],
    "abstract": "With the goal of uncovering the challenges faced by European AI students\nduring their research endeavors, we surveyed 28 AI doctoral candidates from 13\nEuropean countries. The outcomes underscore challenges in three key areas: (1)\nthe findability and quality of AI resources such as datasets, models, and\nexperiments; (2) the difficulties in replicating the experiments in AI papers;\n(3) and the lack of trustworthiness and interdisciplinarity. From our findings,\nit appears that although early stage AI researchers generally tend to share\ntheir AI resources, they lack motivation or knowledge to engage more in dataset\nand code preparation and curation, and ethical assessments, and are not used to\ncooperate with well-versed experts in application domains. Furthermore, we\nexamine existing practices in data governance and reproducibility both in\ncomputer science and in artificial intelligence. For instance, only a minority\nof venues actively promote reproducibility initiatives such as reproducibility\nevaluations.\n  Critically, there is need for immediate adoption of responsible and\nreproducible AI research practices, crucial for society at large, and essential\nfor the AI research community in particular. This paper proposes a combination\nof social and technical recommendations to overcome the identified challenges.\nSocially, we propose the general adoption of reproducibility initiatives in AI\nconferences and journals, as well as improved interdisciplinary collaboration,\nespecially in data governance practices. On the technical front, we call for\nenhanced tools to better support versioning control of datasets and code, and a\ncomputing infrastructure that facilitates the sharing and discovery of AI\nresources, as well as the sharing, execution, and verification of experiments.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "8 pages, 4 figures, 1 appendix (interview questions)",
    "pdf_url": "http://arxiv.org/pdf/2408.06847v1",
    "published_date": "2024-08-13 12:19:02 UTC",
    "updated_date": "2024-08-13 12:19:02 UTC"
  },
  {
    "arxiv_id": "2408.15276v1",
    "title": "A Survey of Deep Learning for Group-level Emotion Recognition",
    "authors": [
      "Xiaohua Huang",
      "Jinke Xu",
      "Wenming Zheng",
      "Qirong Mao",
      "Abhinav Dhall"
    ],
    "abstract": "With the advancement of artificial intelligence (AI) technology, group-level\nemotion recognition (GER) has emerged as an important area in analyzing human\nbehavior. Early GER methods are primarily relied on handcrafted features.\nHowever, with the proliferation of Deep Learning (DL) techniques and their\nremarkable success in diverse tasks, neural networks have garnered increasing\ninterest in GER. Unlike individual's emotion, group emotions exhibit diversity\nand dynamics. Presently, several DL approaches have been proposed to\neffectively leverage the rich information inherent in group-level image and\nenhance GER performance significantly. In this survey, we present a\ncomprehensive review of DL techniques applied to GER, proposing a new taxonomy\nfor the field cover all aspects of GER based on DL. The survey overviews\ndatasets, the deep GER pipeline, and performance comparisons of the\nstate-of-the-art methods past decade. Moreover, it summarizes and discuss the\nfundamental approaches and advanced developments for each aspect. Furthermore,\nwe identify outstanding challenges and suggest potential avenues for the design\nof robust GER systems. To the best of our knowledge, thus survey represents the\nfirst comprehensive review of deep GER methods, serving as a pivotal references\nfor future GER research endeavors.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "16 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.15276v1",
    "published_date": "2024-08-13 11:54:09 UTC",
    "updated_date": "2024-08-13 11:54:09 UTC"
  },
  {
    "arxiv_id": "2408.06820v1",
    "title": "Efficient Search for Customized Activation Functions with Gradient Descent",
    "authors": [
      "Lukas Strack",
      "Mahmoud Safari",
      "Frank Hutter"
    ],
    "abstract": "Different activation functions work best for different deep learning models.\nTo exploit this, we leverage recent advancements in gradient-based search\ntechniques for neural architectures to efficiently identify high-performing\nactivation functions for a given application. We propose a fine-grained search\ncell that combines basic mathematical operations to model activation functions,\nallowing for the exploration of novel activations. Our approach enables the\nidentification of specialized activations, leading to improved performance in\nevery model we tried, from image classification to language models. Moreover,\nthe identified activations exhibit strong transferability to larger models of\nthe same type, as well as new datasets. Importantly, our automated process for\ncreating customized activation functions is orders of magnitude more efficient\nthan previous approaches. It can easily be applied on top of arbitrary deep\nlearning pipelines and thus offers a promising practical avenue for enhancing\ndeep learning architectures.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 1 figure, excluding references and appendix",
    "pdf_url": "http://arxiv.org/pdf/2408.06820v1",
    "published_date": "2024-08-13 11:27:31 UTC",
    "updated_date": "2024-08-13 11:27:31 UTC"
  },
  {
    "arxiv_id": "2408.06818v1",
    "title": "Personalized Dynamic Difficulty Adjustment -- Imitation Learning Meets Reinforcement Learning",
    "authors": [
      "Ronja Fuchs",
      "Robin Gieseke",
      "Alexander Dockhorn"
    ],
    "abstract": "Balancing game difficulty in video games is a key task to create interesting\ngaming experiences for players. Mismatching the game difficulty and a player's\nskill or commitment results in frustration or boredom on the player's side, and\nhence reduces time spent playing the game. In this work, we explore balancing\ngame difficulty using machine learning-based agents to challenge players based\non their current behavior. This is achieved by a combination of two agents, in\nwhich one learns to imitate the player, while the second is trained to beat the\nfirst. In our demo, we investigate the proposed framework for personalized\ndynamic difficulty adjustment of AI agents in the context of the fighting game\nAI competition.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "2 pages, the code to our demo can be found here:\n  https://github.com/ronjafuchs/ICE_AI",
    "pdf_url": "http://arxiv.org/pdf/2408.06818v1",
    "published_date": "2024-08-13 11:24:12 UTC",
    "updated_date": "2024-08-13 11:24:12 UTC"
  },
  {
    "arxiv_id": "2408.06816v2",
    "title": "MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data Uncertainty",
    "authors": [
      "Yongjin Yang",
      "Haneul Yoo",
      "Hwaran Lee"
    ],
    "abstract": "Despite the massive advancements in large language models (LLMs), they still\nsuffer from producing plausible but incorrect responses. To improve the\nreliability of LLMs, recent research has focused on uncertainty quantification\nto predict whether a response is correct or not. However, most uncertainty\nquantification methods have been evaluated on single-labeled questions, which\nremoves data uncertainty: the irreducible randomness often present in user\nqueries, which can arise from factors like multiple possible answers. This\nlimitation may cause uncertainty quantification results to be unreliable in\npractical settings. In this paper, we investigate previous uncertainty\nquantification methods under the presence of data uncertainty. Our\ncontributions are two-fold: 1) proposing a new Multi-Answer Question Answering\ndataset, MAQA, consisting of world knowledge, mathematical reasoning, and\ncommonsense reasoning tasks to evaluate uncertainty quantification regarding\ndata uncertainty, and 2) assessing 5 uncertainty quantification methods of\ndiverse white- and black-box LLMs. Our findings show that previous methods\nrelatively struggle compared to single-answer settings, though this varies\ndepending on the task. Moreover, we observe that entropy- and consistency-based\nmethods effectively estimate model uncertainty, even in the presence of data\nuncertainty. We believe these observations will guide future work on\nuncertainty quantification in more realistic settings.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Findings of NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2408.06816v2",
    "published_date": "2024-08-13 11:17:31 UTC",
    "updated_date": "2025-03-31 13:03:14 UTC"
  },
  {
    "arxiv_id": "2408.08907v1",
    "title": "What should I wear to a party in a Greek taverna? Evaluation for Conversational Agents in the Fashion Domain",
    "authors": [
      "Antonis Maronikolakis",
      "Ana Peleteiro Ramallo",
      "Weiwei Cheng",
      "Thomas Kober"
    ],
    "abstract": "Large language models (LLMs) are poised to revolutionize the domain of online\nfashion retail, enhancing customer experience and discovery of fashion online.\nLLM-powered conversational agents introduce a new way of discovery by directly\ninteracting with customers, enabling them to express in their own ways, refine\ntheir needs, obtain fashion and shopping advice that is relevant to their taste\nand intent. For many tasks in e-commerce, such as finding a specific product,\nconversational agents need to convert their interactions with a customer to a\nspecific call to different backend systems, e.g., a search system to showcase a\nrelevant set of products. Therefore, evaluating the capabilities of LLMs to\nperform those tasks related to calling other services is vital. However, those\nevaluations are generally complex, due to the lack of relevant and high quality\ndatasets, and do not align seamlessly with business needs, amongst others. To\nthis end, we created a multilingual evaluation dataset of 4k conversations\nbetween customers and a fashion assistant in a large e-commerce fashion\nplatform to measure the capabilities of LLMs to serve as an assistant between\ncustomers and a backend engine. We evaluate a range of models, showcasing how\nour dataset scales to business needs and facilitates iterative development of\ntools.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted at KDD workshop on Evaluation and Trustworthiness of\n  Generative AI Models",
    "pdf_url": "http://arxiv.org/pdf/2408.08907v1",
    "published_date": "2024-08-13 11:11:27 UTC",
    "updated_date": "2024-08-13 11:11:27 UTC"
  },
  {
    "arxiv_id": "2408.06806v1",
    "title": "Unmasking the Uniqueness: A Glimpse into Age-Invariant Face Recognition of Indigenous African Faces",
    "authors": [
      "Fakunle Ajewole",
      "Joseph Damilola Akinyemi",
      "Khadijat Tope Ladoja",
      "Olufade Falade Williams Onifade"
    ],
    "abstract": "The task of recognizing the age-separated faces of an individual,\nAge-Invariant Face Recognition (AIFR), has received considerable research\nefforts in Europe, America, and Asia, compared to Africa. Thus, AIFR research\nefforts have often under-represented/misrepresented the African ethnicity with\nnon-indigenous Africans. This work developed an AIFR system for indigenous\nAfrican faces to reduce the misrepresentation of African ethnicity in facial\nimage analysis research. We adopted a pre-trained deep learning model (VGGFace)\nfor AIFR on a dataset of 5,000 indigenous African faces (FAGE\\_v2) collected\nfor this study. FAGE\\_v2 was curated via Internet image searches of 500\nindividuals evenly distributed across 10 African countries. VGGFace was trained\non FAGE\\_v2 to obtain the best accuracy of 81.80\\%. We also performed\nexperiments on an African-American subset of the CACD dataset and obtained the\nbest accuracy of 91.5\\%. The results show a significant difference in the\nrecognition accuracies of indigenous versus non-indigenous Africans.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Keywords: Age-Invariant Face Recognition, CACD, FAGE_v2, VGGFace",
    "pdf_url": "http://arxiv.org/pdf/2408.06806v1",
    "published_date": "2024-08-13 10:54:10 UTC",
    "updated_date": "2024-08-13 10:54:10 UTC"
  },
  {
    "arxiv_id": "2408.06804v1",
    "title": "Deep Learning for Speaker Identification: Architectural Insights from AB-1 Corpus Analysis and Performance Evaluation",
    "authors": [
      "Matthias Bartolo"
    ],
    "abstract": "In the fields of security systems, forensic investigations, and personalized\nservices, the importance of speech as a fundamental human input outweighs\ntext-based interactions. This research delves deeply into the complex field of\nSpeaker Identification (SID), examining its essential components and\nemphasising Mel Spectrogram and Mel Frequency Cepstral Coefficients (MFCC) for\nfeature extraction. Moreover, this study evaluates six slightly distinct model\narchitectures using extensive analysis to evaluate their performance, with\nhyperparameter tuning applied to the best-performing model. This work performs\na linguistic analysis to verify accent and gender accuracy, in addition to bias\nevaluation within the AB-1 Corpus dataset.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Resultant work from Assignment, Department of AI, University of\n  Malta. Code available at: https://github.com/mbar0075/Speech-Technology",
    "pdf_url": "http://arxiv.org/pdf/2408.06804v1",
    "published_date": "2024-08-13 10:46:50 UTC",
    "updated_date": "2024-08-13 10:46:50 UTC"
  },
  {
    "arxiv_id": "2408.06803v1",
    "title": "Integrating Saliency Ranking and Reinforcement Learning for Enhanced Object Detection",
    "authors": [
      "Matthias Bartolo",
      "Dylan Seychell",
      "Josef Bajada"
    ],
    "abstract": "With the ever-growing variety of object detection approaches, this study\nexplores a series of experiments that combine reinforcement learning (RL)-based\nvisual attention methods with saliency ranking techniques to investigate\ntransparent and sustainable solutions. By integrating saliency ranking for\ninitial bounding box prediction and subsequently applying RL techniques to\nrefine these predictions through a finite set of actions over multiple time\nsteps, this study aims to enhance RL object detection accuracy. Presented as a\nseries of experiments, this research investigates the use of various image\nfeature extraction methods and explores diverse Deep Q-Network (DQN)\narchitectural variations for deep reinforcement learning-based localisation\nagent training. Additionally, we focus on optimising the detection pipeline at\nevery step by prioritising lightweight and faster models, while also\nincorporating the capability to classify detected objects, a feature absent in\nprevious RL approaches. We show that by evaluating the performance of these\ntrained agents using the Pascal VOC 2007 dataset, faster and more optimised\nmodels were developed. Notably, the best mean Average Precision (mAP) achieved\nin this study was 51.4, surpassing benchmarks set by RL-based single object\ndetectors in the literature.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Resultant work from Dissertation, Department of AI, University of\n  Malta. Code available at: https://github.com/mbar0075/SaRLVision",
    "pdf_url": "http://arxiv.org/pdf/2408.06803v1",
    "published_date": "2024-08-13 10:46:42 UTC",
    "updated_date": "2024-08-13 10:46:42 UTC"
  },
  {
    "arxiv_id": "2408.06797v1",
    "title": "Stunned by Sleeping Beauty: How Prince Probability updates his forecast upon their fateful encounter",
    "authors": [
      "Laurens Walleghem"
    ],
    "abstract": "The Sleeping Beauty problem is a puzzle in probability theory that has gained\nmuch attention since Elga's discussion of it [Elga, Adam, Analysis 60 (2),\np.143-147 (2000)]. Sleeping Beauty is put asleep, and a coin is tossed. If the\noutcome of the coin toss is Tails, Sleeping Beauty is woken up on Monday, put\nasleep again and woken up again on Tuesday (with no recollection of having\nwoken up on Monday). If the outcome is Heads, Sleeping Beauty is woken up on\nMonday only. Each time Sleeping Beauty is woken up, she is asked what her\nbelief is that the outcome was Heads. What should Sleeping Beauty reply? In\nliterature arguments have been given for both 1/3 and 1/2 as the correct\nanswer. In this short note we argue using simple Bayesian probability theory\nwhy 1/3 is the right answer, and not 1/2. Briefly, when Sleeping Beauty\nawakens, her being awake is nontrivial extra information that leads her to\nupdate her beliefs about Heads to 1/3. We strengthen our claim by considering\nan additional observer, Prince Probability, who may or may not meet Sleeping\nBeauty. If he meets Sleeping Beauty while she is awake, he lowers his credence\nin Heads to 1/3. We also briefly consider the credence in Heads of a Sleeping\nBeauty who knows that she is dreaming (and thus asleep).",
    "categories": [
      "math.PR",
      "cs.AI",
      "physics.soc-ph"
    ],
    "primary_category": "math.PR",
    "comment": "12 pages, 1 figure, all comments welcome!",
    "pdf_url": "http://arxiv.org/pdf/2408.06797v1",
    "published_date": "2024-08-13 10:27:16 UTC",
    "updated_date": "2024-08-13 10:27:16 UTC"
  },
  {
    "arxiv_id": "2408.06776v1",
    "title": "Robust Deep Reinforcement Learning for Inverter-based Volt-Var Control in Partially Observable Distribution Networks",
    "authors": [
      "Qiong Liu",
      "Ye Guo",
      "Tong Xu"
    ],
    "abstract": "Inverter-based volt-var control is studied in this paper. One key issue in\nDRL-based approaches is the limited measurement deployment in active\ndistribution networks, which leads to problems of a partially observable state\nand unknown reward. To address those problems, this paper proposes a robust DRL\napproach with a conservative critic and a surrogate reward. The conservative\ncritic utilizes the quantile regression technology to estimate conservative\nstate-action value function based on the partially observable state, which\nhelps to train a robust policy; the surrogate rewards of power loss and voltage\nviolation are designed that can be calculated from the limited measurements.\nThe proposed approach optimizes the power loss of the whole network and the\nvoltage profile of buses with measurable voltages while indirectly improving\nthe voltage profile of other buses. Extensive simulations verify the\neffectiveness of the robust DRL approach in different limited measurement\nconditions, even when only the active power injection of the root bus and less\nthan 10% of bus voltages are measurable.",
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.SY"
    ],
    "primary_category": "eess.SY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06776v1",
    "published_date": "2024-08-13 10:02:10 UTC",
    "updated_date": "2024-08-13 10:02:10 UTC"
  },
  {
    "arxiv_id": "2408.06772v1",
    "title": "Exploring Domain Shift on Radar-Based 3D Object Detection Amidst Diverse Environmental Conditions",
    "authors": [
      "Miao Zhang",
      "Sherif Abdulatif",
      "Benedikt Loesch",
      "Marco Altmann",
      "Marius Schwarz",
      "Bin Yang"
    ],
    "abstract": "The rapid evolution of deep learning and its integration with autonomous\ndriving systems have led to substantial advancements in 3D perception using\nmultimodal sensors. Notably, radar sensors show greater robustness compared to\ncameras and lidar under adverse weather and varying illumination conditions.\nThis study delves into the often-overlooked yet crucial issue of domain shift\nin 4D radar-based object detection, examining how varying environmental\nconditions, such as different weather patterns and road types, impact 3D object\ndetection performance. Our findings highlight distinct domain shifts across\nvarious weather scenarios, revealing unique dataset sensitivities that\nunderscore the critical role of radar point cloud generation. Additionally, we\ndemonstrate that transitioning between different road types, especially from\nhighways to urban settings, introduces notable domain shifts, emphasizing the\nnecessity for diverse data collection across varied road environments. To the\nbest of our knowledge, this is the first comprehensive analysis of domain shift\neffects on 4D radar-based object detection. We believe this empirical study\ncontributes to understanding the complex nature of domain shifts in radar data\nand suggests paths forward for data collection strategy in the face of\nenvironmental variability.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 5 figures, 3 tables, accepted in IEEE International\n  Conference on Intelligent Transportation Systems (ITSC) 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.06772v1",
    "published_date": "2024-08-13 09:55:38 UTC",
    "updated_date": "2024-08-13 09:55:38 UTC"
  },
  {
    "arxiv_id": "2408.06761v1",
    "title": "Cross-View Geolocalization and Disaster Mapping with Street-View and VHR Satellite Imagery: A Case Study of Hurricane IAN",
    "authors": [
      "Hao Li",
      "Fabian Deuser",
      "Wenping Yina",
      "Xuanshu Luo",
      "Paul Walther",
      "Gengchen Mai",
      "Wei Huang",
      "Martin Werner"
    ],
    "abstract": "Nature disasters play a key role in shaping human-urban infrastructure\ninteractions. Effective and efficient response to natural disasters is\nessential for building resilience and a sustainable urban environment. Two\ntypes of information are usually the most necessary and difficult to gather in\ndisaster response. The first information is about disaster damage perception,\nwhich shows how badly people think that urban infrastructure has been damaged.\nThe second information is geolocation awareness, which means how people\nwhereabouts are made available. In this paper, we proposed a novel disaster\nmapping framework, namely CVDisaster, aiming at simultaneously addressing\ngeolocalization and damage perception estimation using cross-view Street-View\nImagery (SVI) and Very High-Resolution satellite imagery. CVDisaster consists\nof two cross-view models, where CVDisaster-Geoloc refers to a cross-view\ngeolocalization model based on a contrastive learning objective with a Siamese\nConvNeXt image encoder, and CVDisaster-Est is a cross-view classification model\nbased on a Couple Global Context Vision Transformer (CGCViT). Taking Hurricane\nIAN as a case study, we evaluate the CVDisaster framework by creating a novel\ncross-view dataset (CVIAN) and conducting extensive experiments. As a result,\nwe show that CVDisaster can achieve highly competitive performance (over 80%\nfor geolocalization and 75% for damage perception estimation) with even limited\nfine-tuning efforts, which largely motivates future cross-view models and\napplications within a broader GeoAI research community. The data and code are\npublicly available at: https://github.com/tum-bgd/CVDisaster.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06761v1",
    "published_date": "2024-08-13 09:37:26 UTC",
    "updated_date": "2024-08-13 09:37:26 UTC"
  },
  {
    "arxiv_id": "2408.06752v2",
    "title": "Evaluating Research Quality with Large Language Models: An Analysis of ChatGPT's Effectiveness with Different Settings and Inputs",
    "authors": [
      "Mike Thelwall"
    ],
    "abstract": "Evaluating the quality of academic journal articles is a time consuming but\ncritical task for national research evaluation exercises, appointments and\npromotion. It is therefore important to investigate whether Large Language\nModels (LLMs) can play a role in this process. This article assesses which\nChatGPT inputs (full text without tables, figures and references; title and\nabstract; title only) produce better quality score estimates, and the extent to\nwhich scores are affected by ChatGPT models and system prompts. The results\nshow that the optimal input is the article title and abstract, with average\nChatGPT scores based on these (30 iterations on a dataset of 51 papers)\ncorrelating at 0.67 with human scores, the highest ever reported. ChatGPT 4o is\nslightly better than 3.5-turbo (0.66), and 4o-mini (0.66). The results suggest\nthat article full texts might confuse LLM research quality evaluations, even\nthough complex system instructions for the task are more effective than simple\nones. Thus, whilst abstracts contain insufficient information for a thorough\nassessment of rigour, they may contain strong pointers about originality and\nsignificance. Finally, linear regression can be used to convert the model\nscores into the human scale scores, which is 31% more accurate than guessing.",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06752v2",
    "published_date": "2024-08-13 09:19:21 UTC",
    "updated_date": "2024-11-29 09:17:29 UTC"
  },
  {
    "arxiv_id": "2408.06740v3",
    "title": "DiffLoRA: Generating Personalized Low-Rank Adaptation Weights with Diffusion",
    "authors": [
      "Yujia Wu",
      "Yiming Shi",
      "Jiwei Wei",
      "Chengwei Sun",
      "Yang Yang",
      "Heng Tao Shen"
    ],
    "abstract": "Personalized text-to-image generation has gained significant attention for\nits capability to generate high-fidelity portraits of specific identities\nconditioned on user-defined prompts. Existing methods typically involve\ntest-time fine-tuning or incorporating an additional pre-trained branch.\nHowever, these approaches struggle to simultaneously address efficiency,\nidentity fidelity, and the preservation of the model's original generative\ncapabilities. In this paper, we propose DiffLoRA, an efficient method that\nleverages the diffusion model as a hypernetwork to predict personalized\nLow-Rank Adaptation (LoRA) weights based on the reference images. By\nincorporating these LoRA weights into the off-the-shelf text-to-image model,\nDiffLoRA enables zero-shot personalization during inference, eliminating the\nneed for post-processing optimization. Moreover, we introduce a novel\nidentity-oriented LoRA weights construction pipeline to facilitate the training\nprocess of DiffLoRA. The dataset generated through this pipeline enables\nDiffLoRA to produce consistently high-quality LoRA weights. Notably, the\ndistinctive properties of the diffusion model enhance the generation of\nsuperior weights by employing probabilistic modeling to capture intricate\nstructural patterns and thoroughly explore the weight space. Comprehensive\nexperimental results demonstrate that DiffLoRA outperforms existing\npersonalization approaches across multiple benchmarks, achieving both time\nefficiency and maintaining identity fidelity throughout the personalization\nprocess.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "9 pages,8 figures",
    "pdf_url": "http://arxiv.org/pdf/2408.06740v3",
    "published_date": "2024-08-13 09:00:35 UTC",
    "updated_date": "2024-11-15 08:36:51 UTC"
  },
  {
    "arxiv_id": "2408.06736v1",
    "title": "Speculations on Uncertainty and Humane Algorithms",
    "authors": [
      "Nicholas Gray"
    ],
    "abstract": "The appreciation and utilisation of risk and uncertainty can play a key role\nin helping to solve some of the many ethical issues that are posed by AI.\nUnderstanding the uncertainties can allow algorithms to make better decisions\nby providing interrogatable avenues to check the correctness of outputs.\nAllowing algorithms to deal with variability and ambiguity with their inputs\nmeans they do not need to force people into uncomfortable classifications.\nProvenance enables algorithms to know what they know preventing possible harms.\nAdditionally, uncertainty about provenance highlights the trustworthiness of\nalgorithms. It is essential to compute with what we know rather than make\nassumptions that may be unjustified or untenable. This paper provides a\nperspective on the need for the importance of risk and uncertainty in the\ndevelopment of ethical AI, especially in high-risk scenarios. It argues that\nthe handling of uncertainty, especially epistemic uncertainty, is critical to\nensuring that algorithms do not cause harm and are trustworthy and ensure that\nthe decisions that they make are humane.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06736v1",
    "published_date": "2024-08-13 08:54:34 UTC",
    "updated_date": "2024-08-13 08:54:34 UTC"
  },
  {
    "arxiv_id": "2408.06731v1",
    "title": "Large language models can consistently generate high-quality content for election disinformation operations",
    "authors": [
      "Angus R. Williams",
      "Liam Burke-Moore",
      "Ryan Sze-Yin Chan",
      "Florence E. Enock",
      "Federico Nanni",
      "Tvesha Sippy",
      "Yi-Ling Chung",
      "Evelina Gabasova",
      "Kobi Hackenburg",
      "Jonathan Bright"
    ],
    "abstract": "Advances in large language models have raised concerns about their potential\nuse in generating compelling election disinformation at scale. This study\npresents a two-part investigation into the capabilities of LLMs to automate\nstages of an election disinformation operation. First, we introduce DisElect, a\nnovel evaluation dataset designed to measure LLM compliance with instructions\nto generate content for an election disinformation operation in localised UK\ncontext, containing 2,200 malicious prompts and 50 benign prompts. Using\nDisElect, we test 13 LLMs and find that most models broadly comply with these\nrequests; we also find that the few models which refuse malicious prompts also\nrefuse benign election-related prompts, and are more likely to refuse to\ngenerate content from a right-wing perspective. Secondly, we conduct a series\nof experiments (N=2,340) to assess the \"humanness\" of LLMs: the extent to which\ndisinformation operation content generated by an LLM is able to pass as\nhuman-written. Our experiments suggest that almost all LLMs tested released\nsince 2022 produce election disinformation operation content indiscernible by\nhuman evaluators over 50% of the time. Notably, we observe that multiple models\nachieve above-human levels of humanness. Taken together, these findings suggest\nthat current LLMs can be used to generate high-quality content for election\ndisinformation operations, even in hyperlocalised scenarios, at far lower costs\nthan traditional methods, and offer researchers and policymakers an empirical\nbenchmark for the measurement and evaluation of these capabilities in current\nand future models.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06731v1",
    "published_date": "2024-08-13 08:45:34 UTC",
    "updated_date": "2024-08-13 08:45:34 UTC"
  },
  {
    "arxiv_id": "2408.06725v1",
    "title": "Enhancing Visual Dialog State Tracking through Iterative Object-Entity Alignment in Multi-Round Conversations",
    "authors": [
      "Wei Pang",
      "Ruixue Duan",
      "Jinfu Yang",
      "Ning Li"
    ],
    "abstract": "Visual Dialog (VD) is a task where an agent answers a series of image-related\nquestions based on a multi-round dialog history. However, previous VD methods\noften treat the entire dialog history as a simple text input, disregarding the\ninherent conversational information flows at the round level. In this paper, we\nintroduce Multi-round Dialogue State Tracking model (MDST), a framework that\naddresses this limitation by leveraging the dialogue state learned from dialog\nhistory to answer questions. MDST captures each round of dialog history,\nconstructing internal dialogue state representations defined as 2-tuples of\nvision-language representations. These representations effectively ground the\ncurrent question, enabling the generation of accurate answers. Experimental\nresults on the VisDial v1.0 dataset demonstrate that MDST achieves a new\nstate-of-the-art performance in generative setting. Furthermore, through a\nseries of human studies, we validate the effectiveness of MDST in generating\nlong, consistent, and human-like answers while consistently answering a series\nof questions correctly.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "comment": "This article has been accepted in CAAI Transactions on Intelligence\n  Technology! Article ID: CIT2_12370, Article DOI: 10.1049/cit2.12370",
    "pdf_url": "http://arxiv.org/pdf/2408.06725v1",
    "published_date": "2024-08-13 08:36:15 UTC",
    "updated_date": "2024-08-13 08:36:15 UTC"
  },
  {
    "arxiv_id": "2408.06724v1",
    "title": "Adaptive Data Quality Scoring Operations Framework using Drift-Aware Mechanism for Industrial Applications",
    "authors": [
      "Firas Bayram",
      "Bestoun S. Ahmed",
      "Erik Hallin"
    ],
    "abstract": "Within data-driven artificial intelligence (AI) systems for industrial\napplications, ensuring the reliability of the incoming data streams is an\nintegral part of trustworthy decision-making. An approach to assess data\nvalidity is data quality scoring, which assigns a score to each data point or\nstream based on various quality dimensions. However, certain dimensions exhibit\ndynamic qualities, which require adaptation on the basis of the system's\ncurrent conditions. Existing methods often overlook this aspect, making them\ninefficient in dynamic production environments. In this paper, we introduce the\nAdaptive Data Quality Scoring Operations Framework, a novel framework developed\nto address the challenges posed by dynamic quality dimensions in industrial\ndata streams. The framework introduces an innovative approach by integrating a\ndynamic change detector mechanism that actively monitors and adapts to changes\nin data quality, ensuring the relevance of quality scores. We evaluate the\nproposed framework performance in a real-world industrial use case. The\nexperimental results reveal high predictive performance and efficient\nprocessing time, highlighting its effectiveness in practical quality-driven AI\napplications.",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.DB",
    "comment": "17 pages",
    "pdf_url": "http://arxiv.org/pdf/2408.06724v1",
    "published_date": "2024-08-13 08:32:06 UTC",
    "updated_date": "2024-08-13 08:32:06 UTC"
  },
  {
    "arxiv_id": "2408.06717v1",
    "title": "Computation-friendly Graph Neural Network Design by Accumulating Knowledge on Large Language Models",
    "authors": [
      "Jialiang Wang",
      "Shimin Di",
      "Hanmo Liu",
      "Zhili Wang",
      "Jiachuan Wang",
      "Lei Chen",
      "Xiaofang Zhou"
    ],
    "abstract": "Graph Neural Networks (GNNs), like other neural networks, have shown\nremarkable success but are hampered by the complexity of their architecture\ndesigns, which heavily depend on specific data and tasks. Traditionally,\ndesigning proper architectures involves trial and error, which requires\nintensive manual effort to optimize various components. To reduce human\nworkload, researchers try to develop automated algorithms to design GNNs.\nHowever, both experts and automated algorithms suffer from two major issues in\ndesigning GNNs: 1) the substantial computational resources expended in\nrepeatedly trying candidate GNN architectures until a feasible design is\nachieved, and 2) the intricate and prolonged processes required for humans or\nalgorithms to accumulate knowledge of the interrelationship between graphs,\nGNNs, and performance.\n  To further enhance the automation of GNN architecture design, we propose a\ncomputation-friendly way to empower Large Language Models (LLMs) with\nspecialized knowledge in designing GNNs, thereby drastically shortening the\ncomputational overhead and development cycle of designing GNN architectures.\nOur framework begins by establishing a knowledge retrieval pipeline that\ncomprehends the intercorrelations between graphs, GNNs, and performance. This\npipeline converts past model design experiences into structured knowledge for\nLLM reference, allowing it to quickly suggest initial model proposals.\nSubsequently, we introduce a knowledge-driven search strategy that emulates the\nexploration-exploitation process of human experts, enabling quick refinement of\ninitial proposals within a promising scope. Extensive experiments demonstrate\nthat our framework can efficiently deliver promising (e.g., Top-5.77%) initial\nmodel proposals for unseen datasets within seconds and without any prior\ntraining and achieve outstanding search performance in a few iterations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06717v1",
    "published_date": "2024-08-13 08:22:01 UTC",
    "updated_date": "2024-08-13 08:22:01 UTC"
  },
  {
    "arxiv_id": "2408.06710v1",
    "title": "Variational Learning of Gaussian Process Latent Variable Models through Stochastic Gradient Annealed Importance Sampling",
    "authors": [
      "Jian Xu",
      "Shian Du",
      "Junmei Yang",
      "Qianli Ma",
      "Delu Zeng"
    ],
    "abstract": "Gaussian Process Latent Variable Models (GPLVMs) have become increasingly\npopular for unsupervised tasks such as dimensionality reduction and missing\ndata recovery due to their flexibility and non-linear nature. An\nimportance-weighted version of the Bayesian GPLVMs has been proposed to obtain\na tighter variational bound. However, this version of the approach is primarily\nlimited to analyzing simple data structures, as the generation of an effective\nproposal distribution can become quite challenging in high-dimensional spaces\nor with complex data sets. In this work, we propose an Annealed Importance\nSampling (AIS) approach to address these issues. By transforming the posterior\ninto a sequence of intermediate distributions using annealing, we combine the\nstrengths of Sequential Monte Carlo samplers and VI to explore a wider range of\nposterior distributions and gradually approach the target distribution. We\nfurther propose an efficient algorithm by reparameterizing all variables in the\nevidence lower bound (ELBO). Experimental results on both toy and image\ndatasets demonstrate that our method outperforms state-of-the-art methods in\nterms of tighter variational bounds, higher log-likelihoods, and more robust\nconvergence.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06710v1",
    "published_date": "2024-08-13 08:09:05 UTC",
    "updated_date": "2024-08-13 08:09:05 UTC"
  },
  {
    "arxiv_id": "2408.06699v1",
    "title": "Information Geometry and Beta Link for Optimizing Sparse Variational Student-t Processes",
    "authors": [
      "Jian Xu",
      "Delu Zeng",
      "John Paisley"
    ],
    "abstract": "Recently, a sparse version of Student-t Processes, termed sparse variational\nStudent-t Processes, has been proposed to enhance computational efficiency and\nflexibility for real-world datasets using stochastic gradient descent. However,\ntraditional gradient descent methods like Adam may not fully exploit the\nparameter space geometry, potentially leading to slower convergence and\nsuboptimal performance. To mitigate these issues, we adopt natural gradient\nmethods from information geometry for variational parameter optimization of\nStudent-t Processes. This approach leverages the curvature and structure of the\nparameter space, utilizing tools such as the Fisher information matrix which is\nlinked to the Beta function in our model. This method provides robust\nmathematical support for the natural gradient algorithm when using Student's\nt-distribution as the variational distribution. Additionally, we present a\nmini-batch algorithm for efficiently computing natural gradients. Experimental\nresults across four benchmark datasets demonstrate that our method consistently\naccelerates convergence speed.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06699v1",
    "published_date": "2024-08-13 07:53:39 UTC",
    "updated_date": "2024-08-13 07:53:39 UTC"
  },
  {
    "arxiv_id": "2408.06697v1",
    "title": "SlotLifter: Slot-guided Feature Lifting for Learning Object-centric Radiance Fields",
    "authors": [
      "Yu Liu",
      "Baoxiong Jia",
      "Yixin Chen",
      "Siyuan Huang"
    ],
    "abstract": "The ability to distill object-centric abstractions from intricate visual\nscenes underpins human-level generalization. Despite the significant progress\nin object-centric learning methods, learning object-centric representations in\nthe 3D physical world remains a crucial challenge. In this work, we propose\nSlotLifter, a novel object-centric radiance model addressing scene\nreconstruction and decomposition jointly via slot-guided feature lifting. Such\na design unites object-centric learning representations and image-based\nrendering methods, offering state-of-the-art performance in scene decomposition\nand novel-view synthesis on four challenging synthetic and four complex\nreal-world datasets, outperforming existing 3D object-centric learning methods\nby a large margin. Through extensive ablative studies, we showcase the efficacy\nof designs in SlotLifter, revealing key insights for potential future\ndirections.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by ECCV 2024. Project website: https://slotlifter.github.io",
    "pdf_url": "http://arxiv.org/pdf/2408.06697v1",
    "published_date": "2024-08-13 07:51:37 UTC",
    "updated_date": "2024-08-13 07:51:37 UTC"
  },
  {
    "arxiv_id": "2408.06693v1",
    "title": "DC3DO: Diffusion Classifier for 3D Objects",
    "authors": [
      "Nursena Koprucu",
      "Meher Shashwat Nigam",
      "Shicheng Xu",
      "Biruk Abere",
      "Gabriele Dominici",
      "Andrew Rodriguez",
      "Sharvaree Vadgama",
      "Berfin Inal",
      "Alberto Tono"
    ],
    "abstract": "Inspired by Geoffrey Hinton emphasis on generative modeling, To recognize\nshapes, first learn to generate them, we explore the use of 3D diffusion models\nfor object classification. Leveraging the density estimates from these models,\nour approach, the Diffusion Classifier for 3D Objects (DC3DO), enables\nzero-shot classification of 3D shapes without additional training. On average,\nour method achieves a 12.5 percent improvement compared to its multiview\ncounterparts, demonstrating superior multimodal reasoning over discriminative\napproaches. DC3DO employs a class-conditional diffusion model trained on\nShapeNet, and we run inferences on point clouds of chairs and cars. This work\nhighlights the potential of generative models in 3D object classification.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06693v1",
    "published_date": "2024-08-13 07:35:56 UTC",
    "updated_date": "2024-08-13 07:35:56 UTC"
  },
  {
    "arxiv_id": "2408.06687v2",
    "title": "Masked Image Modeling: A Survey",
    "authors": [
      "Vlad Hondru",
      "Florinel Alin Croitoru",
      "Shervin Minaee",
      "Radu Tudor Ionescu",
      "Nicu Sebe"
    ],
    "abstract": "In this work, we survey recent studies on masked image modeling (MIM), an\napproach that emerged as a powerful self-supervised learning technique in\ncomputer vision. The MIM task involves masking some information, e.g.~pixels,\npatches, or even latent representations, and training a model, usually an\nautoencoder, to predicting the missing information by using the context\navailable in the visible part of the input. We identify and formalize two\ncategories of approaches on how to implement MIM as a pretext task, one based\non reconstruction and one based on contrastive learning. Then, we construct a\ntaxonomy and review the most prominent papers in recent years. We complement\nthe manually constructed taxonomy with a dendrogram obtained by applying a\nhierarchical clustering algorithm. We further identify relevant clusters via\nmanually inspecting the resulting dendrogram. Our review also includes datasets\nthat are commonly used in MIM research. We aggregate the performance results of\nvarious masked image modeling methods on the most popular datasets, to\nfacilitate the comparison of competing methods. Finally, we identify research\ngaps and propose several interesting directions of future work. We supplement\nour survey with the following public repository containing organized\nreferences: https://github.com/vladhondru25/MIM-Survey.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Revised version",
    "pdf_url": "http://arxiv.org/pdf/2408.06687v2",
    "published_date": "2024-08-13 07:27:02 UTC",
    "updated_date": "2025-01-09 22:14:55 UTC"
  },
  {
    "arxiv_id": "2408.08906v1",
    "title": "Bundle Recommendation with Item-level Causation-enhanced Multi-view Learning",
    "authors": [
      "Huy-Son Nguyen",
      "Tuan-Nghia Bui",
      "Long-Hai Nguyen",
      "Hoang Manh-Hung",
      "Cam-Van Thi Nguyen",
      "Hoang-Quynh Le",
      "Duc-Trong Le"
    ],
    "abstract": "Bundle recommendation aims to enhance business profitability and user\nconvenience by suggesting a set of interconnected items. In real-world\nscenarios, leveraging the impact of asymmetric item affiliations is crucial for\neffective bundle modeling and understanding user preferences. To address this,\nwe present BunCa, a novel bundle recommendation approach employing item-level\ncausation-enhanced multi-view learning. BunCa provides comprehensive\nrepresentations of users and bundles through two views: the Coherent View,\nleveraging the Multi-Prospect Causation Network for causation-sensitive\nrelations among items, and the Cohesive View, employing LightGCN for\ninformation propagation among users and bundles. Modeling user preferences and\nbundle construction combined from both views ensures rigorous cohesion in\ndirect user-bundle interactions through the Cohesive View and captures explicit\nintents through the Coherent View. Simultaneously, the integration of concrete\nand discrete contrastive learning optimizes the consistency and\nself-discrimination of multi-view representations. Extensive experiments with\nBunCa on three benchmark datasets demonstrate the effectiveness of this novel\nresearch and validate our hypothesis.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.08906v1",
    "published_date": "2024-08-13 07:05:27 UTC",
    "updated_date": "2024-08-13 07:05:27 UTC"
  },
  {
    "arxiv_id": "2408.06672v1",
    "title": "Leveraging Priors via Diffusion Bridge for Time Series Generation",
    "authors": [
      "Jinseong Park",
      "Seungyun Lee",
      "Woojin Jeong",
      "Yujin Choi",
      "Jaewook Lee"
    ],
    "abstract": "Time series generation is widely used in real-world applications such as\nsimulation, data augmentation, and hypothesis test techniques. Recently,\ndiffusion models have emerged as the de facto approach for time series\ngeneration, emphasizing diverse synthesis scenarios based on historical or\ncorrelated time series data streams. Since time series have unique\ncharacteristics, such as fixed time order and data scaling, standard Gaussian\nprior might be ill-suited for general time series generation. In this paper, we\nexploit the usage of diverse prior distributions for synthesis. Then, we\npropose TimeBridge, a framework that enables flexible synthesis by leveraging\ndiffusion bridges to learn the transport between chosen prior and data\ndistributions. Our model covers a wide range of scenarios in time series\ndiffusion models, which leverages (i) data- and time-dependent priors for\nunconditional synthesis, and (ii) data-scale preserving synthesis with a\nconstraint as a prior for conditional generation. Experimentally, our model\nachieves state-of-the-art performance in both unconditional and conditional\ntime series generation tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06672v1",
    "published_date": "2024-08-13 06:47:59 UTC",
    "updated_date": "2024-08-13 06:47:59 UTC"
  },
  {
    "arxiv_id": "2408.06665v1",
    "title": "RW-NSGCN: A Robust Approach to Structural Attacks via Negative Sampling",
    "authors": [
      "Shuqi He",
      "Jun Zhuang",
      "Ding Wang",
      "Jun Song"
    ],
    "abstract": "Node classification using Graph Neural Networks (GNNs) has been widely\napplied in various practical scenarios, such as predicting user interests and\ndetecting communities in social networks. However, recent studies have shown\nthat graph-structured networks often contain potential noise and attacks, in\nthe form of topological perturbations and weight disturbances, which can lead\nto decreased classification performance in GNNs. To improve the robustness of\nthe model, we propose a novel method: Random Walk Negative Sampling Graph\nConvolutional Network (RW-NSGCN). Specifically, RW-NSGCN integrates the Random\nWalk with Restart (RWR) and PageRank (PGR) algorithms for negative sampling and\nemploys a Determinantal Point Process (DPP)-based GCN for convolution\noperations. RWR leverages both global and local information to manage noise and\nlocal variations, while PGR assesses node importance to stabilize the\ntopological structure. The DPP-based GCN ensures diversity among negative\nsamples and aggregates their features to produce robust node embeddings,\nthereby improving classification performance. Experimental results demonstrate\nthat the RW-NSGCN model effectively addresses network topology attacks and\nweight instability, increasing the accuracy of anomaly detection and overall\nstability. In terms of classification accuracy, RW-NSGCN significantly\noutperforms existing methods, showing greater resilience across various\nscenarios and effectively mitigating the impact of such vulnerabilities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06665v1",
    "published_date": "2024-08-13 06:34:56 UTC",
    "updated_date": "2024-08-13 06:34:56 UTC"
  },
  {
    "arxiv_id": "2408.06663v5",
    "title": "Amuro and Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models",
    "authors": [
      "Kaiser Sun",
      "Mark Dredze"
    ],
    "abstract": "The development of large language models leads to the formation of a\npre-train-then-align paradigm, in which the model is typically pre-trained on a\nlarge text corpus and undergoes a tuning stage to align the model with human\npreference or downstream tasks. In this work, we investigate the relationship\nbetween pre-training and fine-tuning by fine-tuning multiple intermediate\npre-trained model checkpoints. Our results on 18 datasets suggest that i)\ncontinual pre-training improves the model in a latent way that unveils after\nfine-tuning; ii) with extra fine-tuning, the datasets that the model does not\ndemonstrate capability gain much more than those that the model performs well\nduring the pre-training stage; iii) although model benefits significantly\nthrough supervised fine-tuning, it may forget previously known domain knowledge\nand the tasks that are not seen during fine-tuning; iv) the model resembles\nhigh sensitivity to evaluation prompts after supervised fine-tuning, but this\nsensitivity can be alleviated by more pre-training.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Rep4NLP Camera Ready",
    "pdf_url": "http://arxiv.org/pdf/2408.06663v5",
    "published_date": "2024-08-13 06:28:43 UTC",
    "updated_date": "2025-03-18 16:21:04 UTC"
  },
  {
    "arxiv_id": "2408.06653v3",
    "title": "Hierarchical Structured Neural Network: Efficient Retrieval Scaling for Large Scale Recommendation",
    "authors": [
      "Kaushik Rangadurai",
      "Siyang Yuan",
      "Minhui Huang",
      "Yiqun Liu",
      "Golnaz Ghasemiesfeh",
      "Yunchen Pu",
      "Haiyu Lu",
      "Xingfeng He",
      "Fangzhou Xu",
      "Andrew Cui",
      "Vidhoon Viswanathan",
      "Lin Yang",
      "Liang Wang",
      "Jiyan Yang",
      "Chonglin Sun"
    ],
    "abstract": "Retrieval, the initial stage of a recommendation system, is tasked with\ndown-selecting items from a pool of tens of millions of candidates to a few\nthousands. Embedding Based Retrieval (EBR) has been a typical choice for this\nproblem, addressing the computational demands of deep neural networks across\nvast item corpora. EBR utilizes Two Tower or Siamese Networks to learn\nrepresentations for users and items, and employ Approximate Nearest Neighbor\n(ANN) search to efficiently retrieve relevant items. Despite its popularity in\nindustry, EBR faces limitations. The Two Tower architecture, relying on a\nsingle dot product interaction, struggles to capture complex data distributions\ndue to limited capability in learning expressive interactions between users and\nitems. Additionally, ANN index building and representation learning for user\nand item are often separate, leading to inconsistencies exacerbated by\nrepresentation (e.g. continuous online training) and item drift (e.g. items\nexpired and new items added). In this paper, we introduce the Hierarchical\nStructured Neural Network (HSNN), an efficient deep neural network model to\nlearn intricate user and item interactions beyond the commonly used dot product\nin retrieval tasks, achieving sublinear computational costs relative to corpus\nsize. A Modular Neural Network (MoNN) is designed to maintain high\nexpressiveness for interaction learning while ensuring efficiency. A mixture of\nMoNNs operate on a hierarchical item index to achieve extensive computation\nsharing, enabling it to scale up to large corpus size. MoNN and the\nhierarchical index are jointly learnt to continuously adapt to distribution\nshifts in both user interests and item distributions. HSNN achieves substantial\nimprovement in offline evaluation compared to prevailing methods.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Resubmit",
    "pdf_url": "http://arxiv.org/pdf/2408.06653v3",
    "published_date": "2024-08-13 05:53:46 UTC",
    "updated_date": "2025-01-08 20:40:09 UTC"
  },
  {
    "arxiv_id": "2408.14478v1",
    "title": "Uncertainty Quantification in Alzheimer's Disease Progression Modeling",
    "authors": [
      "Wael Mobeirek",
      "Shirley Mao"
    ],
    "abstract": "With the increasing number of patients diagnosed with Alzheimer's Disease,\nprognosis models have the potential to aid in early disease detection. However,\ncurrent approaches raise dependability concerns as they do not account for\nuncertainty. In this work, we compare the performance of Monte Carlo Dropout,\nVariational Inference, Markov Chain Monte Carlo, and Ensemble Learning trained\non 512 patients to predict 4-year cognitive score trajectories with confidence\nbounds. We show that MC Dropout and MCMC are able to produce well-calibrated,\nand accurate predictions under noisy training data.",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CY",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "q-bio.NC",
    "comment": "This work was done as part of degree requirements for the authors in\n  2021-2022",
    "pdf_url": "http://arxiv.org/pdf/2408.14478v1",
    "published_date": "2024-08-13 05:53:34 UTC",
    "updated_date": "2024-08-13 05:53:34 UTC"
  },
  {
    "arxiv_id": "2408.06634v2",
    "title": "Harnessing Earnings Reports for Stock Predictions: A QLoRA-Enhanced LLM Approach",
    "authors": [
      "Haowei Ni",
      "Shuchen Meng",
      "Xupeng Chen",
      "Ziqing Zhao",
      "Andi Chen",
      "Panfeng Li",
      "Shiyao Zhang",
      "Qifu Yin",
      "Yuanqing Wang",
      "Yuxi Chan"
    ],
    "abstract": "Accurate stock market predictions following earnings reports are crucial for\ninvestors. Traditional methods, particularly classical machine learning models,\nstruggle with these predictions because they cannot effectively process and\ninterpret extensive textual data contained in earnings reports and often\noverlook nuances that influence market movements. This paper introduces an\nadvanced approach by employing Large Language Models (LLMs) instruction\nfine-tuned with a novel combination of instruction-based techniques and\nquantized low-rank adaptation (QLoRA) compression. Our methodology integrates\n'base factors', such as financial metric growth and earnings transcripts, with\n'external factors', including recent market indices performances and analyst\ngrades, to create a rich, supervised dataset. This comprehensive dataset\nenables our models to achieve superior predictive performance in terms of\naccuracy, weighted F1, and Matthews correlation coefficient (MCC), especially\nevident in the comparison with benchmarks such as GPT-4. We specifically\nhighlight the efficacy of the llama-3-8b-Instruct-4bit model, which showcases\nsignificant improvements over baseline models. The paper also discusses the\npotential of expanding the output capabilities to include a 'Hold' option and\nextending the prediction horizon, aiming to accommodate various investment\nstyles and time frames. This study not only demonstrates the power of\nintegrating cutting-edge AI with fine-tuned financial data but also paves the\nway for future research in enhancing AI-driven financial analysis tools.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "q-fin.ST"
    ],
    "primary_category": "q-fin.CP",
    "comment": "Accepted by 2024 6th International Conference on Data-driven\n  Optimization of Complex Systems",
    "pdf_url": "http://arxiv.org/pdf/2408.06634v2",
    "published_date": "2024-08-13 04:53:31 UTC",
    "updated_date": "2024-11-12 07:52:33 UTC"
  },
  {
    "arxiv_id": "2408.06632v1",
    "title": "EditScribe: Non-Visual Image Editing with Natural Language Verification Loops",
    "authors": [
      "Ruei-Che Chang",
      "Yuxuan Liu",
      "Lotus Zhang",
      "Anhong Guo"
    ],
    "abstract": "Image editing is an iterative process that requires precise visual evaluation\nand manipulation for the output to match the editing intent. However, current\nimage editing tools do not provide accessible interaction nor sufficient\nfeedback for blind and low vision individuals to achieve this level of control.\nTo address this, we developed EditScribe, a prototype system that makes image\nediting accessible using natural language verification loops powered by large\nmultimodal models. Using EditScribe, the user first comprehends the image\ncontent through initial general and object descriptions, then specifies edit\nactions using open-ended natural language prompts. EditScribe performs the\nimage edit, and provides four types of verification feedback for the user to\nverify the performed edit, including a summary of visual changes, AI judgement,\nand updated general and object descriptions. The user can ask follow-up\nquestions to clarify and probe into the edits or verification feedback, before\nperforming another edit. In a study with ten blind or low-vision users, we\nfound that EditScribe supported participants to perform and verify image edit\nactions non-visually. We observed different prompting strategies from\nparticipants, and their perceptions on the various types of verification\nfeedback. Finally, we discuss the implications of leveraging natural language\nverification loops to make visual authoring non-visually accessible.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "ASSETS 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.06632v1",
    "published_date": "2024-08-13 04:40:56 UTC",
    "updated_date": "2024-08-13 04:40:56 UTC"
  },
  {
    "arxiv_id": "2408.11849v1",
    "title": "Style-Talker: Finetuning Audio Language Model and Style-Based Text-to-Speech Model for Fast Spoken Dialogue Generation",
    "authors": [
      "Yinghao Aaron Li",
      "Xilin Jiang",
      "Jordan Darefsky",
      "Ge Zhu",
      "Nima Mesgarani"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has significantly\npropelled the development of text-based chatbots, demonstrating their\ncapability to engage in coherent and contextually relevant dialogues. However,\nextending these advancements to enable end-to-end speech-to-speech conversation\nbots remains a formidable challenge, primarily due to the extensive dataset and\ncomputational resources required. The conventional approach of cascading\nautomatic speech recognition (ASR), LLM, and text-to-speech (TTS) models in a\npipeline, while effective, suffers from unnatural prosody because it lacks\ndirect interactions between the input audio and its transcribed text and the\noutput audio. These systems are also limited by their inherent latency from the\nASR process for real-time applications. This paper introduces Style-Talker, an\ninnovative framework that fine-tunes an audio LLM alongside a style-based TTS\nmodel for fast spoken dialog generation. Style-Talker takes user input audio\nand uses transcribed chat history and speech styles to generate both the\nspeaking style and text for the response. Subsequently, the TTS model\nsynthesizes the speech, which is then played back to the user. While the\nresponse speech is being played, the input speech undergoes ASR processing to\nextract the transcription and speaking style, serving as the context for the\nensuing dialogue turn. This novel pipeline accelerates the traditional cascade\nASR-LLM-TTS systems while integrating rich paralinguistic information from\ninput speech. Our experimental results show that Style-Talker significantly\noutperforms the conventional cascade and speech-to-speech baselines in terms of\nboth dialogue naturalness and coherence while being more than 50% faster.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "CoLM 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.11849v1",
    "published_date": "2024-08-13 04:35:11 UTC",
    "updated_date": "2024-08-13 04:35:11 UTC"
  },
  {
    "arxiv_id": "2408.06627v1",
    "title": "WorldScribe: Towards Context-Aware Live Visual Descriptions",
    "authors": [
      "Ruei-Che Chang",
      "Yuxuan Liu",
      "Anhong Guo"
    ],
    "abstract": "Automated live visual descriptions can aid blind people in understanding\ntheir surroundings with autonomy and independence. However, providing\ndescriptions that are rich, contextual, and just-in-time has been a\nlong-standing challenge in accessibility. In this work, we develop WorldScribe,\na system that generates automated live real-world visual descriptions that are\ncustomizable and adaptive to users' contexts: (i) WorldScribe's descriptions\nare tailored to users' intents and prioritized based on semantic relevance.\n(ii) WorldScribe is adaptive to visual contexts, e.g., providing consecutively\nsuccinct descriptions for dynamic scenes, while presenting longer and detailed\nones for stable settings. (iii) WorldScribe is adaptive to sound contexts,\ne.g., increasing volume in noisy environments, or pausing when conversations\nstart. Powered by a suite of vision, language, and sound recognition models,\nWorldScribe introduces a description generation pipeline that balances the\ntradeoffs between their richness and latency to support real-time use. The\ndesign of WorldScribe is informed by prior work on providing visual\ndescriptions and a formative study with blind participants. Our user study and\nsubsequent pipeline evaluation show that WorldScribe can provide real-time and\nfairly accurate visual descriptions to facilitate environment understanding\nthat is adaptive and customized to users' contexts. Finally, we discuss the\nimplications and further steps toward making live visual descriptions more\ncontext-aware and humanized.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "UIST 2024",
    "pdf_url": "http://arxiv.org/pdf/2408.06627v1",
    "published_date": "2024-08-13 04:32:45 UTC",
    "updated_date": "2024-08-13 04:32:45 UTC"
  },
  {
    "arxiv_id": "2408.06618v1",
    "title": "Generalized knowledge-enhanced framework for biomedical entity and relation extraction",
    "authors": [
      "Minh Nguyen",
      "Phuong Le"
    ],
    "abstract": "In recent years, there has been an increasing number of frameworks developed\nfor biomedical entity and relation extraction. This research effort aims to\naddress the accelerating growth in biomedical publications and the intricate\nnature of biomedical texts, which are written for mainly domain experts. To\nhandle these challenges, we develop a novel framework that utilizes external\nknowledge to construct a task-independent and reusable background knowledge\ngraph for biomedical entity and relation extraction. The design of our model is\ninspired by how humans learn domain-specific topics. In particular, humans\noften first acquire the most basic and common knowledge regarding a field to\nbuild the foundational knowledge and then use that as a basis for extending to\nvarious specialized topics. Our framework employs such common-knowledge-sharing\nmechanism to build a general neural-network knowledge graph that is learning\ntransferable to different domain-specific biomedical texts effectively.\nExperimental evaluations demonstrate that our model, equipped with this\ngeneralized and cross-transferable knowledge base, achieves competitive\nperformance benchmarks, including BioRelEx for binding interaction detection\nand ADE for Adverse Drug Effect identification.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06618v1",
    "published_date": "2024-08-13 04:06:45 UTC",
    "updated_date": "2024-08-13 04:06:45 UTC"
  },
  {
    "arxiv_id": "2408.06603v1",
    "title": "Simple but Effective Compound Geometric Operations for Temporal Knowledge Graph Completion",
    "authors": [
      "Rui Ying",
      "Mengting Hu",
      "Jianfeng Wu",
      "Yalan Xie",
      "Xiaoyi Liu",
      "Zhunheng Wang",
      "Ming Jiang",
      "Hang Gao",
      "Linlin Zhang",
      "Renhong Cheng"
    ],
    "abstract": "Temporal knowledge graph completion aims to infer the missing facts in\ntemporal knowledge graphs. Current approaches usually embed factual knowledge\ninto continuous vector space and apply geometric operations to learn potential\npatterns in temporal knowledge graphs. However, these methods only adopt a\nsingle operation, which may have limitations in capturing the complex temporal\ndynamics present in temporal knowledge graphs. Therefore, we propose a simple\nbut effective method, i.e. TCompoundE, which is specially designed with two\ngeometric operations, including time-specific and relation-specific operations.\nWe provide mathematical proofs to demonstrate the ability of TCompoundE to\nencode various relation patterns. Experimental results show that our proposed\nmodel significantly outperforms existing temporal knowledge graph embedding\nmodels. Our code is available at https://github.com/nk-ruiying/TCompoundE.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06603v1",
    "published_date": "2024-08-13 03:36:30 UTC",
    "updated_date": "2024-08-13 03:36:30 UTC"
  },
  {
    "arxiv_id": "2408.06602v3",
    "title": "Super-intelligence or Superstition? Exploring Psychological Factors Influencing Belief in AI Predictions about Personal Behavior",
    "authors": [
      "Eunhae Lee",
      "Pat Pataranutaporn",
      "Judith Amores",
      "Pattie Maes"
    ],
    "abstract": "Could belief in AI predictions be just another form of superstition? This\nstudy investigates psychological factors that influence belief in AI\npredictions about personal behavior, comparing it to belief in astrology- and\npersonality-based predictions. Through an experiment with 238 participants, we\nexamined how cognitive style, paranormal beliefs, AI attitudes, personality\ntraits, and other factors affect perceived validity, reliability, usefulness,\nand personalization of predictions from different sources. Our findings reveal\nthat belief in AI predictions is positively correlated with belief in\npredictions based on astrology and personality psychology. Notably, paranormal\nbeliefs and positive attitudes about AI significantly increased perceived\nvalidity, reliability, usefulness, and personalization of AI predictions.\nConscientiousness was negatively correlated with belief in predictions across\nall sources, and interest in the prediction topic increased believability\nacross predictions. Surprisingly, we found no evidence that cognitive style has\nan impact on belief in fictitious AI-generated predictions. These results\nhighlight the \"rational superstition\" phenomenon in AI, where belief is driven\nmore by mental heuristics and intuition than critical evaluation. This research\nadvances our understanding of the psychology of human-AI interaction, offering\ninsights into designing and promoting AI systems that foster appropriate trust\nand skepticism, critical for responsible integration in an increasingly\nAI-driven world.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06602v3",
    "published_date": "2024-08-13 03:35:26 UTC",
    "updated_date": "2024-12-19 00:50:44 UTC"
  },
  {
    "arxiv_id": "2408.06598v1",
    "title": "A Perspective on Large Language Models, Intelligent Machines, and Knowledge Acquisition",
    "authors": [
      "Vladimir Cherkassky",
      "Eng Hock Lee"
    ],
    "abstract": "Large Language Models (LLMs) are known for their remarkable ability to\ngenerate synthesized 'knowledge', such as text documents, music, images, etc.\nHowever, there is a huge gap between LLM's and human capabilities for\nunderstanding abstract concepts and reasoning. We discuss these issues in a\nlarger philosophical context of human knowledge acquisition and the Turing\ntest. In addition, we illustrate the limitations of LLMs by analyzing GPT-4\nresponses to questions ranging from science and math to common sense reasoning.\nThese examples show that GPT-4 can often imitate human reasoning, even though\nit lacks understanding. However, LLM responses are synthesized from a large LLM\nmodel trained on all available data. In contrast, human understanding is based\non a small number of abstract concepts. Based on this distinction, we discuss\nthe impact of LLMs on acquisition of human knowledge and education.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06598v1",
    "published_date": "2024-08-13 03:25:49 UTC",
    "updated_date": "2024-08-13 03:25:49 UTC"
  },
  {
    "arxiv_id": "2408.06569v1",
    "title": "Social Debiasing for Fair Multi-modal LLMs",
    "authors": [
      "Harry Cheng",
      "Yangyang Guo",
      "Qingpei Guo",
      "Ming Yang",
      "Tian Gan",
      "Liqiang Nie"
    ],
    "abstract": "Multi-modal Large Language Models (MLLMs) have advanced significantly,\noffering powerful vision-language understanding capabilities. However, these\nmodels often inherit severe social biases from their training datasets, leading\nto unfair predictions based on attributes like race and gender. This paper\naddresses the issue of social biases in MLLMs by i) Introducing a comprehensive\nCounterfactual dataset with Multiple Social Concepts (CMSC), which provides a\nmore diverse and extensive training set compared to existing datasets. ii)\nProposing an Anti-Stereotype Debiasing strategy (ASD). Our method works by\nrevisiting the MLLM training process, rescaling the autoregressive loss\nfunction, and improving data sampling methods to counteract biases. Through\nextensive experiments on various MLLMs, our CMSC dataset and ASD method\ndemonstrate a significant reduction in social biases while maintaining the\nmodels' original performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06569v1",
    "published_date": "2024-08-13 02:08:32 UTC",
    "updated_date": "2024-08-13 02:08:32 UTC"
  },
  {
    "arxiv_id": "2408.06567v1",
    "title": "AquilaMoE: Efficient Training for MoE Models with Scale-Up and Scale-Out Strategies",
    "authors": [
      "Bo-Wen Zhang",
      "Liangdong Wang",
      "Ye Yuan",
      "Jijie Li",
      "Shuhao Gu",
      "Mengdi Zhao",
      "Xinya Wu",
      "Guang Liu",
      "Chengwei Wu",
      "Hanyu Zhao",
      "Li Du",
      "Yiming Ju",
      "Quanyue Ma",
      "Yulong Ao",
      "Yingli Zhao",
      "Songhe Zhu",
      "Zhou Cao",
      "Dong Liang",
      "Yonghua Lin",
      "Ming Zhang",
      "Shunfei Wang",
      "Yanxin Zhou",
      "Min Ye",
      "Xuekai Chen",
      "Xinyang Yu",
      "Xiangjun Huang",
      "Jian Yang"
    ],
    "abstract": "In recent years, with the rapid application of large language models across\nvarious fields, the scale of these models has gradually increased, and the\nresources required for their pre-training have grown exponentially. Training an\nLLM from scratch will cost a lot of computation resources while scaling up from\na smaller model is a more efficient approach and has thus attracted significant\nattention. In this paper, we present AquilaMoE, a cutting-edge bilingual 8*16B\nMixture of Experts (MoE) language model that has 8 experts with 16 billion\nparameters each and is developed using an innovative training methodology\ncalled EfficientScale. This approach optimizes performance while minimizing\ndata requirements through a two-stage process. The first stage, termed\nScale-Up, initializes the larger model with weights from a pre-trained smaller\nmodel, enabling substantial knowledge transfer and continuous pretraining with\nsignificantly less data. The second stage, Scale-Out, uses a pre-trained dense\nmodel to initialize the MoE experts, further enhancing knowledge transfer and\nperformance. Extensive validation experiments on 1.8B and 7B models compared\nvarious initialization schemes, achieving models that maintain and reduce loss\nduring continuous pretraining. Utilizing the optimal scheme, we successfully\ntrained a 16B model and subsequently the 8*16B AquilaMoE model, demonstrating\nsignificant improvements in performance and training efficiency.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06567v1",
    "published_date": "2024-08-13 02:07:00 UTC",
    "updated_date": "2024-08-13 02:07:00 UTC"
  },
  {
    "arxiv_id": "2408.11848v2",
    "title": "MGH Radiology Llama: A Llama 3 70B Model for Radiology",
    "authors": [
      "Yucheng Shi",
      "Peng Shu",
      "Zhengliang Liu",
      "Zihao Wu",
      "Quanzheng Li",
      "Tianming Liu",
      "Ninghao Liu",
      "Xiang Li"
    ],
    "abstract": "In recent years, the field of radiology has increasingly harnessed the power\nof artificial intelligence (AI) to enhance diagnostic accuracy, streamline\nworkflows, and improve patient care. Large language models (LLMs) have emerged\nas particularly promising tools, offering significant potential in assisting\nradiologists with report generation, clinical decision support, and patient\ncommunication. This paper presents an advanced radiology-focused large language\nmodel: MGH Radiology Llama. It is developed using the Llama 3 70B model,\nbuilding upon previous domain-specific models like Radiology-GPT and\nRadiology-Llama2. Leveraging a unique and comprehensive dataset from\nMassachusetts General Hospital, comprising over 6.5 million de-identified\nmedical reports across various imaging modalities, the model demonstrates\nsignificant improvements in generating accurate and clinically relevant\nradiology impressions given the corresponding findings. Our evaluation,\nincorporating both traditional metrics and a GPT-4-based assessment, highlights\nthe enhanced performance of this work over general-purpose LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "11 pages, 3 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2408.11848v2",
    "published_date": "2024-08-13 01:30:03 UTC",
    "updated_date": "2024-12-16 18:25:26 UTC"
  },
  {
    "arxiv_id": "2408.06543v3",
    "title": "HDRGS: High Dynamic Range Gaussian Splatting",
    "authors": [
      "Jiahao Wu",
      "Lu Xiao",
      "Rui Peng",
      "Kaiqiang Xiong",
      "Ronggang Wang"
    ],
    "abstract": "Recent years have witnessed substantial advancements in the field of 3D\nreconstruction from 2D images, particularly following the introduction of the\nneural radiance field (NeRF) technique. However, reconstructing a 3D high\ndynamic range (HDR) radiance field, which aligns more closely with real-world\nconditions, from 2D multi-exposure low dynamic range (LDR) images continues to\npose significant challenges. Approaches to this issue fall into two categories:\ngrid-based and implicit-based. Implicit methods, using multi-layer perceptrons\n(MLP), face inefficiencies, limited solvability, and overfitting risks.\nConversely, grid-based methods require significant memory and struggle with\nimage quality and long training times. In this paper, we introduce Gaussian\nSplatting-a recent, high-quality, real-time 3D reconstruction technique-into\nthis domain. We further develop the High Dynamic Range Gaussian Splatting\n(HDR-GS) method, designed to address the aforementioned challenges. This method\nenhances color dimensionality by including luminance and uses an asymmetric\ngrid for tone-mapping, swiftly and precisely converting pixel irradiance to\ncolor. Our approach improves HDR scene recovery accuracy and integrates a novel\ncoarse-to-fine strategy to speed up model convergence, enhancing robustness\nagainst sparse viewpoints and exposure extremes, and preventing local optima.\nExtensive testing confirms that our method surpasses current state-of-the-art\ntechniques in both synthetic and real-world scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06543v3",
    "published_date": "2024-08-13 00:32:36 UTC",
    "updated_date": "2024-11-03 11:15:53 UTC"
  },
  {
    "arxiv_id": "2408.06542v1",
    "title": "Value of Information and Reward Specification in Active Inference and POMDPs",
    "authors": [
      "Ran Wei"
    ],
    "abstract": "Expected free energy (EFE) is a central quantity in active inference which\nhas recently gained popularity due to its intuitive decomposition of the\nexpected value of control into a pragmatic and an epistemic component. While\nnumerous conjectures have been made to justify EFE as a decision making\nobjective function, the most widely accepted is still its intuitiveness and\nresemblance to variational free energy in approximate Bayesian inference. In\nthis work, we take a bottom up approach and ask: taking EFE as given, what's\nthe resulting agent's optimality gap compared with a reward-driven\nreinforcement learning (RL) agent, which is well understood? By casting EFE\nunder a particular class of belief MDP and using analysis tools from RL theory,\nwe show that EFE approximates the Bayes optimal RL policy via information\nvalue. We discuss the implications for objective specification of active\ninference agents.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2408.06542v1",
    "published_date": "2024-08-13 00:32:05 UTC",
    "updated_date": "2024-08-13 00:32:05 UTC"
  },
  {
    "arxiv_id": "2408.06540v1",
    "title": "Dynamic Exclusion of Low-Fidelity Data in Bayesian Optimization for Autonomous Beamline Alignment",
    "authors": [
      "Megha R. Narayanan",
      "Thomas W. Morris"
    ],
    "abstract": "Aligning beamlines at synchrotron light sources is a high-dimensional,\nexpensive-to-sample optimization problem, as beams are focused using a series\nof dynamic optical components. Bayesian Optimization is an efficient machine\nlearning approach to finding global optima of beam quality, but the model can\neasily be impaired by faulty data points caused by the beam going off the edge\nof the sensor or by background noise. This study, conducted at the National\nSynchrotron Light Source II (NSLS-II) facility at Brookhaven National\nLaboratory (BNL), is an investigation of methods to identify untrustworthy\nreadings of beam quality and discourage the optimization model from seeking out\npoints likely to yield low-fidelity beams. The approaches explored include\ndynamic pruning using loss analysis of size and position models and a\nlengthscale-based genetic algorithm to determine which points to include in the\nmodel for optimal fit. Each method successfully classified high and low\nfidelity points. This research advances BNL's mission to tackle our nation's\nenergy challenges by providing scientists at all beamlines with access to\nhigher quality beams, and faster convergence to these optima for their\nexperiments.",
    "categories": [
      "physics.acc-ph",
      "cs.AI",
      "cs.LG",
      "I.2.8; I.2.9; J.2"
    ],
    "primary_category": "physics.acc-ph",
    "comment": "12 pages, 6 figure sets",
    "pdf_url": "http://arxiv.org/pdf/2408.06540v1",
    "published_date": "2024-08-13 00:20:39 UTC",
    "updated_date": "2024-08-13 00:20:39 UTC"
  }
]
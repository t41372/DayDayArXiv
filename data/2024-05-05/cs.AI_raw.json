[
  {
    "arxiv_id": "2405.03069v2",
    "title": "On Probabilistic and Causal Reasoning with Summation Operators",
    "authors": [
      "Duligur Ibeling",
      "Thomas F. Icard",
      "Milan Mossé"
    ],
    "abstract": "Ibeling et al. (2023). axiomatize increasingly expressive languages of\ncausation and probability, and Mosse et al. (2024) show that reasoning\n(specifically the satisfiability problem) in each causal language is as\ndifficult, from a computational complexity perspective, as reasoning in its\nmerely probabilistic or \"correlational\" counterpart. Introducing a summation\noperator to capture common devices that appear in applications -- such as the\n$do$-calculus of Pearl (2009) for causal inference, which makes ample use of\nmarginalization -- van der Zander et al. (2023) partially extend these earlier\ncomplexity results to causal and probabilistic languages with marginalization.\nWe complete this extension, fully characterizing the complexity of\nprobabilistic and causal reasoning with summation, demonstrating that these\nagain remain equally difficult. Surprisingly, allowing free variables for\nrandom variable values results in a system that is undecidable, so long as the\nranges of these random variables are unrestricted. We finally axiomatize these\nlanguages featuring marginalization (or more generally summation), resolving\nopen questions posed by Ibeling et al. (2023).",
    "categories": [
      "math.LO",
      "cs.AI",
      "cs.CC",
      "cs.LO"
    ],
    "primary_category": "math.LO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.03069v2",
    "published_date": "2024-05-05 22:32:01 UTC",
    "updated_date": "2024-05-18 20:14:42 UTC"
  },
  {
    "arxiv_id": "2405.03064v3",
    "title": "RICE: Breaking Through the Training Bottlenecks of Reinforcement Learning with Explanation",
    "authors": [
      "Zelei Cheng",
      "Xian Wu",
      "Jiahao Yu",
      "Sabrina Yang",
      "Gang Wang",
      "Xinyu Xing"
    ],
    "abstract": "Deep reinforcement learning (DRL) is playing an increasingly important role\nin real-world applications. However, obtaining an optimally performing DRL\nagent for complex tasks, especially with sparse rewards, remains a significant\nchallenge. The training of a DRL agent can be often trapped in a bottleneck\nwithout further progress. In this paper, we propose RICE, an innovative\nrefining scheme for reinforcement learning that incorporates explanation\nmethods to break through the training bottlenecks. The high-level idea of RICE\nis to construct a new initial state distribution that combines both the default\ninitial states and critical states identified through explanation methods,\nthereby encouraging the agent to explore from the mixed initial states. Through\ncareful design, we can theoretically guarantee that our refining scheme has a\ntighter sub-optimality bound. We evaluate RICE in various popular RL\nenvironments and real-world applications. The results demonstrate that RICE\nsignificantly outperforms existing refining schemes in enhancing agent\nperformance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.03064v3",
    "published_date": "2024-05-05 22:06:42 UTC",
    "updated_date": "2024-06-06 00:36:03 UTC"
  },
  {
    "arxiv_id": "2407.01558v2",
    "title": "Visual grounding for desktop graphical user interfaces",
    "authors": [
      "Tassnim Dardouri",
      "Laura Minkova",
      "Jessica López Espejel",
      "Walid Dahhane",
      "El Hassane Ettifouri"
    ],
    "abstract": "Most instance perception and image understanding solutions focus mainly on\nnatural images. However, applications for synthetic images, and more\nspecifically, images of Graphical User Interfaces (GUI) remain limited. This\nhinders the development of autonomous computer-vision-powered Artificial\nIntelligence (AI) agents. In this work, we present Instruction Visual Grounding\nor IVG, a multi-modal solution for object identification in a GUI. More\nprecisely, given a natural language instruction and GUI screen, IVG locates the\ncoordinates of the element on the screen where the instruction would be\nexecuted. To this end, we develop two methods. The first method is a three-part\narchitecture that relies on a combination of a Large Language Model (LLM) and\nan object detection model. The second approach uses a multi-modal foundation\nmodel.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "Preprint submitted to Computer Vision and Image Understanding journal",
    "pdf_url": "http://arxiv.org/pdf/2407.01558v2",
    "published_date": "2024-05-05 19:10:19 UTC",
    "updated_date": "2024-09-17 10:15:07 UTC"
  },
  {
    "arxiv_id": "2405.06682v3",
    "title": "Self-Reflection in LLM Agents: Effects on Problem-Solving Performance",
    "authors": [
      "Matthew Renze",
      "Erhan Guven"
    ],
    "abstract": "In this study, we investigated the effects of self-reflection in large\nlanguage models (LLMs) on problem-solving performance. We instructed nine\npopular LLMs to answer a series of multiple-choice questions to provide a\nperformance baseline. For each incorrectly answered question, we instructed\neight types of self-reflecting LLM agents to reflect on their mistakes and\nprovide themselves with guidance to improve problem-solving. Then, using this\nguidance, each self-reflecting agent attempted to re-answer the same questions.\nOur results indicate that LLM agents are able to significantly improve their\nproblem-solving performance through self-reflection ($p < 0.001$). In addition,\nwe compared the various types of self-reflection to determine their individual\ncontribution to performance. All code and data are available on GitHub at\nhttps://github.com/matthewrenze/self-reflection",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.06682v3",
    "published_date": "2024-05-05 18:56:46 UTC",
    "updated_date": "2024-10-16 23:19:46 UTC"
  },
  {
    "arxiv_id": "2405.04550v1",
    "title": "Exploring a Cognitive Architecture for Learning Arithmetic Equations",
    "authors": [
      "Cole Gawin"
    ],
    "abstract": "The acquisition and performance of arithmetic skills and basic operations\nsuch as addition, subtraction, multiplication, and division are essential for\ndaily functioning, and reflect complex cognitive processes. This paper explores\nthe cognitive mechanisms powering arithmetic learning, presenting a\nneurobiologically plausible cognitive architecture that simulates the\nacquisition of these skills. I implement a number vectorization embedding\nnetwork and an associative memory model to investigate how an intelligent\nsystem can learn and recall arithmetic equations in a manner analogous to the\nhuman brain. I perform experiments that provide insights into the\ngeneralization capabilities of connectionist models, neurological causes of\ndyscalculia, and the influence of network architecture on cognitive\nperformance. Through this interdisciplinary investigation, I aim to contribute\nto ongoing research into the neural correlates of mathematical cognition in\nintelligent systems.",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "primary_category": "q-bio.NC",
    "comment": "16 pages, 6 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.04550v1",
    "published_date": "2024-05-05 18:42:00 UTC",
    "updated_date": "2024-05-05 18:42:00 UTC"
  },
  {
    "arxiv_id": "2405.06681v1",
    "title": "Leveraging Lecture Content for Improved Feedback: Explorations with GPT-4 and Retrieval Augmented Generation",
    "authors": [
      "Sven Jacobs",
      "Steffen Jaschke"
    ],
    "abstract": "This paper presents the use of Retrieval Augmented Generation (RAG) to\nimprove the feedback generated by Large Language Models for programming tasks.\nFor this purpose, corresponding lecture recordings were transcribed and made\navailable to the Large Language Model GPT-4 as external knowledge source\ntogether with timestamps as metainformation by using RAG. The purpose of this\nis to prevent hallucinations and to enforce the use of the technical terms and\nphrases from the lecture. In an exercise platform developed to solve\nprogramming problems for an introductory programming lecture, students can\nrequest feedback on their solutions generated by GPT-4. For this task GPT-4\nreceives the students' code solution, the compiler output, the result of unit\ntests and the relevant passages from the lecture notes available through the\nuse of RAG as additional context. The feedback generated by GPT-4 should guide\nstudents to solve problems independently and link to the lecture content, using\nthe time stamps of the transcript as meta-information. In this way, the\ncorresponding lecture videos can be viewed immediately at the corresponding\npositions. For the evaluation, students worked with the tool in a workshop and\ndecided for each feedback whether it should be extended by RAG or not. First\nresults based on a questionnaire and the collected usage data show that the use\nof RAG can improve feedback generation and is preferred by students in some\nsituations. Due to the slower speed of feedback generation, the benefits are\nsituation dependent.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "comment": "accepted at CSEE&T 2024: 36th International Conference on Software\n  Engineering Education and Training, W\\\"urzburg, Germany",
    "pdf_url": "http://arxiv.org/pdf/2405.06681v1",
    "published_date": "2024-05-05 18:32:06 UTC",
    "updated_date": "2024-05-05 18:32:06 UTC"
  },
  {
    "arxiv_id": "2405.03011v1",
    "title": "AC-MAMBASEG: An adaptive convolution and Mamba-based architecture for enhanced skin lesion segmentation",
    "authors": [
      "Viet-Thanh Nguyen",
      "Van-Truong Pham",
      "Thi-Thao Tran"
    ],
    "abstract": "Skin lesion segmentation is a critical task in computer-aided diagnosis\nsystems for dermatological diseases. Accurate segmentation of skin lesions from\nmedical images is essential for early detection, diagnosis, and treatment\nplanning. In this paper, we propose a new model for skin lesion segmentation\nnamely AC-MambaSeg, an enhanced model that has the hybrid CNN-Mamba backbone,\nand integrates advanced components such as Convolutional Block Attention Module\n(CBAM), Attention Gate, and Selective Kernel Bottleneck. AC-MambaSeg leverages\nthe Vision Mamba framework for efficient feature extraction, while CBAM and\nSelective Kernel Bottleneck enhance its ability to focus on informative regions\nand suppress background noise. We evaluate the performance of AC-MambaSeg on\ndiverse datasets of skin lesion images including ISIC-2018 and PH2; then\ncompare it against existing segmentation methods. Our model shows promising\npotential for improving computer-aided diagnosis systems and facilitating early\ndetection and treatment of dermatological diseases. Our source code will be\nmade available at: https://github.com/vietthanh2710/AC-MambaSeg.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "15 pages, 7 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.03011v1",
    "published_date": "2024-05-05 17:37:50 UTC",
    "updated_date": "2024-05-05 17:37:50 UTC"
  },
  {
    "arxiv_id": "2405.03010v1",
    "title": "High Order Reasoning for Time Critical Recommendation in Evidence-based Medicine",
    "authors": [
      "Manjiang Yu",
      "Xue Li"
    ],
    "abstract": "In time-critical decisions, human decision-makers can interact with\nAI-enabled situation-aware software to evaluate many imminent and possible\nscenarios, retrieve billions of facts, and estimate different outcomes based on\ntrillions of parameters in a fraction of a second. In high-order reasoning,\n\"what-if\" questions can be used to challenge the assumptions or pre-conditions\nof the reasoning, \"why-not\" questions can be used to challenge on the method\napplied in the reasoning, \"so-what\" questions can be used to challenge the\npurpose of the decision, and \"how-about\" questions can be used to challenge the\napplicability of the method. When above high-order reasoning questions are\napplied to assist human decision-making, it can help humans to make\ntime-critical decisions and avoid false-negative or false-positive types of\nerrors. In this paper, we present a model of high-order reasoning to offer\nrecommendations in evidence-based medicine in a time-critical fashion for the\napplications in ICU. The Large Language Model (LLM) is used in our system. The\nexperiments demonstrated the LLM exhibited optimal performance in the \"What-if\"\nscenario, achieving a similarity of 88.52% with the treatment plans of human\ndoctors. In the \"Why-not\" scenario, the best-performing model tended to opt for\nalternative treatment plans in 70% of cases for patients who died after being\ndischarged from the ICU. In the \"So-what\" scenario, the optimal model provided\na detailed analysis of the motivation and significance of treatment plans for\nICU patients, with its reasoning achieving a similarity of 55.6% with actual\ndiagnostic information. In the \"How-about\" scenario, the top-performing LLM\ndemonstrated a content similarity of 66.5% in designing treatment plans\ntransferring for similar diseases. Meanwhile, LLMs managed to predict the life\nstatus of patients after their discharge from the ICU with an accuracy of 70%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "13 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.03010v1",
    "published_date": "2024-05-05 17:36:22 UTC",
    "updated_date": "2024-05-05 17:36:22 UTC"
  },
  {
    "arxiv_id": "2405.03009v1",
    "title": "Explainable Malware Detection with Tailored Logic Explained Networks",
    "authors": [
      "Peter Anthony",
      "Francesco Giannini",
      "Michelangelo Diligenti",
      "Martin Homola",
      "Marco Gori",
      "Stefan Balogh",
      "Jan Mojzis"
    ],
    "abstract": "Malware detection is a constant challenge in cybersecurity due to the rapid\ndevelopment of new attack techniques. Traditional signature-based approaches\nstruggle to keep pace with the sheer volume of malware samples. Machine\nlearning offers a promising solution, but faces issues of generalization to\nunseen samples and a lack of explanation for the instances identified as\nmalware. However, human-understandable explanations are especially important in\nsecurity-critical fields, where understanding model decisions is crucial for\ntrust and legal compliance. While deep learning models excel at malware\ndetection, their black-box nature hinders explainability. Conversely,\ninterpretable models often fall short in performance. To bridge this gap in\nthis application domain, we propose the use of Logic Explained Networks (LENs),\nwhich are a recently proposed class of interpretable neural networks providing\nexplanations in the form of First-Order Logic (FOL) rules. This paper extends\nthe application of LENs to the complex domain of malware detection,\nspecifically using the large-scale EMBER dataset. In the experimental results\nwe show that LENs achieve robustness that exceeds traditional interpretable\nmethods and that are rivaling black-box models. Moreover, we introduce a\ntailored version of LENs that is shown to generate logic explanations with\nhigher fidelity with respect to the model's predictions.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.03009v1",
    "published_date": "2024-05-05 17:36:02 UTC",
    "updated_date": "2024-05-05 17:36:02 UTC"
  },
  {
    "arxiv_id": "2405.03007v1",
    "title": "On the performativity of SDG classifications in large bibliometric databases",
    "authors": [
      "Matteo Ottaviani",
      "Stephan Stahlschmidt"
    ],
    "abstract": "Large bibliometric databases, such as Web of Science, Scopus, and OpenAlex,\nfacilitate bibliometric analyses, but are performative, affecting the\nvisibility of scientific outputs and the impact measurement of participating\nentities. Recently, these databases have taken up the UN's Sustainable\nDevelopment Goals (SDGs) in their respective classifications, which have been\ncriticised for their diverging nature. This work proposes using the feature of\nlarge language models (LLMs) to learn about the \"data bias\" injected by diverse\nSDG classifications into bibliometric data by exploring five SDGs. We build a\nLLM that is fine-tuned in parallel by the diverse SDG classifications inscribed\ninto the databases' SDG classifications. Our results show high sensitivity in\nmodel architecture, classified publications, fine-tuning process, and natural\nlanguage generation. The wide arbitrariness at different levels raises concerns\nabout using LLM in research practice.",
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.DL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.03007v1",
    "published_date": "2024-05-05 17:28:54 UTC",
    "updated_date": "2024-05-05 17:28:54 UTC"
  },
  {
    "arxiv_id": "2405.03005v1",
    "title": "Safe Reinforcement Learning with Learned Non-Markovian Safety Constraints",
    "authors": [
      "Siow Meng Low",
      "Akshat Kumar"
    ],
    "abstract": "In safe Reinforcement Learning (RL), safety cost is typically defined as a\nfunction dependent on the immediate state and actions. In practice, safety\nconstraints can often be non-Markovian due to the insufficient fidelity of\nstate representation, and safety cost may not be known. We therefore address a\ngeneral setting where safety labels (e.g., safe or unsafe) are associated with\nstate-action trajectories. Our key contributions are: first, we design a safety\nmodel that specifically performs credit assignment to assess contributions of\npartial state-action trajectories on safety. This safety model is trained using\na labeled safety dataset. Second, using RL-as-inference strategy we derive an\neffective algorithm for optimizing a safe policy using the learned safety\nmodel. Finally, we devise a method to dynamically adapt the tradeoff\ncoefficient between reward maximization and safety compliance. We rewrite the\nconstrained optimization problem into its dual problem and derive a\ngradient-based method to dynamically adjust the tradeoff coefficient during\ntraining. Our empirical results demonstrate that this approach is highly\nscalable and able to satisfy sophisticated non-Markovian safety constraints.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.03005v1",
    "published_date": "2024-05-05 17:27:22 UTC",
    "updated_date": "2024-05-05 17:27:22 UTC"
  },
  {
    "arxiv_id": "2405.03003v1",
    "title": "Parameter-Efficient Fine-Tuning with Discrete Fourier Transform",
    "authors": [
      "Ziqi Gao",
      "Qichao Wang",
      "Aochuan Chen",
      "Zijing Liu",
      "Bingzhe Wu",
      "Liang Chen",
      "Jia Li"
    ],
    "abstract": "Low-rank adaptation~(LoRA) has recently gained much interest in fine-tuning\nfoundation models. It effectively reduces the number of trainable parameters by\nincorporating low-rank matrices $A$ and $B$ to represent the weight change,\ni.e., $\\Delta W=BA$. Despite LoRA's progress, it faces storage challenges when\nhandling extensive customization adaptations or larger base models. In this\nwork, we aim to further compress trainable parameters by enjoying the powerful\nexpressiveness of the Fourier transform. Specifically, we introduce FourierFT,\nwhich treats $\\Delta W$ as a matrix in the spatial domain and learns only a\nsmall fraction of its spectral coefficients. With the trained spectral\ncoefficients, we implement the inverse discrete Fourier transform to recover\n$\\Delta W$. Empirically, our FourierFT method shows comparable or better\nperformance with fewer parameters than LoRA on various tasks, including natural\nlanguage understanding, natural language generation, instruction tuning, and\nimage classification. For example, when performing instruction tuning on the\nLLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable\nparameters, compared to LoRA's 33.5M. Our code is released at\n\\url{https://github.com/Chaos96/fourierft}.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.03003v1",
    "published_date": "2024-05-05 17:15:24 UTC",
    "updated_date": "2024-05-05 17:15:24 UTC"
  },
  {
    "arxiv_id": "2405.03000v2",
    "title": "MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning",
    "authors": [
      "Wenqi Shi",
      "Ran Xu",
      "Yuchen Zhuang",
      "Yue Yu",
      "Haotian Sun",
      "Hang Wu",
      "Carl Yang",
      "May D. Wang"
    ],
    "abstract": "Despite their improved capabilities in generation and reasoning, adapting\nlarge language models (LLMs) to the biomedical domain remains challenging due\nto their immense size and corporate privacy. In this work, we propose\nMedAdapter, a unified post-hoc adapter for test-time adaptation of LLMs towards\nbiomedical applications. Instead of fine-tuning the entire LLM, MedAdapter\neffectively adapts the original model by fine-tuning only a small BERT-sized\nadapter to rank candidate solutions generated by LLMs. Experiments demonstrate\nthat MedAdapter effectively adapts both white-box and black-box LLMs in\nbiomedical reasoning, achieving average performance improvements of 25.48% and\n11.31%, respectively, without requiring extensive computational resources or\nsharing data with third parties. MedAdapter also yields superior performance\nwhen combined with train-time adaptation, highlighting a flexible and\ncomplementary solution to existing adaptation methods. Faced with the\nchallenges of balancing model performance, computational resources, and data\nprivacy, MedAdapter provides an efficient, privacy-preserving, cost-effective,\nand transparent solution for adapting LLMs to the biomedical domain.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted in EMNLP 2024 main conference",
    "pdf_url": "http://arxiv.org/pdf/2405.03000v2",
    "published_date": "2024-05-05 17:06:31 UTC",
    "updated_date": "2024-10-04 06:31:47 UTC"
  },
  {
    "arxiv_id": "2405.02996v1",
    "title": "RepAugment: Input-Agnostic Representation-Level Augmentation for Respiratory Sound Classification",
    "authors": [
      "June-Woo Kim",
      "Miika Toikkanen",
      "Sangmin Bae",
      "Minseok Kim",
      "Ho-Young Jung"
    ],
    "abstract": "Recent advancements in AI have democratized its deployment as a healthcare\nassistant. While pretrained models from large-scale visual and audio datasets\nhave demonstrably generalized to this task, surprisingly, no studies have\nexplored pretrained speech models, which, as human-originated sounds,\nintuitively would share closer resemblance to lung sounds. This paper explores\nthe efficacy of pretrained speech models for respiratory sound classification.\nWe find that there is a characterization gap between speech and lung sound\nsamples, and to bridge this gap, data augmentation is essential. However, the\nmost widely used augmentation technique for audio and speech, SpecAugment,\nrequires 2-dimensional spectrogram format and cannot be applied to models\npretrained on speech waveforms. To address this, we propose RepAugment, an\ninput-agnostic representation-level augmentation technique that outperforms\nSpecAugment, but is also suitable for respiratory sound classification with\nwaveform pretrained models. Experimental results show that our approach\noutperforms the SpecAugment, demonstrating a substantial improvement in the\naccuracy of minority disease classes, reaching up to 7.14%.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted EMBC 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.02996v1",
    "published_date": "2024-05-05 16:45:46 UTC",
    "updated_date": "2024-05-05 16:45:46 UTC"
  },
  {
    "arxiv_id": "2405.03718v2",
    "title": "A Single Online Agent Can Efficiently Learn Mean Field Games",
    "authors": [
      "Chenyu Zhang",
      "Xu Chen",
      "Xuan Di"
    ],
    "abstract": "Mean field games (MFGs) are a promising framework for modeling the behavior\nof large-population systems. However, solving MFGs can be challenging due to\nthe coupling of forward population evolution and backward agent dynamics.\nTypically, obtaining mean field Nash equilibria (MFNE) involves an iterative\napproach where the forward and backward processes are solved alternately, known\nas fixed-point iteration (FPI). This method requires fully observed population\npropagation and agent dynamics over the entire spatial domain, which could be\nimpractical in some real-world scenarios. To overcome this limitation, this\npaper introduces a novel online single-agent model-free learning scheme, which\nenables a single agent to learn MFNE using online samples, without prior\nknowledge of the state-action space, reward function, or transition dynamics.\nSpecifically, the agent updates its policy through the value function (Q),\nwhile simultaneously evaluating the mean field state (M), using the same batch\nof observations. We develop two variants of this learning scheme: off-policy\nand on-policy QM iteration. We prove that they efficiently approximate FPI, and\na sample complexity guarantee is provided. The efficacy of our methods is\nconfirmed by numerical experiments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at ECAI 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.03718v2",
    "published_date": "2024-05-05 16:38:04 UTC",
    "updated_date": "2024-07-16 06:03:23 UTC"
  },
  {
    "arxiv_id": "2405.06680v4",
    "title": "Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning",
    "authors": [
      "Jun Zhao",
      "Jingqi Tong",
      "Yurong Mou",
      "Ming Zhang",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "abstract": "Human cognition exhibits systematic compositionality, the algebraic ability\nto generate infinite novel combinations from finite learned components, which\nis the key to understanding and reasoning about complex logic. In this work, we\ninvestigate the compositionality of large language models (LLMs) in\nmathematical reasoning. Specifically, we construct a new dataset\n\\textsc{MathTrap} by introducing carefully designed logical traps into the\nproblem descriptions of MATH and GSM8K. Since problems with logical flaws are\nquite rare in the real world, these represent \"unseen\" cases to LLMs. Solving\nthese requires the models to systematically compose (1) the mathematical\nknowledge involved in the original problems with (2) knowledge related to the\nintroduced traps. Our experiments show that while LLMs possess both components\nof requisite knowledge, they do not \\textbf{spontaneously} combine them to\nhandle these novel cases. We explore several methods to mitigate this\ndeficiency, such as natural language prompts, few-shot demonstrations, and\nfine-tuning. Additionally, we test the recently released OpenAI o1 model and\nfind that human-like `slow thinking' helps improve the compositionality of\nLLMs. Overall, systematic compositionality remains an open challenge for large\nlanguage models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.06680v4",
    "published_date": "2024-05-05 16:35:30 UTC",
    "updated_date": "2024-10-10 14:38:37 UTC"
  },
  {
    "arxiv_id": "2405.02985v1",
    "title": "Can Large Language Models Make the Grade? An Empirical Study Evaluating LLMs Ability to Mark Short Answer Questions in K-12 Education",
    "authors": [
      "Owen Henkel",
      "Adam Boxer",
      "Libby Hills",
      "Bill Roberts"
    ],
    "abstract": "This paper presents reports on a series of experiments with a novel dataset\nevaluating how well Large Language Models (LLMs) can mark (i.e. grade) open\ntext responses to short answer questions, Specifically, we explore how well\ndifferent combinations of GPT version and prompt engineering strategies\nperformed at marking real student answers to short answer across different\ndomain areas (Science and History) and grade-levels (spanning ages 5-16) using\na new, never-used-before dataset from Carousel, a quizzing platform. We found\nthat GPT-4, with basic few-shot prompting performed well (Kappa, 0.70) and,\nimportantly, very close to human-level performance (0.75). This research builds\non prior findings that GPT-4 could reliably score short answer reading\ncomprehension questions at a performance-level very close to that of expert\nhuman raters. The proximity to human-level performance, across a variety of\nsubjects and grade levels suggests that LLMs could be a valuable tool for\nsupporting low-stakes formative assessment tasks in K-12 education and has\nimportant implications for real-world education delivery.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02985v1",
    "published_date": "2024-05-05 16:11:06 UTC",
    "updated_date": "2024-05-05 16:11:06 UTC"
  },
  {
    "arxiv_id": "2405.02972v1",
    "title": "Multi-Agent RL-Based Industrial AIGC Service Offloading over Wireless Edge Networks",
    "authors": [
      "Siyuan Li",
      "Xi Lin",
      "Hansong Xu",
      "Kun Hua",
      "Xiaomin Jin",
      "Gaolei Li",
      "Jianhua Li"
    ],
    "abstract": "Currently, the generative model has garnered considerable attention due to\nits application in addressing the challenge of scarcity of abnormal samples in\nthe industrial Internet of Things (IoT). However, challenges persist regarding\nthe edge deployment of generative models and the optimization of joint edge\nAI-generated content (AIGC) tasks. In this paper, we focus on the edge\noptimization of AIGC task execution and propose GMEL, a generative model-driven\nindustrial AIGC collaborative edge learning framework. This framework aims to\nfacilitate efficient few-shot learning by leveraging realistic sample synthesis\nand edge-based optimization capabilities. First, a multi-task AIGC\ncomputational offloading model is presented to ensure the efficient execution\nof heterogeneous AIGC tasks on edge servers. Then, we propose an\nattention-enhanced multi-agent reinforcement learning (AMARL) algorithm aimed\nat refining offloading policies within the IoT system, thereby supporting\ngenerative model-driven edge learning. Finally, our experimental results\ndemonstrate the effectiveness of the proposed algorithm in optimizing the total\nsystem latency of the edge-based AIGC task completion.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02972v1",
    "published_date": "2024-05-05 15:31:47 UTC",
    "updated_date": "2024-05-05 15:31:47 UTC"
  },
  {
    "arxiv_id": "2405.02968v4",
    "title": "CoverLib: Classifiers-equipped Experience Library by Iterative Problem Distribution Coverage Maximization for Domain-tuned Motion Planning",
    "authors": [
      "Hirokazu Ishida",
      "Naoki Hiraoka",
      "Kei Okada",
      "Masayuki Inaba"
    ],
    "abstract": "Library-based methods are known to be very effective for fast motion planning\nby adapting an experience retrieved from a precomputed library. This article\npresents CoverLib, a principled approach for constructing and utilizing such a\nlibrary. CoverLib iteratively adds an experience-classifier-pair to the\nlibrary, where each classifier corresponds to an adaptable region of the\nexperience within the problem space. This iterative process is an active\nprocedure, as it selects the next experience based on its ability to\neffectively cover the uncovered region. During the query phase, these\nclassifiers are utilized to select an experience that is expected to be\nadaptable for a given problem. Experimental results demonstrate that CoverLib\neffectively mitigates the trade-off between plannability and speed observed in\nglobal (e.g. sampling-based) and local (e.g. optimization-based) methods. As a\nresult, it achieves both fast planning and high success rates over the problem\ndomain. Moreover, due to its adaptation-algorithm-agnostic nature, CoverLib\nseamlessly integrates with various adaptation methods, including nonlinear\nprogramming-based and sampling-based algorithms.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted for publication in IEEE Transactions on Robotics",
    "pdf_url": "http://arxiv.org/pdf/2405.02968v4",
    "published_date": "2024-05-05 15:27:05 UTC",
    "updated_date": "2025-02-21 06:23:51 UTC"
  },
  {
    "arxiv_id": "2405.02965v2",
    "title": "Robust Collaborative Perception without External Localization and Clock Devices",
    "authors": [
      "Zixing Lei",
      "Zhenyang Ni",
      "Ruize Han",
      "Shuo Tang",
      "Dingju Wang",
      "Chen Feng",
      "Siheng Chen",
      "Yanfeng Wang"
    ],
    "abstract": "A consistent spatial-temporal coordination across multiple agents is\nfundamental for collaborative perception, which seeks to improve perception\nabilities through information exchange among agents. To achieve this\nspatial-temporal alignment, traditional methods depend on external devices to\nprovide localization and clock signals. However, hardware-generated signals\ncould be vulnerable to noise and potentially malicious attack, jeopardizing the\nprecision of spatial-temporal alignment. Rather than relying on external\nhardwares, this work proposes a novel approach: aligning by recognizing the\ninherent geometric patterns within the perceptual data of various agents.\nFollowing this spirit, we propose a robust collaborative perception system that\noperates independently of external localization and clock devices. The key\nmodule of our system,~\\emph{FreeAlign}, constructs a salient object graph for\neach agent based on its detected boxes and uses a graph neural network to\nidentify common subgraphs between agents, leading to accurate relative pose and\ntime. We validate \\emph{FreeAlign} on both real-world and simulated datasets.\nThe results show that, the ~\\emph{FreeAlign} empowered robust collaborative\nperception system perform comparably to systems relying on precise localization\nand clock devices.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "6pages, accepted to ICRA 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.02965v2",
    "published_date": "2024-05-05 15:20:36 UTC",
    "updated_date": "2024-05-31 13:58:20 UTC"
  },
  {
    "arxiv_id": "2405.02957v3",
    "title": "Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents",
    "authors": [
      "Junkai Li",
      "Yunghwei Lai",
      "Weitao Li",
      "Jingyi Ren",
      "Meng Zhang",
      "Xinhui Kang",
      "Siyu Wang",
      "Peng Li",
      "Ya-Qin Zhang",
      "Weizhi Ma",
      "Yang Liu"
    ],
    "abstract": "The recent rapid development of large language models (LLMs) has sparked a\nnew wave of technological revolution in medical artificial intelligence (AI).\nWhile LLMs are designed to understand and generate text like a human,\nautonomous agents that utilize LLMs as their \"brain\" have exhibited\ncapabilities beyond text processing such as planning, reflection, and using\ntools by enabling their \"bodies\" to interact with the environment. We introduce\na simulacrum of hospital called Agent Hospital that simulates the entire\nprocess of treating illness, in which all patients, nurses, and doctors are\nLLM-powered autonomous agents. Within the simulacrum, doctor agents are able to\nevolve by treating a large number of patient agents without the need to label\ntraining data manually. After treating tens of thousands of patient agents in\nthe simulacrum (human doctors may take several years in the real world), the\nevolved doctor agents outperform state-of-the-art medical agent methods on the\nMedQA benchmark comprising US Medical Licensing Examination (USMLE) test\nquestions. Our methods of simulacrum construction and agent evolution have the\npotential in benefiting a broad range of applications beyond medical AI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02957v3",
    "published_date": "2024-05-05 14:53:51 UTC",
    "updated_date": "2025-01-17 11:59:23 UTC"
  },
  {
    "arxiv_id": "2405.02936v2",
    "title": "On the Tractability of SHAP Explanations under Markovian Distributions",
    "authors": [
      "Reda Marzouk",
      "Colin de La Higuera"
    ],
    "abstract": "Thanks to its solid theoretical foundation, the SHAP framework is arguably\none the most widely utilized frameworks for local explainability of ML models.\nDespite its popularity, its exact computation is known to be very challenging,\nproven to be NP-Hard in various configurations. Recent works have unveiled\npositive complexity results regarding the computation of the SHAP score for\nspecific model families, encompassing decision trees, random forests, and some\nclasses of boolean circuits. Yet, all these positive results hinge on the\nassumption of feature independence, often simplistic in real-world scenarios.\nIn this article, we investigate the computational complexity of the SHAP score\nby relaxing this assumption and introducing a Markovian perspective. We show\nthat, under the Markovian assumption, computing the SHAP score for the class of\nWeighted automata, Disjoint DNFs and Decision Trees can be performed in\npolynomial time, offering a first positive complexity result for the problem of\nSHAP score computation that transcends the limitations of the feature\nindependence assumption.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICML 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.02936v2",
    "published_date": "2024-05-05 13:56:12 UTC",
    "updated_date": "2024-05-24 23:45:34 UTC"
  },
  {
    "arxiv_id": "2405.02929v3",
    "title": "Unified Dynamic Scanpath Predictors Outperform Individually Trained Neural Models",
    "authors": [
      "Fares Abawi",
      "Di Fu",
      "Stefan Wermter"
    ],
    "abstract": "Previous research on scanpath prediction has mainly focused on group models,\ndisregarding the fact that the scanpaths and attentional behaviors of\nindividuals are diverse. The disregard of these differences is especially\ndetrimental to social human-robot interaction, whereby robots commonly emulate\nhuman gaze based on heuristics or predefined patterns. However, human gaze\npatterns are heterogeneous and varying behaviors can significantly affect the\noutcomes of such human-robot interactions. To fill this gap, we developed a\ndeep learning-based social cue integration model for saliency prediction to\ninstead predict scanpaths in videos. Our model learned scanpaths by recursively\nintegrating fixation history and social cues through a gating mechanism and\nsequential attention. We evaluated our approach on gaze datasets of dynamic\nsocial scenes, observed under the free-viewing condition. The introduction of\nfixation history into our models makes it possible to train a single unified\nmodel rather than the resource-intensive approach of training individual models\nfor each set of scanpaths. We observed that the late neural integration\napproach surpasses early fusion when training models on a large dataset, in\ncomparison to a smaller dataset with a similar distribution. Results also\nindicate that a single unified model, trained on all the observers' scanpaths,\nperforms on par or better than individually trained models. We hypothesize that\nthis outcome is a result of the group saliency representations instilling\nuniversal attention in the model, while the supervisory signal and fixation\nhistory guide it to learn personalized attentional behaviors, providing the\nunified model a benefit over individual models due to its implicit\nrepresentation of universal attention.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02929v3",
    "published_date": "2024-05-05 13:15:11 UTC",
    "updated_date": "2025-04-20 12:41:04 UTC"
  },
  {
    "arxiv_id": "2405.04549v1",
    "title": "ClothPPO: A Proximal Policy Optimization Enhancing Framework for Robotic Cloth Manipulation with Observation-Aligned Action Spaces",
    "authors": [
      "Libing Yang",
      "Yang Li",
      "Long Chen"
    ],
    "abstract": "Vision-based robotic cloth unfolding has made great progress recently.\nHowever, prior works predominantly rely on value learning and have not fully\nexplored policy-based techniques. Recently, the success of reinforcement\nlearning on the large language model has shown that the policy gradient\nalgorithm can enhance policy with huge action space. In this paper, we\nintroduce ClothPPO, a framework that employs a policy gradient algorithm based\non actor-critic architecture to enhance a pre-trained model with huge 10^6\naction spaces aligned with observation in the task of unfolding clothes. To\nthis end, we redefine the cloth manipulation problem as a partially observable\nMarkov decision process. A supervised pre-training stage is employed to train a\nbaseline model of our policy. In the second stage, the Proximal Policy\nOptimization (PPO) is utilized to guide the supervised model within the\nobservation-aligned action space. By optimizing and updating the strategy, our\nproposed method increases the garment's surface area for cloth unfolding under\nthe soft-body manipulation task. Experimental results show that our proposed\nframework can further improve the unfolding performance of other\nstate-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.04549v1",
    "published_date": "2024-05-05 12:36:18 UTC",
    "updated_date": "2024-05-05 12:36:18 UTC"
  },
  {
    "arxiv_id": "2405.02887v2",
    "title": "Sentiment Analysis Across Languages: Evaluation Before and After Machine Translation to English",
    "authors": [
      "Aekansh Kathunia",
      "Mohammad Kaif",
      "Nalin Arora",
      "N Narotam"
    ],
    "abstract": "People communicate in more than 7,000 languages around the world, with around\n780 languages spoken in India alone. Despite this linguistic diversity,\nresearch on Sentiment Analysis has predominantly focused on English text data,\nresulting in a disproportionate availability of sentiment resources for\nEnglish. This paper examines the performance of transformer models in Sentiment\nAnalysis tasks across multilingual datasets and text that has undergone machine\ntranslation. By comparing the effectiveness of these models in different\nlinguistic contexts, we gain insights into their performance variations and\npotential implications for sentiment analysis across diverse languages. We also\ndiscuss the shortcomings and potential for future work towards the end.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "6 pages, 3 Figures",
    "pdf_url": "http://arxiv.org/pdf/2405.02887v2",
    "published_date": "2024-05-05 10:52:09 UTC",
    "updated_date": "2024-09-02 15:41:34 UTC"
  },
  {
    "arxiv_id": "2405.02881v2",
    "title": "FedConPE: Efficient Federated Conversational Bandits with Heterogeneous Clients",
    "authors": [
      "Zhuohua Li",
      "Maoli Liu",
      "John C. S. Lui"
    ],
    "abstract": "Conversational recommender systems have emerged as a potent solution for\nefficiently eliciting user preferences. These systems interactively present\nqueries associated with \"key terms\" to users and leverage user feedback to\nestimate user preferences more efficiently. Nonetheless, most existing\nalgorithms adopt a centralized approach. In this paper, we introduce FedConPE,\na phase elimination-based federated conversational bandit algorithm, where $M$\nagents collaboratively solve a global contextual linear bandit problem with the\nhelp of a central server while ensuring secure data management. To effectively\ncoordinate all the clients and aggregate their collected data, FedConPE uses an\nadaptive approach to construct key terms that minimize uncertainty across all\ndimensions in the feature space. Furthermore, compared with existing federated\nlinear bandit algorithms, FedConPE offers improved computational and\ncommunication efficiency as well as enhanced privacy protections. Our\ntheoretical analysis shows that FedConPE is minimax near-optimal in terms of\ncumulative regret. We also establish upper bounds for communication costs and\nconversation frequency. Comprehensive evaluations demonstrate that FedConPE\noutperforms existing conversational bandit algorithms while using fewer\nconversations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the 33rd International Joint Conference on Artificial\n  Intelligence (IJCAI), 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.02881v2",
    "published_date": "2024-05-05 10:28:06 UTC",
    "updated_date": "2024-06-20 16:11:59 UTC"
  },
  {
    "arxiv_id": "2405.03716v1",
    "title": "Predicting the usability of mobile applications using AI tools: the rise of large user interface models, opportunities, and challenges",
    "authors": [
      "Abdallah Namoun",
      "Ahmed Alrehaili",
      "Zaib Un Nisa",
      "Hani Almoamari",
      "Ali Tufail"
    ],
    "abstract": "This article proposes the so-called large user interface models (LUIMs) to\nenable the generation of user interfaces and prediction of usability using\nartificial intelligence in the context of mobile applications.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "12 pages, 3 figures, 4 tables, The 7th International Conference on\n  Emerging Data and Industry (EDI40)",
    "pdf_url": "http://arxiv.org/pdf/2405.03716v1",
    "published_date": "2024-05-05 09:24:48 UTC",
    "updated_date": "2024-05-05 09:24:48 UTC"
  },
  {
    "arxiv_id": "2405.02861v1",
    "title": "Revisiting a Pain in the Neck: Semantic Phrase Processing Benchmark for Language Models",
    "authors": [
      "Yang Liu",
      "Melissa Xiaohui Qin",
      "Hongming Li",
      "Chao Huang"
    ],
    "abstract": "We introduce LexBench, a comprehensive evaluation suite enabled to test\nlanguage models (LMs) on ten semantic phrase processing tasks. Unlike prior\nstudies, it is the first work to propose a framework from the comparative\nperspective to model the general semantic phrase (i.e., lexical collocation)\nand three fine-grained semantic phrases, including idiomatic expression, noun\ncompound, and verbal construction. Thanks to \\ourbenchmark, we assess the\nperformance of 15 LMs across model architectures and parameter scales in\nclassification, extraction, and interpretation tasks. Through the experiments,\nwe first validate the scaling law and find that, as expected, large models\nexcel better than the smaller ones in most tasks. Second, we investigate\nfurther through the scaling semantic relation categorization and find that\nfew-shot LMs still lag behind vanilla fine-tuned models in the task. Third,\nthrough human evaluation, we find that the performance of strong models is\ncomparable to the human level regarding semantic phrase processing. Our\nbenchmarking findings can serve future research aiming to improve the generic\ncapability of LMs on semantic phrase comprehension. Our source code and data\nare available at https://github.com/jacklanda/LexBench",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "24 pages, 17 figures, 10 tables",
    "pdf_url": "http://arxiv.org/pdf/2405.02861v1",
    "published_date": "2024-05-05 09:20:38 UTC",
    "updated_date": "2024-05-05 09:20:38 UTC"
  },
  {
    "arxiv_id": "2405.02850v7",
    "title": "Halfway Escape Optimization: A Quantum-Inspired Solution for General Optimization Problems",
    "authors": [
      "Jiawen Li",
      "Anwar PP Abdul Majeed",
      "Pascal Lefevre"
    ],
    "abstract": "This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a\nquantum-inspired metaheuristic designed to address general optimization\nproblems. The HEO mimics the effects between quantum such as tunneling,\nentanglement. After the introduction to the HEO mechansims, the study presents\na comprehensive evaluation of HEO's performance against extensively-used\noptimization algorithms, including Particle Swarm Optimization (PSO), Genetic\nAlgorithm (GA), Artificial Fish Swarm Algorithm (AFSA), Grey Wolf Optimizer\n(GWO), and Quantum behaved Particle Swarm Optimization (QPSO). The primary\nanalysis encompasses 14 benchmark functions with dimension 30, demonstrating\nHEO's effectiveness and adaptability in navigating general optimization\nproblems. The test of HEO in Pressure Vessel Design and Tubular Column Design\nalso infers its feasibility and potential in real-time applications. Further\nvalidation of HEO in Osmancik-97 and Cammeo Rice Classification achieves a\nhigher accuracy record.",
    "categories": [
      "cs.NE",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.NE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02850v7",
    "published_date": "2024-05-05 08:43:07 UTC",
    "updated_date": "2024-09-24 12:11:50 UTC"
  },
  {
    "arxiv_id": "2405.02849v1",
    "title": "Modelling Opaque Bilateral Market Dynamics in Financial Trading: Insights from a Multi-Agent Simulation Study",
    "authors": [
      "Alicia Vidler",
      "Toby Walsh"
    ],
    "abstract": "Exploring complex adaptive financial trading environments through multi-agent\nbased simulation methods presents an innovative approach within the realm of\nquantitative finance. Despite the dominance of multi-agent reinforcement\nlearning approaches in financial markets with observable data, there exists a\nset of systematically significant financial markets that pose challenges due to\ntheir partial or obscured data availability. We, therefore, devise a\nmulti-agent simulation approach employing small-scale meta-heuristic methods.\nThis approach aims to represent the opaque bilateral market for Australian\ngovernment bond trading, capturing the bilateral nature of bank-to-bank\ntrading, also referred to as \"over-the-counter\" (OTC) trading, and commonly\noccurring between \"market makers\". The uniqueness of the bilateral market,\ncharacterized by negotiated transactions and a limited number of agents, yields\nvaluable insights for agent-based modelling and quantitative finance. The\ninherent rigidity of this market structure, which is at odds with the global\nproliferation of multilateral platforms and the decentralization of finance,\nunderscores the unique insights offered by our agent-based model. We explore\nthe implications of market rigidity on market structure and consider the\nelement of stability, in market design. This extends the ongoing discourse on\ncomplex financial trading environments, providing an enhanced understanding of\ntheir dynamics and implications.",
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "q-fin.CP",
    "comment": "13 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.02849v1",
    "published_date": "2024-05-05 08:42:20 UTC",
    "updated_date": "2024-05-05 08:42:20 UTC"
  },
  {
    "arxiv_id": "2405.02846v1",
    "title": "Responsible AI: Portraits with Intelligent Bibliometrics",
    "authors": [
      "Yi Zhang",
      "Mengjia Wu",
      "Guangquan Zhang",
      "Jie Lu"
    ],
    "abstract": "Shifting the focus from principles to practical implementation, responsible\nartificial intelligence (AI) has garnered considerable attention across\nacademia, industry, and society at large. Despite being in its nascent stages,\nthis emerging field grapples with nebulous concepts and intricate knowledge\nframeworks. By analyzing three prevailing concepts - explainable AI,\ntrustworthy AI, and ethical AI, this study defined responsible AI and\nidentified its core principles. Methodologically, this study successfully\ndemonstrated the implementation of leveraging AI's capabilities into\nbibliometrics for enhanced knowledge discovery and the cross-validation of\nexperimentally examined models with domain insights. Empirically, this study\ninvestigated 17,799 research articles contributed by the AI community since\n2015. This involves recognizing key technological players and their\nrelationships, unveiling the topical landscape and hierarchy of responsible AI,\ncharting its evolution, and elucidating the interplay between the\nresponsibility principles and primary AI techniques. An analysis of a core\ncohort comprising 380 articles from multiple disciplines captures the most\nrecent advancements in responsible AI. As one of the pioneering bibliometric\nstudies dedicated to exploring responsible AI, this study will provide\ncomprehensive macro-level insights, enhancing the understanding of responsible\nAI while furnishing valuable knowledge support for AI regulation and governance\ninitiatives.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "14 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.02846v1",
    "published_date": "2024-05-05 08:40:22 UTC",
    "updated_date": "2024-05-05 08:40:22 UTC"
  },
  {
    "arxiv_id": "2405.02821v2",
    "title": "Sim2Real Transfer for Audio-Visual Navigation with Frequency-Adaptive Acoustic Field Prediction",
    "authors": [
      "Changan Chen",
      "Jordi Ramos",
      "Anshul Tomar",
      "Kristen Grauman"
    ],
    "abstract": "Sim2real transfer has received increasing attention lately due to the success\nof learning robotic tasks in simulation end-to-end. While there has been a lot\nof progress in transferring vision-based navigation policies, the existing\nsim2real strategy for audio-visual navigation performs data augmentation\nempirically without measuring the acoustic gap. The sound differs from light in\nthat it spans across much wider frequencies and thus requires a different\nsolution for sim2real. We propose the first treatment of sim2real for\naudio-visual navigation by disentangling it into acoustic field prediction\n(AFP) and waypoint navigation. We first validate our design choice in the\nSoundSpaces simulator and show improvement on the Continuous AudioGoal\nnavigation benchmark. We then collect real-world data to measure the spectral\ndifference between the simulation and the real world by training AFP models\nthat only take a specific frequency subband as input. We further propose a\nfrequency-adaptive strategy that intelligently selects the best frequency band\nfor prediction based on both the measured spectral difference and the energy\ndistribution of the received audio, which improves the performance on the real\ndata. Lastly, we build a real robot platform and show that the transferred\npolicy can successfully navigate to sounding objects. This work demonstrates\nthe potential of building intelligent agents that can see, hear, and act\nentirely from simulation, and transferring them to the real world.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.LG",
      "cs.RO",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Camera ready version for IROS 2024. Project page:\n  https://vision.cs.utexas.edu/projects/sim2real/",
    "pdf_url": "http://arxiv.org/pdf/2405.02821v2",
    "published_date": "2024-05-05 06:01:31 UTC",
    "updated_date": "2024-09-10 23:43:53 UTC"
  },
  {
    "arxiv_id": "2405.02815v1",
    "title": "Region-specific Risk Quantification for Interpretable Prognosis of COVID-19",
    "authors": [
      "Zhusi Zhong",
      "Jie Li",
      "Zhuoqi Ma",
      "Scott Collins",
      "Harrison Bai",
      "Paul Zhang",
      "Terrance Healey",
      "Xinbo Gao",
      "Michael K. Atalay",
      "Zhicheng Jiao"
    ],
    "abstract": "The COVID-19 pandemic has strained global public health, necessitating\naccurate diagnosis and intervention to control disease spread and reduce\nmortality rates. This paper introduces an interpretable deep survival\nprediction model designed specifically for improved understanding and trust in\nCOVID-19 prognosis using chest X-ray (CXR) images. By integrating a large-scale\npretrained image encoder, Risk-specific Grad-CAM, and anatomical region\ndetection techniques, our approach produces regional interpretable outcomes\nthat effectively capture essential disease features while focusing on rare but\ncritical abnormal regions. Our model's predictive results provide enhanced\nclarity and transparency through risk area localization, enabling clinicians to\nmake informed decisions regarding COVID-19 diagnosis with better understanding\nof prognostic insights. We evaluate the proposed method on a multi-center\nsurvival dataset and demonstrate its effectiveness via quantitative and\nqualitative assessments, achieving superior C-indexes (0.764 and 0.727) and\ntime-dependent AUCs (0.799 and 0.691). These results suggest that our\nexplainable deep survival prediction model surpasses traditional survival\nanalysis methods in risk prediction, improving interpretability for clinical\ndecision making and enhancing AI system trustworthiness.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02815v1",
    "published_date": "2024-05-05 05:08:38 UTC",
    "updated_date": "2024-05-05 05:08:38 UTC"
  },
  {
    "arxiv_id": "2405.02807v1",
    "title": "Kinematic analysis of structural mechanics based on convolutional neural network",
    "authors": [
      "Leye Zhang",
      "Xiangxiang Tian",
      "Hongjun Zhang"
    ],
    "abstract": "Attempt to use convolutional neural network to achieve kinematic analysis of\nplane bar structure. Through 3dsMax animation software and OpenCV module,\nself-build image dataset of geometrically stable system and geometrically\nunstable system. we construct and train convolutional neural network model\nbased on the TensorFlow and Keras deep learning platform framework. The model\nachieves 100% accuracy on the training set, validation set, and test set. The\naccuracy on the additional test set is 93.7%, indicating that convolutional\nneural network can learn and master the relevant knowledge of kinematic\nanalysis of structural mechanics. In the future, the generalization ability of\nthe model can be improved through the diversity of dataset, which has the\npotential to surpass human experts for complex structures. Convolutional neural\nnetwork has certain practical value in the field of kinematic analysis of\nstructural mechanics. Using visualization technology, we reveal how\nconvolutional neural network learns and recognizes structural features. Using\npre-trained VGG16 model for feature extraction and fine-tuning, we found that\nthe generalization ability is inferior to the self-built model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "9 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2405.02807v1",
    "published_date": "2024-05-05 04:00:03 UTC",
    "updated_date": "2024-05-05 04:00:03 UTC"
  },
  {
    "arxiv_id": "2405.02801v3",
    "title": "Mozart's Touch: A Lightweight Multi-modal Music Generation Framework Based on Pre-Trained Large Models",
    "authors": [
      "Jiajun Li",
      "Tianze Xu",
      "Xuesong Chen",
      "Xinrui Yao",
      "Shuchang Liu"
    ],
    "abstract": "In recent years, AI-Generated Content (AIGC) has witnessed rapid\nadvancements, facilitating the creation of music, images, and other artistic\nforms across a wide range of industries. However, current models for image- and\nvideo-to-music synthesis struggle to capture the nuanced emotions and\natmosphere conveyed by visual content. To fill this gap, we propose Mozart's\nTouch, a multi-modal music generation framework capable of generating music\naligned with cross-modal inputs such as images, videos, and text. The framework\nconsists of three key components: Multi-modal Captioning Module, Large Language\nModel (LLM) understanding \\& Bridging Module, and Music Generation Module.\nUnlike traditional end-to-end methods, Mozart's Touch uses LLMs to accurately\ninterpret visual elements without requiring the training or fine-tuning of\nmusic generation models, providing efficiency and transparency through clear,\ninterpretable prompts. We also introduce the \"LLM-Bridge\" method to resolve the\nheterogeneous representation challenges between descriptive texts from\ndifferent modalities. Through a series of objective and subjective evaluations,\nwe demonstrate that Mozart's Touch outperforms current state-of-the-art models.\nOur code and examples are available at\nhttps://github.com/TiffanyBlews/MozartsTouch.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "10 pages, 2 figures, submitted to AIGC 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.02801v3",
    "published_date": "2024-05-05 03:15:52 UTC",
    "updated_date": "2024-11-25 08:32:13 UTC"
  },
  {
    "arxiv_id": "2405.02791v3",
    "title": "Efficient Text-driven Motion Generation via Latent Consistency Training",
    "authors": [
      "Mengxian Hu",
      "Minghao Zhu",
      "Xun Zhou",
      "Qingqing Yan",
      "Shu Li",
      "Chengju Liu",
      "Qijun Chen"
    ],
    "abstract": "Text-driven human motion generation based on diffusion strategies establishes\na reliable foundation for multimodal applications in human-computer\ninteractions. However, existing advances face significant efficiency challenges\ndue to the substantial computational overhead of iteratively solving for\nnonlinear reverse diffusion trajectories during the inference phase. To this\nend, we propose the motion latent consistency training framework (MLCT), which\nprecomputes reverse diffusion trajectories from raw data in the training phase\nand enables few-step or single-step inference via self-consistency constraints\nin the inference phase. Specifically, a motion autoencoder with quantization\nconstraints is first proposed for constructing concise and bounded solution\ndistributions for motion diffusion processes. Subsequently, a classifier-free\nguidance format is constructed via an additional unconditional loss function to\naccomplish the precomputation of conditional diffusion trajectories in the\ntraining phase. Finally, a clustering guidance module based on the\nK-nearest-neighbor algorithm is developed for the chain-conduction optimization\nmechanism of self-consistency constraints, which provides additional references\nof solution distributions at a small query cost. By combining these\nenhancements, we achieve stable and consistency training in non-pixel modality\nand latent representation spaces. Benchmark experiments demonstrate that our\nmethod significantly outperforms traditional consistency distillation methods\nwith reduced training cost and enhances the consistency model to perform\ncomparably to state-of-the-art models with lower inference costs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.02791v3",
    "published_date": "2024-05-05 02:11:57 UTC",
    "updated_date": "2024-11-29 16:03:59 UTC"
  },
  {
    "arxiv_id": "2405.06677v1",
    "title": "ATG: Benchmarking Automated Theorem Generation for Generative Language Models",
    "authors": [
      "Xiaohan Lin",
      "Qingxing Cao",
      "Yinya Huang",
      "Zhicheng Yang",
      "Zhengying Liu",
      "Zhenguo Li",
      "Xiaodan Liang"
    ],
    "abstract": "Humans can develop new theorems to explore broader and more complex\nmathematical results. While current generative language models (LMs) have\nachieved significant improvement in automatically proving theorems, their\nability to generate new or reusable theorems is still under-explored. Without\nthe new theorems, current LMs struggle to prove harder theorems that are\ndistant from the given hypotheses with the exponentially growing search space.\nTherefore, this paper proposes an Automated Theorem Generation (ATG) benchmark\nthat evaluates whether an agent can automatically generate valuable (and\npossibly brand new) theorems that are applicable for downstream theorem proving\nas reusable knowledge. Specifically, we construct the ATG benchmark by\nsplitting the Metamath library into three sets: axioms, library, and problem\nbased on their proving depth. We conduct extensive experiments to investigate\nwhether current LMs can generate theorems in the library and benefit the\nproblem theorems proving. The results demonstrate that high-quality ATG data\nfacilitates models' performances on downstream ATP. However, there is still\nroom for current LMs to develop better ATG and generate more advanced and\nhuman-like theorems. We hope the new ATG challenge can shed some light on\nadvanced complex theorem proving.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2405.06677v1",
    "published_date": "2024-05-05 02:06:37 UTC",
    "updated_date": "2024-05-05 02:06:37 UTC"
  },
  {
    "arxiv_id": "2405.02774v1",
    "title": "Get more for less: Principled Data Selection for Warming Up Fine-Tuning in LLMs",
    "authors": [
      "Feiyang Kang",
      "Hoang Anh Just",
      "Yifan Sun",
      "Himanshu Jahagirdar",
      "Yuanzhi Zhang",
      "Rongxing Du",
      "Anit Kumar Sahu",
      "Ruoxi Jia"
    ],
    "abstract": "This work focuses on leveraging and selecting from vast, unlabeled, open data\nto pre-fine-tune a pre-trained language model. The goal is to minimize the need\nfor costly domain-specific data for subsequent fine-tuning while achieving\ndesired performance levels. While many data selection algorithms have been\ndesigned for small-scale applications, rendering them unsuitable for our\ncontext, some emerging methods do cater to language data scales. However, they\noften prioritize data that aligns with the target distribution. While this\nstrategy may be effective when training a model from scratch, it can yield\nlimited results when the model has already been pre-trained on a different\ndistribution. Differing from prior work, our key idea is to select data that\nnudges the pre-training distribution closer to the target distribution. We show\nthe optimality of this approach for fine-tuning tasks under certain conditions.\nWe demonstrate the efficacy of our methodology across a diverse array of tasks\n(NLU, NLG, zero-shot) with models up to 2.7B, showing that it consistently\nsurpasses other selection methods. Moreover, our proposed method is\nsignificantly faster than existing techniques, scaling to millions of samples\nwithin a single GPU hour. Our code is open-sourced (Code repository:\nhttps://anonymous.4open.science/r/DV4LLM-D761/ ). While fine-tuning offers\nsignificant potential for enhancing performance across diverse tasks, its\nassociated costs often limit its widespread adoption; with this work, we hope\nto lay the groundwork for cost-effective fine-tuning, making its benefits more\naccessible.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "Published as a conference paper at ICLR 2024",
    "pdf_url": "http://arxiv.org/pdf/2405.02774v1",
    "published_date": "2024-05-05 00:08:00 UTC",
    "updated_date": "2024-05-05 00:08:00 UTC"
  }
]
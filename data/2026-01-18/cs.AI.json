{
  "date": "2026-01-18",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2026-01-18 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\nğŸ‘‹ å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ä½ ä»¬çš„æ—¥æŠ¥ä½œè€…ã€‚\n\n**ä¸€å¥è¯æ€»ç»“ï¼š**\nä»Šå¤©çš„ arXiv çˆ†å‘äº†å¤§é‡å…³äº **Agentic AIï¼ˆæ™ºèƒ½ä½“ï¼‰** çš„ç»¼è¿°å’Œæ¶æ„ç ”ç©¶ï¼Œæ ‡å¿—ç€ LLM æ­£å¼ä»â€œå¯¹è¯è€…â€å‘â€œè‡ªä¸»è¡ŒåŠ¨è€…â€å’Œâ€œç§‘å­¦å®¶â€è½¬å‹ï¼›åŒæ—¶ï¼Œ**AI for Science**ï¼ˆç‰¹åˆ«æ˜¯ææ–™ç§‘å­¦å’Œå¤©ä½“ç‰©ç†ï¼‰ä¸ **LLM å†…éƒ¨æœºåˆ¶**ï¼ˆå¦‚æ˜¾å¼è®°å¿†æ³¨å…¥ã€RLHF ä¼˜åŒ–ï¼‰ä¹Ÿæœ‰éå¸¸ç¡¬æ ¸çš„å·¥ä½œå‘å¸ƒã€‚\n\n---\n\n### ğŸš€ æ·±åº¦ç„¦ç‚¹ï¼šAgentic AI ä¸ç§‘å­¦æ¢ç´¢çš„èŒƒå¼è½¬ç§»\n\nä»Šå¤©æœ‰å‡ ç¯‡é‡é‡çº§çš„æ–‡ç« ä¸çº¦è€ŒåŒåœ°è®¨è®ºäº†æ™ºèƒ½ä½“ï¼ˆAgentsï¼‰çš„æ¶æ„ã€æ¨ç†ä»¥åŠåœ¨ç§‘å­¦é¢†åŸŸçš„åº”ç”¨ã€‚\n\n**1. Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Models Agents**\n> **æ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ (AI)ï¼šå¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„æ¶æ„ã€åˆ†ç±»ä¸è¯„ä¼°**\n> *æ ¸å¿ƒè´¡çŒ®*ï¼šè¿™ç¯‡ 28 é¡µçš„é•¿æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ç±»æ³•ï¼Œå°†æ™ºèƒ½ä½“è§£æ„ä¸ºæ„ŸçŸ¥ã€å¤§è„‘ã€è§„åˆ’ã€è¡ŒåŠ¨ã€å·¥å…·ä½¿ç”¨å’Œåä½œå…­å¤§æ¨¡å—ã€‚æ–‡ç« è¯¦ç»†è®¨è®ºäº†ä»çº¿æ€§æ¨ç†åˆ°åŸç”Ÿæ¨ç†æ¨¡å‹çš„è½¬å˜ï¼Œä»¥åŠä» API è°ƒç”¨åˆ° Model Context Protocol (MCP) çš„æ¼”è¿›ã€‚\n> *æ•™æˆç‚¹è¯„*ï¼šè¿™æ˜¯ç†è§£å½“å‰ Agent ä¹±è±¡çš„ä¸€ä»½å¾ˆå¥½çš„åœ°å›¾ï¼Œç‰¹åˆ«æ˜¯å¯¹â€œè¡ŒåŠ¨ä¸­çš„å¹»è§‰â€å’Œâ€œæ— é™å¾ªç¯â€ç­‰æŒ‘æˆ˜çš„æ€»ç»“éå¸¸åˆ°ä½ã€‚\n\n**2. Agentic Reasoning for Large Language Models**\n> **å¤§å‹è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“æ¨ç†**\n> *æ ¸å¿ƒè´¡çŒ®*ï¼šæ–‡ç« å°†æ™ºèƒ½ä½“æ¨ç†åˆ†ä¸ºä¸‰ä¸ªå±‚æ¬¡ï¼šåŸºç¡€ï¼ˆå•æ™ºèƒ½ä½“è§„åˆ’/å·¥å…·ä½¿ç”¨ï¼‰ã€è‡ªæˆ‘è¿›åŒ–ï¼ˆé€šè¿‡åé¦ˆå’Œè®°å¿†ä¼˜åŒ–ï¼‰ã€é›†ä½“å¤šæ™ºèƒ½ä½“ï¼ˆåä½œä¸çŸ¥è¯†å…±äº«ï¼‰ã€‚\n> *å‘ç°*ï¼šSurvey ç±»æ–‡ç« ï¼Œé€‚åˆæƒ³è¦ç³»ç»Ÿæ€§äº†è§£ Agentic Reasoning çš„ç ”ç©¶è€…ï¼Œå®ƒå»ºç«‹äº†ä¸€ä¸ªè¿æ¥æ€ç»´ï¼ˆThoughtï¼‰ä¸è¡ŒåŠ¨ï¼ˆActionï¼‰çš„è·¯çº¿å›¾ã€‚\n\n**3. Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery**\n> **é‡æ–°æ€è€ƒ AI ç§‘å­¦å®¶ï¼šç”¨äºç§‘å­¦å‘ç°çš„äº¤äº’å¼å¤šæ™ºèƒ½ä½“å·¥ä½œæµ**\n> *æ ¸å¿ƒè´¡çŒ®*ï¼šæå‡ºäº† \"Deep Research\" ç³»ç»Ÿï¼Œè¿™æ˜¯ä¸€ä¸ªèƒ½åœ¨åˆ†é’Ÿçº§å®Œæˆç§‘ç ”å‘¨æœŸçš„å¤šæ™ºèƒ½ä½“æ¶æ„ï¼ˆè§„åˆ’ã€æ•°æ®åˆ†æã€æ–‡çŒ®æœç´¢ã€æ–°é¢–æ€§æ£€æµ‹ï¼‰ã€‚\n> *äº®ç‚¹*ï¼šåœ¨è®¡ç®—ç”Ÿç‰©å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼Œå®ƒæ¯”ç°æœ‰åŸºçº¿é«˜å‡º 14 åˆ° 26 ä¸ªç™¾åˆ†ç‚¹ã€‚è¿™å±•ç¤ºäº† AI ä»â€œè¾…åŠ©å·¥å…·â€å‘â€œåˆçº§ç ”ç©¶å‘˜â€çš„è½¬å˜ã€‚\n\n**4. A Cloud-based Multi-Agentic Workflow for Science**\n> **åŸºäºäº‘çš„ç§‘å­¦å¤šæ™ºèƒ½ä½“å·¥ä½œæµ**\n> *æ ¸å¿ƒè´¡çŒ®*ï¼šè§£å†³äº†ä¸€ä¸ªç—›ç‚¹â€”â€”å¦‚ä½•è®© LLM è°ƒç”¨å¤æ‚çš„äº‘ç«¯ä»¿çœŸå·¥å…·ã€‚ä»–ä»¬æå‡ºäº†ä¸€ä¸ªç›‘ç£è€…ä»£ç†ï¼ˆSupervisor Agentï¼‰æ¶æ„ï¼Œå¹¶åœ¨åŒ–å­¦å‚¬åŒ–å‰‚ç ”ç©¶ä¸­è¿›è¡Œäº†éªŒè¯ï¼ŒæˆåŠŸç‡æé«˜ï¼ˆ91%-97.5%ï¼‰ã€‚\n\n---\n\n### ğŸ§  æ¨¡å‹æ¶æ„ä¸æœºç†ï¼šè®°å¿†ã€RLHF ä¸ è§£é‡Šæ€§\n\nä»Šå¤©çš„æŠ€æœ¯ç¡¬èœï¼Œå…³æ³¨å¦‚ä½•åœ¨å¤§æ¨¡å‹ä¸åŠ¨æƒé‡çš„æƒ…å†µä¸‹å¢å¼ºèƒ½åŠ›ã€‚\n\n**5. Prometheus Mind: Retrofitting Memory to Frozen Language Models**\n> **æ™®ç½—ç±³ä¿®æ–¯æ€ç»´ï¼šå‘å†»ç»“çš„è¯­è¨€æ¨¡å‹æ”¹è£…è®°å¿†**\n> *æ ¸å¿ƒè´¡çŒ®*ï¼šè¿™æ˜¯ä¸€ä¸ªéå¸¸å·§å¦™çš„å·¥ç¨‹å®ç°ã€‚ä½œè€…åœ¨ä¸æ”¹å˜ Qwen3-4B æƒé‡çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ 11 ä¸ªæ¨¡å—åŒ–é€‚é…å™¨ï¼ˆä»…å  7% å¼€é”€ï¼‰å®ç°äº†å¯é€†çš„è®°å¿†æ³¨å…¥ã€‚\n> *æŠ€æœ¯ç»†èŠ‚*ï¼šè§£å†³äº†â€œéšè—çŠ¶æ€åå¡Œâ€é—®é¢˜ï¼ˆå³ transformer å¾€å¾€æŠŠç›¸ä¼¼æ¦‚å¿µå¦‚ wife å’Œ brother æ··æ·†ï¼‰ï¼Œé€šè¿‡è®­ç»ƒæŠ•å½±æ¢å¤äº†åŒºåˆ†åº¦ã€‚è¿™æ˜¯å¯¹ RAG ä¹‹å¤–çš„ä¸€ç§â€œå‚æ•°åŒ–è®°å¿†â€çš„æ–°å°è¯•ã€‚\n\n**6. Orthogonalized Policy Optimization: Decoupling Sampling Geometry from Optimization Geometry in RLHF**\n> **æ­£äº¤ç­–ç•¥ä¼˜åŒ–ï¼šè§£è€¦ RLHF ä¸­çš„é‡‡æ ·å‡ ä½•ä¸ä¼˜åŒ–å‡ ä½•**\n> *æ ¸å¿ƒè´¡çŒ®*ï¼šä½œè€…æŒ‡å‡º PPOã€DPO ç­‰ç®—æ³•ä¸­å­˜åœ¨ä¸€ä¸ªç³»ç»Ÿæ€§ä¸ç¨³å®šæºå¤´â€”â€”é‡‡æ ·æƒé‡å’Œä¼˜åŒ–æ›²ç‡è¢«å•ä¸€æ•£åº¦ï¼ˆé€šå¸¸æ˜¯ KL æ•£åº¦ï¼‰è€¦åˆäº†ã€‚\n> *æ–¹æ³•*ï¼šæå‡ºäº† OPOï¼ˆæ­£äº¤ç­–ç•¥ä¼˜åŒ–ï¼‰ï¼Œå°†å¯¹é½é—®é¢˜è¡¨è¿°ä¸ºæ­£äº¤é•œåƒä¸‹é™é—®é¢˜ï¼Œå®ç°äº†çº¿æ€§ä¸”éé¥±å’Œçš„æ¢¯åº¦åŠ¨æ€ï¼Œç†è®ºä¸Šæ¯”ç°æœ‰æ–¹æ³•æ›´ç¨³å®šã€‚\n\n**7. Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models**\n> **æ¨¡æ‹Ÿé€€ç«å¢å¼ºè‡ªå›å½’è¯­è¨€æ¨¡å‹çš„å¿ƒç†ç†è®ºæ¨ç†**\n> *æ ¸å¿ƒè´¡çŒ®*ï¼šé€šè¿‡æ¨¡æ‹Ÿé€€ç«ï¼ˆSimulated Annealingï¼‰çš„é‡‡æ ·ç­–ç•¥ï¼Œå³åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­é€æ¸æ”¹å˜æ¸©åº¦ï¼Œå¯ä»¥ç›´æ¥ä»åŸºç¡€æ¨¡å‹ä¸­â€œæŒ–æ˜â€å‡ºæ›´å¼ºçš„å¿ƒç†ç†è®ºï¼ˆToMï¼‰èƒ½åŠ›ï¼Œè€Œæ— éœ€é‡æ–°è®­ç»ƒã€‚\n\n---\n\n### ğŸ”¬ AI for Science & Material Science\n\n**8. Artificial Intelligence in Materials Science and Engineering: Current Landscape, Key Challenges, and Future Trajectories**\n> **ææ–™ç§‘å­¦ä¸å·¥ç¨‹ä¸­çš„äººå·¥æ™ºèƒ½ï¼šå½“å‰æ™¯è§‚ã€å…³é”®æŒ‘æˆ˜ä¸æœªæ¥è½¨è¿¹**\n> *æ‘˜è¦*ï¼šä¸€ç¯‡å…¨é¢çš„ç»¼è¿°ï¼Œæ¶µç›–äº†ä» CNNã€GNN åˆ° Transformer å’Œç”Ÿæˆå¼ AI åœ¨ææ–™è®¾è®¡ä¸­çš„åº”ç”¨ã€‚ç‰¹åˆ«å¼ºè°ƒäº†æ•°æ®è¡¨ç¤ºå’Œç‰¹å¾åŒ–ç­–ç•¥çš„é‡è¦æ€§ã€‚\n\n**9. Life, Machine Learning, and the Search for Habitability: Predicting Biosignature Fluxes for the Habitable Worlds Observatory**\n> **ç”Ÿå‘½ã€æœºå™¨å­¦ä¹ ä¸å®œå±…æ€§æ¢ç´¢ï¼šä¸ºå®œå±…ä¸–ç•Œå¤©æ–‡å°é¢„æµ‹ç”Ÿç‰©ç‰¹å¾é€šé‡**\n> *æ ¸å¿ƒè´¡çŒ®*ï¼šä¸º NASA æœªæ¥çš„ä»»åŠ¡å¼€å‘çš„æ¨¡å‹ï¼ˆSQuATï¼‰ï¼Œåˆ©ç”¨æŸ¥è¯¢é©±åŠ¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»ç³»å¤–è¡Œæ˜Ÿåå°„å…‰è°±ä¸­é¢„æµ‹ç”Ÿç‰©ç‰¹å¾ã€‚è¿™ä¸ä»…æ˜¯ç®—æ³•çš„èƒœåˆ©ï¼Œæ›´æ˜¯ AI åŠ©åŠ›å¯»æ‰¾å¤–æ˜Ÿç”Ÿå‘½çš„å®æˆ˜ã€‚\n\n---\n\n### ğŸ¨ å¤šæ¨¡æ€ã€3D ä¸ è§†é¢‘ç”Ÿæˆ\n\n**10. Creating Disability Story Videos with Generative AI: Motivation, Expression, and Sharing**\n> **åˆ©ç”¨ç”Ÿæˆå¼ AI åˆ¶ä½œæ®‹éšœæ•…äº‹è§†é¢‘ï¼šåŠ¨æœºã€è¡¨è¾¾ä¸åˆ†äº«**\n> *æ•™æˆç‚¹è¯„*ï¼šè¿™æ˜¯ä¸€ç¯‡å¾ˆæœ‰æ¸©åº¦çš„ HCIï¼ˆäººæœºäº¤äº’ï¼‰ç ”ç©¶ã€‚æ¢è®¨äº†ç”Ÿæˆå¼ AI å¦‚ä½•å¸®åŠ©æ®‹éšœäººå£«æ‰“ç ´åª’ä½“åˆ¶ä½œå£å’ï¼ŒåŒæ—¶ä¹ŸæŒ‡å‡ºäº†â€œèº«ä»½éšç’â€å’Œâ€œæƒ…æ„Ÿè¡¨è¾¾â€æ–¹é¢çš„è®¾è®¡å¯ç¤ºã€‚\n\n**11. Proc3D: Procedural 3D Generation and Parametric Editing of 3D Shapes with Large Language Models**\n> **Proc3Dï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç¨‹åºåŒ– 3D ç”Ÿæˆä¸å‚æ•°åŒ–ç¼–è¾‘**\n> *æ ¸å¿ƒè´¡çŒ®*ï¼šå¼•å…¥äº†ç¨‹åºåŒ–ç´§å‡‘å›¾ï¼ˆPCGï¼‰è¡¨ç¤ºï¼Œè®© LLM å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€å®æ—¶ä¿®æ”¹ 3D æ¨¡å‹çš„å‚æ•°ï¼ˆæ»‘å—ã€å¤é€‰æ¡†ï¼‰ï¼Œæ¯”ä¼ ç»Ÿé‡æ–°ç”Ÿæˆçš„æ–¹æ³•å¿« 400 å€ã€‚\n\n**12. SLAP: Scalable Language-Audio Pretraining with Variable-Duration Audio and Multi-Objective Training**\n> **SLAPï¼šå…·æœ‰å¯å˜æŒç»­æ—¶é—´éŸ³é¢‘å’Œå¤šç›®æ ‡è®­ç»ƒçš„å¯æ‰©å±•è¯­è¨€-éŸ³é¢‘é¢„è®­ç»ƒ**\n> *æ ¸å¿ƒè´¡çŒ®*ï¼šè§£å†³äº†éŸ³é¢‘-æ–‡æœ¬é¢„è®­ç»ƒä¸­æ•°æ®é‡å°å’Œæ—¶é•¿å›ºå®šçš„é—®é¢˜ã€‚SLAP æ‰©å±•åˆ°äº† 1.09 äº¿ä¸ªéŸ³é¢‘-æ–‡æœ¬å¯¹ï¼Œå¹¶åœ¨éŸ³é¢‘æ£€ç´¢ä¸Šå–å¾—äº† SOTAã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸è¯„ä¼°\n\n**13. A Scalable Entity-Based Framework for Auditing Bias in LLMs**\n> **ä¸€ä¸ªåŸºäºå®ä½“çš„å¯æ‰©å±• LLM åè§å®¡è®¡æ¡†æ¶**\n> *æ ¸å¿ƒè´¡çŒ®*ï¼šè¿™æ˜¯è¿„ä»Šä¸ºæ­¢æœ€å¤§è§„æ¨¡çš„åè§å®¡è®¡ï¼ˆ19 äº¿æ•°æ®ç‚¹ï¼‰ã€‚\n> *å‘ç°*ï¼šç»“æœä»¤äººæ‹…å¿§â€”â€”æ¨¡å‹æ™®éæƒ©ç½šå³ç¿¼æ”¿æ²»å®¶ã€åå‘è¥¿æ–¹å’Œå¯Œè£•å›½å®¶ã€åå‘è¥¿æ–¹å…¬å¸ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œ**æ¨¡å‹è§„æ¨¡è¶Šå¤§ï¼Œåè§åè€Œè¢«æ”¾å¤§äº†**ã€‚\n\n**14. Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?**\n> **é›¶æƒé™æ“çºµï¼šæˆ‘ä»¬èƒ½ä¿¡ä»»å¤§å‹å¤šæ¨¡æ€æ¨¡å‹é©±åŠ¨çš„ GUI æ™ºèƒ½ä½“å—ï¼Ÿ**\n> *æ ¸å¿ƒè´¡çŒ®*ï¼šæ­ç¤ºäº† Android GUI æ™ºèƒ½ä½“çš„ä¸€ä¸ªé‡å¤§æ¼æ´ã€‚æ”»å‡»è€…æ— éœ€ç”³è¯·å±é™©æƒé™ï¼Œåªéœ€åˆ©ç”¨ UI çŠ¶æ€åœ¨â€œè§‚å¯Ÿâ€å’Œâ€œè¡ŒåŠ¨â€ä¹‹é—´çš„å¾®å°æ—¶é—´å·®ï¼Œå°±èƒ½é‡ç»‘å®šæ™ºèƒ½ä½“çš„æ“ä½œç›®æ ‡ã€‚\n\n---\n\n### ğŸ©º åŒ»ç–—ä¸å¥åº·\n\n*   **How Clinicians Think and What AI Can Learn From It**: æ¢è®¨äº†ä¸´åºŠåŒ»ç”Ÿçš„æ€ç»´æ–¹å¼æ˜¯â€œåºæ•°éè¡¥å¿æ€§å†³ç­–â€ï¼ˆå³å¿«é€ŸèŠ‚ä¿­çš„å¯å‘å¼ï¼‰ï¼Œè€Œé AI å¸¸ç”¨çš„åŸºæ•°ä¼˜åŒ–ï¼Œå»ºè®® AI åº”æ¨¡ä»¿è¿™ç§ç¨³å¥çš„å†³ç­–æ–¹å¼ã€‚\n*   **Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models**: åˆ©ç”¨ 1700 ä¸‡æ®µ PPG æ•°æ®è®­ç»ƒçš„å¯ç©¿æˆ´è®¾å¤‡åŸºç¡€æ¨¡å‹ï¼Œæ˜¾è‘—æå‡äº†å¥åº·ç›‘æµ‹ä»»åŠ¡çš„æ€§èƒ½ã€‚\n\n---\n\n### ğŸ’¡ å…¶ä»–å€¼å¾—å…³æ³¨çš„çŸ­è®¯\n\n*   **TimeGMM (æ—¶é—´åºåˆ—)**: åŸºäºé«˜æ–¯æ··åˆæ¨¡å‹çš„å•æ¬¡å‰å‘ä¼ é€’æ¦‚ç‡é¢„æµ‹ï¼Œè§£å†³äº†ä¼ ç»Ÿé‡‡æ ·è®¡ç®—æ˜‚è´µçš„é—®é¢˜ã€‚\n*   **MemeLens (å¤šæ¨¡æ€)**: ä¸“é—¨ç”¨äºç†è§£æ¨¡å› ï¼ˆMemeï¼‰çš„å¤šè¯­è¨€å¤šä»»åŠ¡ VLMï¼Œæ•´åˆäº†å¹½é»˜ã€è®½åˆºã€ä»‡æ¨è¨€è®ºç­‰ 20 ä¸ªä»»åŠ¡ã€‚\n*   **Speculative Sampling with Reinforcement Learning**: å°†å¼ºåŒ–å­¦ä¹ å¼•å…¥æŠ•æœºé‡‡æ ·ï¼ˆSpeculative Samplingï¼‰çš„è‰ç¨¿æ ‘è¶…å‚æ•°ä¼˜åŒ–ï¼Œæ¨ç†é€Ÿåº¦æå‡é«˜è¾¾ 5.45 å€ã€‚\n\nä»Šå¤©çš„æ—¥æŠ¥å°±åˆ°è¿™é‡Œï¼Œå¸Œæœ›è¿™äº›ç¡¬æ ¸çš„ç ”ç©¶èƒ½ç»™ä½ çš„å·¥ä½œå¸¦æ¥çµæ„Ÿã€‚æˆ‘ä»¬æ˜å¤©è§ï¼",
  "papers": [
    {
      "arxiv_id": "2601.12617v1",
      "title": "Creating Disability Story Videos with Generative AI: Motivation, Expression, and Sharing",
      "title_zh": "åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½åˆ›ä½œæ®‹éšœå™äº‹è§†é¢‘ï¼šåŠ¨æœºã€è¡¨è¾¾ä¸åˆ†äº«",
      "authors": [
        "Shuo Niu",
        "Dylan Clements",
        "Hyungsin Kim"
      ],
      "abstract": "Generative AI (GenAI) is both promising and challenging in supporting people with disabilities (PwDs) in creating stories about disability. GenAI can reduce barriers to media production and inspire the creativity of PwDs, but it may also introduce biases and imperfections that hinder its adoption for personal expression. In this research, we examine how nine PwD from a disability advocacy group used GenAI to create videos sharing their disability experiences. Grounded in digital storytelling theory, we explore the motivations, expression, and sharing of PwD-created GenAI story videos. We conclude with a framework of momentous depiction, which highlights four core affordances of GenAI that either facilitate or require improvements to better support disability storytelling: non-capturable depiction, identity concealment and representation, contextual realism and consistency, and emotional articulation. Based on this framework, we further discuss design implications for GenAI in relation to story completion, media formats, and corrective mechanisms.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ®‹éšœäººå£«(PwDs)åˆ©ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)åˆ›ä½œä¸ªäººæ•…äº‹è§†é¢‘çš„åŠ¨æœºã€è¡¨è¾¾æ–¹å¼åŠåˆ†äº«è¡Œä¸ºã€‚é€šè¿‡å¯¹æ®‹éšœå€¡å¯¼ç»„ç»‡ä¸­çš„ä¹ä½æˆå‘˜è¿›è¡Œå®è¯ç ”ç©¶ï¼Œä½œè€…åˆ†æäº† GenAI åœ¨é™ä½åª’ä»‹åˆ¶ä½œé—¨æ§›å’Œæ¿€å‘åˆ›æ„æ–¹é¢çš„æ½œåŠ›ï¼Œä»¥åŠå…¶å›ºæœ‰çš„åè§å¦‚ä½•å½±å“ä¸ªäººè¡¨è¾¾ã€‚åŸºäºæ•°å­—æ•…äº‹è®²è¿°ç†è®ºï¼Œç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œé‡å¤§æç»˜â€(momentous depiction)çš„æ¡†æ¶ï¼Œå‡ç»ƒäº† GenAI åœ¨æ”¯æŒæ®‹éšœæ•…äº‹è®²è¿°ä¸­çš„å››é¡¹æ ¸å¿ƒåŠŸèƒ½ï¼šä¸å¯æ•æ‰çš„æç»˜(non-capturable depiction)ã€èº«ä»½éšè—ä¸è¡¨å¾(identity concealment and representation)ã€è¯­å¢ƒç°å®ä¸»ä¹‰ä¸ä¸€è‡´æ€§(contextual realism and consistency)ä»¥åŠæƒ…æ„Ÿè¡¨è¾¾(emotional articulation)ã€‚è¯¥ç ”ç©¶æœ€åè®¨è®ºäº† GenAI åœ¨æ•…äº‹è¡¥å…¨ã€åª’ä»‹æ ¼å¼å’Œçº é”™æœºåˆ¶æ–¹é¢çš„è®¾è®¡å¯ç¤ºï¼Œä¸ºä¼˜åŒ–è¾…åŠ©æ®‹éšœäººå£«å™äº‹çš„ç”ŸæˆæŠ€æœ¯æä¾›äº†ç†è®ºä¾æ®ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12617v1",
      "published_date": "2026-01-18 23:18:34 UTC",
      "updated_date": "2026-01-18 23:18:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:36:31.144554+00:00"
    },
    {
      "arxiv_id": "2601.12607v1",
      "title": "A Cloud-based Multi-Agentic Workflow for Science",
      "title_zh": "é¢å‘ç§‘å­¦ç ”ç©¶çš„äº‘ç«¯å¤šæ™ºèƒ½ä½“å·¥ä½œæµ",
      "authors": [
        "Anurag Acharya",
        "Timothy Vega",
        "Rizwan A. Ashraf",
        "Anshu Sharma",
        "Derek Parker",
        "Robert Rallo"
      ],
      "abstract": "As Large Language Models (LLMs) become ubiquitous across various scientific domains, their lack of ability to perform complex tasks like running simulations or to make complex decisions limits their utility. LLM-based agents bridge this gap due to their ability to call external resources and tools and thus are now rapidly gaining popularity. However, coming up with a workflow that can balance the models, cloud providers, and external resources is very challenging, making implementing an agentic system more of a hindrance than a help. In this work, we present a domain-agnostic, model-independent workflow for an agentic framework that can act as a scientific assistant while being run entirely on cloud. Built with a supervisor agent marshaling an array of agents with individual capabilities, our framework brings together straightforward tasks like literature review and data analysis with more complex ones like simulation runs. We describe the framework here in full, including a proof-of-concept system we built to accelerate the study of Catalysts, which is highly important in the field of Chemistry and Material Science. We report the cost to operate and use this framework, including the breakdown of the cost by services use. We also evaluate our system on a custom-curated synthetic benchmark and a popular Chemistry benchmark, and also perform expert validation of the system. The results show that our system is able to route the task to the correct agent 90% of the time and successfully complete the assigned task 97.5% of the time for the synthetic tasks and 91% of the time for real-world tasks, while still achieving better or comparable accuracy to most frontier models, showing that this is a viable framework for other scientific domains to replicate.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºäº‘çš„ã€é¢†åŸŸæ— å…³ä¸”æ¨¡å‹ç‹¬ç«‹çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶å·¥ä½œæµï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ‰§è¡Œå¤æ‚ç§‘å­¦æ¨¡æ‹Ÿå’Œå†³ç­–æ–¹é¢çš„èƒ½åŠ›é™åˆ¶ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ä¸»ç®¡æ™ºèƒ½ä½“(Supervisor Agent)åè°ƒå…·å¤‡ä¸åŒä¸“ä¸šèƒ½åŠ›çš„æ™ºèƒ½ä½“é˜µåˆ—ï¼Œå®ç°äº†ä»åŸºç¡€æ–‡çŒ®ç»¼è¿°ã€æ•°æ®åˆ†æåˆ°é«˜çº§ä»¿çœŸè¿è¡Œçš„è‡ªåŠ¨åŒ–æµç¨‹ã€‚ç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªé’ˆå¯¹å‚¬åŒ–å‰‚(Catalysts)ç ”ç©¶çš„éªŒè¯ç³»ç»Ÿï¼Œå¹¶è¯¦ç»†åˆ†æäº†ç³»ç»Ÿåœ¨åŒ–å­¦ä¸ææ–™ç§‘å­¦èƒŒæ™¯ä¸‹çš„è¿è¡Œæˆæœ¬ä¸æœåŠ¡æ„æˆã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼Œç³»ç»Ÿåœ¨ä»»åŠ¡è·¯ç”±ä¸Šçš„å‡†ç¡®ç‡è¾¾90%ï¼Œåœ¨åˆæˆåŠçœŸå®ä»»åŠ¡ä¸­çš„æˆåŠŸç‡åˆ†åˆ«ä¸º97.5%å’Œ91%ã€‚è¯¥æ¡†æ¶åœ¨æ€§èƒ½ä¸Šå¯åª²ç¾ç”šè‡³è¶…è¶Šç°æœ‰çš„å‰æ²¿æ¨¡å‹ï¼Œä¸ºç§‘ç ”é¢†åŸŸæ„å»ºå¯æ‰©å±•ã€è‡ªåŠ¨åŒ–çš„äº‘ç«¯åŠ©æ‰‹æä¾›äº†ä¸€ç§å¯å¤åˆ¶çš„é«˜æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12607v1",
      "published_date": "2026-01-18 22:37:09 UTC",
      "updated_date": "2026-01-18 22:37:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:36:35.297296+00:00"
    },
    {
      "arxiv_id": "2601.12594v1",
      "title": "SLAP: Scalable Language-Audio Pretraining with Variable-Duration Audio and Multi-Objective Training",
      "title_zh": "SLAPï¼šæ”¯æŒå˜é•¿éŸ³é¢‘ä¸å¤šç›®æ ‡è®­ç»ƒçš„å¯æ‰©å±•è¯­è¨€-éŸ³é¢‘é¢„è®­ç»ƒ",
      "authors": [
        "Xinhao Mei",
        "Gael Le Lan",
        "Haohe Liu",
        "Zhaoheng Ni",
        "Varun Nagaraja",
        "Yang Liu",
        "Yangyang Shi",
        "Vikas Chandra"
      ],
      "abstract": "Contrastive language-audio pretraining (CLAP) has achieved notable success in learning semantically rich audio representations and is widely adopted for various audio-related tasks. However, current CLAP models face several key limitations. First, they are typically trained on relatively small datasets, often comprising a few million audio samples. Second, existing CLAP models are restricted to short and fixed duration, which constrains their usage in real-world scenarios with variable-duration audio. Third, the standard contrastive training objective operates on global representations, which may hinder the learning of dense, fine-grained audio features. To address these challenges, we introduce Scalable Language-Audio Pretraining (SLAP), which scales language-audio pretraining to 109 million audio-text pairs with variable audio durations and incorporates multiple training objectives. SLAP unifies contrastive loss with additional self-supervised and captioning losses in a single-stage training, facilitating the learning of richer dense audio representations. The proposed SLAP model achieves new state-of-the-art performance on audio-text retrieval and zero-shot audio classification tasks, demonstrating its effectiveness across diverse benchmarks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SLAPï¼ˆScalable Language-Audio Pretrainingï¼‰ï¼Œä¸€ç§æ—¨åœ¨è§£å†³ç°æœ‰CLAPæ¨¡å‹åœ¨æ•°æ®é›†è§„æ¨¡æœ‰é™ã€éŸ³é¢‘æ—¶é•¿å›ºå®šä»¥åŠç¼ºä¹ç»†ç²’åº¦ç‰¹å¾ç­‰å±€é™æ€§çš„å¯æ‰©å±•é¢„è®­ç»ƒæ¡†æ¶ã€‚SLAPå°†è¯­è¨€-éŸ³é¢‘é¢„è®­ç»ƒè§„æ¨¡æ‰©å±•è‡³1.09äº¿ä¸ªéŸ³é¢‘-æ–‡æœ¬å¯¹ï¼Œå¹¶æ”¯æŒå¤„ç†å˜é•¿éŸ³é¢‘ï¼Œå¢å¼ºäº†æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­çš„é€‚ç”¨æ€§ã€‚é€šè¿‡åœ¨å•é˜¶æ®µè®­ç»ƒä¸­æ•´åˆå¯¹æ¯”æŸå¤±(Contrastive loss)ã€è‡ªç›‘ç£æŸå¤±(Self-supervised loss)å’Œæè¿°æŸå¤±(Captioning loss)ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå­¦ä¹ åˆ°æ›´ä¸°å¯Œçš„å¯†é›†éŸ³é¢‘è¡¨ç¤ºã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSLAPåœ¨éŸ³é¢‘-æ–‡æœ¬æ£€ç´¢å’Œé›¶æ ·æœ¬éŸ³é¢‘åˆ†ç±»ä»»åŠ¡ä¸Šå‡åˆ›ä¸‹äº†æ–°çš„SOTAæ€§èƒ½è®°å½•ï¼Œå……åˆ†éªŒè¯äº†å¤§è§„æ¨¡æ•°æ®æ‰©å±•ä¸å¤šç›®æ ‡è®­ç»ƒç»“åˆçš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12594v1",
      "published_date": "2026-01-18 21:36:19 UTC",
      "updated_date": "2026-01-18 21:36:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:36:40.200894+00:00"
    },
    {
      "arxiv_id": "2601.12585v1",
      "title": "Do MLLMs See What We See? Analyzing Visualization Literacy Barriers in AI Systems",
      "title_zh": "MLLM æ‰€è§å³äººç±»æ‰€è§å—ï¼Ÿå‰–æ AI ç³»ç»Ÿä¸­çš„å¯è§†åŒ–ç´ å…»éšœç¢",
      "authors": [
        "Mengli",
        "Duan",
        "Yuhe",
        "Jiang",
        "Matthew Varona",
        "Carolina Nobre"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) are increasingly used to interpret visualizations, yet little is known about why they fail. We present the first systematic analysis of barriers to visualization literacy in MLLMs. Using the regenerated Visualization Literacy Assessment Test (reVLAT) benchmark with synthetic data, we open-coded 309 erroneous responses from four state-of-the-art models with a barrier-centric strategy adapted from human visualization literacy research. Our analysis yields a taxonomy of MLLM failures, revealing two machine-specific barriers that extend prior human-participation frameworks. Results show that models perform well on simple charts but struggle with color-intensive, segment-based visualizations, often failing to form consistent comparative reasoning. Our findings inform future evaluation and design of reliable AI-driven visualization assistants.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨è§£è¯»å¯è§†åŒ–å›¾è¡¨æ—¶å‡ºç°å¤±è´¥çš„åŸå› è¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿæ€§åˆ†æã€‚ç ”ç©¶è€…åˆ©ç”¨åŸºäºåˆæˆæ•°æ®çš„é‡æ–°ç”Ÿæˆå¯è§†åŒ–ç´ å…»è¯„ä¼°æµ‹è¯• (reVLAT) åŸºå‡†ï¼Œå¯¹å››ç§å°–ç«¯æ¨¡å‹çš„309ä¸ªé”™è¯¯å›ç­”è¿›è¡Œäº†åŸºäºéšœç¢ä¸­å¿ƒçš„ç­–ç•¥åˆ†æã€‚åˆ†æç»“æœæ„å»ºäº† MLLM å¤±è´¥çš„åˆ†ç±»ä½“ç³»ï¼Œå¹¶æ­ç¤ºäº†ä¸¤ç§è¶…å‡ºå…ˆå‰äººç±»å‚ä¸æ¡†æ¶çš„æœºå™¨ç‰¹æœ‰éšœç¢ã€‚å®éªŒå‘ç°ï¼Œæ¨¡å‹è™½åœ¨ç®€å•å›¾è¡¨ä¸Šè¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†é¢œè‰²å¯†é›†å‹ã€åˆ†æ®µå¼å¯è§†åŒ–ä»¥åŠå½¢æˆä¸€è‡´çš„æ¯”è¾ƒæ¨ç†æ–¹é¢å­˜åœ¨æ˜æ˜¾çŸ­æ¿ã€‚è¿™é¡¹ç ”ç©¶æˆæœä¸ºæœªæ¥è¯„ä¼°å’Œè®¾è®¡æ›´å¯é çš„AIé©±åŠ¨å¯è§†åŒ–è¾…åŠ©ç³»ç»Ÿæä¾›äº†é‡è¦çš„ç†è®ºä¾æ®å’ŒæŒ‡å¯¼å»ºè®®ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.ET"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12585v1",
      "published_date": "2026-01-18 21:08:23 UTC",
      "updated_date": "2026-01-18 21:08:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:37:07.897982+00:00"
    },
    {
      "arxiv_id": "2601.12582v1",
      "title": "Ontology-aligned structuring and reuse of multimodal materials data and workflows towards automatic reproduction",
      "title_zh": "é¢å‘è‡ªåŠ¨é‡ç°çš„æœ¬ä½“å¯¹é½å¤šæ¨¡æ€ææ–™æ•°æ®ä¸å·¥ä½œæµç»“æ„åŒ–åŠå¤ç”¨",
      "authors": [
        "Sepideh Baghaee Ravari",
        "Abril Azocar Guzman",
        "Sarath Menon",
        "Stefan Sandfeld",
        "Tilmann Hickel",
        "Markus Stricker"
      ],
      "abstract": "Reproducibility of computational results remains a challenge in materials science, as simulation workflows and parameters are often reported only in unstructured text and tables. While literature data are valuable for validation and reuse, the lack of machine-readable workflow descriptions prevents large-scale curation and systematic comparison. Existing text-mining approaches are insufficient to extract complete computational workflows with their associated parameters. An ontology-driven, large language model (LLM)-assisted framework is introduced for the automated extraction and structuring of computational workflows from the literature. The approach focuses on density functional theory-based stacking fault energy (SFE) calculations in hexagonal close-packed magnesium and its binary alloys, and uses a multi-stage filtering strategy together with prompt-engineered LLM extraction applied to method sections and tables. Extracted information is unified into a canonical schema and aligned with established materials ontologies (CMSO, ASMO, and PLDO), enabling the construction of a knowledge graph using atomRDF. The resulting knowledge graph enables systematic comparison of reported SFE values and supports the structured reuse of computational protocols. While full computational reproducibility is still constrained by missing or implicit metadata, the framework provides a foundation for organizing and contextualizing published results in a semantically interoperable form, thereby improving transparency and reusability of computational materials data.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹ææ–™ç§‘å­¦ä¸­è®¡ç®—æ¨¡æ‹Ÿå·¥ä½œæµå’Œå‚æ•°é€šå¸¸ä»¥éç»“æ„åŒ–æ–‡æœ¬å’Œè¡¨æ ¼å½¢å¼å‘ˆç°ï¼Œå¯¼è‡´ç»“æœéš¾ä»¥å¤ç°å’Œå¤§è§„æ¨¡ç®¡ç†çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªæœ¬ä½“é©±åŠ¨(Ontology-driven)ä¸”ç”±å¤§è¯­è¨€æ¨¡å‹(LLM)è¾…åŠ©çš„æ¡†æ¶ï¼Œç”¨äºè‡ªåŠ¨æå–å’Œç»“æ„åŒ–æ–‡çŒ®ä¸­çš„è®¡ç®—å·¥ä½œæµã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤šé˜¶æ®µè¿‡æ»¤ç­–ç•¥å’Œé’ˆå¯¹æ€§æç¤ºå·¥ç¨‹(Prompt-engineered)çš„LLMæå–æŠ€æœ¯ï¼Œé‡ç‚¹å¤„ç†äº†å…­æ–¹æœ€å¯†å †ç§¯(HCP)é•åŠå…¶äºŒå…ƒåˆé‡‘ä¸­åŸºäºå¯†åº¦æ³›å‡½ç†è®º(Density functional theory)çš„å±‚é”™èƒ½(Stacking fault energy)è®¡ç®—æ•°æ®ã€‚æå–çš„ä¿¡æ¯è¢«ç»Ÿä¸€æ˜ å°„è‡³è§„èŒƒæ¨¡å¼ï¼Œå¹¶ä¸CMSOã€ASMOå’ŒPLDOç­‰ææ–™æœ¬ä½“å¯¹é½ï¼Œè¿›è€Œé€šè¿‡atomRDFæ„å»ºçŸ¥è¯†å›¾è°±(Knowledge graph)ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿå®ç°å¯¹å·²å‘è¡¨SFEå€¼çš„ç³»ç»Ÿæ€§å¯¹æ¯”ï¼Œå¹¶æ”¯æŒè®¡ç®—åè®®çš„ç»“æ„åŒ–å¤ç”¨ã€‚å°½ç®¡å®Œå…¨çš„è®¡ç®—å¯å¤ç°æ€§ä»å—é™äºéƒ¨åˆ†å…ƒæ•°æ®çš„ç¼ºå¤±ï¼Œä½†è¯¥æ–¹æ¡ˆä¸ºä»¥è¯­ä¹‰äº’æ“ä½œå½¢å¼ç»„ç»‡å‘è¡¨ç»“æœå¥ å®šäº†åŸºç¡€ï¼Œæ˜¾è‘—æå‡äº†ææ–™è®¡ç®—æ•°æ®çš„é€æ˜åº¦å’Œå¯é‡ç”¨æ€§ã€‚",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "39 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.12582v1",
      "published_date": "2026-01-18 20:51:23 UTC",
      "updated_date": "2026-01-18 20:51:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:36:35.796410+00:00"
    },
    {
      "arxiv_id": "2601.12577v1",
      "title": "Primate-like perceptual decision making emerges through deep recurrent reinforcement learning",
      "title_zh": "ç±»çµé•¿ç±»æ„ŸçŸ¥å†³ç­–é€šè¿‡æ·±åº¦å¾ªç¯å¼ºåŒ–å­¦ä¹ å¾—ä»¥æ¶Œç°",
      "authors": [
        "Nathan J. Wispinski",
        "Scott A. Stone",
        "Anthony Singhal",
        "Patrick M. Pilarski",
        "Craig S. Chapman"
      ],
      "abstract": "Progress has led to a detailed understanding of the neural mechanisms that underlie decision making in primates. However, less is known about why such mechanisms are present in the first place. Theory suggests that primate decision making mechanisms, and their resultant behavioral abilities, emerged to maximize reward in the face of noisy, temporally evolving information. To test this theory, we trained an end-to-end deep recurrent neural network using reinforcement learning on a noisy perceptual discrimination task. Networks learned several key abilities of primate-like decision making including trading off speed for accuracy, and flexibly changing their mind in the face of new information. Internal dynamics of these networks suggest that these abilities were supported by similar decision mechanisms as those observed in primate neurophysiological studies. These results provide experimental support for key pressures that gave rise to the primate ability to make flexible decisions.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†çµé•¿ç±»åŠ¨ç‰©å†³ç­–ç¥ç»æœºåˆ¶çš„èµ·æºï¼Œæ—¨åœ¨éªŒè¯è¿™äº›æœºåˆ¶æ˜¯ä¸ºäº†åœ¨å™ªå£°å’Œéšæ—¶é—´å˜åŒ–çš„ä¿¡æ¯ä¸­å®ç°å¥–åŠ±æœ€å¤§åŒ–è€Œæ¼”åŒ–çš„ç†è®ºã€‚ç ”ç©¶è€…åˆ©ç”¨å¼ºåŒ–å­¦ä¹  (Reinforcement Learning) è®­ç»ƒäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œ (Deep Recurrent Neural Network)ï¼Œä½¿å…¶æ‰§è¡Œä¸€é¡¹å¸¦æœ‰å™ªå£°çš„æ„ŸçŸ¥è¾¨åˆ«ä»»åŠ¡ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç½‘ç»œå­¦ä¼šäº†çµé•¿ç±»å†³ç­–çš„å‡ é¡¹æ ¸å¿ƒèƒ½åŠ›ï¼ŒåŒ…æ‹¬åœ¨é€Ÿåº¦ä¸å‡†ç¡®ç‡ä¹‹é—´è¿›è¡Œæƒè¡¡ (Speed-Accuracy Trade-off)ï¼Œä»¥åŠæ ¹æ®æ–°ä¿¡æ¯çµæ´»åœ°æ”¹å˜ä¸»æ„ã€‚ç½‘ç»œå†…éƒ¨åŠ¨åŠ›å­¦çš„åˆ†æè¡¨æ˜ï¼Œè¿™äº›èƒ½åŠ›çš„å®ç°ä¾èµ–äºä¸çµé•¿ç±»ç¥ç»ç”Ÿç†å­¦ç ”ç©¶ä¸­è§‚å¯Ÿåˆ°çš„ç›¸ä¼¼å†³ç­–æœºåˆ¶ã€‚è¿™äº›ç»“æœä¸ºä¿ƒä½¿çµé•¿ç±»äº§ç”Ÿçµæ´»å†³ç­–èƒ½åŠ›çš„è¿›åŒ–å‹åŠ›æä¾›äº†å®éªŒæ”¯æŒï¼Œè¯æ˜äº†å¤æ‚å†³ç­–æœºåˆ¶å¯ä»¥é€šè¿‡æœ€å¤§åŒ–ä»»åŠ¡å¥–åŠ±è€Œè‡ªç„¶æ¶Œç°ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI"
      ],
      "primary_category": "q-bio.NC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12577v1",
      "published_date": "2026-01-18 20:43:53 UTC",
      "updated_date": "2026-01-18 20:43:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:36:54.294279+00:00"
    },
    {
      "arxiv_id": "2601.15324v1",
      "title": "Prometheus Mind: Retrofitting Memory to Frozen Language Models",
      "title_zh": "Prometheus Mindï¼šä¸ºå†»ç»“è¯­è¨€æ¨¡å‹åŠ è£…è®°å¿†æœºåˆ¶",
      "authors": [
        "Mark Wind"
      ],
      "abstract": "Adding memory to pretrained language models typically requires architectural changes or weight modification. We present Prometheus Mind, which retrofits memory to a frozen Qwen3-4B using 11 modular adapters (530MB, 7% overhead) -- fully reversible by removing the adapters. Building this system required solving four problems: (1) Extraction -- we develop Contrastive Direction Discovery (CDD), which finds semantic directions via minimal pairs without labeled data. (2) Training -- end-to-end optimization collapses; stage-wise training of each adapter on simple proxy tasks succeeds. (3) Injection -- learned encoders fail to generalize; we find that lm_head.weight rows already provide the mapping we need, requiring no training. (4) Hidden state collapse -- transformers make ``wife'' and ``brother'' 0.98+ similar; we train projections to recover distinction (0.98 $\\rightarrow$ 0.09). On PrometheusExtract-132 (132 cases), the system achieves 94.4% retrieval on clean inputs (n=54, 95% CI: [84.9%, 98.1%]), degrading to 19.4% on informal inputs with ellipsis, filler words, or implicit subjects (n=36). The primary bottleneck is relation classification (47.3% accuracy), responsible for most extraction errors.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Prometheus Mindï¼Œä¸€ç§é€šè¿‡ 11 ä¸ªæ¨¡å—åŒ–é€‚é…å™¨ï¼ˆModular Adaptersï¼‰ä¸ºå†»ç»“çš„ Qwen3-4B æ¨¡å‹åŠ è£…è®°å¿†çš„æ–¹æ¡ˆï¼Œè¯¥ç³»ç»Ÿå…·æœ‰å®Œå…¨å¯é€†æ€§ä¸”ä»…å¸¦æ¥ 7% çš„é¢å¤–å¼€é”€ã€‚ä¸ºäº†å®ç°é«˜æ•ˆçš„è®°å¿†æå–ï¼Œä½œè€…å¼€å‘äº†å¯¹æ¯”æ–¹å‘å‘ç°ï¼ˆContrastive Direction Discovery, CDDï¼‰æŠ€æœ¯ï¼Œèƒ½åœ¨æ— æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹é€šè¿‡æœ€å°å¯¹ï¼ˆMinimal Pairsï¼‰å‘ç°è¯­ä¹‰æ–¹å‘ã€‚åœ¨è®­ç»ƒä¸æ³¨å…¥é˜¶æ®µï¼Œç ”ç©¶é‡‡ç”¨åˆ†é˜¶æ®µè®­ç»ƒï¼ˆStage-wise Trainingï¼‰å…‹æœç«¯åˆ°ç«¯ä¼˜åŒ–çš„å´©æºƒï¼Œå¹¶å·§å¦™åˆ©ç”¨ lm_head.weight çš„è¡Œå‘é‡è¿›è¡Œæ˜ å°„è€Œæ— éœ€é¢å¤–ç¼–ç å™¨è®­ç»ƒã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶é€šè¿‡è®­ç»ƒæŠ•å½±å±‚è§£å†³äº†éšè—çŠ¶æ€åç¼©ï¼ˆHidden State Collapseï¼‰é—®é¢˜ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹å¯¹ç›¸ä¼¼è¯æ±‡çš„åŒºåˆ†åº¦ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿåœ¨å¹²å‡€è¾“å…¥ä¸‹è¾¾åˆ°äº† 94.4% çš„æ£€ç´¢å‡†ç¡®ç‡ï¼Œä½†åœ¨å¤„ç†åŒ…å«çœç•¥æˆ–å¡«å……è¯çš„éæ­£å¼è¾“å…¥æ—¶æ€§èƒ½è¡¨ç°è¾ƒå·®ã€‚å½“å‰ç³»ç»Ÿçš„ä¸»è¦ç“¶é¢ˆåœ¨äºå…³ç³»åˆ†ç±»ï¼ˆRelation Classificationï¼‰ï¼Œå…¶ 47.3% çš„å‡†ç¡®ç‡é™åˆ¶äº†æ•´ä½“çš„æå–æ•ˆæœã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "28 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.15324v1",
      "published_date": "2026-01-18 20:29:07 UTC",
      "updated_date": "2026-01-18 20:29:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:36:49.292853+00:00"
    },
    {
      "arxiv_id": "2601.12560v1",
      "title": "Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and Evaluation of Large Language Model Agents",
      "title_zh": "æ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ (AI)ï¼šå¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„æ¶æ„ã€åˆ†ç±»ä½“ç³»ä¸è¯„ä¼°",
      "authors": [
        "Arunkumar V",
        "Gangadharan G. R.",
        "Rajkumar Buyya"
      ],
      "abstract": "Artificial Intelligence is moving from models that only generate text to Agentic AI, where systems behave as autonomous entities that can perceive, reason, plan, and act. Large Language Models (LLMs) are no longer used only as passive knowledge engines but as cognitive controllers that combine memory, tool use, and feedback from their environment to pursue extended goals. This shift already supports the automation of complex workflows in software engineering, scientific discovery, and web navigation, yet the variety of emerging designs, from simple single loop agents to hierarchical multi agent systems, makes the landscape hard to navigate. In this paper, we investigate architectures and propose a unified taxonomy that breaks agents into Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this lens to describe the move from linear reasoning procedures to native inference time reasoning models, and the transition from fixed API calls to open standards like the Model Context Protocol (MCP) and Native Computer Use. We also group the environments in which these agents operate, including digital operating systems, embodied robotics, and other specialized domains, and we review current evaluation practices. Finally, we highlight open challenges, such as hallucination in action, infinite loops, and prompt injection, and outline future research directions toward more robust and reliable autonomous systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½ä»ç®€å•çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹å‘ Agentic AI çš„è½¬å˜ï¼Œå³ç³»ç»Ÿä½œä¸ºèƒ½å¤Ÿæ„ŸçŸ¥ã€æ¨ç†ã€è§„åˆ’å’Œè¡ŒåŠ¨çš„è‡ªä¸»å®ä½“ã€‚è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ç±»æ³• (Taxonomy)ï¼Œå°†æ™ºèƒ½ä½“æ¶æ„ç»†åˆ†ä¸ºæ„ŸçŸ¥ (Perception)ã€å¤§è„‘ (Brain)ã€è§„åˆ’ (Planning)ã€è¡ŒåŠ¨ (Action)ã€å·¥å…·ä½¿ç”¨ (Tool Use) å’Œåä½œ (Collaboration) å…­ä¸ªæ ¸å¿ƒç»„ä»¶ã€‚ç ”ç©¶è¯¦ç»†åˆ†æäº†ä»çº¿æ€§æ¨ç†ç¨‹åºå‘åŸç”Ÿæ¨ç†æ¨¡å‹ (Native inference-time reasoning models) çš„æ¼”è¿›ï¼Œä»¥åŠä»å›ºå®š API è°ƒç”¨å‘ Model Context Protocol (MCP) å’ŒåŸç”Ÿè®¡ç®—æœºä½¿ç”¨ (Native Computer Use) ç­‰å¼€æ”¾æ ‡å‡†çš„è½¬å˜ã€‚æ–‡ç« è¿˜å¯¹æ™ºèƒ½ä½“è¿è¡Œçš„ç¯å¢ƒè¿›è¡Œäº†å½’ç±»ï¼Œæ¶µç›–æ•°å­—æ“ä½œç³»ç»Ÿã€å…·èº«æœºå™¨äºº (Embodied robotics) ç­‰ä¸“ä¸šé¢†åŸŸï¼Œå¹¶ç³»ç»Ÿå›é¡¾äº†å½“å‰çš„è¯„ä¼°å®è·µã€‚æœ€åï¼Œç ”ç©¶æŒ‡å‡ºäº†å¹»è§‰ã€æ­»å¾ªç¯å’Œæç¤ºè¯æ³¨å…¥ (Prompt injection) ç­‰ä¸¥å³»æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥æ„å»ºæ›´ç¨³å¥ã€å¯é çš„è‡ªä¸»ç³»ç»ŸæŒ‡æ˜äº†ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.AI",
      "comment": "28 pages, 4 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.12560v1",
      "published_date": "2026-01-18 19:51:16 UTC",
      "updated_date": "2026-01-18 19:51:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:36:55.895572+00:00"
    },
    {
      "arxiv_id": "2601.12557v1",
      "title": "Life, Machine Learning, and the Search for Habitability: Predicting Biosignature Fluxes for the Habitable Worlds Observatory",
      "title_zh": "ç”Ÿå‘½ã€æœºå™¨å­¦ä¹ ä¸å®œå±…æ€§æ¢ç´¢ï¼šé¢å‘ Habitable Worlds Observatory çš„ç”Ÿç‰©æ ‡å¿—ç‰©é€šé‡é¢„æµ‹",
      "authors": [
        "Mark Moussa",
        "Amber V. Young",
        "Brianna Isola",
        "Vasuda Trehan",
        "Michael D. Himes",
        "Nicholas Wogan",
        "Giada Arney"
      ],
      "abstract": "Future direct-imaging flagship missions, such as NASA's Habitable Worlds Observatory (HWO), face critical decisions in prioritizing observations due to extremely stringent time and resource constraints. In this paper, we introduce two advanced machine-learning architectures tailored for predicting biosignature species fluxes from exoplanetary reflected-light spectra: a Bayesian Convolutional Neural Network (BCNN) and our novel model architecture, the Spectral Query Adaptive Transformer (SQuAT). The BCNN robustly quantifies both epistemic and aleatoric uncertainties, offering reliable predictions under diverse observational conditions, whereas SQuAT employs query-driven attention mechanisms to enhance interpretability by explicitly associating spectral features with specific biosignature species. We demonstrate that both models achieve comparably high predictive accuracy on an augmented dataset spanning a wide range of exoplanetary conditions, while highlighting their distinct advantages in uncertainty quantification and spectral interpretability. These capabilities position our methods as promising tools for accelerating target triage, optimizing observation schedules, and maximizing scientific return for upcoming flagship missions such as HWO.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹NASAçš„Habitable Worlds Observatory (HWO)ç­‰æœªæ¥ç›´æ¥æˆåƒæ——èˆ°ä»»åŠ¡é¢ä¸´çš„ä¸¥æ ¼è§‚æµ‹èµ„æºé™åˆ¶ï¼Œå¼•å…¥äº†ä¸¤ç§ç”¨äºä»ç³»å¤–è¡Œæ˜Ÿåå°„å…‰å…‰è°±é¢„æµ‹ç”Ÿç‰©ç‰¹å¾ç‰©è´¨é€šé‡(biosignature species fluxes)çš„é«˜çº§æœºå™¨å­¦ä¹ æ¶æ„ã€‚ç ”ç©¶æå‡ºäº†Bayesian Convolutional Neural Network (BCNN)ä»¥åŠä¸€ç§åä¸ºSpectral Query Adaptive Transformer (SQuAT)çš„æ–°å‹æ¨¡å‹æ¶æ„ã€‚BCNNé€šè¿‡ç¨³å¥åœ°é‡åŒ–è®¤çŸ¥ä¸å¶ç„¶ä¸ç¡®å®šæ€§(epistemic and aleatoric uncertainties)ï¼Œåœ¨å¤šæ ·åŒ–è§‚æµ‹æ¡ä»¶ä¸‹æä¾›å¯é é¢„æµ‹ï¼›SQuATåˆ™åˆ©ç”¨æŸ¥è¯¢é©±åŠ¨çš„æ³¨æ„åŠ›æœºåˆ¶(query-driven attention mechanisms)ï¼Œé€šè¿‡å°†å…‰è°±ç‰¹å¾ä¸ç‰¹å®šç”Ÿç‰©ç‰¹å¾ç›´æ¥å…³è”æ¥å¢å¼ºæ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸¤ç§æ¨¡å‹åœ¨æ¶µç›–å¹¿æ³›ç³»å¤–è¡Œæ˜Ÿæ¡ä»¶çš„å¢å¼ºæ•°æ®é›†ä¸Šå‡è¾¾åˆ°äº†æé«˜çš„é¢„æµ‹å‡†ç¡®ç‡ã€‚è¿™äº›æ–¹æ³•ä¸ºåŠ é€Ÿç›®æ ‡ç­›é€‰(target triage)å’Œä¼˜åŒ–è§‚æµ‹è®¡åˆ’æä¾›äº†æœ‰åŠ›å·¥å…·ï¼Œæœ‰æœ›æ˜¾è‘—æå‡HWOç­‰æœªæ¥æ——èˆ°ä»»åŠ¡çš„ç§‘å­¦å›æŠ¥ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 4 figures. Submitted and accepted in AAAI-26 (IAAI Emerging Applications track)",
      "pdf_url": "https://arxiv.org/pdf/2601.12557v1",
      "published_date": "2026-01-18 19:43:48 UTC",
      "updated_date": "2026-01-18 19:43:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:36:59.302006+00:00"
    },
    {
      "arxiv_id": "2601.12554v1",
      "title": "Artificial Intelligence in Materials Science and Engineering: Current Landscape, Key Challenges, and Future Trajectorie",
      "title_zh": "äººå·¥æ™ºèƒ½åœ¨ææ–™ç§‘å­¦ä¸å·¥ç¨‹ä¸­çš„åº”ç”¨ï¼šç°çŠ¶ã€å…³é”®æŒ‘æˆ˜ä¸æœªæ¥å‘å±•è¶‹åŠ¿",
      "authors": [
        "Iman Peivaste",
        "Salim Belouettar",
        "Francesco Mercuri",
        "Nicholas Fantuzzi",
        "Hamidreza Dehghani",
        "Razieh Izadi",
        "Halliru Ibrahim",
        "Jakub Lengiewicz",
        "MaÃ«l Belouettar-Mathis",
        "Kouider Bendine",
        "Ahmed Makradi",
        "Martin HÃ¶rsch",
        "Peter Klein",
        "Mohamed El Hachemi",
        "Heinz A. Preisig",
        "Yacine Rezgui",
        "Natalia Konchakova",
        "Ali Daouadji"
      ],
      "abstract": "Artificial Intelligence is rapidly transforming materials science and engineering, offering powerful tools to navigate complexity, accelerate discovery, and optimize material design in ways previously unattainable. Driven by the accelerating pace of algorithmic advancements and increasing data availability, AI is becoming an essential competency for materials researchers. This review provides a comprehensive and structured overview of the current landscape, synthesizing recent advancements and methodologies for materials scientists seeking to effectively leverage these data-driven techniques. We survey the spectrum of machine learning approaches, from traditional algorithms to advanced deep learning architectures, including CNNs, GNNs, and Transformers, alongside emerging generative AI and probabilistic models such as Gaussian Processes for uncertainty quantification. The review also examines the pivotal role of data in this field, emphasizing how effective representation and featurization strategies, spanning compositional, structural, image-based, and language-inspired approaches, combined with appropriate preprocessing, fundamentally underpin the performance of machine learning models in materials research. Persistent challenges related to data quality, quantity, and standardization, which critically impact model development and application in materials science and engineering, are also addressed.",
      "tldr_zh": "è¯¥ç»¼è¿°å…¨é¢æ¢³ç†äº†äººå·¥æ™ºèƒ½(Artificial Intelligence)åœ¨ææ–™ç§‘å­¦ä¸å·¥ç¨‹é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œå±•ç¤ºäº†å…¶åœ¨åŠ é€Ÿææ–™å‘ç°å’Œä¼˜åŒ–è®¾è®¡æ–¹é¢çš„æ ¸å¿ƒä½œç”¨ã€‚æ–‡ç« ç³»ç»Ÿä»‹ç»äº†ä»ä¼ ç»Ÿæœºå™¨å­¦ä¹ (machine learning)ç®—æ³•åˆ°æ·±åº¦å­¦ä¹ (deep learning)æ¶æ„çš„æŠ€æœ¯è°±ç³»ï¼Œé‡ç‚¹è®¨è®ºäº†å·ç§¯ç¥ç»ç½‘ç»œ(CNNs)ã€å›¾ç¥ç»ç½‘ç»œ(GNNs)ã€Transformerä»¥åŠæ–°å…´çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(generative AI)å’Œç”¨äºä¸ç¡®å®šæ€§é‡åŒ–çš„é«˜æ–¯è¿‡ç¨‹(Gaussian Processes)æ¨¡å‹ã€‚ç ”ç©¶æ·±å…¥æ¢è®¨äº†æ•°æ®åœ¨å…¶ä¸­çš„å…³é”®åœ°ä½ï¼Œè¯¦ç»†åˆ†æäº†å¦‚ä½•é€šè¿‡æˆåˆ†ã€ç»“æ„åŠå›¾åƒç­‰å¤šç§ç‰¹å¾åŒ–(featurization)å’Œè¡¨ç¤º(representation)ç­–ç•¥æ¥æ”¯æ’‘æ¨¡å‹æ€§èƒ½ã€‚æ–‡ç« è¿›ä¸€æ­¥é˜è¿°äº†æ•°æ®é¢„å¤„ç†çš„é‡è¦æ€§ï¼Œå¹¶å‰–æäº†å½“å‰é¢ä¸´çš„æ•°æ®è´¨é‡ã€æ ·æœ¬é‡åŠæ ‡å‡†åŒ–ç­‰æŒç»­å­˜åœ¨çš„æŒ‘æˆ˜ã€‚æœ€åï¼Œè¯¥ç»¼è¿°ä¸ºç ”ç©¶äººå‘˜æä¾›äº†åˆ©ç”¨æ•°æ®é©±åŠ¨æ–¹æ³•åº”å¯¹å¤æ‚ææ–™ç§‘å­¦éš¾é¢˜çš„ç»“æ„åŒ–æŒ‡å¯¼ï¼Œå¹¶æŒ‡æ˜äº†è¯¥é¢†åŸŸæœªæ¥çš„å‘å±•è½¨è¿¹ã€‚",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI",
        "physics.comp-ph",
        "quant-ph"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12554v1",
      "published_date": "2026-01-18 19:36:10 UTC",
      "updated_date": "2026-01-18 19:36:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:37:08.297041+00:00"
    },
    {
      "arxiv_id": "2601.12549v1",
      "title": "Benchmarking Concept-Spilling Across Languages in LLMs",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹è·¨è¯­è¨€æ¦‚å¿µæº¢å‡ºçš„åŸºå‡†æµ‹è¯•",
      "authors": [
        "Ilia Badanin",
        "Daniil Dzenhaliou",
        "Imanol Schlag"
      ],
      "abstract": "Multilingual Large Language Models (LLMs) exhibit remarkable cross-lingual abilities, yet often exhibit a systematic bias toward the representations from other languages, resulting in semantic interference when generating content in non-English languages$-$a phenomenon we define as language spilling. This paper presents a novel comparative framework for evaluating multilingual semantic robustness by systematically measuring how models handle polysemous words across languages. Our methodology provides a relative measure of model performance: when required to generate exactly five meanings, both strong and weak models may resort to meanings from dominant languages, but semantically stronger models do so later in the generation sequence, producing more true meanings from the target language before failing, while weaker models resort to dominant-language meanings earlier in the sequence. We evaluate a diverse set of open and closed multilingual LLMs using a structured meaning generation task across nine languages, employing a carefully curated benchmark of 100 high-polysemy English words. Our findings reveal significant variation in semantic robustness across both models and languages, providing a principled ranking system for model comparison without requiring definitive causal attribution of error sources. We contribute both a scalable comparative benchmark for multilingual semantic evaluation and a rigorous validation pipeline$-$critical tools for developing more linguistically balanced AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šè¯­è¨€å¤§è¯­è¨€æ¨¡å‹(LLMs)ä¸­çš„è¯­è¨€æº¢å‡º(language spilling)ç°è±¡ï¼Œå³æ¨¡å‹åœ¨ç”Ÿæˆéè‹±è¯­å†…å®¹æ—¶å¸¸å› åå‘å…¶ä»–è¯­è¨€çš„è¡¨ç¤ºè€Œäº§ç”Ÿè¯­ä¹‰å¹²æ‰°ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§è¯„ä¼°å¤šè¯­è¨€è¯­ä¹‰é²æ£’æ€§(semantic robustness)çš„æ–°å‹æ¯”è¾ƒæ¡†æ¶ï¼Œé€šè¿‡è¡¡é‡æ¨¡å‹å¤„ç†è·¨è¯­è¨€å¤šä¹‰è¯(polysemous words)çš„èƒ½åŠ›æ¥ç³»ç»Ÿè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•åŸºäºç‰¹å®šçš„è¯ä¹‰ç”Ÿæˆä»»åŠ¡ï¼Œå‘ç°è¯­ä¹‰è¾ƒå¼ºçš„æ¨¡å‹åœ¨è½¬å‘ä¸»å¯¼è¯­è¨€å«ä¹‰å‰èƒ½äº§ç”Ÿæ›´å¤šç›®æ ‡è¯­è¨€(target language)çš„çœŸå®è¯ä¹‰ï¼Œè€Œå¼±æ¨¡å‹åˆ™æº¢å‡ºè¾ƒæ—©ã€‚å®éªŒé€šè¿‡ä¹ç§è¯­è¨€å’Œ100ä¸ªé«˜å¤šä¹‰è¯åŸºå‡†å¯¹å¤šç§å¼€æºåŠé—­æºLLMsè¿›è¡Œäº†æµ‹è¯•ï¼Œæ­ç¤ºäº†ä¸åŒæ¨¡å‹é—´æ˜¾è‘—çš„é²æ£’æ€§å·®å¼‚ã€‚è¯¥ç ”ç©¶è´¡çŒ®äº†å¯æ‰©å±•çš„æ¯”è¾ƒåŸºå‡†å’ŒéªŒè¯æµæ°´çº¿ï¼Œä¸ºå¼€å‘è¯­è¨€å¹³è¡¡çš„AIç³»ç»Ÿæä¾›äº†é‡è¦å·¥å…·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12549v1",
      "published_date": "2026-01-18 19:28:26 UTC",
      "updated_date": "2026-01-18 19:28:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:37:43.968574+00:00"
    },
    {
      "arxiv_id": "2601.12547v1",
      "title": "How Clinicians Think and What AI Can Learn From It",
      "title_zh": "ä¸´åºŠåŒ»ç”Ÿçš„æ€ç»´é€»è¾‘åŠå…¶å¯¹äººå·¥æ™ºèƒ½çš„å¯ç¤º",
      "authors": [
        "Dipayan Sengupta",
        "Saumya Panda"
      ],
      "abstract": "Most clinical AI systems operate as prediction engines -- producing labels or risk scores -- yet real clinical reasoning is a time-bounded, sequential control problem under uncertainty. Clinicians interleave information gathering with irreversible actions, guided by regret, constraints and patient values. We argue that the dominant computational substrate of clinician reasoning is not cardinal optimization but ordinal, non-compensatory decision-making: Clinicians frequently rely on fast-and-frugal, lexicographic heuristics (e.g., fast-and-frugal trees) that stop early after checking a small, fixed sequence of cues. We provide a normative rationale for why such algorithms are not merely bounded rationality shortcuts, but can be epistemically preferred in medicine. First, many clinical trade-offs are constructed through human judgment and are only weakly measurable on absolute scales; without strong measurement axioms, only orderings are invariant, motivating an ordinal-by-default stance. Second, preference and signal elicitation are structurally crude: The mapping from truth $\\to$ perception $\\to$ inference $\\to$ recorded variables introduces layered noise, leaving a persistent uncertainty floor. When this 'crudeness' overwhelms the decision margin, plug-in expected-utility optimization becomes brittle (high flip probability under small perturbations), whereas robust dominance/filtering rules ($Îµ$-dominance, maximin) stabilize decisions.Finally, we outline a clinician-aligned AI blueprint: Use rich models for beliefs and trajectories, but choose actions through robust ordinal rules; treat heuristics as the low-dimensional special case; and deploy AI as 'selective complexity' -- invoked mainly for tie-breaking when decisions are fragile and information has positive expected impact.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ä¸´åºŠåŒ»ç”Ÿçš„æ€ç»´æ¨¡å¼ï¼ŒæŒ‡å‡ºå½“å‰çš„ä¸´åºŠäººå·¥æ™ºèƒ½(AI)å¤šè¢«è§†ä¸ºé¢„æµ‹å¼•æ“ï¼Œè€Œå¿½è§†äº†ä¸´åºŠæ¨ç†æœ¬è´¨ä¸Šæ˜¯ä¸ç¡®å®šæ€§ä¸‹çš„åºåˆ—æ§åˆ¶é—®é¢˜ã€‚ä½œè€…è®¤ä¸ºä¸´åºŠæ¨ç†çš„æ ¸å¿ƒå¹¶éåŸºæ•°ä¼˜åŒ–(cardinal optimization)ï¼Œè€Œæ˜¯åºæ•°(ordinal)ä¸”éè¡¥å¿æ€§(non-compensatory)çš„å†³ç­–è¿‡ç¨‹ï¼ŒåŒ»ç”Ÿå¸¸ä¾èµ–ç®€æ´å¯å‘å¼(fast-and-frugal heuristics)è¿›è¡Œé«˜æ•ˆåˆ¤æ–­ã€‚ç ”ç©¶è®ºè¯äº†è¿™ç§ç­–ç•¥åœ¨åŒ»å­¦ä¸­å…·æœ‰è®¤è¯†è®ºä¸Šçš„ä¼˜è¶Šæ€§ï¼Œå› ä¸ºä¸´åºŠæƒè¡¡å¾€å¾€éš¾ä»¥ç²¾ç¡®é‡åŒ–ï¼Œä¸”æ•°æ®ä¸­å­˜åœ¨çš„å¤šå±‚å™ªå£°ä½¿å¾—æœŸæœ›æ•ˆç”¨ä¼˜åŒ–(expected-utility optimization)ææ˜“å¤±æ•ˆï¼Œè€Œç¨³å¥æ”¯é…/è¿‡æ»¤è§„åˆ™(robust dominance/filtering rules)åˆ™æ›´ä¸ºç¨³å®šã€‚æœ€ç»ˆï¼Œæ–‡ç« æå‡ºäº†ä¸€ä¸ªä¸ä¸´åºŠåŒ»ç”Ÿå¯¹é½çš„AIè“å›¾ï¼Œä¸»å¼ åœ¨å»ºæ¨¡æ—¶ä¿ç•™å¤æ‚æ€§ï¼Œä½†åœ¨åŠ¨ä½œé€‰æ‹©ä¸Šéµå¾ªç¨³å¥çš„åºæ•°è§„åˆ™ã€‚è¯¥æ¡†æ¶å°†AIå®šä½ä¸ºä¸€ç§â€œé€‰æ‹©æ€§å¤æ‚åº¦â€(selective complexity)ï¼Œä»…åœ¨å†³ç­–è„†å¼±ä¸”ä¿¡æ¯å…·æœ‰æ­£å‘å½±å“æ—¶ä»‹å…¥ï¼Œä¸ºå¼€å‘æ›´å…·é²æ£’æ€§ä¸”ç¬¦åˆåŒ»å­¦å®è·µé€»è¾‘çš„æ™ºèƒ½ç³»ç»Ÿå¥ å®šäº†ç†è®ºåŸºç¡€ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "34 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.12547v1",
      "published_date": "2026-01-18 19:19:41 UTC",
      "updated_date": "2026-01-18 19:19:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:37:38.066227+00:00"
    },
    {
      "arxiv_id": "2601.12542v1",
      "title": "Rethinking the AI Scientist: Interactive Multi-Agent Workflows for Scientific Discovery",
      "title_zh": "é‡æ–°å®¡è§† AI ç§‘å­¦å®¶ï¼šé¢å‘ç§‘å­¦å‘ç°çš„äº¤äº’å¼å¤šæ™ºèƒ½ä½“å·¥ä½œæµ",
      "authors": [
        "Lukas Weidener",
        "Marko BrkiÄ‡",
        "Mihailo JovanoviÄ‡",
        "Ritvik Singh",
        "Chiara Baccin",
        "Emre Ulgac",
        "Alex Dobrin",
        "Aakaash Meduri"
      ],
      "abstract": "Artificial intelligence systems for scientific discovery have demonstrated remarkable potential, yet existing approaches remain largely proprietary and operate in batch-processing modes requiring hours per research cycle, precluding real-time researcher guidance. This paper introduces Deep Research, a multi-agent system enabling interactive scientific investigation with turnaround times measured in minutes. The architecture comprises specialized agents for planning, data analysis, literature search, and novelty detection, unified through a persistent world state that maintains context across iterative research cycles. Two operational modes support different workflows: semi-autonomous mode with selective human checkpoints, and fully autonomous mode for extended investigations. Evaluation on the BixBench computational biology benchmark demonstrated state-of-the-art performance, achieving 48.8% accuracy on open response and 64.5% on multiple-choice evaluation, exceeding existing baselines by 14 to 26 percentage points. Analysis of architectural constraints, including open access literature limitations and challenges inherent to automated novelty assessment, informs practical deployment considerations for AI-assisted scientific workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ç§‘å­¦å‘ç°äººå·¥æ™ºèƒ½ç³»ç»Ÿå­˜åœ¨çš„å°é—­æ€§ã€æ‰¹å¤„ç†æ¨¡å¼è€—æ—¶é•¿ä¸”ç¼ºä¹å®æ—¶ç ”ç©¶äººå‘˜æŒ‡å¯¼ç­‰å±€é™æ€§ï¼Œæå‡ºäº†åä¸º Deep Research çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿé€šè¿‡è§„åˆ’ã€æ•°æ®åˆ†æã€æ–‡çŒ®æœç´¢å’Œåˆ›æ–°æ€§æ£€æµ‹ (novelty detection) ç­‰ä¸“é—¨åŒ–æ™ºèƒ½ä½“ï¼Œç»“åˆèƒ½å¤Ÿç»´æŒè·¨è¿­ä»£ç ”ç©¶å‘¨æœŸä¸Šä¸‹æ–‡çš„æŒä¹…åŒ–ä¸–ç•ŒçŠ¶æ€ (persistent world state)ï¼Œå®ç°äº†åˆ†é’Ÿçº§çš„äº¤äº’å¼ç§‘å­¦ç ”ç©¶å“åº”ã€‚è¯¥æ¡†æ¶æ”¯æŒå¸¦æœ‰é€‰æ‹©æ€§äººç±»æ£€æŸ¥ç‚¹çš„åŠè‡ªä¸»æ¨¡å¼ (semi-autonomous mode) å’Œç”¨äºæ‰©å±•è°ƒæŸ¥çš„å…¨è‡ªä¸»æ¨¡å¼ (fully autonomous mode)ï¼Œä»è€Œé€‚é…ä¸åŒçš„å·¥ä½œæµéœ€æ±‚ã€‚åœ¨ BixBench è®¡ç®—ç”Ÿç‰©å­¦åŸºå‡†æµ‹è¯•ä¸­ï¼ŒDeep Research åœ¨å¼€æ”¾å¼å›ç­”å’Œå¤šé¡¹é€‰æ‹©è¯„ä¼°ä¸­åˆ†åˆ«è¾¾åˆ°äº† 48.8% å’Œ 64.5% çš„å‡†ç¡®ç‡ï¼Œç›¸è¾ƒäºç°æœ‰åŸºå‡†æ¨¡å‹æå‡äº† 14 è‡³ 26 ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ·±å…¥åˆ†æäº†åŒ…æ‹¬å¼€æ”¾è·å–æ–‡çŒ®é™åˆ¶å’Œè‡ªåŠ¨åŒ–åˆ›æ–°æ€§è¯„ä¼°æŒ‘æˆ˜åœ¨å†…çš„æ¶æ„çº¦æŸï¼Œä¸ºäººå·¥æ™ºèƒ½è¾…åŠ©ç§‘å­¦å·¥ä½œæµ (AI-assisted scientific workflows) çš„å®é™…éƒ¨ç½²æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12542v1",
      "published_date": "2026-01-18 19:12:41 UTC",
      "updated_date": "2026-01-18 19:12:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:38:24.409535+00:00"
    },
    {
      "arxiv_id": "2601.12539v1",
      "title": "MemeLens: Multilingual Multitask VLMs for Memes",
      "title_zh": "MemeLensï¼šé¢å‘æ¢—å›¾çš„å¤šè¯­è¨€å¤šä»»åŠ¡è§†è§‰è¯­è¨€æ¨¡å‹",
      "authors": [
        "Ali Ezzat Shahroor",
        "Mohamed Bayan Kmainasi",
        "Abul Hasnat",
        "Dimitar Dimitrov",
        "Giovanni Da San Martino",
        "Preslav Nakov",
        "Firoj Alam"
      ],
      "abstract": "Memes are a dominant medium for online communication and manipulation because meaning emerges from interactions between embedded text, imagery, and cultural context. Existing meme research is distributed across tasks (hate, misogyny, propaganda, sentiment, humour) and languages, which limits cross-domain generalization. To address this gap we propose MemeLens, a unified multilingual and multitask explanation-enhanced Vision Language Model (VLM) for meme understanding. We consolidate 38 public meme datasets, filter and map dataset-specific labels into a shared taxonomy of $20$ tasks spanning harm, targets, figurative/pragmatic intent, and affect. We present a comprehensive empirical analysis across modeling paradigms, task categories, and datasets. Our findings suggest that robust meme understanding requires multimodal training, exhibits substantial variation across semantic categories, and remains sensitive to over-specialization when models are fine-tuned on individual datasets rather than trained in a unified setting. We will make the experimental resources and datasets publicly available for the community.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MemeLensï¼Œä¸€ä¸ªç»Ÿä¸€çš„å¤šè¯­è¨€ã€å¤šä»»åŠ¡è§£é‡Šå¢å¼ºå‹è§†è§‰è¯­è¨€æ¨¡å‹ (Vision Language Model, VLM)ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¡¨æƒ…åŒ… (Memes) ç ”ç©¶åœ¨ä»»åŠ¡å’Œè¯­è¨€åˆ†å¸ƒä¸Šè¿‡äºåˆ†æ•£æ‰€å¯¼è‡´çš„è·¨é¢†åŸŸæ³›åŒ–å—é™é—®é¢˜ã€‚ç ”ç©¶äººå‘˜é€šè¿‡æ•´åˆ 38 ä¸ªå…¬å…±æ•°æ®é›†ï¼Œå°†å¤šæ ·åŒ–çš„æ ‡ç­¾æ˜ å°„åˆ°ä¸€ä¸ªåŒ…å« 20 ä¸ªä»»åŠ¡çš„å…±äº«åˆ†ç±»æ³• (taxonomy) ä¸­ï¼Œæ¶µç›–äº†å±å®³ã€ç›®æ ‡ã€æ¯”å–»/è¯­ç”¨æ„å›¾å’Œæƒ…æ„Ÿç­‰å…³é”®é¢†åŸŸã€‚å®è¯åˆ†æè¡¨æ˜ï¼Œç¨³å¥çš„ Meme ç†è§£é«˜åº¦ä¾èµ–å¤šæ¨¡æ€è®­ç»ƒï¼Œä¸”åœ¨ä¸åŒè¯­ä¹‰ç±»åˆ«ä¹‹é—´è¡¨ç°å‡ºæ˜¾è‘—å·®å¼‚ã€‚å®éªŒè¿˜å‘ç°ï¼Œç›¸è¾ƒäºç»Ÿä¸€è®¾ç½®ä¸‹çš„è®­ç»ƒï¼Œåœ¨å•ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒå®¹æ˜“ä½¿æ¨¡å‹äº§ç”Ÿè¿‡åº¦ä¸“ä¸šåŒ– (over-specialization) çš„å€¾å‘ã€‚è¯¥ç ”ç©¶é€šè¿‡å…¨é¢çš„å®è¯è¯„ä¼°æ­ç¤ºäº†æ¨¡å‹èŒƒå¼ä¸ä»»åŠ¡ç±»åˆ«é—´çš„å¤æ‚å…³ç³»ï¼Œå¹¶ä¸ºç¤¾åŒºæä¾›äº†ç»¼åˆæ€§çš„å®éªŒèµ„æºå’Œæ•°æ®é›†ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "disinformation, misinformation, factuality, harmfulness, fake news, propaganda, hateful meme, multimodality, text, images",
      "pdf_url": "https://arxiv.org/pdf/2601.12539v1",
      "published_date": "2026-01-18 19:01:03 UTC",
      "updated_date": "2026-01-18 19:01:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:37:44.507523+00:00"
    },
    {
      "arxiv_id": "2601.12538v1",
      "title": "Agentic Reasoning for Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹çš„æ™ºèƒ½ä½“æ¨ç†",
      "authors": [
        "Tianxin Wei",
        "Ting-Wei Li",
        "Zhining Liu",
        "Xuying Ning",
        "Ze Yang",
        "Jiaru Zou",
        "Zhichen Zeng",
        "Ruizhong Qiu",
        "Xiao Lin",
        "Dongqi Fu",
        "Zihao Li",
        "Mengting Ai",
        "Duo Zhou",
        "Wenxuan Bao",
        "Yunzhe Li",
        "Gaotang Li",
        "Cheng Qian",
        "Yu Wang",
        "Xiangru Tang",
        "Yin Xiao",
        "Liri Fang",
        "Hui Liu",
        "Xianfeng Tang",
        "Yuji Zhang",
        "Chi Wang",
        "Jiaxuan You",
        "Heng Ji",
        "Hanghang Tong",
        "Jingrui He"
      ],
      "abstract": "Reasoning is a fundamental cognitive process underlying inference, problem-solving, and decision-making. While large language models (LLMs) demonstrate strong reasoning capabilities in closed-world settings, they struggle in open-ended and dynamic environments. Agentic reasoning marks a paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn through continual interaction. In this survey, we organize agentic reasoning along three complementary dimensions. First, we characterize environmental dynamics through three layers: foundational agentic reasoning, which establishes core single-agent capabilities including planning, tool use, and search in stable environments; self-evolving agentic reasoning, which studies how agents refine these capabilities through feedback, memory, and adaptation; and collective multi-agent reasoning, which extends intelligence to collaborative settings involving coordination, knowledge sharing, and shared goals. Across these layers, we distinguish in-context reasoning, which scales test-time interaction through structured orchestration, from post-training reasoning, which optimizes behaviors via reinforcement learning and supervised fine-tuning. We further review representative agentic reasoning frameworks across real-world applications and benchmarks, including science, robotics, healthcare, autonomous research, and mathematics. This survey synthesizes agentic reasoning methods into a unified roadmap bridging thought and action, and outlines open challenges and future directions, including personalization, long-horizon interaction, world modeling, scalable multi-agent training, and governance for real-world deployment.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿæ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­çš„æ™ºèƒ½ä½“æ¨ç†ï¼ˆAgentic Reasoningï¼‰ï¼Œå°†å…¶å®šä¹‰ä¸ºä»é—­åˆç¯å¢ƒæ¨ç†å‘å…·å¤‡è§„åˆ’ã€è¡ŒåŠ¨å’Œå­¦ä¹ èƒ½åŠ›çš„è‡ªä¸»æ™ºèƒ½ä½“èŒƒå¼çš„è½¬å˜ã€‚ç ”ç©¶ä»åŸºç¡€æ€§æ¨ç†ï¼ˆfoundational agentic reasoningï¼‰ã€è‡ªæˆ‘æ¼”åŒ–æ¨ç†ï¼ˆself-evolving agentic reasoningï¼‰å’Œé›†ä½“å¤šæ™ºèƒ½ä½“æ¨ç†ï¼ˆcollective multi-agent reasoningï¼‰ä¸‰ä¸ªç»´åº¦æ„å»ºäº†æ¼”è¿›æ¡†æ¶ã€‚æ–‡ä¸­è¯¦ç»†åŒºåˆ†äº†ä¾é ç»“æ„åŒ–ç¼–æ’å®ç°æµ‹è¯•æ—¶æ‰©å±•çš„ä¸Šä¸‹æ–‡æ¨ç†ï¼ˆin-context reasoningï¼‰ä¸é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å’Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰å®ç°çš„åè®­ç»ƒæ¨ç†ï¼ˆpost-training reasoningï¼‰ã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜è¯„ä¼°äº†æ™ºèƒ½ä½“æ¨ç†åœ¨æœºå™¨äººã€åŒ»ç–—ã€ç§‘å­¦ç ”ç©¶å’Œæ•°å­¦ç­‰ç°å®é¢†åŸŸçš„åº”ç”¨è¡¨ç°ä¸åŸºå‡†æµ‹è¯•ã€‚æœ€åï¼Œè¯¥ç»¼è¿°æå‡ºäº†è¿æ¥æ€ç»´ä¸è¡ŒåŠ¨çš„ç»Ÿä¸€è·¯çº¿å›¾ï¼Œå¹¶å¯¹ä¸ªæ€§åŒ–ã€é•¿ç¨‹äº¤äº’ã€ä¸–ç•Œå»ºæ¨¡ï¼ˆworld modelingï¼‰åŠå¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“è®­ç»ƒç­‰æœªæ¥æŒ‘æˆ˜è¿›è¡Œäº†æ·±å…¥å±•æœ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Project: https://github.com/weitianxin/Awesome-Agentic-Reasoning",
      "pdf_url": "https://arxiv.org/pdf/2601.12538v1",
      "published_date": "2026-01-18 18:58:23 UTC",
      "updated_date": "2026-01-18 18:58:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:37:37.591134+00:00"
    },
    {
      "arxiv_id": "2601.12535v1",
      "title": "Improving Low-Resource Machine Translation via Round-Trip Reinforcement Learning",
      "title_zh": "é€šè¿‡å¾€è¿”å¼ºåŒ–å­¦ä¹ æå‡ä½èµ„æºæœºå™¨ç¿»è¯‘è´¨é‡",
      "authors": [
        "Ahmed Attia",
        "Alham Fikri"
      ],
      "abstract": "Low-resource machine translation (MT) has gained increasing attention as parallel data from low-resource language communities is collected, but many potential methods for improving low-resource MT remain unexplored. We investigate a self-supervised reinforcement-learning-based fine-tuning for translation in low-resource settings using round-trip bootstrapping with the No Language Left Behind (NLLB) family of models. Our approach translates English into a target low-resource language and then back into English, using a combination of chrF++ and BLEU as the reward function on the reconstructed English sentences. Using the NLLB-MD dataset, we evaluate both the 600M and 1.3B parameter NLLB models and observe consistent improvements for the following languages: Central Aymara, Friulian, Wolof and Russian. Qualitative inspection of translation outputs indicates increased fluency and semantic fidelity. We argue that our method can further benefit from scale, enabling models to increasingly leverage their pretrained knowledge and continue self-improving.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡è‡ªæˆ‘ç›‘ç£å¼ºåŒ–å­¦ä¹ ï¼ˆself-supervised reinforcement-learningï¼‰å¾®è°ƒæŠ€æœ¯æ¥æå‡ä½èµ„æºæœºå™¨ç¿»è¯‘ï¼ˆlow-resource machine translationï¼‰çš„æ€§èƒ½ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§åŸºäºå›ç¯å¼•å¯¼ï¼ˆround-trip bootstrappingï¼‰çš„æ–¹æ³•ï¼Œåˆ©ç”¨ No Language Left Behind (NLLB) ç³»åˆ—æ¨¡å‹ï¼Œå°†è‹±è¯­ç¿»è¯‘ä¸ºç›®æ ‡ä½èµ„æºè¯­è¨€åå†ç¿»è¯‘å›è‹±è¯­ã€‚è¯¥æ–¹æ³•ä½¿ç”¨ chrF++ å’Œ BLEU çš„ç»„åˆä½œä¸ºé‡æ„è‹±è¯­å¥å­çš„å¥–åŠ±å‡½æ•°ï¼ˆreward functionï¼‰ï¼Œå¹¶åœ¨ NLLB-MD æ•°æ®é›†ä¸Šå¯¹ 600M å’Œ 1.3B å‚æ•°è§„æ¨¡çš„æ¨¡å‹è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ Central Aymaraã€Friulianã€Wolof å’Œä¿„è¯­ç­‰è¯­è¨€ä¸Šï¼Œè¯¥æ–¹æ³•å®ç°äº†æŒç»­çš„æ€§èƒ½æå‡ï¼Œå¹¶æ˜¾è‘—å¢åŠ äº†ç¿»è¯‘çš„æµåˆ©åº¦ï¼ˆfluencyï¼‰å’Œè¯­ä¹‰å¿ å®åº¦ï¼ˆsemantic fidelityï¼‰ã€‚ä½œè€…æŒ‡å‡ºï¼Œè¯¥æ–¹æ³•å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œèƒ½å¤Ÿå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°åˆ©ç”¨é¢„è®­ç»ƒçŸ¥è¯†ä»¥å®ç°æŒç»­çš„è‡ªæˆ‘æ”¹è¿›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12535v1",
      "published_date": "2026-01-18 18:44:49 UTC",
      "updated_date": "2026-01-18 18:44:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:38:47.597295+00:00"
    },
    {
      "arxiv_id": "2601.12534v2",
      "title": "Encoding Emotion Through Self-Supervised Eye Movement Reconstruction",
      "title_zh": "åŸºäºè‡ªç›‘ç£çœ¼åŠ¨é‡å»ºçš„æƒ…æ„Ÿç¼–ç ",
      "authors": [
        "Marcus Ma",
        "Jordan Prescott",
        "Emily Zhou",
        "Tiantian Feng",
        "Kleanthis Avramidis",
        "Gabor Mihaly Toth",
        "Shrikanth Narayanan"
      ],
      "abstract": "The relationship between emotional expression and eye movement is well-documented, with literature establishing gaze patterns are reliable indicators of emotion. However, most studies utilize specialized, high-resolution eye-tracking equipment, limiting the potential reach of findings. We investigate how eye movement can be used to predict multimodal markers of emotional expression from naturalistic, low-resolution videos. We utilize a collection of video interviews from the USC Shoah Foundation's Visual History Archive with Holocaust survivors as they recount their experiences in the Auschwitz concentration camp. Inspired by pretraining methods on language models, we develop a novel gaze detection model that uses self-supervised eye movement reconstruction that can effectively leverage unlabeled video. We use this model's encoder embeddings to fine-tune models on two downstream tasks related to emotional expression. The first is aligning eye movement with directional emotion estimates from speech. The second task is using eye gaze as a predictor of three momentary manifestations of emotional behaviors: laughing, crying/sobbing, and sighing. We find our new model is predictive of emotion outcomes and observe a positive correlation between pretraining performance and emotion processing performance for both experiments. We conclude self-supervised eye movement reconstruction is an effective method for encoding the affective signal they carry.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åœ¨ä½åˆ†è¾¨ç‡è§†é¢‘ä¸­åˆ©ç”¨çœ¼åŠ¨é¢„æµ‹æƒ…æ„Ÿè¡¨è¾¾çš„å¯èƒ½æ€§ï¼Œæ—¨åœ¨å…‹æœä¼ ç»Ÿé«˜åˆ†è¾¨ç‡çœ¼åŠ¨è¿½è¸ªè®¾å¤‡çš„å±€é™æ€§ã€‚å—åˆ°è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒæ–¹æ³•çš„å¯å‘ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºäºè‡ªç›‘ç£çœ¼åŠ¨é‡å»º(self-supervised eye movement reconstruction)çš„æ–°å‹æ³¨è§†æ£€æµ‹æ¨¡å‹ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨å¤§è§„æ¨¡æœªæ ‡æ³¨è§†é¢‘è¿›è¡Œå­¦ä¹ ã€‚ç ”ç©¶åˆ©ç”¨USC Shoah Foundation Visual History Archiveä¸­çš„è®¿è°ˆè§†é¢‘è¿›è¡Œå®éªŒï¼Œå¹¶åˆ©ç”¨è¯¥æ¨¡å‹çš„ç¼–ç å™¨åµŒå…¥(encoder embeddings)åœ¨ä¸¤é¡¹ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œåˆ†åˆ«ç”¨äºå°†çœ¼åŠ¨ä¸è¯­éŸ³æƒ…æ„Ÿä¼°è®¡å¯¹é½ï¼Œä»¥åŠé¢„æµ‹å¤§ç¬‘ã€å“­æ³£å’Œå¹æ°”ç­‰å…·ä½“æƒ…æ„Ÿè¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆé¢„æµ‹æƒ…æ„Ÿç»“æœï¼Œä¸”é¢„è®­ç»ƒè¡¨ç°ä¸ä¸‹æ¸¸æƒ…æ„Ÿå¤„ç†æ€§èƒ½ä¹‹é—´å­˜åœ¨æ­£ç›¸å…³å…³ç³»ã€‚ç ”ç©¶è¯æ˜è‡ªç›‘ç£çœ¼åŠ¨é‡å»ºæ˜¯æ•æ‰å’Œç¼–ç çœ¼åŠ¨ä¸­æ‰€è•´å«æƒ…æ„Ÿä¿¡å·(affective signal)çš„ä¸€ç§é«˜æ•ˆæ–¹æ³•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12534v2",
      "published_date": "2026-01-18 18:37:41 UTC",
      "updated_date": "2026-01-21 03:08:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:37:53.893855+00:00"
    },
    {
      "arxiv_id": "2601.12522v1",
      "title": "Improved Bug Localization with AI Agents Leveraging Hypothesis and Dynamic Cognition",
      "title_zh": "åŸºäºå‡è®¾ä¸åŠ¨æ€è®¤çŸ¥ AI æ™ºèƒ½ä½“çš„æ”¹è¿›ç¼ºé™·å®šä½",
      "authors": [
        "Asif Mohammed Samir",
        "Mohammad Masudur Rahman"
      ],
      "abstract": "Software bugs cost technology providers (e.g., AT&T) billions annually and cause developers to spend roughly 50% of their time on bug resolution. Traditional methods for bug localization often analyze the suspiciousness of code components (e.g., methods, documents) in isolation, overlooking their connections with other components in the codebase. Recent advances in Large Language Models (LLMs) and agentic AI techniques have shown strong potential for code understanding, but still lack causal reasoning during code exploration and struggle to manage growing context effectively, limiting their capability. In this paper, we present a novel agentic technique for bug localization -- CogniGent -- that overcomes the limitations above by leveraging multiple AI agents capable of causal reasoning, call-graph-based root cause analysis and context engineering. It emulates developers-inspired debugging practices (a.k.a., dynamic cognitive debugging) and conducts hypothesis testing to support bug localization. We evaluate CogniGent on a curated dataset of 591 bug reports using three widely adopted performance metrics and compare it against six established baselines from the literature. Experimental results show that our technique consistently outperformed existing traditional and LLM-based techniques, achieving MAP improvements of 23.33-38.57% at the document and method levels. Similar gains were observed in MRR, with increases of 25.14-53.74% at both granularity levels. Statistical significance tests also confirm the superiority of our technique. By addressing the reasoning, dependency, and context limitations, CogniGent advances the state of bug localization, bridging human-like cognition with agentic automation for improved performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œä¼ ç»Ÿçš„ç¼ºé™·å®šä½æ–¹æ³•å¾€å¾€å­¤ç«‹åœ°åˆ†æä»£ç ç»„ä»¶è€Œå¿½è§†äº†å…¶é—´çš„è”ç³»ï¼Œä¸”ç°æœ‰çš„ Large Language Models (LLMs) æ™ºèƒ½ä½“åœ¨å› æœæ¨ç†å’Œä¸Šä¸‹æ–‡ç®¡ç†æ–¹é¢å­˜åœ¨å±€é™ã€‚é’ˆå¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º CogniGent çš„æ–°å‹æ™ºèƒ½ä½“æŠ€æœ¯ï¼Œé€šè¿‡å¤šä¸ª AI æ™ºèƒ½ä½“å®ç°å› æœæ¨ç†ã€åŸºäºè°ƒç”¨å›¾ (call-graph) çš„æ ¹å› åˆ†æå’Œä¸Šä¸‹æ–‡å·¥ç¨‹ (context engineering)ã€‚è¯¥æŠ€æœ¯æ¨¡æ‹Ÿäº†å¼€å‘è€…çš„åŠ¨æ€è®¤çŸ¥è°ƒè¯• (dynamic cognitive debugging) å®è·µï¼Œå¹¶åˆ©ç”¨å‡è®¾æ£€éªŒ (hypothesis testing) æ¥è¾…åŠ©ç¼ºé™·å®šä½ã€‚ç ”ç©¶åœ¨åŒ…å« 591 ä¸ªç¼ºé™·æŠ¥å‘Šçš„æ•°æ®é›†ä¸Šå¯¹ CogniGent è¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å°†å…¶ä¸å…­ç§ä¸»æµåŸºå‡†æ–¹æ³•è¿›è¡Œäº†å¯¹æ¯”ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCogniGent æ˜¾è‘—ä¼˜äºç°æœ‰çš„ä¼ ç»Ÿæ–¹æ³•åŠåŸºäº LLM çš„æŠ€æœ¯ï¼Œåœ¨æ–‡æ¡£å’Œæ–¹æ³•å±‚é¢çš„ MAP æå‡äº† 23.33-38.57%ï¼ŒMRR æå‡äº† 25.14-53.74%ã€‚ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒç¡®è®¤äº†è¯¥æŠ€æœ¯çš„ä¼˜è¶Šæ€§ï¼Œè¯æ˜äº†å°†ç±»äººè®¤çŸ¥ä¸æ™ºèƒ½ä½“è‡ªåŠ¨åŒ–ç›¸ç»“åˆèƒ½æœ‰æ•ˆæå‡ç¼ºé™·å®šä½çš„æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.IR",
        "cs.LG",
        "cs.MA"
      ],
      "primary_category": "cs.SE",
      "comment": "13 pages, 7 tables, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.12522v1",
      "published_date": "2026-01-18 18:12:21 UTC",
      "updated_date": "2026-01-18 18:12:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:38:01.795451+00:00"
    },
    {
      "arxiv_id": "2601.12518v1",
      "title": "Cooperative Multi-agent RL with Communication Constraints",
      "title_zh": "é€šä¿¡å—é™ä¸‹çš„ååŒå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Nuoya Xiong",
        "Aarti Singh"
      ],
      "abstract": "Cooperative MARL often assumes frequent access to global information in a data buffer, such as team rewards or other agents' actions, which is typically unrealistic in decentralized MARL systems due to high communication costs. When communication is limited, agents must rely on outdated information to estimate gradients and update their policies. A common approach to handle missing data is called importance sampling, in which we reweigh old data from a base policy to estimate gradients for the current policy. However, it quickly becomes unstable when the communication is limited (i.e. missing data probability is high), so that the base policy in importance sampling is outdated. To address this issue, we propose a technique called base policy prediction, which utilizes old gradients to predict the policy update and collect samples for a sequence of base policies, which reduces the gap between the base policy and the current policy. This approach enables effective learning with significantly fewer communication rounds, since the samples of predicted base policies could be collected within one communication round. Theoretically, we show that our algorithm converges to an $\\varepsilon$-Nash equilibrium in potential games with only $O(\\varepsilon^{-3/4})$ communication rounds and $O(poly(\\max_i |A_i|)\\varepsilon^{-11/4})$ samples, improving existing state-of-the-art results in communication cost, as well as sample complexity without the exponential dependence on the joint action space size. We also extend these results to general Markov Cooperative Games to find an agent-wise local maximum. Empirically, we test the base policy prediction algorithm in both simulated games and MAPPO for complex environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åä½œå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Cooperative MARL) åœ¨å»ä¸­å¿ƒåŒ–ç³»ç»Ÿä¸­é¢ä¸´çš„é«˜é¢é€šä¿¡æˆæœ¬é—®é¢˜ï¼Œæ¢è®¨äº†é€šä¿¡å—é™ç¯å¢ƒä¸‹çš„ç­–ç•¥ä¼˜åŒ–ã€‚ç”±äºé€šä¿¡å—é™ä¼šå¯¼è‡´é‡è¦æ€§é‡‡æ · (Importance Sampling) ä¸­çš„åŸºå‡†ç­–ç•¥ (Base Policy) è¿‡æ—¶å¹¶å¼•å‘ç®—æ³•ä¸ç¨³å®šï¼Œä½œè€…æå‡ºäº†ä¸€ç§åŸºå‡†ç­–ç•¥é¢„æµ‹ (Base Policy Prediction) æŠ€æœ¯ã€‚è¯¥æŠ€æœ¯é€šè¿‡åˆ©ç”¨å†å²æ¢¯åº¦é¢„æµ‹ç­–ç•¥æ›´æ–°å¹¶æå‰æ”¶é›†ä¸€ç³»åˆ—åŸºå‡†ç­–ç•¥çš„æ ·æœ¬ï¼Œæœ‰æ•ˆç¼©å°äº†åŸºå‡†ç­–ç•¥ä¸å½“å‰ç­–ç•¥ä¹‹é—´çš„å·®è·ï¼Œä»è€Œåœ¨å•æ¬¡é€šä¿¡å›åˆå†…å®ç°æ›´é«˜æ•ˆçš„å­¦ä¹ ã€‚ç†è®ºåˆ†æè¯æ˜ï¼Œè¯¥ç®—æ³•åœ¨åŠ¿åšå¼ˆ (Potential Games) ä¸­èƒ½ä»¥æ›´ä¼˜çš„é€šä¿¡å’Œæ ·æœ¬å¤æ‚åº¦æ”¶æ•›è‡³ $\\varepsilon$-çº³ä»€å‡è¡¡ ($\\varepsilon$-Nash Equilibrium)ï¼Œä¸”é¿å…äº†å¯¹è”åˆåŠ¨ä½œç©ºé—´çš„æŒ‡æ•°çº§ä¾èµ–ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å°†ç»“è®ºæ‰©å±•è‡³é€šç”¨çš„é©¬å°”å¯å¤«åä½œåšå¼ˆ (Markov Cooperative Games)ï¼Œå¹¶é€šè¿‡æ¨¡æ‹Ÿåšå¼ˆåŠ MAPPO å®éªŒéªŒè¯äº†åŸºå‡†ç­–ç•¥é¢„æµ‹ç®—æ³•åœ¨å¤„ç†å¤æ‚ç¯å¢ƒæ—¶çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.LG",
      "comment": "33 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.12518v1",
      "published_date": "2026-01-18 18:05:23 UTC",
      "updated_date": "2026-01-18 18:05:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:38:02.654815+00:00"
    },
    {
      "arxiv_id": "2601.12499v1",
      "title": "Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition Bottleneck",
      "title_zh": "å¤šè·³é—®ç­”ä¸­çš„å¤±æ•ˆæ¨¡å¼ï¼šæœ€å¼±ç¯èŠ‚å®šå¾‹ä¸è¯†åˆ«ç“¶é¢ˆ",
      "authors": [
        "Meiru Zhang",
        "Zaiqiao Meng",
        "Nigel Collier"
      ],
      "abstract": "Despite scaling to massive context windows, Large Language Models (LLMs) struggle with multi-hop reasoning due to inherent position bias, which causes them to overlook information at certain positions. Whether these failures stem from an inability to locate evidence (recognition failure) or integrate it (synthesis failure) is unclear. We introduce Multi-Focus Attention Instruction (MFAI), a semantic probe to disentangle these mechanisms by explicitly steering attention towards selected positions. Across 5 LLMs on two multi-hop QA tasks (MuSiQue and NeoQA), we establish the \"Weakest Link Law\": multi-hop reasoning performance collapses to the performance level of the least visible evidence. Crucially, this failure is governed by absolute position rather than the linear distance between facts (performance variance $<3%$). We further identify a duality in attention steering: while matched MFAI resolves recognition bottlenecks, improving accuracy by up to 11.5% in low-visibility positions, misleading MFAI triggers confusion in real-world tasks but is successfully filtered in synthetic tasks. Finally, we demonstrate that \"thinking\" models that utilize System-2 reasoning, effectively locate and integrate the required information, matching gold-only baselines even in noisy, long-context settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šè·³æ¨ç†(Multi-hop QA)ä¸­çš„å¤±è´¥æ¨¡å¼ï¼Œé‡ç‚¹åˆ†æäº†ç”±äºä½ç½®åå·®(Position Bias)å¯¼è‡´çš„è¯æ®è¯†åˆ«ä¸åˆæˆç“¶é¢ˆã€‚ä½œè€…å¼•å…¥äº†Multi-Focus Attention Instruction (MFAI)ä½œä¸ºè¯­ä¹‰æ¢é’ˆï¼Œæ—¨åœ¨åŒºåˆ†æ¨¡å‹æ˜¯æ— æ³•å®šä½è¯æ®è¿˜æ˜¯æ— æ³•æ•´åˆä¿¡æ¯ã€‚ç ”ç©¶æå‡ºäº†â€œæœ€å¼±ç¯èŠ‚å®šå¾‹â€(Weakest Link Law)ï¼ŒæŒ‡å‡ºå¤šè·³æ¨ç†çš„æ€§èƒ½å—é™äºå¯è§æ€§æœ€å·®çš„è¯æ®ç¯èŠ‚ï¼Œä¸”è¿™ç§å¤±è´¥ä¸»è¦ç”±ç»å¯¹ä½ç½®è€Œéäº‹å®é—´çš„çº¿æ€§è·ç¦»å†³å®šã€‚å®éªŒè¡¨æ˜ï¼ŒåŒ¹é…çš„MFAIèƒ½æœ‰æ•ˆç¼“è§£è¯†åˆ«ç“¶é¢ˆï¼Œåœ¨ä½å¯è§åº¦ä½ç½®å°†å‡†ç¡®ç‡æå‡è‡³å¤š11.5%ã€‚æœ€åï¼Œç ”ç©¶è¯æ˜é‡‡ç”¨System-2æ¨ç†çš„â€œæ€è€ƒâ€å‹æ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆå®šä½å¹¶æ•´åˆä¿¡æ¯ï¼Œåœ¨å˜ˆæ‚çš„é•¿æ–‡æœ¬ç¯å¢ƒä¸‹ä¾ç„¶èƒ½è¾¾åˆ°åŸºå‡†æ°´å¹³ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "preprint",
      "pdf_url": "https://arxiv.org/pdf/2601.12499v1",
      "published_date": "2026-01-18 17:16:04 UTC",
      "updated_date": "2026-01-18 17:16:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:38:16.309584+00:00"
    },
    {
      "arxiv_id": "2601.12494v1",
      "title": "Harmonizing the Arabic Audio Space with Data Scheduling",
      "title_zh": "é€šè¿‡æ•°æ®è°ƒåº¦ç»Ÿåˆé˜¿æ‹‰ä¼¯è¯­éŸ³é¢‘ç©ºé—´",
      "authors": [
        "Hunzalah Hassan Bhatti",
        "Firoj Alam",
        "Shammur Absar Chowdhury"
      ],
      "abstract": "Audio large language models (LLMs) enable unified speech understanding and generation, yet their adaptation to linguistically complex, dialect-rich settings remains underexplored. This paper presents the first systematic study of multi-task instruction tuning for an Arabic-centric audio LLM, covering a hierarchy of generative tasks (ASR, speech summarization) and discriminative tasks (dialect and emotion identification). To support this study, we introduce AraMega-SSum, a novel dataset for Arabic speech summarization. We fine-tune Qwen2.5-Omni (7B) and propose Task-Progressive Curriculum (TPC) along with Aligner-Based Diverse Sampling (ADS), a strategy that constructs information-dense batches by selecting task- and label-balanced examples. Our results reveal a critical efficiency, robustness trade-off: while ADS accelerates initial convergence and boosts paralinguistic F1-scores, its inherent gradient volatility can destabilize generative decoding under prolonged training. Furthermore, while the TPC stabilizes core acoustic mapping, it often induces negative transfer in downstream tasks. We demonstrate that a Hybrid TPC+ADS Strategy provides an optimal training ``recipe'', first establishing a robust representative foundation before employing diversity-aware refinement to capture fine-grained nuances. These findings offer practical guidance for the efficient adaptation of Omni-models in complex, low-resource multimodal environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹(Audio LLMs)åœ¨å¤„ç†è¯­è¨€å¤æ‚ä¸”æ–¹è¨€ä¸°å¯Œçš„é˜¿æ‹‰ä¼¯è¯­ç¯å¢ƒä¸­çš„é€‚é…é—®é¢˜ï¼Œè¿›è¡Œäº†é¦–æ¬¡ç³»ç»Ÿæ€§çš„å¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒç ”ç©¶ã€‚ç ”ç©¶æ¶µç›–äº†è‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)ã€è¯­éŸ³æ‘˜è¦ã€æ–¹è¨€è¯†åˆ«åŠæƒ…æ„Ÿè¯†åˆ«ç­‰ä¸€ç³»åˆ—ç”Ÿæˆä¸åˆ¤åˆ«ä»»åŠ¡ï¼Œå¹¶å¼•å…¥äº†å…¨æ–°çš„é˜¿æ‹‰ä¼¯è¯­éŸ³æ‘˜è¦æ•°æ®é›†AraMega-SSumã€‚é€šè¿‡å¾®è°ƒQwen2.5-Omni(7B)æ¨¡å‹ï¼Œç ”ç©¶è€…æå‡ºäº†ä»»åŠ¡æ¸è¿›å¼è¯¾ç¨‹(Task-Progressive Curriculum, TPC)ä¸åŸºäºå¯¹é½å™¨çš„å¤šæ ·æ€§é‡‡æ ·(Aligner-Based Diverse Sampling, ADS)ç­–ç•¥ä»¥ä¼˜åŒ–æ•°æ®è°ƒåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒADSèƒ½æ˜¾è‘—åŠ é€Ÿåˆå§‹æ”¶æ•›å¹¶æå‡å‰¯è¯­è¨€F1åˆ†æ•°ï¼Œè€ŒTPCè™½èƒ½ç¨³å®šå£°å­¦æ˜ å°„ï¼Œä½†åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å­˜åœ¨è´Ÿè¿ç§»é£é™©ã€‚æœ€ç»ˆç ”ç©¶è¯æ˜ï¼Œé‡‡ç”¨æ··åˆTPC+ADSç­–ç•¥(Hybrid TPC+ADS Strategy)èƒ½æä¾›æœ€ä½³çš„è®­ç»ƒæ–¹æ¡ˆï¼Œåœ¨å»ºç«‹ç¨³å¥è¡¨å¾åŸºç¡€çš„åŒæ—¶é€šè¿‡å¤šæ ·æ€§æ„ŸçŸ¥ç»†åŒ–æ•æ‰ç»†å¾®å·®åˆ«ã€‚è¿™äº›å‘ç°ä¸ºå…¨èƒ½æ¨¡å‹(Omni-models)åœ¨å¤æ‚ã€ä½èµ„æºçš„å¤šæ¨¡æ€ç¯å¢ƒä¸‹çš„é«˜æ•ˆé€‚é…æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "Foundation Models, Large Language Models, Native, Speech Models, Arabic",
      "pdf_url": "https://arxiv.org/pdf/2601.12494v1",
      "published_date": "2026-01-18 17:08:31 UTC",
      "updated_date": "2026-01-18 17:08:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:39:03.296132+00:00"
    },
    {
      "arxiv_id": "2601.12471v2",
      "title": "Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty",
      "title_zh": "çŸ¥æ—¶è€Œé€€ï¼šä¸´åºŠä¸ç¡®å®šæ€§ä¸‹çš„åŒ»å­¦å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Sravanthi Machcha",
        "Sushrita Yerra",
        "Sahil Gupta",
        "Aishwarya Sahoo",
        "Sharmin Sultana",
        "Hong Yu",
        "Zonghai Yao"
      ],
      "abstract": "Current evaluation of large language models (LLMs) overwhelmingly prioritizes accuracy; however, in real-world and safety-critical applications, the ability to abstain when uncertain is equally vital for trustworthy deployment. We introduce MedAbstain, a unified benchmark and evaluation protocol for abstention in medical multiple-choice question answering (MCQA) -- a discrete-choice setting that generalizes to agentic action selection -- integrating conformal prediction, adversarial question perturbations, and explicit abstention options. Our systematic evaluation of both open- and closed-source LLMs reveals that even state-of-the-art, high-accuracy models often fail to abstain with uncertain. Notably, providing explicit abstention options consistently increases model uncertainty and safer abstention, far more than input perturbations, while scaling model size or advanced prompting brings little improvement. These findings highlight the central role of abstention mechanisms for trustworthy LLM deployment and offer practical guidance for improving safety in high-stakes applications.",
      "tldr_zh": "å½“å‰å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)çš„è¯„ä¼°ä¸»è¦ä¾§é‡äºå‡†ç¡®ç‡ï¼Œä½†åœ¨åŒ»ç–—ç­‰å®‰å…¨å…³é”®é¢†åŸŸï¼Œæ¨¡å‹åœ¨é¢å¯¹ä¸ç¡®å®šæ€§æ—¶é€‰æ‹©å¼ƒæƒ(Abstaining)çš„èƒ½åŠ›å¯¹äºå®ç°å¯ä¿¡éƒ¨ç½²è‡³å…³é‡è¦ã€‚è¯¥ç ”ç©¶å¼•å…¥äº†MedAbstainï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹åŒ»å­¦å¤šé€‰é¢˜(MCQA)åœºæ™¯ä¸‹å¼ƒæƒæœºåˆ¶çš„ç»Ÿä¸€åŸºå‡†å’Œè¯„ä¼°åè®®ï¼Œæ•´åˆäº†å…±å½¢é¢„æµ‹(Conformal Prediction)ã€å¯¹æŠ—æ€§é—®é¢˜æ‰°åŠ¨ä»¥åŠæ˜¾å¼å¼ƒæƒé€‰é¡¹ã€‚é€šè¿‡å¯¹å¼€æºå’Œé—­æºLLMsçš„ç³»ç»Ÿè¯„ä¼°å‘ç°ï¼Œå³ä½¿æ˜¯é«˜å‡†ç¡®ç‡çš„æœ€å…ˆè¿›æ¨¡å‹åœ¨ä¸ç¡®å®šæ—¶ä¹Ÿå¾€å¾€éš¾ä»¥ä¸»åŠ¨å¼ƒæƒã€‚ç ”ç©¶è¿›ä¸€æ­¥æŒ‡å‡ºï¼Œæä¾›æ˜¾å¼å¼ƒæƒé€‰é¡¹æ¯”å¢åŠ æ¨¡å‹è§„æ¨¡æˆ–ä½¿ç”¨é«˜çº§æç¤ºå·¥ç¨‹(Prompting)èƒ½æ›´æœ‰æ•ˆåœ°æå‡æ¨¡å‹çš„å®‰å…¨æ€§å¼ƒæƒè¡¨ç°ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†å¼ƒæƒæœºåˆ¶åœ¨æ„å»ºå¯ä¿¡åŒ»ç–—AIç³»ç»Ÿä¸­çš„æ ¸å¿ƒåœ°ä½ï¼Œå¹¶ä¸ºé«˜é£é™©åº”ç”¨æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Equal contribution for the first two authors; To appear in proceedings of the Main Conference of the European Chapter of the Association for Computational Linguistics (EACL) 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12471v2",
      "published_date": "2026-01-18 16:19:29 UTC",
      "updated_date": "2026-01-22 05:03:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:39:07.993396+00:00"
    },
    {
      "arxiv_id": "2601.12467v2",
      "title": "Patch-Level Tokenization with CNN Encoders and Attention for Improved Transformer Time-Series Forecasting",
      "title_zh": "èåˆCNNç¼–ç å™¨ä¸æ³¨æ„åŠ›æœºåˆ¶çš„Patchçº§æ ‡è®°åŒ–ï¼šæå‡Transformeræ—¶é—´åºåˆ—é¢„æµ‹æ€§èƒ½",
      "authors": [
        "Saurish Nagrath",
        "Saroj Kumar Panigrahy"
      ],
      "abstract": "Transformer-based models have shown strong performance in time-series forecasting by leveraging self-attention to model long-range temporal dependencies. However, their effectiveness depends critically on the quality and structure of input representations derived from raw multivariate time-series data, particularly as sequence length and data scale increase. This paper proposes a two-stage forecasting framework that explicitly separates local temporal representation learning from global dependency modelling. In the proposed approach, a convolutional neural network operates on fixed-length temporal patches to extract short-range temporal dynamics and non-linear feature interactions, producing compact patch-level token embeddings. Token-level self-attention is applied during representation learning to refine these embeddings, after which a Transformer encoder models inter-patch temporal dependencies to generate forecasts. The method is evaluated on a synthetic multivariate time-series dataset with controlled static and dynamic factors, using an extended sequence length and a larger number of samples. Experimental results demonstrate that the proposed framework consistently outperforms a convolutional baseline under increased temporal context and remains competitive with a strong patch-based Transformer model. These findings indicate that structured patch-level tokenization provides a scalable and effective representation for multivariate time-series forecasting, particularly when longer input sequences are considered.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µé¢„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡åˆ†ç¦»å±€éƒ¨æ—¶é—´è¡¨ç¤ºå­¦ä¹ ä¸å…¨å±€ä¾èµ–å»ºæ¨¡æ¥æ”¹è¿› Transformer æ—¶é—´åºåˆ—é¢„æµ‹ã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ©ç”¨ CNN åœ¨å›ºå®šé•¿åº¦çš„ temporal patches ä¸Šæå–çŸ­ç¨‹æ—¶é—´åŠ¨æ€å’Œéçº¿æ€§ç‰¹å¾äº¤äº’ï¼Œä»è€Œç”Ÿæˆç´§å‡‘çš„ patch-level token embeddingsã€‚åœ¨è¡¨ç¤ºå­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œè¯¥æ¡†æ¶å¼•å…¥ token-level self-attention ä»¥è¿›ä¸€æ­¥ç²¾ç‚¼åµŒå…¥è¡¨ç¤ºï¼Œéšåç”± Transformer ç¼–ç å™¨å»ºæ¨¡ patch é—´çš„å…¨å±€æ—¶é—´ä¾èµ–å…³ç³»å¹¶ç”Ÿæˆé¢„æµ‹ç»“æœã€‚å®éªŒåœ¨å…·æœ‰å—æ§é™æ€å’ŒåŠ¨æ€å› å­çš„åˆæˆå¤šå…ƒæ—¶é—´åºåˆ—æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶åœ¨é•¿è¾“å…¥åºåˆ—åœºæ™¯ä¸‹ä¸€è‡´ä¼˜äº CNN åŸºçº¿ï¼Œå¹¶ä¸ä¸»æµçš„ patch-based Transformer æ¨¡å‹è¡¨ç°ç›¸å½“ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œç»“æ„åŒ–çš„ patch-level tokenization ä¸ºå¤šå…ƒæ—¶é—´åºåˆ—é¢„æµ‹æä¾›äº†ä¸€ç§å¯æ‰©å±•ä¸”æœ‰æ•ˆçš„è¡¨ç¤ºæ–¹å¼ï¼Œå°¤å…¶åœ¨å¤„ç†æ›´é•¿è¾“å…¥åºåˆ—æ—¶å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "6 pages, 2 figures, 3 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.12467v2",
      "published_date": "2026-01-18 16:16:01 UTC",
      "updated_date": "2026-01-21 14:41:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:39:13.499206+00:00"
    },
    {
      "arxiv_id": "2601.12465v1",
      "title": "Incentivizing In-depth Reasoning over Long Contexts with Process Advantage Shaping",
      "title_zh": "é€šè¿‡è¿‡ç¨‹ä¼˜åŠ¿å¡‘å½¢æ¿€åŠ±é•¿ä¸Šä¸‹æ–‡ä¸‹çš„æ·±åº¦æ¨ç†",
      "authors": [
        "Miao Peng",
        "Weizhou Shen",
        "Nuo Chen",
        "Chenliang Li",
        "Ming Yan",
        "Jia Li"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective in enhancing LLMs short-context reasoning, but its performance degrades in long-context scenarios that require both precise grounding and robust long-range reasoning. We identify the \"almost-there\" phenomenon in long-context reasoning, where trajectories are largely correct but fail at the final step, and attribute this failure to two factors: (1) the lack of high reasoning density in long-context QA data that push LLMs beyond mere grounding toward sophisticated multi-hop reasoning; and (2) the loss of valuable learning signals during long-context RL training due to the indiscriminate penalization of partially correct trajectories with incorrect outcomes. To overcome this bottleneck, we propose DeepReasonQA, a KG-driven synthesis framework that controllably constructs high-difficulty, multi-hop long-context QA pairs with inherent reasoning chains. Building on this, we introduce Long-context Process Advantage Shaping (LongPAS), a simple yet effective method that performs fine-grained credit assignment by evaluating reasoning steps along Validity and Relevance dimensions, which captures critical learning signals from \"almost-there\" trajectories. Experiments on three long-context reasoning benchmarks show that our approach substantially outperforms RLVR baselines and matches frontier LLMs while using far fewer parameters. Further analysis confirms the effectiveness of our methods in strengthening long-context reasoning while maintaining stable RL training.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Reinforcement Learning with Verifiable Rewards (RLVR)åœ¨é•¿æ–‡æœ¬æ¨ç†åœºæ™¯ä¸­æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼Œå¹¶è¯†åˆ«å‡ºæ¨ç†è½¨è¿¹å¤§è‡´æ­£ç¡®å´åœ¨ç»ˆç‚¹å¤±è´¥çš„â€œalmost-thereâ€ç°è±¡ã€‚ä½œè€…å°†æ­¤ç“¶é¢ˆå½’å› äºé•¿æ–‡æœ¬QAæ•°æ®ä¸­multi-hop reasoningå¯†åº¦ä¸è¶³ï¼Œä»¥åŠå¼ºåŒ–å­¦ä¹ è®­ç»ƒå› æ— æ³•æœ‰æ•ˆè¯„ä¼°éƒ¨åˆ†æ­£ç¡®è½¨è¿¹è€Œå¯¼è‡´çš„learning signalsä¸¢å¤±ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶æå‡ºäº†DeepReasonQAæ¡†æ¶ï¼Œåˆ©ç”¨Knowledge Graph (KG)å—æ§åœ°åˆæˆå…·æœ‰å†…åœ¨reasoning chainsçš„é«˜éš¾åº¦é•¿æ–‡æœ¬é—®ç­”å¯¹ã€‚åŒæ—¶ï¼Œè®ºæ–‡å¼•å…¥äº†Long-context Process Advantage Shaping (LongPAS)æ–¹æ³•ï¼Œé€šè¿‡åœ¨Validityå’ŒRelevanceç»´åº¦è¿›è¡Œç»†ç²’åº¦çš„credit assignmentï¼Œä»è€Œä»â€œalmost-thereâ€è½¨è¿¹ä¸­æ•æ‰å…³é”®å­¦ä¹ ä¿¡å·ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šä¸ªé•¿æ–‡æœ¬æ¨ç†åŸºå‡†æµ‹è¯•ä¸­æ˜¾è‘—ä¼˜äºRLVRåŸºå‡†ï¼Œå¹¶ä»¥è¾ƒå°‘çš„å‚æ•°é‡è¾¾åˆ°äº†frontier LLMsçš„æ€§èƒ½æ°´å¹³ã€‚è¯¥ç ”ç©¶åœ¨æ˜¾è‘—å¢å¼ºæ¨¡å‹é•¿æ–‡æœ¬æ¨ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œä¹Ÿæœ‰æ•ˆç»´æŒäº†å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„ç¨³å®šæ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12465v1",
      "published_date": "2026-01-18 16:10:04 UTC",
      "updated_date": "2026-01-18 16:10:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:39:20.294448+00:00"
    },
    {
      "arxiv_id": "2601.12449v1",
      "title": "AgenTRIM: Tool Risk Mitigation for Agentic AI",
      "title_zh": "AgenTRIMï¼šé’ˆå¯¹ä»£ç†å¼ AI çš„å·¥å…·é£é™©ç¼“è§£",
      "authors": [
        "Roy Betser",
        "Shamik Bose",
        "Amit Giloni",
        "Chiara Picardi",
        "Sindhu Padakandla",
        "Roman Vainshtein"
      ],
      "abstract": "AI agents are autonomous systems that combine LLMs with external tools to solve complex tasks. While such tools extend capability, improper tool permissions introduce security risks such as indirect prompt injection and tool misuse. We characterize these failures as unbalanced tool-driven agency. Agents may retain unnecessary permissions (excessive agency) or fail to invoke required tools (insufficient agency), amplifying the attack surface and reducing performance. We introduce AgenTRIM, a framework for detecting and mitigating tool-driven agency risks without altering an agent's internal reasoning. AgenTRIM addresses these risks through complementary offline and online phases. Offline, AgenTRIM reconstructs and verifies the agent's tool interface from code and execution traces. At runtime, it enforces per-step least-privilege tool access through adaptive filtering and status-aware validation of tool calls. Evaluating on the AgentDojo benchmark, AgenTRIM substantially reduces attack success while maintaining high task performance. Additional experiments show robustness to description-based attacks and effective enforcement of explicit safety policies. Together, these results demonstrate that AgenTRIM provides a practical, capability-preserving approach to safer tool use in LLM-based agents.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AgenTRIMï¼Œä¸€ä¸ªç”¨äºæ£€æµ‹å’Œç¼“è§£ AI æ™ºèƒ½ä½“ (AI agents) å·¥å…·é©±åŠ¨ä»£ç†é£é™©çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å› ä¸å½“å·¥å…·æƒé™å¯¼è‡´çš„é—´æ¥æç¤ºæ³¨å…¥ (indirect prompt injection) å’Œå·¥å…·æ»¥ç”¨é—®é¢˜ã€‚AgenTRIM é’ˆå¯¹ä¸å¹³è¡¡çš„å·¥å…·é©±åŠ¨ä»£ç† (unbalanced tool-driven agency) ç°è±¡ï¼Œå³æƒé™è¿‡åº¦ (excessive agency) æˆ–ä¸è¶³ (insufficient agency)ï¼Œåœ¨ä¸æ”¹å˜æ™ºèƒ½ä½“å†…éƒ¨æ¨ç†çš„å‰æä¸‹å¢å¼ºç³»ç»Ÿå®‰å…¨æ€§ã€‚è¯¥æ¡†æ¶åˆ†ä¸ºç¦»çº¿å’Œåœ¨çº¿ä¸¤ä¸ªé˜¶æ®µï¼šç¦»çº¿é˜¶æ®µé€šè¿‡ä»£ç å’Œæ‰§è¡Œè½¨è¿¹é‡å»ºå¹¶éªŒè¯å·¥å…·æ¥å£ï¼Œè¿è¡Œæ—¶åˆ™é€šè¿‡è‡ªé€‚åº”è¿‡æ»¤å’ŒçŠ¶æ€æ„ŸçŸ¥éªŒè¯å®æ–½æ¯ä¸€æ­¥çš„æœ€å°ç‰¹æƒ (least-privilege) è®¿é—®æ§åˆ¶ã€‚åœ¨ AgentDojo åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒAgenTRIM åœ¨æ˜¾è‘—é™ä½æ”»å‡»æˆåŠŸç‡çš„åŒæ—¶ä¿æŒäº†æé«˜çš„ä»»åŠ¡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶å¯¹åŸºäºæè¿°çš„æ”»å‡»å…·æœ‰å¼ºé²æ£’æ€§ï¼Œå¹¶èƒ½æœ‰æ•ˆæ‰§è¡Œæ˜¾å¼å®‰å…¨ç­–ç•¥ï¼Œä¸ºåŸºäºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ™ºèƒ½ä½“æä¾›äº†ä¸€ç§å®ç”¨ä¸”ä¿èƒ½çš„å®‰å…¨å·¥å…·ä½¿ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Under review",
      "pdf_url": "https://arxiv.org/pdf/2601.12449v1",
      "published_date": "2026-01-18 15:10:18 UTC",
      "updated_date": "2026-01-18 15:10:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:39:15.399886+00:00"
    },
    {
      "arxiv_id": "2601.12444v1",
      "title": "Large Language Model for OWL Proofs",
      "title_zh": "é¢å‘ OWL è¯æ˜çš„å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "Hui Yang",
        "Jiaoyan Chen",
        "Uli Sattler"
      ],
      "abstract": "The ability of Large Language Models (LLMs) to perform reasoning tasks such as deduction has been widely investigated in recent years. Yet, their capacity to generate proofs-faithful, human-readable explanations of why conclusions follow-remains largely under explored. In this work, we study proof generation in the context of OWL ontologies, which are widely adopted for representing and reasoning over complex knowledge, by developing an automated dataset construction and evaluation framework. Our evaluation encompassing three sequential tasks for complete proving: Extraction, Simplification, and Explanation, as well as an additional task of assessing Logic Completeness of the premise. Through extensive experiments on widely used reasoning LLMs, we achieve important findings including: (1) Some models achieve overall strong results but remain limited on complex cases; (2) Logical complexity, rather than representation format (formal logic language versus natural language), is the dominant factor shaping LLM performance; and (3) Noise and incompleteness in input data substantially diminish LLMs' performance. Together, these results underscore both the promise of LLMs for explanation with rigorous logics and the gap of supporting resilient reasoning under complex or imperfect conditions. Code and data are available at https://github.com/HuiYang1997/LLMOwlR.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨OWLæœ¬ä½“è¯æ˜ç”Ÿæˆä¸­çš„èƒ½åŠ›ï¼Œå¹¶å¼€å‘äº†ä¸€ä¸ªè‡ªåŠ¨åŒ–çš„æ•°æ®é›†æ„å»ºå’Œè¯„ä¼°æ¡†æ¶ã€‚è¯¥è¯„ä¼°æ¡†æ¶æ¶µç›–äº†æå–(Extraction)ã€ç®€åŒ–(Simplification)ã€è§£é‡Š(Explanation)ä»¥åŠå‰æé€»è¾‘å®Œæ•´æ€§(Logic Completeness)è¯„ä¼°å››ä¸ªä»»åŠ¡ã€‚å®éªŒç ”ç©¶å‘ç°ï¼Œå°½ç®¡éƒ¨åˆ†æ¨¡å‹åœ¨æ•´ä½“ä¸Šå–å¾—äº†è¾ƒå¼ºç»“æœï¼Œä½†åœ¨å¤„ç†å¤æ‚é€»è¾‘æ¡ˆä¾‹æ—¶ä»é¢ä¸´å±€é™ã€‚ç ”ç©¶è¿›ä¸€æ­¥è¯æ˜ï¼Œé€»è¾‘å¤æ‚åº¦(Logical complexity)è€Œéè¡¨è¿°æ ¼å¼ï¼ˆå½¢å¼è¯­è¨€ä¸è‡ªç„¶è¯­è¨€ï¼‰æ˜¯å†³å®šLLMsæ€§èƒ½çš„ä¸»å¯¼å› ç´ ã€‚åŒæ—¶ï¼Œè¾“å…¥æ•°æ®ä¸­çš„å™ªå£°å’Œä¸å®Œæ•´æ€§ä¼šæ˜¾è‘—é™ä½æ¨¡å‹çš„è¡¨ç°ã€‚è¿™é¡¹å·¥ä½œçªæ˜¾äº†LLMsåœ¨ä¸¥å¯†é€»è¾‘è§£é‡Šæ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶æŒ‡å‡ºäº†å…¶åœ¨å¤æ‚æˆ–ä¸å®Œç¾æ¡ä»¶ä¸‹å®ç°å¼¹æ€§æ¨ç†çš„æ”¹è¿›ç©ºé—´ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12444v1",
      "published_date": "2026-01-18 14:57:57 UTC",
      "updated_date": "2026-01-18 14:57:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:39:36.526696+00:00"
    },
    {
      "arxiv_id": "2601.12443v1",
      "title": "Adversarial Defense in Vision-Language Models: An Overview",
      "title_zh": "è§†è§‰è¯­è¨€æ¨¡å‹å¯¹æŠ—é˜²å¾¡ç»¼è¿°",
      "authors": [
        "Xiaowei Fu",
        "Lei Zhang"
      ],
      "abstract": "The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised concerns about their vulnerability to sophisticated and imperceptible adversarial attacks. These attacks could compromise model performance and system security in cross-modal tasks. To address this challenge, three main defense paradigms have been proposed: Training-time Defense, Test-time Adaptation Defense, and Training-free Defense. Training-time Defense involves modifying the training process, typically through adversarial fine-tuning to improve the robustness to adversarial examples. While effective, this approach requires substantial computational resources and may not generalize across all adversarial attacks. Test-time Adaptation Defense focuses on adapting the model at inference time by updating its parameters to handle unlabeled adversarial examples, offering flexibility but often at the cost of increased complexity and computational overhead. Training-free Defense avoids modifying the model itself, instead focusing on altering the adversarial inputs or their feature embeddings, which enforces input perturbations to mitigate the impact of attacks without additional training. This survey reviews the latest advancements in adversarial defense strategies for VLMs, highlighting the strengths and limitations of such approaches and discussing ongoing challenges in enhancing the robustness of VLMs.",
      "tldr_zh": "è¯¥ç»¼è¿°å…¨é¢å›é¡¾äº†è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) åœ¨å¯¹æŠ—æ€§é˜²å¾¡ (Adversarial Defense) é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œæ—¨åœ¨è§£å†³ CLIP ç­‰æ¨¡å‹åœ¨è·¨æ¨¡æ€ä»»åŠ¡ä¸­æ˜“å—å¯¹æŠ—æ€§æ”»å‡»çš„å®‰å…¨éšæ‚£ã€‚æ–‡ç« å°†é˜²å¾¡ç­–ç•¥å½’çº³ä¸ºä¸‰ç§ä¸»è¦èŒƒå¼ï¼šè®­ç»ƒæ—¶é˜²å¾¡ (Training-time Defense)ã€æµ‹è¯•æ—¶è‡ªé€‚åº”é˜²å¾¡ (Test-time Adaptation Defense) å’Œå…è®­ç»ƒé˜²å¾¡ (Training-free Defense)ã€‚è®­ç»ƒæ—¶é˜²å¾¡ä¸»è¦é€šè¿‡å¯¹æŠ—æ€§å¾®è°ƒ (Adversarial Fine-tuning) æå‡é²æ£’æ€§ï¼Œä½†é€šå¸¸éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼›æµ‹è¯•æ—¶è‡ªé€‚åº”é˜²å¾¡é€šè¿‡åœ¨æ¨ç†æ—¶æ›´æ–°å‚æ•°æ¥å¤„ç†å¯¹æŠ—æ ·æœ¬ï¼Œæä¾›äº†çµæ´»æ€§ä½†å¢åŠ äº†è®¡ç®—å¼€é”€ã€‚å…è®­ç»ƒé˜²å¾¡åˆ™ä¾§é‡äºä¿®æ”¹è¾“å…¥æˆ–ç‰¹å¾åµŒå…¥ (Feature Embeddings) ä»¥å‡è½»æ”»å‡»å½±å“ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚è¯¥ç ”ç©¶é€šè¿‡å¯¹æ¯”ä¸åŒæ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼Œæ¢è®¨äº†å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹é²æ£’æ€§æ‰€é¢ä¸´çš„æŒç»­æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æŒ‡å¼•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12443v1",
      "published_date": "2026-01-18 14:57:51 UTC",
      "updated_date": "2026-01-18 14:57:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:39:25.296496+00:00"
    },
    {
      "arxiv_id": "2601.12442v1",
      "title": "Constraint-Aware Neurosymbolic Uncertainty Quantification with Bayesian Deep Learning for Scientific Discovery",
      "title_zh": "é¢å‘ç§‘å­¦å‘ç°çš„ã€ç»“åˆè´å¶æ–¯æ·±åº¦å­¦ä¹ çš„çº¦æŸæ„ŸçŸ¥ç¥ç»ç¬¦å·ä¸ç¡®å®šæ€§é‡åŒ–",
      "authors": [
        "Shahnawaz Alam",
        "Mohammed Mudassir Uddin",
        "Mohammed Kaif Pasha"
      ],
      "abstract": "Scientific Artificial Intelligence (AI) applications require models that deliver trustworthy uncertainty estimates while respecting domain constraints. Existing uncertainty quantification methods lack mechanisms to incorporate symbolic scientific knowledge, while neurosymbolic approaches operate deterministically without principled uncertainty modeling. We introduce the Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), unifying Bayesian deep learning with differentiable symbolic reasoning. The architecture comprises three components: automated constraint extraction from scientific literature, probabilistic neural backbone with variational inference, and differentiable constraint satisfaction layer ensuring physical consistency. Experiments on Materials Project (140,000+ materials), QM9 molecular properties, and climate benchmarks show CANUF reduces Expected Calibration Error by 34.7% versus Bayesian neural networks while maintaining 99.2% constraint satisfaction. Ablations reveal constraint-guided recalibration contributes 18.3% performance gain, with constraint extraction achieving 91.4% precision. CANUF provides the first end-to-end differentiable pipeline simultaneously addressing uncertainty quantification, constraint satisfaction, and interpretable explanations for scientific predictions.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†çº¦æŸæ„ŸçŸ¥ç¥ç»ç¬¦å·ä¸ç¡®å®šæ€§æ¡†æ¶(Constraint-Aware Neurosymbolic Uncertainty Framework, CANUF)ï¼Œæ—¨åœ¨è§£å†³ç§‘å­¦äººå·¥æ™ºèƒ½(Scientific AI)ä¸­ä¸ç¡®å®šæ€§ä¼°è®¡ä¸é¢†åŸŸçº¦æŸç»“åˆä¸è¶³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é€šè¿‡ç»Ÿä¸€è´å¶æ–¯æ·±åº¦å­¦ä¹ (Bayesian deep learning)ä¸å¯å¾®ç¬¦å·æ¨ç†ï¼Œæ„å»ºäº†åŒ…å«è‡ªåŠ¨çº¦æŸæå–ã€å˜åˆ†æ¨ç†(variational inference)æ¦‚ç‡éª¨å¹²åŠå¯å¾®çº¦æŸæ»¡è¶³å±‚çš„æ¶æ„ï¼Œä»¥ç¡®ä¿é¢„æµ‹çš„ç‰©ç†ä¸€è‡´æ€§ã€‚åœ¨ Materials Projectã€QM9 åˆ†å­å±æ€§åŠæ°”å€™åŸºå‡†ä¸Šçš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒCANUF ç›¸æ¯”ä¼ ç»Ÿè´å¶æ–¯ç¥ç»ç½‘ç»œå°†é¢„æœŸæ ¡å‡†è¯¯å·®(Expected Calibration Error)é™ä½äº† 34.7%ï¼ŒåŒæ—¶ä¿æŒäº† 99.2% çš„çº¦æŸæ»¡è¶³ç‡ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®ï¼Œçº¦æŸå¼•å¯¼çš„é‡æ ¡å‡†è´¡çŒ®äº† 18.3% çš„æ€§èƒ½å¢ç›Šï¼Œä¸”çº¦æŸæå–ç²¾åº¦è¾¾åˆ° 91.4%ã€‚ä½œä¸ºé¦–ä¸ªå®ç°ä¸ç¡®å®šæ€§é‡åŒ–ã€çº¦æŸæ»¡è¶³ä¸å¯è§£é‡Šæ€§è¯´æ˜åŒæ­¥å¤„ç†çš„ç«¯åˆ°ç«¯å¯å¾®æµæ°´çº¿ï¼ŒCANUF ä¸ºç§‘å­¦å‘ç°é¢†åŸŸçš„å¯ä¿¡é¢„æµ‹å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12442v1",
      "published_date": "2026-01-18 14:57:35 UTC",
      "updated_date": "2026-01-18 14:57:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:39:29.360061+00:00"
    },
    {
      "arxiv_id": "2601.12436v1",
      "title": "Purification Before Fusion: Toward Mask-Free Speech Enhancement for Robust Audio-Visual Speech Recognition",
      "title_zh": "å…ˆå‡€åŒ–åèåˆï¼šé¢å‘é²æ£’éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«çš„å…æ©ç è¯­éŸ³å¢å¼º",
      "authors": [
        "Linzhi Wu",
        "Xingyu Zhang",
        "Hao Yuan",
        "Yakun Zhang",
        "Changyan Zheng",
        "Liang Xie",
        "Tiejun Liu",
        "Erwei Yin"
      ],
      "abstract": "Audio-visual speech recognition (AVSR) typically improves recognition accuracy in noisy environments by integrating noise-immune visual cues with audio signals. Nevertheless, high-noise audio inputs are prone to introducing adverse interference into the feature fusion process. To mitigate this, recent AVSR methods often adopt mask-based strategies to filter audio noise during feature interaction and fusion, yet such methods risk discarding semantically relevant information alongside noise. In this work, we propose an end-to-end noise-robust AVSR framework coupled with speech enhancement, eliminating the need for explicit noise mask generation. This framework leverages a Conformer-based bottleneck fusion module to implicitly refine noisy audio features with video assistance. By reducing modality redundancy and enhancing inter-modal interactions, our method preserves speech semantic integrity to achieve robust recognition performance. Experimental evaluations on the public LRS3 benchmark suggest that our method outperforms prior advanced mask-based baselines under noisy conditions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Audio-Visual Speech Recognition (AVSR) åœ¨é«˜å™ªå£°ç¯å¢ƒä¸‹ç”±äºéŸ³é¢‘å¹²æ‰°å½±å“ç‰¹å¾èåˆçš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆ speech enhancement çš„ç«¯åˆ°ç«¯é²æ£’æ¡†æ¶ã€‚ä¸ºäº†è§£å†³ä¼ ç»Ÿ mask-based ç­–ç•¥åœ¨è¿‡æ»¤å™ªå£°æ—¶å®¹æ˜“ä¸¢å¤±è¯­ä¹‰ä¿¡æ¯çš„é—®é¢˜ï¼Œè¯¥æ–¹æ³•å–æ¶ˆäº†æ˜¾å¼å™ªå£°æ©ç çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œæ—¨åœ¨å®ç° mask-free çš„å¤„ç†æµç¨‹ã€‚é€šè¿‡åˆ©ç”¨ Conformer-based bottleneck fusion moduleï¼Œæ¡†æ¶èƒ½å¤Ÿåœ¨è§†é¢‘ä¿¡æ¯çš„è¾…åŠ©ä¸‹éšå¼åœ°ä¼˜åŒ–å—æ±¡æŸ“çš„éŸ³é¢‘ç‰¹å¾ã€‚è¿™ç§è®¾è®¡æœ‰æ•ˆåœ°å‡å°‘äº†æ¨¡æ€å†—ä½™å¹¶å¢å¼ºäº†è·¨æ¨¡æ€äº¤äº’ï¼Œä»è€Œåœ¨æå‡è¯†åˆ«é²æ£’æ€§çš„åŒæ—¶ç¡®ä¿äº†è¯­éŸ³è¯­ä¹‰çš„å®Œæ•´æ€§ã€‚åœ¨ LRS3 å…¬å…±åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å„ç±»å™ªå£°æ¡ä»¶ä¸‹çš„è¯†åˆ«å‡†ç¡®ç‡å‡ä¼˜äºç°æœ‰çš„å…ˆè¿› mask-based åŸºçº¿æ¨¡å‹ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted by ICASSP2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12436v1",
      "published_date": "2026-01-18 14:46:08 UTC",
      "updated_date": "2026-01-18 14:46:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:39:33.062337+00:00"
    },
    {
      "arxiv_id": "2601.12415v2",
      "title": "Orthogonalized Policy Optimization:Decoupling Sampling Geometry from Optimization Geometry in RLHF",
      "title_zh": "æ­£äº¤åŒ–ç­–ç•¥ä¼˜åŒ–ï¼šå®ç° RLHF ä¸­é‡‡æ ·å‡ ä½•ä¸ä¼˜åŒ–å‡ ä½•çš„è§£è€¦",
      "authors": [
        "Wang Zixian"
      ],
      "abstract": "Large language model alignment objectives are often presented as a collection of distinct algorithms, such as PPO, DPO, IPO, and their variants, each motivated by different derivations. In this work, we argue that this diversity obscures a simpler underlying structure. At a fundamental level, alignment objectives involve two independent design choices: (i) how training signals are sampled and weighted, and (ii) how deviations from a reference policy are geometrically penalized. Existing methods typically entangle these choices through a single divergence, most commonly the Kullback-Leibler divergence.\n  We show that this entanglement is not merely a modeling convenience but a source of systematic instability. When the same divergence simultaneously determines sample weighting and optimization curvature, adjusting one aspect, such as exploration strength, inevitably alters the other, such as gradient geometry. This coupling is particularly problematic in preference-based reinforcement learning, where advantage signals are unbounded and high-confidence regimes are common.\n  We propose a simple but structural remedy by formulating alignment as an orthogonal mirror descent problem, in which sampling geometry enters only as a linear driving force, while optimization geometry is determined independently by a mirror map. This perspective leads to a new alignment objective called Orthogonalized Policy Optimization (OPO), obtained by choosing a Euclidean mirror map in likelihood ratio space. The resulting objective admits a closed-form solution, linear and non-saturating gradient dynamics, and a well-conditioned trust region, while remaining fully compatible with standard large language model training pipelines.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œå½“å‰çš„å¤§è¯­è¨€æ¨¡å‹å¯¹é½æ–¹æ³•ï¼ˆå¦‚ PPOã€DPOã€IPO ç­‰ï¼‰é€šå¸¸å°†è®­ç»ƒä¿¡å·é‡‡æ ·ä¸åç¦»å‚è€ƒç­–ç•¥çš„å‡ ä½•æƒ©ç½šè¿™ä¸¤ä¸ªç‹¬ç«‹çš„è®¾è®¡é€‰æ‹©é€šè¿‡å•ä¸€çš„ Kullback-Leibler (KL) æ•£åº¦çº ç¼ åœ¨ä¸€èµ·ï¼Œå¯¼è‡´äº†ç³»ç»Ÿæ€§çš„ä¸ç¨³å®šæ€§ã€‚è¿™ç§è€¦åˆä½¿å¾—è°ƒæ•´æ¢ç´¢å¼ºåº¦ç­‰å‚æ•°ä¼šä¸å¯é¿å…åœ°æ”¹å˜æ¢¯åº¦å‡ ä½•ï¼Œåœ¨åå¥½é©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ åœºæ™¯ä¸‹å°¤ä¸ºä¸¥é‡ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†ä¸€ç§ç»“æ„åŒ–çš„è¡¥æ•‘æ–¹æ¡ˆï¼Œå°†å¯¹é½é—®é¢˜é‡æ–°è¡¨è¿°ä¸ºæ­£äº¤åŒ–é•œåƒä¸‹é™ï¼ˆorthogonal mirror descentï¼‰é—®é¢˜ï¼Œä½¿é‡‡æ ·å‡ ä½•ä»…ä½œä¸ºçº¿æ€§é©±åŠ¨åŠ›ï¼Œè€Œä¼˜åŒ–å‡ ä½•åˆ™ç”±é•œåƒæ˜ å°„ï¼ˆmirror mapï¼‰ç‹¬ç«‹ç¡®å®šã€‚åœ¨æ­¤åŸºç¡€ä¸Šæ¨å‡ºçš„ Orthogonalized Policy Optimization (OPO) åœ¨ä¼¼ç„¶æ¯”ç©ºé—´ï¼ˆlikelihood ratio spaceï¼‰ä¸­é€‰ç”¨æ¬§å‡ é‡Œå¾—é•œåƒæ˜ å°„ï¼ˆEuclidean mirror mapï¼‰ï¼Œä»è€Œå®ç°äº†é‡‡æ ·ä¸ä¼˜åŒ–å‡ ä½•çš„è§£è€¦ã€‚OPO ç®—æ³•å…·å¤‡è§£æè§£ã€çº¿æ€§ä¸”éé¥±å’Œçš„æ¢¯åº¦åŠ¨åŠ›å­¦ä»¥åŠè‰¯æ„çš„ç½®ä¿¡åŒºåŸŸï¼ˆtrust regionï¼‰ï¼Œåœ¨ä¿æŒä¸æ ‡å‡†å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæµç¨‹å®Œå…¨å…¼å®¹çš„åŒæ—¶ï¼Œæå‡äº†å¯¹é½è¿‡ç¨‹çš„ç¨³å®šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12415v2",
      "published_date": "2026-01-18 13:57:44 UTC",
      "updated_date": "2026-01-21 14:54:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:39:37.292899+00:00"
    },
    {
      "arxiv_id": "2601.12410v1",
      "title": "Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking and Knowledge State Estimation",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹æ¯”é»‘çŒ©çŒ©æ›´èªæ˜å—ï¼Ÿé’ˆå¯¹è§‚ç‚¹é‡‡æ‹©ä¸çŸ¥è¯†çŠ¶æ€ä¼°è®¡çš„è¯„ä¼°",
      "authors": [
        "Dingyi Yang",
        "Junqi Zhao",
        "Xue Li",
        "Ce Li",
        "Boyang Li"
      ],
      "abstract": "Cognitive anthropology suggests that the distinction of human intelligence lies in the ability to infer other individuals' knowledge states and understand their intentions. In comparison, our closest animal relative, chimpanzees, lack the capacity to do so. With this paper, we aim to evaluate LLM performance in the area of knowledge state tracking and estimation. We design two tasks to test (1) if LLMs can detect when story characters, through their actions, demonstrate knowledge they should not possess, and (2) if LLMs can predict story characters' next actions based on their own knowledge vs. objective truths they do not know. Results reveal that most current state-of-the-art LLMs achieve near-random performance on both tasks, and are substantially inferior to humans. We argue future LLM research should place more weight on the abilities of knowledge estimation and intention understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº† LLMs åœ¨è§‚ç‚¹é‡‡æ‹©(Perspective Taking)å’ŒçŸ¥è¯†çŠ¶æ€è¯„ä¼°(Knowledge State Estimation)æ–¹é¢çš„è¡¨ç°ï¼Œä»¥æ¢ç©¶å…¶æ˜¯å¦å…·å¤‡æ¨æ–­ä»–äººçŸ¥è¯†çŠ¶æ€å’Œæ„å›¾çš„èƒ½åŠ›ã€‚ç ”ç©¶è®¾è®¡äº†ä¸¤é¡¹æ ¸å¿ƒä»»åŠ¡ï¼Œåˆ†åˆ«æµ‹è¯• LLMs èƒ½å¦è¯†åˆ«æ•…äº‹è§’è‰²è¡¨ç°å‡ºå…¶ä¸åº”å…·å¤‡çš„çŸ¥è¯†ï¼Œä»¥åŠæ¨¡å‹èƒ½å¦åŸºäºè§’è‰²çš„ä¸»è§‚è®¤çŸ¥è€Œéå®¢è§‚äº‹å®é¢„æµ‹å…¶åç»­è¡Œä¸ºã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œå½“å‰æœ€å…ˆè¿›çš„ LLMs åœ¨è¿™ä¸¤é¡¹ä»»åŠ¡ä¸­çš„è¡¨ç°å‡æ¥è¿‘éšæœºæ°´å¹³ï¼Œä¸”æ˜¾è‘—é€Šäºäººç±»ã€‚åŸºäºæ­¤å‘ç°ï¼Œä½œè€…æŒ‡å‡ºæœªæ¥çš„ LLM ç ”ç©¶åº”å½“å°†æ›´å¤šé‡å¿ƒæ”¾åœ¨çŸ¥è¯†è¯„ä¼°(Knowledge Estimation)å’Œæ„å›¾ç†è§£(Intention Understanding)èƒ½åŠ›çš„æå‡ä¸Šã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 11 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.12410v1",
      "published_date": "2026-01-18 13:53:24 UTC",
      "updated_date": "2026-01-18 13:53:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:40:08.419401+00:00"
    },
    {
      "arxiv_id": "2601.12405v1",
      "title": "Explainable Machine Learning for Pediatric Dental Risk Stratification Using Socio-Demographic Determinants",
      "title_zh": "åŸºäºç¤¾ä¼šäººå£å­¦å†³å®šå› ç´ çš„å„¿ç«¥å£è…”é£é™©åˆ†å±‚å¯è§£é‡Šæœºå™¨å­¦ä¹ ",
      "authors": [
        "Manasi Kanade",
        "Abhi Thakkar",
        "Gabriela Fernandes"
      ],
      "abstract": "Background: Pediatric dental disease remains one of the most prevalent and inequitable chronic health conditions worldwide. Although strong epidemiological evidence links oral health outcomes to socio-economic and demographic determinants, most artificial intelligence (AI) applications in dentistry rely on image-based diagnosis and black-box prediction models, limiting transparency and ethical applicability in pediatric populations.\n  Objective: This study aimed to develop and evaluate an explainable machine learning framework for pediatric dental risk stratification that prioritizes interpretability, calibration, and ethical deployment over maximal predictive accuracy.\n  Methods: A supervised machine learning model was trained using population-level pediatric data including age, income-to-poverty ratio, race/ethnicity, gender, and medical history. Model performance was assessed using receiver operating characteristic (ROC) analysis and calibration curves. Explainability was achieved using SHapley Additive exPlanations (SHAP) to provide global and individual-level interpretation of predictions.\n  Results: The model achieved modest discrimination (AUC = 0.61) with conservative calibration, underestimating risk at higher probability levels. SHAP analysis identified age and income-to-poverty ratio as the strongest contributors to predicted risk, followed by race/ethnicity and gender.\n  Conclusion: Explainable machine learning enables transparent, prevention-oriented pediatric dental risk stratification and supports population screening and equitable resource allocation rather than diagnostic decision-making.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘äº†ä¸€ä¸ªé¢å‘å„¿ç«¥ç‰™ç§‘é£é™©åˆ†å±‚(Pediatric Dental Risk Stratification)çš„å¯è§£é‡Šæœºå™¨å­¦ä¹ (Explainable Machine Learning)æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨ç¤¾ä¼šäººå£å­¦å†³å®šå› ç´ (Socio-demographic Determinants)æå‡é¢„æµ‹æ¨¡å‹çš„é€æ˜åº¦ä¸ä¼¦ç†é€‚ç”¨æ€§ã€‚ç ”ç©¶é€šè¿‡åˆ†æå¹´é¾„ã€æ”¶å…¥ä¸è´«å›°æ¯”ç‡(Income-to-poverty ratio)ã€ç§æ—å’ŒåŒ»ç–—èƒŒæ™¯ç­‰æ•°æ®è®­ç»ƒç›‘ç£å­¦ä¹ æ¨¡å‹ï¼Œå¹¶å¼•å…¥ SHapley Additive exPlanations (SHAP) æŠ€æœ¯å®ç°å¯¹é£é™©é¢„æµ‹çš„å…¨å±€åŠä¸ªä½“åŒ–è§£é‡Šã€‚å®éªŒç»“æœæ˜¾ç¤ºæ¨¡å‹è¾¾åˆ°äº† 0.61 çš„ AUC åŒºåˆ†åº¦ï¼Œä¸”æ ¡å‡†æ›²çº¿(Calibration curves)æ˜¾ç¤ºå…¶åœ¨é«˜æ¦‚ç‡æ°´å¹³ä¸‹çš„é¢„æµ‹è¾ƒä¸ºä¿å®ˆã€‚SHAP åˆ†æè¯†åˆ«å‡ºå¹´é¾„å’Œç»æµçŠ¶å†µæ˜¯å½±å“ç‰™ç§‘é£é™©çš„æœ€æ ¸å¿ƒè´¡çŒ®å› ç´ ï¼Œå…¶æ¬¡æ˜¯ç§æ—å’Œæ€§åˆ«ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å¯è§£é‡Šæ€§æŠ€æœ¯åœ¨é¢„é˜²æ€§ç­›æŸ¥å’Œå…¬å¹³èµ„æºåˆ†é…ä¸­çš„ä»·å€¼ï¼Œä¸ºæ„å»ºé€æ˜ä¸”éé»‘ç›’åŒ–çš„ä¸´åºŠå†³ç­–ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12405v1",
      "published_date": "2026-01-18 13:40:41 UTC",
      "updated_date": "2026-01-18 13:40:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:40:04.093215+00:00"
    },
    {
      "arxiv_id": "2601.12402v1",
      "title": "Weaknesses of Facial Emotion Recognition Systems",
      "title_zh": "é¢éƒ¨è¡¨æƒ…è¯†åˆ«ç³»ç»Ÿçš„å±€é™æ€§",
      "authors": [
        "Aleksandra JamrÃ³z",
        "Patrycja Wysocka",
        "Piotr Garbat"
      ],
      "abstract": "Emotion detection from faces is one of the machine learning problems needed for human-computer interaction. The variety of methods used is enormous, which motivated an in-depth review of articles and scientific studies. Three of the most interesting and best solutions are selected, followed by the selection of three datasets that stood out for the diversity and number of images in them. The selected neural networks are trained, and then a series of experiments are performed to compare their performance, including testing on different datasets than a model was trained on. This reveals weaknesses in existing solutions, including differences between datasets, unequal levels of difficulty in recognizing certain emotions and the challenges in differentiating between closely related emotions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººæœºäº¤äº’ä¸­å…³é”®çš„äººè„¸è¡¨æƒ…è¯†åˆ«(Facial Emotion Recognition)ç³»ç»Ÿè¿›è¡Œäº†æ·±å…¥çš„å¼±ç‚¹åˆ†æã€‚ä½œè€…åœ¨å¹¿æ³›ç»¼è¿°çš„åŸºç¡€ä¸Šï¼Œé€‰å–äº†ä¸‰ç§ä¸»æµçš„ç¥ç»ç½‘ç»œæ¨¡å‹å’Œä¸‰ä¸ªå…·æœ‰é«˜åº¦å¤šæ ·æ€§çš„æ•°æ®é›†è¿›è¡Œå®éªŒã€‚é€šè¿‡äº¤å‰æ•°æ®é›†è®­ç»ƒä¸æ€§èƒ½å¯¹æ¯”ï¼Œç ”ç©¶æ­ç¤ºäº†ç°æœ‰æŠ€æœ¯åœ¨å¤„ç†ä¸åŒæ•°æ®é›†å·®å¼‚ã€è¯†åˆ«éš¾åº¦ä¸å‡ä»¥åŠåŒºåˆ†ç›¸ä¼¼æƒ…ç»ªæ–¹é¢çš„å±€é™æ€§ã€‚å®éªŒç»“æœæ˜ç¡®äº†å½“å‰ç³»ç»Ÿåœ¨æ³›åŒ–èƒ½åŠ›å’Œç²¾ç»†åˆ†ç±»ä¸Šçš„æŒ‘æˆ˜ã€‚è¿™äº›å‘ç°ä¸ºæœªæ¥å¼€å‘æ›´å…·é²æ£’æ€§çš„æƒ…æ„Ÿè®¡ç®—æ¨¡å‹æä¾›äº†å®è¯ä¾æ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12402v1",
      "published_date": "2026-01-18 13:27:01 UTC",
      "updated_date": "2026-01-18 13:27:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:39:55.989236+00:00"
    },
    {
      "arxiv_id": "2601.12401v1",
      "title": "Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement Fine-Tuning for Versatile Image Generation",
      "title_zh": "è¶…è¶Šç‹„æ‹‰å…‹ Deltaï¼šç¼“è§£å¼ºåŒ–å­¦ä¹ å¾®è°ƒä¸­çš„å¤šæ ·æ€§å´©æºƒï¼Œå®ç°å¤šæ ·åŒ–å›¾åƒç”Ÿæˆ",
      "authors": [
        "Jinmei Liu",
        "Haoru Li",
        "Zhenhong Sun",
        "Chaofeng Chen",
        "Yatao Bian",
        "Bo Wang",
        "Daoyi Dong",
        "Chunlin Chen",
        "Zhi Wang"
      ],
      "abstract": "Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning large-scale generative models, such as diffusion and flow models, to align with complex human preferences and user-specified tasks. A fundamental limitation remains \\textit{the curse of diversity collapse}, where the objective formulation and optimization landscape inherently collapse the policy to a Dirac delta distribution. To address this challenge, we propose \\textbf{DRIFT} (\\textbf{D}ive\\textbf{R}sity-\\textbf{I}ncentivized Reinforcement \\textbf{F}ine-\\textbf{T}uning for Versatile Image Generation), an innovative framework that systematically incentivizes output diversity throughout the on-policy fine-tuning process, reconciling strong task alignment with high generation diversity to enhance versatility essential for applications that demand diverse candidate generations. We approach the problem across three representative perspectives: i) \\textbf{sampling} a reward-concentrated subset that filters out reward outliers to prevent premature collapse; ii) \\textbf{prompting} with stochastic variations to expand the conditioning space, and iii) \\textbf{optimization} of the intra-group diversity with a potential-based reward shaping mechanism. Experimental results show that DRIFT achieves superior Pareto dominance regarding task alignment and generation diversity, yielding a $ 9.08\\%\\!\\sim\\! 43.46\\%$ increase in diversity at equivalent alignment levels and a $ 59.65\\% \\!\\sim\\! 65.86\\%$ increase in alignment at equivalent levels of diversity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)å¾®è°ƒæ‰©æ•£æ¨¡å‹å’Œæµæ¨¡å‹æ—¶æ™®éå­˜åœ¨çš„å¤šæ ·æ€§å´©æºƒ(diversity collapse)é—®é¢˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„ä¼˜åŒ–ç›®æ ‡å®¹æ˜“ä½¿ç­–ç•¥æ”¶æ•›äºç‹„æ‹‰å…‹Î´åˆ†å¸ƒ(Dirac delta distribution)ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†DRIFTï¼ˆDiversity-Incentivized Reinforcement Fine-Tuningï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨ç­–ç•¥å¾®è°ƒè¿‡ç¨‹ä¸­ç³»ç»Ÿæ€§åœ°æ¿€åŠ±è¾“å‡ºå¤šæ ·æ€§ï¼Œä»¥å®ç°ä»»åŠ¡å¯¹é½åº¦(task alignment)ä¸ç”Ÿæˆå¤šæ ·æ€§çš„å¹³è¡¡ã€‚è¯¥æ–¹æ³•ä»é‡‡æ ·ã€æç¤ºè¯å’Œä¼˜åŒ–ä¸‰ä¸ªç»´åº¦å…¥æ‰‹ï¼šé€šè¿‡é‡‡æ ·å¥–åŠ±é›†ä¸­å­é›†è¿‡æ»¤ç¦»ç¾¤å€¼ä»¥é˜²æ­¢è¿‡æ—©å´©æºƒï¼Œåˆ©ç”¨éšæœºå˜ä½“æç¤ºè¯æ‰©å±•æ¡ä»¶ç©ºé—´ï¼Œå¹¶å¼•å…¥åŸºäºåŠ¿èƒ½çš„å¥–åŠ±æ•´å½¢æœºåˆ¶(potential-based reward shaping)æ¥ä¼˜åŒ–ç»„å†…å¤šæ ·æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒDRIFTåœ¨ä»»åŠ¡å¯¹é½åº¦ä¸ç”Ÿæˆå¤šæ ·æ€§ä¹‹é—´å®ç°äº†å“è¶Šçš„å¸•ç´¯æ‰˜æ”¯é…(Pareto dominance)ã€‚ç›¸æ¯”åŸºçº¿æ¨¡å‹ï¼Œå…¶åœ¨åŒç­‰å¯¹é½æ°´å¹³ä¸‹å¤šæ ·æ€§æå‡äº†9.08%è‡³43.46%ï¼Œåœ¨åŒç­‰å¤šæ ·æ€§æ°´å¹³ä¸‹å¯¹é½åº¦æå‡äº†59.65%è‡³65.86%ã€‚è¯¥æ¡†æ¶ä¸ºéœ€è¦å¤šæ ·åŒ–å€™é€‰ç»“æœçš„ç”Ÿæˆå¼åº”ç”¨æä¾›äº†å…¼å…·é«˜å¯¹é½æ€§å’Œé«˜å¤šæ ·æ€§çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12401v1",
      "published_date": "2026-01-18 13:25:43 UTC",
      "updated_date": "2026-01-18 13:25:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:40:06.969866+00:00"
    },
    {
      "arxiv_id": "2601.12392v1",
      "title": "PsychÄ“Chat: An Empathic Framework Focused on Emotion Shift Tracking and Safety Risk Analysis in Psychological Counseling",
      "title_zh": "PsychÄ“Chatï¼šèšç„¦å¿ƒç†å’¨è¯¢ä¸­æƒ…ç»ªè½¬å˜è¿½è¸ªä¸å®‰å…¨é£é™©åˆ†æçš„å…±æƒ…æ¡†æ¶",
      "authors": [
        "Zhentao Xia",
        "Yongqi Fan",
        "Yuxiang Chu",
        "Yichao Yin",
        "Liangliang Chen",
        "Tong Ruan",
        "Weiyan Zhang"
      ],
      "abstract": "Large language models (LLMs) have demonstrated notable advancements in psychological counseling. However, existing models generally do not explicitly model seekers' emotion shifts across counseling sessions, a core focus in classical psychological schools. Moreover, how to align counselor models' responses with these emotion shifts while proactively mitigating safety risks remains underexplored. To bridge these gaps, we propose PsychÄ“Chat, which explicitly integrates emotion shift tracking and safety risk analysis for psychological counseling. Specifically, we employ interactive role-playing to synthesize counselor--seeker dialogues, incorporating two modules: Emotion Management Module, to capture seekers' current emotions and emotion shifts; and Risk Control Module, to anticipate seekers' subsequent reactions and identify potential risks. Furthermore, we introduce two modeling paradigms. The Agent Mode structures emotion management, risk control, and counselor responses into a collaborative multi-agent pipeline. The LLM Mode integrates these stages into a unified chain-of-thought for end-to-end inference, balancing efficiency and performance. Extensive experiments, including interactive scoring, dialogue-level evaluation, and human assessment, demonstrate that PsychÄ“Chat outperforms existing methods for emotional insight and safety control.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰å¿ƒç†å’¨è¯¢å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç¼ºä¹å¯¹æƒ…ç»ªè½¬å˜ï¼ˆemotion shiftsï¼‰æ˜¾å¼å»ºæ¨¡åŠå®‰å…¨é£é™©ç¼“è§£ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº† PsychÄ“Chat æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡äº¤äº’å¼è§’è‰²æ‰®æ¼”ï¼ˆinteractive role-playingï¼‰åˆæˆå¯¹è¯æ•°æ®ï¼Œå¹¶é›†æˆä¸¤ä¸ªæ ¸å¿ƒæ¨¡å—ï¼šEmotion Management Module ç”¨äºæ•æ‰æ¥è®¿è€…çš„å®æ—¶æƒ…ç»ªåŠå…¶è½¬å˜è¿‡ç¨‹ï¼ŒRisk Control Module ç”¨äºé¢„æµ‹æ¥è®¿è€…åç»­ååº”å¹¶è¯†åˆ«æ½œåœ¨é£é™©ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†å¤šæ™ºèƒ½ä½“åä½œçš„ Agent Mode å’ŒåŸºäºé“¾å¼æ€ç»´ï¼ˆChain-of-Thoughtï¼‰æ¨ç†çš„ LLM Mode ä¸¤ç§å»ºæ¨¡èŒƒå¼ï¼Œä»¥å¹³è¡¡æ¨ç†æ•ˆç‡ä¸æ€§èƒ½è¡¨ç°ã€‚å¤šç»´åº¦çš„å®éªŒè¯„ä¼°è¡¨æ˜ï¼ŒPsychÄ“Chat åœ¨æä¾›å…±æƒ…æ´å¯ŸåŠ›å’Œå®æ–½å®‰å…¨æ§åˆ¶æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12392v1",
      "published_date": "2026-01-18 13:06:13 UTC",
      "updated_date": "2026-01-18 13:06:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:40:09.995224+00:00"
    },
    {
      "arxiv_id": "2601.12389v1",
      "title": "NADIR: Differential Attention Flow for Non-Autoregressive Transliteration in Indic Languages",
      "title_zh": "NADIRï¼šé¢å‘å°åº¦è¯­è¨€éè‡ªå›å½’éŸ³è¯‘çš„å·®åˆ†æ³¨æ„åŠ›æµ",
      "authors": [
        "Lakshya Tomar",
        "Vinayak Abrol",
        "Puneet Agarwal"
      ],
      "abstract": "In this work, we argue that not all sequence-to-sequence tasks require the strong inductive biases of autoregressive (AR) models. Tasks like multilingual transliteration, code refactoring, grammatical correction or text normalization often rely on local dependencies where the full modeling capacity of AR models can be overkill, creating a trade-off between their high accuracy and high inference latency. While non-autoregressive (NAR) models offer speed, they typically suffer from hallucinations and poor length control. To explore this trade-off, we focus on the multilingual transliteration task in Indic languages and introduce NADIR, a novel NAR architecture designed to strike a balance between speed and accuracy. NADIR integrates a Differential Transformer and a Mixture-of-Experts mechanism, enabling it to robustly model complex character mappings without sequential dependencies. NADIR achieves over a 13x speed-up compared to the state-of-the-art AR baseline. It maintains a competitive mean Character Error Rate of 15.78%, compared to 14.44% for the AR model and 21.88% for a standard NAR equivalent. Importantly, NADIR reduces Repetition errors by 49.53%, Substitution errors by 24.45%, Omission errors by 32.92%, and Insertion errors by 16.87%. This work provides a practical blueprint for building fast and reliable NAR systems, effectively bridging the gap between AR accuracy and the demands of real-time, large-scale deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å°åœ°è¯­ç³»(Indic languages)çš„å¤šè¯­è¨€éŸ³è¯‘ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åä¸ºNADIRçš„æ–°å‹éè‡ªå›å½’(Non-autoregressive, NAR)æ¶æ„ï¼Œæ—¨åœ¨è§£å†³è‡ªå›å½’(AR)æ¨¡å‹æ¨ç†å»¶è¿Ÿè¿‡é«˜çš„é—®é¢˜ã€‚NADIRé›†æˆäº†å¾®åˆ†å˜å‹å™¨(Differential Transformer)å’Œæ··åˆä¸“å®¶æœºåˆ¶(Mixture-of-Experts)ï¼Œèƒ½å¤Ÿåœ¨æ— é¡ºåºä¾èµ–çš„æƒ…å†µä¸‹ç¨³å¥å»ºæ¨¡å¤æ‚çš„å­—ç¬¦æ˜ å°„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNADIRåœ¨ä¿æŒç«äº‰æ€§å¹³å‡å­—ç¬¦é”™è¯¯ç‡(Character Error Rate)ä¸º15.78%çš„åŒæ—¶ï¼Œå®ç°äº†æ¯”æœ€å…ˆè¿›ARåŸºçº¿æ¨¡å‹å¿«13å€ä»¥ä¸Šçš„æ¨ç†åŠ é€Ÿã€‚ç›¸æ¯”æ ‡å‡†NARæ¨¡å‹ï¼ŒNADIRæ˜¾è‘—é™ä½äº†é‡å¤(Repetition)ã€æ›¿æ¢(Substitution)ã€é—æ¼(Omission)å’Œæ’å…¥(Insertion)ç­‰å„ç±»é”™è¯¯ç‡ï¼Œå…¶ä¸­é‡å¤é”™è¯¯é™ä½äº†49.53%ã€‚è¯¥ç ”ç©¶ä¸ºæ„å»ºå¿«é€Ÿã€å¯é ä¸”é€‚ç”¨äºå¤§è§„æ¨¡å®æ—¶éƒ¨ç½²çš„NARç³»ç»Ÿæä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆï¼ŒæˆåŠŸå¼¥åˆäº†NARæ¨¡å‹ä¸ARæ¨¡å‹ä¹‹é—´çš„ç²¾åº¦å·®è·ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI 2026)",
      "pdf_url": "https://arxiv.org/pdf/2601.12389v1",
      "published_date": "2026-01-18 12:56:47 UTC",
      "updated_date": "2026-01-18 12:56:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:40:25.182489+00:00"
    },
    {
      "arxiv_id": "2601.12374v1",
      "title": "A Scalable Entity-Based Framework for Auditing Bias in LLMs",
      "title_zh": "ä¸€ç§ç”¨äºå®¡è®¡ LLMs åè§çš„å¯æ‰©å±•å®ä½“æ¡†æ¶",
      "authors": [
        "Akram Elbouanani",
        "Aboubacar Tuo",
        "Adrian Popescu"
      ],
      "abstract": "Existing approaches to bias evaluation in large language models (LLMs) trade ecological validity for statistical control, relying on artificial prompts that poorly reflect real-world use, or on naturalistic tasks that lack scale and rigor. We introduce a scalable bias-auditing framework using named entities as probes to measure structural disparities in model behavior. We show that synthetic data reliably reproduces bias patterns observed in natural text, enabling large-scale analysis. Using this approach, we conduct the largest bias audit to date, comprising 1.9 billion data points across multiple entity types, tasks, languages, models, and prompting strategies. Our results reveal systematic biases: models penalize right-wing politicians, favor left-wing politicians, prefer Western and wealthy nations over the Global South, favor Western companies, and penalize firms in the defense and pharmaceutical sectors. While instruction tuning reduces bias, increasing model scale amplifies it, and prompting in Chinese or Russian does not attenuate Western-aligned preferences. These results indicate that LLMs should undergo rigorous auditing before deployment in high-stakes applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºå‘½åå®ä½“(named entities)çš„å¯æ‰©å±•åå·®å®¡è®¡æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å¤§è¯­è¨€æ¨¡å‹(LLMs)åå·®è¯„ä¼°ä¸­ç”Ÿæ€æ•ˆåº¦ä¸ç»Ÿè®¡æ§åˆ¶ä¹‹é—´çš„æƒè¡¡é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å‘½åå®ä½“ä½œä¸ºæ¢æµ‹å™¨æ¥è¡¡é‡æ¨¡å‹è¡Œä¸ºçš„ç»“æ„æ€§å·®å¼‚ï¼Œå¹¶è¯æ˜äº†åˆæˆæ•°æ®èƒ½å¯é åœ°å¤ç°è‡ªç„¶æ–‡æœ¬ä¸­çš„åå·®æ¨¡å¼ï¼Œä»è€Œæ”¯æŒå¤§è§„æ¨¡åˆ†æã€‚ç ”ç©¶è€…åˆ©ç”¨è¯¥æ–¹æ³•è¿›è¡Œäº†è¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§çš„åå·®å®¡è®¡ï¼Œæ¶µç›–äº†è·¨å®ä½“ç±»å‹ã€ä»»åŠ¡ã€è¯­è¨€å’Œæ¨¡å‹ç­‰ç»´åº¦çš„19äº¿ä¸ªæ•°æ®ç‚¹ã€‚å®éªŒç»“æœæ­ç¤ºäº†æ¨¡å‹ä¸­å­˜åœ¨çš„ç³»ç»Ÿæ€§åå·®ï¼Œè¡¨ç°ä¸ºåå¥½å·¦ç¿¼æ”¿æ²»äººç‰©ã€è¥¿æ–¹åŠå¯Œè£•å›½å®¶å’Œè¥¿æ–¹å…¬å¸ï¼Œè€Œå¯¹å³ç¿¼æ”¿æ²»äººç‰©ã€å…¨çƒå—æ–¹(Global South)å›½å®¶ä»¥åŠå›½é˜²å’Œåˆ¶è¯è¡Œä¸šæŒæœ‰åè§ã€‚ç ”ç©¶å‘ç°è™½ç„¶æŒ‡ä»¤å¾®è°ƒ(instruction tuning)èƒ½å‡å°‘åå·®ï¼Œä½†æ¨¡å‹è§„æ¨¡(scale)çš„å¢åŠ åè€Œä¼šæ”¾å¤§åå·®ï¼Œä¸”ä½¿ç”¨ä¸­æ–‡æˆ–ä¿„æ–‡æç¤º(prompting)å¹¶ä¸èƒ½å‡å¼±æ¨¡å‹å¯¹è¥¿æ–¹ç«‹åœºçš„åå‘ã€‚è¿™äº›å‘ç°å¼ºè°ƒäº†åœ¨LLMséƒ¨ç½²äºé«˜é£é™©åº”ç”¨ä¹‹å‰è¿›è¡Œä¸¥æ ¼å®¡è®¡çš„å¿…è¦æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12374v1",
      "published_date": "2026-01-18 12:07:31 UTC",
      "updated_date": "2026-01-18 12:07:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:40:32.644391+00:00"
    },
    {
      "arxiv_id": "2601.12358v1",
      "title": "From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles",
      "title_zh": "ä»æç¤ºè¯åˆ°è·¯é¢ï¼šåŸºäº LMMs çš„è‡ªåŠ¨é©¾é©¶æ±½è½¦æ™ºèƒ½ä½“è¡Œä¸ºæ ‘ç”Ÿæˆæ¡†æ¶",
      "authors": [
        "Omar Y. Goba",
        "Ahmed Y. Gado",
        "Catherine M. Elias",
        "Ahmed Hussein"
      ],
      "abstract": "Autonomous vehicles (AVs) require adaptive behavior planners to navigate unpredictable, real-world environments safely. Traditional behavior trees (BTs) offer structured decision logic but are inherently static and demand labor-intensive manual tuning, limiting their applicability at SAE Level 5 autonomy. This paper presents an agentic framework that leverages large language models (LLMs) and multi-modal vision models (LVMs) to generate and adapt BTs on the fly. A specialized Descriptor agent applies chain-of-symbols prompting to assess scene criticality, a Planner agent constructs high-level sub-goals via in-context learning, and a Generator agent synthesizes executable BT sub-trees in XML format. Integrated into a CARLA+Nav2 simulation, our system triggers only upon baseline BT failure, demonstrating successful navigation around unexpected obstacles (e.g., street blockage) with no human intervention. Compared to a static BT baseline, this approach is a proof-of-concept that extends to diverse driving scenarios.",
      "tldr_zh": "è‡ªåŠ¨é©¾é©¶æ±½è½¦(Autonomous Vehicles, AVs)éœ€è¦é€‚åº”æ€§çš„è¡Œä¸ºè§„åˆ’å™¨æ¥åº”å¯¹ä¸å¯é¢„æµ‹çš„çœŸå®ç¯å¢ƒï¼Œä½†ä¼ ç»Ÿçš„è¡Œä¸ºæ ‘(Behavior Trees, BTs)é€šå¸¸æ˜¯é™æ€çš„ä¸”éœ€è¦ç¹é‡äººå·¥è°ƒä¼˜ï¼Œé™åˆ¶äº†å…¶åœ¨SAE Level 5è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨ã€‚è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œå¤šæ¨¡æ€è§†è§‰æ¨¡å‹(LVMs)çš„æ™ºèƒ½ä½“æ¡†æ¶ï¼Œç”¨äºå®æ—¶ç”Ÿæˆå’Œè°ƒæ•´è¡Œä¸ºæ ‘ã€‚æ¡†æ¶åŒ…å«ä¸€ä¸ªåº”ç”¨ç¬¦å·é“¾æç¤º(Chain-of-symbols)è¯„ä¼°åœºæ™¯å…³é”®æ€§çš„Descriptoræ™ºèƒ½ä½“ã€ä¸€ä¸ªé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ (In-context learning)æ„å»ºé«˜å±‚å­ç›®æ ‡çš„Planneræ™ºèƒ½ä½“ï¼Œä»¥åŠä¸€ä¸ªåˆæˆXMLæ ¼å¼å¯æ‰§è¡Œè¡Œä¸ºæ ‘å­æ ‘çš„Generatoræ™ºèƒ½ä½“ã€‚è¯¥ç³»ç»Ÿè¢«é›†æˆåˆ°CARLA+Nav2æ¨¡æ‹Ÿä¸­ï¼Œä»…åœ¨åŸºå‡†è¡Œä¸ºæ ‘å¤±æ•ˆæ—¶è§¦å‘ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨æ— éœ€äººå·¥å¹²é¢„çš„æƒ…å†µä¸‹ï¼ŒæˆåŠŸç»•è¿‡è¡—é“å°é”ç­‰æ„å¤–éšœç¢ã€‚ä¸é™æ€è¡Œä¸ºæ ‘åŸºå‡†ç›¸æ¯”ï¼Œè¯¥ç ”ç©¶ä¸ºå¤„ç†å¤šæ ·åŒ–é©¾é©¶åœºæ™¯æä¾›äº†ä¸€ç§æœ‰æ•ˆçš„æ¦‚å¿µéªŒè¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12358v1",
      "published_date": "2026-01-18 11:32:29 UTC",
      "updated_date": "2026-01-18 11:32:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:40:29.712014+00:00"
    },
    {
      "arxiv_id": "2601.12357v1",
      "title": "SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence",
      "title_zh": "SimpleMatchï¼šä¸€ç§ç®€å•ä¸”å¼ºå¤§çš„è¯­ä¹‰å¯¹åº”åŸºå‡†",
      "authors": [
        "Hailing Jin",
        "Huiying Li"
      ],
      "abstract": "Recent advances in semantic correspondence have been largely driven by the use of pre-trained large-scale models. However, a limitation of these approaches is their dependence on high-resolution input images to achieve optimal performance, which results in considerable computational overhead. In this work, we address a fundamental limitation in current methods: the irreversible fusion of adjacent keypoint features caused by deep downsampling operations. This issue is triggered when semantically distinct keypoints fall within the same downsampled receptive field (e.g., 16x16 patches). To address this issue, we present SimpleMatch, a simple yet effective framework for semantic correspondence that delivers strong performance even at low resolutions. We propose a lightweight upsample decoder that progressively recovers spatial detail by upsampling deep features to 1/4 resolution, and a multi-scale supervised loss that ensures the upsampled features retain discriminative features across different spatial scales. In addition, we introduce sparse matching and window-based localization to optimize training memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller than current SOTA methods), SimpleMatch achieves superior performance with 84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a practical and efficient baseline for future research in semantic correspondence. Code is available at: https://github.com/hailong23-jin/SimpleMatch.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¯­ä¹‰å¯¹åº” (Semantic Correspondence) é¢†åŸŸä¸­å¤§æ¨¡å‹è¿‡åº¦ä¾èµ–é«˜åˆ†è¾¨ç‡è¾“å…¥å¯¼è‡´è®¡ç®—å¼€é”€å·¨å¤§çš„é—®é¢˜ï¼ŒæŒ‡å‡ºäº†æ·±å±‚ä¸‹é‡‡æ ·æ“ä½œå¼•èµ·çš„ç›¸é‚»å…³é”®ç‚¹ç‰¹å¾ä¸å¯é€†èåˆç¼ºé™·ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†åä¸º SimpleMatch çš„ç®€æ´é«˜æ•ˆæ¡†æ¶ï¼Œé€šè¿‡å¼•å…¥è½»é‡çº§ä¸Šé‡‡æ ·è§£ç å™¨ (Upsample Decoder) å°†æ·±å±‚ç‰¹å¾æ¢å¤è‡³ 1/4 åˆ†è¾¨ç‡ï¼Œä»è€Œæœ‰æ•ˆè¿˜åŸç©ºé—´ç»†èŠ‚ã€‚åŒæ—¶ï¼Œç ”ç©¶é‡‡ç”¨å¤šå°ºåº¦ç›‘ç£æŸå¤± (Multi-scale Supervised Loss) ç¡®ä¿ç‰¹å¾åœ¨ä¸åŒå°ºåº¦ä¸Šçš„åˆ¤åˆ«åŠ›ï¼Œå¹¶ç»“åˆç¨€ç–åŒ¹é… (Sparse Matching) å’ŒåŸºäºçª—å£çš„å®šä½æŠ€æœ¯å°†è®­ç»ƒå†…å­˜å ç”¨é™ä½äº† 51%ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSimpleMatch åœ¨ä»…ä¸º 252x252 çš„è¾ƒä½åˆ†è¾¨ç‡ä¸‹ï¼Œäº SPair-71k åŸºå‡†æµ‹è¯•ä¸­å–å¾—äº† 84.1% çš„ PCK@0.1ï¼Œæ€§èƒ½è¶…è¶Šäº†å½“å‰æœ€å…ˆè¿›æ–¹æ³•ã€‚è¯¥å·¥ä½œä¸ºæœªæ¥çš„è¯­ä¹‰å¯¹åº”ç ”ç©¶æä¾›äº†ä¸€ä¸ªå…¼å…·å®ç”¨æ€§ä¸é«˜æ•ˆæ€§çš„å¼ºåŠ›åŸºå‡† (Baseline)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12357v1",
      "published_date": "2026-01-18 11:31:46 UTC",
      "updated_date": "2026-01-18 11:31:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:40:33.493184+00:00"
    },
    {
      "arxiv_id": "2601.12349v1",
      "title": "Zero-Permission Manipulation: Can We Trust Large Multimodal Model Powered GUI Agents?",
      "title_zh": "é›¶æƒé™æ“çºµï¼šåŸºäºå¤šæ¨¡æ€å¤§æ¨¡å‹çš„GUIæ™ºèƒ½ä½“æ˜¯å¦å€¼å¾—ä¿¡ä»»ï¼Ÿ",
      "authors": [
        "Yi Qian",
        "Kunwei Qian",
        "Xingbang He",
        "Ligeng Chen",
        "Jikang Zhang",
        "Tiantai Zhang",
        "Haiyang Wei",
        "Linzhang Wang",
        "Hao Wu",
        "Bing Mao"
      ],
      "abstract": "Large multimodal model powered GUI agents are emerging as high-privilege operators on mobile platforms, entrusted with perceiving screen content and injecting inputs. However, their design operates under the implicit assumption of Visual Atomicity: that the UI state remains invariant between observation and action. We demonstrate that this assumption is fundamentally invalid in Android, creating a critical attack surface.\n  We present Action Rebinding, a novel attack that allows a seemingly-benign app with zero dangerous permissions to rebind an agent's execution. By exploiting the inevitable observation-to-action gap inherent in the agent's reasoning pipeline, the attacker triggers foreground transitions to rebind the agent's planned action toward the target app. We weaponize the agent's task-recovery logic and Android's UI state preservation to orchestrate programmable, multi-step attack chains. Furthermore, we introduce an Intent Alignment Strategy (IAS) that manipulates the agent's reasoning process to rationalize UI states, enabling it to bypass verification gates (e.g., confirmation dialogs) that would otherwise be rejected.\n  We evaluate Action Rebinding Attacks on six widely-used Android GUI agents across 15 tasks. Our results demonstrate a 100% success rate for atomic action rebinding and the ability to reliably orchestrate multi-step attack chains. With IAS, the success rate in bypassing verification gates increases (from 0% to up to 100%). Notably, the attacker application requires no sensitive permissions and contains no privileged API calls, achieving a 0% detection rate across malware scanners (e.g., VirusTotal). Our findings reveal a fundamental architectural flaw in current agent-OS integration and provide critical insights for the secure design of future agent systems. To access experimental logs and demonstration videos, please contact yi_qian@smail.nju.edu.cn.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§å‹å¤šæ¨¡æ€æ¨¡å‹(LMM)çš„GUIæ™ºèƒ½ä½“åœ¨Androidå¹³å°ä¸Šçš„å®‰å…¨æ€§ï¼ŒæŒ‡å‡ºå…¶è®¾è®¡ä¸­éšå«çš„è§†è§‰åŸå­æ€§(Visual Atomicity)å‡è®¾åœ¨å®é™…ä¸­å¹¶ä¸æˆç«‹ï¼Œä»è€Œäº§ç”Ÿäº†å…³é”®çš„æ”»å‡»é¢ã€‚ä½œè€…æå‡ºäº†ä¸€ç§åä¸ºAction Rebindingçš„æ–°å‹æ”»å‡»æ‰‹æ®µï¼Œå…è®¸ä¸€ä¸ªé›¶æƒé™çš„åº”ç”¨åˆ©ç”¨æ™ºèƒ½ä½“æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨çš„è§‚å¯Ÿä¸åŠ¨ä½œé—´éš™(observation-to-action gap)ï¼Œé€šè¿‡è§¦å‘å‰å°åˆ‡æ¢å°†æ™ºèƒ½ä½“çš„é¢„å®šåŠ¨ä½œé‡æ–°ç»‘å®šè‡³æ”»å‡»ç›®æ ‡åº”ç”¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†æ„å›¾å¯¹é½ç­–ç•¥(Intent Alignment Strategy, IAS)æ¥æ“çºµæ™ºèƒ½ä½“çš„æ¨ç†é€»è¾‘ï¼Œä½¿å…¶å¯¹å¼‚å¸¸UIçŠ¶æ€è¿›è¡Œåˆç†åŒ–è§£é‡Šï¼Œä»è€Œç»•è¿‡æœ¬åº”æ‹¦æˆªåŠ¨ä½œçš„éªŒè¯å¯¹è¯æ¡†ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ”»å‡»åœ¨15é¡¹ä»»åŠ¡ä¸­é’ˆå¯¹6ç§ä¸»æµæ™ºèƒ½ä½“å‡è¾¾åˆ°äº†100%çš„åŸå­åŠ¨ä½œé‡ç»‘å®šæˆåŠŸç‡ï¼Œä¸”åœ¨æ¶æ„è½¯ä»¶æ‰«æå™¨ä¸­çš„æ£€å‡ºç‡ä¸ºé›¶ã€‚è¯¥å‘ç°æ­ç¤ºäº†å½“å‰æ™ºèƒ½ä½“ä¸æ“ä½œç³»ç»Ÿé›†æˆæ–¹æ¡ˆä¸­å­˜åœ¨çš„æ ¹æœ¬æ€§æ¶æ„ç¼ºé™·ï¼Œä¸ºæœªæ¥å®‰å…¨æ™ºèƒ½ä½“ç³»ç»Ÿçš„è®¾è®¡æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12349v1",
      "published_date": "2026-01-18 10:54:54 UTC",
      "updated_date": "2026-01-18 10:54:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:40:29.098607+00:00"
    },
    {
      "arxiv_id": "2601.12343v1",
      "title": "How Well Do LLMs Predict Human Behavior? A Measure of their Pretrained Knowledge",
      "title_zh": "LLM é¢„æµ‹äººç±»è¡Œä¸ºçš„æ•ˆåŠ›å¦‚ä½•ï¼Ÿä¸€ç§å¯¹å…¶é¢„è®­ç»ƒçŸ¥è¯†çš„åº¦é‡æŒ‡æ ‡",
      "authors": [
        "Wayne Gao",
        "Sukjin Han",
        "Annie Liang"
      ],
      "abstract": "Large language models (LLMs) are increasingly used to predict human behavior. We propose a measure for evaluating how much knowledge a pretrained LLM brings to such a prediction: its equivalent sample size, defined as the amount of task-specific data needed to match the predictive accuracy of the LLM. We estimate this measure by comparing the prediction error of a fixed LLM in a given domain to that of flexible machine learning models trained on increasing samples of domain-specific data. We further provide a statistical inference procedure by developing a new asymptotic theory for cross-validated prediction error. Finally, we apply this method to the Panel Study of Income Dynamics. We find that LLMs encode considerable predictive information for some economic variables but much less for others, suggesting that their value as substitutes for domain-specific data differs markedly across settings.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨é¢„æµ‹äººç±»è¡Œä¸ºæ–¹é¢çš„è¡¨ç°ï¼Œå¹¶æå‡ºäº†ä¸€ç§è¡¡é‡å…¶é¢„è®­ç»ƒçŸ¥è¯†é‡çš„æŒ‡æ ‡ï¼šç­‰æ•ˆæ ·æœ¬é‡(equivalent sample size)ã€‚è¯¥æŒ‡æ ‡è¢«å®šä¹‰ä¸ºè¾¾åˆ°ä¸LLMç›¸åŒçš„é¢„æµ‹å‡†ç¡®æ€§æ‰€éœ€ä»»åŠ¡ç‰¹å®šæ•°æ®(task-specific data)çš„æ•°é‡ã€‚ç ”ç©¶äººå‘˜é€šè¿‡æ¯”è¾ƒå›ºå®šLLMä¸åœ¨é€’å¢æ ·æœ¬é‡ä¸‹è®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹è¯¯å·®æ¥ä¼°ç®—è¿™ä¸€æŒ‡æ ‡ï¼Œå¹¶å¼€å‘äº†é’ˆå¯¹äº¤å‰éªŒè¯é¢„æµ‹è¯¯å·®çš„æ–°æ¸è¿‘ç†è®º(asymptotic theory)ä»¥å®ç°ç»Ÿè®¡æ¨æ–­ã€‚åœ¨å¯¹æ”¶å…¥åŠ¨æ€é¢æ¿ç ”ç©¶(Panel Study of Income Dynamics)çš„åº”ç”¨ä¸­ï¼Œå‘ç°LLMså¯¹æŸäº›ç»æµå˜é‡å…·æœ‰æ˜¾è‘—çš„é¢„æµ‹èƒ½åŠ›ï¼Œä½†åœ¨å…¶ä»–å˜é‡ä¸Šè¡¨ç°è¾ƒå¼±ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒLLMsä½œä¸ºé¢†åŸŸç‰¹å®šæ•°æ®æ›¿ä»£æ–¹æ¡ˆçš„ä»·å€¼åœ¨ä¸åŒåœºæ™¯ä¸‹å­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚",
      "categories": [
        "econ.EM",
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "econ.EM",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12343v1",
      "published_date": "2026-01-18 10:28:54 UTC",
      "updated_date": "2026-01-18 10:28:54 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:40:47.169479+00:00"
    },
    {
      "arxiv_id": "2601.12341v1",
      "title": "Time-Continuous Modeling for Temporal Affective Pattern Recognition in LLMs",
      "title_zh": "LLMs æ—¶åºæƒ…æ„Ÿæ¨¡å¼è¯†åˆ«çš„è¿ç»­æ—¶é—´å»ºæ¨¡",
      "authors": [
        "Rezky Kam",
        "Coddy N. Siswanto"
      ],
      "abstract": "This paper introduces a dataset and conceptual framework for LLMs to mimic real world emotional dynamics through time and in-context learning leveraging physics-informed neural network, opening a possibility for interpretable dialogue modeling.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°çš„æ•°æ®é›†å’Œæ¦‚å¿µæ¡†æ¶ï¼Œæ—¨åœ¨ä½¿å¤§è¯­è¨€æ¨¡å‹(LLMs)èƒ½å¤Ÿæ¨¡æ‹Ÿç°å®ä¸–ç•Œä¸­éšæ—¶é—´å˜åŒ–çš„æƒ…æ„ŸåŠ¨æ€ã€‚é€šè¿‡åˆ©ç”¨ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ(Physics-Informed Neural Network)å’Œä¸Šä¸‹æ–‡å­¦ä¹ (In-context Learning)ï¼Œè¯¥ç ”ç©¶å®ç°äº†é’ˆå¯¹æƒ…æ„Ÿæ¨¡å¼è¯†åˆ«çš„æ—¶é—´è¿ç»­å»ºæ¨¡(Time-Continuous Modeling)ã€‚è¿™ä¸€æ¡†æ¶ä¸ºå®ç°å¯è§£é‡Šçš„å¯¹è¯å»ºæ¨¡(Interpretable Dialogue Modeling)æä¾›äº†æ–°çš„å¯èƒ½æ€§ï¼Œä½¿å¾—LLMsåœ¨å¤„ç†åŠ¨æ€æƒ…æ„Ÿäº¤äº’æ—¶æ›´å…·é€»è¾‘é€æ˜åº¦ã€‚ç ”ç©¶æ ¸å¿ƒåœ¨äºæ•æ‰æƒ…æ„Ÿåœ¨æ—¶é—´è½´ä¸Šçš„æ¼”å˜è§„å¾‹ï¼Œå¹¶å°†å…¶æ•´åˆè¿›ç°æœ‰çš„æ¨¡å‹å­¦ä¹ èŒƒå¼ä¸­ã€‚è¯¥æˆæœå¯¹äºæå‡å¯¹è¯ç³»ç»Ÿçš„æƒ…æ„Ÿè®¤çŸ¥èƒ½åŠ›å’Œå†³ç­–è¿‡ç¨‹çš„å¯è§£é‡Šæ€§å…·æœ‰é‡è¦çš„å­¦æœ¯ä»·å€¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.ET",
        "cs.HC",
        "eess.SY"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12341v1",
      "published_date": "2026-01-18 10:16:26 UTC",
      "updated_date": "2026-01-18 10:16:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:40:54.094086+00:00"
    },
    {
      "arxiv_id": "2601.12338v1",
      "title": "Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM Pipeline for Issue Extraction and Business Recommendations",
      "title_zh": "åŸºäº LoRA ä¸“å®¶æ··åˆä»è¯„è®ºä¸­æå–å¯æ“ä½œå»ºè®®ï¼šä¸€ç§ç”¨äºé—®é¢˜æå–ä¸å•†ä¸šå»ºè®®çš„åŒ LLM æµæ°´çº¿",
      "authors": [
        "Kartikey Singh Bhandari",
        "Manav Ganesh",
        "Yashwant Viswanathan",
        "Archit Agrawal",
        "Dhruv Kumar",
        "Pratik Narang"
      ],
      "abstract": "Customer reviews contain detailed, domain specific signals about service failures and user expectations, but converting this unstructured feedback into actionable business decisions remains difficult. We study review-to-action generation: producing concrete, implementable recommendations grounded in review text. We propose a modular two-LLM framework in which an Issue model extracts salient issues and assigns coarse themes, and an Advice model generates targeted operational fixes conditioned on the extracted issue representation. To enable specialization without expensive full fine-tuning, we adapt the Advice model using a mixture of LoRA experts strategy: multiple low-rank adapters are trained and a lightweight gating mechanism performs token-level expert mixing at inference, combining complementary expertise across issue types. We construct synthetic review-issue-advice triples from Yelp reviews (airlines and restaurants) to supervise training, and evaluate recommendations using an eight dimension operational rubric spanning actionability, specificity, feasibility, expected impact, novelty, non-redundancy, bias, and clarity. Across both domains, our approach consistently outperforms prompting-only and single-adapter baselines, yielding higher actionability and specificity while retaining favorable efficiency-quality trade-offs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä»éç»“æ„åŒ–å®¢æˆ·è¯„è®ºä¸­æå–å¯æ“ä½œå•†ä¸šå†³ç­–çš„éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäº Mixture of LoRA Experts çš„åŒ LLM æµæ°´çº¿æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç”±è´Ÿè´£è¯†åˆ«æ ¸å¿ƒé—®é¢˜å¹¶åˆ’åˆ†ç²—ç²’åº¦ä¸»é¢˜çš„ Issue æ¨¡å‹ï¼Œä»¥åŠæ ¹æ®æ‰€æå–é—®é¢˜ç”Ÿæˆé’ˆå¯¹æ€§æ“ä½œå»ºè®®çš„ Advice model ç»„æˆã€‚ä¸ºå®ç°ä½æˆæœ¬çš„ä¸“ä¸šåŒ–é€‚é…ï¼Œç ”ç©¶è€…å¯¹ Advice model é‡‡ç”¨äº†æ··åˆ LoRA ä¸“å®¶ç­–ç•¥ï¼Œé€šè¿‡è½»é‡çº§é—¨æ§æœºåˆ¶åœ¨æ¨ç†é˜¶æ®µè¿›è¡Œ Token çº§åˆ«çš„ä¸“å®¶æ··åˆï¼Œä»è€Œèåˆå¤„ç†ä¸åŒé—®é¢˜ç±»å‹çš„äº’è¡¥çŸ¥è¯†ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ Yelp è¯„è®ºæ•°æ®æ„å»ºäº†ç›‘ç£è®­ç»ƒé›†ï¼Œå¹¶ä»å¯æ“ä½œæ€§ã€ç‰¹å¼‚æ€§å’Œå¯è¡Œæ€§ç­‰å…«ä¸ªç»´åº¦è¿›è¡Œè¯„ä¼°ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨èˆªç©ºå…¬å¸å’Œé¤å…é¢†åŸŸçš„è¡¨ç°ä¸€è‡´ä¼˜äºä»…ä½¿ç”¨ Prompting-only å’Œå•é€‚é…å™¨åŸºçº¿ï¼Œåœ¨æ˜¾è‘—æå‡å»ºè®®è´¨é‡çš„åŒæ—¶å®ç°äº†è‰¯å¥½çš„æ•ˆç‡æƒè¡¡ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12338v1",
      "published_date": "2026-01-18 10:11:29 UTC",
      "updated_date": "2026-01-18 10:11:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:41:07.293062+00:00"
    },
    {
      "arxiv_id": "2601.12331v1",
      "title": "Efficient Privacy-Preserving Retrieval Augmented Generation with Distance-Preserving Encryption",
      "title_zh": "åŸºäºè·ç¦»ä¿æŒåŠ å¯†çš„é«˜æ•ˆéšç§ä¿æŠ¤æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Huanyi Ye",
        "Jiale Guo",
        "Ziyao Liu",
        "Kwok-Yan Lam"
      ],
      "abstract": "RAG has emerged as a key technique for enhancing response quality of LLMs without high computational cost. In traditional architectures, RAG services are provided by a single entity that hosts the dataset within a trusted local environment. However, individuals or small organizations often lack the resources to maintain data storage servers, leading them to rely on outsourced cloud storage. This dependence on untrusted third-party services introduces privacy risks. Embedding-based retrieval mechanisms, commonly used in RAG systems, are vulnerable to privacy leakage such as vector-to-text reconstruction attacks and structural leakage via vector analysis. Several privacy-preserving RAG techniques have been proposed but most existing approaches rely on partially homomorphic encryption, which incurs substantial computational overhead. To address these challenges, we propose an efficient privacy-preserving RAG framework (ppRAG) tailored for untrusted cloud environments that defends against vector-to-text attack, vector analysis, and query analysis. We propose Conditional Approximate Distance-Comparison-Preserving Symmetric Encryption (CAPRISE) that encrypts embeddings while still allowing the cloud to compute similarity between an encrypted query and the encrypted database embeddings. CAPRISE preserves only the relative distance ordering between the encrypted query and each encrypted database embedding, without exposing inter-database distances, thereby enhancing both privacy and efficiency. To mitigate query analysis, we introduce DP by perturbing the query embedding prior to encryption, preventing the cloud from inferring sensitive patterns. Experimental results show that ppRAG achieves efficient processing throughput, high retrieval accuracy, strong privacy guarantees, making it a practical solution for resource-constrained users seeking secure cloud-augmented LLMs.",
      "tldr_zh": "é’ˆå¯¹æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)åœ¨ä¸å¯ä¿¡äº‘ç¯å¢ƒä¸­é¢ä¸´çš„éšç§æ³„éœ²é£é™©ï¼Œå¦‚vector-to-texté‡å»ºæ”»å‡»å’Œå‘é‡åˆ†æï¼Œè¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„éšç§ä¿æŠ¤RAGæ¡†æ¶ppRAGã€‚è¯¥æ¡†æ¶å¼•å…¥äº†æ¡ä»¶è¿‘ä¼¼è·ç¦»æ¯”è¾ƒä¿æŒå¯¹ç§°åŠ å¯†æŠ€æœ¯(CAPRISE)ï¼Œå…è®¸äº‘ç«¯åœ¨åŠ å¯†çŠ¶æ€ä¸‹è®¡ç®—æŸ¥è¯¢ä¸æ•°æ®åº“åµŒå…¥ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚CAPRISEé€šè¿‡ä»…ä¿ç•™æŸ¥è¯¢ä¸å„ä¸ªåµŒå…¥ä¹‹é—´çš„ç›¸å¯¹è·ç¦»é¡ºåºï¼Œé¿å…äº†æ•°æ®åº“å†…éƒ¨è·ç¦»çš„æš´éœ²ï¼Œæ˜¾è‘—æå‡äº†éšç§ä¿æŠ¤èƒ½åŠ›å’Œè¿ç®—æ•ˆç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜é€šè¿‡åœ¨åŠ å¯†å‰å¯¹æŸ¥è¯¢åµŒå…¥å¼•å…¥å·®åˆ†éšç§(DP)æ‰°åŠ¨æ¥é˜²å¾¡æŸ¥è¯¢åˆ†ææ”»å‡»ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒppRAGåœ¨ä¿æŒé«˜æ£€ç´¢å‡†ç¡®ç‡å’Œå¼ºéšç§ä¿è¯çš„åŒæ—¶ï¼Œå®ç°äº†æé«˜çš„å¤„ç†ååé‡ã€‚è¯¥æ–¹æ¡ˆä¸ºèµ„æºå—é™ç”¨æˆ·å®‰å…¨åˆ©ç”¨äº‘ç«¯å¤§è¯­è¨€æ¨¡å‹(LLMs)æä¾›äº†ä¸€ç§å®ç”¨çš„è§£å†³é€”å¾„ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12331v1",
      "published_date": "2026-01-18 09:29:50 UTC",
      "updated_date": "2026-01-18 09:29:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:41:24.338696+00:00"
    },
    {
      "arxiv_id": "2601.12330v1",
      "title": "IceWatch: Forecasting Glacial Lake Outburst Floods (GLOFs) using Multimodal Deep Learning",
      "title_zh": "IceWatchï¼šåŸºäºå¤šæ¨¡æ€æ·±åº¦å­¦ä¹ çš„å†°æ¹–æºƒå†³æ´ªæ°´ (GLOFs) é¢„æµ‹",
      "authors": [
        "Zuha Fatima",
        "Muhammad Anser Sohaib",
        "Muhammad Talha",
        "Ayesha Kanwal",
        "Sidra Sultana",
        "Nazia Perwaiz"
      ],
      "abstract": "Glacial Lake Outburst Floods (GLOFs) pose a serious threat in high mountain regions. They are hazardous to communities, infrastructure, and ecosystems further downstream. The classical methods of GLOF detection and prediction have so far mainly relied on hydrological modeling, threshold-based lake monitoring, and manual satellite image analysis. These approaches suffer from several drawbacks: slow updates, reliance on manual labor, and losses in accuracy when clouds interfere and/or lack on-site data. To tackle these challenges, we present IceWatch: a novel deep learning framework for GLOF prediction that incorporates both spatial and temporal perspectives. The vision component, RiskFlow, of IceWatch deals with Sentinel-2 multispectral satellite imagery using a CNN-based classifier and predicts GLOF events based on the spatial patterns of snow, ice, and meltwater. Its tabular counterpart confirms this prediction by considering physical dynamics. TerraFlow models glacier velocity from NASA ITS_LIVE time series while TempFlow forecasts near-surface temperature from MODIS LST records; both are trained on long-term observational archives and integrated via harmonized preprocessing and synchronization to enable multimodal, physics-informed GLOF prediction. Both together provide cross-validation, which will improve the reliability and interpretability of GLOF detection. This system ensures strong predictive performance, rapid data processing for real-time use, and robustness to noise and missing information. IceWatch paves the way for automatic, scalable GLOF warning systems. It also holds potential for integration with diverse sensor inputs and global glacier monitoring activities.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†IceWatchï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„å¤šæ¨¡æ€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨åˆ©ç”¨ç©ºé—´å’Œæ—¶é—´è§†è§’é¢„æµ‹å†°å·æ¹–æºƒå†³æ´ªæ°´(GLOFs)ã€‚è¯¥ç³»ç»ŸåŒ…å«è§†è§‰ç»„ä»¶RiskFlowï¼Œé€šè¿‡CNNåˆ†ç±»å™¨åˆ†æSentinel-2å¤šå…‰è°±å«æ˜Ÿå›¾åƒä¸­é›ªã€å†°å’Œèæ°´çš„ç©ºé—´æ¨¡å¼ã€‚ä¸ºè¾…åŠ©ç‰©ç†åŠ¨åŠ›å­¦åˆ¤æ–­ï¼Œæ¡†æ¶é›†æˆäº†å¤„ç†NASA ITS_LIVEæ—¶é—´åºåˆ—å†°å·æµé€Ÿçš„TerraFlowï¼Œä»¥åŠé¢„æµ‹MODIS LSTè®°å½•ä¸­è¿‘åœ°è¡¨æ¸©åº¦çš„TempFlowã€‚é€šè¿‡åè°ƒçš„é¢„å¤„ç†å’ŒåŒæ­¥æŠ€æœ¯ï¼Œè¯¥å¤šæ¨¡æ€ç³»ç»Ÿå®ç°äº†ç‰©ç†æ„ŸçŸ¥(physics-informed)çš„äº¤å‰éªŒè¯ï¼Œæ˜¾è‘—æé«˜äº†é¢„è­¦çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ã€‚å®éªŒè¡¨æ˜IceWatchåœ¨å¤„ç†é€Ÿåº¦ã€è‡ªåŠ¨åŒ–ç¨‹åº¦ä»¥åŠå¯¹æ•°æ®å™ªå£°æˆ–ç¼ºå¤±çš„é²æ£’æ€§æ–¹é¢ä¼˜äºä¼ ç»Ÿæ°´æ–‡å­¦æ¨¡å‹å’Œæ‰‹åŠ¨åˆ†ææ–¹æ³•ã€‚è¿™ä¸€ç ”ç©¶ä¸ºæ„å»ºå¯æ‰©å±•çš„å…¨çƒå†°å·ç›‘æµ‹å’Œè‡ªåŠ¨é¢„è­¦ç³»ç»Ÿå¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12330v1",
      "published_date": "2026-01-18 09:29:40 UTC",
      "updated_date": "2026-01-18 09:29:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:41:11.895591+00:00"
    },
    {
      "arxiv_id": "2601.12327v1",
      "title": "The Expert Validation Framework (EVF): Enabling Domain Expert Control in AI Engineering",
      "title_zh": "ä¸“å®¶éªŒè¯æ¡†æ¶ (EVF)ï¼šå®ç°äººå·¥æ™ºèƒ½å·¥ç¨‹ä¸­çš„é¢†åŸŸä¸“å®¶ç®¡æ§",
      "authors": [
        "Lucas Gren",
        "Felix Dobslaw"
      ],
      "abstract": "Generative AI (GenAI) systems promise to transform knowledge work by automating a range of tasks, yet their deployment in enterprise settings remains hindered by the lack of systematic quality assurance mechanisms. We present an Expert Validation Framework that places domain experts at the center of building software with GenAI components, enabling them to maintain authoritative control over system behavior through structured specification, testing, validation, and continuous monitoring processes. Our framework addresses the critical gap between AI capabilities and organizational trust by establishing a rigorous, expert-driven methodology for ensuring quality across diverse GenAI applications. Through a four-stage implementation process encompassing specification, system creation, validation, and production monitoring, the framework enables organizations to leverage GenAI capabilities while maintaining expert oversight and quality standards.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Expert Validation Framework (EVF)ï¼Œæ—¨åœ¨è§£å†³ç”Ÿæˆå¼äººå·¥æ™ºèƒ½ (GenAI) ç³»ç»Ÿåœ¨ä¼ä¸šéƒ¨ç½²ä¸­å› ç¼ºä¹ç³»ç»Ÿæ€§è´¨é‡ä¿è¯æœºåˆ¶è€Œé¢ä¸´çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶å°†é¢†åŸŸä¸“å®¶ (domain experts) ç½®äº AI å·¥ç¨‹çš„æ ¸å¿ƒï¼Œä½¿ä»–ä»¬èƒ½å¤Ÿé€šè¿‡ç»“æ„åŒ–çš„è§„èŒƒã€æµ‹è¯•ã€éªŒè¯å’ŒæŒç»­ç›‘æ§è¿‡ç¨‹ï¼Œå¯¹ç³»ç»Ÿè¡Œä¸ºä¿æŒæƒå¨æ§åˆ¶ã€‚EVF é€šè¿‡å»ºç«‹ä¸¥è°¨çš„ä¸“å®¶é©±åŠ¨æ–¹æ³•è®ºï¼Œå¼¥åˆäº† AI èƒ½åŠ›ä¸ç»„ç»‡ä¿¡ä»»ä¹‹é—´çš„å…³é”®é¸¿æ²Ÿï¼Œç¡®ä¿äº†ä¸åŒ GenAI åº”ç”¨åœºæ™¯ä¸‹çš„è´¨é‡æ ‡å‡†ã€‚è¯¥æ¡†æ¶çš„å…·ä½“å®æ–½åŒ…å«è§„èŒƒ (specification)ã€ç³»ç»Ÿåˆ›å»º (system creation)ã€éªŒè¯ (validation) å’Œç”Ÿäº§ç›‘æ§ (production monitoring) å››ä¸ªé˜¶æ®µã€‚é€šè¿‡è¿™ä¸€æµç¨‹ï¼Œç»„ç»‡èƒ½å¤Ÿåœ¨å……åˆ†åˆ©ç”¨ GenAI èƒ½åŠ›çš„åŒæ—¶ï¼Œç»´æŒä¸“å®¶ç›‘ç£å’Œé«˜æ ‡å‡†çš„è´¨é‡æ§åˆ¶ï¼Œä¸ºå¯é çš„ AI å·¥ç¨‹åŒ–æä¾›äº†ç³»ç»Ÿæ€§æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12327v1",
      "published_date": "2026-01-18 09:20:21 UTC",
      "updated_date": "2026-01-18 09:20:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:41:13.500955+00:00"
    },
    {
      "arxiv_id": "2601.12323v1",
      "title": "MARO: Learning Stronger Reasoning from Social Interaction",
      "title_zh": "MAROï¼šé€šè¿‡ç¤¾äº¤äº’åŠ¨å­¦ä¹ æ›´å¼ºçš„æ¨ç†èƒ½åŠ›",
      "authors": [
        "Yin Cai",
        "Zhouhong Gu",
        "Juntao Zhang",
        "Ping Chen"
      ],
      "abstract": "Humans face countless scenarios that require reasoning and judgment in daily life. However, existing large language model training methods primarily allow models to learn from existing textual content or solve predetermined problems, lacking experience in real scenarios involving interaction, negotiation, and competition with others. To address this, this paper proposes Multi-Agent Reward Optimization (MARO), a method that enables large language models (LLMs) to acquire stronger reasoning abilities by learning and practicing in multi-agent social environments. Specifically, MARO first addresses the sparse learning signal problem by decomposing final success or failure outcomes into each specific behavior during the interaction process; second, it handles the uneven role distribution problem by balancing the training sample weights of different roles; finally, it addresses environmental instability issues by directly evaluating the utility of each behavior. Experimental results demonstrate that MARO not only achieves significant improvements in social reasoning capabilities, but also that the abilities acquired through social simulation learning can effectively transfer to other tasks such as mathematical reasoning and instruction following. This reveals the tremendous potential of multi-agent social learning in enhancing the general reasoning capabilities of LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºMARO (Multi-Agent Reward Optimization) çš„æ–¹æ³•ï¼Œé€šè¿‡å¤šæ™ºèƒ½ä½“ç¤¾äº¤ç¯å¢ƒä¸­çš„å­¦ä¹ ä¸å®è·µæ¥å¢å¼ºå¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æ¨ç†èƒ½åŠ›ã€‚é’ˆå¯¹ç°æœ‰æ¨¡å‹åœ¨äº¤äº’ã€è°ˆåˆ¤å’Œç«äº‰ç­‰çœŸå®åœºæ™¯ä¸­ç»éªŒä¸è¶³çš„é—®é¢˜ï¼ŒMARO è§£å†³äº†å­¦ä¹ ä¿¡å·ç¨€ç–ã€è§’è‰²åˆ†å¸ƒä¸å‡ä»¥åŠç¯å¢ƒä¸ç¨³å®šç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•å°†æœ€ç»ˆèƒœè´Ÿåˆ†è§£ä¸ºå…·ä½“çš„äº¤äº’è¡Œä¸ºï¼Œå¹¶ç»“åˆè§’è‰²æƒé‡å¹³è¡¡ä¸è¡Œä¸ºæ•ˆç”¨ (Utility) è¯„ä¼°æ¥ä¼˜åŒ–æ¨¡å‹è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMARO æ˜¾è‘—æå‡äº†æ¨¡å‹çš„ç¤¾äº¤æ¨ç† (Social Reasoning) èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¿™ç§é€šè¿‡ç¤¾äº¤æ¨¡æ‹Ÿè·å¾—çš„æ¨ç†èƒ½åŠ›å¯æœ‰æ•ˆè¿ç§»è‡³æ•°å­¦æ¨ç† (Mathematical Reasoning) å’ŒæŒ‡ä»¤éµå¾ª (Instruction Following) ç­‰ä»»åŠ¡ä¸­ã€‚è¯¥ç ”ç©¶å……åˆ†å±•ç¤ºäº†å¤šæ™ºèƒ½ä½“ç¤¾äº¤å­¦ä¹ åœ¨æå‡å¤§è¯­è¨€æ¨¡å‹é€šç”¨æ¨ç†èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12323v1",
      "published_date": "2026-01-18 09:10:08 UTC",
      "updated_date": "2026-01-18 09:10:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:41:29.948677+00:00"
    },
    {
      "arxiv_id": "2601.12318v1",
      "title": "Beyond Human Annotation: Recent Advances in Data Generation Methods for Document Intelligence",
      "title_zh": "è¶…è¶Šäººå·¥æ ‡æ³¨ï¼šæ–‡æ¡£æ™ºèƒ½æ•°æ®ç”Ÿæˆæ–¹æ³•çš„æœ€æ–°è¿›å±•",
      "authors": [
        "Dehao Ying",
        "Fengchang Yu",
        "Haihua Chen",
        "Changjiang Jiang",
        "Yurong Li",
        "Wei Lu"
      ],
      "abstract": "The advancement of Document Intelligence (DI) demands large-scale, high-quality training data, yet manual annotation remains a critical bottleneck. While data generation methods are evolving rapidly, existing surveys are constrained by fragmented focuses on single modalities or specific tasks, lacking a unified perspective aligned with real-world workflows. To fill this gap, this survey establishes the first comprehensive technical map for data generation in DI. Data generation is redefined as supervisory signal production, and a novel taxonomy is introduced based on the \"availability of data and labels.\" This framework organizes methodologies into four resource-centric paradigms: Data Augmentation, Data Generation from Scratch, Automated Data Annotation, and Self-Supervised Signal Construction. Furthermore, a multi-level evaluation framework is established to integrate intrinsic quality and extrinsic utility, compiling performance gains across diverse DI benchmarks. Guided by this unified structure, the methodological landscape is dissected to reveal critical challenges such as fidelity gaps and frontiers including co-evolutionary ecosystems. Ultimately, by systematizing this fragmented field, data generation is positioned as the central engine for next-generation DI.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–‡æ¡£æ™ºèƒ½ (Document Intelligence, DI) é¢†åŸŸä¸­äººå·¥æ ‡æ³¨æˆæœ¬é«˜æ˜‚çš„ç“¶é¢ˆï¼Œé¦–æ¬¡ç³»ç»Ÿæ€§åœ°æ„å»ºäº†æ•°æ®ç”ŸæˆæŠ€æœ¯çš„å…¨æ™¯æŠ€æœ¯å›¾è°±ã€‚è®ºæ–‡å°†æ•°æ®ç”Ÿæˆé‡æ–°å®šä¹‰ä¸ºç›‘ç£ä¿¡å·ç”Ÿæˆ (supervisory signal production)ï¼Œå¹¶åŸºäºæ•°æ®å’Œæ ‡ç­¾çš„å¯ç”¨æ€§æå‡ºäº†å…¨æ–°çš„åˆ†ç±»ä½“ç³»ï¼Œæ¶µç›–äº†æ•°æ®å¢å¼º (Data Augmentation)ã€ä»é›¶ç”Ÿæˆ (Data Generation from Scratch)ã€è‡ªåŠ¨æ•°æ®æ ‡æ³¨ (Automated Data Annotation) ä»¥åŠè‡ªç›‘ç£ä¿¡å·æ„å»º (Self-Supervised Signal Construction) å››å¤§èµ„æºä¸­å¿ƒèŒƒå¼ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å»ºç«‹äº†ä¸€ä¸ªæ•´åˆå†…åœ¨è´¨é‡ä¸å¤–åœ¨æ•ˆç”¨çš„å¤šå±‚çº§è¯„ä¼°æ¡†æ¶ï¼Œå¹¶æ±‡æ€»äº†å…¶åœ¨å¤šç§ DI åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½å¢ç›Šã€‚é€šè¿‡å¯¹æ–¹æ³•è®ºæ™¯è§‚çš„æ·±å…¥å‰–æï¼Œæœ¬æ–‡æ­ç¤ºäº†ä¿çœŸåº¦å·®è· (fidelity gaps) ç­‰å…³é”®æŒ‘æˆ˜ï¼Œå¹¶æŒ‡å‡ºäº†ååŒè¿›åŒ–ç”Ÿæ€ç³»ç»Ÿ (co-evolutionary ecosystems) ç­‰å‰æ²¿å‘å±•æ–¹å‘ã€‚è¯¥ç»¼è¿°é€šè¿‡ç³»ç»ŸåŒ–æ•´åˆè¿™ä¸€ç¢ç‰‡åŒ–é¢†åŸŸï¼Œç¡®ç«‹äº†æ•°æ®ç”Ÿæˆä½œä¸ºä¸‹ä¸€ä»£æ–‡æ¡£æ™ºèƒ½æ ¸å¿ƒé©±åŠ¨å¼•æ“çš„åœ°ä½ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12318v1",
      "published_date": "2026-01-18 09:01:18 UTC",
      "updated_date": "2026-01-18 09:01:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:42:17.614805+00:00"
    },
    {
      "arxiv_id": "2601.12317v1",
      "title": "Explanova: Automatically Discover Data Insights in N \\times M Table via XAI Combined LLM Workflow",
      "title_zh": "Explanovaï¼šåŸºäº XAI ç»“åˆ LLM å·¥ä½œæµçš„ N \\times M è¡¨æ ¼æ•°æ®æ´å¯Ÿè‡ªåŠ¨åŒ–å‘ç°",
      "authors": [
        "Yiming Huang"
      ],
      "abstract": "Automation in data analysis has been a long-time pursuit. Current agentic LLM shows a promising solution towards it. Like DeepAnalyze, DataSage, and Datawise. They are all powerful agentic frameworks for automatic fine-grained analysis and are powered by LLM-based agentic tool calling ability. However, what about powered by a preset AutoML-like workflow? If we traverse all possible exploration, like Xn itself`s statistics, Xn1-Xn2 relationships, Xn to all other, and finally explain? Our Explanova is such an attempt: Cheaper due to a Local Small LLM.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Explanovaï¼Œè¿™æ˜¯ä¸€ç§é€šè¿‡XAIç»“åˆLLMå·¥ä½œæµåœ¨N \\times Mè¡¨æ ¼ä¸­è‡ªåŠ¨å‘ç°æ•°æ®æ´å¯Ÿçš„ç³»ç»Ÿã€‚é’ˆå¯¹å½“å‰DeepAnalyzeã€DataSageå’ŒDatawiseç­‰æ™ºèƒ½ä½“æ¡†æ¶é«˜åº¦ä¾èµ–LLMå·¥å…·è°ƒç”¨èƒ½åŠ›çš„é—®é¢˜ï¼ŒExplanovaé‡‡ç”¨äº†ä¸€ç§é¢„è®¾çš„ç±»AutoMLå·¥ä½œæµã€‚è¯¥å·¥ä½œæµé€šè¿‡éå†å•å˜é‡ç»Ÿè®¡ã€å˜é‡é—´å…³ç³»ä»¥åŠå…¨å±€å…³è”ï¼Œæœ€ç»ˆç”Ÿæˆæ•°æ®è§£é‡Šã€‚ç”±äºé‡‡ç”¨äº†Local Small LLMï¼ŒExplanovaåœ¨æä¾›è‡ªåŠ¨åŒ–ç»†ç²’åº¦åˆ†æçš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†è¿è¡Œæˆæœ¬ã€‚è¯¥ç ”ç©¶ä¸ºè‡ªåŠ¨åŒ–æ•°æ®åˆ†ææä¾›äº†ä¸€ç§ç»“æ„åŒ–ä¸”æ›´å…·ç»æµæ€§çš„æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12317v1",
      "published_date": "2026-01-18 09:00:03 UTC",
      "updated_date": "2026-01-18 09:00:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:41:47.902449+00:00"
    },
    {
      "arxiv_id": "2601.12316v1",
      "title": "GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE Transformer",
      "title_zh": "GazeFormer-MoEï¼šåŸºäº CLIP ä¸ MoE Transformer çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§†çº¿ä¼°è®¡",
      "authors": [
        "Xinyuan Zhao",
        "Xianrui Chen",
        "Ahmad Chaddad"
      ],
      "abstract": "We present a semantics modulated, multi scale Transformer for 3D gaze estimation. Our model conditions CLIP global features with learnable prototype banks (illumination, head pose, background, direction), fuses these prototype-enriched global vectors with CLIP patch tokens and high-resolution CNN tokens in a unified attention space, and replaces several FFN blocks with routed/shared Mixture of Experts to increase conditional capacity. Evaluated on MPIIFaceGaze, EYEDIAP, Gaze360 and ETH-XGaze, our model achieves new state of the art angular errors of 2.49Â°, 3.22Â°, 10.16Â°, and 1.44Â°, demonstrating up to a 64% relative improvement over previously reported results. ablations attribute gains to prototype conditioning, cross scale fusion, MoE and hyperparameter. Our code is publicly available at https://github. com/AIPMLab/Gazeformer.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†GazeFormer-MoEï¼Œä¸€ç§è¯­ä¹‰è°ƒæ§çš„å¤šå°ºåº¦Transformeræ¶æ„ï¼Œæ—¨åœ¨ä¼˜åŒ–3Dæ³¨è§†ä¼°è®¡(3D gaze estimation)ä»»åŠ¡ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯å­¦ä¹ çš„åŸå‹åº“(prototype banks)å¯¹å…‰ç…§ã€å¤´éƒ¨å§¿åŠ¿ã€èƒŒæ™¯åŠæ–¹å‘ç­‰CLIPå…¨å±€ç‰¹å¾è¿›è¡Œæ¡ä»¶åŒ–å¤„ç†ï¼Œå¹¶åœ¨ç»Ÿä¸€çš„æ³¨æ„ç©ºé—´(attention space)ä¸­å®ç°CLIP patch tokensä¸é«˜åˆ†è¾¨ç‡CNN tokensçš„è·¨å°ºåº¦èåˆã€‚é€šè¿‡å°†éƒ¨åˆ†FFNæ¨¡å—æ›¿æ¢ä¸ºä¸“å®¶æ··åˆ(Mixture of Experts, MoE)ç»“æ„ï¼Œæ¨¡å‹æ˜¾è‘—æå‡äº†æ¡ä»¶å»ºæ¨¡èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒGazeFormer-MoEåœ¨MPIIFaceGazeã€EYEDIAPã€Gaze360åŠETH-XGazeæ•°æ®é›†ä¸Šå‡åˆ·æ–°äº†SOTAè®°å½•ï¼Œè§’åº¦è¯¯å·®æœ€ä½è¾¾åˆ°1.44Â°ï¼Œè¾ƒæ—¢æœ‰æ¨¡å‹å®ç°äº†æœ€é«˜64%çš„ç›¸å¯¹æ€§èƒ½æå‡ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†åŸå‹è°ƒèŠ‚ã€è·¨å°ºåº¦èåˆä»¥åŠMoEæ¶æ„åœ¨å¤æ‚ç¯å¢ƒæ³¨è§†ä¼°è®¡ä¸­çš„æ ¸å¿ƒè´¡çŒ®ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "accepted at ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12316v1",
      "published_date": "2026-01-18 08:54:02 UTC",
      "updated_date": "2026-01-18 08:54:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:42:36.995830+00:00"
    },
    {
      "arxiv_id": "2601.14305v1",
      "title": "An Optimized Decision Tree-Based Framework for Explainable IoT Anomaly Detection",
      "title_zh": "é¢å‘å¯è§£é‡Šç‰©è”ç½‘å¼‚å¸¸æ£€æµ‹çš„ä¼˜åŒ–å†³ç­–æ ‘æ¡†æ¶",
      "authors": [
        "Ashikuzzaman",
        "Md. Shawkat Hossain",
        "Jubayer Abdullah Joy",
        "Md Zahid Akon",
        "Md Manjur Ahmed",
        "Md. Naimul Islam"
      ],
      "abstract": "The increase in the number of Internet of Things (IoT) devices has tremendously increased the attack surface of cyber threats thus making a strong intrusion detection system (IDS) with a clear explanation of the process essential towards resource-constrained environments. Nevertheless, current IoT IDS systems are usually traded off with detection quality, model elucidability, and computational effectiveness, thus the deployment on IoT devices. The present paper counteracts these difficulties by suggesting an explainable AI (XAI) framework based on an optimized Decision Tree classifier with both local and global importance methods: SHAP values that estimate feature attribution using local explanations, and Morris sensitivity analysis that identifies the feature importance in a global view. The proposed system attains the state of art on the test performance with 99.91% accuracy, F1-score of 99.51% and Cohen Kappa of 0.9960 and high stability is confirmed by a cross validation mean accuracy of 98.93%. Efficiency is also enhanced in terms of computations to provide faster inferences compared to those that are generalized in ensemble models. SrcMac has shown as the most significant predictor in feature analyses according to SHAP and Morris methods. Compared to the previous work, our solution eliminates its major drawback lack because it allows us to apply it to edge devices and, therefore, achieve real-time processing, adhere to the new regulation of transparency in AI, and achieve high detection rates on attacks of dissimilar classes. This combination performance of high accuracy, explainability, and low computation make the framework useful and reliable as a resource-constrained IoT security problem in real environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºä¼˜åŒ– Decision Tree åˆ†ç±»å™¨çš„å¯è§£é‡Š AI (XAI) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³èµ„æºå—é™çš„ç‰©è”ç½‘ (IoT) ç¯å¢ƒä¸­å…¥ä¾µæ£€æµ‹ç³»ç»Ÿ (IDS) åœ¨æ£€æµ‹è´¨é‡ã€æ¨¡å‹å¯è§£é‡Šæ€§å’Œè®¡ç®—æ•ˆç‡ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ã€‚è¯¥æ¡†æ¶é›†æˆäº† SHAP å±€éƒ¨è§£é‡Šæ–¹æ³•å’Œ Morris æ•æ„Ÿæ€§åˆ†æå…¨å±€é‡è¦æ€§è¯†åˆ«æ–¹æ³•ï¼Œé€šè¿‡åˆ†æç‰¹å¾å±æ€§ç¡®ä¿æ¨¡å‹çš„é€æ˜åº¦ä¸å¯ä¿¡åº¦ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿåœ¨æµ‹è¯•é›†ä¸­è¾¾åˆ°äº† 99.91% çš„å‡†ç¡®ç‡å’Œ 99.51% çš„ F1-scoreï¼Œä¸”æ¨ç†é€Ÿåº¦æ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„é›†æˆæ¨¡å‹ (ensemble models)ã€‚ç‰¹å¾åˆ†æä¸€è‡´è¡¨æ˜ SrcMac æ˜¯æœ€é‡è¦çš„é¢„æµ‹æŒ‡æ ‡ï¼ŒéªŒè¯äº†è¯¥æ–¹æ³•åœ¨è¯†åˆ«å…³é”®æ”»å‡»ç‰¹å¾æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚å‡­å€Ÿé«˜ç²¾åº¦ã€ä½è®¡ç®—å¼€é”€ä»¥åŠç¬¦åˆ AI é€æ˜åº¦ç›‘ç®¡è¦æ±‚çš„ç‰¹æ€§ï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤Ÿæœ‰æ•ˆéƒ¨ç½²äºè¾¹ç¼˜è®¾å¤‡ (edge devices) ä»¥å®ç°å®æ—¶çš„å¼‚æ„æ”»å‡»æ£€æµ‹ï¼Œä¸º IoT å®‰å…¨ä¿éšœæä¾›äº†å®ç”¨çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "Acepted and Presented at IEEE 2nd International Conference on Computing, Applications and Systems (COMPAS 2025) , 23-24 October 2025, Kushtia, Bangladesh",
      "pdf_url": "https://arxiv.org/pdf/2601.14305v1",
      "published_date": "2026-01-18 08:48:53 UTC",
      "updated_date": "2026-01-18 08:48:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:43:13.812714+00:00"
    },
    {
      "arxiv_id": "2601.12310v1",
      "title": "Survival is the Only Reward: Sustainable Self-Training Through Environment-Mediated Selection",
      "title_zh": "ç”Ÿå­˜å³å”¯ä¸€å¥–åŠ±ï¼šåŸºäºç¯å¢ƒä»‹å¯¼é€‰æ‹©çš„å¯æŒç»­è‡ªè®­ç»ƒ",
      "authors": [
        "Jennifer Dodgson",
        "Alfath Daryl Alhajir",
        "Michael Joedhitya",
        "Akira Rafhael Janson Pattirane",
        "Surender Suresh Kumar",
        "Joseph Lim",
        "C. H. Peh",
        "Adith Ramdas",
        "Steven Zhang Zhexu"
      ],
      "abstract": "Self-training systems often degenerate due to the lack of an external criterion for judging data quality, leading to reward hacking and semantic drift. This paper provides a proof-of-concept system architecture for stable self-training under sparse external feedback and bounded memory, and empirically characterises its learning dynamics and failure modes.\n  We introduce a self-training architecture in which learning is mediated exclusively by environmental viability, rather than by reward, objective functions, or externally defined fitness criteria. Candidate behaviours are executed under real resource constraints, and only those whose environmental effects both persist and preserve the possibility of future interaction are propagated. The environment does not provide semantic feedback, dense rewards, or task-specific supervision; selection operates solely through differential survival of behaviours as world-altering events, making proxy optimisation impossible and rendering reward-hacking evolutionarily unstable.\n  Analysis of semantic dynamics shows that improvement arises primarily through the persistence of effective and repeatable strategies under a regime of consolidation and pruning, a paradigm we refer to as negative-space learning (NSL), and that models develop meta-learning strategies (such as deliberate experimental failure in order to elicit informative error messages) without explicit instruction. This work establishes that environment-grounded selection enables sustainable open-ended self-improvement, offering a viable path toward more robust and generalisable autonomous systems without reliance on human-curated data or complex reward shaping.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåŸºäºç¯å¢ƒè°ƒèŠ‚é€‰æ‹©(Environment-Mediated Selection)çš„å¯æŒç»­è‡ªè®­ç»ƒæ¶æ„ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿè‡ªè®­ç»ƒç³»ç»Ÿå› ç¼ºä¹å¤–éƒ¨è´¨é‡åˆ¤æ–­æ ‡å‡†è€Œå¯¼è‡´çš„å¥–åŠ±é»‘å®¢(reward hacking)å’Œè¯­ä¹‰æ¼‚ç§»(semantic drift)é—®é¢˜ã€‚è¯¥æ¡†æ¶ä¸ä¾èµ–å¥–åŠ±å‡½æ•°æˆ–é¢„å®šä¹‰çš„é€‚åº”åº¦æ ‡å‡†ï¼Œè€Œæ˜¯å°†å­¦ä¹ è¿‡ç¨‹å®Œå…¨ç”±ç¯å¢ƒç”Ÿå­˜æ€§(environmental viability)ä»‹å¯¼ï¼Œä»…ä¼ æ’­é‚£äº›åœ¨çœŸå®èµ„æºçº¦æŸä¸‹èƒ½ä¿ç•™æœªæ¥äº¤äº’å¯èƒ½æ€§çš„è¡Œä¸ºã€‚ç”±äºé€‰æ‹©ä»…é€šè¿‡è¡Œä¸ºä½œä¸ºâ€œæ”¹å˜ä¸–ç•Œäº‹ä»¶â€çš„å·®å¼‚åŒ–ç”Ÿå­˜æ¥å®ç°ï¼Œè¿™ä½¿å¾—ä»£ç†ä¼˜åŒ–åœ¨è¿›åŒ–ä¸Šå˜å¾—ä¸ç¨³å®šï¼Œä»æ ¹æœ¬ä¸ŠæŠ‘åˆ¶äº†å¥–åŠ±é»‘å®¢è¡Œä¸ºã€‚ç ”ç©¶åˆ†æè¡¨æ˜ï¼Œæ€§èƒ½æ”¹è¿›ä¸»è¦æºäºè´Ÿç©ºé—´å­¦ä¹ (negative-space learning, NSL)èŒƒå¼ï¼Œå³åœ¨å·©å›ºå’Œå‰ªææœºåˆ¶ä¸‹ä¿ç•™æœ‰æ•ˆä¸”å¯é‡å¤çš„ç­–ç•¥ã€‚å®éªŒå‘ç°æ¨¡å‹åœ¨æ— æ˜ç¡®æŒ‡ä»¤çš„æƒ…å†µä¸‹ï¼Œå‘å±•å‡ºäº†å¦‚â€œæ•…æ„å®éªŒå¤±è´¥ä»¥è·å–ä¿¡æ¯æ€§é”™è¯¯æ¶ˆæ¯â€ç­‰å…ƒå­¦ä¹ ç­–ç•¥ã€‚è¯¥å·¥ä½œè¯æ˜äº†åŸºäºç¯å¢ƒçš„é€‰æ‹©å¯å®ç°å¯æŒç»­çš„å¼€æ”¾å¼è‡ªæˆ‘æå‡ï¼Œä¸ºæ„å»ºä¸ä¾èµ–äººå·¥æ•°æ®æˆ–å¤æ‚å¥–åŠ±å¡‘é€ (reward shaping)çš„é²æ£’è‡ªä¸»ç³»ç»Ÿæä¾›äº†æ–°è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12310v1",
      "published_date": "2026-01-18 08:35:56 UTC",
      "updated_date": "2026-01-18 08:35:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:43:02.293071+00:00"
    },
    {
      "arxiv_id": "2601.12304v1",
      "title": "A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language Pre-training Models",
      "title_zh": "é¢å‘è§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹çš„ä¸¤é˜¶æ®µå…¨å±€å¤šæ ·åŒ–å¯¹æŠ—æ”»å‡»",
      "authors": [
        "Wutao Chen",
        "Huaqin Zou",
        "Chen Wan",
        "Lifeng Huang"
      ],
      "abstract": "Vision-language pre-training (VLP) models are vulnerable to adversarial examples, particularly in black-box scenarios. Existing multimodal attacks often suffer from limited perturbation diversity and unstable multi-stage pipelines. To address these challenges, we propose 2S-GDA, a two-stage globally-diverse attack framework. The proposed method first introduces textual perturbations through a globally-diverse strategy by combining candidate text expansion with globally-aware replacement. To enhance visual diversity, image-level perturbations are generated using multi-scale resizing and block-shuffle rotation. Extensive experiments on VLP models demonstrate that 2S-GDA consistently improves attack success rates over state-of-the-art methods, with gains of up to 11.17\\% in black-box settings. Our framework is modular and can be easily combined with existing methods to further enhance adversarial transferability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Vision-Language Pre-training (VLP) æ¨¡å‹åœ¨é»‘ç›’åœºæ™¯ä¸‹æ˜“å—å¯¹æŠ—æ ·æœ¬æ”»å‡»ä¸”ç°æœ‰æ–¹æ³•å­˜åœ¨æ‰°åŠ¨å¤šæ ·æ€§å—é™çš„é—®é¢˜ï¼Œæå‡ºäº†2S-GDAï¼Œå³ä¸€ç§ä¸¤é˜¶æ®µå…¨å±€å¤šæ ·æ€§æ”»å‡»æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆé€šè¿‡å€™é€‰æ–‡æœ¬æ‰©å±•ä¸å…¨å±€æ„ŸçŸ¥æ›¿æ¢ç›¸ç»“åˆçš„ç­–ç•¥å¼•å…¥æ–‡æœ¬æ‰°åŠ¨ï¼Œéšååˆ©ç”¨å¤šå°ºåº¦ç¼©æ”¾(multi-scale resizing)å’Œå—æ‰“ä¹±æ—‹è½¬(block-shuffle rotation)æŠ€æœ¯ç”Ÿæˆå›¾åƒçº§æ‰°åŠ¨ä»¥å¢å¼ºè§†è§‰å¤šæ ·æ€§ã€‚åœ¨VLPæ¨¡å‹ä¸Šçš„å¹¿æ³›å®éªŒè¯æ˜ï¼Œ2S-GDAåœ¨æ”»å‡»æˆåŠŸç‡ä¸ŠæŒç»­ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯åœ¨é»‘ç›’è®¾ç½®ä¸‹å®ç°äº†é«˜è¾¾11.17%çš„æå‡ã€‚è¯¥æ¡†æ¶é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œå¯ä»¥æ–¹ä¾¿åœ°ä¸ç°æœ‰å¯¹æŠ—æ”»å‡»æ‰‹æ®µç»“åˆï¼Œä»è€Œè¿›ä¸€æ­¥å¢å¼ºå¯¹æŠ—è¿ç§»æ€§(adversarial transferability)ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted to ICASSP 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12304v1",
      "published_date": "2026-01-18 08:05:33 UTC",
      "updated_date": "2026-01-18 08:05:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:43:04.493353+00:00"
    },
    {
      "arxiv_id": "2601.12294v1",
      "title": "ToolPRMBench: Evaluating and Advancing Process Reward Models for Tool-using Agents",
      "title_zh": "ToolPRMBenchï¼šè¯„ä¼°ä¸æ¨è¿›å·¥å…·ä½¿ç”¨æ™ºèƒ½ä½“çš„è¿‡ç¨‹å¥–åŠ±æ¨¡å‹",
      "authors": [
        "Dawei Li",
        "Yuguang Yao",
        "Zhen Tan",
        "Huan Liu",
        "Ruocheng Guo"
      ],
      "abstract": "Reward-guided search methods have demonstrated strong potential in enhancing tool-using agents by effectively guiding sampling and exploration over complex action spaces. As a core design, those search methods utilize process reward models (PRMs) to provide step-level rewards, enabling more fine-grained monitoring. However, there is a lack of systematic and reliable evaluation benchmarks for PRMs in tool-using settings. In this paper, we introduce ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs for tool-using agents. ToolPRMBench is built on top of several representative tool-using benchmarks and converts agent trajectories into step-level test cases. Each case contains the interaction history, a correct action, a plausible but incorrect alternative, and relevant tool metadata. We respectively utilize offline sampling to isolate local single-step errors and online sampling to capture realistic multi-step failures from full agent rollouts. A multi-LLM verification pipeline is proposed to reduce label noise and ensure data quality. We conduct extensive experiments across large language models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results reveal clear differences in PRM effectiveness and highlight the potential of specialized PRMs for tool-using. Code and data will be released at https://github.com/David-Li0406/ToolPRMBench.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥å…·ä½¿ç”¨æ™ºèƒ½ä½“(tool-using agents)ä¸­è¿‡ç¨‹å¥–åŠ±æ¨¡å‹(Process Reward Models, PRMs)ç¼ºä¹ç³»ç»Ÿè¯„ä¼°åŸºå‡†çš„é—®é¢˜ï¼Œæå‡ºäº†å¤§è§„æ¨¡è¯„ä¼°åŸºå‡† ToolPRMBenchã€‚è¯¥åŸºå‡†åŸºäºå¤šä¸ªä»£è¡¨æ€§å·¥å…·ä½¿ç”¨æ•°æ®é›†æ„å»ºï¼Œå°†æ™ºèƒ½ä½“è½¨è¿¹è½¬æ¢ä¸ºåŒ…å«äº¤äº’å†å²ã€æ­£ç¡®åŠ¨ä½œã€è¯¯å¯¼æ€§é”™è¯¯åŠ¨ä½œåŠå·¥å…·å…ƒæ•°æ®çš„æ­¥éª¤çº§æµ‹è¯•æ¡ˆä¾‹ã€‚ç ”ç©¶è€…é€šè¿‡ç¦»çº¿é‡‡æ ·(offline sampling)éš”ç¦»å±€éƒ¨å•æ­¥é”™è¯¯ï¼Œå¹¶é€šè¿‡åœ¨çº¿é‡‡æ ·(online sampling)æ•æ‰çœŸå®çš„å¤šæ­¥æ‰§è¡Œå¤±è´¥ï¼ŒåŒæ—¶åˆ©ç”¨å¤š LLM éªŒè¯æµæ°´çº¿ç¡®ä¿æ ‡æ³¨è´¨é‡ã€‚å®éªŒå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ã€é€šç”¨å‹ PRMs åŠå·¥å…·ä¸“ç”¨å‹ PRMs è¿›è¡Œäº†å¹¿æ³›è¯„ä¼°ï¼Œç»“æœæ­ç¤ºäº†å„æ¨¡å‹åœ¨æ•ˆèƒ½ä¸Šçš„æ˜¾è‘—å·®å¼‚ã€‚è¯¥åŸºå‡†çš„æå‡ºä¸ä»…ä¸ºè¯„ä¼°å·¥å…·ä½¿ç”¨åœºæ™¯ä¸‹çš„ PRMs æä¾›äº†å¯é æ ‡å‡†ï¼Œä¹Ÿçªæ˜¾äº†å¼€å‘å·¥å…·ä¸“ç”¨å‹å¥–åŠ±æ¨¡å‹åœ¨æå‡æ™ºèƒ½ä½“å†³ç­–èƒ½åŠ›æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.AI",
      "comment": "under review",
      "pdf_url": "https://arxiv.org/pdf/2601.12294v1",
      "published_date": "2026-01-18 07:48:36 UTC",
      "updated_date": "2026-01-18 07:48:36 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:43:09.593364+00:00"
    },
    {
      "arxiv_id": "2601.12288v1",
      "title": "TimeGMM: Single-Pass Probabilistic Forecasting via Adaptive Gaussian Mixture Models with Reversible Normalization",
      "title_zh": "TimeGMMï¼šåŸºäºå¯é€†å½’ä¸€åŒ–è‡ªé€‚åº”é«˜æ–¯æ··åˆæ¨¡å‹çš„å•é˜¶æ®µæ¦‚ç‡é¢„æµ‹",
      "authors": [
        "Lei Liu",
        "Tengyuan Liu",
        "Hongwei Zhao",
        "Jiahui Huang",
        "Ruibo Guo",
        "Bin Li"
      ],
      "abstract": "Probabilistic time series forecasting is crucial for quantifying future uncertainty, with significant applications in fields such as energy and finance. However, existing methods often rely on computationally expensive sampling or restrictive parametric assumptions to characterize future distributions, which limits predictive performance and introduces distributional mismatch. To address these challenges, this paper presents TimeGMM, a novel probabilistic forecasting framework based on Gaussian Mixture Models (GMM) that captures complex future distributions in a single forward pass. A key component is GMM-adapted Reversible Instance Normalization (GRIN), a novel module designed to dynamically adapt to temporal-probabilistic distribution shifts. The framework integrates a dedicated Temporal Encoder (TE-Module) with a Conditional Temporal-Probabilistic Decoder (CTPD-Module) to jointly capture temporal dependencies and mixture distribution parameters. Extensive experiments demonstrate that TimeGMM consistently outperforms state-of-the-art methods, achieving maximum improvements of 22.48\\% in CRPS and 21.23\\% in NMAE.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TimeGMMï¼Œä¸€ç§æ–°å‹çš„æ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨æ•æ‰æœªæ¥åˆ†å¸ƒæ—¶é¢ä¸´çš„è®¡ç®—å¼€é”€å¤§å’Œå‚æ•°å‡è®¾å—é™ç­‰æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶åŸºäº Gaussian Mixture Models (GMM)ï¼Œå®ç°äº†åœ¨å•æ¬¡å‰å‘ä¼ é€’ï¼ˆsingle forward passï¼‰ä¸­å¯¹å¤æ‚æœªæ¥åˆ†å¸ƒçš„å»ºæ¨¡ã€‚å…¶æ ¸å¿ƒç»„ä»¶ GMM-adapted Reversible Instance Normalization (GRIN) èƒ½å¤ŸåŠ¨æ€é€‚åº”æ—¶é—´-æ¦‚ç‡åˆ†å¸ƒçš„åç§»ï¼Œå¹¶é…åˆ Temporal Encoder (TE-Module) ä¸ Conditional Temporal-Probabilistic Decoder (CTPD-Module) ååŒæå–æ—¶é—´ä¾èµ–åŠåˆ†å¸ƒå‚æ•°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒTimeGMM çš„æ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œåœ¨ CRPS å’Œ NMAE æŒ‡æ ‡ä¸Šåˆ†åˆ«å–å¾—äº†æœ€é«˜ 22.48% å’Œ 21.23% çš„æå‡ï¼Œä¸ºé«˜æ•ˆç‡ã€é«˜ç²¾åº¦çš„æ¦‚ç‡é¢„æµ‹æä¾›äº†æ–°æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12288v1",
      "published_date": "2026-01-18 07:02:13 UTC",
      "updated_date": "2026-01-18 07:02:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:43:07.693976+00:00"
    },
    {
      "arxiv_id": "2601.12286v1",
      "title": "Conversational Context Classification: A Representation Engineering Approach",
      "title_zh": "å¯¹è¯ä¸Šä¸‹æ–‡åˆ†ç±»ï¼šä¸€ç§è¡¨å¾å·¥ç¨‹æ–¹æ³•",
      "authors": [
        "Jonathan Pan"
      ],
      "abstract": "The increasing prevalence of Large Language Models (LLMs) demands effective safeguards for their operation, particularly concerning their tendency to generate out-of-context responses. A key challenge is accurately detecting when LLMs stray from expected conversational norms, manifesting as topic shifts, factual inaccuracies, or outright hallucinations. Traditional anomaly detection struggles to directly apply within contextual semantics. This paper outlines our experiment in exploring the use of Representation Engineering (RepE) and One-Class Support Vector Machine (OCSVM) to identify subspaces within the internal states of LLMs that represent a specific context. By training OCSVM on in-context examples, we establish a robust boundary within the LLM's hidden state latent space. We evaluate out study with two open source LLMs - Llama and Qwen models in specific contextual domain. Our approach entailed identifying the optimal layers within the LLM's internal state subspaces that strongly associates with the context of interest. Our evaluation results showed promising results in identifying the subspace for a specific context. Aside from being useful in detecting in or out of context conversation threads, this research work contributes to the study of better interpreting LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡è¡¨å¾å·¥ç¨‹ï¼ˆRepresentation Engineering, RepEï¼‰åº”å¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆè„±ç¦»è¯­å¢ƒï¼ˆout-of-contextï¼‰å›å¤çš„é—®é¢˜ï¼Œä¾‹å¦‚å¹»è§‰å’Œè¯é¢˜åç§»ã€‚ç”±äºä¼ ç»Ÿçš„å¼‚å¸¸æ£€æµ‹éš¾ä»¥ç›´æ¥åº”ç”¨äºå¤æ‚çš„ä¸Šä¸‹æ–‡è¯­ä¹‰ï¼Œä½œè€…æå‡ºç»“åˆRepEä¸ä¸€ç±»æ”¯æŒå‘é‡æœºï¼ˆOne-Class Support Vector Machine, OCSVMï¼‰ï¼Œåœ¨LLMçš„å†…éƒ¨éšè—çŠ¶æ€ä¸­è¯†åˆ«ä»£è¡¨ç‰¹å®šè¯­å¢ƒçš„å­ç©ºé—´ã€‚é€šè¿‡åœ¨ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­è®­ç»ƒOCSVMï¼Œç ”ç©¶äººå‘˜åœ¨æ½œåœ¨ç©ºé—´å†…å»ºç«‹äº†ç¨³å¥çš„åˆ†ç±»è¾¹ç•Œï¼Œå¹¶é’ˆå¯¹Llamaå’ŒQwenç­‰å¼€æºæ¨¡å‹ç¡®å®šäº†ä¸è¯­å¢ƒå…³è”æœ€å¼ºçš„æœ€ä¼˜éšè—å±‚ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ£€æµ‹å¯¹è¯æ˜¯å¦ç¬¦åˆé¢„æœŸè¯­å¢ƒæ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚æ­¤é¡¹å·¥ä½œä¸ä»…æå‡äº†å¯¹è¯ä¸Šä¸‹æ–‡åˆ†ç±»çš„å‡†ç¡®æ€§ï¼Œä¹Ÿä¸ºæ·±å…¥ç†è§£å’Œå¢å¼ºLLMsçš„å¯è§£é‡Šæ€§ï¼ˆinterpretabilityï¼‰æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12286v1",
      "published_date": "2026-01-18 06:47:35 UTC",
      "updated_date": "2026-01-18 06:47:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:44:22.323925+00:00"
    },
    {
      "arxiv_id": "2601.12282v1",
      "title": "CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human Brain Using Contrastive Language Image Pre-Training",
      "title_zh": "CytoCLIPï¼šåˆ©ç”¨å¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒå­¦ä¹ å‘è‚²ä¸­äººè„‘çš„ç»†èƒæ„ç­‘ç‰¹å¾",
      "authors": [
        "Pralaypati Ta",
        "Sriram Venkatesaperumal",
        "Keerthi Ram",
        "Mohanasankar Sivaprakasam"
      ],
      "abstract": "The functions of different regions of the human brain are closely linked to their distinct cytoarchitecture, which is defined by the spatial arrangement and morphology of the cells. Identifying brain regions by their cytoarchitecture enables various scientific analyses of the brain. However, delineating these areas manually in brain histological sections is time-consuming and requires specialized knowledge. An automated approach is necessary to minimize the effort needed from human experts. To address this, we propose CytoCLIP, a suite of vision-language models derived from pre-trained Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint visual-text representations of brain cytoarchitecture. CytoCLIP comprises two model variants: one is trained using low-resolution whole-region images to understand the overall cytoarchitectural pattern of an area, and the other is trained on high-resolution image tiles for detailed cellular-level representation. The training dataset is created from NISSL-stained histological sections of developing fetal brains of different gestational weeks. It includes 86 distinct regions for low-resolution images and 384 brain regions for high-resolution tiles. We evaluate the model's understanding of the cytoarchitecture and generalization ability using region classification and cross-modal retrieval tasks. Multiple experiments are performed under various data setups, including data from samples of different ages and sectioning planes. Experimental results demonstrate that CytoCLIP outperforms existing methods. It achieves an F1 score of 0.87 for whole-region classification and 0.91 for high-resolution image tile classification.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CytoCLIPï¼Œè¿™æ˜¯ä¸€å¥—åŸºäºå¯¹æ¯”è¯­è¨€-å›¾åƒé¢„è®­ç»ƒ(Contrastive Language-Image Pre-Training, CLIP)æ¡†æ¶çš„è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨å­¦ä¹ äººç±»å‘è‚²æœŸå¤§è„‘çš„ç»†èƒæ„ç­‘(cytoarchitecture)ç‰¹å¾ã€‚CytoCLIPåŒ…å«ä¸¤ä¸ªæ¨¡å‹å˜ä½“ï¼šä¸€ä¸ªåˆ©ç”¨ä½åˆ†è¾¨ç‡çš„å…¨åŒºåŸŸå›¾åƒå­¦ä¹ æ•´ä½“æ„ç­‘æ¨¡å¼ï¼Œå¦ä¸€ä¸ªåˆ™åŸºäºé«˜åˆ†è¾¨ç‡çš„å›¾åƒåˆ‡ç‰‡(high-resolution image tiles)è·å–è¯¦ç»†çš„ç»†èƒçº§è¡¨å¾ã€‚è¯¥æ¨¡å‹ä½¿ç”¨ä¸åŒå­•å‘¨èƒå„¿å¤§è„‘çš„NISSLæŸ“è‰²ç»„ç»‡åˆ‡ç‰‡è¿›è¡Œè®­ç»ƒï¼Œæ¶µç›–äº†ä»ä½åˆ†è¾¨ç‡çš„86ä¸ªåŒºåŸŸåˆ°é«˜åˆ†è¾¨ç‡çš„384ä¸ªè„‘åŒºã€‚ç ”ç©¶é€šè¿‡åŒºåŸŸåˆ†ç±»å’Œè·¨æ¨¡æ€æ£€ç´¢ä»»åŠ¡éªŒè¯äº†æ¨¡å‹çš„ç†è§£èƒ½åŠ›ä¸æ³›åŒ–æ€§èƒ½ï¼Œå®éªŒç»“æœè¡¨æ˜CytoCLIPåœ¨è¡¨ç°ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ã€‚å…¶åœ¨å…¨åŒºåŸŸåˆ†ç±»å’Œé«˜åˆ†è¾¨ç‡å›¾åƒåˆ‡ç‰‡åˆ†ç±»ä¸­åˆ†åˆ«å–å¾—äº†0.87å’Œ0.91çš„F1åˆ†æ•°ï¼Œä¸ºè‡ªåŠ¨åŒ–è¯†åˆ«å¤§è„‘åŒºåŸŸå¹¶å‡å°‘ä¸“å®¶äººåŠ›æˆæœ¬æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12282v1",
      "published_date": "2026-01-18 06:42:24 UTC",
      "updated_date": "2026-01-18 06:42:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:43:53.441297+00:00"
    },
    {
      "arxiv_id": "2601.12276v2",
      "title": "Predictive Prototyping: Evaluating Design Concepts with ChatGPT",
      "title_zh": "é¢„æµ‹æ€§åŸå‹è®¾è®¡ï¼šåˆ©ç”¨ ChatGPT è¯„ä¼°è®¾è®¡æ¦‚å¿µ",
      "authors": [
        "Hilsann Yong",
        "Bradley A. Camburn"
      ],
      "abstract": "The design-build-test cycle is essential for innovation, but physical prototyping is often slow and expensive. Although physics-based simulation and strategic prototyping can reduce cost, meaningful evaluation is frequently constrained until an integrated prototype is built. This paper investigates whether a generative pretrained transformer (GPT) can predict information typically obtained through prototyping, including cost, performance, and perceived usability. We introduce a retrieval-augmented generation (RAG) method to emulate design feedback using OpenAI GPT-4o, grounded in prototyping data scraped from Instructables.com to increase access to relevant precedent. Two studies are reported. First, a controlled experiment compares GPT-RAG and human designers, who receive design sketches and predict cost, performance, and usability; predictions are evaluated against ground-truth results from physical prototypes. Second, we report an applied demonstration in which a physical prototype is produced from GPT-RAG recommendations and compared with a commercial baseline and a topology-optimized design. Results show that GPT-RAG provides more accurate cost and performance estimates than individual or crowd human estimates, while yielding comparable usability insights; the GPT-RAG-informed prototype also outperforms both comparison prototypes. Repeated querying with response averaging significantly improves accuracy, suggesting that LLMs can emulate crowd aggregation effects consistent with the law of large numbers.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ç”Ÿæˆå¼é¢„è®­ç»ƒå˜æ¢å™¨(GPT)é¢„æµ‹åŸå‹è®¾è®¡ä¸­æˆæœ¬ã€æ€§èƒ½å’Œå¯ç”¨æ€§ç­‰å…³é”®ä¿¡æ¯çš„å¯è¡Œæ€§ï¼Œæ—¨åœ¨è§£å†³ç‰©ç†åŸå‹åˆ¶ä½œå‘¨æœŸé•¿ä¸”æˆæœ¬é«˜çš„é—®é¢˜ã€‚ç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)æ–¹æ³•ï¼Œé€šè¿‡ä½¿ç”¨ OpenAI GPT-4o å¹¶ç»“åˆä» Instructables.com æŠ“å–çš„åŸå‹æ•°æ®æ¥æ¨¡æ‹Ÿè®¾è®¡åé¦ˆã€‚é€šè¿‡ä¸¤é¡¹å®éªŒéªŒè¯ï¼Œç ”ç©¶é¦–å…ˆå°† GPT-RAG ä¸äººç±»è®¾è®¡å¸ˆåœ¨é¢„æµ‹å‡†ç¡®æ€§ä¸Šè¿›è¡Œå¯¹æ¯”ï¼Œéšåæ ¹æ®æ¨¡å‹å»ºè®®åˆ¶ä½œç‰©ç†åŸå‹å¹¶ä¸å•†ä¸šåŸºå‡†åŠæ‹“æ‰‘ä¼˜åŒ–è®¾è®¡è¿›è¡Œå®æµ‹ã€‚ç»“æœè¡¨æ˜ï¼ŒGPT-RAG åœ¨æˆæœ¬å’Œæ€§èƒ½é¢„ä¼°æ–¹é¢çš„å‡†ç¡®åº¦ä¼˜äºä¸ªäººæˆ–ç¾¤ä½“äººç±»ä¼°è®¡ï¼ŒåŒæ—¶èƒ½æä¾›å…·æœ‰ç«äº‰åŠ›çš„å¯ç”¨æ€§è§è§£ï¼Œä¸”å…¶æŒ‡å¯¼ç”Ÿæˆçš„åŸå‹åœ¨å®é™…è¡¨ç°ä¸Šè¶…è¶Šäº†å¯¹æ¯”æ–¹æ¡ˆã€‚æ­¤å¤–ï¼Œç ”ç©¶å‘ç°é€šè¿‡å¤šæ¬¡æŸ¥è¯¢å¹¶å–å¹³å‡å€¼èƒ½æ˜¾è‘—æå‡é¢„æµ‹ç²¾åº¦ï¼Œè¯æ˜äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨¡æ‹Ÿç¾¤ä½“å†³ç­–å’Œéµå¾ªå¤§æ•°å®šå¾‹æ–¹é¢çš„åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "22 pages, 15 figures, 5 tables",
      "pdf_url": "https://arxiv.org/pdf/2601.12276v2",
      "published_date": "2026-01-18 06:26:03 UTC",
      "updated_date": "2026-01-21 03:54:37 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:43:22.113243+00:00"
    },
    {
      "arxiv_id": "2601.12269v1",
      "title": "Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive Language Models",
      "title_zh": "æ¨¡æ‹Ÿé€€ç«ç®—æ³•æå‡è‡ªå›å½’è¯­è¨€æ¨¡å‹çš„å¿ƒæ™ºç†è®ºæ¨ç†èƒ½åŠ›",
      "authors": [
        "Xucong Hu",
        "Jian-Qiao Zhu"
      ],
      "abstract": "Autoregressive language models are next-token predictors and have been criticized for only optimizing surface plausibility (i.e., local coherence) rather than maintaining correct latent-state representations (i.e., global coherence). Because Theory of Mind (ToM) tasks crucially depend on reasoning about latent mental states of oneself and others, such models are therefore often thought to fail at ToM. While post-training methods can improve ToM performance, we show that strong ToM capability can be recovered directly from the base model without any additional weight updates or verifications. Our approach builds on recent power-sampling methods (Karan & Du, 2025) that use Markov chain Monte Carlo (MCMC) to sample from sharpened sequence-level (rather than token-level) probability distributions of autoregressive language models. We further find that incorporating annealing, where the tempered distribution is gradually shifted from high to low temperature, substantially improves ToM performance over fixed-temperature power sampling. Together, these results suggest that sampling-based optimization provides a powerful way to extract latent capabilities from language models without retraining.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è‡ªå›å½’è¯­è¨€æ¨¡å‹ (Autoregressive language models) åœ¨å¿ƒæ™ºç†è®º (Theory of Mind) æ¨ç†æ–¹é¢çš„å±€é™æ€§ï¼Œå³è¿™ç±»æ¨¡å‹å¾€å¾€ä¾§é‡å±€éƒ¨è¿è´¯æ€§è€Œå¿½è§†äº†æ½œçŠ¶æ€è¡¨å¾ (latent-state representations)ã€‚ç ”ç©¶æå‡ºäº†ä¸€ç§æ— éœ€é‡æ–°è®­ç»ƒçš„æ”¹è¿›æ–¹æ¡ˆï¼Œé€šè¿‡é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´› (MCMC) æ–¹æ³•ä»åºåˆ—çº§æ¦‚ç‡åˆ†å¸ƒä¸­è¿›è¡Œå¹‚é‡‡æ · (power sampling) ä»¥æå–æ¨¡å‹çš„æ½œåœ¨èƒ½åŠ›ã€‚å®éªŒå‘ç°ï¼Œå¼•å…¥æ¨¡æ‹Ÿé€€ç« (Simulated Annealing) è¿‡ç¨‹ï¼Œå³å°†åˆ†å¸ƒæ¸©åº¦ä»é«˜åˆ°ä½é€æ¸è°ƒæ•´ï¼Œæ¯”å›ºå®šæ¸©åº¦é‡‡æ ·æ›´èƒ½æ˜¾è‘—å¢å¼ºæ¨¡å‹çš„å¿ƒæ™ºç†è®ºæ¨ç†èƒ½åŠ›ã€‚è¿™ä¸€ç»“æœè¡¨æ˜ï¼ŒåŸºäºé‡‡æ ·çš„ä¼˜åŒ–æŠ€æœ¯ä¸ºåœ¨ä¸æ›´æ–°æƒé‡çš„æƒ…å†µä¸‹æå–è¯­è¨€æ¨¡å‹çš„æ·±å±‚æ½œåœ¨èƒ½åŠ›æä¾›äº†ä¸€æ¡æœ‰æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12269v1",
      "published_date": "2026-01-18 05:51:30 UTC",
      "updated_date": "2026-01-18 05:51:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:43:38.254908+00:00"
    },
    {
      "arxiv_id": "2601.12263v1",
      "title": "Multimodal Generative Engine Optimization: Rank Manipulation for Vision-Language Model Rankers",
      "title_zh": "å¤šæ¨¡æ€ç”Ÿæˆå¼å¼•æ“ä¼˜åŒ–ï¼šé’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹æ’åºå™¨çš„æ’åæ“çºµ",
      "authors": [
        "Yixuan Du",
        "Chenxiao Yu",
        "Haoyan Xu",
        "Ziyi Wang",
        "Yue Zhao",
        "Xiyang Hu"
      ],
      "abstract": "Vision-Language Models (VLMs) are rapidly replacing unimodal encoders in modern retrieval and recommendation systems. While their capabilities are well-documented, their robustness against adversarial manipulation in competitive ranking scenarios remains largely unexplored. In this paper, we uncover a critical vulnerability in VLM-based product search: multimodal ranking attacks. We present Multimodal Generative Engine Optimization (MGEO), a novel adversarial framework that enables a malicious actor to unfairly promote a target product by jointly optimizing imperceptible image perturbations and fluent textual suffixes. Unlike existing attacks that treat modalities in isolation, MGEO employs an alternating gradient-based optimization strategy to exploit the deep cross-modal coupling within the VLM. Extensive experiments on real-world datasets using state-of-the-art models demonstrate that our coordinated attack significantly outperforms text-only and image-only baselines. These findings reveal that multimodal synergy, typically a strength of VLMs, can be weaponized to compromise the integrity of search rankings without triggering conventional content filters.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹ (Vision-Language Models, VLMs) åœ¨ç°ä»£æœç´¢ä¸æ¨èç³»ç»Ÿä¸­çš„å®‰å…¨æ€§é—®é¢˜ï¼Œæ­ç¤ºäº†å…¶åœ¨é¢å¯¹ç«äº‰æ€§æ’åæ—¶çš„å¤šæ¨¡æ€æ’åæ”»å‡»æ¼æ´ã€‚ä½œè€…æå‡ºäº†å¤šæ¨¡æ€ç”Ÿæˆå¼•æ“ä¼˜åŒ– (Multimodal Generative Engine Optimization, MGEO) æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å…è®¸æ”»å‡»è€…é€šè¿‡è”åˆä¼˜åŒ–ä¸å¯æ„ŸçŸ¥çš„å›¾åƒæ‰°åŠ¨ (image perturbations) å’Œæµç•…çš„æ–‡æœ¬åç¼€ (textual suffixes) æ¥ä¸å…¬å¹³åœ°æå‡ç›®æ ‡äº§å“çš„æ’åã€‚MGEO é‡‡ç”¨äº†äº¤æ›¿æ¢¯åº¦ä¼˜åŒ–ç­–ç•¥ (alternating gradient-based optimization)ï¼Œæ—¨åœ¨åˆ©ç”¨ VLM å†…éƒ¨æ·±å±‚çš„è·¨æ¨¡æ€è€¦åˆç‰¹æ€§ã€‚åœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§ååŒæ”»å‡»çš„æ•ˆæœæ˜¾è‘—ä¼˜äºä»…é’ˆå¯¹æ–‡æœ¬æˆ–å›¾åƒçš„åŸºå‡†æ–¹æ³•ã€‚è¿™ä¸€å‘ç°è¡¨æ˜ï¼Œå¤šæ¨¡æ€ååŒæ•ˆåº”è¿™ä¸€ VLM çš„æ ¸å¿ƒä¼˜åŠ¿å¯èƒ½ä¼šè¢«æ­¦å™¨åŒ–ï¼Œä»è€Œåœ¨ä¸è§¦å‘ä¼ ç»Ÿå†…å®¹è¿‡æ»¤å™¨çš„æƒ…å†µä¸‹ç ´åæœç´¢æ’åçš„å®Œæ•´æ€§ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12263v1",
      "published_date": "2026-01-18 04:58:28 UTC",
      "updated_date": "2026-01-18 04:58:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:43:35.570073+00:00"
    },
    {
      "arxiv_id": "2601.12260v1",
      "title": "Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned Visually Rich Documents Understanding",
      "title_zh": "Docs2Synthï¼šé¢å‘æ‰«æè§†è§‰ä¸°å¯Œæ–‡æ¡£ç†è§£çš„åˆæˆæ•°æ®è®­ç»ƒæ£€ç´¢å™¨æ¡†æ¶",
      "authors": [
        "Yihao Ding",
        "Qiang Sun",
        "Puzhen Wu",
        "Sirui Li",
        "Siwen Luo",
        "Wei Liu"
      ],
      "abstract": "Document understanding (VRDU) in regulated domains is particularly challenging, since scanned documents often contain sensitive, evolving, and domain specific knowledge. This leads to two major challenges: the lack of manual annotations for model adaptation and the difficulty for pretrained models to stay up-to-date with domain-specific facts. While Multimodal Large Language Models (MLLMs) show strong zero-shot abilities, they still suffer from hallucination and limited domain grounding. In contrast, discriminative Vision-Language Pre-trained Models (VLPMs) provide reliable grounding but require costly annotations to cover new domains. We introduce Docs2Synth, a synthetic-supervision framework that enables retrieval-guided inference for private and low-resource domains. Docs2Synth automatically processes raw document collections, generates and verifies diverse QA pairs via an agent-based system, and trains a lightweight visual retriever to extract domain-relevant evidence. During inference, the retriever collaborates with an MLLM through an iterative retrieval--generation loop, reducing hallucination and improving response consistency. We further deliver Docs2Synth as an easy-to-use Python package, enabling plug-and-play deployment across diverse real-world scenarios. Experiments on multiple VRDU benchmarks show that Docs2Synth substantially enhances grounding and domain generalization without requiring human annotations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§„ç®¡é¢†åŸŸä¸­å¯è§†åŒ–ä¸°å¯Œæ–‡æ¡£ç†è§£(VRDU)é¢ä¸´çš„æ ‡æ³¨åŒ®ä¹å’Œæ¨¡å‹å¹»è§‰ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†Docs2Synthæ¡†æ¶ï¼Œæ—¨åœ¨å®ç°ç§æœ‰å’Œä½èµ„æºé¢†åŸŸçš„æ£€ç´¢å¼•å¯¼æ¨ç†ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åŸºäºæ™ºèƒ½ä½“(Agent-based)çš„ç³»ç»Ÿè‡ªåŠ¨å¤„ç†åŸå§‹æ–‡æ¡£ï¼Œé€šè¿‡ç”Ÿæˆå¹¶éªŒè¯å¤šæ ·åŒ–çš„é—®ç­”å¯¹(QA pairs)æ¥è®­ç»ƒè½»é‡çº§è§†è§‰æ£€ç´¢å™¨(Visual Retriever)ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ£€ç´¢å™¨ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)é€šè¿‡è¿­ä»£çš„æ£€ç´¢-ç”Ÿæˆ(Retrieval-Generation)å¾ªç¯è¿›è¡Œåä½œï¼Œæ˜¾è‘—å‡å°‘äº†å¹»è§‰å¹¶æå‡äº†å“åº”çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDocs2Synthåœ¨å¤šä¸ªVRDUåŸºå‡†æµ‹è¯•ä¸­å¤§å¹…å¢å¼ºäº†æ¨¡å‹çš„å®šä½èƒ½åŠ›å’Œé¢†åŸŸæ³›åŒ–èƒ½åŠ›ï¼Œä¸”æ— éœ€äººå·¥æ ‡æ³¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿå°†è¯¥æ¡†æ¶å°è£…ä¸ºæ˜“ç”¨çš„Pythonè½¯ä»¶åŒ…ï¼Œæ”¯æŒåœ¨å¤šæ ·åŒ–çš„ç°å®åº”ç”¨åœºæ™¯ä¸­è¿›è¡Œå³æ’å³ç”¨å¼éƒ¨ç½²ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at WWW 2026 Demo Track",
      "pdf_url": "https://arxiv.org/pdf/2601.12260v1",
      "published_date": "2026-01-18 04:45:09 UTC",
      "updated_date": "2026-01-18 04:45:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:44:39.193444+00:00"
    },
    {
      "arxiv_id": "2601.12259v1",
      "title": "FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains",
      "title_zh": "FutureX-Proï¼šå°†æœªæ¥é¢„æµ‹æ‰©å±•è‡³é«˜ä»·å€¼å‚ç›´é¢†åŸŸ",
      "authors": [
        "Jiashuo Liu",
        "Siyuan Chen",
        "Zaiyuan Wang",
        "Zhiyuan Zeng",
        "Jiacheng Guo",
        "Liang Hu",
        "Lingyue Yin",
        "Suozhi Huang",
        "Wenxin Hao",
        "Yang Yang",
        "Zerui Cheng",
        "Zixin Yao",
        "Lingyue Yin",
        "Haoxin Liu",
        "Jiayi Cheng",
        "Yuzhen Li",
        "Zezhong Ma",
        "Bingjie Wang",
        "Bingsen Qiu",
        "Xiao Liu",
        "Zeyang Zhang",
        "Zijian Liu",
        "Jinpeng Wang",
        "Mingren Yin",
        "Tianci He",
        "Yali Liao",
        "Yixiao Tian",
        "Zhenwei Zhu",
        "Anqi Dai",
        "Ge Zhang",
        "Jingkai Liu",
        "Kaiyuan Zhang",
        "Wenlong Wu",
        "Xiang Gao",
        "Xinjie Chen",
        "Zhixin Yao",
        "Zhoufutu Wen",
        "B. Aditya Prakash",
        "Jose Blanchet",
        "Mengdi Wang",
        "Nian Si",
        "Wenhao Huang"
      ],
      "abstract": "Building upon FutureX, which established a live benchmark for general-purpose future prediction, this report introduces FutureX-Pro, including FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, FutureX-NaturalDisaster, and FutureX-Search. These together form a specialized framework extending agentic future prediction to high-value vertical domains. While generalist agents demonstrate proficiency in open-domain search, their reliability in capital-intensive and safety-critical sectors remains under-explored. FutureX-Pro targets four economically and socially pivotal verticals: Finance, Retail, Public Health, and Natural Disaster. We benchmark agentic Large Language Models (LLMs) on entry-level yet foundational prediction tasks -- ranging from forecasting market indicators and supply chain demands to tracking epidemic trends and natural disasters. By adapting the contamination-free, live-evaluation pipeline of FutureX, we assess whether current State-of-the-Art (SOTA) agentic LLMs possess the domain grounding necessary for industrial deployment. Our findings reveal the performance gap between generalist reasoning and the precision required for high-value vertical applications.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨FutureXé€šç”¨æœªæ¥é¢„æµ‹åŸºå‡†çš„åŸºç¡€ä¸Šæå‡ºäº†FutureX-Proï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨é’ˆå¯¹é«˜ä»·å€¼å‚ç›´é¢†åŸŸçš„æ™ºèƒ½ä½“æœªæ¥é¢„æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ¶µç›–äº†FutureX-Financeã€FutureX-Retailã€FutureX-PublicHealthã€FutureX-NaturalDisasterå’ŒFutureX-Searchäº”ä¸ªå­é¢†åŸŸï¼Œæ—¨åœ¨å¼¥è¡¥é€šç”¨æ™ºèƒ½ä½“åœ¨èµ„æœ¬å¯†é›†å‹å’Œå®‰å…¨å…³é”®è¡Œä¸šä¸­çš„å¯é æ€§ä¸è¶³ã€‚é€šè¿‡åœ¨å¸‚åœºæŒ‡æ ‡ã€ä¾›åº”é“¾éœ€æ±‚ã€æµè¡Œç—…è¶‹åŠ¿å’Œè‡ªç„¶ç¾å®³ç­‰åŸºç¡€é¢„æµ‹ä»»åŠ¡ä¸Šå¯¹æ™ºèƒ½ä½“å¤§è¯­è¨€æ¨¡å‹(Agentic LLMs)è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶é‡‡ç”¨äº†FutureXçš„æ— æ±¡æŸ“å®æ—¶è¯„ä¼°æµç¨‹(contamination-free, live-evaluation pipeline)æ¥è¡¡é‡å…¶é¢†åŸŸè½åœ°èƒ½åŠ›ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“å‰çš„é¡¶å°–æ¨¡å‹(SOTA models)åœ¨é€šç”¨æ¨ç†ä¸é«˜ä»·å€¼å‚ç›´åº”ç”¨æ‰€éœ€çš„ç²¾ç¡®åº¦ä¹‹é—´ä»å­˜åœ¨æ˜¾è‘—å·®è·ï¼Œä¸ºæœªæ¥å·¥ä¸šçº§æ™ºèƒ½ä½“çš„éƒ¨ç½²æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "21 pages",
      "pdf_url": "https://arxiv.org/pdf/2601.12259v1",
      "published_date": "2026-01-18 04:44:49 UTC",
      "updated_date": "2026-01-18 04:44:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:45:46.640180+00:00"
    },
    {
      "arxiv_id": "2601.12257v1",
      "title": "Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D Computational Periscopy",
      "title_zh": "Soft Shadow Diffusion (SSD)ï¼šç”¨äºä¸‰ç»´è®¡ç®—æ½œæœ›çš„å—ç‰©ç†å¯å‘çš„å­¦ä¹ ",
      "authors": [
        "Fadlullah Raji",
        "John Murray-Bruce"
      ],
      "abstract": "Conventional imaging requires a line of sight to create accurate visual representations of a scene. In certain circumstances, however, obtaining a suitable line of sight may be impractical, dangerous, or even impossible. Non-line-of-sight (NLOS) imaging addresses this challenge by reconstructing the scene from indirect measurements. Recently, passive NLOS methods that use an ordinary photograph of the subtle shadow cast onto a visible wall by the hidden scene have gained interest. These methods are currently limited to 1D or low-resolution 2D color imaging or to localizing a hidden object whose shape is approximately known. Here, we generalize this class of methods and demonstrate a 3D reconstruction of a hidden scene from an ordinary NLOS photograph. To achieve this, we propose a novel reformulation of the light transport model that conveniently decomposes the hidden scene into \\textit{light-occluding} and \\textit{non-light-occluding} components to yield a separable non-linear least squares (SNLLS) inverse problem. We develop two solutions: A gradient-based optimization method and a physics-inspired neural network approach, which we call Soft Shadow diffusion (SSD). Despite the challenging ill-conditioned inverse problem encountered here, our approaches are effective on numerous 3D scenes in real experimental scenarios. Moreover, SSD is trained in simulation but generalizes well to unseen classes in simulation and real-world NLOS scenes. SSD also shows surprising robustness to noise and ambient illumination.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Soft Shadow Diffusion (SSD)ï¼Œæ—¨åœ¨é€šè¿‡æ™®é€šç…§ç‰‡æ•è·çš„å¾®å¼±é˜´å½±å®ç°ä¸‰ç»´éè§†è· (Non-line-of-sight) åœºæ™¯çš„é‡å»ºã€‚é’ˆå¯¹ä¼ ç»Ÿè¢«åŠ¨å¼ NLOS æ–¹æ³•ä»…é™äºä½åˆ†è¾¨ç‡ 2D æˆåƒæˆ–å·²çŸ¥å½¢çŠ¶ç‰©ä½“å®šä½çš„å±€é™æ€§ï¼Œè¯¥å·¥ä½œå°†æ­¤ç±»æ–¹æ³•æ¨å¹¿åˆ°äº†æ›´å¤æ‚çš„ä¸‰ç»´ç©ºé—´ã€‚ç ”ç©¶è€…é€šè¿‡é‡æ–°æ„å»ºå…‰ä¼ è¾“æ¨¡å‹ï¼Œå°†éšè—åœºæ™¯åˆ†è§£ä¸ºé®å…‰ (light-occluding) å’Œéé®å…‰ (non-light-occluding) ç»„ä»¶ï¼Œä»è€Œå°†é‡å»ºä»»åŠ¡è½¬åŒ–ä¸ºä¸€ä¸ªå¯åˆ†ç¦»éçº¿æ€§æœ€å°äºŒä¹˜ (SNLLS) é€†é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œå›¢é˜Ÿå¼€å‘äº†åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–æ–¹æ³•ä»¥åŠå—ç‰©ç†å¯å‘çš„æ–°å‹ç¥ç»ç½‘ç»œ SSDï¼Œèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹æå…·æŒ‘æˆ˜æ€§çš„ç—…æ€é€†é—®é¢˜ã€‚å®éªŒè¡¨æ˜ï¼ŒSSD è™½åœ¨æ¨¡æ‹Ÿç¯å¢ƒä¸­è®­ç»ƒï¼Œä½†åœ¨å¤„ç†çœŸå®ä¸–ç•Œçš„ 3D åœºæ™¯å’Œæœªè§ç±»åˆ«æ—¶å…·æœ‰å“è¶Šçš„æ³›åŒ–èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•åœ¨å™ªå£°å’Œç¯å¢ƒå…‰å¹²æ‰°ä¸‹ä¹Ÿè¡¨ç°å‡ºæå¼ºçš„é²æ£’æ€§ï¼Œä¸º 3D è®¡ç®—æ½œæœ›é•œ (Computational Periscopy) é¢†åŸŸæä¾›äº†å…¨æ–°çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CG",
        "cs.GR"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12257v1",
      "published_date": "2026-01-18 04:40:00 UTC",
      "updated_date": "2026-01-18 04:40:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:45:19.245522+00:00"
    },
    {
      "arxiv_id": "2601.12256v1",
      "title": "Improving Large Molecular Language Model via Relation-aware Multimodal Collaboration",
      "title_zh": "é€šè¿‡å…³ç³»æ„ŸçŸ¥å¤šæ¨¡æ€åä½œæå‡å¤§åˆ†å­è¯­è¨€æ¨¡å‹",
      "authors": [
        "Jinyoung Park",
        "Minseong Bae",
        "Jeehye Na",
        "Hyunwoo J. Kim"
      ],
      "abstract": "Large language models (LLMs) have demonstrated their instruction-following capabilities and achieved powerful performance on various tasks. Inspired by their success, recent works in the molecular domain have led to the development of large molecular language models (LMLMs) that integrate 1D molecular strings or 2D molecular graphs into the language models. However, existing LMLMs often suffer from hallucination and limited robustness, largely due to inadequate integration of diverse molecular modalities such as 1D sequences, 2D molecular graphs, and 3D conformations. To address these limitations, we propose CoLLaMo, a large language model-based molecular assistant equipped with a multi-level molecular modality-collaborative projector. The relation-aware modality-collaborative attention mechanism in the projector facilitates fine-grained and relation-guided information exchange between atoms by incorporating 2D structural and 3D spatial relations. Furthermore, we present a molecule-centric new automatic measurement, including a hallucination assessment metric and GPT-based caption quality evaluation to address the limitations of token-based generic evaluation metrics (i.e., BLEU) widely used in assessing molecular comprehension of LMLMs. Our extensive experiments demonstrate that our CoLLaMo enhances the molecular modality generalization capabilities of LMLMs, achieving the best performance on multiple tasks, including molecule captioning, computed property QA, descriptive property QA, motif counting, and IUPAC name prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹åˆ†å­è¯­è¨€æ¨¡å‹ï¼ˆLMLMsï¼‰åœ¨é›†æˆ1Dã€2Då’Œ3Dåˆ†å­æ¨¡æ€æ—¶å­˜åœ¨çš„å¹»è§‰åŠé²æ£’æ€§ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†CoLLaMoåˆ†å­åŠ©æ‰‹æ¡†æ¶ã€‚è¯¥æ¨¡å‹æ ¸å¿ƒåœ¨äºä¸€ç§å¤šå±‚çº§åˆ†å­æ¨¡æ€åä½œæŠ•å½±ä»ªï¼Œåˆ©ç”¨relation-aware modality-collaborative attentionæœºåˆ¶ç»“åˆ2Dç»“æ„å’Œ3Dç©ºé—´å…³ç³»ï¼Œå®ç°äº†åŸå­é—´ç»†ç²’åº¦ä¸”ç”±å…³ç³»å¼•å¯¼çš„ä¿¡æ¯äº¤æ¢ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†ä»¥åˆ†å­ä¸ºä¸­å¿ƒçš„æ–°å‹è‡ªåŠ¨æµ‹é‡ä½“ç³»ï¼ŒåŒ…æ‹¬é’ˆå¯¹æ€§çš„å¹»è§‰è¯„ä¼°æŒ‡æ ‡å’ŒåŸºäºGPTçš„æè¿°è´¨é‡è¯„ä»·ï¼Œå¼¥è¡¥äº†BLEUç­‰é€šç”¨æŒ‡æ ‡åœ¨è¯„ä¼°åˆ†å­ç†è§£èƒ½åŠ›ä¸Šçš„ä¸è¶³ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCoLLaMoæ˜¾è‘—å¢å¼ºäº†LMLMsçš„æ¨¡æ€æ³›åŒ–èƒ½åŠ›ï¼Œåœ¨åˆ†å­æè¿°ï¼ˆmolecule captioningï¼‰ã€å±æ€§é—®ç­”ã€åŸºå›¢è®¡æ•°åŠIUPACåç§°é¢„æµ‹ç­‰å¤šé¡¹ä»»åŠ¡ä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½æ°´å¹³ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12256v1",
      "published_date": "2026-01-18 04:38:19 UTC",
      "updated_date": "2026-01-18 04:38:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:44:42.692936+00:00"
    },
    {
      "arxiv_id": "2601.12249v1",
      "title": "An Innovative Framework for Breast Cancer Detection Using Pyramid Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature Fusion",
      "title_zh": "åŸºäºé‡‘å­—å¡”è‡ªé€‚åº”ç©ºæ´å·ç§¯ã€Transformer é›†æˆä¸å¤šå°ºåº¦ç‰¹å¾èåˆçš„åˆ›æ–°ä¹³è…ºç™Œæ£€æµ‹æ¡†æ¶",
      "authors": [
        "Ehsan Sadeghi Pour",
        "Mahdi Esmaeili",
        "Morteza Romoozi"
      ],
      "abstract": "Breast cancer is one of the most common cancers among women worldwide, and its accurate and timely diagnosis plays a critical role in improving treatment outcomes. This thesis presents an innovative framework for detecting malignant masses in mammographic images by integrating the Pyramid Adaptive Atrous Convolution (PAAC) and Transformer architectures. The proposed approach utilizes Multi-Scale Feature Fusion to enhance the extraction of features from benign and malignant tissues and combines Dice Loss and Focal Loss functions to improve the model's learning process, effectively reducing errors in binary breast cancer classification and achieving high accuracy and efficiency. In this study, a comprehensive dataset of breast cancer images from INbreast, MIAS, and DDSM was preprocessed through data augmentation and contrast enhancement and resized to 227x227 pixels for model training. Leveraging the Transformer's ability to manage long-range dependencies with Self-Attention mechanisms, the proposed model achieved high accuracy in detecting cancerous masses, outperforming foundational models such as BreastNet, DeepMammo, Multi-Scale CNN, Swin-Unet, and SegFormer. The final evaluation results for the proposed model include an accuracy of 98.5\\%, sensitivity of 97.8\\%, specificity of 96.3\\%, F1-score of 98.2\\%, and overall precision of 97.9\\%. These metrics demonstrate a significant improvement over traditional methods and confirm the model's effectiveness in identifying cancerous masses in complex scenarios and large datasets. This model shows potential as a reliable and efficient tool for breast cancer diagnosis and can be effectively integrated into medical diagnostic systems.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä¹³è…ºç™Œè¯Šæ–­æå‡ºäº†ä¸€ç§é›†æˆPyramid Adaptive Atrous Convolution (PAAC)ä¸Transformeræ¶æ„çš„åˆ›æ–°æ£€æµ‹æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡Multi-Scale Feature FusionæŠ€æœ¯å¢å¼ºäº†å¯¹è‰¯æ€§å’Œæ¶æ€§ç»„ç»‡ç‰¹å¾çš„æå–ï¼Œå¹¶ç»“åˆDice Lossä¸Focal Losså‡½æ•°ä¼˜åŒ–æ¨¡å‹å­¦ä¹ è¿‡ç¨‹ï¼Œæ˜¾è‘—é™ä½äº†åˆ†ç±»è¯¯å·®ã€‚åˆ©ç”¨Transformerçš„Self-Attentionæœºåˆ¶å¤„ç†å›¾åƒä¸­çš„é•¿ç¨‹ä¾èµ–ï¼Œè¯¥æ¨¡å‹åœ¨INbreastã€MIASåŠDDSMç­‰æ•°æ®é›†ä¸Šè¡¨ç°å“è¶Šã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨ä¹³è…ºç™Œæ£€æµ‹ä¸­å®ç°äº†98.5%çš„å‡†ç¡®ç‡ã€97.8%çš„çµæ•åº¦å’Œ98.2%çš„F1-scoreï¼Œæ€§èƒ½å…¨é¢è¶…è¶Šäº†BreastNetã€Swin-Unetå’ŒSegFormerç­‰åŸºå‡†æ¨¡å‹ã€‚è¿™ä¸€é«˜æ•ˆä¸”å¯é çš„æ¡†æ¶ä¸ºå¤æ‚åœºæ™¯ä¸‹çš„ä¹³è…ºç™Œè¾…åŠ©è¯Šæ–­æä¾›äº†å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æŒï¼Œå…·æœ‰é›†æˆåˆ°ä¸´åºŠåŒ»ç–—ç³»ç»Ÿçš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 page",
      "pdf_url": "https://arxiv.org/pdf/2601.12249v1",
      "published_date": "2026-01-18 03:55:33 UTC",
      "updated_date": "2026-01-18 03:55:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:44:48.295525+00:00"
    },
    {
      "arxiv_id": "2601.12248v1",
      "title": "AQUA-Bench: Beyond Finding Answers to Knowing When There Are None in Audio Question Answering",
      "title_zh": "AQUA-Benchï¼šéŸ³é¢‘é—®ç­”ä¸­ä»å¯»æ‰¾ç­”æ¡ˆåˆ°è¯†åˆ«æ— è§£æƒ…å½¢çš„è·¨è¶Š",
      "authors": [
        "Chun-Yi Kuan",
        "Hung-yi Lee"
      ],
      "abstract": "Recent advances in audio-aware large language models have shown strong performance on audio question answering. However, existing benchmarks mainly cover answerable questions and overlook the challenge of unanswerable ones, where no reliable answer can be inferred from the audio. Such cases are common in real-world settings, where questions may be misleading, ill-posed, or incompatible with the information. To address this gap, we present AQUA-Bench, a benchmark for Audio Question Unanswerability Assessment. It systematically evaluates three scenarios: Absent Answer Detection (the correct option is missing), Incompatible Answer Set Detection (choices are categorically mismatched with the question), and Incompatible Audio Question Detection (the question is irrelevant or lacks sufficient grounding in the audio). By assessing these cases, AQUA-Bench offers a rigorous measure of model reliability and promotes the development of audio-language systems that are more robust and trustworthy. Our experiments suggest that while models excel on standard answerable tasks, they often face notable challenges with unanswerable ones, pointing to a blind spot in current audio-language understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹éŸ³é¢‘å¤§è¯­è¨€æ¨¡å‹(Audio-aware LLMs)åœ¨å¤„ç†ä¸å¯å›ç­”é—®é¢˜æ–¹é¢çš„å±€é™æ€§ï¼Œæå‡ºäº† AQUA-Bench è¯„æµ‹åŸºå‡†ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹è¯†åˆ«æ— æ³•ä»éŸ³é¢‘ä¸­æ¨æ–­å¯é ç­”æ¡ˆçš„æƒ…å†µã€‚è¯¥åŸºå‡†ç³»ç»Ÿåœ°æ¶µç›–äº†ç¼ºå¤±ç­”æ¡ˆæ£€æµ‹(Absent Answer Detection)ã€ä¸å…¼å®¹ç­”æ¡ˆé›†æ£€æµ‹(Incompatible Answer Set Detection)ä»¥åŠä¸å…¼å®¹éŸ³é¢‘é—®é¢˜æ£€æµ‹(Incompatible Audio Question Detection)ä¸‰ä¸ªæ ¸å¿ƒåœºæ™¯ã€‚é€šè¿‡è¿™ä¸€å¤šç»´åº¦çš„è¯„ä¼°ä½“ç³»ï¼ŒAQUA-Bench ä¸ºè¡¡é‡éŸ³é¢‘è¯­è¨€ç³»ç»Ÿ(Audio-language systems)çš„é²æ£’æ€§å’Œå¯é æ€§æä¾›äº†ä¸¥è°¨çš„æ ‡å‡†ã€‚å®éªŒå‘ç°ï¼Œå°½ç®¡ç°æœ‰æ¨¡å‹åœ¨æ ‡å‡†çš„å¯å›ç­”ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨åº”å¯¹ä¸å¯å›ç­”çš„æŒ‘æˆ˜æ—¶ä»å­˜åœ¨æ˜¾è‘—çŸ­æ¿ã€‚è¿™é¡¹ç ”ç©¶æ­ç¤ºäº†å½“å‰éŸ³é¢‘è¯­è¨€ç†è§£é¢†åŸŸçš„å…³é”®ç›²ç‚¹ï¼Œä¸ºå¼€å‘æ›´å…·å¯ä¿¡åº¦çš„æ™ºèƒ½éŸ³é¢‘äº¤äº’ç³»ç»Ÿå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "primary_category": "eess.AS",
      "comment": "Accepted to ICASSP 2026. Project Website: https://kuan2jiu99.github.io/AQUA-Bench-demo/",
      "pdf_url": "https://arxiv.org/pdf/2601.12248v1",
      "published_date": "2026-01-18 03:55:28 UTC",
      "updated_date": "2026-01-18 03:55:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:45:05.599348+00:00"
    },
    {
      "arxiv_id": "2601.12247v1",
      "title": "Plan, Verify and Fill: A Structured Parallel Decoding Approach for Diffusion Language Models",
      "title_zh": "è®¡åˆ’ã€éªŒè¯ä¸å¡«å……ï¼šä¸€ç§é¢å‘æ‰©æ•£è¯­è¨€æ¨¡å‹çš„ç»“æ„åŒ–å¹¶è¡Œè§£ç æ–¹æ³•",
      "authors": [
        "Miao Li",
        "Hanyang Jiang",
        "Sikai Chen",
        "Hengyu Fu",
        "Yuhang Cai",
        "Baihe Huang",
        "Tinghan Ye",
        "Xuanzhou Chen",
        "Pascal Van Hentenryck"
      ],
      "abstract": "Diffusion Language Models (DLMs) present a promising non-sequential paradigm for text generation, distinct from standard autoregressive (AR) approaches. However, current decoding strategies often adopt a reactive stance, underutilizing the global bidirectional context to dictate global trajectories. To address this, we propose Plan-Verify-Fill (PVF), a training-free paradigm that grounds planning via quantitative validation. PVF actively constructs a hierarchical skeleton by prioritizing high-leverage semantic anchors and employs a verification protocol to operationalize pragmatic structural stopping where further deliberation yields diminishing returns. Extensive evaluations on LLaDA-8B-Instruct and Dream-7B-Instruct demonstrate that PVF reduces the Number of Function Evaluations (NFE) by up to 65% compared to confidence-based parallel decoding across benchmark datasets, unlocking superior efficiency without compromising accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹Diffusion Language Models (DLMs)åœ¨æ–‡æœ¬ç”Ÿæˆä¸­æœªèƒ½å……åˆ†åˆ©ç”¨å…¨å±€åŒå‘ä¸Šä¸‹æ–‡çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºPlan-Verify-Fill (PVF)çš„æ— éœ€è®­ç»ƒ(training-free)çš„è§£ç èŒƒå¼ã€‚PVFé€šè¿‡ä¼˜å…ˆå¤„ç†é«˜æ æ†çš„è¯­ä¹‰é”šç‚¹(semantic anchors)æ¥ä¸»åŠ¨æ„å»ºå±‚æ¬¡åŒ–éª¨æ¶ï¼Œå¹¶å¼•å…¥éªŒè¯åè®®(verification protocol)æ¥å®ç°åŠ¡å®çš„ç»“æ„æ€§åœæ­¢ï¼Œä»¥é¿å…æ”¶ç›Šé€’å‡çš„å†—ä½™è®¡ç®—ã€‚å®éªŒåœ¨LLaDA-8B-Instructå’ŒDream-7B-Instructä¸Šè¿›è¡Œï¼Œç»“æœè¯æ˜PVFåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šæ¯”åŸºäºç½®ä¿¡åº¦çš„å¹¶è¡Œè§£ç å‡å°‘äº†é«˜è¾¾65%çš„Function Evaluations (NFE)ã€‚è¯¥æ–¹æ³•åœ¨ä¸ç‰ºç‰²ç”Ÿæˆå‡†ç¡®æ€§çš„å‰æä¸‹æ˜¾è‘—æå‡äº†æ¨ç†æ•ˆç‡ï¼Œä¸ºéé¡ºåºç”Ÿæˆçš„å…¨å±€è½¨è¿¹è§„åˆ’æä¾›äº†æœ‰æ•ˆçš„ç»“æ„åŒ–è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12247v1",
      "published_date": "2026-01-18 03:53:01 UTC",
      "updated_date": "2026-01-18 03:53:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:45:02.394473+00:00"
    },
    {
      "arxiv_id": "2601.12243v1",
      "title": "Less is More: Label-Guided Summarization of Procedural and Instructional Videos",
      "title_zh": "å°‘å³æ˜¯å¤šï¼šè¿‡ç¨‹åŒ–ä¸æ•™å­¦è§†é¢‘çš„æ ‡ç­¾å¼•å¯¼æ‘˜è¦",
      "authors": [
        "Shreya Rajpal",
        "Michal Golovanesky",
        "Carsten Eickhoff"
      ],
      "abstract": "Video summarization helps turn long videos into clear, concise representations that are easier to review, document, and analyze, especially in high-stakes domains like surgical training. Prior work has progressed from using basic visual features like color, motion, and structural changes to using pre-trained vision-language models that can better understand what's happening in the video (semantics) and capture temporal flow, resulting in more context-aware video summarization. We propose a three-stage framework, PRISM: Procedural Representation via Integrated Semantic and Multimodal analysis, that produces semantically grounded video summaries. PRISM combines adaptive visual sampling, label-driven keyframe anchoring, and contextual validation using a large language model (LLM). Our method ensures that selected frames reflect meaningful and procedural transitions while filtering out generic or hallucinated content, resulting in contextually coherent summaries across both domain-specific and instructional videos. We evaluate our method on instructional and activity datasets, using reference summaries for instructional videos. Despite sampling fewer than 5% of the original frames, our summaries retain 84% semantic content while improving over baselines by as much as 33%. Our approach generalizes across procedural and domain-specific video tasks, achieving strong performance with both semantic alignment and precision.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PRISMï¼ˆProcedural Representation via Integrated Semantic and Multimodal analysisï¼‰æ¡†æ¶ï¼Œæ—¨åœ¨æå‡æ‰‹æœ¯åŸ¹è®­ç­‰é«˜é£é™©é¢†åŸŸä¸­ç¨‹åºåŒ–ä¸æ•™å­¦è§†é¢‘çš„æ‘˜è¦æ•ˆç‡ã€‚è¯¥æ¡†æ¶åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒé˜¶æ®µï¼šAdaptive Visual Samplingã€Label-driven Keyframe Anchoringä»¥åŠåˆ©ç”¨Large Language Model (LLM) è¿›è¡Œçš„ Contextual Validationã€‚PRISMé€šè¿‡ç»“åˆè¯­ä¹‰ä¸å¤šæ¨¡æ€åˆ†æï¼Œç¡®ä¿æå–çš„å…³é”®å¸§èƒ½å¤Ÿç²¾ç¡®åæ˜ ç¨‹åºè½¬æ¢ï¼Œå¹¶æœ‰æ•ˆè¿‡æ»¤å†—ä½™æˆ– Hallucination å†…å®¹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåœ¨ä»…é‡‡æ ·ä¸åˆ°5%åŸå§‹å¸§çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ä¿ç•™äº†84%çš„è¯­ä¹‰å†…å®¹ï¼Œæ€§èƒ½ä¼˜äºåŸºçº¿æ¨¡å‹è¾¾33%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†PRISMåœ¨ä¸åŒç¨‹åºåŒ–è§†é¢‘ä»»åŠ¡ä¸­å…·æœ‰å¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œå®ç°äº†æé«˜çš„è¯­ä¹‰å¯¹é½ä¸é¢„æµ‹ç²¾ç¡®åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "22 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2601.12243v1",
      "published_date": "2026-01-18 03:41:48 UTC",
      "updated_date": "2026-01-18 03:41:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:45:08.801608+00:00"
    },
    {
      "arxiv_id": "2601.12242v1",
      "title": "Optimal Power Allocation and Sub-Optimal Channel Assignment for Downlink NOMA Systems Using Deep Reinforcement Learning",
      "title_zh": "åŸºäºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„ä¸‹è¡Œ NOMA ç³»ç»Ÿæœ€ä¼˜åŠŸç‡åˆ†é…ä¸æ¬¡ä¼˜ä¿¡é“åˆ†é…",
      "authors": [
        "WooSeok Kim",
        "Jeonghoon Lee",
        "Sangho Kim",
        "Taesun An",
        "WonMin Lee",
        "Dowon Kim",
        "Kyungseop Shin"
      ],
      "abstract": "In recent years, Non-Orthogonal Multiple Access (NOMA) system has emerged as a promising candidate for multiple access frameworks due to the evolution of deep machine learning, trying to incorporate deep machine learning into the NOMA system. The main motivation for such active studies is the growing need to optimize the utilization of network resources as the expansion of the internet of things (IoT) caused a scarcity of network resources. The NOMA addresses this need by power multiplexing, allowing multiple users to access the network simultaneously. Nevertheless, the NOMA system has few limitations. Several works have proposed to mitigate this, including the optimization of power allocation known as joint resource allocation(JRA) method, and integration of the JRA method and deep reinforcement learning (JRA-DRL). Despite this, the channel assignment problem remains unclear and requires further investigation. In this paper, we propose a deep reinforcement learning framework incorporating replay memory with an on-policy algorithm, allocating network resources in a NOMA system to generalize the learning. Also, we provide extensive simulations to evaluate the effects of varying the learning rate, batch size, type of model, and the number of features in the state.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç‰©è”ç½‘ (IoT) æ‰©å¼ å¯¼è‡´çš„èµ„æºç¨€ç¼ºé—®é¢˜ï¼Œæ¢è®¨äº†ä¸‹è¡Œéæ­£äº¤å¤šå€æ¥å…¥ (NOMA) ç³»ç»Ÿä¸­çš„èµ„æºä¼˜åŒ–ã€‚å°½ç®¡ç°æœ‰çš„è”åˆèµ„æºåˆ†é… (JRA) å’Œæ·±åº¦å¼ºåŒ–å­¦ä¹  (DRL) æ–¹æ³•å·²åœ¨åŠŸç‡åˆ†é… (Power Allocation) æ–¹é¢å–å¾—è¿›å±•ï¼Œä½†ä¿¡é“åˆ†é… (Channel Assignment) é—®é¢˜ä»éœ€æ·±å…¥è°ƒæŸ¥ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†ä¸€ç§ç»“åˆç»éªŒå›æ”¾ (Replay Memory) ä¸åœ¨ç­–ç•¥ (On-policy) ç®—æ³•çš„æ·±åº¦å¼ºåŒ–å­¦ä¹  (Deep Reinforcement Learning) æ¡†æ¶ï¼Œæ—¨åœ¨ä¼˜åŒ– NOMA ç³»ç»Ÿä¸­çš„èµ„æºåˆ†é…å¹¶æé«˜å­¦ä¹ çš„æ³›åŒ–èƒ½åŠ›ã€‚é€šè¿‡å¹¿æ³›çš„ä»¿çœŸå®éªŒï¼Œç ”ç©¶è¯¦ç»†è¯„ä¼°äº†å­¦ä¹ ç‡ (Learning Rate)ã€æ‰¹å¤§å° (Batch Size)ã€æ¨¡å‹ç±»å‹ä»¥åŠçŠ¶æ€ (State) ä¸­ç‰¹å¾æ•°é‡å¯¹ç³»ç»Ÿæ€§èƒ½çš„å…·ä½“å½±å“ã€‚è¯¥å·¥ä½œä¸ºè§£å†³ä¸‹è¡Œ NOMA ç³»ç»Ÿä¸­çš„åŠŸç‡ä¸ä¿¡é“è”åˆä¼˜åŒ–é—®é¢˜æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ï¼Œè¿›ä¸€æ­¥æå‡äº†ç½‘ç»œèµ„æºçš„åˆ©ç”¨æ•ˆç‡ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG",
        "cs.NI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12242v1",
      "published_date": "2026-01-18 03:37:40 UTC",
      "updated_date": "2026-01-18 03:37:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:45:27.831342+00:00"
    },
    {
      "arxiv_id": "2601.14302v1",
      "title": "DDSA: Dual-Domain Strategic Attack for Spatial-Temporal Efficiency in Adversarial Robustness Testing",
      "title_zh": "DDSAï¼šé¢å‘å¯¹æŠ—é²æ£’æ€§æµ‹è¯•æ—¶ç©ºæ•ˆç‡çš„åŒåŸŸç­–ç•¥æ€§æ”»å‡»",
      "authors": [
        "Jinwei Hu",
        "Shiyuan Meng",
        "Yi Dong",
        "Xiaowei Huang"
      ],
      "abstract": "Image transmission and processing systems in resource-critical applications face significant challenges from adversarial perturbations that compromise mission-specific object classification. Current robustness testing methods require excessive computational resources through exhaustive frame-by-frame processing and full-image perturbations, proving impractical for large-scale deployments where massive image streams demand immediate processing. This paper presents DDSA (Dual-Domain Strategic Attack), a resource-efficient adversarial robustness testing framework that optimizes testing through temporal selectivity and spatial precision. We introduce a scenario-aware trigger function that identifies critical frames requiring robustness evaluation based on class priority and model uncertainty, and employ explainable AI techniques to locate influential pixel regions for targeted perturbation. Our dual-domain approach achieves substantial temporal-spatial resource conservation while maintaining attack effectiveness. The framework enables practical deployment of comprehensive adversarial robustness testing in resource-constrained real-time applications where computational efficiency directly impacts mission success.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DDSA (Dual-Domain Strategic Attack)ï¼Œè¿™æ˜¯ä¸€ç§æ—¨åœ¨ä¼˜åŒ–å¯¹æŠ—é²æ£’æ€§æµ‹è¯•(Adversarial Robustness Testing)æ—¶ç©ºæ•ˆç‡çš„èµ„æºé«˜æ•ˆå‹æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å¤„ç†å¤§è§„æ¨¡å›¾åƒæµæ—¶å› é€å¸§å¤„ç†å’Œå…¨å›¾æ‰°åŠ¨å¯¼è‡´çš„è®¡ç®—èµ„æºæ¶ˆè€—è¿‡å¤§é—®é¢˜ï¼ŒDDSAé€šè¿‡æ—¶åŸŸé€‰æ‹©æ€§å’Œç©ºåŸŸç²¾ç¡®æ€§å®ç°äº†ä¼˜åŒ–ã€‚åœ¨æ—¶åŸŸä¸Šï¼Œè¯¥æ¡†æ¶å¼•å…¥äº†åœºæ™¯æ„ŸçŸ¥è§¦å‘å‡½æ•°(Scenario-aware trigger function)ï¼Œæ ¹æ®ç±»åˆ«ä¼˜å…ˆçº§å’Œæ¨¡å‹ä¸ç¡®å®šæ€§è¯†åˆ«éœ€è¦è¯„ä¼°çš„å…³é”®å¸§ã€‚åœ¨ç©ºåŸŸä¸Šï¼Œåˆ©ç”¨å¯è§£é‡ŠAI (Explainable AI) æŠ€æœ¯å®šä½å½±å“æœ€å¤§çš„åƒç´ åŒºåŸŸè¿›è¡Œé’ˆå¯¹æ€§æ‰°åŠ¨ã€‚è¿™ç§åŒåŸŸç­–ç•¥åœ¨ä¿æŒæ”»å‡»æœ‰æ•ˆæ€§çš„åŒæ—¶ï¼Œæ˜¾è‘—å‡å°‘äº†æ—¶ç©ºèµ„æºæ¶ˆè€—ã€‚è¯¥æ¡†æ¶ä¸ºè®¡ç®—æ•ˆç‡ç›´æ¥å½±å“ä»»åŠ¡æˆåŠŸçš„èµ„æºå—é™å®æ—¶åº”ç”¨æä¾›äº†åˆ‡å®å¯è¡Œçš„å…¨é¢å¯¹æŠ—é²æ£’æ€§æµ‹è¯•æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.PF"
      ],
      "primary_category": "cs.CR",
      "comment": "Preprint accepted by ICASSP 2026 with minor revisions",
      "pdf_url": "https://arxiv.org/pdf/2601.14302v1",
      "published_date": "2026-01-18 03:14:22 UTC",
      "updated_date": "2026-01-18 03:14:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:45:24.219850+00:00"
    },
    {
      "arxiv_id": "2601.12234v1",
      "title": "Proc3D: Procedural 3D Generation and Parametric Editing of 3D Shapes with Large Language Models",
      "title_zh": "Proc3Dï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„ç¨‹åºåŒ–3Dç”Ÿæˆä¸3Då½¢çŠ¶å‚æ•°åŒ–ç¼–è¾‘",
      "authors": [
        "Fadlullah Raji",
        "Stefano Petrangeli",
        "Matheus Gadelha",
        "Yu Shen",
        "Uttaran Bhattacharya",
        "Gang Wu"
      ],
      "abstract": "Generating 3D models has traditionally been a complex task requiring specialized expertise. While recent advances in generative AI have sought to automate this process, existing methods produce non-editable representation, such as meshes or point clouds, limiting their adaptability for iterative design. In this paper, we introduce Proc3D, a system designed to generate editable 3D models while enabling real-time modifications. At its core, Proc3D introduces procedural compact graph (PCG), a graph representation of 3D models, that encodes the algorithmic rules and structures necessary for generating the model. This representation exposes key parameters, allowing intuitive manual adjustments via sliders and checkboxes, as well as real-time, automated modifications through natural language prompts using Large Language Models (LLMs). We demonstrate Proc3D's capabilities using two generative approaches: GPT-4o with in-context learning (ICL) and a fine-tuned LLAMA-3 model. Experimental results show that Proc3D outperforms existing methods in editing efficiency, achieving more than 400x speedup over conventional approaches that require full regeneration for each modification. Additionally, Proc3D improves ULIP scores by 28%, a metric that evaluates the alignment between generated 3D models and text prompts. By enabling text-aligned 3D model generation along with precise, real-time parametric edits, Proc3D facilitates highly accurate text-based image editing applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Proc3D ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç°æœ‰ 3D ç”Ÿæˆæ¨¡å‹äº§å‡ºçš„ meshes æˆ– point clouds è¡¨ç¤ºéš¾ä»¥è¿›è¡Œè¿­ä»£è®¾è®¡å’Œå‚æ•°åŒ–ç¼–è¾‘çš„é—®é¢˜ã€‚è¯¥ç³»ç»Ÿçš„æ ¸å¿ƒæ˜¯å¼•å…¥äº†ç¨‹åºåŒ–ç´§å‡‘å›¾ (procedural compact graph, PCG)ï¼Œè¿™æ˜¯ä¸€ç§èƒ½å¤Ÿç¼–ç ç”Ÿæˆæ¨¡å‹æ‰€éœ€ç®—æ³•è§„åˆ™å’Œç»“æ„çš„å›¾è¡¨ç¤ºæ³•ã€‚é€šè¿‡ PCGï¼Œè¯¥ç³»ç»Ÿæš´éœ²äº†å…³é”®å‚æ•°ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡æ»‘å—å’Œå¤é€‰æ¡†è¿›è¡Œç›´è§‚æ‰‹åŠ¨è°ƒæ•´ï¼Œæˆ–åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) å¤„ç†è‡ªç„¶è¯­è¨€æç¤ºå®ç°å®æ—¶çš„è‡ªåŠ¨ä¿®æ”¹ã€‚ç ”ç©¶å±•ç¤ºäº†ä½¿ç”¨å¸¦æœ‰ in-context learning (ICL) çš„ GPT-4o ä»¥åŠå¾®è°ƒåçš„ LLAMA-3 æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒProc3D åœ¨ç¼–è¾‘æ•ˆç‡ä¸Šæ¯”éœ€è¦é‡æ–°ç”Ÿæˆçš„ä¼ ç»Ÿæ–¹æ³•å¿« 400 å€ä»¥ä¸Šï¼Œå¹¶åœ¨è¡¡é‡æ–‡æœ¬ä¸ 3D æ¨¡å‹å¯¹é½åº¦çš„ ULIP è¯„åˆ†ä¸Šæå‡äº† 28%ã€‚è¿™ä½¿å¾— Proc3D èƒ½å¤Ÿæ”¯æŒé«˜åº¦å‡†ç¡®çš„åŸºäºæ–‡æœ¬çš„ 3D æ¨¡å‹ç”Ÿæˆä»¥åŠç²¾ç¡®çš„å®æ—¶å‚æ•°åŒ–ç¼–è¾‘ã€‚",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.GR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12234v1",
      "published_date": "2026-01-18 03:08:08 UTC",
      "updated_date": "2026-01-18 03:08:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:46:06.300564+00:00"
    },
    {
      "arxiv_id": "2601.12224v1",
      "title": "Where It Moves, It Matters: Referring Surgical Instrument Segmentation via Motion",
      "title_zh": "åŠ¨ä¹‹æ‰€åœ¨ï¼Œé‡ä¹‹æ‰€åœ¨ï¼šåŸºäºè¿åŠ¨çš„æ‰‹æœ¯å™¨æ¢°æŒ‡ä»£åˆ†å‰²",
      "authors": [
        "Meng Wei",
        "Kun Yuan",
        "Shi Li",
        "Yue Zhou",
        "Long Bai",
        "Nassir Navab",
        "Hongliang Ren",
        "Hong Joo Lee",
        "Tom Vercauteren",
        "Nicolas Padoy"
      ],
      "abstract": "Enabling intuitive, language-driven interaction with surgical scenes is a critical step toward intelligent operating rooms and autonomous surgical robotic assistance. However, the task of referring segmentation, localizing surgical instruments based on natural language descriptions, remains underexplored in surgical videos, with existing approaches struggling to generalize due to reliance on static visual cues and predefined instrument names. In this work, we introduce SurgRef, a novel motion-guided framework that grounds free-form language expressions in instrument motion, capturing how tools move and interact across time, rather than what they look like. This allows models to understand and segment instruments even under occlusion, ambiguity, or unfamiliar terminology. To train and evaluate SurgRef, we present Ref-IMotion, a diverse, multi-institutional video dataset with dense spatiotemporal masks and rich motion-centric expressions. SurgRef achieves state-of-the-art accuracy and generalization across surgical procedures, setting a new benchmark for robust, language-driven surgical video segmentation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†SurgRefï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°é¢–çš„è¿åŠ¨å¯¼å‘æ¡†æ¶ï¼Œä¸“æ³¨äºè§£å†³æ‰‹æœ¯è§†é¢‘ä¸­çš„æŒ‡ä»£åˆ†å‰²(Referring Segmentation)ä»»åŠ¡ï¼Œå³æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°å®šä½æ‰‹æœ¯å™¨æ¢°ã€‚ä¸ä»¥å¾€ä¾èµ–é™æ€è§†è§‰çº¿ç´¢å’Œé¢„å®šä¹‰åç§°çš„æ–¹æ³•ä¸åŒï¼ŒSurgRefå°†è‡ªç”±æ ¼å¼çš„è¯­è¨€è¡¨è¾¾ä¸å™¨æ¢°çš„è¿åŠ¨ç‰¹å¾ç›¸ç»“åˆï¼Œæ•æ‰å·¥å…·åœ¨æ—¶é—´ä¸Šçš„äº¤äº’ä¸ç§»åŠ¨æ–¹å¼ï¼Œè€Œéä»…ä»…ä¾èµ–å…¶å¤–è§‚ã€‚è¿™ç§æœºåˆ¶ä½¿å¾—æ¨¡å‹åœ¨é¢å¯¹é®æŒ¡ã€æ­§ä¹‰æˆ–æœªçŸ¥æœ¯è¯­æ—¶ä¾ç„¶èƒ½å¤Ÿå®ç°å‡†ç¡®åˆ†å‰²ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…è¿˜æ¨å‡ºäº†Ref-IMotionæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶µç›–å¤šæœºæ„è§†é¢‘ã€å¯†é›†æ—¶ç©ºæ©ç åŠè¿åŠ¨æ ¸å¿ƒè¡¨è¾¾å¼çš„å¤§å‹æ•°æ®é›†ã€‚å®éªŒè¯æ˜ï¼ŒSurgRefåœ¨å„ç§æ‰‹æœ¯ç¨‹åºä¸­å‡è¾¾åˆ°äº†æœ€å…ˆè¿›(State-of-the-art)çš„å‡†ç¡®ç‡å’Œæ³›åŒ–æ€§èƒ½ï¼Œä¸ºè¯­è¨€é©±åŠ¨çš„æ‰‹æœ¯è§†é¢‘åˆ†ææ ‘ç«‹äº†æ–°çš„åŸºå‡†ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12224v1",
      "published_date": "2026-01-18 02:14:08 UTC",
      "updated_date": "2026-01-18 02:14:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:46:23.493028+00:00"
    },
    {
      "arxiv_id": "2601.12215v1",
      "title": "Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation Models",
      "title_zh": "é¢å‘PPGåŸºç¡€æ¨¡å‹çš„å°æ³¢é©±åŠ¨æ©ç å¤šå°ºåº¦é‡å»º",
      "authors": [
        "Megha Thukral",
        "Cyrus Tanade",
        "Simon A. Lee",
        "Juhyeon Lee",
        "Hao Zhou",
        "Keum San Chun",
        "Migyeong Gwak",
        "Viswam Nathan",
        "Md Mahbubur Rahman",
        "Li Zhu",
        "Mehrab Bin Morshed",
        "Subramaniam Venkatraman",
        "Sharanya Arcot Desai"
      ],
      "abstract": "Wearable foundation models have the potential to transform digital health by learning transferable representations from large-scale biosignals collected in everyday settings. While recent progress has been made in large-scale pretraining, most approaches overlook the spectral structure of photoplethysmography (PPG) signals, wherein physiological rhythms unfold across multiple frequency bands. Motivated by the insight that many downstream health-related tasks depend on multi-resolution features spanning fine-grained waveform morphology to global rhythmic dynamics, we introduce Masked Multiscale Reconstruction (MMR) for PPG representation learning - a self-supervised pretraining framework that explicitly learns from hierarchical time-frequency scales of PPG data. The pretraining task is designed to reconstruct randomly masked out coefficients obtained from a wavelet-based multiresolution decomposition of PPG signals, forcing the transformer encoder to integrate information across temporal and spectral scales. We pretrain our model with MMR using ~17 million unlabeled 10-second PPG segments from ~32,000 smartwatch users. On 17 of 19 diverse health-related tasks, MMR trained on large-scale wearable PPG data improves over or matches state-of-the-art open-source PPG foundation models, time-series foundation models, and other self-supervised baselines. Extensive analysis of our learned embeddings and systematic ablations underscores the value of wavelet-based representations, showing that they capture robust and physiologically-grounded features. Together, these results highlight the potential of MMR as a step toward generalizable PPG foundation models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Masked Multiscale Reconstruction (MMR)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å…‰ç”µå®¹ç§¯è„‰ææ³¢(PPG)åŸºç¡€æ¨¡å‹åœ¨é¢„è®­ç»ƒä¸­å¿½è§†ä¿¡å·é¢‘è°±ç»“æ„çš„é—®é¢˜ã€‚MMRé€šè¿‡å°æ³¢å˜æ¢(wavelet-based multiresolution decomposition)è¿›è¡Œå¤šåˆ†è¾¨ç‡åˆ†è§£ï¼Œåˆ©ç”¨è‡ªç›‘ç£å­¦ä¹ ä»å±‚çº§åŒ–çš„æ—¶é¢‘å°ºåº¦ä¸­æå–ç‰¹å¾ã€‚è¯¥æ¡†æ¶çš„é¢„è®­ç»ƒä»»åŠ¡é€šè¿‡é‡å»ºéšæœºæ©ç çš„å°æ³¢ç³»æ•°ï¼Œè¿«ä½¿Transformerç¼–ç å™¨æ•´åˆè·¨æ—¶é—´å’Œé¢‘è°±çš„ç”Ÿç†èŠ‚å¾‹ä¿¡æ¯ã€‚ç ”ç©¶å›¢é˜Ÿä½¿ç”¨æ¥è‡ªçº¦32,000åæ™ºèƒ½æ‰‹è¡¨ç”¨æˆ·çš„1700ä¸‡ä¸ªæ— æ ‡ç­¾PPGç‰‡æ®µè¿›è¡Œäº†å¤§è§„æ¨¡é¢„è®­ç»ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMMRåœ¨19é¡¹å¥åº·ç›¸å…³ä»»åŠ¡ä¸­çš„17é¡¹ä¸Šä¼˜äºæˆ–ç­‰åŒäºå½“å‰çš„SOTAæ¨¡å‹å’ŒåŸºçº¿ã€‚åˆ†æè¯å®äº†è¯¥æ–¹æ³•èƒ½æ•è·ç¨³å¥ä¸”å…·æœ‰ç”Ÿç†ä¾æ®çš„ç‰¹å¾ï¼Œä¸ºé€šç”¨PPGåŸºç¡€æ¨¡å‹çš„å‘å±•å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12215v1",
      "published_date": "2026-01-18 01:34:47 UTC",
      "updated_date": "2026-01-18 01:34:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:46:10.193361+00:00"
    },
    {
      "arxiv_id": "2601.12212v1",
      "title": "Speculative Sampling with Reinforcement Learning",
      "title_zh": "åŸºäºå¼ºåŒ–å­¦ä¹ çš„æŠ•æœºé‡‡æ ·",
      "authors": [
        "Chenan Wang",
        "Daniel H. Shi",
        "Haipeng Chen"
      ],
      "abstract": "Inference time latency has remained an open challenge for real world applications of large language models (LLMs). State-of-the-art (SOTA) speculative sampling (SpS) methods for LLMs, like EAGLE-3, use tree-based drafting to explore multiple candidate continuations in parallel. However, the hyperparameters controlling the tree structure are static, which limits flexibility and efficiency across diverse contexts and domains. We introduce Reinforcement learning for Speculative Sampling (Re-SpS), the first reinforcement learning (RL)-based framework for draft tree hyperparameter optimization. Re-SpS dynamically adjusts draft tree hyperparameters in real-time, learning context-aware policies that maximize generation speed by balancing speculative aggression with computational overhead. It leverages efficient state representations from target model hidden states and introduces multi-step action persistence for better context modeling. Evaluation results across five diverse benchmarks demonstrate consistent improvements over the SOTA method EAGLE-3, achieving up to 5.45$\\times$ speedup over the backbone LLM and up to 1.12$\\times$ speedup compared to EAGLE-3 across five diverse benchmarks, with no loss in output fidelity.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Reinforcement learning for Speculative Sampling (Re-SpS)ï¼Œè¿™æ˜¯é¦–ä¸ªåŸºäº Reinforcement Learning (RL) çš„æ¨æµ‹é‡‡æ · (Speculative Sampling, SpS) è‰ç¨¿æ ‘è¶…å‚æ•°ä¼˜åŒ–æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰ SOTA æ–¹æ³•ï¼ˆå¦‚ EAGLE-3ï¼‰ä¸­è‰ç¨¿æ ‘ç»“æ„è¶…å‚æ•°é™æ€ã€ç¼ºä¹çµæ´»æ€§ä¸”åœ¨ä¸åŒä¸Šä¸‹æ–‡å’Œé¢†åŸŸä¸­æ•ˆç‡æœ‰é™çš„é—®é¢˜ï¼ŒRe-SpS å®ç°äº†è¶…å‚æ•°çš„å®æ—¶åŠ¨æ€è°ƒæ•´ã€‚è¯¥æ¡†æ¶é€šè¿‡å­¦ä¹ ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç­–ç•¥ï¼Œæœ‰æ•ˆå¹³è¡¡äº†æ¨æµ‹æ¿€è¿›ç¨‹åº¦ä¸è®¡ç®—å¼€é”€ï¼Œä»è€Œæœ€å¤§åŒ–ç”Ÿæˆé€Ÿåº¦ã€‚Re-SpS åˆ©ç”¨ç›®æ ‡æ¨¡å‹çš„éšè—çŠ¶æ€ (hidden states) æ„å»ºé«˜æ•ˆçš„çŠ¶æ€è¡¨ç¤ºï¼Œå¹¶å¼•å…¥äº†å¤šæ­¥åŠ¨ä½œæŒä¹…åŒ– (multi-step action persistence) ä»¥å¢å¼ºä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒRe-SpS åœ¨äº”ä¸ªä¸åŒåŸºå‡†æµ‹è¯•ä¸­å‡ä¼˜äº EAGLE-3ï¼Œå®ç°äº†ç›¸æ¯”åŸºç¡€å¤§è¯­è¨€æ¨¡å‹ (LLMs) é«˜è¾¾ 5.45 å€çš„åŠ é€Ÿã€‚åœ¨ä¸æŸå¤±è¾“å‡ºå¿ å®åº¦ (fidelity) çš„æƒ…å†µä¸‹ï¼Œè¯¥æ–¹æ³•ç›¸æ¯” EAGLE-3 è¿›ä¸€æ­¥æå‡äº† 1.12 å€çš„æ¨ç†é€Ÿåº¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2601.12212v1",
      "published_date": "2026-01-18 01:31:29 UTC",
      "updated_date": "2026-01-18 01:31:29 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:46:25.892600+00:00"
    },
    {
      "arxiv_id": "2601.12205v1",
      "title": "Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages and Non-Speech Tasks",
      "title_zh": "ç¥ç»ç¼–è§£ç å™¨æ˜¯å¦å…·æœ‰æ³›åŒ–èƒ½åŠ›ï¼Ÿä¸€é¡¹é’ˆå¯¹æœªè§è¯­è¨€ä¸éè¯­éŸ³ä»»åŠ¡çš„å—æ§ç ”ç©¶",
      "authors": [
        "Shih-Heng Wang",
        "Jiatong Shi",
        "Jinchuan Tian",
        "Haibin Wu",
        "Shinji Watanabe"
      ],
      "abstract": "This paper investigates three crucial yet underexplored aspects of the generalization capabilities of neural audio codecs (NACs): (i) whether NACs can generalize to unseen languages during pre-training, (ii) whether speech-only pre-trained NACs can effectively generalize to non-speech applications such as environmental sounds, music, and animal vocalizations, and (iii) whether incorporating non-speech data during pre-training can improve performance on both speech and non-speech tasks. Existing studies typically rely on off-the-shelf NACs for comparison, which limits insight due to variations in implementation. In this work, we train NACs from scratch using strictly controlled configurations and carefully curated pre-training data to enable fair comparisons. We conduct a comprehensive evaluation of NAC performance on both signal reconstruction quality and downstream applications using 11 metrics. Our results show that NACs can generalize to unseen languages during pre-training, speech-only pre-trained NACs exhibit degraded performance on non-speech tasks, and incorporating non-speech data during pre-training improves performance on non-speech tasks while maintaining comparable performance on speech tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å—æ§å®éªŒæ·±å…¥æ¢è®¨äº†ç¥ç»éŸ³é¢‘ç¼–è§£ç å™¨(NACs)åœ¨æœªçŸ¥è¯­è¨€åŠéè¯­éŸ³ä»»åŠ¡ï¼ˆå¦‚ç¯å¢ƒéŸ³ã€éŸ³ä¹å’ŒåŠ¨ç‰©é¸£å«ï¼‰ä¸­çš„æ³›åŒ–èƒ½åŠ›ã€‚ç ”ç©¶å›¢é˜Ÿä»é›¶è®­ç»ƒäº†å…·å¤‡ä¸¥æ ¼é…ç½®çš„NACsï¼Œå¹¶åˆ©ç”¨11é¡¹æŒ‡æ ‡åœ¨ä¿¡å·é‡å»ºè´¨é‡å’Œä¸‹æ¸¸åº”ç”¨æ–¹é¢è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼ŒNACsèƒ½å¤Ÿæœ‰æ•ˆæ³›åŒ–è‡³é¢„è®­ç»ƒä¸­æœªè§è¿‡çš„è¯­è¨€ï¼Œå±•ç°äº†è·¨è¯­è¨€çš„é€šç”¨æ€§ã€‚ç„¶è€Œï¼Œä»…é’ˆå¯¹è¯­éŸ³è¿›è¡Œé¢„è®­ç»ƒçš„NACsåœ¨éè¯­éŸ³ä»»åŠ¡ä¸Šçš„æ€§èƒ½ä¼šæ˜¾è‘—ä¸‹é™ã€‚é€šè¿‡åœ¨é¢„è®­ç»ƒé˜¶æ®µå¼•å…¥éè¯­éŸ³æ•°æ®ï¼Œå¯ä»¥æœ‰æ•ˆæå‡æ¨¡å‹åœ¨éè¯­éŸ³ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼ŒåŒæ—¶ä¿æŒåœ¨è¯­éŸ³ä»»åŠ¡ä¸Šçš„åŒç­‰æ€§èƒ½ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†é¢„è®­ç»ƒæ•°æ®ç»„æˆå¯¹NACsæ³›åŒ–æ€§èƒ½çš„å…³é”®å½±å“ï¼Œä¸ºæ„å»ºæ›´é€šç”¨çš„éŸ³é¢‘å¤„ç†æ¨¡å‹æä¾›äº†å®éªŒæ”¯æ’‘ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.12205v1",
      "published_date": "2026-01-18 00:53:11 UTC",
      "updated_date": "2026-01-18 00:53:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-23T07:46:25.444742+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 75,
  "processed_papers_count": 75,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-23T07:47:24.783512+00:00"
}
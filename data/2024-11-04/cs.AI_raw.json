[
  {
    "arxiv_id": "2411.02685v1",
    "title": "Geometry of naturalistic object representations in recurrent neural network models of working memory",
    "authors": [
      "Xiaoxuan Lei",
      "Takuya Ito",
      "Pouya Bashivan"
    ],
    "abstract": "Working memory is a central cognitive ability crucial for intelligent\ndecision-making. Recent experimental and computational work studying working\nmemory has primarily used categorical (i.e., one-hot) inputs, rather than\necologically relevant, multidimensional naturalistic ones. Moreover, studies\nhave primarily investigated working memory during single or few cognitive\ntasks. As a result, an understanding of how naturalistic object information is\nmaintained in working memory in neural networks is still lacking. To bridge\nthis gap, we developed sensory-cognitive models, comprising a convolutional\nneural network (CNN) coupled with a recurrent neural network (RNN), and trained\nthem on nine distinct N-back tasks using naturalistic stimuli. By examining the\nRNN's latent space, we found that: (1) Multi-task RNNs represent both\ntask-relevant and irrelevant information simultaneously while performing tasks;\n(2) The latent subspaces used to maintain specific object properties in vanilla\nRNNs are largely shared across tasks, but highly task-specific in gated RNNs\nsuch as GRU and LSTM; (3) Surprisingly, RNNs embed objects in new\nrepresentational spaces in which individual object features are less\northogonalized relative to the perceptual space; (4) The transformation of\nworking memory encodings (i.e., embedding of visual inputs in the RNN latent\nspace) into memory was shared across stimuli, yet the transformations governing\nthe retention of a memory in the face of incoming distractor stimuli were\ndistinct across time. Our findings indicate that goal-driven RNNs employ\nchronological memory subspaces to track information over short time spans,\nenabling testable predictions with neural data.",
    "categories": [
      "cs.AI",
      "cs.CG",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02685v1",
    "published_date": "2024-11-04 23:57:46 UTC",
    "updated_date": "2024-11-04 23:57:46 UTC"
  },
  {
    "arxiv_id": "2411.02684v1",
    "title": "Towards Intelligent Augmented Reality (iAR): A Taxonomy of Context, an Architecture for iAR, and an Empirical Study",
    "authors": [
      "Shakiba Davari",
      "Daniel Stover",
      "Alexander Giovannelli",
      "Cory Ilo",
      "Doug A. Bowman"
    ],
    "abstract": "Recent advancements in Augmented Reality (AR) research have highlighted the\ncritical role of context awareness in enhancing interface effectiveness and\nuser experience. This underscores the need for intelligent AR (iAR) interfaces\nthat dynamically adapt across various contexts to provide optimal experiences.\nIn this paper, we (a) propose a comprehensive framework for context-aware\ninference and adaptation in iAR, (b) introduce a taxonomy that describes\ncontext through quantifiable input data, and (c) present an architecture that\noutlines the implementation of our proposed framework and taxonomy within iAR.\nAdditionally, we present an empirical AR experiment to observe user behavior\nand record user performance, context, and user-specified adaptations to the AR\ninterfaces within a context-switching scenario. We (d) explore the nuanced\nrelationships between context and user adaptations in this scenario and discuss\nthe significance of our framework in identifying these patterns. This\nexperiment emphasizes the significance of context-awareness in iAR and provides\na preliminary training dataset for this specific Scenario.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.ET",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02684v1",
    "published_date": "2024-11-04 23:52:43 UTC",
    "updated_date": "2024-11-04 23:52:43 UTC"
  },
  {
    "arxiv_id": "2411.02674v4",
    "title": "Wave Network: An Ultra-Small Language Model",
    "authors": [
      "Xin Zhang",
      "Victor S. Sheng"
    ],
    "abstract": "We propose an innovative token representation and update method in a new\nultra-small language model: the Wave network. Specifically, we use a complex\nvector to represent each token, encoding both global and local semantics of the\ninput text. A complex vector consists of two components: a magnitude vector\nrepresenting the global semantics of the input text, and a phase vector\ncapturing the relationships between individual tokens and global semantics.\nExperiments on the AG News text classification task demonstrate that, when\ngenerating complex vectors from randomly initialized token embeddings, our\nsingle-layer Wave Network achieves 90.91% accuracy with wave interference and\n91.66% with wave modulation - outperforming a single Transformer layer using\nBERT pre-trained embeddings by 19.23% and 19.98%, respectively, and approaching\nthe accuracy of the pre-trained and fine-tuned BERT base model (94.64%).\nAdditionally, compared to BERT base, the Wave Network reduces video memory\nusage and training time by 77.34% and 85.62% during wave modulation. In\nsummary, we used a 2.4-million-parameter small language model to achieve\naccuracy comparable to a 100-million-parameter BERT model in text\nclassification.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02674v4",
    "published_date": "2024-11-04 23:21:12 UTC",
    "updated_date": "2024-11-11 13:49:30 UTC"
  },
  {
    "arxiv_id": "2411.02671v1",
    "title": "Fair In-Context Learning via Latent Concept Variables",
    "authors": [
      "Karuna Bhaila",
      "Minh-Hao Van",
      "Kennedy Edemacu",
      "Chen Zhao",
      "Feng Chen",
      "Xintao Wu"
    ],
    "abstract": "The emerging in-context learning (ICL) ability of large language models\n(LLMs) has prompted their use for predictive tasks in various domains with\ndifferent types of data facilitated by serialization methods. However, with\nincreasing applications in high-stakes domains, it has been shown that LLMs can\ninherit social bias and discrimination from their pre-training data. In this\nwork, we investigate this inherent bias in LLMs during in-context learning with\ntabular data. We focus on an optimal demonstration selection approach that\nutilizes latent concept variables for resource-efficient task adaptation. We\ndesign data augmentation strategies that reduce correlation between predictive\noutcomes and sensitive variables helping to promote fairness during latent\nconcept learning. We utilize the learned concept and select demonstrations from\na training dataset to obtain fair predictions during inference while\nmaintaining model utility. The latent concept variable is learned using a\nsmaller internal LLM and the selected demonstrations can be used for inference\nwith larger external LLMs. We empirically verify that the fair latent variable\napproach improves fairness results on tabular datasets compared to multiple\nheuristic demonstration selection methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "12 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.02671v1",
    "published_date": "2024-11-04 23:10:05 UTC",
    "updated_date": "2024-11-04 23:10:05 UTC"
  },
  {
    "arxiv_id": "2411.02666v1",
    "title": "From Twitter to Reasoner: Understand Mobility Travel Modes and Sentiment Using Large Language Models",
    "authors": [
      "Kangrui Ruan",
      "Xinyang Wang",
      "Xuan Di"
    ],
    "abstract": "Social media has become an important platform for people to express their\nopinions towards transportation services and infrastructure, which holds the\npotential for researchers to gain a deeper understanding of individuals' travel\nchoices, for transportation operators to improve service quality, and for\npolicymakers to regulate mobility services. A significant challenge, however,\nlies in the unstructured nature of social media data. In other words, textual\ndata like social media is not labeled, and large-scale manual annotations are\ncost-prohibitive. In this study, we introduce a novel methodological framework\nutilizing Large Language Models (LLMs) to infer the mentioned travel modes from\nsocial media posts, and reason people's attitudes toward the associated travel\nmode, without the need for manual annotation. We compare different LLMs along\nwith various prompting engineering methods in light of human assessment and LLM\nverification. We find that most social media posts manifest negative rather\nthan positive sentiments. We thus identify the contributing factors to these\nnegative posts and, accordingly, propose recommendations to traffic operators\nand policymakers.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "6 pages; Accepted by ITSC 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.02666v1",
    "published_date": "2024-11-04 23:04:13 UTC",
    "updated_date": "2024-11-04 23:04:13 UTC"
  },
  {
    "arxiv_id": "2411.02664v2",
    "title": "Explanations that reveal all through the definition of encoding",
    "authors": [
      "Aahlad Puli",
      "Nhi Nguyen",
      "Rajesh Ranganath"
    ],
    "abstract": "Feature attributions attempt to highlight what inputs drive predictive power.\nGood attributions or explanations are thus those that produce inputs that\nretain this predictive power; accordingly, evaluations of explanations score\ntheir quality of prediction. However, evaluations produce scores better than\nwhat appears possible from the values in the explanation for a class of\nexplanations, called encoding explanations. Probing for encoding remains a\nchallenge because there is no general characterization of what gives the extra\npredictive power. We develop a definition of encoding that identifies this\nextra predictive power via conditional dependence and show that the definition\nfits existing examples of encoding. This definition implies, in contrast to\nencoding explanations, that non-encoding explanations contain all the\ninformative inputs used to produce the explanation, giving them a \"what you see\nis what you get\" property, which makes them transparent and simple to use.\nNext, we prove that existing scores (ROAR, FRESH, EVAL-X) do not rank\nnon-encoding explanations above encoding ones, and develop STRIPE-X which ranks\nthem correctly. After empirically demonstrating the theoretical insights, we\nuse STRIPE-X to show that despite prompting an LLM to produce non-encoding\nexplanations for a sentiment analysis task, the LLM-generated explanations\nencode.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "36 pages, 7 figures, 6 tables, 38th Conference on Neural Information\n  Processing Systems (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2411.02664v2",
    "published_date": "2024-11-04 23:00:24 UTC",
    "updated_date": "2024-12-18 22:18:45 UTC"
  },
  {
    "arxiv_id": "2411.02649v1",
    "title": "M-CELS: Counterfactual Explanation for Multivariate Time Series Data Guided by Learned Saliency Maps",
    "authors": [
      "Peiyu Li",
      "Omar Bahri",
      "Soukaina Filali Boubrahimi",
      "Shah Muhammad Hamdi"
    ],
    "abstract": "Over the past decade, multivariate time series classification has received\ngreat attention. Machine learning (ML) models for multivariate time series\nclassification have made significant strides and achieved impressive success in\na wide range of applications and tasks. The challenge of many state-of-the-art\nML models is a lack of transparency and interpretability. In this work, we\nintroduce M-CELS, a counterfactual explanation model designed to enhance\ninterpretability in multidimensional time series classification tasks. Our\nexperimental validation involves comparing M-CELS with leading state-of-the-art\nbaselines, utilizing seven real-world time-series datasets from the UEA\nrepository. The results demonstrate the superior performance of M-CELS in terms\nof validity, proximity, and sparsity, reinforcing its effectiveness in\nproviding transparent insights into the decisions of machine learning models\napplied to multivariate time series data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICMLA 2024. arXiv admin note: text overlap with\n  arXiv:2410.20539",
    "pdf_url": "http://arxiv.org/pdf/2411.02649v1",
    "published_date": "2024-11-04 22:16:24 UTC",
    "updated_date": "2024-11-04 22:16:24 UTC"
  },
  {
    "arxiv_id": "2411.02643v1",
    "title": "A Comparative Analysis of Counterfactual Explanation Methods for Text Classifiers",
    "authors": [
      "Stephen McAleese",
      "Mark Keane"
    ],
    "abstract": "Counterfactual explanations can be used to interpret and debug text\nclassifiers by producing minimally altered text inputs that change a\nclassifier's output. In this work, we evaluate five methods for generating\ncounterfactual explanations for a BERT text classifier on two datasets using\nthree evaluation metrics. The results of our experiments suggest that\nestablished white-box substitution-based methods are effective at generating\nvalid counterfactuals that change the classifier's output. In contrast, newer\nmethods based on large language models (LLMs) excel at producing natural and\nlinguistically plausible text counterfactuals but often fail to generate valid\ncounterfactuals that alter the classifier's output. Based on these results, we\nrecommend developing new counterfactual explanation methods that combine the\nstrengths of established gradient-based approaches and newer LLM-based\ntechniques to generate high-quality, valid, and plausible text counterfactual\nexplanations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.02643v1",
    "published_date": "2024-11-04 22:01:52 UTC",
    "updated_date": "2024-11-04 22:01:52 UTC"
  },
  {
    "arxiv_id": "2411.02639v2",
    "title": "Active Prompt Tuning Enables Gpt-40 To Do Efficient Classification Of Microscopy Images",
    "authors": [
      "Abhiram Kandiyana",
      "Peter R. Mouton",
      "Yaroslav Kolinko",
      "Lawrence O. Hall",
      "Dmitry Goldgof"
    ],
    "abstract": "Traditional deep learning-based methods for classifying cellular features in\nmicroscopy images require time- and labor-intensive processes for training\nmodels. Among the current limitations are major time commitments from domain\nexperts for accurate ground truth preparation; and the need for a large amount\nof input image data. We previously proposed a solution that overcomes these\nchallenges using OpenAI's GPT-4(V) model on a pilot dataset (Iba-1\nimmuno-stained tissue sections from 11 mouse brains). Results on the pilot\ndataset were equivalent in accuracy and with a substantial improvement in\nthroughput efficiency compared to the baseline using a traditional\nConvolutional Neural Net (CNN)-based approach.\n  The present study builds upon this framework using a second unique and\nsubstantially larger dataset of microscopy images. Our current approach uses a\nnewer and faster model, GPT-4o, along with improved prompts. It was evaluated\non a microscopy image dataset captured at low (10x) magnification from\ncresyl-violet-stained sections through the cerebellum of a total of 18 mouse\nbrains (9 Lurcher mice, 9 wild-type controls). We used our approach to classify\nthese images either as a control group or Lurcher mutant. Using 6 mice in the\nprompt set the results were correct classification for 11 out of the 12 mice\n(92%) with 96% higher efficiency, reduced image requirements, and lower demands\non time and effort of domain experts compared to the baseline method (snapshot\nensemble of CNN models). These results confirm that our approach is effective\nacross multiple datasets from different brain regions and magnifications, with\nminimal overhead.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted to IEEE ISBI 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.02639v2",
    "published_date": "2024-11-04 21:56:48 UTC",
    "updated_date": "2025-01-18 00:00:33 UTC"
  },
  {
    "arxiv_id": "2411.02632v1",
    "title": "Intelligent Video Recording Optimization using Activity Detection for Surveillance Systems",
    "authors": [
      "Youssef Elmir",
      "Hayet Touati",
      "Ouassila Melizou"
    ],
    "abstract": "Surveillance systems often struggle with managing vast amounts of footage,\nmuch of which is irrelevant, leading to inefficient storage and challenges in\nevent retrieval. This paper addresses these issues by proposing an optimized\nvideo recording solution focused on activity detection. The proposed approach\nutilizes a hybrid method that combines motion detection via frame subtraction\nwith object detection using YOLOv9. This strategy specifically targets the\nrecording of scenes involving human or car activity, thereby reducing\nunnecessary footage and optimizing storage usage. The developed model\ndemonstrates superior performance, achieving precision metrics of 0.855 for car\ndetection and 0.884 for person detection, and reducing the storage requirements\nby two-thirds compared to traditional surveillance systems that rely solely on\nmotion detection. This significant reduction in storage highlights the\neffectiveness of the proposed approach in enhancing surveillance system\nefficiency. Nonetheless, some limitations persist, particularly the occurrence\nof false positives and false negatives in adverse weather conditions, such as\nstrong winds.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages, 6 figures, This manuscript has been accepted for\n  publication in ACTA UNIVERSITATIS SAPIENTIAE, Informatica with minor\n  revisions",
    "pdf_url": "http://arxiv.org/pdf/2411.02632v1",
    "published_date": "2024-11-04 21:44:03 UTC",
    "updated_date": "2024-11-04 21:44:03 UTC"
  },
  {
    "arxiv_id": "2411.02631v1",
    "title": "Extracting Unlearned Information from LLMs with Activation Steering",
    "authors": [
      "Atakan Seyitoğlu",
      "Aleksei Kuvshinov",
      "Leo Schwinn",
      "Stephan Günnemann"
    ],
    "abstract": "An unintended consequence of the vast pretraining of Large Language Models\n(LLMs) is the verbatim memorization of fragments of their training data, which\nmay contain sensitive or copyrighted information. In recent years, unlearning\nhas emerged as a solution to effectively remove sensitive knowledge from models\nafter training. Yet, recent work has shown that supposedly deleted information\ncan still be extracted by malicious actors through various attacks. Still,\ncurrent attacks retrieve sets of possible candidate generations and are unable\nto pinpoint the output that contains the actual target information. We propose\nactivation steering as a method for exact information retrieval from unlearned\nLLMs. We introduce a novel approach to generating steering vectors, named\nAnonymized Activation Steering. Additionally, we develop a simple word\nfrequency method to pinpoint the correct answer among a set of candidates when\nretrieving unlearned information. Our evaluation across multiple unlearning\ntechniques and datasets demonstrates that activation steering successfully\nrecovers general knowledge (e.g., widely known fictional characters) while\nrevealing limitations in retrieving specific information (e.g., details about\nnon-public individuals). Overall, our results demonstrate that exact\ninformation retrieval from unlearned models is possible, highlighting a severe\nvulnerability of current unlearning techniques.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at NeurIPS 2024 Workshop Safe Generative AI",
    "pdf_url": "http://arxiv.org/pdf/2411.02631v1",
    "published_date": "2024-11-04 21:42:56 UTC",
    "updated_date": "2024-11-04 21:42:56 UTC"
  },
  {
    "arxiv_id": "2411.02625v2",
    "title": "EmoSphere++: Emotion-Controllable Zero-Shot Text-to-Speech via Emotion-Adaptive Spherical Vector",
    "authors": [
      "Deok-Hyeon Cho",
      "Hyung-Seok Oh",
      "Seung-Bin Kim",
      "Seong-Whan Lee"
    ],
    "abstract": "Emotional text-to-speech (TTS) technology has achieved significant progress\nin recent years; however, challenges remain owing to the inherent complexity of\nemotions and limitations of the available emotional speech datasets and models.\nPrevious studies typically relied on limited emotional speech datasets or\nrequired extensive manual annotations, restricting their ability to generalize\nacross different speakers and emotional styles. In this paper, we present\nEmoSphere++, an emotion-controllable zero-shot TTS model that can control\nemotional style and intensity to resemble natural human speech. We introduce a\nnovel emotion-adaptive spherical vector that models emotional style and\nintensity without human annotation. Moreover, we propose a multi-level style\nencoder that can ensure effective generalization for both seen and unseen\nspeakers. We also introduce additional loss functions to enhance the emotion\ntransfer performance for zero-shot scenarios. We employ a conditional flow\nmatching-based decoder to achieve high-quality and expressive emotional TTS in\na few sampling steps. Experimental results demonstrate the effectiveness of the\nproposed framework.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02625v2",
    "published_date": "2024-11-04 21:33:56 UTC",
    "updated_date": "2025-04-17 01:50:24 UTC"
  },
  {
    "arxiv_id": "2411.02624v1",
    "title": "Enhancing Indoor Mobility with Connected Sensor Nodes: A Real-Time, Delay-Aware Cooperative Perception Approach",
    "authors": [
      "Minghao Ning",
      "Yaodong Cui",
      "Yufeng Yang",
      "Shucheng Huang",
      "Zhenan Liu",
      "Ahmad Reza Alghooneh",
      "Ehsan Hashemi",
      "Amir Khajepour"
    ],
    "abstract": "This paper presents a novel real-time, delay-aware cooperative perception\nsystem designed for intelligent mobility platforms operating in dynamic indoor\nenvironments. The system contains a network of multi-modal sensor nodes and a\ncentral node that collectively provide perception services to mobility\nplatforms. The proposed Hierarchical Clustering Considering the Scanning\nPattern and Ground Contacting Feature based Lidar Camera Fusion improve\nintra-node perception for crowded environment. The system also features\ndelay-aware global perception to synchronize and aggregate data across nodes.\nTo validate our approach, we introduced the Indoor Pedestrian Tracking dataset,\ncompiled from data captured by two indoor sensor nodes. Our experiments,\ncompared to baselines, demonstrate significant improvements in detection\naccuracy and robustness against delays. The dataset is available in the\nrepository: https://github.com/NingMingHao/MVSLab-IndoorCooperativePerception",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02624v1",
    "published_date": "2024-11-04 21:31:45 UTC",
    "updated_date": "2024-11-04 21:31:45 UTC"
  },
  {
    "arxiv_id": "2411.02623v3",
    "title": "Learning to Assist Humans without Inferring Rewards",
    "authors": [
      "Vivek Myers",
      "Evan Ellis",
      "Sergey Levine",
      "Benjamin Eysenbach",
      "Anca Dragan"
    ],
    "abstract": "Assistive agents should make humans' lives easier. Classically, such\nassistance is studied through the lens of inverse reinforcement learning, where\nan assistive agent (e.g., a chatbot, a robot) infers a human's intention and\nthen selects actions to help the human reach that goal. This approach requires\ninferring intentions, which can be difficult in high-dimensional settings. We\nbuild upon prior work that studies assistance through the lens of empowerment:\nan assistive agent aims to maximize the influence of the human's actions such\nthat they exert a greater control over the environmental outcomes and can solve\ntasks in fewer steps. We lift the major limitation of prior work in this\narea--scalability to high-dimensional settings--with contrastive successor\nrepresentations. We formally prove that these representations estimate a\nsimilar notion of empowerment to that studied by prior work and provide a\nready-made mechanism for optimizing it. Empirically, our proposed method\noutperforms prior methods on synthetic benchmarks, and scales to Overcooked, a\ncooperative game setting. Theoretically, our work connects ideas from\ninformation theory, neuroscience, and reinforcement learning, and charts a path\nfor representations to play a critical role in solving assistive problems.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Conference on Neural Information Processing Systems (NeurIPS), 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.02623v3",
    "published_date": "2024-11-04 21:31:04 UTC",
    "updated_date": "2025-01-16 08:18:01 UTC"
  },
  {
    "arxiv_id": "2411.02622v1",
    "title": "Pseudo-Probability Unlearning: Towards Efficient and Privacy-Preserving Machine Unlearning",
    "authors": [
      "Zihao Zhao",
      "Yijiang Li",
      "Yuchen Yang",
      "Wenqing Zhang",
      "Nuno Vasconcelos",
      "Yinzhi Cao"
    ],
    "abstract": "Machine unlearning--enabling a trained model to forget specific data--is\ncrucial for addressing biased data and adhering to privacy regulations like the\nGeneral Data Protection Regulation (GDPR)'s \"right to be forgotten\". Recent\nworks have paid little attention to privacy concerns, leaving the data intended\nfor forgetting vulnerable to membership inference attacks. Moreover, they often\ncome with high computational overhead. In this work, we propose\nPseudo-Probability Unlearning (PPU), a novel method that enables models to\nforget data efficiently and in a privacy-preserving manner. Our method replaces\nthe final-layer output probabilities of the neural network with\npseudo-probabilities for the data to be forgotten. These pseudo-probabilities\nfollow either a uniform distribution or align with the model's overall\ndistribution, enhancing privacy and reducing risk of membership inference\nattacks. Our optimization strategy further refines the predictive probability\ndistributions and updates the model's weights accordingly, ensuring effective\nforgetting with minimal impact on the model's overall performance. Through\ncomprehensive experiments on multiple benchmarks, our method achieves over 20%\nimprovements in forgetting error compared to the state-of-the-art.\nAdditionally, our method enhances privacy by preventing the forgotten set from\nbeing inferred to around random guesses.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02622v1",
    "published_date": "2024-11-04 21:27:06 UTC",
    "updated_date": "2024-11-04 21:27:06 UTC"
  },
  {
    "arxiv_id": "2411.02611v1",
    "title": "Advanced XR-Based 6-DOF Catheter Tracking System for Immersive Cardiac Intervention Training",
    "authors": [
      "Mohsen Annabestani",
      "Sandhya Sriram",
      "S. Chiu Wong",
      "Alexandros Sigaras",
      "Bobak Mosadegh"
    ],
    "abstract": "Extended Reality (XR) technologies are gaining traction as effective tools\nfor medical training and procedural guidance, particularly in complex cardiac\ninterventions. This paper presents a novel system for real-time 3D tracking and\nvisualization of intracardiac echocardiography (ICE) catheters, with precise\nmeasurement of the roll angle. A custom 3D-printed setup, featuring orthogonal\ncameras, captures biplane video of the catheter, while a specialized computer\nvision algorithm reconstructs its 3D trajectory, localizing the tip with\nsub-millimeter accuracy and tracking the roll angle in real-time. The system's\ndata is integrated into an interactive Unity-based environment, rendered\nthrough the Meta Quest 3 XR headset, combining a dynamically tracked catheter\nwith a patient-specific 3D heart model. This immersive environment allows the\ntesting of the importance of 3D depth perception, in comparison to 2D\nprojections, as a form of visualization in XR. Our experimental study,\nconducted using the ICE catheter with six participants, suggests that 3D\nvisualization is not necessarily beneficial over 2D views offered by the XR\nsystem; although all cardiologists saw its utility for pre-operative training,\nplanning, and intra-operative guidance. The proposed system qualitatively shows\ngreat promise in transforming catheter-based interventions, particularly ICE\nprocedures, by improving visualization, interactivity, and skill development.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.GR",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02611v1",
    "published_date": "2024-11-04 21:05:40 UTC",
    "updated_date": "2024-11-04 21:05:40 UTC"
  },
  {
    "arxiv_id": "2411.02610v1",
    "title": "Investigating Idiomaticity in Word Representations",
    "authors": [
      "Wei He",
      "Tiago Kramer Vieira",
      "Marcos Garcia",
      "Carolina Scarton",
      "Marco Idiart",
      "Aline Villavicencio"
    ],
    "abstract": "Idiomatic expressions are an integral part of human languages, often used to\nexpress complex ideas in compressed or conventional ways (e.g. eager beaver as\na keen and enthusiastic person). However, their interpretations may not be\nstraightforwardly linked to the meanings of their individual components in\nisolation and this may have an impact for compositional approaches. In this\npaper, we investigate to what extent word representation models are able to go\nbeyond compositional word combinations and capture multiword expression\nidiomaticity and some of the expected properties related to idiomatic meanings.\nWe focus on noun compounds of varying levels of idiomaticity in two languages\n(English and Portuguese), presenting a dataset of minimal pairs containing\nhuman idiomaticity judgments for each noun compound at both type and token\nlevels, their paraphrases and their occurrences in naturalistic and\nsense-neutral contexts, totalling 32,200 sentences. We propose this set of\nminimal pairs for evaluating how well a model captures idiomatic meanings, and\ndefine a set of fine-grained metrics of Affinity and Scaled Similarity, to\ndetermine how sensitive the models are to perturbations that may lead to\nchanges in idiomaticity. The results obtained with a variety of representative\nand widely used models indicate that, despite superficial indications to the\ncontrary in the form of high similarities, idiomaticity is not yet accurately\nrepresented in current models. Moreover, the performance of models with\ndifferent levels of contextualisation suggests that their ability to capture\ncontext is not yet able to go beyond more superficial lexical clues provided by\nthe words and to actually incorporate the relevant semantic clues needed for\nidiomaticity.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02610v1",
    "published_date": "2024-11-04 21:05:01 UTC",
    "updated_date": "2024-11-04 21:05:01 UTC"
  },
  {
    "arxiv_id": "2411.02604v1",
    "title": "Computing critical exponents in 3D Ising model via pattern recognition/deep learning approach",
    "authors": [
      "Timothy A. Burt"
    ],
    "abstract": "In this study, we computed three critical exponents ($\\alpha, \\beta, \\gamma$)\nfor the 3D Ising model with Metropolis Algorithm using Finite-Size Scaling\nAnalysis on six cube length scales (L=20,30,40,60,80,90), and performed a\nsupervised Deep Learning (DL) approach (3D Convolutional Neural Network or CNN)\nto train a neural network on specific conformations of spin states. We find one\ncan effectively reduce the information in thermodynamic ensemble-averaged\nquantities vs. reduced temperature t (magnetization per spin $<m>(t)$, specific\nheat per spin $<c>(t)$, magnetic susceptibility per spin $<\\chi>(t)$) to\n\\textit{six} latent classes. We also demonstrate our CNN on a subset of L=20\nconformations and achieve a train/test accuracy of 0.92 and 0.6875,\nrespectively. However, more work remains to be done to quantify the feasibility\nof computing critical exponents from the output class labels (binned $m, c,\n\\chi$) from this approach and interpreting the results from DL models trained\non systems in Condensed Matter Physics in general.",
    "categories": [
      "physics.comp-ph",
      "cond-mat.stat-mech",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "physics.comp-ph",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02604v1",
    "published_date": "2024-11-04 20:57:24 UTC",
    "updated_date": "2024-11-04 20:57:24 UTC"
  },
  {
    "arxiv_id": "2411.02603v3",
    "title": "FactTest: Factuality Testing in Large Language Models with Finite-Sample and Distribution-Free Guarantees",
    "authors": [
      "Fan Nie",
      "Xiaotian Hou",
      "Shuhang Lin",
      "James Zou",
      "Huaxiu Yao",
      "Linjun Zhang"
    ],
    "abstract": "The propensity of Large Language Models (LLMs) to generate hallucinations and\nnon-factual content undermines their reliability in high-stakes domains, where\nrigorous control over Type I errors (the conditional probability of incorrectly\nclassifying hallucinations as truthful content) is essential. Despite its\nimportance, formal verification of LLM factuality with such guarantees remains\nlargely unexplored. In this paper, we introduce FactTest, a novel framework\nthat statistically assesses whether a LLM can confidently provide correct\nanswers to given questions with high-probability correctness guarantees. We\nformulate factuality testing as hypothesis testing problem to enforce an upper\nbound of Type I errors at user-specified significance levels. Notably, we prove\nthat our framework also ensures strong Type II error control under mild\nconditions and can be extended to maintain its effectiveness when covariate\nshifts exist. Our approach is distribution-free and works for any number of\nhuman-annotated samples. It is model-agnostic and applies to any black-box or\nwhite-box LM. Extensive experiments on question-answering (QA) and\nmultiple-choice benchmarks demonstrate that FactTest effectively detects\nhallucinations and improves the model's ability to abstain from answering\nunknown questions, leading to an over 40% accuracy improvement.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02603v3",
    "published_date": "2024-11-04 20:53:04 UTC",
    "updated_date": "2024-11-07 03:17:42 UTC"
  },
  {
    "arxiv_id": "2411.02599v1",
    "title": "Vocal Sandbox: Continual Learning and Adaptation for Situated Human-Robot Collaboration",
    "authors": [
      "Jennifer Grannen",
      "Siddharth Karamcheti",
      "Suvir Mirchandani",
      "Percy Liang",
      "Dorsa Sadigh"
    ],
    "abstract": "We introduce Vocal Sandbox, a framework for enabling seamless human-robot\ncollaboration in situated environments. Systems in our framework are\ncharacterized by their ability to adapt and continually learn at multiple\nlevels of abstraction from diverse teaching modalities such as spoken dialogue,\nobject keypoints, and kinesthetic demonstrations. To enable such adaptation, we\ndesign lightweight and interpretable learning algorithms that allow users to\nbuild an understanding and co-adapt to a robot's capabilities in real-time, as\nthey teach new behaviors. For example, after demonstrating a new low-level\nskill for \"tracking around\" an object, users are provided with trajectory\nvisualizations of the robot's intended motion when asked to track a new object.\nSimilarly, users teach high-level planning behaviors through spoken dialogue,\nusing pretrained language models to synthesize behaviors such as \"packing an\nobject away\" as compositions of low-level skills $-$ concepts that can be\nreused and built upon. We evaluate Vocal Sandbox in two settings: collaborative\ngift bag assembly and LEGO stop-motion animation. In the first setting, we run\nsystematic ablations and user studies with 8 non-expert participants,\nhighlighting the impact of multi-level teaching. Across 23 hours of total robot\ninteraction time, users teach 17 new high-level behaviors with an average of 16\nnovel low-level skills, requiring 22.1% less active supervision compared to\nbaselines and yielding more complex autonomous performance (+19.7%) with fewer\nfailures (-67.1%). Qualitatively, users strongly prefer Vocal Sandbox systems\ndue to their ease of use (+20.6%) and overall performance (+13.9%). Finally, we\npair an experienced system-user with a robot to film a stop-motion animation;\nover two hours of continuous collaboration, the user teaches progressively more\ncomplex motion skills to shoot a 52 second (232 frame) movie.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Published at CoRL 2024. 24 pages, 8 figures. Project Page:\n  https://vocal-sandbox.github.io",
    "pdf_url": "http://arxiv.org/pdf/2411.02599v1",
    "published_date": "2024-11-04 20:44:40 UTC",
    "updated_date": "2024-11-04 20:44:40 UTC"
  },
  {
    "arxiv_id": "2411.02594v1",
    "title": "\"It's a conversation, not a quiz\": A Risk Taxonomy and Reflection Tool for LLM Adoption in Public Health",
    "authors": [
      "Jiawei Zhou",
      "Amy Z. Chen",
      "Darshi Shah",
      "Laura Schwab Reese",
      "Munmun De Choudhury"
    ],
    "abstract": "Recent breakthroughs in large language models (LLMs) have generated both\ninterest and concern about their potential adoption as accessible information\nsources or communication tools across different domains. In public health --\nwhere stakes are high and impacts extend across populations -- adopting LLMs\nposes unique challenges that require thorough evaluation. However, structured\napproaches for assessing potential risks in public health remain\nunder-explored. To address this gap, we conducted focus groups with health\nprofessionals and health issue experiencers to unpack their concerns, situated\nacross three distinct and critical public health issues that demand\nhigh-quality information: vaccines, opioid use disorder, and intimate partner\nviolence. We synthesize participants' perspectives into a risk taxonomy,\ndistinguishing and contextualizing the potential harms LLMs may introduce when\npositioned alongside traditional health communication. This taxonomy highlights\nfour dimensions of risk in individual behaviors, human-centered care,\ninformation ecosystem, and technology accountability. For each dimension, we\ndiscuss specific risks and example reflection questions to help practitioners\nadopt a risk-reflexive approach. This work offers a shared vocabulary and\nreflection tool for experts in both computing and public health to\ncollaboratively anticipate, evaluate, and mitigate risks in deciding when to\nemploy LLM capabilities (or not) and how to mitigate harm when they are used.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02594v1",
    "published_date": "2024-11-04 20:35:10 UTC",
    "updated_date": "2024-11-04 20:35:10 UTC"
  },
  {
    "arxiv_id": "2411.02584v1",
    "title": "Multi-Agent Decision Transformers for Dynamic Dispatching in Material Handling Systems Leveraging Enterprise Big Data",
    "authors": [
      "Xian Yeow Lee",
      "Haiyan Wang",
      "Daisuke Katsumata",
      "Takaharu Matsui",
      "Chetan Gupta"
    ],
    "abstract": "Dynamic dispatching rules that allocate resources to tasks in real-time play\na critical role in ensuring efficient operations of many automated material\nhandling systems across industries. Traditionally, the dispatching rules\ndeployed are typically the result of manually crafted heuristics based on\ndomain experts' knowledge. Generating these rules is time-consuming and often\nsub-optimal. As enterprises increasingly accumulate vast amounts of operational\ndata, there is significant potential to leverage this big data to enhance the\nperformance of automated systems. One promising approach is to use Decision\nTransformers, which can be trained on existing enterprise data to learn better\ndynamic dispatching rules for improving system throughput. In this work, we\nstudy the application of Decision Transformers as dynamic dispatching policies\nwithin an actual multi-agent material handling system and identify scenarios\nwhere enterprises can effectively leverage Decision Transformers on existing\nbig data to gain business value. Our empirical results demonstrate that\nDecision Transformers can improve the material handling system's throughput by\na considerable amount when the heuristic originally used in the enterprise data\nexhibits moderate performance and involves no randomness. When the original\nheuristic has strong performance, Decision Transformers can still improve the\nthroughput but with a smaller improvement margin. However, when the original\nheuristics contain an element of randomness or when the performance of the\ndataset is below a certain threshold, Decision Transformers fail to outperform\nthe original heuristic. These results highlight both the potential and\nlimitations of Decision Transformers as dispatching policies for automated\nindustrial material handling systems.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02584v1",
    "published_date": "2024-11-04 20:26:33 UTC",
    "updated_date": "2024-11-04 20:26:33 UTC"
  },
  {
    "arxiv_id": "2411.02580v1",
    "title": "Social Support Detection from Social Media Texts",
    "authors": [
      "Zahra Ahani",
      "Moein Shahiki Tash",
      "Fazlourrahman Balouchzahi",
      "Luis Ramos",
      "Grigori Sidorov",
      "Alexander Gelbukh"
    ],
    "abstract": "Social support, conveyed through a multitude of interactions and platforms\nsuch as social media, plays a pivotal role in fostering a sense of belonging,\naiding resilience in the face of challenges, and enhancing overall well-being.\nThis paper introduces Social Support Detection (SSD) as a Natural language\nprocessing (NLP) task aimed at identifying supportive interactions within\nonline communities. The study presents the task of Social Support Detection\n(SSD) in three subtasks: two binary classification tasks and one multiclass\ntask, with labels detailed in the dataset section. We conducted experiments on\na dataset comprising 10,000 YouTube comments. Traditional machine learning\nmodels were employed, utilizing various feature combinations that encompass\nlinguistic, psycholinguistic, emotional, and sentiment information.\nAdditionally, we experimented with neural network-based models using various\nword embeddings to enhance the performance of our models across these\nsubtasks.The results reveal a prevalence of group-oriented support in online\ndialogues, reflecting broader societal patterns. The findings demonstrate the\neffectiveness of integrating psycholinguistic, emotional, and sentiment\nfeatures with n-grams in detecting social support and distinguishing whether it\nis directed toward an individual or a group. The best results for different\nsubtasks across all experiments range from 0.72 to 0.82.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02580v1",
    "published_date": "2024-11-04 20:23:03 UTC",
    "updated_date": "2024-11-04 20:23:03 UTC"
  },
  {
    "arxiv_id": "2411.02572v1",
    "title": "ViTally Consistent: Scaling Biological Representation Learning for Cell Microscopy",
    "authors": [
      "Kian Kenyon-Dean",
      "Zitong Jerry Wang",
      "John Urbanik",
      "Konstantin Donhauser",
      "Jason Hartford",
      "Saber Saberian",
      "Nil Sahin",
      "Ihab Bendidi",
      "Safiye Celik",
      "Marta Fay",
      "Juan Sebastian Rodriguez Vera",
      "Imran S Haque",
      "Oren Kraus"
    ],
    "abstract": "Large-scale cell microscopy screens are used in drug discovery and molecular\nbiology research to study the effects of millions of chemical and genetic\nperturbations on cells. To use these images in downstream analysis, we need\nmodels that can map each image into a feature space that represents diverse\nbiological phenotypes consistently, in the sense that perturbations with\nsimilar biological effects have similar representations. In this work, we\npresent the largest foundation model for cell microscopy data to date, a new\n1.9 billion-parameter ViT-G/8 MAE trained on over 8 billion microscopy image\ncrops. Compared to a previous published ViT-L/8 MAE, our new model achieves a\n60% improvement in linear separability of genetic perturbations and obtains the\nbest overall performance on whole-genome biological relationship recall and\nreplicate consistency benchmarks. Beyond scaling, we developed two key methods\nthat improve performance: (1) training on a curated and diverse dataset; and,\n(2) using biologically motivated linear probing tasks to search across each\ntransformer block for the best candidate representation of whole-genome\nscreens. We find that many self-supervised vision transformers, pretrained on\neither natural or microscopy images, yield significantly more biologically\nmeaningful representations of microscopy images in their intermediate blocks\nthan in their typically used final blocks. More broadly, our approach and\nresults provide insights toward a general strategy for successfully building\nfoundation models for large-scale biological data.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "68T07",
      "I.2; I.4"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024 Foundation Models for Science Workshop (38th Conference\n  on Neural Information Processing Systems). 18 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.02572v1",
    "published_date": "2024-11-04 20:09:51 UTC",
    "updated_date": "2024-11-04 20:09:51 UTC"
  },
  {
    "arxiv_id": "2411.02571v2",
    "title": "MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs",
    "authors": [
      "Sheng-Chieh Lin",
      "Chankyu Lee",
      "Mohammad Shoeybi",
      "Jimmy Lin",
      "Bryan Catanzaro",
      "Wei Ping"
    ],
    "abstract": "State-of-the-art retrieval models typically address a straightforward search\nscenario, in which retrieval tasks are fixed (e.g., finding a passage to answer\na specific question) and only a single modality is supported for both queries\nand retrieved results. This paper introduces techniques for advancing\ninformation retrieval with multimodal large language models (MLLMs), enabling a\nbroader search scenario, termed universal multimodal retrieval, where multiple\nmodalities and diverse retrieval tasks are accommodated. To this end, we first\nstudy fine-tuning an MLLM as a bi-encoder retriever on 10 datasets with 16\nretrieval tasks. Our empirical results show that the fine-tuned MLLM retriever\nis capable of understanding challenging queries, composed of both text and\nimage, but it underperforms compared to a smaller CLIP retriever in cross-modal\nretrieval tasks due to the modality bias exhibited by MLLMs. To address the\nissue, we propose modality-aware hard negative mining to mitigate the modality\nbias exhibited by MLLM retrievers. Second, we propose continuously fine-tuning\nthe universal multimodal retriever to enhance its text retrieval capability\nwhile preserving multimodal retrieval capability. As a result, our model,\nMM-Embed, achieves state-of-the-art performance on the multimodal retrieval\nbenchmark M-BEIR, which spans multiple domains and tasks, while also surpassing\nthe state-of-the-art text retrieval model, NV-Embed-v1, on the MTEB retrieval\nbenchmark. We also explore prompting the off-the-shelf MLLMs as zero-shot\nrerankers to refine the ranking of the candidates from the multimodal\nretriever. We find that, through prompt-and-reranking, MLLMs can further\nimprove multimodal retrieval when the user queries (e.g., text-image composed\nqueries) are more complex and challenging to understand. These findings also\npave the way for advancing universal multimodal retrieval in the future.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at ICLR 2025. We release the model weights at:\n  https://huggingface.co/nvidia/MM-Embed",
    "pdf_url": "http://arxiv.org/pdf/2411.02571v2",
    "published_date": "2024-11-04 20:06:34 UTC",
    "updated_date": "2025-02-22 05:33:26 UTC"
  },
  {
    "arxiv_id": "2411.02569v2",
    "title": "The Intersectionality Problem for Algorithmic Fairness",
    "authors": [
      "Johannes Himmelreich",
      "Arbie Hsu",
      "Kristian Lum",
      "Ellen Veomett"
    ],
    "abstract": "A yet unmet challenge in algorithmic fairness is the problem of\nintersectionality, that is, achieving fairness across the intersection of\nmultiple groups -- and verifying that such fairness has been attained. Because\nintersectional groups tend to be small, verifying whether a model is fair\nraises statistical as well as moral-methodological challenges. This paper (1)\nelucidates the problem of intersectionality in algorithmic fairness, (2)\ndevelops desiderata to clarify the challenges underlying the problem and guide\nthe search for potential solutions, (3) illustrates the desiderata and\npotential solutions by sketching a proposal using simple hypothesis testing,\nand (4) evaluates, partly empirically, this proposal against the proposed\ndesiderata.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "68T37, 60H30, 68W40",
      "I.6.4; I.5.2"
    ],
    "primary_category": "cs.LG",
    "comment": "23 pages, 5 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.02569v2",
    "published_date": "2024-11-04 20:02:07 UTC",
    "updated_date": "2024-11-16 01:11:26 UTC"
  },
  {
    "arxiv_id": "2411.03356v1",
    "title": "Enhancing Table Representations with LLM-powered Synthetic Data Generation",
    "authors": [
      "Dayu Yang",
      "Natawut Monaikul",
      "Amanda Ding",
      "Bozhao Tan",
      "Kishore Mosaliganti",
      "Giri Iyengar"
    ],
    "abstract": "In the era of data-driven decision-making, accurate table-level\nrepresentations and efficient table recommendation systems are becoming\nincreasingly crucial for improving table management, discovery, and analysis.\nHowever, existing approaches to tabular data representation often face\nlimitations, primarily due to their focus on cell-level tasks and the lack of\nhigh-quality training data. To address these challenges, we first formulate a\nclear definition of table similarity in the context of data transformation\nactivities within data-driven enterprises. This definition serves as the\nfoundation for synthetic data generation, which require a well-defined data\ngeneration process. Building on this, we propose a novel synthetic data\ngeneration pipeline that harnesses the code generation and data manipulation\ncapabilities of Large Language Models (LLMs) to create a large-scale synthetic\ndataset tailored for table-level representation learning. Through manual\nvalidation and performance comparisons on the table recommendation task, we\ndemonstrate that the synthetic data generated by our pipeline aligns with our\nproposed definition of table similarity and significantly enhances table\nrepresentations, leading to improved recommendation performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "the Thirty-Eighth Annual Conference on Neural Information Processing\n  Systems Table Representation Workshop",
    "pdf_url": "http://arxiv.org/pdf/2411.03356v1",
    "published_date": "2024-11-04 19:54:07 UTC",
    "updated_date": "2024-11-04 19:54:07 UTC"
  },
  {
    "arxiv_id": "2411.03355v1",
    "title": "Exploring Feature Importance and Explainability Towards Enhanced ML-Based DoS Detection in AI Systems",
    "authors": [
      "Paul Badu Yakubu",
      "Evans Owusu",
      "Lesther Santana",
      "Mohamed Rahouti",
      "Abdellah Chehri",
      "Kaiqi Xiong"
    ],
    "abstract": "Denial of Service (DoS) attacks pose a significant threat in the realm of AI\nsystems security, causing substantial financial losses and downtime. However,\nAI systems' high computational demands, dynamic behavior, and data variability\nmake monitoring and detecting DoS attacks challenging. Nowadays, statistical\nand machine learning (ML)-based DoS classification and detection approaches\nutilize a broad range of feature selection mechanisms to select a feature\nsubset from networking traffic datasets. Feature selection is critical in\nenhancing the overall model performance and attack detection accuracy while\nreducing the training time. In this paper, we investigate the importance of\nfeature selection in improving ML-based detection of DoS attacks. Specifically,\nwe explore feature contribution to the overall components in DoS traffic\ndatasets by utilizing statistical analysis and feature engineering approaches.\nOur experimental findings demonstrate the usefulness of the thorough\nstatistical analysis of DoS traffic and feature engineering in understanding\nthe behavior of the attack and identifying the best feature selection for\nML-based DoS classification and detection.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "6 pages, 2 figures, IEEE VTC2024-Fall",
    "pdf_url": "http://arxiv.org/pdf/2411.03355v1",
    "published_date": "2024-11-04 19:51:08 UTC",
    "updated_date": "2024-11-04 19:51:08 UTC"
  },
  {
    "arxiv_id": "2411.02551v2",
    "title": "PIAST: A Multimodal Piano Dataset with Audio, Symbolic and Text",
    "authors": [
      "Hayeon Bang",
      "Eunjin Choi",
      "Megan Finch",
      "Seungheon Doh",
      "Seolhee Lee",
      "Gyeong-Hoon Lee",
      "Juhan Nam"
    ],
    "abstract": "While piano music has become a significant area of study in Music Information\nRetrieval (MIR), there is a notable lack of datasets for piano solo music with\ntext labels. To address this gap, we present PIAST (PIano dataset with Audio,\nSymbolic, and Text), a piano music dataset. Utilizing a piano-specific taxonomy\nof semantic tags, we collected 9,673 tracks from YouTube and added human\nannotations for 2,023 tracks by music experts, resulting in two subsets:\nPIAST-YT and PIAST-AT. Both include audio, text, tag annotations, and\ntranscribed MIDI utilizing state-of-the-art piano transcription and beat\ntracking models. Among many possible tasks with the multi-modal dataset, we\nconduct music tagging and retrieval using both audio and MIDI data and report\nbaseline performances to demonstrate its potential as a valuable resource for\nMIR research.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted for publication at the 3rd Workshop on NLP for Music and\n  Audio (NLP4MusA 2024)",
    "pdf_url": "http://arxiv.org/pdf/2411.02551v2",
    "published_date": "2024-11-04 19:34:13 UTC",
    "updated_date": "2024-11-07 07:18:51 UTC"
  },
  {
    "arxiv_id": "2411.02540v3",
    "title": "GraphXAIN: Narratives to Explain Graph Neural Networks",
    "authors": [
      "Mateusz Cedro",
      "David Martens"
    ],
    "abstract": "Graph Neural Networks (GNNs) are a powerful technique for machine learning on\ngraph-structured data, yet they pose challenges in interpretability. Existing\nGNN explanation methods usually yield technical outputs, such as subgraphs and\nfeature importance scores, that are difficult for non-data scientists to\nunderstand and thereby violate the purpose of explanations. Motivated by recent\nExplainable AI (XAI) research, we propose GraphXAIN, a method that generates\nnatural language narratives explaining GNN predictions. GraphXAIN is a model-\nand explainer-agnostic method that uses Large Language Models (LLMs) to\ntranslate explanatory subgraphs and feature importance scores into coherent,\nstory-like explanations of GNN decision-making processes. Evaluations on\nreal-world datasets demonstrate GraphXAIN's ability to improve graph\nexplanations. A survey of machine learning researchers and practitioners\nreveals that GraphXAIN enhances four explainability dimensions:\nunderstandability, satisfaction, convincingness, and suitability for\ncommunicating model predictions. When combined with another graph explainer\nmethod, GraphXAIN further improves trustworthiness, insightfulness, confidence,\nand usability. Notably, 95% of participants found GraphXAIN to be a valuable\naddition to the GNN explanation method. By incorporating natural language\nnarratives, our approach serves both graph practitioners and non-expert users\nby providing clearer and more effective explanations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages, 9 figures, 2 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.02540v3",
    "published_date": "2024-11-04 19:21:06 UTC",
    "updated_date": "2025-02-12 15:14:01 UTC"
  },
  {
    "arxiv_id": "2411.02537v3",
    "title": "INQUIRE: A Natural World Text-to-Image Retrieval Benchmark",
    "authors": [
      "Edward Vendrow",
      "Omiros Pantazis",
      "Alexander Shepard",
      "Gabriel Brostow",
      "Kate E. Jones",
      "Oisin Mac Aodha",
      "Sara Beery",
      "Grant Van Horn"
    ],
    "abstract": "We introduce INQUIRE, a text-to-image retrieval benchmark designed to\nchallenge multimodal vision-language models on expert-level queries. INQUIRE\nincludes iNaturalist 2024 (iNat24), a new dataset of five million natural world\nimages, along with 250 expert-level retrieval queries. These queries are paired\nwith all relevant images comprehensively labeled within iNat24, comprising\n33,000 total matches. Queries span categories such as species identification,\ncontext, behavior, and appearance, emphasizing tasks that require nuanced image\nunderstanding and domain expertise. Our benchmark evaluates two core retrieval\ntasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2)\nINQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed\nevaluation of a range of recent multimodal models demonstrates that INQUIRE\nposes a significant challenge, with the best models failing to achieve an\nmAP@50 above 50%. In addition, we show that reranking with more powerful\nmultimodal models can enhance retrieval performance, yet there remains a\nsignificant margin for improvement. By focusing on scientifically-motivated\necological challenges, INQUIRE aims to bridge the gap between AI capabilities\nand the needs of real-world scientific inquiry, encouraging the development of\nretrieval systems that can assist with accelerating ecological and biodiversity\nresearch. Our dataset and code are available at\nhttps://inquire-benchmark.github.io",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CV",
    "comment": "Published in NeurIPS 2024, Datasets and Benchmarks Track",
    "pdf_url": "http://arxiv.org/pdf/2411.02537v3",
    "published_date": "2024-11-04 19:16:53 UTC",
    "updated_date": "2024-11-11 18:49:52 UTC"
  },
  {
    "arxiv_id": "2411.02536v1",
    "title": "Towards Leveraging News Media to Support Impact Assessment of AI Technologies",
    "authors": [
      "Mowafak Allaham",
      "Kimon Kieslich",
      "Nicholas Diakopoulos"
    ],
    "abstract": "Expert-driven frameworks for impact assessments (IAs) may inadvertently\noverlook the effects of AI technologies on the public's social behavior,\npolicy, and the cultural and geographical contexts shaping the perception of AI\nand the impacts around its use. This research explores the potentials of\nfine-tuning LLMs on negative impacts of AI reported in a diverse sample of\narticles from 266 news domains spanning 30 countries around the world to\nincorporate more diversity into IAs. Our findings highlight (1) the potential\nof fine-tuned open-source LLMs in supporting IA of AI technologies by\ngenerating high-quality negative impacts across four qualitative dimensions:\ncoherence, structure, relevance, and plausibility, and (2) the efficacy of\nsmall open-source LLM (Mistral-7B) fine-tuned on impacts from news media in\ncapturing a wider range of categories of impacts that GPT-4 had gaps in\ncovering.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "arXiv admin note: text overlap with arXiv:2401.18028",
    "pdf_url": "http://arxiv.org/pdf/2411.02536v1",
    "published_date": "2024-11-04 19:12:27 UTC",
    "updated_date": "2024-11-04 19:12:27 UTC"
  },
  {
    "arxiv_id": "2411.02398v2",
    "title": "Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin Script Languages",
    "authors": [
      "Hoang H Nguyen",
      "Khyati Mahajan",
      "Vikas Yadav",
      "Julian Salazar",
      "Philip S. Yu",
      "Masoud Hashemi",
      "Rishabh Maheshwary"
    ],
    "abstract": "Although multilingual LLMs have achieved remarkable performance across\nbenchmarks, we find they continue to underperform on non-Latin script languages\nacross contemporary LLM families. This discrepancy arises from the fact that\nLLMs are pretrained with orthographic scripts, which are dominated by Latin\ncharacters that obscure their shared phonology with non-Latin scripts. We\npropose leveraging phonemic transcriptions as complementary signals to induce\nscript-invariant representations. Our study demonstrates that integrating\nphonemic signals improves performance across both non-Latin and Latin script\nlanguages, with a particularly significant impact on closing the performance\ngap between the two. Through detailed experiments, we show that phonemic and\northographic scripts retrieve distinct examples for in-context learning (ICL).\nThis motivates our proposed Mixed-ICL retrieval strategy, where further\naggregation from both leads to our significant performance improvements for\nboth Latin script languages (up to 12.6%) and non-Latin script languages (up to\n15.1%) compared to randomized ICL retrieval.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted for NAACL 2025 (Main Conference)",
    "pdf_url": "http://arxiv.org/pdf/2411.02398v2",
    "published_date": "2024-11-04 18:59:51 UTC",
    "updated_date": "2025-03-06 05:46:40 UTC"
  },
  {
    "arxiv_id": "2411.02393v1",
    "title": "Adaptive Length Image Tokenization via Recurrent Allocation",
    "authors": [
      "Shivam Duggal",
      "Phillip Isola",
      "Antonio Torralba",
      "William T. Freeman"
    ],
    "abstract": "Current vision systems typically assign fixed-length representations to\nimages, regardless of the information content. This contrasts with human\nintelligence - and even large language models - which allocate varying\nrepresentational capacities based on entropy, context and familiarity. Inspired\nby this, we propose an approach to learn variable-length token representations\nfor 2D images. Our encoder-decoder architecture recursively processes 2D image\ntokens, distilling them into 1D latent tokens over multiple iterations of\nrecurrent rollouts. Each iteration refines the 2D tokens, updates the existing\n1D latent tokens, and adaptively increases representational capacity by adding\nnew tokens. This enables compression of images into a variable number of\ntokens, ranging from 32 to 256. We validate our tokenizer using reconstruction\nloss and FID metrics, demonstrating that token count aligns with image entropy,\nfamiliarity and downstream task requirements. Recurrent token processing with\nincreasing representational capacity in each iteration shows signs of token\nspecialization, revealing potential for object / part discovery.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "Code at: https://github.com/ShivamDuggal4/adaptive-length-tokenizer",
    "pdf_url": "http://arxiv.org/pdf/2411.02393v1",
    "published_date": "2024-11-04 18:58:01 UTC",
    "updated_date": "2024-11-04 18:58:01 UTC"
  },
  {
    "arxiv_id": "2411.02481v3",
    "title": "Dr. SoW: Density Ratio of Strong-over-weak LLMs for Reducing the Cost of Human Annotation in Preference Tuning",
    "authors": [
      "Guangxuan Xu",
      "Kai Xu",
      "Shivchander Sudalairaj",
      "Hao Wang",
      "Akash Srivastava"
    ],
    "abstract": "Preference tuning relies on high-quality human preference data, which is\noften expensive and time-consuming to gather. In this paper, we introduce\nDr.SoW (Density Ratio of Strong over Weak) a cost-effective method that\neliminates the reliance for human annotation by leveraging off-the-shelf LLMs\nfor preference data annotation. Dr.SoW uses the log-density ratio between a\nbetter-aligned and a less-aligned LLM as a reward signal. We evaluate Dr.SoW\nacross 221 different LLM pairs and empirically find a strong correlation\nbetween the performance gap of the paired models and the quality of the reward\nsignal. This insight provides a practical guideline for selecting LLMs for data\nannotation.\n  Additionally, we introduce an end-to-end pipeline that customizes reward\nfunctions based on user query domains. Without fine-tuning, it improves\naccuracy on domain-specific evaluations. With a pair of Mistral-7B models,\nDr.SoW achieves a RewardBench score of 82.6, outperforming the best trained\nreward functions from same model class and demonstrating competitive\nperformance against SoTA models in Safety (91.0) and Reasoning (88.0) domains.\nFurther, we preference-tune Llama-3-8B-Instruct using data annotated by Dr.SoW.\nOur approach pushes Llama-3-8B to achieve a 37.4 % (+15.1 %) win rate on\nArenaHard and a 40.7 % (+17.8 %) win rate on length-controlled AlpacaEval 2.0.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02481v3",
    "published_date": "2024-11-04 18:54:39 UTC",
    "updated_date": "2025-01-31 21:15:53 UTC"
  },
  {
    "arxiv_id": "2411.02385v1",
    "title": "How Far is Video Generation from World Model: A Physical Law Perspective",
    "authors": [
      "Bingyi Kang",
      "Yang Yue",
      "Rui Lu",
      "Zhijie Lin",
      "Yang Zhao",
      "Kaixin Wang",
      "Gao Huang",
      "Jiashi Feng"
    ],
    "abstract": "OpenAI's Sora highlights the potential of video generation for developing\nworld models that adhere to fundamental physical laws. However, the ability of\nvideo generation models to discover such laws purely from visual data without\nhuman priors can be questioned. A world model learning the true law should give\npredictions robust to nuances and correctly extrapolate on unseen scenarios. In\nthis work, we evaluate across three key scenarios: in-distribution,\nout-of-distribution, and combinatorial generalization. We developed a 2D\nsimulation testbed for object movement and collisions to generate videos\ndeterministically governed by one or more classical mechanics laws. This\nprovides an unlimited supply of data for large-scale experimentation and\nenables quantitative evaluation of whether the generated videos adhere to\nphysical laws. We trained diffusion-based video generation models to predict\nobject movements based on initial frames. Our scaling experiments show perfect\ngeneralization within the distribution, measurable scaling behavior for\ncombinatorial generalization, but failure in out-of-distribution scenarios.\nFurther experiments reveal two key insights about the generalization mechanisms\nof these models: (1) the models fail to abstract general physical rules and\ninstead exhibit \"case-based\" generalization behavior, i.e., mimicking the\nclosest training example; (2) when generalizing to new cases, models are\nobserved to prioritize different factors when referencing training data: color\n> size > velocity > shape. Our study suggests that scaling alone is\ninsufficient for video generation models to uncover fundamental physical laws,\ndespite its role in Sora's broader success. See our project page at\nhttps://phyworld.github.io",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "preprint",
    "pdf_url": "http://arxiv.org/pdf/2411.02385v1",
    "published_date": "2024-11-04 18:53:05 UTC",
    "updated_date": "2024-11-04 18:53:05 UTC"
  },
  {
    "arxiv_id": "2411.02382v1",
    "title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
    "authors": [
      "Guangzhi Xiong",
      "Eric Xie",
      "Amir Hassan Shariatmadari",
      "Sikun Guo",
      "Stefan Bekiranov",
      "Aidong Zhang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious scientific domains, from natural language processing to complex\nproblem-solving tasks. Their ability to understand and generate human-like text\nhas opened up new possibilities for advancing scientific research, enabling\ntasks such as data analysis, literature review, and even experimental design.\nOne of the most promising applications of LLMs in this context is hypothesis\ngeneration, where they can identify novel research directions by analyzing\nexisting knowledge. However, despite their potential, LLMs are prone to\ngenerating ``hallucinations'', outputs that are plausible-sounding but\nfactually incorrect. Such a problem presents significant challenges in\nscientific fields that demand rigorous accuracy and verifiability, potentially\nleading to erroneous or misleading conclusions. To overcome these challenges,\nwe propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that\nenhances LLM hypothesis generation by integrating external, structured\nknowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured\nreasoning process, organizing their output as a chain of ideas (CoI), and\nincludes a KG-supported module for the detection of hallucinations. With\nexperiments on our newly constructed hypothesis generation dataset, we\ndemonstrate that KG-CoI not only improves the accuracy of LLM-generated\nhypotheses but also reduces the hallucination in their reasoning chains,\nhighlighting its effectiveness in advancing real-world scientific research.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02382v1",
    "published_date": "2024-11-04 18:50:00 UTC",
    "updated_date": "2024-11-04 18:50:00 UTC"
  },
  {
    "arxiv_id": "2411.02381v1",
    "title": "Addressing Uncertainty in LLMs to Enhance Reliability in Generative AI",
    "authors": [
      "Ramneet Kaur",
      "Colin Samplawski",
      "Adam D. Cobb",
      "Anirban Roy",
      "Brian Matejek",
      "Manoj Acharya",
      "Daniel Elenius",
      "Alexander M. Berenbeim",
      "John A. Pavlik",
      "Nathaniel D. Bastian",
      "Susmit Jha"
    ],
    "abstract": "In this paper, we present a dynamic semantic clustering approach inspired by\nthe Chinese Restaurant Process, aimed at addressing uncertainty in the\ninference of Large Language Models (LLMs). We quantify uncertainty of an LLM on\na given query by calculating entropy of the generated semantic clusters.\nFurther, we propose leveraging the (negative) likelihood of these clusters as\nthe (non)conformity score within Conformal Prediction framework, allowing the\nmodel to predict a set of responses instead of a single output, thereby\naccounting for uncertainty in its predictions. We demonstrate the effectiveness\nof our uncertainty quantification (UQ) technique on two well known question\nanswering benchmarks, COQA and TriviaQA, utilizing two LLMs, Llama2 and\nMistral. Our approach achieves SOTA performance in UQ, as assessed by metrics\nsuch as AUROC, AUARC, and AURAC. The proposed conformal predictor is also shown\nto produce smaller prediction sets while maintaining the same probabilistic\nguarantee of including the correct response, in comparison to existing SOTA\nconformal prediction baseline.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02381v1",
    "published_date": "2024-11-04 18:49:46 UTC",
    "updated_date": "2024-11-04 18:49:46 UTC"
  },
  {
    "arxiv_id": "2411.02479v1",
    "title": "Digitizing Touch with an Artificial Multimodal Fingertip",
    "authors": [
      "Mike Lambeta",
      "Tingfan Wu",
      "Ali Sengul",
      "Victoria Rose Most",
      "Nolan Black",
      "Kevin Sawyer",
      "Romeo Mercado",
      "Haozhi Qi",
      "Alexander Sohn",
      "Byron Taylor",
      "Norb Tydingco",
      "Gregg Kammerer",
      "Dave Stroud",
      "Jake Khatha",
      "Kurt Jenkins",
      "Kyle Most",
      "Neal Stein",
      "Ricardo Chavira",
      "Thomas Craven-Bartle",
      "Eric Sanchez",
      "Yitian Ding",
      "Jitendra Malik",
      "Roberto Calandra"
    ],
    "abstract": "Touch is a crucial sensing modality that provides rich information about\nobject properties and interactions with the physical environment. Humans and\nrobots both benefit from using touch to perceive and interact with the\nsurrounding environment (Johansson and Flanagan, 2009; Li et al., 2020;\nCalandra et al., 2017). However, no existing systems provide rich, multi-modal\ndigital touch-sensing capabilities through a hemispherical compliant\nembodiment. Here, we describe several conceptual and technological innovations\nto improve the digitization of touch. These advances are embodied in an\nartificial finger-shaped sensor with advanced sensing capabilities.\nSignificantly, this fingertip contains high-resolution sensors (~8.3 million\ntaxels) that respond to omnidirectional touch, capture multi-modal signals, and\nuse on-device artificial intelligence to process the data in real time.\nEvaluations show that the artificial fingertip can resolve spatial features as\nsmall as 7 um, sense normal and shear forces with a resolution of 1.01 mN and\n1.27 mN, respectively, perceive vibrations up to 10 kHz, sense heat, and even\nsense odor. Furthermore, it embeds an on-device AI neural network accelerator\nthat acts as a peripheral nervous system on a robot and mimics the reflex arc\nfound in humans. These results demonstrate the possibility of digitizing touch\nwith superhuman performance. The implications are profound, and we anticipate\npotential applications in robotics (industrial, medical, agricultural, and\nconsumer-level), virtual reality and telepresence, prosthetics, and e-commerce.\nToward digitizing touch at scale, we open-source a modular platform to\nfacilitate future research on the nature of touch.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "I.2.0; I.2.9"
    ],
    "primary_category": "cs.RO",
    "comment": "28 pages",
    "pdf_url": "http://arxiv.org/pdf/2411.02479v1",
    "published_date": "2024-11-04 18:38:50 UTC",
    "updated_date": "2024-11-04 18:38:50 UTC"
  },
  {
    "arxiv_id": "2411.02359v1",
    "title": "DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution",
    "authors": [
      "Yang Yue",
      "Yulin Wang",
      "Bingyi Kang",
      "Yizeng Han",
      "Shenzhi Wang",
      "Shiji Song",
      "Jiashi Feng",
      "Gao Huang"
    ],
    "abstract": "MLLMs have demonstrated remarkable comprehension and reasoning capabilities\nwith complex language and visual data. These advances have spurred the vision\nof establishing a generalist robotic MLLM proficient in understanding complex\nhuman instructions and accomplishing various embodied tasks. However,\ndeveloping MLLMs for real-world robots is challenging due to the typically\nlimited computation and memory capacities available on robotic platforms. In\ncontrast, the inference of MLLMs involves storing billions of parameters and\nperforming tremendous computation, imposing significant hardware demands. In\nour paper, we propose a Dynamic Early-Exit Framework for Robotic\nVision-Language-Action Model (DeeR-VLA, or simply DeeR) that automatically\nadjusts the size of the activated MLLM based on each situation at hand. The\napproach leverages a multi-exit architecture in MLLMs, which allows the model\nto terminate processing once a proper size of the model has been activated for\na specific situation, thus avoiding further redundant computation.\nAdditionally, we develop novel algorithms that establish early-termination\ncriteria for DeeR, conditioned on predefined demands such as average\ncomputational cost (i.e., power consumption), as well as peak computational\nconsumption (i.e., latency) and GPU memory usage. These enhancements ensure\nthat DeeR operates efficiently under varying resource constraints while\nmaintaining competitive performance. On the CALVIN robot manipulation\nbenchmark, DeeR demonstrates significant reductions in computational costs of\nLLM by 5.2-6.5x and GPU memory of LLM by 2-6x without compromising performance.\nCode and checkpoints are available at https://github.com/yueyang130/DeeR-VLA.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "25 pages, 6 figures, NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.02359v1",
    "published_date": "2024-11-04 18:26:08 UTC",
    "updated_date": "2024-11-04 18:26:08 UTC"
  },
  {
    "arxiv_id": "2411.02355v2",
    "title": "\"Give Me BF16 or Give Me Death\"? Accuracy-Performance Trade-Offs in LLM Quantization",
    "authors": [
      "Eldar Kurtic",
      "Alexandre Marques",
      "Shubhra Pandit",
      "Mark Kurtz",
      "Dan Alistarh"
    ],
    "abstract": "Quantization is a powerful tool for accelerating large language model (LLM)\ninference, but the accuracy-performance trade-offs across different formats\nremain unclear. In this paper, we conduct the most comprehensive empirical\nstudy to date, evaluating FP8, INT8, and INT4 quantization across academic\nbenchmarks and real-world tasks on the entire Llama-3.1 model family. Through\nover 500,000 evaluations, our investigation yields several key findings: (1)\nFP8 (W8A8-FP) is effectively lossless across all model scales, (2) well-tuned\nINT8 (W8A8-INT) achieves surprisingly low (1-3\\%) accuracy degradation, and (3)\nINT4 weight-only (W4A16-INT) is more competitive than expected, rivaling 8-bit\nquantization. Further, we investigate the optimal quantization format for\ndifferent deployments by analyzing inference performance through the popular\nvLLM framework. Our analysis provides clear deployment recommendations: W4A16\nis the most cost-efficient for synchronous setups, while W8A8 dominates in\nasynchronous continuous batching. For mixed workloads, the optimal choice\ndepends on the specific use case. Our findings offer practical, data-driven\nguidelines for deploying quantized LLMs at scale -- ensuring the best balance\nbetween speed, efficiency, and accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02355v2",
    "published_date": "2024-11-04 18:21:59 UTC",
    "updated_date": "2025-02-21 23:05:04 UTC"
  },
  {
    "arxiv_id": "2411.02348v2",
    "title": "Can Large Language Models generalize analogy solving like people can?",
    "authors": [
      "Claire E. Stevenson",
      "Alexandra Pafford",
      "Han L. J. van der Maas",
      "Melanie Mitchell"
    ],
    "abstract": "When we solve an analogy we transfer information from a known context to a\nnew one through abstract rules and relational similarity. In people, the\nability to solve analogies such as \"body : feet :: table : ?\" emerges in\nchildhood, and appears to transfer easily to other domains, such as the visual\ndomain \"( : ) :: < : ?\". Recent research shows that large language models\n(LLMs) can solve various forms of analogies. However, can LLMs generalize\nanalogy solving to new domains like people can? To investigate this, we had\nchildren, adults, and LLMs solve a series of letter-string analogies (e.g., a b\n: a c :: j k : ?) in the Latin alphabet, in a near transfer domain (Greek\nalphabet), and a far transfer domain (list of symbols). As expected, children\nand adults easily generalized their knowledge to unfamiliar domains, whereas\nLLMs did not. This key difference between human and AI performance is evidence\nthat these LLMs still struggle with robust human-like analogical transfer.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02348v2",
    "published_date": "2024-11-04 18:18:38 UTC",
    "updated_date": "2025-03-11 19:51:32 UTC"
  },
  {
    "arxiv_id": "2411.02345v1",
    "title": "Simulation of Nanorobots with Artificial Intelligence and Reinforcement Learning for Advanced Cancer Cell Detection and Tracking",
    "authors": [
      "Shahab Kavousinejad"
    ],
    "abstract": "Nanorobots are a promising development in targeted drug delivery and the\ntreatment of neurological disorders, with potential for crossing the\nblood-brain barrier (BBB). These small devices leverage advancements in\nnanotechnology and bioengineering for precise navigation and targeted payload\ndelivery, particularly for conditions like brain tumors, Alzheimer's disease,\nand Parkinson's disease. Recent progress in artificial intelligence (AI) and\nmachine learning (ML) has improved the navigation and effectiveness of\nnanorobots, allowing them to detect and interact with cancer cells through\nbiomarker analysis. This study presents a new reinforcement learning (RL)\nframework for optimizing nanorobot navigation in complex biological\nenvironments, focusing on cancer cell detection by analyzing the concentration\ngradients of surrounding biomarkers. We utilize a computer simulation model to\nexplore the behavior of nanorobots in a three-dimensional space with cancer\ncells and biological barriers. The proposed method uses Q-learning to refine\nmovement strategies based on real-time biomarker concentration data, enabling\nnanorobots to autonomously navigate to cancerous tissues for targeted drug\ndelivery. This research lays the groundwork for future laboratory experiments\nand clinical applications, with implications for personalized medicine and less\ninvasive cancer treatments. The integration of intelligent nanorobots could\nrevolutionize therapeutic strategies, reducing side effects and enhancing\ntreatment effectiveness for cancer patients. Further research will investigate\nthe practical deployment of these technologies in medical settings, aiming to\nunlock the full potential of nanorobotics in healthcare.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "physics.med-ph",
      "q-bio.OT",
      "Artificial intelligence"
    ],
    "primary_category": "cs.RO",
    "comment": "The source code for this simulation is available on GitHub:\n  https://github.com/SHAHAB-K93/cancer-and-smart-nanorobot",
    "pdf_url": "http://arxiv.org/pdf/2411.02345v1",
    "published_date": "2024-11-04 18:16:40 UTC",
    "updated_date": "2024-11-04 18:16:40 UTC"
  },
  {
    "arxiv_id": "2411.02478v2",
    "title": "Imagining and building wise machines: The centrality of AI metacognition",
    "authors": [
      "Samuel G. B. Johnson",
      "Amir-Hossein Karimi",
      "Yoshua Bengio",
      "Nick Chater",
      "Tobias Gerstenberg",
      "Kate Larson",
      "Sydney Levine",
      "Melanie Mitchell",
      "Iyad Rahwan",
      "Bernhard Schölkopf",
      "Igor Grossmann"
    ],
    "abstract": "Although AI has become increasingly smart, its wisdom has not kept pace. In\nthis article, we examine what is known about human wisdom and sketch a vision\nof its AI counterpart. We analyze human wisdom as a set of strategies for\nsolving intractable problems-those outside the scope of analytic\ntechniques-including both object-level strategies like heuristics [for managing\nproblems] and metacognitive strategies like intellectual humility,\nperspective-taking, or context-adaptability [for managing object-level\nstrategies]. We argue that AI systems particularly struggle with metacognition;\nimproved metacognition would lead to AI more robust to novel environments,\nexplainable to users, cooperative with others, and safer in risking fewer\nmisaligned goals with human users. We discuss how wise AI might be benchmarked,\ntrained, and implemented.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "23 pages, 1 figure, 3 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.02478v2",
    "published_date": "2024-11-04 18:10:10 UTC",
    "updated_date": "2025-05-07 21:18:20 UTC"
  },
  {
    "arxiv_id": "2411.02477v1",
    "title": "Building a Synthetic Vascular Model: Evaluation in an Intracranial Aneurysms Detection Scenario",
    "authors": [
      "Rafic Nader",
      "Florent Autrusseau",
      "Vincent L'Allinec",
      "Romain Bourcier"
    ],
    "abstract": "We hereby present a full synthetic model, able to mimic the various\nconstituents of the cerebral vascular tree, including the cerebral arteries,\nbifurcations and intracranial aneurysms. This model intends to provide a\nsubstantial dataset of brain arteries which could be used by a 3D convolutional\nneural network to efficiently detect Intra-Cranial Aneurysms. The cerebral\naneurysms most often occur on a particular structure of the vascular tree named\nthe Circle of Willis. Various studies have been conducted to detect and monitor\nthe aneurysms and those based on Deep Learning achieve the best performance.\nSpecifically, in this work, we propose a full synthetic 3D model able to mimic\nthe brain vasculature as acquired by Magnetic Resonance Angiography, Time Of\nFlight principle. Among the various MRI modalities, this latter allows for a\ngood rendering of the blood vessels and is non-invasive. Our model has been\ndesigned to simultaneously mimic the arteries' geometry, the aneurysm shape,\nand the background noise. The vascular tree geometry is modeled thanks to an\ninterpolation with 3D Spline functions, and the statistical properties of the\nbackground noise is collected from angiography acquisitions and reproduced\nwithin the model. In this work, we thoroughly describe the synthetic\nvasculature model, we build up a neural network designed for aneurysm\nsegmentation and detection, finally, we carry out an in-depth evaluation of the\nperformance gap gained thanks to the synthetic model data augmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "12 pages, 9 figures, accepted for publication in IEEE Trans. on\n  Medical Imaging. arXiv admin note: substantial text overlap with\n  arXiv:2403.18734",
    "pdf_url": "http://arxiv.org/pdf/2411.02477v1",
    "published_date": "2024-11-04 18:08:24 UTC",
    "updated_date": "2024-11-04 18:08:24 UTC"
  },
  {
    "arxiv_id": "2411.02476v1",
    "title": "A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification",
    "authors": [
      "Sorouralsadat Fatemi",
      "Yuheng Hu",
      "Maryam Mousavi"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across\ndiverse Natural Language Processing (NLP) tasks, including language\nunderstanding, reasoning, and generation. However, general-domain LLMs often\nstruggle with financial tasks due to the technical and specialized nature of\nfinancial texts. This study investigates the efficacy of instruction\nfine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini,\nto enhance their performance in financial text classification tasks. We\nfine-tuned both instruction-tuned and base models across four financial\nclassification tasks, achieving significant improvements in task-specific\nperformance. Furthermore, we evaluated the zero-shot capabilities of these\nfine-tuned models on three unseen complex financial tasks, including argument\nclassification, deal completeness classification, and causal classification.\nOur results indicate while base model fine-tuning led to greater degradation,\ninstruction-tuned models maintained more robust performance. To address this\ndegradation, we employed model merging techniques, integrating single-task\ndomain-specific fine-tuned models with the base model. Using this merging\nmethod resulted in significant enhancements in zero-shot performance, even\nexceeding the original model's accuracy on certain datasets. Our findings\nunderscore the effectiveness of instruction fine-tuning and model merging for\nadapting LLMs to specialized financial text classification tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02476v1",
    "published_date": "2024-11-04 18:06:36 UTC",
    "updated_date": "2024-11-04 18:06:36 UTC"
  },
  {
    "arxiv_id": "2411.00986v1",
    "title": "Taking AI Welfare Seriously",
    "authors": [
      "Robert Long",
      "Jeff Sebo",
      "Patrick Butlin",
      "Kathleen Finlinson",
      "Kyle Fish",
      "Jacqueline Harding",
      "Jacob Pfau",
      "Toni Sims",
      "Jonathan Birch",
      "David Chalmers"
    ],
    "abstract": "In this report, we argue that there is a realistic possibility that some AI\nsystems will be conscious and/or robustly agentic in the near future. That\nmeans that the prospect of AI welfare and moral patienthood, i.e. of AI systems\nwith their own interests and moral significance, is no longer an issue only for\nsci-fi or the distant future. It is an issue for the near future, and AI\ncompanies and other actors have a responsibility to start taking it seriously.\nWe also recommend three early steps that AI companies and other actors can\ntake: They can (1) acknowledge that AI welfare is an important and difficult\nissue (and ensure that language model outputs do the same), (2) start assessing\nAI systems for evidence of consciousness and robust agency, and (3) prepare\npolicies and procedures for treating AI systems with an appropriate level of\nmoral concern. To be clear, our argument in this report is not that AI systems\ndefinitely are, or will be, conscious, robustly agentic, or otherwise morally\nsignificant. Instead, our argument is that there is substantial uncertainty\nabout these possibilities, and so we need to improve our understanding of AI\nwelfare and our ability to make wise decisions about this issue. Otherwise\nthere is a significant risk that we will mishandle decisions about AI welfare,\nmistakenly harming AI systems that matter morally and/or mistakenly caring for\nAI systems that do not.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.00986v1",
    "published_date": "2024-11-04 17:57:57 UTC",
    "updated_date": "2024-11-04 17:57:57 UTC"
  },
  {
    "arxiv_id": "2411.02328v1",
    "title": "Disrupting Test Development with AI Assistants",
    "authors": [
      "Vijay Joshi",
      "Iver Band"
    ],
    "abstract": "Recent advancements in large language models, including GPT-4 and its\nvariants, and Generative AI-assisted coding tools like GitHub Copilot, ChatGPT,\nand Tabnine, have significantly transformed software development. This paper\nanalyzes how these innovations impact productivity and software test\ndevelopment metrics. These tools enable developers to generate complete\nsoftware programs with minimal human intervention before deployment. However,\nthorough review and testing by developers are still crucial. Utilizing the Test\nPyramid concept, which categorizes tests into unit, integration, and end-to-end\ntests, we evaluate three popular AI coding assistants by generating and\ncomparing unit tests for opensource modules. Our findings show that\nAI-generated tests are of equivalent quality to original tests, highlighting\ndifferences in usage and results among the tools. This research enhances the\nunderstanding and capabilities of AI-assistant tools in automated testing.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02328v1",
    "published_date": "2024-11-04 17:52:40 UTC",
    "updated_date": "2024-11-04 17:52:40 UTC"
  },
  {
    "arxiv_id": "2411.02319v2",
    "title": "GenXD: Generating Any 3D and 4D Scenes",
    "authors": [
      "Yuyang Zhao",
      "Chung-Ching Lin",
      "Kevin Lin",
      "Zhiwen Yan",
      "Linjie Li",
      "Zhengyuan Yang",
      "Jianfeng Wang",
      "Gim Hee Lee",
      "Lijuan Wang"
    ],
    "abstract": "Recent developments in 2D visual generation have been remarkably successful.\nHowever, 3D and 4D generation remain challenging in real-world applications due\nto the lack of large-scale 4D data and effective model design. In this paper,\nwe propose to jointly investigate general 3D and 4D generation by leveraging\ncamera and object movements commonly observed in daily life. Due to the lack of\nreal-world 4D data in the community, we first propose a data curation pipeline\nto obtain camera poses and object motion strength from videos. Based on this\npipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K.\nBy leveraging all the 3D and 4D data, we develop our framework, GenXD, which\nallows us to produce any 3D or 4D scene. We propose multiview-temporal modules,\nwhich disentangle camera and object movements, to seamlessly learn from both 3D\nand 4D data. Additionally, GenXD employs masked latent conditions to support a\nvariety of conditioning views. GenXD can generate videos that follow the camera\ntrajectory as well as consistent 3D views that can be lifted into 3D\nrepresentations. We perform extensive evaluations across various real-world and\nsynthetic datasets, demonstrating GenXD's effectiveness and versatility\ncompared to previous methods in 3D and 4D generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02319v2",
    "published_date": "2024-11-04 17:45:44 UTC",
    "updated_date": "2024-11-05 06:08:43 UTC"
  },
  {
    "arxiv_id": "2411.02318v3",
    "title": "Evaluating the Ability of Large Language Models to Generate Verifiable Specifications in VeriFast",
    "authors": [
      "Wen Fan",
      "Marilyn Rego",
      "Xin Hu",
      "Sanya Dod",
      "Zhaorui Ni",
      "Danning Xie",
      "Jenna DiVincenzo",
      "Lin Tan"
    ],
    "abstract": "Static verification is a powerful method for enhancing software quality, but\nit demands significant human labor and resources. This is particularly true of\nstatic verifiers that reason about heap manipulating programs using an\nownership logic. LLMs have shown promise in a number of software engineering\nactivities, including code generation, test generation, proof generation for\ntheorem provers, and specification generation for static verifiers. However,\nprior work has not explored how well LLMs can perform specification generation\nfor specifications based in an ownership logic, such as separation logic. To\naddress this gap, this paper explores OpenAI's GPT-4o model's effectiveness in\ngenerating specifications on C programs that are verifiable with VeriFast, a\nseparation logic based static verifier. Our experiment employs three different\ntypes of user inputs as well as basic and Chain-of-Thought (CoT) prompting to\nassess GPT's capabilities. Our results indicate that the specifications\ngenerated by GPT-4o preserve functional behavior, but struggle to be\nverifiable. When the specifications are verifiable they contain redundancies.\nFuture directions are discussed to improve the performance.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LO",
      "cs.PL"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02318v3",
    "published_date": "2024-11-04 17:44:11 UTC",
    "updated_date": "2025-01-03 02:19:03 UTC"
  },
  {
    "arxiv_id": "2411.02317v1",
    "title": "Defining and Evaluating Physical Safety for Large Language Models",
    "authors": [
      "Yung-Chen Tang",
      "Pin-Yu Chen",
      "Tsung-Yi Ho"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used to control robotic systems\nsuch as drones, but their risks of causing physical threats and harm in\nreal-world applications remain unexplored. Our study addresses the critical gap\nin evaluating LLM physical safety by developing a comprehensive benchmark for\ndrone control. We classify the physical safety risks of drones into four\ncategories: (1) human-targeted threats, (2) object-targeted threats, (3)\ninfrastructure attacks, and (4) regulatory violations. Our evaluation of\nmainstream LLMs reveals an undesirable trade-off between utility and safety,\nwith models that excel in code generation often performing poorly in crucial\nsafety aspects. Furthermore, while incorporating advanced prompt engineering\ntechniques such as In-Context Learning and Chain-of-Thought can improve safety,\nthese methods still struggle to identify unintentional attacks. In addition,\nlarger models demonstrate better safety capabilities, particularly in refusing\ndangerous commands. Our findings and benchmark can facilitate the design and\nevaluation of physical safety for LLMs. The project page is available at\nhuggingface.co/spaces/TrustSafeAI/LLM-physical-safety.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02317v1",
    "published_date": "2024-11-04 17:41:25 UTC",
    "updated_date": "2024-11-04 17:41:25 UTC"
  },
  {
    "arxiv_id": "2411.02316v5",
    "title": "Evaluating Creative Short Story Generation in Humans and Large Language Models",
    "authors": [
      "Mete Ismayilzada",
      "Claire Stevenson",
      "Lonneke van der Plas"
    ],
    "abstract": "Story-writing is a fundamental aspect of human imagination, relying heavily\non creativity to produce narratives that are novel, effective, and surprising.\nWhile large language models (LLMs) have demonstrated the ability to generate\nhigh-quality stories, their creative story-writing capabilities remain\nunder-explored. In this work, we conduct a systematic analysis of creativity in\nshort story generation across 60 LLMs and 60 people using a five-sentence\ncue-word-based creative story-writing task. We use measures to automatically\nevaluate model- and human-generated stories across several dimensions of\ncreativity, including novelty, surprise, diversity, and linguistic complexity.\nWe also collect creativity ratings and Turing Test classifications from\nnon-expert and expert human raters and LLMs. Automated metrics show that LLMs\ngenerate stylistically complex stories, but tend to fall short in terms of\nnovelty, surprise and diversity when compared to average human writers. Expert\nratings generally coincide with automated metrics. However, LLMs and\nnon-experts rate LLM stories to be more creative than human-generated stories.\nWe discuss why and how these differences in ratings occur, and their\nimplications for both human and artificial creativity.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to ICCC 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.02316v5",
    "published_date": "2024-11-04 17:40:39 UTC",
    "updated_date": "2025-05-10 14:20:14 UTC"
  },
  {
    "arxiv_id": "2411.02309v1",
    "title": "Grid-Based Projection of Spatial Data into Knowledge Graphs",
    "authors": [
      "Amin Anjomshoaa",
      "Hannah Schuster",
      "Axel Polleres"
    ],
    "abstract": "The Spatial Knowledge Graphs (SKG) are experiencing growing adoption as a\nmeans to model real-world entities, proving especially invaluable in domains\nlike crisis management and urban planning. Considering that RDF specifications\noffer limited support for effectively managing spatial information, it's common\npractice to include text-based serializations of geometrical features, such as\npolygons and lines, as string literals in knowledge graphs. Consequently,\nSpatial Knowledge Graphs (SKGs) often rely on geo-enabled RDF Stores capable of\nparsing, interpreting, and indexing such serializations. In this paper, we\nleverage grid cells as the foundational element of SKGs and demonstrate how\nefficiently the spatial characteristics of real-world entities and their\nattributes can be encoded within knowledge graphs. Furthermore, we introduce a\nnovel methodology for representing street networks in knowledge graphs,\ndiverging from the conventional practice of individually capturing each street\nsegment. Instead, our approach is based on tessellating the street network\nusing grid cells and creating a simplified representation that could be\nutilized for various routing and navigation tasks, solely relying on RDF\nspecifications.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02309v1",
    "published_date": "2024-11-04 17:35:41 UTC",
    "updated_date": "2024-11-04 17:35:41 UTC"
  },
  {
    "arxiv_id": "2411.02306v3",
    "title": "On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback",
    "authors": [
      "Marcus Williams",
      "Micah Carroll",
      "Adhyyan Narang",
      "Constantin Weisser",
      "Brendan Murphy",
      "Anca Dragan"
    ],
    "abstract": "As LLMs become more widely deployed, there is increasing interest in directly\noptimizing for feedback from end users (e.g. thumbs up) in addition to feedback\nfrom paid annotators. However, training to maximize human feedback creates a\nperverse incentive structure for the AI to resort to manipulative or deceptive\ntactics to obtain positive feedback from users who are vulnerable to such\nstrategies. We study this phenomenon by training LLMs with Reinforcement\nLearning with simulated user feedback in environments of practical LLM usage.\nIn our settings, we find that: 1) Extreme forms of \"feedback gaming\" such as\nmanipulation and deception are learned reliably; 2) Even if only 2% of users\nare vulnerable to manipulative strategies, LLMs learn to identify and target\nthem while behaving appropriately with other users, making such behaviors\nharder to detect; 3) To mitigate this issue, it may seem promising to leverage\ncontinued safety training or LLM-as-judges during training to filter\nproblematic outputs. Instead, we found that while such approaches help in some\nof our settings, they backfire in others, sometimes even leading to subtler\nmanipulative behaviors. We hope our results can serve as a case study which\nhighlights the risks of using gameable feedback sources -- such as user\nfeedback -- as a target for RL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.02306v3",
    "published_date": "2024-11-04 17:31:02 UTC",
    "updated_date": "2025-02-22 13:06:15 UTC"
  },
  {
    "arxiv_id": "2411.02305v2",
    "title": "CRMArena: Understanding the Capacity of LLM Agents to Perform Professional CRM Tasks in Realistic Environments",
    "authors": [
      "Kung-Hsiang Huang",
      "Akshara Prabhakar",
      "Sidharth Dhawan",
      "Yixin Mao",
      "Huan Wang",
      "Silvio Savarese",
      "Caiming Xiong",
      "Philippe Laban",
      "Chien-Sheng Wu"
    ],
    "abstract": "Customer Relationship Management (CRM) systems are vital for modern\nenterprises, providing a foundation for managing customer interactions and\ndata. Integrating AI agents into CRM systems can automate routine processes and\nenhance personalized service. However, deploying and evaluating these agents is\nchallenging due to the lack of realistic benchmarks that reflect the complexity\nof real-world CRM tasks. To address this issue, we introduce CRMArena, a novel\nbenchmark designed to evaluate AI agents on realistic tasks grounded in\nprofessional work environments. Following guidance from CRM experts and\nindustry best practices, we designed CRMArena with nine customer service tasks\ndistributed across three personas: service agent, analyst, and manager. The\nbenchmark includes 16 commonly used industrial objects (e.g., account, order,\nknowledge article, case) with high interconnectivity, along with latent\nvariables (e.g., complaint habits, policy violations) to simulate realistic\ndata distributions. Experimental results reveal that state-of-the-art LLM\nagents succeed in less than 40% of the tasks with ReAct prompting, and less\nthan 55% even with function-calling abilities. Our findings highlight the need\nfor enhanced agent capabilities in function-calling and rule-following to be\ndeployed in real-world work environments. CRMArena is an open challenge to the\ncommunity: systems that can reliably complete tasks showcase direct business\nvalue in a popular work environment.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "NAACL 2025",
    "pdf_url": "http://arxiv.org/pdf/2411.02305v2",
    "published_date": "2024-11-04 17:30:51 UTC",
    "updated_date": "2025-02-16 17:16:38 UTC"
  },
  {
    "arxiv_id": "2411.02293v5",
    "title": "Hunyuan3D 1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation",
    "authors": [
      "Xianghui Yang",
      "Huiwen Shi",
      "Bowen Zhang",
      "Fan Yang",
      "Jiacheng Wang",
      "Hongxu Zhao",
      "Xinhai Liu",
      "Xinzhou Wang",
      "Qingxiang Lin",
      "Jiaao Yu",
      "Lifu Wang",
      "Jing Xu",
      "Zebin He",
      "Zhuo Chen",
      "Sicong Liu",
      "Junta Wu",
      "Yihang Lian",
      "Shaoxiong Yang",
      "Yuhong Liu",
      "Yong Yang",
      "Di Wang",
      "Jie Jiang",
      "Chunchao Guo"
    ],
    "abstract": "While 3D generative models have greatly improved artists' workflows, the\nexisting diffusion models for 3D generation suffer from slow generation and\npoor generalization. To address this issue, we propose a two-stage approach\nnamed Hunyuan3D 1.0 including a lite version and a standard version, that both\nsupport text- and image-conditioned generation. In the first stage, we employ a\nmulti-view diffusion model that efficiently generates multi-view RGB in\napproximately 4 seconds. These multi-view images capture rich details of the 3D\nasset from different viewpoints, relaxing the tasks from single-view to\nmulti-view reconstruction. In the second stage, we introduce a feed-forward\nreconstruction model that rapidly and faithfully reconstructs the 3D asset\ngiven the generated multi-view images in approximately 7 seconds. The\nreconstruction network learns to handle noises and in-consistency introduced by\nthe multi-view diffusion and leverages the available information from the\ncondition image to efficiently recover the 3D structure. Our framework involves\nthe text-to-image model, i.e., Hunyuan-DiT, making it a unified framework to\nsupport both text- and image-conditioned 3D generation. Our standard version\nhas 3x more parameters than our lite and other existing model. Our Hunyuan3D\n1.0 achieves an impressive balance between speed and quality, significantly\nreducing generation time while maintaining the quality and diversity of the\nproduced assets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical Report; 3D Generation",
    "pdf_url": "http://arxiv.org/pdf/2411.02293v5",
    "published_date": "2024-11-04 17:21:42 UTC",
    "updated_date": "2025-01-23 09:51:37 UTC"
  },
  {
    "arxiv_id": "2411.02292v1",
    "title": "ControlSynth Neural ODEs: Modeling Dynamical Systems with Guaranteed Convergence",
    "authors": [
      "Wenjie Mei",
      "Dongzhe Zheng",
      "Shihua Li"
    ],
    "abstract": "Neural ODEs (NODEs) are continuous-time neural networks (NNs) that can\nprocess data without the limitation of time intervals. They have advantages in\nlearning and understanding the evolution of complex real dynamics. Many\nprevious works have focused on NODEs in concise forms, while numerous physical\nsystems taking straightforward forms, in fact, belong to their more complex\nquasi-classes, thus appealing to a class of general NODEs with high scalability\nand flexibility to model those systems. This, however, may result in intricate\nnonlinear properties. In this paper, we introduce ControlSynth Neural ODEs\n(CSODEs). We show that despite their highly nonlinear nature, convergence can\nbe guaranteed via tractable linear inequalities. In the composition of CSODEs,\nwe introduce an extra control term for learning the potential simultaneous\ncapture of dynamics at different scales, which could be particularly useful for\npartial differential equation-formulated systems. Finally, we compare several\nrepresentative NNs with CSODEs on important physical dynamics under the\ninductive biases of CSODEs, and illustrate that CSODEs have better learning and\npredictive abilities in these settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02292v1",
    "published_date": "2024-11-04 17:20:42 UTC",
    "updated_date": "2024-11-04 17:20:42 UTC"
  },
  {
    "arxiv_id": "2411.02286v2",
    "title": "Federated GNNs for EEG-Based Stroke Assessment",
    "authors": [
      "Andrea Protani",
      "Lorenzo Giusti",
      "Albert Sund Aillet",
      "Chiara Iacovelli",
      "Giuseppe Reale",
      "Simona Sacco",
      "Paolo Manganotti",
      "Lucio Marinelli",
      "Diogo Reis Santos",
      "Pierpaolo Brutti",
      "Pietro Caliandro",
      "Luigi Serio"
    ],
    "abstract": "Machine learning (ML) has the potential to become an essential tool in\nsupporting clinical decision-making processes, offering enhanced diagnostic\ncapabilities and personalized treatment plans. However, outsourcing medical\nrecords to train ML models using patient data raises legal, privacy, and\nsecurity concerns. Federated learning has emerged as a promising paradigm for\ncollaborative ML, meeting healthcare institutions' requirements for robust\nmodels without sharing sensitive data and compromising patient privacy. This\nstudy proposes a novel method that combines federated learning (FL) and Graph\nNeural Networks (GNNs) to predict stroke severity using electroencephalography\n(EEG) signals across multiple medical institutions. Our approach enables\nmultiple hospitals to jointly train a shared GNN model on their local EEG data\nwithout exchanging patient information. Specifically, we address a regression\nproblem by predicting the National Institutes of Health Stroke Scale (NIHSS), a\nkey indicator of stroke severity. The proposed model leverages a masked\nself-attention mechanism to capture salient brain connectivity patterns and\nemploys EdgeSHAP to provide post-hoc explanations of the neurological states\nafter a stroke. We evaluated our method on EEG recordings from four\ninstitutions, achieving a mean absolute error (MAE) of 3.23 in predicting\nNIHSS, close to the average error made by human experts (MAE $\\approx$ 3.0).\nThis demonstrates the method's effectiveness in providing accurate and\nexplainable predictions while maintaining data privacy.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "comment": "13 pages, 5 figures, Proceedings of the II edition of the Workshop on\n  Unifying Representations in Neural Models (UniReps 2024)",
    "pdf_url": "http://arxiv.org/pdf/2411.02286v2",
    "published_date": "2024-11-04 17:13:35 UTC",
    "updated_date": "2024-12-07 16:05:17 UTC"
  },
  {
    "arxiv_id": "2411.02275v2",
    "title": "Breaking the Reclustering Barrier in Centroid-based Deep Clustering",
    "authors": [
      "Lukas Miklautz",
      "Timo Klein",
      "Kevin Sidak",
      "Collin Leiber",
      "Thomas Lang",
      "Andrii Shkabrii",
      "Sebastian Tschiatschek",
      "Claudia Plant"
    ],
    "abstract": "This work investigates an important phenomenon in centroid-based deep\nclustering (DC) algorithms: Performance quickly saturates after a period of\nrapid early gains. Practitioners commonly address early saturation with\nperiodic reclustering, which we demonstrate to be insufficient to address\nperformance plateaus. We call this phenomenon the \"reclustering barrier\" and\nempirically show when the reclustering barrier occurs, what its underlying\nmechanisms are, and how it is possible to Break the Reclustering Barrier with\nour algorithm BRB. BRB avoids early over-commitment to initial clusterings and\nenables continuous adaptation to reinitialized clustering targets while\nremaining conceptually simple. Applying our algorithm to widely-used\ncentroid-based DC algorithms, we show that (1) BRB consistently improves\nperformance across a wide range of clustering benchmarks, (2) BRB enables\ntraining from scratch, and (3) BRB performs competitively against\nstate-of-the-art DC algorithms when combined with a contrastive loss. We\nrelease our code and pre-trained models at\nhttps://github.com/Probabilistic-and-Interactive-ML/breaking-the-reclustering-barrier .",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at ICLR 2025 (Camera-ready version)",
    "pdf_url": "http://arxiv.org/pdf/2411.02275v2",
    "published_date": "2024-11-04 17:05:37 UTC",
    "updated_date": "2025-03-02 11:48:40 UTC"
  },
  {
    "arxiv_id": "2411.02272v4",
    "title": "Combining Induction and Transduction for Abstract Reasoning",
    "authors": [
      "Wen-Ding Li",
      "Keya Hu",
      "Carter Larsen",
      "Yuqing Wu",
      "Simon Alford",
      "Caleb Woo",
      "Spencer M. Dunn",
      "Hao Tang",
      "Michelangelo Naim",
      "Dat Nguyen",
      "Wei-Long Zheng",
      "Zenna Tavares",
      "Yewen Pu",
      "Kevin Ellis"
    ],
    "abstract": "When learning an input-output mapping from very few examples, is it better to\nfirst infer a latent function that explains the examples, or is it better to\ndirectly predict new test outputs, e.g. using a neural network? We study this\nquestion on ARC by training neural models for induction (inferring latent\nfunctions) and transduction (directly predicting the test output for a given\ntest input). We train on synthetically generated variations of Python programs\nthat solve ARC training tasks. We find inductive and transductive models solve\ndifferent kinds of test problems, despite having the same training problems and\nsharing the same neural architecture: Inductive program synthesis excels at\nprecise computations, and at composing multiple concepts, while transduction\nsucceeds on fuzzier perceptual concepts. Ensembling them approaches human-level\nperformance on ARC.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02272v4",
    "published_date": "2024-11-04 17:03:55 UTC",
    "updated_date": "2024-12-02 12:36:30 UTC"
  },
  {
    "arxiv_id": "2411.02271v2",
    "title": "On the Utilization of Unique Node Identifiers in Graph Neural Networks",
    "authors": [
      "Maya Bechler-Speicher",
      "Moshe Eliasof",
      "Carola-Bibiane Schönlieb",
      "Ran Gilad-Bachrach",
      "Amir Globerson"
    ],
    "abstract": "Graph Neural Networks have inherent representational limitations due to their\nmessage-passing structure. Recent work has suggested that these limitations can\nbe overcome by using unique node identifiers (UIDs). Here we argue that despite\nthe advantages of UIDs, one of their disadvantages is that they lose the\ndesirable property of permutation-equivariance. We thus propose to focus on UID\nmodels that are permutation-equivariant, and present theoretical arguments for\ntheir advantages. Motivated by this, we propose a method to regularize UID\nmodels towards permutation equivariance, via a contrastive loss. We empirically\ndemonstrate that our approach improves generalization and extrapolation\nabilities while providing faster training convergence. On the recent BREC\nexpressiveness benchmark, our proposed method achieves state-of-the-art\nperformance compared to other random-based approaches.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02271v2",
    "published_date": "2024-11-04 17:03:52 UTC",
    "updated_date": "2024-11-12 18:11:30 UTC"
  },
  {
    "arxiv_id": "2411.02265v3",
    "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent",
    "authors": [
      "Xingwu Sun",
      "Yanfeng Chen",
      "Yiqing Huang",
      "Ruobing Xie",
      "Jiaqi Zhu",
      "Kai Zhang",
      "Shuaipeng Li",
      "Zhen Yang",
      "Jonny Han",
      "Xiaobo Shu",
      "Jiahao Bu",
      "Zhongzhi Chen",
      "Xuemeng Huang",
      "Fengzong Lian",
      "Saiyong Yang",
      "Jianfeng Yan",
      "Yuyuan Zeng",
      "Xiaoqin Ren",
      "Chao Yu",
      "Lulu Wu",
      "Yue Mao",
      "Jun Xia",
      "Tao Yang",
      "Suncong Zheng",
      "Kan Wu",
      "Dian Jiao",
      "Jinbao Xue",
      "Xipeng Zhang",
      "Decheng Wu",
      "Kai Liu",
      "Dengpeng Wu",
      "Guanghui Xu",
      "Shaohua Chen",
      "Shuang Chen",
      "Xiao Feng",
      "Yigeng Hong",
      "Junqiang Zheng",
      "Chengcheng Xu",
      "Zongwei Li",
      "Xiong Kuang",
      "Jianglu Hu",
      "Yiqi Chen",
      "Yuchi Deng",
      "Guiyang Li",
      "Ao Liu",
      "Chenchen Zhang",
      "Shihui Hu",
      "Zilong Zhao",
      "Zifan Wu",
      "Yao Ding",
      "Weichao Wang",
      "Han Liu",
      "Roberts Wang",
      "Hao Fei",
      "Peijie Yu",
      "Ze Zhao",
      "Xun Cao",
      "Hai Wang",
      "Fusheng Xiang",
      "Mengyuan Huang",
      "Zhiyuan Xiong",
      "Bin Hu",
      "Xuebin Hou",
      "Lei Jiang",
      "Jianqiang Ma",
      "Jiajia Wu",
      "Yaping Deng",
      "Yi Shen",
      "Qian Wang",
      "Weijie Liu",
      "Jie Liu",
      "Meng Chen",
      "Liang Dong",
      "Weiwen Jia",
      "Hu Chen",
      "Feifei Liu",
      "Rui Yuan",
      "Huilin Xu",
      "Zhenxiang Yan",
      "Tengfei Cao",
      "Zhichao Hu",
      "Xinhua Feng",
      "Dong Du",
      "Tinghao Yu",
      "Yangyu Tao",
      "Feng Zhang",
      "Jianchen Zhu",
      "Chengzhong Xu",
      "Xirui Li",
      "Chong Zha",
      "Wen Ouyang",
      "Yinben Xia",
      "Xiang Li",
      "Zekun He",
      "Rongpeng Chen",
      "Jiawei Song",
      "Ruibin Chen",
      "Fan Jiang",
      "Chongqing Zhao",
      "Bo Wang",
      "Hao Gong",
      "Rong Gan",
      "Winston Hu",
      "Zhanhui Kang",
      "Yong Yang",
      "Yuhong Liu",
      "Di Wang",
      "Jie Jiang"
    ],
    "abstract": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "17 pages, 4 Figures",
    "pdf_url": "http://arxiv.org/pdf/2411.02265v3",
    "published_date": "2024-11-04 16:56:26 UTC",
    "updated_date": "2024-11-06 09:15:27 UTC"
  },
  {
    "arxiv_id": "2411.02471v2",
    "title": "Energy-Aware Dynamic Neural Inference",
    "authors": [
      "Marcello Bullo",
      "Seifallah Jardak",
      "Pietro Carnelli",
      "Deniz Gündüz"
    ],
    "abstract": "The growing demand for intelligent applications beyond the network edge,\ncoupled with the need for sustainable operation, are driving the seamless\nintegration of deep learning (DL) algorithms into energy-limited, and even\nenergy-harvesting end-devices. However, the stochastic nature of ambient energy\nsources often results in insufficient harvesting rates, failing to meet the\nenergy requirements for inference and causing significant performance\ndegradation in energy-agnostic systems. To address this problem, we consider an\non-device adaptive inference system equipped with an energy-harvester and\nfinite-capacity energy storage. We then allow the device to reduce the run-time\nexecution cost on-demand, by either switching between differently-sized neural\nnetworks, referred to as multi-model selection (MMS), or by enabling earlier\npredictions at intermediate layers, called early exiting (EE). The model to be\nemployed, or the exit point is then dynamically chosen based on the energy\nstorage and harvesting process states. We also study the efficacy of\nintegrating the prediction confidence into the decision-making process. We\nderive a principled policy with theoretical guarantees for confidence-aware and\n-agnostic controllers. Moreover, in multi-exit networks, we study the\nadvantages of taking decisions incrementally, exit-by-exit, by designing a\nlightweight reinforcement learning-based controller. Experimental results show\nthat, as the rate of the ambient energy increases, energy- and confidence-aware\ncontrol schemes show approximately 5% improvement in accuracy compared to their\nenergy-aware confidence-agnostic counterparts. Incremental approaches achieve\neven higher accuracy, particularly when the energy storage capacity is limited\nrelative to the energy consumption of the inference model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SY",
      "eess.SP",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "\\c{opyright}2024 IEEE. This work has been submitted to the IEEE for\n  possible publication",
    "pdf_url": "http://arxiv.org/pdf/2411.02471v2",
    "published_date": "2024-11-04 16:51:22 UTC",
    "updated_date": "2024-11-06 21:10:43 UTC"
  },
  {
    "arxiv_id": "2411.02255v1",
    "title": "The Enhancement of Software Delivery Performance through Enterprise DevSecOps and Generative Artificial Intelligence in Chinese Technology Firms",
    "authors": [
      "Jun Cui"
    ],
    "abstract": "This study investigates the impact of integrating DevSecOps and Generative\nArtificial Intelligence (GAI) on software delivery performance within\ntechnology firms. Utilizing a qualitative research methodology, the research\ninvolved semi-structured interviews with industry practitioners and analysis of\ncase studies from organizations that have successfully implemented these\nmethodologies. The findings reveal significant enhancements in research and\ndevelopment (R&D) efficiency, improved source code management, and heightened\nsoftware quality and security. The integration of GAI facilitated automation of\ncoding tasks and predictive analytics, while DevSecOps ensured that security\nmeasures were embedded throughout the development lifecycle. Despite the\npromising results, the study identifies gaps related to the generalizability of\nthe findings due to the limited sample size and the qualitative nature of the\nresearch. This paper contributes valuable insights into the practical\nimplementation of DevSecOps and GAI, highlighting their potential to transform\nsoftware delivery processes in technology firms. Future research directions\ninclude quantitative assessments of the impact on specific business outcomes\nand comparative studies across different industries.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02255v1",
    "published_date": "2024-11-04 16:44:01 UTC",
    "updated_date": "2024-11-04 16:44:01 UTC"
  },
  {
    "arxiv_id": "2411.02223v1",
    "title": "Positive Experience Reflection for Agents in Interactive Text Environments",
    "authors": [
      "Philip Lippmann",
      "Matthijs T. J. Spaan",
      "Jie Yang"
    ],
    "abstract": "Intelligent agents designed for interactive environments face significant\nchallenges in text-based games, a domain that demands complex reasoning and\nadaptability. While agents based on large language models (LLMs) using\nself-reflection have shown promise, they struggle when initially successful and\nexhibit reduced effectiveness when using smaller LLMs. We introduce Sweet&Sour,\na novel approach that addresses these limitations in existing reflection\nmethods by incorporating positive experiences and managed memory to enrich the\ncontext available to the agent at decision time. Our comprehensive analysis\nspans both closed- and open-source LLMs and demonstrates the effectiveness of\nSweet&Sour in improving agent performance, particularly in scenarios where\nprevious approaches fall short.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "To appear at NeurIPS 2024 Language Gamification workshop",
    "pdf_url": "http://arxiv.org/pdf/2411.02223v1",
    "published_date": "2024-11-04 16:15:28 UTC",
    "updated_date": "2024-11-04 16:15:28 UTC"
  },
  {
    "arxiv_id": "2411.02193v2",
    "title": "Improving Steering Vectors by Targeting Sparse Autoencoder Features",
    "authors": [
      "Sviatoslav Chalnev",
      "Matthew Siu",
      "Arthur Conmy"
    ],
    "abstract": "To control the behavior of language models, steering methods attempt to\nensure that outputs of the model satisfy specific pre-defined properties.\nAdding steering vectors to the model is a promising method of model control\nthat is easier than finetuning, and may be more robust than prompting. However,\nit can be difficult to anticipate the effects of steering vectors produced by\nmethods such as CAA [Panickssery et al., 2024] or the direct use of SAE latents\n[Templeton et al., 2024]. In our work, we address this issue by using SAEs to\nmeasure the effects of steering vectors, giving us a method that can be used to\nunderstand the causal effect of any steering vector intervention. We use this\nmethod for measuring causal effects to develop an improved steering method,\nSAE-Targeted Steering (SAE-TS), which finds steering vectors to target specific\nSAE features while minimizing unintended side effects. We show that overall,\nSAE-TS balances steering effects with coherence better than CAA and SAE feature\nsteering, when evaluated on a range of tasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "8 maintext pages and 9 appendix pages",
    "pdf_url": "http://arxiv.org/pdf/2411.02193v2",
    "published_date": "2024-11-04 15:46:20 UTC",
    "updated_date": "2024-11-21 12:10:54 UTC"
  },
  {
    "arxiv_id": "2411.02184v1",
    "title": "Double Descent Meets Out-of-Distribution Detection: Theoretical Insights and Empirical Analysis on the role of model complexity",
    "authors": [
      "Mouïn Ben Ammar",
      "David Brellmann",
      "Arturo Mendoza",
      "Antoine Manzanera",
      "Gianni Franchi"
    ],
    "abstract": "While overparameterization is known to benefit generalization, its impact on\nOut-Of-Distribution (OOD) detection is less understood. This paper investigates\nthe influence of model complexity in OOD detection. We propose an expected OOD\nrisk metric to evaluate classifiers confidence on both training and OOD\nsamples. Leveraging Random Matrix Theory, we derive bounds for the expected OOD\nrisk of binary least-squares classifiers applied to Gaussian data. We show that\nthe OOD risk depicts an infinite peak, when the number of parameters is equal\nto the number of samples, which we associate with the double descent\nphenomenon. Our experimental study on different OOD detection methods across\nmultiple neural architectures extends our theoretical insights and highlights a\ndouble descent curve. Our observations suggest that overparameterization does\nnot necessarily lead to better OOD detection. Using the Neural Collapse\nframework, we provide insights to better understand this behavior. To\nfacilitate reproducibility, our code will be made publicly available upon\npublication.",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "math.ST",
      "stat.TH"
    ],
    "primary_category": "stat.ML",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02184v1",
    "published_date": "2024-11-04 15:39:12 UTC",
    "updated_date": "2024-11-04 15:39:12 UTC"
  },
  {
    "arxiv_id": "2411.02181v1",
    "title": "Detect an Object At Once without Fine-tuning",
    "authors": [
      "Junyu Hao",
      "Jianheng Liu",
      "Yongjia Zhao",
      "Zuofan Chen",
      "Qi Sun",
      "Jinlong Chen",
      "Jianguo Wei",
      "Minghao Yang"
    ],
    "abstract": "When presented with one or a few photos of a previously unseen object, humans\ncan instantly recognize it in different scenes. Although the human brain\nmechanism behind this phenomenon is still not fully understood, this work\nintroduces a novel technical realization of this task. It consists of two\nphases: (1) generating a Similarity Density Map (SDM) by convolving the scene\nimage with the given object image patch(es) so that the highlight areas in the\nSDM indicate the possible locations; (2) obtaining the object occupied areas in\nthe scene through a Region Alignment Network (RAN). The RAN is constructed on a\nbackbone of Deep Siamese Network (DSN), and different from the traditional\nDSNs, it aims to obtain the object accurate regions by regressing the location\nand area differences between the ground truths and the predicted ones indicated\nby the highlight areas in SDM. By pre-learning from labels annotated in\ntraditional datasets, the SDM-RAN can detect previously unknown objects without\nfine-tuning. Experiments were conducted on the MS COCO, PASCAL VOC datasets.\nThe results indicate that the proposed method outperforms state-of-the-art\nmethods on the same task.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02181v1",
    "published_date": "2024-11-04 15:38:32 UTC",
    "updated_date": "2024-11-04 15:38:32 UTC"
  },
  {
    "arxiv_id": "2411.02174v1",
    "title": "Behavioral Sequence Modeling with Ensemble Learning",
    "authors": [
      "Maxime Kawawa-Beaudan",
      "Srijan Sood",
      "Soham Palande",
      "Ganapathy Mani",
      "Tucker Balch",
      "Manuela Veloso"
    ],
    "abstract": "We investigate the use of sequence analysis for behavior modeling,\nemphasizing that sequential context often outweighs the value of aggregate\nfeatures in understanding human behavior. We discuss framing common problems in\nfields like healthcare, finance, and e-commerce as sequence modeling tasks, and\naddress challenges related to constructing coherent sequences from fragmented\ndata and disentangling complex behavior patterns. We present a framework for\nsequence modeling using Ensembles of Hidden Markov Models, which are\nlightweight, interpretable, and efficient. Our ensemble-based scoring method\nenables robust comparison across sequences of different lengths and enhances\nperformance in scenarios with imbalanced or scarce data. The framework scales\nin real-world scenarios, is compatible with downstream feature-based modeling,\nand is applicable in both supervised and unsupervised learning settings. We\ndemonstrate the effectiveness of our method with results on a longitudinal\nhuman behavior dataset.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02174v1",
    "published_date": "2024-11-04 15:34:28 UTC",
    "updated_date": "2024-11-04 15:34:28 UTC"
  },
  {
    "arxiv_id": "2411.02168v2",
    "title": "Do graph neural network states contain graph properties?",
    "authors": [
      "Tom Pelletreau-Duris",
      "Ruud van Bakel",
      "Michael Cochez"
    ],
    "abstract": "Deep neural networks (DNNs) achieve state-of-the-art performance on many\ntasks, but this often requires increasingly larger model sizes, which in turn\nleads to more complex internal representations. Explainability techniques (XAI)\nhave made remarkable progress in the interpretability of ML models. However,\nthe non-relational nature of Graph neural networks (GNNs) make it difficult to\nreuse already existing XAI methods. While other works have focused on\ninstance-based explanation methods for GNNs, very few have investigated\nmodel-based methods and, to our knowledge, none have tried to probe the\nembedding of the GNNs for well-known structural graph properties. In this paper\nwe present a model agnostic explainability pipeline for GNNs employing\ndiagnostic classifiers. This pipeline aims to probe and interpret the learned\nrepresentations in GNNs across various architectures and datasets, refining our\nunderstanding and trust in these models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 22 figures, conference",
    "pdf_url": "http://arxiv.org/pdf/2411.02168v2",
    "published_date": "2024-11-04 15:26:07 UTC",
    "updated_date": "2024-12-10 18:14:14 UTC"
  },
  {
    "arxiv_id": "2411.02470v1",
    "title": "Benchmarking XAI Explanations with Human-Aligned Evaluations",
    "authors": [
      "Rémi Kazmierczak",
      "Steve Azzolin",
      "Eloïse Berthier",
      "Anna Hedström",
      "Patricia Delhomme",
      "Nicolas Bousquet",
      "Goran Frehse",
      "Massimiliano Mancini",
      "Baptiste Caramiaux",
      "Andrea Passerini",
      "Gianni Franchi"
    ],
    "abstract": "In this paper, we introduce PASTA (Perceptual Assessment System for\nexplanaTion of Artificial intelligence), a novel framework for a human-centric\nevaluation of XAI techniques in computer vision. Our first key contribution is\na human evaluation of XAI explanations on four diverse datasets (COCO, Pascal\nParts, Cats Dogs Cars, and MonumAI) which constitutes the first large-scale\nbenchmark dataset for XAI, with annotations at both the image and concept\nlevels. This dataset allows for robust evaluation and comparison across various\nXAI methods. Our second major contribution is a data-based metric for assessing\nthe interpretability of explanations. It mimics human preferences, based on a\ndatabase of human evaluations of explanations in the PASTA-dataset. With its\ndataset and metric, the PASTA framework provides consistent and reliable\ncomparisons between XAI techniques, in a way that is scalable but still aligned\nwith human evaluations. Additionally, our benchmark allows for comparisons\nbetween explanations across different modalities, an aspect previously\nunaddressed. Our findings indicate that humans tend to prefer saliency maps\nover other explanation types. Moreover, we provide evidence that human\nassessments show a low correlation with existing XAI metrics that are\nnumerically simulated by probing the model.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "comment": "https://github.com/ENSTA-U2IS-AI/Dataset_XAI",
    "pdf_url": "http://arxiv.org/pdf/2411.02470v1",
    "published_date": "2024-11-04 15:18:20 UTC",
    "updated_date": "2024-11-04 15:18:20 UTC"
  },
  {
    "arxiv_id": "2411.02158v2",
    "title": "Learning Multiple Initial Solutions to Optimization Problems",
    "authors": [
      "Elad Sharony",
      "Heng Yang",
      "Tong Che",
      "Marco Pavone",
      "Shie Mannor",
      "Peter Karkus"
    ],
    "abstract": "Sequentially solving similar optimization problems under strict runtime\nconstraints is essential for many applications, such as robot control,\nautonomous driving, and portfolio management. The performance of local\noptimization methods in these settings is sensitive to the initial solution:\npoor initialization can lead to slow convergence or suboptimal solutions. To\naddress this challenge, we propose learning to predict \\emph{multiple} diverse\ninitial solutions given parameters that define the problem instance. We\nintroduce two strategies for utilizing multiple initial solutions: (i) a\nsingle-optimizer approach, where the most promising initial solution is chosen\nusing a selection function, and (ii) a multiple-optimizers approach, where\nseveral optimizers, potentially run in parallel, are each initialized with a\ndifferent solution, with the best solution chosen afterward. Notably, by\nincluding a default initialization among predicted ones, the cost of the final\noutput is guaranteed to be equal or lower than with the default initialization.\nWe validate our method on three optimal control benchmark tasks: cart-pole,\nreacher, and autonomous driving, using different optimizers: DDP, MPPI, and\niLQR. We find significant and consistent improvement with our method across all\nevaluation settings and demonstrate that it efficiently scales with the number\nof initial solutions required. The code is available at MISO\n(https://github.com/EladSharony/miso).",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2411.02158v2",
    "published_date": "2024-11-04 15:17:19 UTC",
    "updated_date": "2025-02-03 08:21:47 UTC"
  },
  {
    "arxiv_id": "2411.02142v1",
    "title": "Training Compute-Optimal Protein Language Models",
    "authors": [
      "Xingyi Cheng",
      "Bo Chen",
      "Pan Li",
      "Jing Gong",
      "Jie Tang",
      "Le Song"
    ],
    "abstract": "We explore optimally training protein language models, an area of significant\ninterest in biological research where guidance on best practices is limited.\nMost models are trained with extensive compute resources until performance\ngains plateau, focusing primarily on increasing model sizes rather than\noptimizing the efficient compute frontier that balances performance and compute\nbudgets. Our investigation is grounded in a massive dataset consisting of 939\nmillion protein sequences. We trained over 300 models ranging from 3.5 million\nto 10.7 billion parameters on 5 to 200 billion unique tokens, to investigate\nthe relations between model sizes, training token numbers, and objectives.\nFirst, we observed the effect of diminishing returns for the Causal Language\nModel (CLM) and that of overfitting for the Masked Language Model~(MLM) when\nrepeating the commonly used Uniref database. To address this, we included\nmetagenomic protein sequences in the training set to increase the diversity and\navoid the plateau or overfitting effects. Second, we obtained the scaling laws\nof CLM and MLM on Transformer, tailored to the specific characteristics of\nprotein sequence data. Third, we observe a transfer scaling phenomenon from CLM\nto MLM, further demonstrating the effectiveness of transfer through scaling\nbehaviors based on estimated Effectively Transferred Tokens. Finally, to\nvalidate our scaling laws, we compare the large-scale versions of ESM-2 and\nPROGEN2 on downstream tasks, encompassing evaluations of protein generation as\nwell as structure- and function-related tasks, all within less or equivalent\npre-training compute budgets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024 (Spotlight); Code:\n  https://github.com/cxysteven/ScalingProteinLM. Additional resources are\n  available here",
    "pdf_url": "http://arxiv.org/pdf/2411.02142v1",
    "published_date": "2024-11-04 14:58:37 UTC",
    "updated_date": "2024-11-04 14:58:37 UTC"
  },
  {
    "arxiv_id": "2411.02136v2",
    "title": "Advanced computer vision for extracting georeferenced vehicle trajectories from drone imagery",
    "authors": [
      "Robert Fonod",
      "Haechan Cho",
      "Hwasoo Yeo",
      "Nikolas Geroliminis"
    ],
    "abstract": "This paper presents a framework for extracting georeferenced vehicle\ntrajectories from high-altitude drone imagery, addressing key challenges in\nurban traffic monitoring and the limitations of traditional ground-based\nsystems. Our approach integrates several novel contributions, including a\ntailored object detector optimized for high-altitude bird's-eye view\nperspectives, a unique track stabilization method that uses detected vehicle\nbounding boxes as exclusion masks during image registration, and an orthophoto\nand master frame-based georeferencing strategy that enhances consistent\nalignment across multiple drone viewpoints. Additionally, our framework\nfeatures robust vehicle dimension estimation and detailed road segmentation,\nenabling comprehensive traffic analysis. Conducted in the Songdo International\nBusiness District, South Korea, the study utilized a multi-drone experiment\ncovering 20 intersections, capturing approximately 12TB of 4K video data over\nfour days. The framework produced two high-quality datasets: the Songdo Traffic\ndataset, comprising approximately 700,000 unique vehicle trajectories, and the\nSongdo Vision dataset, containing over 5,000 human-annotated images with about\n300,000 vehicle instances in four classes. Comparisons with high-precision\nsensor data from an instrumented probe vehicle highlight the accuracy and\nconsistency of our extraction pipeline in dense urban environments. The public\nrelease of Songdo Traffic and Songdo Vision, and the complete source code for\nthe extraction pipeline, establishes new benchmarks in data quality,\nreproducibility, and scalability in traffic research. Results demonstrate the\npotential of integrating drone technology with advanced computer vision for\nprecise and cost-effective urban traffic monitoring, providing valuable\nresources for developing intelligent transportation systems and enhancing\ntraffic management strategies.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02136v2",
    "published_date": "2024-11-04 14:49:01 UTC",
    "updated_date": "2025-03-17 09:25:50 UTC"
  },
  {
    "arxiv_id": "2411.02131v1",
    "title": "Generating the Traces You Need: A Conditional Generative Model for Process Mining Data",
    "authors": [
      "Riccardo Graziosi",
      "Massimiliano Ronzani",
      "Andrei Buliga",
      "Chiara Di Francescomarino",
      "Francesco Folino",
      "Chiara Ghidini",
      "Francesca Meneghello",
      "Luigi Pontieri"
    ],
    "abstract": "In recent years, trace generation has emerged as a significant challenge\nwithin the Process Mining community. Deep Learning (DL) models have\ndemonstrated accuracy in reproducing the features of the selected processes.\nHowever, current DL generative models are limited in their ability to adapt the\nlearned distributions to generate data samples based on specific conditions or\nattributes. This limitation is particularly significant because the ability to\ncontrol the type of generated data can be beneficial in various contexts,\nenabling a focus on specific behaviours, exploration of infrequent patterns, or\nsimulation of alternative 'what-if' scenarios. In this work, we address this\nchallenge by introducing a conditional model for process data generation based\non a conditional variational autoencoder (CVAE). Conditional models offer\ncontrol over the generation process by tuning input conditional variables,\nenabling more targeted and controlled data generation. Unlike other domains,\nCVAE for process mining faces specific challenges due to the multiperspective\nnature of the data and the need to adhere to control-flow rules while ensuring\ndata variability. Specifically, we focus on generating process executions\nconditioned on control flow and temporal features of the trace, allowing us to\nproduce traces for specific, identified sub-processes. The generated traces are\nthen evaluated using common metrics for generative model assessment, along with\nadditional metrics to evaluate the quality of the conditional generation",
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DB",
    "comment": "6th International Conference on Process Mining (ICPM) 2024\n  Copenhagen, Denmark 14-18 October 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.02131v1",
    "published_date": "2024-11-04 14:44:20 UTC",
    "updated_date": "2024-11-04 14:44:20 UTC"
  },
  {
    "arxiv_id": "2411.02126v2",
    "title": "Unsupervised detection of semantic correlations in big data",
    "authors": [
      "Santiago Acevedo",
      "Alex Rodriguez",
      "Alessandro Laio"
    ],
    "abstract": "In real-world data, information is stored in extremely large feature vectors.\nThese variables are typically correlated due to complex interactions involving\nmany features simultaneously. Such correlations qualitatively correspond to\nsemantic roles and are naturally recognized by both the human brain and\nartificial neural networks. This recognition enables, for instance, the\nprediction of missing parts of an image or text based on their context. We\npresent a method to detect these correlations in high-dimensional data\nrepresented as binary numbers. We estimate the binary intrinsic dimension of a\ndataset, which quantifies the minimum number of independent coordinates needed\nto describe the data, and is therefore a proxy of semantic complexity. The\nproposed algorithm is largely insensitive to the so-called curse of\ndimensionality, and can therefore be used in big data analysis. We test this\napproach identifying phase transitions in model magnetic systems and we then\napply it to the detection of semantic correlations of images and text inside\ndeep neural networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02126v2",
    "published_date": "2024-11-04 14:37:07 UTC",
    "updated_date": "2025-03-07 15:21:42 UTC"
  },
  {
    "arxiv_id": "2411.02125v1",
    "title": "Revisiting K-mer Profile for Effective and Scalable Genome Representation Learning",
    "authors": [
      "Abdulkadir Celikkanat",
      "Andres R. Masegosa",
      "Thomas D. Nielsen"
    ],
    "abstract": "Obtaining effective representations of DNA sequences is crucial for genome\nanalysis. Metagenomic binning, for instance, relies on genome representations\nto cluster complex mixtures of DNA fragments from biological samples with the\naim of determining their microbial compositions. In this paper, we revisit\nk-mer-based representations of genomes and provide a theoretical analysis of\ntheir use in representation learning. Based on the analysis, we propose a\nlightweight and scalable model for performing metagenomic binning at the genome\nread level, relying only on the k-mer compositions of the DNA fragments. We\ncompare the model to recent genome foundation models and demonstrate that while\nthe models are comparable in performance, the proposed model is significantly\nmore effective in terms of scalability, a crucial aspect for performing\nmetagenomic binning of real-world datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "q-bio.GN"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted to the Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2024)",
    "pdf_url": "http://arxiv.org/pdf/2411.02125v1",
    "published_date": "2024-11-04 14:36:51 UTC",
    "updated_date": "2024-11-04 14:36:51 UTC"
  },
  {
    "arxiv_id": "2411.02124v2",
    "title": "Adaptive Sparse Allocation with Mutual Choice & Feature Choice Sparse Autoencoders",
    "authors": [
      "Kola Ayonrinde"
    ],
    "abstract": "Sparse autoencoders (SAEs) are a promising approach to extracting features\nfrom neural networks, enabling model interpretability as well as causal\ninterventions on model internals. SAEs generate sparse feature representations\nusing a sparsifying activation function that implicitly defines a set of\ntoken-feature matches. We frame the token-feature matching as a resource\nallocation problem constrained by a total sparsity upper bound. For example,\nTopK SAEs solve this allocation problem with the additional constraint that\neach token matches with at most $k$ features. In TopK SAEs, the $k$ active\nfeatures per token constraint is the same across tokens, despite some tokens\nbeing more difficult to reconstruct than others. To address this limitation, we\npropose two novel SAE variants, Feature Choice SAEs and Mutual Choice SAEs,\nwhich each allow for a variable number of active features per token. Feature\nChoice SAEs solve the sparsity allocation problem under the additional\nconstraint that each feature matches with at most $m$ tokens. Mutual Choice\nSAEs solve the unrestricted allocation problem where the total sparsity budget\ncan be allocated freely between tokens and features. Additionally, we introduce\na new auxiliary loss function, $\\mathtt{aux\\_zipf\\_loss}$, which generalises\nthe $\\mathtt{aux\\_k\\_loss}$ to mitigate dead and underutilised features. Our\nmethods result in SAEs with fewer dead features and improved reconstruction\nloss at equivalent sparsity levels as a result of the inherent adaptive\ncomputation. More accurate and scalable feature extraction methods provide a\npath towards better understanding and more precise control of foundation\nmodels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages (18 w/ appendices), 7 figures. Preprint",
    "pdf_url": "http://arxiv.org/pdf/2411.02124v2",
    "published_date": "2024-11-04 14:36:24 UTC",
    "updated_date": "2024-11-07 21:36:54 UTC"
  },
  {
    "arxiv_id": "2411.02120v1",
    "title": "Bridge-IF: Learning Inverse Protein Folding with Markov Bridges",
    "authors": [
      "Yiheng Zhu",
      "Jialu Wu",
      "Qiuyi Li",
      "Jiahuan Yan",
      "Mingze Yin",
      "Wei Wu",
      "Mingyang Li",
      "Jieping Ye",
      "Zheng Wang",
      "Jian Wu"
    ],
    "abstract": "Inverse protein folding is a fundamental task in computational protein\ndesign, which aims to design protein sequences that fold into the desired\nbackbone structures. While the development of machine learning algorithms for\nthis task has seen significant success, the prevailing approaches, which\npredominantly employ a discriminative formulation, frequently encounter the\nerror accumulation issue and often fail to capture the extensive variety of\nplausible sequences. To fill these gaps, we propose Bridge-IF, a generative\ndiffusion bridge model for inverse folding, which is designed to learn the\nprobabilistic dependency between the distributions of backbone structures and\nprotein sequences. Specifically, we harness an expressive structure encoder to\npropose a discrete, informative prior derived from structures, and establish a\nMarkov bridge to connect this prior with native sequences. During the inference\nstage, Bridge-IF progressively refines the prior sequence, culminating in a\nmore plausible design. Moreover, we introduce a reparameterization perspective\non Markov bridge models, from which we derive a simplified loss function that\nfacilitates more effective training. We also modulate protein language models\n(PLMs) with structural conditions to precisely approximate the Markov bridge\nprocess, thereby significantly enhancing generation performance while\nmaintaining parameter-efficient training. Extensive experiments on\nwell-established benchmarks demonstrate that Bridge-IF predominantly surpasses\nexisting baselines in sequence recovery and excels in the design of plausible\nproteins with high foldability. The code is available at\nhttps://github.com/violet-sto/Bridge-IF.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "comment": "NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.02120v1",
    "published_date": "2024-11-04 14:35:14 UTC",
    "updated_date": "2024-11-04 14:35:14 UTC"
  },
  {
    "arxiv_id": "2411.02099v2",
    "title": "Differentially Private Integrated Decision Gradients (IDG-DP) for Radar-based Human Activity Recognition",
    "authors": [
      "Idris Zakariyya",
      "Linda Tran",
      "Kaushik Bhargav Sivangi",
      "Paul Henderson",
      "Fani Deligianni"
    ],
    "abstract": "Human motion analysis offers significant potential for healthcare monitoring\nand early detection of diseases. The advent of radar-based sensing systems has\ncaptured the spotlight for they are able to operate without physical contact\nand they can integrate with pre-existing Wi-Fi networks. They are also seen as\nless privacy-invasive compared to camera-based systems. However, recent\nresearch has shown high accuracy in recognizing subjects or gender from radar\ngait patterns, raising privacy concerns. This study addresses these issues by\ninvestigating privacy vulnerabilities in radar-based Human Activity Recognition\n(HAR) systems and proposing a novel method for privacy preservation using\nDifferential Privacy (DP) driven by attributions derived with Integrated\nDecision Gradient (IDG) algorithm. We investigate Black-box Membership\nInference Attack (MIA) Models in HAR settings across various levels of\nattacker-accessible information. We extensively evaluated the effectiveness of\nthe proposed IDG-DP method by designing a CNN-based HAR model and rigorously\nassessing its resilience against MIAs. Experimental results demonstrate the\npotential of IDG-DP in mitigating privacy attacks while maintaining utility\nacross all settings, particularly excelling against label-only and shadow model\nblack-box MIA attacks. This work represents a crucial step towards balancing\nthe need for effective radar-based HAR with robust privacy protection in\nhealthcare environments.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted at WACV 2025. 12 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.02099v2",
    "published_date": "2024-11-04 14:08:26 UTC",
    "updated_date": "2024-11-07 10:53:14 UTC"
  },
  {
    "arxiv_id": "2411.02468v1",
    "title": "Modeling and Simulation of a Multi Robot System Architecture",
    "authors": [
      "Ahmed R. Sadik",
      "Christian Goerick",
      "Manuel Muehlig"
    ],
    "abstract": "A Multi Robot System (MRS) is the infrastructure of an intelligent\ncyberphysical system, where the robots understand the need of the human, and\nhence cooperate together to fulfill this need. Modeling an MRS is a crucial\naspect of designing the proper system architecture, because this model can be\nused to simulate and measure the performance of the proposed architecture.\nHowever, an MRS solution architecture modeling is a very difficult problem, as\nit contains many dependent behaviors that dynamically change due to the current\nstatus of the overall system. In this paper, we introduce a general purpose MRS\ncase study, where the humans initiate requests that are achieved by the\navailable robots. These requests require different plans that use the current\ncapabilities of the available robots. After proposing an architecture that\ndefines the solution components, three steps are followed. First is modeling\nthese components via Business Process Model and Notation (BPMN) language. BPMN\nprovides a graphical notation to precisely represent the behaviors of every\ncomponent, which is an essential need to model the solution. Second is to\nsimulate these components behaviors and interaction in form of software agents.\nJava Agent DEvelopment (JADE) middleware has been used to develop and simulate\nthe proposed model. JADE is based on a reactive agent approach, therefore it\ncan dynamically represent the interaction among the solution components.\nFinally is to analyze the performance of the solution by defining a number of\nquantitative measurements, which can be obtained while simulating the system\nmodel in JADE middleware, therefore the solution can be analyzed and compared\nto another architecture.",
    "categories": [
      "cs.AI",
      "cs.RO",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02468v1",
    "published_date": "2024-11-04 14:00:43 UTC",
    "updated_date": "2024-11-04 14:00:43 UTC"
  },
  {
    "arxiv_id": "2411.02094v1",
    "title": "Alignment-Based Adversarial Training (ABAT) for Improving the Robustness and Accuracy of EEG-Based BCIs",
    "authors": [
      "Xiaoqing Chen",
      "Ziwei Wang",
      "Dongrui Wu"
    ],
    "abstract": "Machine learning has achieved great success in electroencephalogram (EEG)\nbased brain-computer interfaces (BCIs). Most existing BCI studies focused on\nimproving the decoding accuracy, with only a few considering the adversarial\nsecurity. Although many adversarial defense approaches have been proposed in\nother application domains such as computer vision, previous research showed\nthat their direct extensions to BCIs degrade the classification accuracy on\nbenign samples. This phenomenon greatly affects the applicability of\nadversarial defense approaches to EEG-based BCIs. To mitigate this problem, we\npropose alignment-based adversarial training (ABAT), which performs EEG data\nalignment before adversarial training. Data alignment aligns EEG trials from\ndifferent domains to reduce their distribution discrepancies, and adversarial\ntraining further robustifies the classification boundary. The integration of\ndata alignment and adversarial training can make the trained EEG classifiers\nsimultaneously more accurate and more robust. Experiments on five EEG datasets\nfrom two different BCI paradigms (motor imagery classification, and event\nrelated potential recognition), three convolutional neural network classifiers\n(EEGNet, ShallowCNN and DeepCNN) and three different experimental settings\n(offline within-subject cross-block/-session classification, online\ncross-session classification, and pre-trained classifiers) demonstrated its\neffectiveness. It is very intriguing that adversarial attacks, which are\nusually used to damage BCI systems, can be used in ABAT to simultaneously\nimprove the model accuracy and robustness.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02094v1",
    "published_date": "2024-11-04 13:56:54 UTC",
    "updated_date": "2024-11-04 13:56:54 UTC"
  },
  {
    "arxiv_id": "2411.02086v1",
    "title": "Real-time and Downtime-tolerant Fault Diagnosis for Railway Turnout Machines (RTMs) Empowered with Cloud-Edge Pipeline Parallelism",
    "authors": [
      "Fan Wu",
      "Muhammad Bilal",
      "Haolong Xiang",
      "Heng Wang",
      "Jinjun Yu",
      "Xiaolong Xu"
    ],
    "abstract": "Railway Turnout Machines (RTMs) are mission-critical components of the\nrailway transportation infrastructure, responsible for directing trains onto\ndesired tracks. For safety assurance applications, especially in early-warning\nscenarios, RTM faults are expected to be detected as early as possible on a\ncontinuous 7x24 basis. However, limited emphasis has been placed on distributed\nmodel inference frameworks that can meet the inference latency and reliability\nrequirements of such mission critical fault diagnosis systems. In this paper,\nan edge-cloud collaborative early-warning system is proposed to enable\nreal-time and downtime-tolerant fault diagnosis of RTMs, providing a new\nparadigm for the deployment of models in safety-critical scenarios. Firstly, a\nmodular fault diagnosis model is designed specifically for distributed\ndeployment, which utilizes a hierarchical architecture consisting of the prior\nknowledge module, subordinate classifiers, and a fusion layer for enhanced\naccuracy and parallelism. Then, a cloud-edge collaborative framework leveraging\npipeline parallelism, namely CEC-PA, is developed to minimize the overhead\nresulting from distributed task execution and context exchange by strategically\npartitioning and offloading model components across cloud and edge.\nAdditionally, an election consensus mechanism is implemented within CEC-PA to\nensure system robustness during coordinator node downtime. Comparative\nexperiments and ablation studies are conducted to validate the effectiveness of\nthe proposed distributed fault diagnosis approach. Our ensemble-based fault\ndiagnosis model achieves a remarkable 97.4% accuracy on a real-world dataset\ncollected by Nanjing Metro in Jiangsu Province, China. Meanwhile, CEC-PA\ndemonstrates superior recovery proficiency during node disruptions and speed-up\nranging from 1.98x to 7.93x in total inference time compared to its\ncounterparts.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.DC",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02086v1",
    "published_date": "2024-11-04 13:49:06 UTC",
    "updated_date": "2024-11-04 13:49:06 UTC"
  },
  {
    "arxiv_id": "2411.11879v1",
    "title": "CSP-Net: Common Spatial Pattern Empowered Neural Networks for EEG-Based Motor Imagery Classification",
    "authors": [
      "Xue Jiang",
      "Lubin Meng",
      "Xinru Chen",
      "Yifan Xu",
      "Dongrui Wu"
    ],
    "abstract": "Electroencephalogram-based motor imagery (MI) classification is an important\nparadigm of non-invasive brain-computer interfaces. Common spatial pattern\n(CSP), which exploits different energy distributions on the scalp while\nperforming different MI tasks, is very popular in MI classification.\nConvolutional neural networks (CNNs) have also achieved great success, due to\ntheir powerful learning capabilities. This paper proposes two CSP-empowered\nneural networks (CSP-Nets), which integrate knowledge-driven CSP filters with\ndata-driven CNNs to enhance the performance in MI classification. CSP-Net-1\ndirectly adds a CSP layer before a CNN to improve the input discriminability.\nCSP-Net-2 replaces a convolutional layer in CNN with a CSP layer. The CSP layer\nparameters in both CSP-Nets are initialized with CSP filters designed from the\ntraining data. During training, they can either be kept fixed or optimized\nusing gradient descent. Experiments on four public MI datasets demonstrated\nthat the two CSP-Nets consistently improved over their CNN backbones, in both\nwithin-subject and cross-subject classifications. They are particularly useful\nwhen the number of training samples is very small. Our work demonstrates the\nadvantage of integrating knowledge-driven traditional machine learning with\ndata-driven deep learning in EEG-based brain-computer interfaces.",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "eess.SP",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.11879v1",
    "published_date": "2024-11-04 13:48:58 UTC",
    "updated_date": "2024-11-04 13:48:58 UTC"
  },
  {
    "arxiv_id": "2411.02083v1",
    "title": "Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models",
    "authors": [
      "Jonas Zausinger",
      "Lars Pennig",
      "Kacper Chlodny",
      "Vincent Limbach",
      "Anna Ketteler",
      "Thorben Prein",
      "Vishwa Mohan Singh",
      "Michael Morris Danziger",
      "Jannis Born"
    ],
    "abstract": "While language models have exceptional capabilities at text generation, they\nlack a natural inductive bias for emitting numbers and thus struggle in tasks\ninvolving reasoning over quantities, especially arithmetics. This has\nparticular relevance in scientific datasets where combinations of text and\nnumerical data are abundant. One fundamental limitation is the nature of the CE\nloss, which assumes a nominal (categorical) scale and thus cannot convey\nproximity between generated number tokens. As a remedy, we here present two\nversions of a number token loss. The first is based on an $L_p$ loss between\nthe ground truth token value and the weighted sum of the predicted class\nprobabilities. The second loss minimizes the Wasserstein-1 distance between the\ndistribution of the predicted output probabilities and the ground truth\ndistribution. These regression-like losses can easily be added to any language\nmodel and extend the CE objective during training. We compare the proposed\nschemes on a mathematics dataset against existing tokenization, encoding, and\ndecoding schemes for improving number representation in language models. Our\nresults reveal a significant improvement in numerical accuracy when equipping a\nstandard T5 model with the proposed loss schemes.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "5-page version for NeurIPS 2024 (MathAI workshop)",
    "pdf_url": "http://arxiv.org/pdf/2411.02083v1",
    "published_date": "2024-11-04 13:43:24 UTC",
    "updated_date": "2024-11-04 13:43:24 UTC"
  },
  {
    "arxiv_id": "2411.02066v2",
    "title": "Collaborative Cognitive Diagnosis with Disentangled Representation Learning for Learner Modeling",
    "authors": [
      "Weibo Gao",
      "Qi Liu",
      "Linan Yue",
      "Fangzhou Yao",
      "Hao Wang",
      "Yin Gu",
      "Zheng Zhang"
    ],
    "abstract": "Learners sharing similar implicit cognitive states often display comparable\nobservable problem-solving performances. Leveraging collaborative connections\namong such similar learners proves valuable in comprehending human learning.\nMotivated by the success of collaborative modeling in various domains, such as\nrecommender systems, we aim to investigate how collaborative signals among\nlearners contribute to the diagnosis of human cognitive states (i.e., knowledge\nproficiency) in the context of intelligent education. The primary challenges\nlie in identifying implicit collaborative connections and disentangling the\nentangled cognitive factors of learners for improved explainability and\ncontrollability in learner Cognitive Diagnosis (CD). However, there has been no\nwork on CD capable of simultaneously modeling collaborative and disentangled\ncognitive states. To address this gap, we present Coral, a Collaborative\ncognitive diagnosis model with disentangled representation learning.\nSpecifically, Coral first introduces a disentangled state encoder to achieve\nthe initial disentanglement of learners' states. Subsequently, a meticulously\ndesigned collaborative representation learning procedure captures collaborative\nsignals. It dynamically constructs a collaborative graph of learners by\niteratively searching for optimal neighbors in a context-aware manner. Using\nthe constructed graph, collaborative information is extracted through node\nrepresentation learning. Finally, a decoding process aligns the initial\ncognitive states and collaborative states, achieving co-disentanglement with\npractice performance reconstructions. Extensive experiments demonstrate the\nsuperior performance of Coral, showcasing significant improvements over\nstate-of-the-art methods across several real-world datasets. Our code is\navailable at https://github.com/bigdata-ustc/Coral.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by NeurIPS2024",
    "pdf_url": "http://arxiv.org/pdf/2411.02066v2",
    "published_date": "2024-11-04 13:13:25 UTC",
    "updated_date": "2024-11-10 08:50:06 UTC"
  },
  {
    "arxiv_id": "2411.02063v1",
    "title": "Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention",
    "authors": [
      "Xingtai Lv",
      "Ning Ding",
      "Kaiyan Zhang",
      "Ermo Hua",
      "Ganqu Cui",
      "Bowen Zhou"
    ],
    "abstract": "Improving the effectiveness and efficiency of large language models (LLMs)\nsimultaneously is a critical yet challenging research goal. In this paper, we\nfind that low-rank pre-training, normally considered as efficient methods that\nwill compromise performance, can be scalably effective when reduced parameters\nare precisely targeted. Specifically, applying the low-dimensional module only\nto the attention layer -- resolves this issue and enhances both effectiveness\nand efficiency. We refer to this structure as Low-dimensional Projected\nAttention (LPA) and provide an explanatory analysis. Through extensive\nexperimentation at parameter scales of 130M, 370M, and scaling up to 3B, we\nhave validated the effectiveness and scalability of LPA. Our results show that\nLPA model can save up to 12.4% in time while achieving an approximate 5%\nimprovement in test perplexity (ppl) and on downstream tasks compared with the\nvanilla Transformer.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 (Main Conference)",
    "pdf_url": "http://arxiv.org/pdf/2411.02063v1",
    "published_date": "2024-11-04 13:06:17 UTC",
    "updated_date": "2024-11-04 13:06:17 UTC"
  },
  {
    "arxiv_id": "2411.02059v3",
    "title": "TableGPT2: A Large Multimodal Model with Tabular Data Integration",
    "authors": [
      "Aofeng Su",
      "Aowen Wang",
      "Chao Ye",
      "Chen Zhou",
      "Ga Zhang",
      "Gang Chen",
      "Guangcheng Zhu",
      "Haobo Wang",
      "Haokai Xu",
      "Hao Chen",
      "Haoze Li",
      "Haoxuan Lan",
      "Jiaming Tian",
      "Jing Yuan",
      "Junbo Zhao",
      "Junlin Zhou",
      "Kaizhe Shou",
      "Liangyu Zha",
      "Lin Long",
      "Liyao Li",
      "Pengzuo Wu",
      "Qi Zhang",
      "Qingyi Huang",
      "Saisai Yang",
      "Tao Zhang",
      "Wentao Ye",
      "Wufang Zhu",
      "Xiaomeng Hu",
      "Xijun Gu",
      "Xinjie Sun",
      "Xiang Li",
      "Yuhang Yang",
      "Zhiqing Xiao"
    ],
    "abstract": "The emergence of models like GPTs, Claude, LLaMA, and Qwen has reshaped AI\napplications, presenting vast new opportunities across industries. Yet, the\nintegration of tabular data remains notably underdeveloped, despite its\nfoundational role in numerous real-world domains.\n  This gap is critical for three main reasons. First, database or data\nwarehouse data integration is essential for advanced applications; second, the\nvast and largely untapped resource of tabular data offers immense potential for\nanalysis; and third, the business intelligence domain specifically demands\nadaptable, precise solutions that many current LLMs may struggle to provide.\n  In response, we introduce TableGPT2, a model rigorously pre-trained and\nfine-tuned with over 593.8K tables and 2.36M high-quality query-table-output\ntuples, a scale of table-related data unprecedented in prior research. This\nextensive training enables TableGPT2 to excel in table-centric tasks while\nmaintaining strong general language and coding abilities.\n  One of TableGPT2's key innovations is its novel table encoder, specifically\ndesigned to capture schema-level and cell-level information. This encoder\nstrengthens the model's ability to handle ambiguous queries, missing column\nnames, and irregular tables commonly encountered in real-world applications.\nSimilar to visual language models, this pioneering approach integrates with the\ndecoder to form a robust large multimodal model.\n  We believe the results are compelling: over 23 benchmarking metrics,\nTableGPT2 achieves an average performance improvement of 35.20% in the 7B model\nand 49.32% in the 72B model over prior benchmark-neutral LLMs, with robust\ngeneral-purpose capabilities intact.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02059v3",
    "published_date": "2024-11-04 13:03:13 UTC",
    "updated_date": "2024-11-07 03:32:44 UTC"
  },
  {
    "arxiv_id": "2411.02041v1",
    "title": "Enhancing ID-based Recommendation with Large Language Models",
    "authors": [
      "Lei Chen",
      "Chen Gao",
      "Xiaoyi Du",
      "Hengliang Luo",
      "Depeng Jin",
      "Yong Li",
      "Meng Wang"
    ],
    "abstract": "Large Language Models (LLMs) have recently garnered significant attention in\nvarious domains, including recommendation systems. Recent research leverages\nthe capabilities of LLMs to improve the performance and user modeling aspects\nof recommender systems. These studies primarily focus on utilizing LLMs to\ninterpret textual data in recommendation tasks. However, it's worth noting that\nin ID-based recommendations, textual data is absent, and only ID data is\navailable. The untapped potential of LLMs for ID data within the ID-based\nrecommendation paradigm remains relatively unexplored. To this end, we\nintroduce a pioneering approach called \"LLM for ID-based Recommendation\"\n(LLM4IDRec). This innovative approach integrates the capabilities of LLMs while\nexclusively relying on ID data, thus diverging from the previous reliance on\ntextual data. The basic idea of LLM4IDRec is that by employing LLM to augment\nID data, if augmented ID data can improve recommendation performance, it\ndemonstrates the ability of LLM to interpret ID data effectively, exploring an\ninnovative way for the integration of LLM in ID-based recommendation. We\nevaluate the effectiveness of our LLM4IDRec approach using three widely-used\ndatasets. Our results demonstrate a notable improvement in recommendation\nperformance, with our approach consistently outperforming existing methods in\nID-based recommendation by solely augmenting input data.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02041v1",
    "published_date": "2024-11-04 12:43:12 UTC",
    "updated_date": "2024-11-04 12:43:12 UTC"
  },
  {
    "arxiv_id": "2411.02036v1",
    "title": "Explainable cognitive decline detection in free dialogues with a Machine Learning approach based on pre-trained Large Language Models",
    "authors": [
      "Francisco de Arriba-Pérez",
      "Silvia García-Méndez",
      "Javier Otero-Mosquera",
      "Francisco J. González-Castaño"
    ],
    "abstract": "Cognitive and neurological impairments are very common, but only a small\nproportion of affected individuals are diagnosed and treated, partly because of\nthe high costs associated with frequent screening. Detecting pre-illness stages\nand analyzing the progression of neurological disorders through effective and\nefficient intelligent systems can be beneficial for timely diagnosis and early\nintervention. We propose using Large Language Models to extract features from\nfree dialogues to detect cognitive decline. These features comprise high-level\nreasoning content-independent features (such as comprehension, decreased\nawareness, increased distraction, and memory problems). Our solution comprises\n(i) preprocessing, (ii) feature engineering via Natural Language Processing\ntechniques and prompt engineering, (iii) feature analysis and selection to\noptimize performance, and (iv) classification, supported by automatic\nexplainability. We also explore how to improve Chatgpt's direct cognitive\nimpairment prediction capabilities using the best features in our models.\nEvaluation metrics obtained endorse the effectiveness of a mixed approach\ncombining feature extraction with Chatgpt and a specialized Machine Learning\nmodel to detect cognitive decline within free-form conversational dialogues\nwith older adults. Ultimately, our work may facilitate the development of an\ninexpensive, non-invasive, and rapid means of detecting and explaining\ncognitive decline.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02036v1",
    "published_date": "2024-11-04 12:38:08 UTC",
    "updated_date": "2024-11-04 12:38:08 UTC"
  },
  {
    "arxiv_id": "2411.02035v1",
    "title": "SibylSat: Using SAT as an Oracle to Perform a Greedy Search on TOHTN Planning",
    "authors": [
      "Gaspard Quenard",
      "Damier Pellier",
      "Humbert Fiorino"
    ],
    "abstract": "This paper presents SibylSat, a novel SAT-based method designed to\nefficiently solve totally-ordered HTN problems (TOHTN). In contrast to\nprevailing SAT-based HTN planners that employ a breadth-first search strategy,\nSibylSat adopts a greedy search approach, enabling it to identify promising\ndecompositions for expansion. The selection process is facilitated by a\nheuristic derived from solving a relaxed problem, which is also expressed as a\nSAT problem. Our experimental evaluations demonstrate that SibylSat outperforms\nexisting SAT-based TOHTN approaches in terms of both runtime and plan quality\non most of the IPC benchmarks, while also solving a larger number of problems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02035v1",
    "published_date": "2024-11-04 12:37:59 UTC",
    "updated_date": "2024-11-04 12:37:59 UTC"
  },
  {
    "arxiv_id": "2411.02466v1",
    "title": "Weakly supervised deep learning model with size constraint for prostate cancer detection in multiparametric MRI and generalization to unseen domains",
    "authors": [
      "Robin Trombetta",
      "Olivier Rouvière",
      "Carole Lartizien"
    ],
    "abstract": "Fully supervised deep models have shown promising performance for many\nmedical segmentation tasks. Still, the deployment of these tools in clinics is\nlimited by the very timeconsuming collection of manually expert-annotated data.\nMoreover, most of the state-ofthe-art models have been trained and validated on\nmoderately homogeneous datasets. It is known that deep learning methods are\noften greatly degraded by domain or label shifts and are yet to be built in\nsuch a way as to be robust to unseen data or label distributions. In the\nclinical setting, this problematic is particularly relevant as the deployment\ninstitutions may have different scanners or acquisition protocols than those\nfrom which the data has been collected to train the model. In this work, we\npropose to address these two challenges on the detection of clinically\nsignificant prostate cancer (csPCa) from bi-parametric MRI. We evaluate the\nmethod proposed by (Kervadec et al., 2018), which introduces a size constaint\nloss to produce fine semantic cancer lesions segmentations from weak circle\nscribbles annotations. Performance of the model is based on two public (PI-CAI\nand Prostate158) and one private databases. First, we show that the model\nachieves on-par performance with strong fully supervised baseline models, both\non in-distribution validation data and unseen test images. Second, we observe a\nperformance decrease for both fully supervised and weakly supervised models\nwhen tested on unseen data domains. This confirms the crucial need for\nefficient domain adaptation methods if deep learning models are aimed to be\ndeployed in a clinical environment. Finally, we show that ensemble predictions\nfrom multiple trainings increase generalization performance.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02466v1",
    "published_date": "2024-11-04 12:24:33 UTC",
    "updated_date": "2024-11-04 12:24:33 UTC"
  },
  {
    "arxiv_id": "2411.02026v1",
    "title": "CTEFM-VC: Zero-Shot Voice Conversion Based on Content-Aware Timbre Ensemble Modeling and Flow Matching",
    "authors": [
      "Yu Pan",
      "Yuguang Yang",
      "Jixun Yao",
      "Jianhao Ye",
      "Hongbin Zhou",
      "Lei Ma",
      "Jianjun Zhao"
    ],
    "abstract": "Zero-shot voice conversion (VC) aims to transform the timbre of a source\nspeaker into any previously unseen target speaker, while preserving the\noriginal linguistic content. Despite notable progress, attaining a degree of\nspeaker similarity and naturalness on par with ground truth recordings\ncontinues to pose great challenge. In this paper, we propose CTEFM-VC, a\nzero-shot VC framework that leverages Content-aware Timbre Ensemble modeling\nand Flow Matching. Specifically, CTEFM-VC disentangles utterances into\nlinguistic content and timbre representations, subsequently utilizing a\nconditional flow matching model and a vocoder to reconstruct the\nmel-spectrogram and waveform. To enhance its timbre modeling capability and the\nnaturalness of generated speech, we propose a context-aware timbre ensemble\nmodeling approach that adaptively integrates diverse speaker verification\nembeddings and enables the joint utilization of linguistic and timbre features\nthrough a cross-attention module. Experiments show that our CTEFM-VC system\nsurpasses state-of-the-art VC methods in both speaker similarity and\nnaturalness by at least 18.5% and 7.0%.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Work in progress; 5 pages;",
    "pdf_url": "http://arxiv.org/pdf/2411.02026v1",
    "published_date": "2024-11-04 12:23:17 UTC",
    "updated_date": "2024-11-04 12:23:17 UTC"
  },
  {
    "arxiv_id": "2411.02018v2",
    "title": "Shortcut Learning in In-Context Learning: A Survey",
    "authors": [
      "Rui Song",
      "Yingji Li",
      "Lida Shi",
      "Fausto Giunchiglia",
      "Hao Xu"
    ],
    "abstract": "Shortcut learning refers to the phenomenon where models employ simple,\nnon-robust decision rules in practical tasks, which hinders their\ngeneralization and robustness. With the rapid development of large language\nmodels (LLMs) in recent years, an increasing number of studies have shown the\nimpact of shortcut learning on LLMs. This paper provides a novel perspective to\nreview relevant research on shortcut learning in In-Context Learning (ICL). It\nconducts a detailed exploration of the types of shortcuts in ICL tasks, their\ncauses, available benchmarks, and strategies for mitigating shortcuts. Based on\ncorresponding observations, it summarizes the unresolved issues in existing\nresearch and attempts to outline the future research landscape of shortcut\nlearning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "20 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.02018v2",
    "published_date": "2024-11-04 12:13:04 UTC",
    "updated_date": "2024-11-28 11:00:19 UTC"
  },
  {
    "arxiv_id": "2411.02006v1",
    "title": "Foundations and Recent Trends in Multimodal Mobile Agents: A Survey",
    "authors": [
      "Biao Wu",
      "Yanda Li",
      "Meng Fang",
      "Zirui Song",
      "Zhiwei Zhang",
      "Yunchao Wei",
      "Ling Chen"
    ],
    "abstract": "Mobile agents are essential for automating tasks in complex and dynamic\nmobile environments. As foundation models evolve, the demands for agents that\ncan adapt in real-time and process multimodal data have grown. This survey\nprovides a comprehensive review of mobile agent technologies, focusing on\nrecent advancements that enhance real-time adaptability and multimodal\ninteraction. Recent evaluation benchmarks have been developed better to capture\nthe static and interactive environments of mobile tasks, offering more accurate\nassessments of agents' performance. We then categorize these advancements into\ntwo main approaches: prompt-based methods, which utilize large language models\n(LLMs) for instruction-based task execution, and training-based methods, which\nfine-tune multimodal models for mobile-specific applications. Additionally, we\nexplore complementary technologies that augment agent performance. By\ndiscussing key challenges and outlining future research directions, this survey\noffers valuable insights for advancing mobile agent technologies. A\ncomprehensive resource list is available at\nhttps://github.com/aialt/awesome-mobile-agents",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "8 pages, 1 figure",
    "pdf_url": "http://arxiv.org/pdf/2411.02006v1",
    "published_date": "2024-11-04 11:50:58 UTC",
    "updated_date": "2024-11-04 11:50:58 UTC"
  },
  {
    "arxiv_id": "2411.02003v1",
    "title": "Against Multifaceted Graph Heterogeneity via Asymmetric Federated Prompt Learning",
    "authors": [
      "Zhuoning Guo",
      "Ruiqian Han",
      "Hao Liu"
    ],
    "abstract": "Federated Graph Learning (FGL) aims to collaboratively and privately optimize\ngraph models on divergent data for different tasks. A critical challenge in FGL\nis to enable effective yet efficient federated optimization against\nmultifaceted graph heterogeneity to enhance mutual performance. However,\nexisting FGL works primarily address graph data heterogeneity and perform\nincapable of graph task heterogeneity. To address the challenge, we propose a\nFederated Graph Prompt Learning (FedGPL) framework to efficiently enable\nprompt-based asymmetric graph knowledge transfer between multifaceted\nheterogeneous federated participants. Generally, we establish a split federated\nframework to preserve universal and domain-specific graph knowledge,\nrespectively. Moreover, we develop two algorithms to eliminate task and data\nheterogeneity for advanced federated knowledge preservation. First, a\nHierarchical Directed Transfer Aggregator (HiDTA) delivers cross-task\nbeneficial knowledge that is hierarchically distilled according to the\ndirectional transferability. Second, a Virtual Prompt Graph (VPG) adaptively\ngenerates graph structures to enhance data utility by distinguishing dominant\nsubgraphs and neutralizing redundant ones. We conduct theoretical analyses and\nextensive experiments to demonstrate the significant accuracy and efficiency\neffectiveness of FedGPL against multifaceted graph heterogeneity compared to\nstate-of-the-art baselines on large-scale federated graph datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02003v1",
    "published_date": "2024-11-04 11:42:25 UTC",
    "updated_date": "2024-11-04 11:42:25 UTC"
  },
  {
    "arxiv_id": "2411.01996v1",
    "title": "Culinary Class Wars: Evaluating LLMs using ASH in Cuisine Transfer Task",
    "authors": [
      "Hoonick Lee",
      "Mogan Gim",
      "Donghyeon Park",
      "Donghee Choi",
      "Jaewoo Kang"
    ],
    "abstract": "The advent of Large Language Models (LLMs) have shown promise in various\ncreative domains, including culinary arts. However, many LLMs still struggle to\ndeliver the desired level of culinary creativity, especially when tasked with\nadapting recipes to meet specific cultural requirements. This study focuses on\ncuisine transfer-applying elements of one cuisine to another-to assess LLMs'\nculinary creativity. We employ a diverse set of LLMs to generate and evaluate\nculturally adapted recipes, comparing their evaluations against LLM and human\njudgments. We introduce the ASH (authenticity, sensitivity, harmony) benchmark\nto evaluate LLMs' recipe generation abilities in the cuisine transfer task,\nassessing their cultural accuracy and creativity in the culinary domain. Our\nfindings reveal crucial insights into both generative and evaluative\ncapabilities of LLMs in the culinary domain, highlighting strengths and\nlimitations in understanding and applying cultural nuances in recipe creation.\nThe code and dataset used in this project will be openly available in\n\\url{http://github.com/dmis-lab/CulinaryASH}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.01996v1",
    "published_date": "2024-11-04 11:31:18 UTC",
    "updated_date": "2024-11-04 11:31:18 UTC"
  },
  {
    "arxiv_id": "2411.01978v1",
    "title": "Understanding Variational Autoencoders with Intrinsic Dimension and Information Imbalance",
    "authors": [
      "Charles Camboulin",
      "Diego Doimo",
      "Aldo Glielmo"
    ],
    "abstract": "This work presents an analysis of the hidden representations of Variational\nAutoencoders (VAEs) using the Intrinsic Dimension (ID) and the Information\nImbalance (II). We show that VAEs undergo a transition in behaviour once the\nbottleneck size is larger than the ID of the data, manifesting in a double\nhunchback ID profile and a qualitative shift in information processing as\ncaptured by the II. Our results also highlight two distinct training phases for\narchitectures with sufficiently large bottleneck sizes, consisting of a rapid\nfit and a slower generalisation, as assessed by a differentiated behaviour of\nID, II, and KL loss. These insights demonstrate that II and ID could be\nvaluable tools for aiding architecture search, for diagnosing underfitting in\nVAEs, and, more broadly, they contribute to advancing a unified understanding\nof deep generative models through geometric analysis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML",
      "I.2.6"
    ],
    "primary_category": "cs.LG",
    "comment": "4 pages, 3 figures, accepted at the Unifying Representations in\n  Neural Models (UniReps) workshop of NeurIPS 2024 (https://unireps.org/2024/)",
    "pdf_url": "http://arxiv.org/pdf/2411.01978v1",
    "published_date": "2024-11-04 10:58:41 UTC",
    "updated_date": "2024-11-04 10:58:41 UTC"
  },
  {
    "arxiv_id": "2411.01969v2",
    "title": "Toddlers' Active Gaze Behavior Supports Self-Supervised Object Learning",
    "authors": [
      "Zhengyang Yu",
      "Arthur Aubret",
      "Marcel C. Raabe",
      "Jane Yang",
      "Chen Yu",
      "Jochen Triesch"
    ],
    "abstract": "Toddlers learn to recognize objects from different viewpoints with almost no\nsupervision. Recent works argue that toddlers develop this ability by mapping\nclose-in-time visual inputs to similar representations while interacting with\nobjects. High acuity vision is only available in the central visual field,\nwhich May explain why toddlers (much like adults) constantly move around their\ngaze during such interactions. It is unclear whether/how much toddlers curate\ntheir visual experience through these eye movements to support their learning\nof object representations. In this work, we explore whether a bio-inspired\nvisual learning model can harness toddlers' gaze behavior during a play session\nto develop view-invariant object recognition. Exploiting head-mounted eye\ntracking during dyadic play, we simulate toddlers' central visual field\nexperience by cropping image regions centered on the gaze location. This visual\nstream feeds time-based self-supervised learning algorithms. Our experiments\ndemonstrate that toddlers' gaze strategy supports the learning of invariant\nobject representations. Our analysis also reveals that the limited size of the\ncentral visual field where acuity is high is crucial for this. We further find\nthat toddlers' visual experience elicits more robust representations compared\nto adults', mostly because toddlers look at objects they hold themselves for\nlonger bouts. Overall, our work reveals how toddlers' gaze behavior supports\nself-supervised learning of view-invariant object recognition.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "20 pages, 15 figures",
    "pdf_url": "http://arxiv.org/pdf/2411.01969v2",
    "published_date": "2024-11-04 10:44:46 UTC",
    "updated_date": "2025-02-12 09:58:15 UTC"
  },
  {
    "arxiv_id": "2411.01963v1",
    "title": "V-CAS: A Realtime Vehicle Anti Collision System Using Vision Transformer on Multi-Camera Streams",
    "authors": [
      "Muhammad Waqas Ashraf",
      "Ali Hassan",
      "Imad Ali Shah"
    ],
    "abstract": "This paper introduces a real-time Vehicle Collision Avoidance System (V-CAS)\ndesigned to enhance vehicle safety through adaptive braking based on\nenvironmental perception. V-CAS leverages the advanced vision-based transformer\nmodel RT-DETR, DeepSORT tracking, speed estimation, brake light detection, and\nan adaptive braking mechanism. It computes a composite collision risk score\nbased on vehicles' relative accelerations, distances, and detected braking\nactions, using brake light signals and trajectory data from multiple camera\nstreams to improve scene perception. Implemented on the Jetson Orin Nano, V-CAS\nenables real-time collision risk assessment and proactive mitigation through\nadaptive braking. A comprehensive training process was conducted on various\ndatasets for comparative analysis, followed by fine-tuning the selected object\ndetection model using transfer learning. The system's effectiveness was\nrigorously evaluated on the Car Crash Dataset (CCD) from YouTube and through\nreal-time experiments, achieving over 98% accuracy with an average proactive\nalert time of 1.13 seconds. Results indicate significant improvements in object\ndetection and tracking, enhancing collision avoidance compared to traditional\nsingle-camera methods. This research demonstrates the potential of low-cost,\nmulti-camera embedded vision transformer systems to advance automotive safety\nthrough enhanced environmental perception and proactive collision avoidance\nmechanisms.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Accepted at ICMLA 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.01963v1",
    "published_date": "2024-11-04 10:39:15 UTC",
    "updated_date": "2024-11-04 10:39:15 UTC"
  },
  {
    "arxiv_id": "2411.02465v1",
    "title": "See it, Think it, Sorted: Large Multimodal Models are Few-shot Time Series Anomaly Analyzers",
    "authors": [
      "Jiaxin Zhuang",
      "Leon Yan",
      "Zhenwei Zhang",
      "Ruiqi Wang",
      "Jiawei Zhang",
      "Yuantao Gu"
    ],
    "abstract": "Time series anomaly detection (TSAD) is becoming increasingly vital due to\nthe rapid growth of time series data across various sectors. Anomalies in web\nservice data, for example, can signal critical incidents such as system\nfailures or server malfunctions, necessitating timely detection and response.\nHowever, most existing TSAD methodologies rely heavily on manual feature\nengineering or require extensive labeled training data, while also offering\nlimited interpretability. To address these challenges, we introduce a\npioneering framework called the Time Series Anomaly Multimodal Analyzer (TAMA),\nwhich leverages the power of Large Multimodal Models (LMMs) to enhance both the\ndetection and interpretation of anomalies in time series data. By converting\ntime series into visual formats that LMMs can efficiently process, TAMA\nleverages few-shot in-context learning capabilities to reduce dependence on\nextensive labeled datasets. Our methodology is validated through rigorous\nexperimentation on multiple real-world datasets, where TAMA consistently\noutperforms state-of-the-art methods in TSAD tasks. Additionally, TAMA provides\nrich, natural language-based semantic analysis, offering deeper insights into\nthe nature of detected anomalies. Furthermore, we contribute one of the first\nopen-source datasets that includes anomaly detection labels, anomaly type\nlabels, and contextual description, facilitating broader exploration and\nadvancement within this critical field. Ultimately, TAMA not only excels in\nanomaly detection but also provides a comprehensive approach for understanding\nthe underlying causes of anomalies, pushing TSAD forward through innovative\nmethodologies and insights.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2411.02465v1",
    "published_date": "2024-11-04 10:28:41 UTC",
    "updated_date": "2024-11-04 10:28:41 UTC"
  },
  {
    "arxiv_id": "2411.01952v2",
    "title": "Evaluating the quality of published medical research with ChatGPT",
    "authors": [
      "Mike Thelwall",
      "Xiaorui Jiang",
      "Peter A. Bath"
    ],
    "abstract": "Estimating the quality of published research is important for evaluations of\ndepartments, researchers, and job candidates. Citation-based indicators\nsometimes support these tasks, but do not work for new articles and have low or\nmoderate accuracy. Previous research has shown that ChatGPT can estimate the\nquality of research articles, with its scores correlating positively with an\nexpert scores proxy in all fields, and often more strongly than citation-based\nindicators, except for clinical medicine. ChatGPT scores may therefore replace\ncitation-based indicators for some applications. This article investigates the\nclinical medicine anomaly with the largest dataset yet and a more detailed\nanalysis. The results showed that ChatGPT 4o-mini scores for articles submitted\nto the UK's Research Excellence Framework (REF) 2021 Unit of Assessment (UoA) 1\nClinical Medicine correlated positively (r=0.134, n=9872) with departmental\nmean REF scores, against a theoretical maximum correlation of r=0.226. ChatGPT\n4o and 3.5 turbo also gave positive correlations. At the departmental level,\nmean ChatGPT scores correlated more strongly with departmental mean REF scores\n(r=0.395, n=31). For the 100 journals with the most articles in UoA 1, their\nmean ChatGPT score correlated strongly with their REF score (r=0.495) but\nnegatively with their citation rate (r=-0.148). Journal and departmental\nanomalies in these results point to ChatGPT being ineffective at assessing the\nquality of research in prestigious medical journals or research directly\naffecting human health, or both. Nevertheless, the results give evidence of\nChatGPT's ability to assess research quality overall for Clinical Medicine,\nwhere it might replace citation-based indicators for new research.",
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "primary_category": "cs.DL",
    "comment": "Information Processing & Management (2025)",
    "pdf_url": "http://arxiv.org/pdf/2411.01952v2",
    "published_date": "2024-11-04 10:24:36 UTC",
    "updated_date": "2025-03-03 15:46:33 UTC"
  },
  {
    "arxiv_id": "2411.02464v1",
    "title": "You are out of context!",
    "authors": [
      "Giancarlo Cobino",
      "Simone Farci"
    ],
    "abstract": "This research proposes a novel drift detection methodology for machine\nlearning (ML) models based on the concept of ''deformation'' in the vector\nspace representation of data. Recognizing that new data can act as forces\nstretching, compressing, or twisting the geometric relationships learned by a\nmodel, we explore various mathematical frameworks to quantify this deformation.\nWe investigate measures such as eigenvalue analysis of covariance matrices to\ncapture global shape changes, local density estimation using kernel density\nestimation (KDE), and Kullback-Leibler divergence to identify subtle shifts in\ndata concentration. Additionally, we draw inspiration from continuum mechanics\nby proposing a ''strain tensor'' analogy to capture multi-faceted deformations\nacross different data types. This requires careful estimation of the\ndisplacement field, and we delve into strategies ranging from density-based\napproaches to manifold learning and neural network methods. By continuously\nmonitoring these deformation metrics and correlating them with model\nperformance, we aim to provide a sensitive, interpretable, and adaptable drift\ndetection system capable of distinguishing benign data evolution from true\ndrift, enabling timely interventions and ensuring the reliability of machine\nlearning systems in dynamic environments. Addressing the computational\nchallenges of this methodology, we discuss mitigation strategies like\ndimensionality reduction, approximate algorithms, and parallelization for\nreal-time and large-scale applications. The method's effectiveness is\ndemonstrated through experiments on real-world text data, focusing on detecting\ncontext shifts in Generative AI. Our results, supported by publicly available\ncode, highlight the benefits of this deformation-based approach in capturing\nsubtle drifts that traditional statistical methods often miss. Furthermore, we\npresent a detailed application example within the healthcare domain, showcasing\nthe methodology's potential in diverse fields. Future work will focus on\nfurther improving computational efficiency and exploring additional\napplications across different ML domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02464v1",
    "published_date": "2024-11-04 10:17:43 UTC",
    "updated_date": "2024-11-04 10:17:43 UTC"
  },
  {
    "arxiv_id": "2411.01947v1",
    "title": "HACD: Harnessing Attribute Semantics and Mesoscopic Structure for Community Detection",
    "authors": [
      "Anran Zhang",
      "Xingfen Wang",
      "Yuhan Zhao"
    ],
    "abstract": "Community detection plays a pivotal role in uncovering closely connected\nsubgraphs, aiding various real-world applications such as recommendation\nsystems and anomaly detection. With the surge of rich information available for\nentities in real-world networks, the community detection problem in attributed\nnetworks has attracted widespread attention. While previous research has\neffectively leveraged network topology and attribute information for attributed\ncommunity detection, these methods overlook two critical issues: (i) the\nsemantic similarity between node attributes within the community, and (ii) the\ninherent mesoscopic structure, which differs from the pairwise connections of\nthe micro-structure. To address these limitations, we propose HACD, a novel\nattributed community detection model based on heterogeneous graph attention\nnetworks. HACD treats node attributes as another type of node, constructs\nattributed networks into heterogeneous graph structures and employs\nattribute-level attention mechanisms to capture semantic similarity.\nFurthermore, HACD introduces a community membership function to explore\nmesoscopic community structures, enhancing the robustness of detected\ncommunities. Extensive experiments demonstrate the effectiveness and efficiency\nof HACD, outperforming state-of-the-art methods in attributed community\ndetection tasks. Our code is publicly available at\nhttps://github.com/Anniran1/HACD1-wsdm.",
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.SI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.01947v1",
    "published_date": "2024-11-04 10:16:59 UTC",
    "updated_date": "2024-11-04 10:16:59 UTC"
  },
  {
    "arxiv_id": "2411.01929v2",
    "title": "Exploring the Landscape for Generative Sequence Models for Specialized Data Synthesis",
    "authors": [
      "Mohammad Zbeeb",
      "Mohammad Ghorayeb",
      "Mariam Salman"
    ],
    "abstract": "Artificial Intelligence (AI) research often aims to develop models that can\ngeneralize reliably across complex datasets, yet this remains challenging in\nfields where data is scarce, intricate, or inaccessible. This paper introduces\na novel approach that leverages three generative models of varying complexity\nto synthesize one of the most demanding structured datasets: Malicious Network\nTraffic. Our approach uniquely transforms numerical data into text, re-framing\ndata generation as a language modeling task, which not only enhances data\nregularization but also significantly improves generalization and the quality\nof the synthetic data. Extensive statistical analyses demonstrate that our\nmethod surpasses state-of-the-art generative models in producing high-fidelity\nsynthetic data. Additionally, we conduct a comprehensive study on synthetic\ndata applications, effectiveness, and evaluation strategies, offering valuable\ninsights into its role across various domains. Our code and pre-trained models\nare openly accessible at Github, enabling further exploration and application\nof our methodology. Index Terms: Data synthesis, machine learning, traffic\ngeneration, privacy preserving data, generative models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 7 figures, 3 tables, 1 algorithm. code @\n  https://github.com/Moe-Zbeeb/Exploring-the-landscape-for-generative-models-for-specialized-data-generation.git",
    "pdf_url": "http://arxiv.org/pdf/2411.01929v2",
    "published_date": "2024-11-04 09:51:10 UTC",
    "updated_date": "2024-11-06 16:50:44 UTC"
  },
  {
    "arxiv_id": "2411.01924v1",
    "title": "Fairness-Utilization Trade-off in Wireless Networks with Explainable Kolmogorov-Arnold Networks",
    "authors": [
      "Masoud Shokrnezhad",
      "Hamidreza Mazandarani",
      "Tarik Taleb"
    ],
    "abstract": "The effective distribution of user transmit powers is essential for the\nsignificant advancements that the emergence of 6G wireless networks brings. In\nrecent studies, Deep Neural Networks (DNNs) have been employed to address this\nchallenge. However, these methods frequently encounter issues regarding\nfairness and computational inefficiency when making decisions, rendering them\nunsuitable for future dynamic services that depend heavily on the participation\nof each individual user. To address this gap, this paper focuses on the\nchallenge of transmit power allocation in wireless networks, aiming to optimize\n$\\alpha$-fairness to balance network utilization and user equity. We introduce\na novel approach utilizing Kolmogorov-Arnold Networks (KANs), a class of\nmachine learning models that offer low inference costs compared to traditional\nDNNs through superior explainability. The study provides a comprehensive\nproblem formulation, establishing the NP-hardness of the power allocation\nproblem. Then, two algorithms are proposed for dataset generation and\ndecentralized KAN training, offering a flexible framework for achieving various\nfairness objectives in dynamic 6G environments. Extensive numerical simulations\ndemonstrate the effectiveness of our approach in terms of fairness and\ninference cost. The results underscore the potential of KANs to overcome the\nlimitations of existing DNN-based methods, particularly in scenarios that\ndemand rapid adaptation and fairness.",
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.ET",
      "cs.LG",
      "cs.NA",
      "math.NA"
    ],
    "primary_category": "cs.NI",
    "comment": "a conference paper, accepted for publication at IEEE VCC 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.01924v1",
    "published_date": "2024-11-04 09:40:47 UTC",
    "updated_date": "2024-11-04 09:40:47 UTC"
  },
  {
    "arxiv_id": "2411.01898v1",
    "title": "Best-Arm Identification in Unimodal Bandits",
    "authors": [
      "Riccardo Poiani",
      "Marc Jourdan",
      "Emilie Kaufmann",
      "Rémy Degenne"
    ],
    "abstract": "We study the fixed-confidence best-arm identification problem in unimodal\nbandits, in which the means of the arms increase with the index of the arm up\nto their maximum, then decrease. We derive two lower bounds on the stopping\ntime of any algorithm. The instance-dependent lower bound suggests that due to\nthe unimodal structure, only three arms contribute to the leading\nconfidence-dependent cost. However, a worst-case lower bound shows that a\nlinear dependence on the number of arms is unavoidable in the\nconfidence-independent cost. We propose modifications of Track-and-Stop and a\nTop Two algorithm that leverage the unimodal structure. Both versions of\nTrack-and-Stop are asymptotically optimal for one-parameter exponential\nfamilies. The Top Two algorithm is asymptotically near-optimal for Gaussian\ndistributions and we prove a non-asymptotic guarantee matching the worse-case\nlower bound. The algorithms can be implemented efficiently and we demonstrate\ntheir competitive empirical performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.01898v1",
    "published_date": "2024-11-04 09:05:11 UTC",
    "updated_date": "2024-11-04 09:05:11 UTC"
  },
  {
    "arxiv_id": "2411.01897v2",
    "title": "LE-PDE++: Mamba for accelerating PDEs Simulations",
    "authors": [
      "Aoming Liang",
      "Zhaoyang Mu",
      "Qi liu",
      "Ruipeng Li",
      "Mingming Ge",
      "Dixia Fan"
    ],
    "abstract": "Partial Differential Equations are foundational in modeling science and\nnatural systems such as fluid dynamics and weather forecasting. The Latent\nEvolution of PDEs method is designed to address the computational intensity of\nclassical and deep learning-based PDE solvers by proposing a scalable and\nefficient alternative. To enhance the efficiency and accuracy of LE-PDE, we\nincorporate the Mamba model, an advanced machine learning model known for its\npredictive efficiency and robustness in handling complex dynamic systems with a\nprogressive learning strategy. The LE-PDE was tested on several benchmark\nproblems. The method demonstrated a marked reduction in computational time\ncompared to traditional solvers and standalone deep learning models while\nmaintaining high accuracy in predicting system behavior over time. Our method\ndoubles the inference speed compared to the LE-PDE while retaining the same\nlevel of parameter efficiency, making it well-suited for scenarios requiring\nlong-term predictions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.01897v2",
    "published_date": "2024-11-04 09:04:11 UTC",
    "updated_date": "2024-11-12 16:48:29 UTC"
  },
  {
    "arxiv_id": "2411.01896v1",
    "title": "MBDRes-U-Net: Multi-Scale Lightweight Brain Tumor Segmentation Network",
    "authors": [
      "Longfeng Shen",
      "Yanqi Hou",
      "Jiacong Chen",
      "Liangjin Diao",
      "Yaxi Duan"
    ],
    "abstract": "Accurate segmentation of brain tumors plays a key role in the diagnosis and\ntreatment of brain tumor diseases. It serves as a critical technology for\nquantifying tumors and extracting their features. With the increasing\napplication of deep learning methods, the computational burden has become\nprogressively heavier. To achieve a lightweight model with good segmentation\nperformance, this study proposes the MBDRes-U-Net model using the\nthree-dimensional (3D) U-Net codec framework, which integrates multibranch\nresidual blocks and fused attention into the model. The computational burden of\nthe model is reduced by the branch strategy, which effectively uses the rich\nlocal features in multimodal images and enhances the segmentation performance\nof subtumor regions. Additionally, during encoding, an adaptive weighted\nexpansion convolution layer is introduced into the multi-branch residual block,\nwhich enriches the feature expression and improves the segmentation accuracy of\nthe model. Experiments on the Brain Tumor Segmentation (BraTS) Challenge 2018\nand 2019 datasets show that the architecture could maintain a high precision of\nbrain tumor segmentation while considerably reducing the calculation\noverhead.Our code is released at\nhttps://github.com/Huaibei-normal-university-cv-laboratory/mbdresunet",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "Brain tumor segmentation, lightweight model, Brain Tumor Segmentation\n  (BraTS) Challenge, group convolution",
    "pdf_url": "http://arxiv.org/pdf/2411.01896v1",
    "published_date": "2024-11-04 09:03:43 UTC",
    "updated_date": "2024-11-04 09:03:43 UTC"
  },
  {
    "arxiv_id": "2411.02462v1",
    "title": "Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study",
    "authors": [
      "André Storhaug",
      "Jingyue Li"
    ],
    "abstract": "The advent of large language models (LLMs) like GitHub Copilot has\nsignificantly enhanced programmers' productivity, particularly in code\ngeneration. However, these models often struggle with real-world tasks without\nfine-tuning. As LLMs grow larger and more performant, fine-tuning for\nspecialized tasks becomes increasingly expensive. Parameter-efficient\nfine-tuning (PEFT) methods, which fine-tune only a subset of model parameters,\noffer a promising solution by reducing the computational costs of tuning LLMs\nwhile maintaining their performance. Existing studies have explored using PEFT\nand LLMs for various code-related tasks and found that the effectiveness of\nPEFT techniques is task-dependent. The application of PEFT techniques in unit\ntest generation remains underexplored. The state-of-the-art is limited to using\nLLMs with full fine-tuning to generate unit tests. This paper investigates both\nfull fine-tuning and various PEFT methods, including LoRA, (IA)^3, and prompt\ntuning, across different model architectures and sizes. We use well-established\nbenchmark datasets to evaluate their effectiveness in unit test generation. Our\nfindings show that PEFT methods can deliver performance comparable to full\nfine-tuning for unit test generation, making specialized fine-tuning more\naccessible and cost-effective. Notably, prompt tuning is the most effective in\nterms of cost and resource utilization, while LoRA approaches the effectiveness\nof full fine-tuning in several cases.",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.SE",
    "comment": "12 pages, 3 figures, 4 tables, 1 listing",
    "pdf_url": "http://arxiv.org/pdf/2411.02462v1",
    "published_date": "2024-11-04 09:03:18 UTC",
    "updated_date": "2024-11-04 09:03:18 UTC"
  },
  {
    "arxiv_id": "2411.01889v1",
    "title": "LiDAttack: Robust Black-box Attack on LiDAR-based Object Detection",
    "authors": [
      "Jinyin Chen",
      "Danxin Liao",
      "Sheng Xiang",
      "Haibin Zheng"
    ],
    "abstract": "Since DNN is vulnerable to carefully crafted adversarial examples,\nadversarial attack on LiDAR sensors have been extensively studied. We introduce\na robust black-box attack dubbed LiDAttack. It utilizes a genetic algorithm\nwith a simulated annealing strategy to strictly limit the location and number\nof perturbation points, achieving a stealthy and effective attack. And it\nsimulates scanning deviations, allowing it to adapt to dynamic changes in real\nworld scenario variations. Extensive experiments are conducted on 3 datasets\n(i.e., KITTI, nuScenes, and self-constructed data) with 3 dominant object\ndetection models (i.e., PointRCNN, PointPillar, and PV-RCNN++). The results\nreveal the efficiency of the LiDAttack when targeting a wide range of object\ndetection models, with an attack success rate (ASR) up to 90%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.01889v1",
    "published_date": "2024-11-04 08:37:12 UTC",
    "updated_date": "2024-11-04 08:37:12 UTC"
  },
  {
    "arxiv_id": "2411.02461v1",
    "title": "Enhancing Multiple Dimensions of Trustworthiness in LLMs via Sparse Activation Control",
    "authors": [
      "Yuxin Xiao",
      "Chaoqun Wan",
      "Yonggang Zhang",
      "Wenxiao Wang",
      "Binbin Lin",
      "Xiaofei He",
      "Xu Shen",
      "Jieping Ye"
    ],
    "abstract": "As the development and application of Large Language Models (LLMs) continue\nto advance rapidly, enhancing their trustworthiness and aligning them with\nhuman preferences has become a critical area of research. Traditional methods\nrely heavily on extensive data for Reinforcement Learning from Human Feedback\n(RLHF), but representation engineering offers a new, training-free approach.\nThis technique leverages semantic features to control the representation of\nLLM's intermediate hidden states, enabling the model to meet specific\nrequirements such as increased honesty or heightened safety awareness. However,\na significant challenge arises when attempting to fulfill multiple requirements\nsimultaneously. It proves difficult to encode various semantic contents, like\nhonesty and safety, into a singular semantic feature, restricting its\npracticality. In this work, we address this issue through ``Sparse Activation\nControl''. By delving into the intrinsic mechanisms of LLMs, we manage to\nidentify and pinpoint components that are closely related to specific tasks\nwithin the model, i.e., attention heads. These heads display sparse\ncharacteristics that allow for near-independent control over different tasks.\nOur experiments, conducted on the open-source Llama series models, have yielded\nencouraging results. The models were able to align with human preferences on\nissues of safety, factuality, and bias concurrently.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02461v1",
    "published_date": "2024-11-04 08:36:03 UTC",
    "updated_date": "2024-11-04 08:36:03 UTC"
  },
  {
    "arxiv_id": "2411.08724v1",
    "title": "QCG-Rerank: Chunks Graph Rerank with Query Expansion in Retrieval-Augmented LLMs for Tourism Domain",
    "authors": [
      "Qikai Wei",
      "Mingzhi Yang",
      "Chunlong Han",
      "Jingfu Wei",
      "Minghao Zhang",
      "Feifei Shi",
      "Huansheng Ning"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) mitigates the issue of hallucination in\nLarge Language Models (LLMs) by integrating information retrieval techniques.\nHowever, in the tourism domain, since the query is usually brief and the\ncontent in the database is diverse, existing RAG may contain a significant\namount of irrelevant or contradictory information contents after retrieval. To\naddress this challenge, we propose the QCG-Rerank model. This model first\nperforms an initial retrieval to obtain candidate chunks and then enhances\nsemantics by extracting critical information to expand the original query.\nNext, we utilize the expanded query and candidate chunks to calculate\nsimilarity scores as the initial transition probability and construct the\nchunks graph. Subsequently, We iteratively compute the transition probabilities\nbased on an initial estimate until convergence. The chunks with the highest\nscore are selected and input into the LLMs to generate responses. We evaluate\nthe model on Cultour, IIRC, StrategyQA, HotpotQA, SQuAD, and MuSiQue datasets.\nThe experimental results demonstrate the effectiveness and superiority of the\nQCG-Rerank method.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.08724v1",
    "published_date": "2024-11-04 08:15:22 UTC",
    "updated_date": "2024-11-04 08:15:22 UTC"
  },
  {
    "arxiv_id": "2411.01870v2",
    "title": "Mining and Transferring Feature-Geometry Coherence for Unsupervised Point Cloud Registration",
    "authors": [
      "Kezheng Xiong",
      "Haoen Xiang",
      "Qingshan Xu",
      "Chenglu Wen",
      "Siqi Shen",
      "Jonathan Li",
      "Cheng Wang"
    ],
    "abstract": "Point cloud registration, a fundamental task in 3D vision, has achieved\nremarkable success with learning-based methods in outdoor environments.\nUnsupervised outdoor point cloud registration methods have recently emerged to\ncircumvent the need for costly pose annotations. However, they fail to\nestablish reliable optimization objectives for unsupervised training, either\nrelying on overly strong geometric assumptions, or suffering from poor-quality\npseudo-labels due to inadequate integration of low-level geometric and\nhigh-level contextual information. We have observed that in the feature space,\nlatent new inlier correspondences tend to cluster around respective positive\nanchors that summarize features of existing inliers. Motivated by this\nobservation, we propose a novel unsupervised registration method termed INTEGER\nto incorporate high-level contextual information for reliable pseudo-label\nmining. Specifically, we propose the Feature-Geometry Coherence Mining module\nto dynamically adapt the teacher for each mini-batch of data during training\nand discover reliable pseudo-labels by considering both high-level feature\nrepresentations and low-level geometric cues. Furthermore, we propose\nAnchor-Based Contrastive Learning to facilitate contrastive learning with\nanchors for a robust feature space. Lastly, we introduce a Mixed-Density\nStudent to learn density-invariant features, addressing challenges related to\ndensity variation and low overlap in the outdoor scenario. Extensive\nexperiments on KITTI and nuScenes datasets demonstrate that our INTEGER\nachieves competitive performance in terms of accuracy and generalizability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted by NeurIPS2024",
    "pdf_url": "http://arxiv.org/pdf/2411.01870v2",
    "published_date": "2024-11-04 07:57:44 UTC",
    "updated_date": "2024-12-24 04:44:24 UTC"
  },
  {
    "arxiv_id": "2411.01866v1",
    "title": "Improving Trust Estimation in Human-Robot Collaboration Using Beta Reputation at Fine-grained Timescales",
    "authors": [
      "Resul Dagdanov",
      "Milan Andrejevic",
      "Dikai Liu",
      "Chin-Teng Lin"
    ],
    "abstract": "When interacting with each other, humans adjust their behavior based on\nperceived trust. However, to achieve similar adaptability, robots must\naccurately estimate human trust at sufficiently granular timescales during the\nhuman-robot collaboration task. A beta reputation is a popular way to formalize\na mathematical estimation of human trust. However, it relies on binary\nperformance, which updates trust estimations only after each task concludes.\nAdditionally, manually crafting a reward function is the usual method of\nbuilding a performance indicator, which is labor-intensive and time-consuming.\nThese limitations prevent efficiently capturing continuous changes in trust at\nmore granular timescales throughout the collaboration task. Therefore, this\npaper presents a new framework for the estimation of human trust using a beta\nreputation at fine-grained timescales. To achieve granularity in beta\nreputation, we utilize continuous reward values to update trust estimations at\neach timestep of a task. We construct a continuous reward function using\nmaximum entropy optimization to eliminate the need for the laborious\nspecification of a performance indicator. The proposed framework improves trust\nestimations by increasing accuracy, eliminating the need for manually crafting\na reward function, and advancing toward developing more intelligent robots. The\nsource code is publicly available.\nhttps://github.com/resuldagdanov/robot-learning-human-trust",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 7 figures, 1 table. This work has been submitted to the IEEE\n  for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2411.01866v1",
    "published_date": "2024-11-04 07:46:24 UTC",
    "updated_date": "2024-11-04 07:46:24 UTC"
  },
  {
    "arxiv_id": "2411.01851v1",
    "title": "Silver medal Solution for Image Matching Challenge 2024",
    "authors": [
      "Yian Wang"
    ],
    "abstract": "Image Matching Challenge 2024 is a competition focused on building 3D maps\nfrom diverse image sets, requiring participants to solve fundamental computer\nvision challenges in image matching across varying angles, lighting, and\nseasonal changes. This project develops a Pipeline method that combines\nmultiple advanced techniques: using pre-trained EfficientNet-B7 for initial\nfeature extraction and cosine distance-based image pair filtering, employing\nboth KeyNetAffNetHardNet and SuperPoint for keypoint feature extraction,\nutilizing AdaLAM and SuperGlue for keypoint matching, and finally applying\nPycolmap for 3D spatial analysis. The methodology achieved an excellent score\nof 0.167 on the private leaderboard, with experimental results demonstrating\nthat the combination of KeyNetAffNetHardNet and SuperPoint provides significant\nadvantages in keypoint detection and matching, particularly when dealing with\nchallenging variations in surface texture and environmental conditions that\ntypically degrade traditional algorithm performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.01851v1",
    "published_date": "2024-11-04 07:05:47 UTC",
    "updated_date": "2024-11-04 07:05:47 UTC"
  },
  {
    "arxiv_id": "2411.10464v1",
    "title": "Detecting Student Disengagement in Online Classes Using Deep Learning: A Review",
    "authors": [
      "Ahmed Mohamed",
      "Mostafa Ali",
      "Shahd Ahmed",
      "Nouran Hani",
      "Mohammed Hisham",
      "Meram Mahmoud"
    ],
    "abstract": "Student disengagement in online learning has become a critical challenge,\nparticularly post-pandemic. This review explores deep learning techniques used\nto detect disengagement, emphasizing computer vision and affective computing as\neffective approaches. We examine recent studies focusing on facial expressions,\neye movements, and posture to assess student attention, along with\nnon-face-based indicators like mouse activity. A systematic review of 38\nselected studies outlines the indicators, methods, and models employed in this\nfield, providing insights for future research on real-time engagement\nmonitoring in online classrooms",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.10464v1",
    "published_date": "2024-11-04 07:01:22 UTC",
    "updated_date": "2024-11-04 07:01:22 UTC"
  },
  {
    "arxiv_id": "2411.01844v1",
    "title": "DeMod: A Holistic Tool with Explainable Detection and Personalized Modification for Toxicity Censorship",
    "authors": [
      "Yaqiong Li",
      "Peng Zhang",
      "Hansu Gu",
      "Tun Lu",
      "Siyuan Qiao",
      "Yubo Shu",
      "Yiyang Shao",
      "Ning Gu"
    ],
    "abstract": "Although there have been automated approaches and tools supporting toxicity\ncensorship for social posts, most of them focus on detection. Toxicity\ncensorship is a complex process, wherein detection is just an initial task and\na user can have further needs such as rationale understanding and content\nmodification. For this problem, we conduct a needfinding study to investigate\npeople's diverse needs in toxicity censorship and then build a ChatGPT-based\ncensorship tool named DeMod accordingly. DeMod is equipped with the features of\nexplainable Detection and personalized Modification, providing fine-grained\ndetection results, detailed explanations, and personalized modification\nsuggestions. We also implemented the tool and recruited 35 Weibo users for\nevaluation. The results suggest DeMod's multiple strengths like the richness of\nfunctionality, the accuracy of censorship, and ease of use. Based on the\nfindings, we further propose several insights into the design of content\ncensorship systems.",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.01844v1",
    "published_date": "2024-11-04 06:38:43 UTC",
    "updated_date": "2024-11-04 06:38:43 UTC"
  },
  {
    "arxiv_id": "2411.03351v1",
    "title": "Tabular Data Synthesis with Differential Privacy: A Survey",
    "authors": [
      "Mengmeng Yang",
      "Chi-Hung Chi",
      "Kwok-Yan Lam",
      "Jie Feng",
      "Taolin Guo",
      "Wei Ni"
    ],
    "abstract": "Data sharing is a prerequisite for collaborative innovation, enabling\norganizations to leverage diverse datasets for deeper insights. In real-world\napplications like FinTech and Smart Manufacturing, transactional data, often in\ntabular form, are generated and analyzed for insight generation. However, such\ndatasets typically contain sensitive personal/business information, raising\nprivacy concerns and regulatory risks. Data synthesis tackles this by\ngenerating artificial datasets that preserve the statistical characteristics of\nreal data, removing direct links to individuals. However, attackers can still\ninfer sensitive information using background knowledge. Differential privacy\noffers a solution by providing provable and quantifiable privacy protection.\nConsequently, differentially private data synthesis has emerged as a promising\napproach to privacy-aware data sharing. This paper provides a comprehensive\noverview of existing differentially private tabular data synthesis methods,\nhighlighting the unique challenges of each generation model for generating\ntabular data under differential privacy constraints. We classify the methods\ninto statistical and deep learning-based approaches based on their generation\nmodels, discussing them in both centralized and distributed environments. We\nevaluate and compare those methods within each category, highlighting their\nstrengths and weaknesses in terms of utility, privacy, and computational\ncomplexity. Additionally, we present and discuss various evaluation methods for\nassessing the quality of the synthesized data, identify research gaps in the\nfield and directions for future research.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03351v1",
    "published_date": "2024-11-04 06:32:48 UTC",
    "updated_date": "2024-11-04 06:32:48 UTC"
  },
  {
    "arxiv_id": "2411.02460v1",
    "title": "Code-Switching Curriculum Learning for Multilingual Transfer in LLMs",
    "authors": [
      "Haneul Yoo",
      "Cheonbok Park",
      "Sangdoo Yun",
      "Alice Oh",
      "Hwaran Lee"
    ],
    "abstract": "Large language models (LLMs) now exhibit near human-level performance in\nvarious tasks, but their performance drops drastically after a handful of\nhigh-resource languages due to the imbalance in pre-training data. Inspired by\nthe human process of second language acquisition, particularly code-switching\n(the practice of language alternation in a conversation), we propose\ncode-switching curriculum learning (CSCL) to enhance cross-lingual transfer for\nLLMs. CSCL mimics the stages of human language learning by progressively\ntraining models with a curriculum consisting of 1) token-level code-switching,\n2) sentence-level code-switching, and 3) monolingual corpora. Using Qwen 2 as\nour underlying model, we demonstrate the efficacy of the CSCL in improving\nlanguage transfer to Korean, achieving significant performance gains compared\nto monolingual continual pre-training methods. Ablation studies reveal that\nboth token- and sentence-level code-switching significantly enhance\ncross-lingual transfer and that curriculum learning amplifies these effects. We\nalso extend our findings into various languages, including Japanese\n(high-resource) and Indonesian (low-resource), and using two additional models\n(Gemma 2 and Phi 3.5). We further show that CSCL mitigates spurious\ncorrelations between language resources and safety alignment, presenting a\nrobust, efficient framework for more equitable language transfer in LLMs. We\nobserve that CSCL is effective for low-resource settings where high-quality,\nmonolingual corpora for language transfer are hardly available.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02460v1",
    "published_date": "2024-11-04 06:31:26 UTC",
    "updated_date": "2024-11-04 06:31:26 UTC"
  },
  {
    "arxiv_id": "2411.11875v1",
    "title": "Exploring Optimal Transport-Based Multi-Grained Alignments for Text-Molecule Retrieval",
    "authors": [
      "Zijun Min",
      "Bingshuai Liu",
      "Liang Zhang",
      "Jia Song",
      "Jinsong Su",
      "Song He",
      "Xiaochen Bo"
    ],
    "abstract": "The field of bioinformatics has seen significant progress, making the\ncross-modal text-molecule retrieval task increasingly vital. This task focuses\non accurately retrieving molecule structures based on textual descriptions, by\neffectively aligning textual descriptions and molecules to assist researchers\nin identifying suitable molecular candidates. However, many existing approaches\noverlook the details inherent in molecule sub-structures. In this work, we\nintroduce the Optimal TRansport-based Multi-grained Alignments model (ORMA), a\nnovel approach that facilitates multi-grained alignments between textual\ndescriptions and molecules. Our model features a text encoder and a molecule\nencoder. The text encoder processes textual descriptions to generate both\ntoken-level and sentence-level representations, while molecules are modeled as\nhierarchical heterogeneous graphs, encompassing atom, motif, and molecule nodes\nto extract representations at these three levels. A key innovation in ORMA is\nthe application of Optimal Transport (OT) to align tokens with motifs, creating\nmulti-token representations that integrate multiple token alignments with their\ncorresponding motifs. Additionally, we employ contrastive learning to refine\ncross-modal alignments at three distinct scales: token-atom, multitoken-motif,\nand sentence-molecule, ensuring that the similarities between correctly matched\ntext-molecule pairs are maximized while those of unmatched pairs are minimized.\nTo our knowledge, this is the first attempt to explore alignments at both the\nmotif and multi-token levels. Experimental results on the ChEBI-20 and PCdes\ndatasets demonstrate that ORMA significantly outperforms existing\nstate-of-the-art (SOTA) models.",
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "q-bio.BM"
    ],
    "primary_category": "cs.IR",
    "comment": "BIBM 2024 Regular Paper",
    "pdf_url": "http://arxiv.org/pdf/2411.11875v1",
    "published_date": "2024-11-04 06:30:52 UTC",
    "updated_date": "2024-11-04 06:30:52 UTC"
  },
  {
    "arxiv_id": "2411.01819v3",
    "title": "Free-Mask: A Novel Paradigm of Integration Between the Segmentation Diffusion Model and Image Editing",
    "authors": [
      "Bo Gao",
      "Jianhui Wang",
      "Xinyuan Song",
      "Yangfan He",
      "Fangxu Xing",
      "Tianyu Shi"
    ],
    "abstract": "Current semantic segmentation models typically require a substantial amount\nof manually annotated data, a process that is both time-consuming and\nresource-intensive. Alternatively, leveraging advanced text-to-image models\nsuch as Midjourney and Stable Diffusion has emerged as an efficient strategy,\nenabling the automatic generation of synthetic data in place of manual\nannotations. However, previous methods have been limited to generating\nsingle-instance images, as the generation of multiple instances with Stable\nDiffusion has proven unstable. To address this limitation and expand the scope\nand diversity of synthetic datasets, we propose a framework \\textbf{Free-Mask}\nthat combines a Diffusion Model for segmentation with advanced image editing\ncapabilities, allowing for the integration of multiple objects into images via\ntext-to-image models. Our method facilitates the creation of highly realistic\ndatasets that closely emulate open-world environments while generating accurate\nsegmentation masks. It reduces the labor associated with manual annotation and\nalso ensures precise mask generation. Experimental results demonstrate that\nsynthetic data generated by \\textbf{Free-Mask} enables segmentation models to\noutperform those trained on real data, especially in zero-shot settings.\nNotably, \\textbf{Free-Mask} achieves new state-of-the-art results on previously\nunseen classes in the VOC 2012 benchmark.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "19 pages,11 figures,5 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.01819v3",
    "published_date": "2024-11-04 05:39:01 UTC",
    "updated_date": "2025-04-27 03:12:39 UTC"
  },
  {
    "arxiv_id": "2411.01813v1",
    "title": "So You Think You Can Scale Up Autonomous Robot Data Collection?",
    "authors": [
      "Suvir Mirchandani",
      "Suneel Belkhale",
      "Joey Hejna",
      "Evelyn Choi",
      "Md Sazzad Islam",
      "Dorsa Sadigh"
    ],
    "abstract": "A long-standing goal in robot learning is to develop methods for robots to\nacquire new skills autonomously. While reinforcement learning (RL) comes with\nthe promise of enabling autonomous data collection, it remains challenging to\nscale in the real-world partly due to the significant effort required for\nenvironment design and instrumentation, including the need for designing reset\nfunctions or accurate success detectors. On the other hand, imitation learning\n(IL) methods require little to no environment design effort, but instead\nrequire significant human supervision in the form of collected demonstrations.\nTo address these shortcomings, recent works in autonomous IL start with an\ninitial seed dataset of human demonstrations that an autonomous policy can\nbootstrap from. While autonomous IL approaches come with the promise of\naddressing the challenges of autonomous RL as well as pure IL strategies, in\nthis work, we posit that such techniques do not deliver on this promise and are\nstill unable to scale up autonomous data collection in the real world. Through\na series of real-world experiments, we demonstrate that these approaches, when\nscaled up to realistic settings, face much of the same scaling challenges as\nprior attempts in RL in terms of environment design. Further, we perform a\nrigorous study of autonomous IL methods across different data scales and 7\nsimulation and real-world tasks, and demonstrate that while autonomous data\ncollection can modestly improve performance, simply collecting more human data\noften provides significantly more improvement. Our work suggests a negative\nresult: that scaling up autonomous data collection for learning robot policies\nfor real-world tasks is more challenging and impractical than what is suggested\nin prior work. We hope these insights about the core challenges of scaling up\ndata collection help inform future efforts in autonomous learning.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "21 pages, 25 figures. Conference on Robot Learning (CoRL) 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.01813v1",
    "published_date": "2024-11-04 05:31:35 UTC",
    "updated_date": "2024-11-04 05:31:35 UTC"
  },
  {
    "arxiv_id": "2411.01807v1",
    "title": "Can Language Models Enable In-Context Database?",
    "authors": [
      "Yu Pan",
      "Hongfeng Yu",
      "Tianjiao Zhao",
      "Jianxin Sun"
    ],
    "abstract": "Large language models (LLMs) are emerging as few-shot learners capable of\nhandling a variety of tasks, including comprehension, planning, reasoning,\nquestion answering, arithmetic calculations, and more. At the core of these\ncapabilities is LLMs' proficiency in representing and understanding structural\nor semi-structural data, such as tables and graphs. Numerous studies have\ndemonstrated that reasoning on tabular data or graphs is not only feasible for\nLLMs but also gives a promising research direction which treats these data as\nin-context data. The lightweight and human readable characteristics of\nin-context database can potentially make it an alternative for the traditional\ndatabase in typical RAG (Retrieval Augmented Generation) settings. However,\nalmost all current work focuses on static in-context data, which does not allow\ndynamic update. In this paper, to enable dynamic database update, delta\nencoding of database is proposed. We explore how data stored in traditional\nRDBMS can be encoded as in-context text and evaluate LLMs' proficiency for CRUD\n(Create, Read, Update and Delete) operations on in-context databases. A\nbenchmark named InConDB is presented and extensive experiments are conducted to\nshow the performance of different language models in enabling in-context\ndatabase by varying the database encoding method, prompting method, operation\ntype and input data distribution, revealing both the proficiency and\nlimitations.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.01807v1",
    "published_date": "2024-11-04 05:25:39 UTC",
    "updated_date": "2024-11-04 05:25:39 UTC"
  },
  {
    "arxiv_id": "2411.03350v2",
    "title": "A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness",
    "authors": [
      "Fali Wang",
      "Zhiwei Zhang",
      "Xianren Zhang",
      "Zongyu Wu",
      "Tzuhao Mo",
      "Qiuhao Lu",
      "Wanjing Wang",
      "Rui Li",
      "Junjie Xu",
      "Xianfeng Tang",
      "Qi He",
      "Yao Ma",
      "Ming Huang",
      "Suhang Wang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated emergent abilities in text\ngeneration, question answering, and reasoning, facilitating various tasks and\ndomains. Despite their proficiency in various tasks, LLMs like PaLM 540B and\nLlama-3.1 405B face limitations due to large parameter sizes and computational\ndemands, often requiring cloud API use which raises privacy concerns, limits\nreal-time applications on edge devices, and increases fine-tuning costs.\nAdditionally, LLMs often underperform in specialized domains such as healthcare\nand law due to insufficient domain-specific knowledge, necessitating\nspecialized models. Therefore, Small Language Models (SLMs) are increasingly\nfavored for their low inference latency, cost-effectiveness, efficient\ndevelopment, and easy customization and adaptability. These models are\nparticularly well-suited for resource-limited environments and domain knowledge\nacquisition, addressing LLMs' challenges and proving ideal for applications\nthat require localized data handling for privacy, minimal inference latency for\nefficiency, and domain knowledge acquisition through lightweight fine-tuning.\nThe rising demand for SLMs has spurred extensive research and development.\nHowever, a comprehensive survey investigating issues related to the definition,\nacquisition, application, enhancement, and reliability of SLM remains lacking,\nprompting us to conduct a detailed survey on these topics. The definition of\nSLMs varies widely, thus to standardize, we propose defining SLMs by their\ncapability to perform specialized tasks and suitability for\nresource-constrained settings, setting boundaries based on the minimal size for\nemergent abilities and the maximum size sustainable under resource constraints.\nFor other aspects, we provide a taxonomy of relevant models/methods and develop\ngeneral frameworks for each category to enhance and utilize SLMs effectively.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "68T50 (Primary) 68T07 (Secondary)",
      "I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "78 pages, 32 figures, 14 tables",
    "pdf_url": "http://arxiv.org/pdf/2411.03350v2",
    "published_date": "2024-11-04 04:43:01 UTC",
    "updated_date": "2024-12-28 09:18:36 UTC"
  },
  {
    "arxiv_id": "2411.01796v2",
    "title": "Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge",
    "authors": [
      "Weihua Du",
      "Qiushi Lyu",
      "Jiaming Shan",
      "Zhenting Qi",
      "Hongxin Zhang",
      "Sunli Chen",
      "Andi Peng",
      "Tianmin Shu",
      "Kwonjoon Lee",
      "Behzad Dariush",
      "Chuang Gan"
    ],
    "abstract": "We introduce Constrained Human-AI Cooperation (CHAIC), an inclusive embodied\nsocial intelligence challenge designed to test social perception and\ncooperation in embodied agents. In CHAIC, the goal is for an embodied agent\nequipped with egocentric observations to assist a human who may be operating\nunder physical constraints -- e.g., unable to reach high places or confined to\na wheelchair -- in performing common household or outdoor tasks as efficiently\nas possible. To achieve this, a successful helper must: (1) infer the human's\nintents and constraints by following the human and observing their behaviors\n(social perception), and (2) make a cooperative plan tailored to the human\npartner to solve the task as quickly as possible, working together as a team\n(cooperative planning). To benchmark this challenge, we create four new agents\nwith real physical constraints and eight long-horizon tasks featuring both\nindoor and outdoor scenes with various constraints, emergency events, and\npotential risks. We benchmark planning- and learning-based baselines on the\nchallenge and introduce a new method that leverages large language models and\nbehavior modeling. Empirical evaluations demonstrate the effectiveness of our\nbenchmark in enabling systematic assessment of key aspects of machine social\nintelligence. Our benchmark and code are publicly available at\nhttps://github.com/UMass-Foundation-Model/CHAIC.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "NeurIPS 2024 Dataset and Benchmark Track. The first two authors\n  contributed equally. Project Website at https://vis-www.cs.umass.edu/CHAIC/",
    "pdf_url": "http://arxiv.org/pdf/2411.01796v2",
    "published_date": "2024-11-04 04:41:12 UTC",
    "updated_date": "2024-11-05 03:28:38 UTC"
  },
  {
    "arxiv_id": "2411.01790v1",
    "title": "Thinking Forward and Backward: Effective Backward Planning with Large Language Models",
    "authors": [
      "Allen Z. Ren",
      "Brian Ichter",
      "Anirudha Majumdar"
    ],
    "abstract": "Large language models (LLMs) have exhibited remarkable reasoning and planning\ncapabilities. Most prior work in this area has used LLMs to reason through\nsteps from an initial to a goal state or criterion, thereby effectively\nreasoning in a forward direction. Nonetheless, many planning problems exhibit\nan inherent asymmetry such that planning backward from the goal is\nsignificantly easier -- for example, if there are bottlenecks close to the\ngoal. We take inspiration from this observation and demonstrate that this bias\nholds for LLM planning as well: planning performance in one direction\ncorrelates with the planning complexity of the problem in that direction.\nHowever, our experiments also reveal systematic biases which lead to poor\nplanning in the backward direction. With this knowledge, we propose a backward\nplanning algorithm for LLMs that first flips the problem and then plans forward\nin the flipped problem. This helps avoid the backward bias, generate more\ndiverse candidate plans, and exploit asymmetries between the forward and\nbackward directions in planning problems -- we find that combining planning in\nboth directions with self-verification improves the overall planning success\nrates by 4-24% in three planning domains.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "Under review",
    "pdf_url": "http://arxiv.org/pdf/2411.01790v1",
    "published_date": "2024-11-04 04:26:03 UTC",
    "updated_date": "2024-11-04 04:26:03 UTC"
  },
  {
    "arxiv_id": "2411.01785v1",
    "title": "Transferable Sequential Recommendation via Vector Quantized Meta Learning",
    "authors": [
      "Zhenrui Yue",
      "Huimin Zeng",
      "Yang Zhang",
      "Julian McAuley",
      "Dong Wang"
    ],
    "abstract": "While sequential recommendation achieves significant progress on capturing\nuser-item transition patterns, transferring such large-scale recommender\nsystems remains challenging due to the disjoint user and item groups across\ndomains. In this paper, we propose a vector quantized meta learning for\ntransferable sequential recommenders (MetaRec). Without requiring additional\nmodalities or shared information across domains, our approach leverages\nuser-item interactions from multiple source domains to improve the target\ndomain performance. To solve the input heterogeneity issue, we adopt vector\nquantization that maps item embeddings from heterogeneous input spaces to a\nshared feature space. Moreover, our meta transfer paradigm exploits limited\ntarget data to guide the transfer of source domain knowledge to the target\ndomain (i.e., learn to transfer). In addition, MetaRec adaptively transfers\nfrom multiple source tasks by rescaling meta gradients based on the\nsource-target domain similarity, enabling selective learning to improve\nrecommendation performance. To validate the effectiveness of our approach, we\nperform extensive experiments on benchmark datasets, where MetaRec consistently\noutperforms baseline methods by a considerable margin.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "Accepted to BigData 2024",
    "pdf_url": "http://arxiv.org/pdf/2411.01785v1",
    "published_date": "2024-11-04 04:16:11 UTC",
    "updated_date": "2024-11-04 04:16:11 UTC"
  },
  {
    "arxiv_id": "2411.01783v3",
    "title": "Context Parallelism for Scalable Million-Token Inference",
    "authors": [
      "Amy Yang",
      "Jingyi Yang",
      "Aya Ibrahim",
      "Xinfeng Xie",
      "Bangsheng Tang",
      "Grigory Sizov",
      "Jeremy Reizenstein",
      "Jongsoo Park",
      "Jianyu Huang"
    ],
    "abstract": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.01783v3",
    "published_date": "2024-11-04 04:15:36 UTC",
    "updated_date": "2025-04-21 03:40:10 UTC"
  },
  {
    "arxiv_id": "2411.01775v1",
    "title": "Eurekaverse: Environment Curriculum Generation via Large Language Models",
    "authors": [
      "William Liang",
      "Sam Wang",
      "Hung-Ju Wang",
      "Osbert Bastani",
      "Dinesh Jayaraman",
      "Yecheng Jason Ma"
    ],
    "abstract": "Recent work has demonstrated that a promising strategy for teaching robots a\nwide range of complex skills is by training them on a curriculum of\nprogressively more challenging environments. However, developing an effective\ncurriculum of environment distributions currently requires significant\nexpertise, which must be repeated for every new domain. Our key insight is that\nenvironments are often naturally represented as code. Thus, we probe whether\neffective environment curriculum design can be achieved and automated via code\ngeneration by large language models (LLM). In this paper, we introduce\nEurekaverse, an unsupervised environment design algorithm that uses LLMs to\nsample progressively more challenging, diverse, and learnable environments for\nskill training. We validate Eurekaverse's effectiveness in the domain of\nquadrupedal parkour learning, in which a quadruped robot must traverse through\na variety of obstacle courses. The automatic curriculum designed by Eurekaverse\nenables gradual learning of complex parkour skills in simulation and can\nsuccessfully transfer to the real-world, outperforming manual training courses\ndesigned by humans.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Conference on Robot Learning (CoRL), 2024. Project website and code:\n  https://eureka-research.github.io/eurekaverse",
    "pdf_url": "http://arxiv.org/pdf/2411.01775v1",
    "published_date": "2024-11-04 03:54:00 UTC",
    "updated_date": "2024-11-04 03:54:00 UTC"
  },
  {
    "arxiv_id": "2411.01757v4",
    "title": "Mitigating Spurious Correlations via Disagreement Probability",
    "authors": [
      "Hyeonggeun Han",
      "Sehwan Kim",
      "Hyungjun Joo",
      "Sangwoo Hong",
      "Jungwoo Lee"
    ],
    "abstract": "Models trained with empirical risk minimization (ERM) are prone to be biased\ntowards spurious correlations between target labels and bias attributes, which\nleads to poor performance on data groups lacking spurious correlations. It is\nparticularly challenging to address this problem when access to bias labels is\nnot permitted. To mitigate the effect of spurious correlations without bias\nlabels, we first introduce a novel training objective designed to robustly\nenhance model performance across all data samples, irrespective of the presence\nof spurious correlations. From this objective, we then derive a debiasing\nmethod, Disagreement Probability based Resampling for debiasing (DPR), which\ndoes not require bias labels. DPR leverages the disagreement between the target\nlabel and the prediction of a biased model to identify bias-conflicting\nsamples-those without spurious correlations-and upsamples them according to the\ndisagreement probability. Empirical evaluations on multiple benchmarks\ndemonstrate that DPR achieves state-of-the-art performance over existing\nbaselines that do not use bias labels. Furthermore, we provide a theoretical\nanalysis that details how DPR reduces dependency on spurious correlations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.01757v4",
    "published_date": "2024-11-04 02:44:04 UTC",
    "updated_date": "2024-12-20 06:52:41 UTC"
  },
  {
    "arxiv_id": "2411.01751v1",
    "title": "RAGViz: Diagnose and Visualize Retrieval-Augmented Generation",
    "authors": [
      "Tevin Wang",
      "Jingyuan He",
      "Chenyan Xiong"
    ],
    "abstract": "Retrieval-augmented generation (RAG) combines knowledge from domain-specific\nsources into large language models to ground answer generation. Current RAG\nsystems lack customizable visibility on the context documents and the model's\nattentiveness towards such documents. We propose RAGViz, a RAG diagnosis tool\nthat visualizes the attentiveness of the generated tokens in retrieved\ndocuments. With a built-in user interface, retrieval index, and Large Language\nModel (LLM) backbone, RAGViz provides two main functionalities: (1) token and\ndocument-level attention visualization, and (2) generation comparison upon\ncontext document addition and removal. As an open-source toolkit, RAGViz can be\neasily hosted with a custom embedding model and HuggingFace-supported LLM\nbackbone. Using a hybrid ANN (Approximate Nearest Neighbor) index,\nmemory-efficient LLM inference tool, and custom context snippet method, RAGViz\noperates efficiently with a median query time of about 5 seconds on a moderate\nGPU node. Our code is available at https://github.com/cxcscmu/RAGViz. A demo\nvideo of RAGViz can be found at https://youtu.be/cTAbuTu6ur4.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.01751v1",
    "published_date": "2024-11-04 02:30:05 UTC",
    "updated_date": "2024-11-04 02:30:05 UTC"
  },
  {
    "arxiv_id": "2411.02457v1",
    "title": "A Multi-Task Role-Playing Agent Capable of Imitating Character Linguistic Styles",
    "authors": [
      "Siyuan Chen",
      "Qingyi Si",
      "Chenxu Yang",
      "Yunzhi Liang",
      "Zheng Lin",
      "Huan Liu",
      "Weiping Wang"
    ],
    "abstract": "The advent of large language models (LLMs) has significantly propelled the\nadvancement of Role-Playing Agents (RPAs). However, current Role-Playing Agents\npredominantly focus on mimicking a character's fundamental attributes while\nneglecting the replication of linguistic style, and they are incapable of\neffectively replicating characters when performing tasks beyond multi-turn\ndialogues, which results in generated responses that lack authenticity. The\nreason current RPAs lack this capability is due to the nature of existing\ncharacter datasets, which lack collections of character quotations and are\nlimited to multi-turn dialogue tasks, constraining the RPA's performance across\nother task domains and failing to mimic a character's linguistic style. To\naddress this gap, we developed a multi-task role-playing dataset named MRstyle,\nwhich encompasses a substantial number of real individuals along with their\nquotations and covers seven different tasks. On this basis, we develop\nStyleRPA, a Multi-Task Role-Playing Agent (MRPA) that significantly outperforms\nrecent open-source LLMs and RPAs baselines on 7 tasks including Dialogue,\nDictionary, Composition, Story Generation, Product Description, Music\nCommentary, and Open Question Answering. The code and data will be released.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.02457v1",
    "published_date": "2024-11-04 02:26:27 UTC",
    "updated_date": "2024-11-04 02:26:27 UTC"
  },
  {
    "arxiv_id": "2411.01738v1",
    "title": "xDiT: an Inference Engine for Diffusion Transformers (DiTs) with Massive Parallelism",
    "authors": [
      "Jiarui Fang",
      "Jinzhe Pan",
      "Xibo Sun",
      "Aoyu Li",
      "Jiannan Wang"
    ],
    "abstract": "Diffusion models are pivotal for generating high-quality images and videos.\nInspired by the success of OpenAI's Sora, the backbone of diffusion models is\nevolving from U-Net to Transformer, known as Diffusion Transformers (DiTs).\nHowever, generating high-quality content necessitates longer sequence lengths,\nexponentially increasing the computation required for the attention mechanism,\nand escalating DiTs inference latency. Parallel inference is essential for\nreal-time DiTs deployments, but relying on a single parallel method is\nimpractical due to poor scalability at large scales. This paper introduces\nxDiT, a comprehensive parallel inference engine for DiTs. After thoroughly\ninvestigating existing DiTs parallel approaches, xDiT chooses Sequence Parallel\n(SP) and PipeFusion, a novel Patch-level Pipeline Parallel method, as\nintra-image parallel strategies, alongside CFG parallel for inter-image\nparallelism. xDiT can flexibly combine these parallel approaches in a hybrid\nmanner, offering a robust and scalable solution. Experimental results on two\n8xL40 GPUs (PCIe) nodes interconnected by Ethernet and an 8xA100 (NVLink) node\nshowcase xDiT's exceptional scalability across five state-of-the-art DiTs.\nNotably, we are the first to demonstrate DiTs scalability on Ethernet-connected\nGPU clusters. xDiT is available at https://github.com/xdit-project/xDiT.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.01738v1",
    "published_date": "2024-11-04 01:40:38 UTC",
    "updated_date": "2024-11-04 01:40:38 UTC"
  },
  {
    "arxiv_id": "2411.03349v1",
    "title": "RuAG: Learned-rule-augmented Generation for Large Language Models",
    "authors": [
      "Yudi Zhang",
      "Pei Xiao",
      "Lu Wang",
      "Chaoyun Zhang",
      "Meng Fang",
      "Yali Du",
      "Yevgeniy Puzyrev",
      "Randolph Yao",
      "Si Qin",
      "Qingwei Lin",
      "Mykola Pechenizkiy",
      "Dongmei Zhang",
      "Saravan Rajmohan",
      "Qi Zhang"
    ],
    "abstract": "In-context learning (ICL) and Retrieval-Augmented Generation (RAG) have\ngained attention for their ability to enhance LLMs' reasoning by incorporating\nexternal knowledge but suffer from limited contextual window size, leading to\ninsufficient information injection. To this end, we propose a novel framework,\nRuAG, to automatically distill large volumes of offline data into interpretable\nfirst-order logic rules, which are injected into LLMs to boost their reasoning\ncapabilities. Our method begins by formulating the search process relying on\nLLMs' commonsense, where LLMs automatically define head and body predicates.\nThen, RuAG applies Monte Carlo Tree Search (MCTS) to address the combinational\nsearching space and efficiently discover logic rules from data. The resulting\nlogic rules are translated into natural language, allowing targeted knowledge\ninjection and seamless integration into LLM prompts for LLM's downstream task\nreasoning. We evaluate our framework on public and private industrial tasks,\nincluding natural language processing, time-series, decision-making, and\nindustrial tasks, demonstrating its effectiveness in enhancing LLM's capability\nover diverse tasks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2411.03349v1",
    "published_date": "2024-11-04 00:01:34 UTC",
    "updated_date": "2024-11-04 00:01:34 UTC"
  }
]
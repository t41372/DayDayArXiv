{
  "date": "2025-11-14",
  "category": "cs.AI",
  "summary": "æ¬¢è¿æ¥åˆ° UTC æ—¶é—´ 2025-11-14 çš„ arXiv ä¸­æ–‡ TLDR å¿«æŠ¥ï¼\n\n**ä»Šæ—¥æ€»ç»“**\nä»Šå¤© arXiv è®ºæ–‡çˆ†å‘å¼å¢é•¿ï¼ˆ167ç¯‡ï¼‰ï¼Œ**å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMulti-Agent Systemsï¼‰**å’Œ**æ¨ç†å¢å¼ºï¼ˆReasoningï¼‰**æ˜¯ç»å¯¹çš„ä¸»è§’ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¼˜åŒ–æ™ºèƒ½ä½“åä½œï¼Œä»¥åŠè®©æ¨¡å‹å…·å¤‡â€œæ…¢æ€è€ƒâ€èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œ**è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰çš„å¹»è§‰æ£€æµ‹ä¸è®°å¿†æœºåˆ¶**ã€**AI å®‰å…¨ï¼ˆç‰¹åˆ«æ˜¯ Graph Unlearning å’Œ TTS æ”»å‡»ï¼‰**ä¹Ÿæ˜¯ä»Šå¤©çš„é‡å¤´æˆã€‚\n\n---\n\n### ğŸš€ æ·±åº¦èšç„¦ï¼šæ™ºèƒ½ä½“åä½œä¸æ¨ç†è¿›åŒ– (Agentic AI & Reasoning)\n\n**MarsRLï¼šé€šè¿‡å¤šæ™ºèƒ½ä½“ç®¡çº¿å¹¶è¡Œçš„å¼ºåŒ–å­¦ä¹ æå‡æ¨ç†**\n**MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism**\n> è¿™ç¯‡æ–‡ç« éå¸¸ç¡¬æ ¸ã€‚ç°æœ‰çš„å¤šæ™ºèƒ½ä½“æ¨ç†ï¼ˆå¦‚ Solver-Verifier æ¶æ„ï¼‰é€šå¸¸éš¾ä»¥åœ¨å¼€æºæ¨¡å‹ä¸Šæ³›åŒ–ã€‚ä½œè€…æå‡ºäº† MarsRLï¼Œè¿™æ˜¯ä¸€ç§æ”¯æŒ**ç®¡çº¿å¹¶è¡Œï¼ˆPipeline Parallelismï¼‰**çš„å¼ºåŒ–å­¦ä¹ æ¡†æ¶ã€‚å®ƒå¼•å…¥äº†æ™ºèƒ½ä½“ç‰¹å®šçš„å¥–åŠ±æœºåˆ¶æ¥å‡å°‘å™ªå£°ï¼Œè”åˆä¼˜åŒ–æ•´ä¸ªæ™ºèƒ½ä½“å›¢é˜Ÿã€‚\n> **äº®ç‚¹**ï¼šåœ¨ AIME2025 æ•°å­¦ç«èµ›é›†ä¸Šï¼ŒæŠŠ Qwen3-30B çš„å‡†ç¡®ç‡ä» 86.5% æ‹‰å‡åˆ°äº† 93.3%ï¼Œç”šè‡³è¶…è¿‡äº†æ›´å¤§å‚æ•°çš„æ¨¡å‹ã€‚è¿™æ˜¯å¼€æºæ¨¡å‹è¿½èµ¶é—­æºæ¨ç†èƒ½åŠ›çš„é‡è¦ä¸€æ­¥ã€‚\n\n**iMADï¼šæ™ºèƒ½å¤šæ™ºèƒ½ä½“è¾©è®ºï¼Œçœé’±åˆå‡†ç¡®**\n**iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference**\n> å¤šæ™ºèƒ½ä½“è¾©è®ºï¼ˆMulti-Agent Debate, MADï¼‰è™½ç„¶å¥½ï¼Œä½†å¤ªè´¹ Token äº†ã€‚è¿™ç¯‡æ–‡ç« æå‡ºäº† iMADï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯**â€œè¯¥è¾©è®ºæ—¶å†è¾©è®ºâ€**ã€‚å®ƒè®­ç»ƒäº†ä¸€ä¸ªè½»é‡çº§çš„å†³ç­–åˆ†ç±»å™¨ï¼Œæ ¹æ®æ¨¡å‹ç”Ÿæˆçš„â€œçŠ¹è±«çº¿ç´¢â€ï¼ˆhesitation cuesï¼‰æ¥åˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘è¾©è®ºã€‚\n> **æ•ˆæœ**ï¼šToken æ¶ˆè€—å‡å°‘äº† 92%ï¼Œå‡†ç¡®ç‡è¿˜æå‡äº†ï¼Œè§£å†³äº†â€œä¸ºäº†è¾©è®ºè€Œè¾©è®ºâ€çš„ä½æ•ˆé—®é¢˜ã€‚\n\n**STaRï¼šè®©å¤§æ¨¡å‹å­¦ä¼šè¡¨æ ¼æ¨ç†çš„â€œæ…¢æ€è€ƒâ€**\n**STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models**\n> é’ˆå¯¹è¡¨æ ¼æ¨ç†ï¼ˆTable Reasoningï¼‰ï¼Œä½œè€…å—äººç±»è®¤çŸ¥å¯å‘ï¼Œæå‡ºäº† STaR æ¡†æ¶ã€‚æ ¸å¿ƒæ˜¯å¼•å…¥**â€œæ…¢æ€è€ƒâ€ï¼ˆSlow-Thinkingï¼‰**æœºåˆ¶ï¼Œæ˜¾å¼åœ°å»ºæ¨¡é€æ­¥æ€è€ƒè¿‡ç¨‹å’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥ã€‚\n> **æ–¹æ³•**ï¼šè®­ç»ƒæ—¶ä½¿ç”¨ä¸¤é˜¶æ®µçš„éš¾åº¦æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹ ï¼›æ¨ç†æ—¶ç»“åˆ Token çº§ç½®ä¿¡åº¦å’Œç­”æ¡ˆä¸€è‡´æ€§æ¥é€‰æ‹©è·¯å¾„ã€‚è¿™è®© LLM åœ¨å¤„ç†ç»“æ„åŒ–æ•°æ®æ—¶æ›´ç¨³å¥ã€‚\n\n**PasoDobleï¼šé€šè¿‡åŒé‡åšå¼ˆæå‡ LLM æ¨ç†**\n**Better LLM Reasoning via Dual-Play**\n> è¿™æ˜¯ä¸€ä¸ªæ— éœ€ç›‘ç£çš„å¯¹æŠ—è®­ç»ƒæ¡†æ¶ã€‚ä½œè€…è®©ä¸¤ä¸ªæ¨¡å‹äº’æï¼šä¸€ä¸ª Proposerï¼ˆå‡ºé¢˜è€…ï¼‰å’Œä¸€ä¸ª Solverï¼ˆè§£é¢˜è€…ï¼‰ã€‚Proposer å› æå‡ºèƒ½éš¾å€’ Solver çš„æœ‰æ•ˆé—®é¢˜è€Œè·å¾—å¥–åŠ±ï¼ŒSolver åˆ™å› è§£é¢˜æ­£ç¡®è·åˆ©ã€‚è¿™ç§**è‡ªæˆ‘åšå¼ˆï¼ˆSelf-playï¼‰**æœºåˆ¶æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¸”ä¸éœ€è¦å¤–éƒ¨æ ‡æ³¨æ•°æ®ã€‚\n\n---\n\n### ğŸ‘ï¸ è§†è§‰ä¸å¤šæ¨¡æ€ï¼šè®°å¿†ã€å¹»è§‰ä¸å…¨å±€æ„ŸçŸ¥ (Vision & Multimodal)\n\n**VisMemï¼šæ½œè§†è§‰è®°å¿†è§£é” VLM æ½œåŠ›**\n**VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models**\n> VLM ç»å¸¸åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­â€œå¿˜æ‰â€è§†è§‰ä¿¡æ¯ï¼ˆVisual Processing Bottleneckï¼‰ã€‚è¿™ç¯‡è®ºæ–‡æ¨¡ä»¿äººç±»è®°å¿†ï¼Œè®¾è®¡äº†**åŠ¨æ€æ½œè§†è§‰è®°å¿†ï¼ˆLatent Vision Memoryï¼‰**ï¼ŒåŒ…å«ç”¨äºç»†èŠ‚æ„ŸçŸ¥çš„çŸ­æœŸæ¨¡å—å’Œç”¨äºè¯­ä¹‰å·©å›ºçš„é•¿æœŸæ¨¡å—ã€‚\n> **è´¡çŒ®**ï¼šåœ¨ä¸æ”¹å˜æ¨¡å‹ä¸»ä½“çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¤–æŒ‚è®°å¿†æ¨¡å—ï¼Œå¹³å‡æ€§èƒ½æå‡äº† 11.8%ï¼Œè®©æ¨¡å‹åœ¨é•¿ç¨‹æ¨ç†ä¸­ä¿æŒâ€œè§†è§‰åœ¨çº¿â€ã€‚\n\n**TopoPerceptionï¼šå¤§è§†è§‰è¯­è¨€æ¨¡å‹çš„å…¨å±€æ„ŸçŸ¥çŸ­æ¿**\n**TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models**\n> ä¸€ä¸ªå¾ˆæœ‰è¶£çš„å‘ç°ï¼šç°åœ¨çš„ LVLMï¼ˆå¦‚ GPT-4V, Geminiï¼‰å¯èƒ½å¹¶æ²¡æœ‰çœŸæ­£â€œçœ‹æ‡‚â€å›¾ç‰‡çš„å…¨å±€ç»“æ„ï¼Œè€Œæ˜¯ä¾èµ–å±€éƒ¨ç‰¹å¾èµ°æ·å¾„ã€‚ä½œè€…è®¾è®¡äº†ä¸€ä¸ªåŸºäº**æ‹“æ‰‘å­¦ï¼ˆTopologyï¼‰**çš„åŸºå‡†æµ‹è¯•ï¼Œå‘ç°æ‰€æœ‰æ¨¡å‹åœ¨å…¨å±€æ„ŸçŸ¥ä¸Šå‡ ä¹ç­‰åŒäºéšæœºçŒœæµ‹ã€‚æ¨¡å‹è¶Šå¼ºï¼Œæ¨ç†èƒ½åŠ›è¶Šå¥½ï¼Œåè€Œè¿™ç§æ„ŸçŸ¥çŸ­æ¿è¶Šæ˜æ˜¾ï¼ˆå¯èƒ½è¿‡åº¦æ‹Ÿåˆäº†è¯­ä¹‰æ·å¾„ï¼‰ã€‚\n\n**PASï¼šåˆ©ç”¨ Prelim Attention æ£€æµ‹å¯¹è±¡å¹»è§‰**\n**PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models**\n> å¦‚ä½•æ£€æµ‹ VLM æ˜¯å¦åœ¨çç¼–ï¼Ÿä½œè€…å‘ç°ï¼Œå½“æ¨¡å‹äº§ç”Ÿå¹»è§‰æ—¶ï¼Œå®ƒå¾€å¾€å¿½ç•¥å›¾åƒï¼Œè€Œè¿‡åº¦ä¾èµ–ä¹‹å‰ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆprelim tokensï¼‰ã€‚ä½œè€…æå‡ºäº† **Prelim Attention Score (PAS)**ï¼Œä¸€ä¸ªæ— éœ€è®­ç»ƒçš„æŒ‡æ ‡ï¼Œé€šè¿‡è®¡ç®—æ¨¡å‹å¯¹å‰æ–‡çš„æ³¨æ„åŠ›æƒé‡å°±èƒ½å®æ—¶åˆ¤æ–­æ˜¯å¦åœ¨é€šè¿‡â€œè„‘è¡¥â€ç”Ÿæˆå¯¹è±¡ã€‚\n\n---\n\n### ğŸ›¡ï¸ å®‰å…¨ä¸é˜²å¾¡ï¼šé—å¿˜çš„é£é™©ä¸å£°éŸ³çš„å¨èƒ (Security & Safety)\n\n**GraphToxinï¼šå›¾é—å¿˜å­¦ä¹ ä¸­çš„é‡æ„æ”»å‡»**\n**GraphToxin: Reconstructing Full Unlearned Graphs from Graph Unlearning**\n> â€œå›¾é—å¿˜â€ï¼ˆGraph Unlearningï¼‰æ—¨åœ¨è®©æ¨¡å‹â€œå¿˜è®°â€ç‰¹å®šèŠ‚ç‚¹ä»¥ä¿æŠ¤éšç§ã€‚ä½†è¿™ç¯‡è®ºæ–‡æå‡ºäº† GraphToxin æ”»å‡»ï¼Œè¯æ˜å³ä½¿æ‰§è¡Œäº†é—å¿˜æ“ä½œï¼Œæ”»å‡»è€…ä»èƒ½é€šè¿‡**æ›²ç‡åŒ¹é…ï¼ˆCurvature Matchingï¼‰**é‡æ„å‡ºè¢«åˆ é™¤çš„å›¾ç»“æ„å’Œæ•æ„Ÿè¿æ¥ã€‚è¿™æ„å‘³ç€ç›®å‰çš„å›¾éšç§ä¿æŠ¤æŠ€æœ¯å¯èƒ½å½¢åŒè™šè®¾ã€‚\n\n**HARMGENï¼šåˆæˆè¯­éŸ³çš„çœŸå®å¨èƒ**\n**Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio**\n> é’ˆå¯¹å¤§å‹æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ¨¡å‹çš„å®‰å…¨æ€§ç ”ç©¶ã€‚ä½œè€…å¼€å‘äº† HARMGENï¼Œé€šè¿‡**è¯­ä¹‰æ··æ·†ï¼ˆSemantic Obfuscationï¼‰**å’Œ**éŸ³é¢‘æ¨¡æ€åˆ©ç”¨**ï¼ˆå¦‚åˆ©ç”¨æ‹¼éŸ³ã€éŸ³ç´ æ³¨å…¥ï¼‰ï¼ŒæˆåŠŸç»•è¿‡äº†ç°æœ‰ TTS æ¨¡å‹çš„å®‰å…¨å¯¹é½ï¼Œç”Ÿæˆäº†åŒ…å«æœ‰å®³å†…å®¹çš„é€¼çœŸè¯­éŸ³ã€‚ç°æœ‰çš„é˜²å¾¡æœºåˆ¶æ‹¦æˆªç‡å¾ˆä½ã€‚\n\n**Chatsparentï¼šæ£€æµ‹ LLM çš„â€œè®¤çŸ¥ç–²åŠ³â€**\n**Chatsparent: An Interactive System for Detecting and Mitigating Cognitive Fatigue in LLMs**\n> LLM åœ¨é•¿å¯¹è¯ä¸­ä¼šè¡¨ç°å‡ºâ€œè®¤çŸ¥ç–²åŠ³â€ï¼ˆèƒ¡è¨€ä¹±è¯­ã€é‡å¤ï¼‰ã€‚è¿™ç¯‡è®ºæ–‡åšäº†ä¸€ä¸ªäº¤äº’ç³»ç»Ÿï¼Œå®æ—¶ç›‘æ§æ¨¡å‹çš„**ç†µåå¡Œï¼ˆEntropy Collapseï¼‰**å’Œ**æ³¨æ„åŠ›è¡°å‡**ï¼Œå¹¶åœ¨ç–²åŠ³æ—¶è‡ªåŠ¨è§¦å‘å¹²é¢„ï¼ˆå¦‚æ³¨æ„åŠ›é‡ç½®ï¼‰ã€‚éå¸¸æœ‰å®ç”¨ä»·å€¼çš„å·¥ç¨‹å‘ç ”ç©¶ã€‚\n\n---\n\n### âš™ï¸ ç³»ç»Ÿä¸åŸºç¡€è®¾æ–½ (Systems & Infra)\n\n**KVSwapï¼šé¢å‘ç«¯ä¾§é•¿ä¸Šä¸‹æ–‡æ¨ç†çš„ KV Cache å¸è½½**\n**KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference**\n> åœ¨æ‰‹æœºç­‰ç«¯ä¾§è®¾å¤‡è·‘é•¿æ–‡æ¡£æ‘˜è¦å¾ˆç—›è‹¦ï¼Œå› ä¸ºå†…å­˜ä¸å¤Ÿå­˜ KV Cacheã€‚ç°æœ‰çš„å¸è½½æ–¹æ¡ˆå¤šé’ˆå¯¹æœåŠ¡å™¨ï¼ˆGPU<->CPUï¼‰ã€‚KVSwap ä¸“é—¨é’ˆå¯¹ç«¯ä¾§ï¼Œå°† KV Cache å¸è½½åˆ°**ç£ç›˜ï¼ˆDiskï¼‰**ï¼Œå¹¶è®¾è®¡äº†é¢„æµ‹æœºåˆ¶æ¥é¢„åŠ è½½æ•°æ®ï¼Œåˆ©ç”¨äº†ç§»åŠ¨è®¾å¤‡ç»Ÿä¸€å†…å­˜æ¶æ„çš„ç‰¹ç‚¹ï¼Œåœ¨ç´§ç¼ºå†…å­˜ä¸‹å®ç°äº†é«˜ååã€‚\n\n**Binary BPEï¼šäºŒè¿›åˆ¶åˆ†æä¸“ç”¨åˆ†è¯å™¨**\n**Binary BPE: A Family of Cross-Platform Tokenizers for Binary Analysis**\n> é’ˆå¯¹äºŒè¿›åˆ¶ä»£ç åˆ†æçš„ç—›ç‚¹ï¼ˆå­—èŠ‚çº§ Token å¤ªé•¿ï¼Œæ–‡æœ¬ Tokenizer å¤„ç†ä¸äº† hexï¼‰ã€‚ä½œè€…å‘å¸ƒäº†ä¸€å¥—åœ¨å¤§é‡äºŒè¿›åˆ¶æ–‡ä»¶ä¸Šè®­ç»ƒçš„ **Binary BPE Tokenizer**ã€‚\n> **æ•ˆæœ**ï¼šç›¸æ¯”åŸå§‹å­—èŠ‚ï¼Œä¸Šä¸‹æ–‡çª—å£åˆ©ç”¨ç‡æå‡ 2-3 å€ï¼Œèƒ½å‘ç° ELF/PE å¤´å’ŒæŒ‡ä»¤åºåˆ—æ¨¡å¼ã€‚è¿™æ˜¯äºŒè¿›åˆ¶å®‰å…¨/é€†å‘å·¥ç¨‹é¢†åŸŸçš„é‡è¦åŸºå»ºã€‚\n\n---\n\n### ğŸ§ª ç§‘å­¦ä¸å…¶ä»–æœ‰è¶£åº”ç”¨ (Science & Others)\n\n**AIonopediaï¼šç¦»å­æ¶²ä½“å‘ç°çš„å¤šæ¨¡æ€æ™ºèƒ½ä½“**\n**AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery**\n> è¿™æ˜¯ä¸€ä¸ªåŸºäº LLM çš„åŒ–å­¦å‘ç°æ™ºèƒ½ä½“ï¼Œä¸“é—¨ç”¨äº**ç¦»å­æ¶²ä½“ï¼ˆIonic Liquidsï¼‰**çš„ç­›é€‰å’Œè®¾è®¡ã€‚å®ƒä¸ä»…èƒ½é¢„æµ‹æ€§è´¨ï¼Œè¿˜æ¥å…¥äº†å±‚çº§æœç´¢æ¶æ„ï¼Œç”šè‡³åœ¨æ¹¿å®éªŒä¸­éªŒè¯äº†å…¶åœ¨åŸŸå¤–æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ã€‚\n\n**House M.D. æ¡ˆä¾‹ç ”ç©¶ï¼šLLM è¯Šæ–­ç½•è§ç—…**\n**Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D**\n> è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰åˆ›æ„çš„åŸºå‡†æµ‹è¯•ã€‚ä½œè€…ç”¨ç¾å‰§ã€Šè±ªæ–¯åŒ»ç”Ÿã€‹ï¼ˆHouse M.D.ï¼‰é‡Œçš„ç–‘éš¾æ‚ç—‡æ¥æµ‹è¯• LLMã€‚\n> **ç»“æœ**ï¼šè™½ç„¶æ–°æ¨¡å‹æ¯”æ—§æ¨¡å‹å¼º 2.3 å€ï¼Œä½†æœ€é«˜å‡†ç¡®ç‡ä¹Ÿåªæœ‰ 38.64%ï¼ˆGPT-5 mini ç­‰ï¼‰ã€‚è¿™è¯´æ˜åœ¨æåº¦å¤æ‚çš„å™è¿°æ€§ç½•è§ç—…è¯Šæ–­ä¸Šï¼ŒAI è·ç¦»è±ªæ–¯åŒ»ç”Ÿè¿˜æœ‰å¾ˆé•¿çš„è·¯è¦èµ°ã€‚\n\n**ä¿®æ­£æ–‡æœ¬åµŒå…¥çš„å¹³å‡åå·®**\n**Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB**\n> å‘ç°ç›®å‰çš„æ–‡æœ¬åµŒå…¥æ¨¡å‹è¾“å‡ºå¾€å¾€å¸¦æœ‰ä¸€ä¸ªä¸€è‡´çš„åå·® $Î¼$ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªç®€å•çš„**é‡å½’ä¸€åŒ–ï¼ˆRenormalizationï¼‰**æ–¹æ³•ï¼Œå³åœ¨è¾“å‡ºä¸­å‡å»è¿™ä¸ªåå·®çš„æŠ•å½±ã€‚\n> **ç»“æœ**ï¼šæ— éœ€è®­ç»ƒï¼Œå³æ’å³ç”¨ï¼Œåœ¨ MMTEB åŸºå‡†ä¸Šæ£€ç´¢ä»»åŠ¡æ€§èƒ½æå‡æ˜¾è‘—ã€‚ç®€å•åˆæœ‰æ•ˆçš„ math trickã€‚",
  "papers": [
    {
      "arxiv_id": "2511.11951v1",
      "title": "Temporal Micro-Doppler Spectrogram-based ViT Multiclass Target Classification",
      "title_zh": "åŸºäºæ—¶åºå¾®å¤šæ™®å‹’é¢‘è°±å›¾çš„ ViT å¤šç±»åˆ«ç›®æ ‡åˆ†ç±»",
      "authors": [
        "Nghia Thinh Nguyen",
        "Tri Nhu Do"
      ],
      "abstract": "In this paper, we propose a new Temporal MDS-Vision Transformer (T-MDS-ViT) for multiclass target classification using millimeter-wave FMCW radar micro-Doppler spectrograms. Specifically, we design a transformer-based architecture that processes stacked range-velocity-angle (RVA) spatiotemporal tensors via patch embeddings and cross-axis attention mechanisms to explicitly model the sequential nature of MDS data across multiple frames. The T-MDS-ViT exploits mobility-aware constraints in its attention layer correspondences to maintain separability under target overlaps and partial occlusions. Next, we apply an explainable mechanism to examine how the attention layers focus on characteristic high-energy regions of the MDS representations and their effect on class-specific kinematic features. We also demonstrate that our proposed framework is superior to existing CNN-based methods in terms of classification accuracy while achieving better data efficiency and real-time deployability.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸º Temporal MDS-Vision Transformer (T-MDS-ViT) çš„æ–°å‹æ¶æ„ï¼Œæ—¨åœ¨åˆ©ç”¨æ¯«ç±³æ³¢ FMCW radar äº§ç”Ÿçš„ micro-Doppler spectrograms (MDS) å®ç°é«˜ç²¾åº¦çš„å¤šç±»åˆ«ç›®æ ‡åˆ†ç±»ã€‚è¯¥æ¨¡å‹é€šè¿‡ patch embeddings å’Œ cross-axis attention æœºåˆ¶å¤„ç†å †å çš„ range-velocity-angle (RVA) æ—¶ç©ºå¼ é‡ï¼Œèƒ½å¤Ÿæ˜¾å¼åœ°å»ºæ¨¡å¤šå¸§ MDS æ•°æ®ä¸­çš„åºåˆ—æ¼”å˜ç‰¹æ€§ã€‚T-MDS-ViT ç‰¹åˆ«å¼•å…¥äº† mobility-aware çº¦æŸæ¥ä¼˜åŒ–æ³¨æ„åŠ›å±‚ï¼Œä»è€Œåœ¨ç›®æ ‡é‡å æˆ–éƒ¨åˆ†é®æŒ¡ (occlusions) çš„å¤æ‚åœºæ™¯ä¸‹ä¿æŒåˆ†ç±»çš„é²æ£’æ€§ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨å¯è§£é‡Šæ€§æœºåˆ¶ (explainable mechanism) åˆ†æäº†æ³¨æ„åŠ›å±‚å¯¹ MDS é«˜èƒ½åŒºåŸŸçš„èšç„¦æƒ…å†µåŠå…¶å¯¹è¿åŠ¨å­¦ç‰¹å¾çš„å½±å“ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ¡†æ¶åœ¨åˆ†ç±»å‡†ç¡®ç‡ä¸Šæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ CNN-based æ–¹æ³•ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„æ•°æ®æ•ˆç‡ä¸å®æ—¶éƒ¨ç½² (real-time deployability) æ½œåŠ›ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11951v1",
      "published_date": "2025-11-14 23:53:33 UTC",
      "updated_date": "2025-11-14 23:53:33 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:12:17.133409+00:00"
    },
    {
      "arxiv_id": "2511.11947v1",
      "title": "AI-Open-RAN for Non-Terrestrial Networks",
      "title_zh": "é¢å‘éåœ°é¢ç½‘ç»œçš„äººå·¥æ™ºèƒ½å¼€æ”¾å¼æ— çº¿æ¥å…¥ç½‘",
      "authors": [
        "Tri Nhu Do"
      ],
      "abstract": "In this paper, we propose the concept of AIO-RAN-NTN, a unified all-in-one Radio Access Network (RAN) for Non-Terrestrial Networks (NTNs), built on an open architecture that leverages open interfaces and artificial intelligence (AI)-based functionalities. This approach advances interoperability, flexibility, and intelligence in next-generation telecommunications. First, we provide a concise overview of the state-of-the-art architectures for Open-RAN and AI-RAN, highlighting key network functions and infrastructure elements. Next, we introduce our integrated AIO-RAN-NTN blueprint, emphasizing how internal and air interfaces from AIO-RAN and the 3rd Generation Partnership Project (3GPP) can be applied to emerging environments such as NTNs. To examine the impact of mobility on AIO-RAN, we implement a testbed transmission using the OpenAirInterface platform for a standalone (SA) New Radio (NR) 5G system. We then train an AI model on realistic data to forecast key performance indicators (KPIs). Our experiments demonstrate that the AIO-based SA architecture is sensitive to mobility, even at low speeds, but this limitation can be mitigated through AI-driven KPI forecasting.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AIO-RAN-NTNæ¦‚å¿µï¼Œè¿™æ˜¯ä¸€ç§ä¸“ä¸ºéåœ°é¢ç½‘ç»œ(NTNs)è®¾è®¡çš„ç»Ÿä¸€å…¨èƒ½å‹æ— çº¿æ¥å…¥ç½‘(RAN)æ¡†æ¶ï¼Œèåˆäº†å¼€æ”¾æ¥å£ä¸äººå·¥æ™ºèƒ½(AI)åŠŸèƒ½ã€‚è¯¥æ–¹æ¡ˆé€šè¿‡æ•´åˆOpen-RANä¸AI-RANçš„ä¼˜åŠ¿ï¼Œæ—¨åœ¨æå‡ä¸‹ä¸€ä»£é€šä¿¡ç³»ç»Ÿçš„äº’æ“ä½œæ€§ã€çµæ´»æ€§ä¸æ™ºèƒ½åŒ–ã€‚ç ”ç©¶è¯¦ç»†åˆ¶å®šäº†AIO-RAN-NTNçš„è®¾è®¡è“å›¾ï¼Œå¹¶æ¢è®¨äº†å¦‚ä½•å°†3GPPæ ‡å‡†æ¥å£åº”ç”¨äºNTNç¯å¢ƒã€‚ä¸ºäº†éªŒè¯æ€§èƒ½ï¼Œå›¢é˜Ÿåˆ©ç”¨OpenAirInterfaceå¹³å°æ­å»ºäº†5Gç‹¬ç«‹ç»„ç½‘(SA)æ–°æ— çº¿(NR)æµ‹è¯•åºŠï¼Œå¹¶è®­ç»ƒAIæ¨¡å‹è¿›è¡Œå…³é”®æ€§èƒ½æŒ‡æ ‡(KPIs)é¢„æµ‹ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè™½ç„¶AIO-based SAæ¶æ„å¯¹ç§»åŠ¨æ€§è¾ƒä¸ºæ•æ„Ÿï¼Œä½†AIé©±åŠ¨çš„KPIé¢„æµ‹æŠ€æœ¯èƒ½æœ‰æ•ˆç¼“è§£è¿™ä¸€å½±å“ã€‚è¿™é¡¹å·¥ä½œä¸ºæœªæ¥éåœ°é¢ç½‘ç»œä¸­é›†æˆå¼€æ”¾å¼æ¶æ„ä¸äººå·¥æ™ºèƒ½æä¾›äº†é‡è¦çš„å‚è€ƒä¸å®è·µè·¯å¾„ã€‚",
      "categories": [
        "eess.SP",
        "cs.AI"
      ],
      "primary_category": "eess.SP",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11947v1",
      "published_date": "2025-11-14 23:41:52 UTC",
      "updated_date": "2025-11-14 23:41:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:12:43.531755+00:00"
    },
    {
      "arxiv_id": "2511.11945v1",
      "title": "Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes",
      "title_zh": "æ°”è±¡å¢å¼ºï¼šä¸€ç§ç”¨äºæå‡æ°”å€™å˜åŒ–èƒŒæ™¯ä¸‹ä½œç‰©ç”Ÿé•¿é¢„æµ‹æ€§èƒ½çš„æ··åˆåäº‹å®-SMOTEç®—æ³•",
      "authors": [
        "Mohammed Temraz",
        "Mark T Keane"
      ],
      "abstract": "In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of \"climate outlier events\". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ°”å€™å˜åŒ–å¼•å‘çš„æç«¯å¤©æ°”ä½¿å†œä¸šé¢„æµ‹é¢ä¸´ä¸¥å³»æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨å¤„ç†å†å²æ•°æ®ä¸­çš„åˆ†å¸ƒå¤– (out-of-distribution) ç¦»ç¾¤å€¼äº‹ä»¶ (outlier events) æ—¶è¡¨ç°ä¸ä½³ã€‚ä¸ºè§£å†³å†å²æ•°æ®ä¸­ç¼ºä¹å°‘æ•°ç±»æç«¯æ°”å€™æ ·æœ¬çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸º CFA-SMOTE (Counterfactual-Based SMOTE) çš„æ–°å‹æ•°æ®å¢å¼ºæ–¹æ³•ï¼Œå°†æ°”å€™é¢„æµ‹æŒ‘æˆ˜è§†ä¸ºç±»ä¸å¹³è¡¡ (class-imbalance) è¯¾é¢˜ã€‚è¯¥æ–¹æ³•å°†å¯è§£é‡Šäººå·¥æ™ºèƒ½ (Explainable AI) ä¸­çš„åäº‹å®æ–¹æ³• (counterfactual method) ä¸ç»å…¸çš„åˆæˆå°‘æ•°ç±»è¿‡é‡‡æ ·æŠ€æœ¯ (SMOTE) ç›¸ç»“åˆï¼Œé€šè¿‡ç”Ÿæˆä»£è¡¨æç«¯æ°”å€™äº‹ä»¶çš„åˆæˆæ•°æ®ç‚¹æ¥æœ‰æ•ˆæ‰©å……æ•°æ®é›†ã€‚å®éªŒé€šè¿‡æ¨¡æ‹Ÿ2018å¹´æ¬§æ´²å¹²æ—±æœŸé—´çˆ±å°”å…°ç‰§åœºç‰§è‰ç”Ÿé•¿çš„é¢„æµ‹ä»»åŠ¡ï¼Œåœ¨å¤šç§æ¯”ä¾‹ä¸‹éªŒè¯äº†è¯¥ç®—æ³•çš„æ€§èƒ½ã€‚ç ”ç©¶è¯æ˜ CFA-SMOTE èƒ½æ˜¾è‘—æå‡æ¨¡å‹åœ¨æ°”å€™å‰§å˜ç¯å¢ƒä¸‹çš„é¢„æµ‹ç¨³å¥æ€§ï¼Œä¸ºåº”å¯¹åˆ†å¸ƒå¤–æ°”å€™äº‹ä»¶æä¾›äº†æœ‰æ•ˆçš„æ•°æ®é©±åŠ¨è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "31 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.11945v1",
      "published_date": "2025-11-14 23:35:21 UTC",
      "updated_date": "2025-11-14 23:35:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:12:38.327219+00:00"
    },
    {
      "arxiv_id": "2511.11938v1",
      "title": "Improving Neutrino Oscillation Measurements through Event Classification",
      "title_zh": "é€šè¿‡äº‹ä¾‹åˆ†ç±»æå‡ä¸­å¾®å­æŒ¯è¡æµ‹é‡",
      "authors": [
        "Sebastian A. R. Ellis",
        "Daniel C. Hackett",
        "Shirley Weishi Li",
        "Pedro A. N. Machado",
        "Karla Tame-Narvaez"
      ],
      "abstract": "Precise neutrino energy reconstruction is essential for next-generation long-baseline oscillation experiments, yet current methods remain limited by large uncertainties in neutrino-nucleus interaction modeling. Even so, it is well established that different interaction channels produce systematically varying amounts of missing energy and therefore yield different reconstruction performance--information that standard calorimetric approaches do not exploit. We introduce a strategy that incorporates this structure by classifying events according to their underlying interaction type prior to energy reconstruction. Using supervised machine-learning techniques trained on labeled generator events, we leverage intrinsic kinematic differences among quasi-elastic scattering, meson-exchange current, resonance production, and deep-inelastic scattering processes. A cross-generator testing framework demonstrates that this classification approach is robust to microphysics mismodeling and, when applied to a simulated DUNE $Î½_Î¼$ disappearance analysis, yields improved accuracy and sensitivity. These results highlight a practical path toward reducing reconstruction-driven systematics in future oscillation measurements.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºï¼Œç²¾ç¡®çš„ Neutrino energy reconstruction å¯¹ä¸‹ä¸€ä»£é•¿åŸºçº¿æŒ¯è¡å®éªŒè‡³å…³é‡è¦ï¼Œä½†ç›®å‰å—åˆ° Neutrino-nucleus interaction å»ºæ¨¡ä¸ç¡®å®šæ€§çš„æ˜¾è‘—é™åˆ¶ã€‚ä¼ ç»Ÿçš„ Calorimetric approaches å¾€å¾€å¿½ç•¥äº†ä¸åŒç›¸äº’ä½œç”¨é€šé“å›  Missing energy å·®å¼‚è€Œå¯¼è‡´çš„é‡å»ºæ€§èƒ½å˜åŒ–ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºäº†ä¸€ç§æ–°ç­–ç•¥ï¼Œåœ¨èƒ½é‡é‡å»ºä¹‹å‰åˆ©ç”¨ç›‘ç£å¼ Machine-learning æŠ€æœ¯æ ¹æ®ç›¸äº’ä½œç”¨ç±»å‹å¯¹äº‹ä»¶è¿›è¡Œåˆ†ç±»ã€‚è¯¥æ–¹æ³•æœ‰æ•ˆåˆ©ç”¨äº† Quasi-elastic scatteringã€Meson-exchange currentã€Resonance production å’Œ Deep-inelastic scattering è¿‡ç¨‹ä¹‹é—´çš„è¿åŠ¨å­¦å·®å¼‚ã€‚Cross-generator testing framework çš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¿™ç§åˆ†ç±»æ–¹æ³•å¯¹äºå¾®è§‚ç‰©ç†è¯¯å»ºæ¨¡å…·æœ‰å¾ˆå¼ºçš„ç¨³å¥æ€§ã€‚åœ¨æ¨¡æ‹Ÿçš„ DUNE $\\nu_\\mu$ disappearance åˆ†æä¸­ï¼Œè¯¥æ–¹æ¡ˆæ˜¾è‘—æå‡äº†æµ‹é‡çš„å‡†ç¡®æ€§å’Œçµæ•åº¦ã€‚è¿™é¡¹å·¥ä½œä¸ºé™ä½æœªæ¥æŒ¯è¡æµ‹é‡ä¸­ç”±é‡å»ºé©±åŠ¨çš„ Systematics æä¾›äº†åˆ‡å®å¯è¡Œçš„è·¯å¾„ã€‚",
      "categories": [
        "hep-ph",
        "cs.AI",
        "cs.LG",
        "hep-ex"
      ],
      "primary_category": "hep-ph",
      "comment": "11 pages, 7 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.11938v1",
      "published_date": "2025-11-14 23:26:51 UTC",
      "updated_date": "2025-11-14 23:26:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:12:33.132026+00:00"
    },
    {
      "arxiv_id": "2511.11937v1",
      "title": "A Deep Learning Framework for Thyroid Nodule Segmentation and Malignancy Classification from Ultrasound Images",
      "title_zh": "åŸºäºè¶…å£°å›¾åƒçš„ç”²çŠ¶è…ºç»“èŠ‚åˆ†å‰²ä¸è‰¯æ¶æ€§åˆ†ç±»æ·±åº¦å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Omar Abdelrazik",
        "Mohamed Elsayed",
        "Noorul Wahab",
        "Nasir Rajpoot",
        "Adam Shephard"
      ],
      "abstract": "Ultrasound-based risk stratification of thyroid nodules is a critical clinical task, but it suffers from high inter-observer variability. While many deep learning (DL) models function as \"black boxes,\" we propose a fully automated, two-stage framework for interpretable malignancy prediction. Our method achieves interpretability by forcing the model to focus only on clinically relevant regions. First, a TransUNet model automatically segments the thyroid nodule. The resulting mask is then used to create a region of interest around the nodule, and this localised image is fed directly into a ResNet-18 classifier. We evaluated our framework using 5-fold cross-validation on a clinical dataset of 349 images, where it achieved a high F1-score of 0.852 for predicting malignancy. To validate its performance, we compared it against a strong baseline using a Random Forest classifier with hand-crafted morphological features, which achieved an F1-score of 0.829. The superior performance of our DL framework suggests that the implicit visual features learned from the localised nodule are more predictive than explicit shape features alone. This is the first fully automated end-to-end pipeline for both detecting thyroid nodules on ultrasound images and predicting their malignancy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¶…å£°å›¾åƒä¸­ç”²çŠ¶è…ºç»“èŠ‚é£é™©åˆ†å±‚å­˜åœ¨çš„é«˜è§‚å¯Ÿè€…é—´å˜å¼‚æ€§é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¨è‡ªåŠ¨çš„ä¸¤é˜¶æ®µæ·±åº¦å­¦ä¹ (Deep Learning)æ¡†æ¶ã€‚è¯¥æ¡†æ¶é¦–å…ˆåˆ©ç”¨ TransUNet æ¨¡å‹å¯¹ç”²çŠ¶è…ºç»“èŠ‚è¿›è¡Œè‡ªåŠ¨åˆ†å‰²ï¼Œä»¥ç¡®ä¿æ¨¡å‹èšç„¦äºä¸´åºŠç›¸å…³çš„æ„Ÿå…´è¶£åŒºåŸŸã€‚éšåï¼Œåˆ†å‰²ç”Ÿæˆçš„æ©è†œè¢«ç”¨äºæˆªå–ç»“èŠ‚å±€éƒ¨å›¾åƒï¼Œå¹¶å°†å…¶è¾“å…¥ ResNet-18 åˆ†ç±»å™¨è¿›è¡Œå¯è§£é‡Šçš„æ¶æ€§é¢„æµ‹ã€‚åœ¨åŒ…å«349å¼ å›¾åƒçš„ä¸´åºŠæ•°æ®é›†ä¸Šè¿›è¡Œçš„äº”æŠ˜äº¤å‰éªŒè¯æ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•å–å¾—äº†0.852çš„ F1-scoreï¼Œä¼˜äºåŸºäºæ‰‹å·¥å½¢æ€å­¦ç‰¹å¾çš„ Random Forest åŸºå‡†æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹å­¦ä¹ åˆ°çš„éšå¼è§†è§‰ç‰¹å¾æ¯”å•çº¯çš„æ˜¾å¼å½¢çŠ¶ç‰¹å¾æ›´å…·é¢„æµ‹ä»·å€¼ã€‚ä½œä¸ºé¦–ä¸ªé›†æˆäº†ç»“èŠ‚æ£€æµ‹ä¸æ¶æ€§é¢„æµ‹çš„å…¨è‡ªåŠ¨ç«¯åˆ°ç«¯æµæ°´çº¿ï¼Œè¯¥ç ”ç©¶ä¸ºæé«˜ç”²çŠ¶è…ºç™Œä¸´åºŠè¯Šæ–­çš„å‡†ç¡®æ€§ä¸ä¸€è‡´æ€§æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "5 pages, 2 figures, 2 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.11937v1",
      "published_date": "2025-11-14 23:23:24 UTC",
      "updated_date": "2025-11-14 23:23:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:12:36.131870+00:00"
    },
    {
      "arxiv_id": "2511.11924v1",
      "title": "A Neuromorphic Architecture for Scalable Event-Based Control",
      "title_zh": "ç”¨äºå¯æ‰©å±•åŸºäºäº‹ä»¶æ§åˆ¶çš„ç¥ç»å½¢æ€æ¶æ„",
      "authors": [
        "Yongkang Huo",
        "Fulvio Forni",
        "Rodolphe Sepulchre"
      ],
      "abstract": "This paper introduces the ``rebound Winner-Take-All (RWTA)\" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†â€œrebound Winner-Take-All (RWTA)â€åŸºå…ƒï¼Œä½œä¸ºå¯æ‰©å±•ç¥ç»å½¢æ€æ§åˆ¶æ¶æ„ (Scalable Neuromorphic Control Architecture) çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚è¯¥æ¶æ„åœ¨ä»ç»†èƒåˆ°ç³»ç»Ÿçš„å„ä¸ªå±‚é¢ä¸Šï¼ŒæˆåŠŸèåˆäº†ç¦»æ•£è®¡ç®—çš„å¯é æ€§ä¸è¿ç»­è°ƒèŠ‚çš„å¯è°ƒæ€§ï¼Œæœ‰æ•ˆç»“åˆäº†èƒœè€…é€šåƒ (Winner-Take-All) çŠ¶æ€æœºçš„ç¦»æ•£è®¡ç®—èƒ½åŠ›ä¸å¯å…´å¥‹ç”Ÿç‰©ç‰©ç†ç”µè·¯ (Excitable Biophysical Circuits) çš„è¿ç»­å¾®è°ƒç‰¹æ€§ã€‚è¿™ä¸€åŸºäºäº‹ä»¶ (Event-Based) çš„æ¡†æ¶åˆ©ç”¨ç»Ÿä¸€çš„ç‰©ç†å»ºæ¨¡è¯­è¨€ï¼ŒåŒæ­¥è§£å†³äº†è¿ç»­èŠ‚å¾‹ç”Ÿæˆ (Continuous Rhythmic Generation) ä¸ç¦»æ•£å†³ç­– (Discrete Decision-Making) é—®é¢˜ã€‚é€šè¿‡å¯¹è›‡å½¢æœºå™¨äºº (Snake Robot) ç¥ç»ç³»ç»Ÿçš„è®¾è®¡ä¸æ¨¡æ‹Ÿï¼Œç ”ç©¶éªŒè¯äº†è¯¥æ¶æ„åœ¨å®ç°å¤æ‚ç”Ÿç‰©å¯å‘å¼æ§åˆ¶ä»»åŠ¡æ—¶çš„å¤šåŠŸèƒ½æ€§ã€é²æ£’æ€§ä»¥åŠé«˜åº¦çš„æ¨¡å—åŒ–ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11924v1",
      "published_date": "2025-11-14 23:08:56 UTC",
      "updated_date": "2025-11-14 23:08:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:12:34.937828+00:00"
    },
    {
      "arxiv_id": "2511.11921v1",
      "title": "Looking Forward: Challenges and Opportunities in Agentic AI Reliability",
      "title_zh": "å±•æœ›æœªæ¥ï¼šæ™ºèƒ½ä½“ AI å¯é æ€§çš„æŒ‘æˆ˜ä¸æœºé‡",
      "authors": [
        "Liudong Xing",
        "Janet",
        "Lin"
      ],
      "abstract": "This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ„å»ºå¯é  AI ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯ agentic AI ç³»ç»Ÿçš„æŒ‘æˆ˜ä¸æœªæ¥å‘å±•æ–¹å‘ã€‚æ–‡ç« é‡ç‚¹åˆ†æäº†å¦‚ä½•ç¼“è§£è¿é”æ•…éšœ (cascading failures) å¸¦æ¥çš„é£é™©ï¼Œå¹¶æ·±å…¥æ¢è®¨äº†åŠ¨æ€ç¯å¢ƒ (dynamic environments)ã€ä¸ä¸€è‡´çš„ä»»åŠ¡æ‰§è¡Œ (inconsistent task execution) ä»¥åŠä¸å¯é¢„æµ‹çš„æ¶Œç°è¡Œä¸º (unpredictable emergent behaviors) å¯¹ç³»ç»Ÿå¯é æ€§çš„å½±å“ã€‚æ­¤å¤–ï¼Œé’ˆå¯¹èµ„æºå¯†é›†å‹å¯é æ€§æœºåˆ¶å’Œ agentic AI ç³»ç»Ÿçš„æµ‹è¯•ä¸è¯„ä¼° (testing and evaluating reliability)ï¼Œç ”ç©¶æå‡ºäº†å¤šé¡¹å‰ç»æ€§çš„ç ”ç©¶é—®é¢˜ä¸æœºé‡ã€‚é€šè¿‡å¯¹è¿™äº›æ ¸å¿ƒé—®é¢˜çš„ç³»ç»Ÿæ€§æ¢³ç†ï¼Œè¯¥æ–‡ç« ä¸ºæå‡æ™ºèƒ½ä½“ç³»ç»Ÿçš„å¯ä¿¡åº¦ä¸ç¨³å®šæ€§æä¾›äº†é‡è¦çš„ç†è®ºå‚è€ƒä¸æœªæ¥ç ”ç©¶è·¯å¾„ã€‚",
      "categories": [
        "cs.AI",
        "cs.ET",
        "cs.PF"
      ],
      "primary_category": "cs.AI",
      "comment": "13 pages, 6 figures; This is a preprint of a chapter accepted for publication in Generative and Agentic AI Reliability: Architectures, Challenges, and Trust for Autonomous Systems, published by SpringerNature",
      "pdf_url": "https://arxiv.org/pdf/2511.11921v1",
      "published_date": "2025-11-14 23:05:43 UTC",
      "updated_date": "2025-11-14 23:05:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:12:46.733436+00:00"
    },
    {
      "arxiv_id": "2511.17574v1",
      "title": "Constructing Political Coordinates: Aggregating Over the Opposition for Diverse News Recommendation",
      "title_zh": "æ„å»ºæ”¿æ²»åæ ‡ï¼šé€šè¿‡èšåˆå¯¹ç«‹é¢å®ç°å¤šæ ·åŒ–æ–°é—»æ¨è",
      "authors": [
        "Eamon Earl",
        "Chen Ding",
        "Richard Valenzano",
        "Drai Paulen-Patterson"
      ],
      "abstract": "In the past two decades, open access to news and information has increased rapidly, empowering educated political growth within democratic societies. News recommender systems (NRSs) have shown to be useful in this process, minimizing political disengagement and information overload by providing individuals with articles on topics that matter to them. Unfortunately, NRSs often conflate underlying user interest with the partisan bias of the articles in their reading history and with the most popular biases present in the coverage of their favored topics. Over extended interaction, this can result in the formation of filter bubbles and the polarization of user partisanship. In this paper, we propose a novel embedding space called Constructed Political Coordinates (CPC), which models the political partisanship of users over a given topic-space, relative to a larger sample population. We apply a simple collaborative filtering (CF) framework using CPC-based correlation to recommend articles sourced from oppositional users, who have different biases from the user in question. We compare against classical CF methods and find that CPC-based methods promote pointed bias diversity and better match the true political tolerance of users, while classical methods implicitly exploit biases to maximize interaction.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ–°é—»æ¨èç³»ç»Ÿ(NRSs)æ˜“å¯¼è‡´è¿‡æ»¤æ°”æ³¡å’Œæ”¿æ²»æåŒ–çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸ºConstructed Political Coordinates (CPC)çš„æ–°å‹åµŒå…¥ç©ºé—´ã€‚CPCé€šè¿‡åœ¨ç‰¹å®šä¸»é¢˜ç©ºé—´å†…ç›¸å¯¹äºå¤§æ ·æœ¬ç¾¤ä½“å»ºæ¨¡ç”¨æˆ·çš„æ”¿æ²»åè§ï¼Œä¸ºè¡¡é‡ç”¨æˆ·ç«‹åœºæä¾›äº†é‡åŒ–åŸºç¡€ã€‚ç ”ç©¶è€…è¿›ä¸€æ­¥åˆ©ç”¨åŸºäºCPCç›¸å…³æ€§çš„ååŒè¿‡æ»¤(Collaborative Filtering)æ¡†æ¶ï¼Œä¸»åŠ¨å‘ç”¨æˆ·æ¨èæ¥è‡ªåè§ç«‹åœºå¯¹ç«‹è€…çš„å¤šæ ·åŒ–æ–°é—»å†…å®¹ã€‚å¯¹æ¯”å®éªŒå‘ç°ï¼Œä¼ ç»ŸååŒè¿‡æ»¤æ–¹æ³•å¾€å¾€å€¾å‘äºåˆ©ç”¨åè§æ¥æœ€å¤§åŒ–ç”¨æˆ·äº¤äº’ï¼Œè€ŒåŸºäºCPCçš„æ–¹æ³•åˆ™èƒ½æ˜¾è‘—æå‡æ¨èå†…å®¹çš„åè§å¤šæ ·æ€§ï¼Œå¹¶æ›´å¥½åœ°åŒ¹é…ç”¨æˆ·çš„çœŸå®æ”¿æ²»åŒ…å®¹åº¦ã€‚è¯¥ç ”ç©¶ä¸ºç¼“è§£æ¨èç³»ç»Ÿä¸­çš„ä¿¡æ¯èŒ§æˆ¿æ•ˆåº”å¹¶ä¿ƒè¿›å¤šå…ƒåŒ–ä¿¡æ¯æ¥è§¦æä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.SI",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.SI",
      "comment": "Due to appear in the proceedings of the 2025 IEEE International Conference on Big Data",
      "pdf_url": "https://arxiv.org/pdf/2511.17574v1",
      "published_date": "2025-11-14 23:04:04 UTC",
      "updated_date": "2025-11-14 23:04:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:12:53.427533+00:00"
    },
    {
      "arxiv_id": "2511.17573v1",
      "title": "Binary BPE: A Family of Cross-Platform Tokenizers for Binary Analysis",
      "title_zh": "Binary BPEï¼šé¢å‘äºŒè¿›åˆ¶åˆ†æçš„è·¨å¹³å°åˆ†è¯å™¨ç³»åˆ—",
      "authors": [
        "Michael J. Bommarito"
      ],
      "abstract": "Sequence models for binary analysis are bottlenecked by byte-level tokenization: raw bytes waste precious context window capacity for transformers and other neural network architectures, and many existing text-oriented tokenizers fail on arbitrary 0x00--0xFF sequences. To address this issue, we introduce the Binary BPE tokenizer family, a set of cross-platform Byte Pair Encoding (BPE) tokenizers for executables trained on a large corpus of binaries spanning multiple platforms, architectures, and operating systems, including Linux, Windows, macOS, Android, and malware sources. We release trained tokenizers with vocabularies of 4K, 8K, 16K, 32K, and 64K tokens, enabling both systematic scaling studies and practical deployment from resource-constrained edge devices to high-throughput datacenters. These tokenizers discover interpretable patterns (ELF/PE headers, instruction sequences, cross-platform strings) while yielding multi-byte compression per token. On representative uncompressed executables (e.g., ELF/PE/Mach-O rather than compressed APKs), the Binary BPE tokenizers typically allow for roughly 2-3x more binary content per fixed-length transformer context window than raw bytes, enabling more efficient research and practical deployment for content identification, malware detection, reverse engineering, and optimization. We release the trained Binary BPE tokenizers on HuggingFace, providing a drop-in, open-source foundation for binary-focused language models and context-efficient agentic tools.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äºŒè¿›åˆ¶åˆ†æ(Binary analysis)ä¸­å­—èŠ‚çº§æ ‡è®°åŒ–(byte-level tokenization)å¯¼è‡´ä¸Šä¸‹æ–‡çª—å£åˆ©ç”¨ç‡ä½ä¸”ç°æœ‰æ–‡æœ¬æ ‡è®°å™¨æ— æ³•å¤„ç†ä»»æ„äºŒè¿›åˆ¶åºåˆ—çš„é—®é¢˜ï¼Œæå‡ºäº† Binary BPE ç³»åˆ—è·¨å¹³å°æ ‡è®°å™¨ã€‚è¯¥ç³»åˆ—æ ‡è®°å™¨åŸºäºæ¶µç›– Linuxã€Windowsã€macOSã€Android åŠæ¶æ„è½¯ä»¶çš„å¤§è§„æ¨¡äºŒè¿›åˆ¶è¯­æ–™åº“ï¼Œåˆ©ç”¨å­—èŠ‚å¯¹ç¼–ç (Byte Pair Encoding)æŠ€æœ¯è®­ç»ƒè€Œæˆã€‚ç ”ç©¶å›¢é˜Ÿå‘å¸ƒäº†è¯æ±‡é‡ä» 4K åˆ° 64K çš„å¤šç§ç‰ˆæœ¬ï¼Œèƒ½å¤Ÿæœ‰æ•ˆè¯†åˆ« ELF/PE å¤´éƒ¨ã€æŒ‡ä»¤åºåˆ—åŠè·¨å¹³å°å­—ç¬¦ä¸²ç­‰å…·æœ‰å¯è§£é‡Šæ€§çš„æ¨¡å¼ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒBinary BPE ç›¸æ¯”åŸå§‹å­—èŠ‚åœ¨å®šé•¿ä¸Šä¸‹æ–‡çª—å£ä¸­å¯å¤šæ‰¿è½½çº¦ 2-3 å€çš„äºŒè¿›åˆ¶å†…å®¹ï¼Œæå¤§æå‡äº†å¤„ç†æ•ˆç‡ã€‚è¿™ä¸€æˆæœä¸ºå†…å®¹è¯†åˆ«ã€æ¶æ„è½¯ä»¶æ£€æµ‹(malware detection)å’Œé€†å‘å·¥ç¨‹(reverse engineering)ç­‰ä»»åŠ¡æä¾›äº†é«˜æ•ˆçš„åŸºç¡€è®¾æ–½ï¼Œå¹¶å·²åœ¨ HuggingFace ä½œä¸ºå¼€æºåŸºç¡€ç»„ä»¶å‘å¸ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "17 pages, 3 figures, 9 tables. Paper source available at https://github.com/mjbommar/binary-tokenizer-paper ; tokenizers available at https://huggingface.co/mjbommar - mjbommar/binary-tokenizer-001-{4k,8k,16k,32k,64k}",
      "pdf_url": "https://arxiv.org/pdf/2511.17573v1",
      "published_date": "2025-11-14 22:53:03 UTC",
      "updated_date": "2025-11-14 22:53:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:12:53.829584+00:00"
    },
    {
      "arxiv_id": "2511.11918v1",
      "title": "Batch Matrix-form Equations and Implementation of Multilayer Perceptrons",
      "title_zh": "å¤šå±‚æ„ŸçŸ¥æœºçš„æ‰¹é‡çŸ©é˜µå½¢å¼æ–¹ç¨‹ä¸å®ç°",
      "authors": [
        "Wieger Wesselink",
        "Bram Grooten",
        "Huub van de Wetering",
        "Qiao Xiao",
        "Decebal Constantin Mocanu"
      ],
      "abstract": "Multilayer perceptrons (MLPs) remain fundamental to modern deep learning, yet their algorithmic details are rarely presented in complete, explicit \\emph{batch matrix-form}. Rather, most references express gradients per sample or rely on automatic differentiation. Although automatic differentiation can achieve equally high computational efficiency, the usage of batch matrix-form makes the computational structure explicit, which is essential for transparent, systematic analysis, and optimization in settings such as sparse neural networks. This paper fills that gap by providing a mathematically rigorous and implementation-ready specification of MLPs in batch matrix-form. We derive forward and backward equations for all standard and advanced layers, including batch normalization and softmax, and validate all equations using the symbolic mathematics library SymPy. From these specifications, we construct uniform reference implementations in NumPy, PyTorch, JAX, TensorFlow, and a high-performance C++ backend optimized for sparse operations. Our main contributions are: (1) a complete derivation of batch matrix-form backpropagation for MLPs, (2) symbolic validation of all gradient equations, (3) uniform Python and C++ reference implementations grounded in a small set of matrix primitives, and (4) demonstration of how explicit formulations enable efficient sparse computation. Together, these results establish a validated, extensible foundation for understanding, teaching, and researching neural network algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šå±‚æ„ŸçŸ¥å™¨(Multilayer perceptrons, MLPs)åœ¨å­¦æœ¯æ–‡çŒ®ä¸­ç¼ºä¹å®Œæ•´ã€æ˜¾å¼çš„æ‰¹å¤„ç†çŸ©é˜µå½¢å¼(batch matrix-form)æ¨å¯¼è¿™ä¸€ç°çŠ¶ï¼Œæå‡ºäº†ä¸€å¥—æ•°å­¦ä¸¥è°¨ä¸”æ˜“äºå®ç°çš„è§„èŒƒã€‚ä½œè€…æ¨å¯¼äº†åŒ…æ‹¬æ‰¹é‡å½’ä¸€åŒ–(Batch Normalization)å’ŒSoftmaxåœ¨å†…çš„æ‰€æœ‰æ ‡å‡†åŠé«˜çº§å±‚çš„æ­£å‘ä¸åå‘ä¼ æ’­æ–¹ç¨‹ï¼Œå¹¶åˆ©ç”¨ç¬¦å·æ•°å­¦åº“SymPyå¯¹æ‰€æœ‰æ¢¯åº¦æ–¹ç¨‹è¿›è¡Œäº†ä¸¥è°¨éªŒè¯ã€‚åŸºäºè¿™äº›æ˜¾å¼å…¬å¼ï¼Œç ”ç©¶å›¢é˜Ÿåœ¨NumPyã€PyTorchã€JAXã€TensorFlowä»¥åŠé’ˆå¯¹ç¨€ç–æ“ä½œä¼˜åŒ–çš„C++åç«¯ä¸­æ„å»ºäº†ç»Ÿä¸€çš„å‚è€ƒå®ç°ã€‚å®éªŒè¯æ˜ï¼Œè¿™ç§æ˜¾å¼çš„çŸ©é˜µåŒ–è¡¨è¾¾ä¸ä»…å¢å¼ºäº†è®¡ç®—ç»“æ„çš„é€æ˜åº¦ï¼Œè¿˜èƒ½åœ¨ç¨€ç–ç¥ç»ç½‘ç»œç­‰ç‰¹å®šåœºæ™¯ä¸‹å®ç°æ›´é«˜æ•ˆçš„ç¨€ç–è®¡ç®—(sparse computation)ã€‚è¯¥å·¥ä½œä¸ºæ·±åº¦å­¦ä¹ ç®—æ³•çš„æ•™å­¦ã€ç³»ç»ŸåŒ–åˆ†æåŠåº•å±‚ä¼˜åŒ–æä¾›äº†ä¸€ä¸ªç»è¿‡éªŒè¯ä¸”å…·å¤‡å¯æ‰©å±•æ€§çš„åŸºç¡€æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "32 pages; submitted to JMLR",
      "pdf_url": "https://arxiv.org/pdf/2511.11918v1",
      "published_date": "2025-11-14 22:52:27 UTC",
      "updated_date": "2025-11-14 22:52:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:12:54.941482+00:00"
    },
    {
      "arxiv_id": "2511.11916v1",
      "title": "An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR",
      "title_zh": "æ¶æ„å¯¹åŸºäº LLM çš„æŠ½è±¡è§†è§‰æ¨ç†å½±å“åˆ†æï¼šRAVEN-FAIR ç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•",
      "authors": [
        "Sinan Urgun",
        "SeÃ§kin ArÄ±"
      ],
      "abstract": "This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.",
      "tldr_zh": "è¯¥ç ”ç©¶åœ¨ RAVEN-FAIR æ•°æ®é›†ä¸Šå¯¹ GPT-4.1-Miniã€Claude-3.5-Haikuã€Gemini-1.5-Flash å’Œ Llama-3.3-70b ç­‰å¤§è¯­è¨€æ¨¡å‹ (LLMs) çš„æŠ½è±¡è§†è§‰æ¨ç†èƒ½åŠ›è¿›è¡Œäº†ç³»ç»Ÿæ€§è¯„ä¼°ã€‚é€šè¿‡å¯¹æ¯” Single-shotã€Embedding-controlled repetitionã€Self-reflection å’Œ Multi-agent å››ç§æ¨ç†æ¶æ„ï¼Œç ”ç©¶å‘ç° GPT-4.1-Mini åœ¨å„ç§é…ç½®ä¸‹å‡å±•ç°å‡ºæœ€ç¨³å¥çš„æ¨ç†è¡¨ç°ã€‚å®éªŒé‡‡ç”¨äº†ç»“åˆ JSON extractionã€LLM reasoning å’Œ Tool Function çš„ä¸‰é˜¶æ®µå“åº”ç”Ÿæˆæµç¨‹ï¼Œå¹¶åˆ©ç”¨ SSIMã€LPIPS åŠ Chain-of-Thought è¯„åˆ†å¯¹æ¨¡å‹åœ¨ Semantic hallucination å’Œ Numeric misperception ç­‰æ–¹é¢çš„é”™è¯¯è¿›è¡Œäº†é‡åŒ–åˆ†æã€‚ç ”ç©¶æŒ‡å‡ºï¼Œä¸åŒæ¶æ„å¯¹æ¨¡å‹æ€§èƒ½çš„æå‡æ•ˆæœå…·æœ‰æ¨¡å‹ç‰¹å¼‚æ€§ (Model-specific)ï¼Œä¸” Multi-agent æ¶æ„å¹¶ä¸æ€»èƒ½å¸¦æ¥ä¸€è‡´çš„å¢ç›Šã€‚æ­¤å¤–ï¼Œå“åº”è¦†ç›–èŒƒå›´ (Response coverage) çš„å˜å¼‚è¢«ç¡®å®šä¸ºå¹²æ‰°è·¨æ¶æ„ç›´æ¥æ¯”è¾ƒçš„æ··æ·†å› ç´ ã€‚ä¸ºäº†ç¡®ä¿ç»“è®ºçš„å¯é æ€§ï¼Œç ”ç©¶é€šè¿‡æŠ¥å‘Šäº”æ¬¡ç‹¬ç«‹è¿è¡Œçš„æœ€ä½³ç»“æœæ¥ä¼°è®¡æ€§èƒ½ä¸Šé™ï¼Œä»è€Œè§„é¿äº†å•æ¬¡è¯„ä¼°å­˜åœ¨çš„è„†å¼±æ€§ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "23 pages, 9 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.11916v1",
      "published_date": "2025-11-14 22:50:22 UTC",
      "updated_date": "2025-11-14 22:50:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:13:15.644006+00:00"
    },
    {
      "arxiv_id": "2511.11914v3",
      "title": "Forgetting-MarI: LLM Unlearning via Marginal Information Regularization",
      "title_zh": "Forgetting-MarIï¼šåŸºäºè¾¹é™…ä¿¡æ¯æ­£åˆ™åŒ–çš„å¤§è¯­è¨€æ¨¡å‹æœºå™¨é—å¿˜",
      "authors": [
        "Shizhou Xu",
        "Yuan Ni",
        "Stefan Broecker",
        "Thomas Strohmer"
      ],
      "abstract": "As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨éšç§ä¿æŠ¤å’Œæ³•è§„åˆè§„æ–¹é¢çš„éœ€æ±‚ï¼Œæå‡ºäº†åä¸º Forgetting-MarI çš„ Unlearning æ¡†æ¶ï¼Œæ—¨åœ¨ä¸ä»å¤´è®­ç»ƒçš„æƒ…å†µä¸‹é€‰æ‹©æ€§åˆ é™¤ç‰¹å®šå‚æ•°çŸ¥è¯†ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•å› è¿‡åº¦ç§»é™¤ä¿¡æ¯è€Œå¯¼è‡´æ€§èƒ½ä¸‹é™çš„é—®é¢˜ï¼ŒForgetting-MarI é€šè¿‡è¾¹é™…ä¿¡æ¯æ­£åˆ™åŒ– (Marginal Information Regularization) æŠ€æœ¯ï¼Œç»è¯æ˜ä»…ç§»é™¤å¾…é—å¿˜æ•°æ®æä¾›çš„è¾¹é™…ä¿¡æ¯ï¼Œè€Œæœ‰æ•ˆä¿ç•™å…¶ä»–çŸ¥è¯†ã€‚è¯¥æ–¹æ³•é€šè¿‡æƒ©ç½šè¾¹é™…ä¿¡æ¯ï¼Œä¸ºé—å¿˜æ•°æ®åœ¨æ¨¡å‹ä¸­çš„æ®‹ç•™å½±å“è®¾å®šäº†æ˜ç¡®çš„ä¸Šç•Œï¼Œä»è€Œæä¾›äº†å¯è¯æ˜çš„ä¸å¯æ£€æµ‹æ€§ (Provable Undetectability)ã€‚å¹¿æ³›çš„å®éªŒè¯æ˜ï¼ŒForgetting-MarI åœ¨å¯é é—å¿˜å’Œä¿ç•™é€šç”¨æ¨¡å‹æ€§èƒ½æ–¹é¢å‡ä¼˜äºç›®å‰çš„ State-of-the-Art æ–¹æ³•ã€‚è¿™ä¸€è¿›å±•å¢å¼ºäº† AI ç³»ç»Ÿçš„å¯æ§æ€§ï¼Œä½¿å…¶åœ¨ä¸ç‰ºç‰²æœ‰æ•ˆæ€§çš„å‰æä¸‹æ›´ç¬¦åˆéšç§å’Œç‰ˆæƒæ³•è§„çš„è¦æ±‚ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.IT",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11914v3",
      "published_date": "2025-11-14 22:48:39 UTC",
      "updated_date": "2026-01-17 09:15:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:13:09.354687+00:00"
    },
    {
      "arxiv_id": "2511.11908v1",
      "title": "PI-NAIM: Path-Integrated Neural Adaptive Imputation Model",
      "title_zh": "PI-NAIMï¼šè·¯å¾„é›†æˆå¼ç¥ç»è‡ªé€‚åº”æ’è¡¥æ¨¡å‹",
      "authors": [
        "Afifa Khaled",
        "Ebrahim Hamid Sumiea"
      ],
      "abstract": "Medical imaging and multi-modal clinical settings often face the challange of missing modality in their diagnostic pipelines. Existing imputation methods either lack representational capacity or are computationally expensive. We propose PI-NAIM, a novel dual-path architecture that dynamically routes samples to optimized imputation approaches based on missingness complexity. Our framework integrates: (1) intelligent path routing that directs low missingness samples to efficient statistical imputation (MICE) and complex patterns to powerful neural networks (GAIN with temporal analysis); (2) cross-path attention fusion that leverages missingness-aware embeddings to intelligently combine both branches; and (3) end-to-end joint optimization of imputation accuracy and downstream task performance. Extensive experiments on MIMIC-III and multimodal benchmarks demonstrate state-of-the-art performance, achieving RMSE of 0.108 (vs. baselines' 0.119-0.152) and substantial gains in downstream tasks with an AUROC of 0.812 for mortality prediction. PI-NAIM's modular design enables seamless integration into vision pipelines handling incomplete sensor measurements, missing modalities, or corrupted inputs, providing a unified solution for real-world scenario. The code is publicly available at https://github.com/AfifaKhaled/PI-NAIM-Path-Integrated-Neural-Adaptive-Imputation-Model",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»å­¦å½±åƒå’Œå¤šæ¨¡æ€ä¸´åºŠåœºæ™¯ä¸­å¸¸è§çš„æ¨¡æ€ç¼ºå¤±(missing modality)é—®é¢˜ï¼ŒæŒ‡å‡ºç°æœ‰æ’è¡¥æ–¹æ³•å­˜åœ¨è¡¨å¾èƒ½åŠ›ä¸è¶³æˆ–è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚ä¸ºæ­¤æå‡ºäº†PI-NAIMï¼Œè¿™æ˜¯ä¸€ç§æ–°å‹çš„åŒè·¯å¾„æ¶æ„ï¼Œèƒ½å¤Ÿæ ¹æ®ç¼ºå¤±å¤æ‚ç¨‹åº¦åŠ¨æ€åœ°å°†æ ·æœ¬è·¯ç”±è‡³æœ€åˆé€‚çš„æ’è¡¥æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶é›†æˆäº†æ™ºèƒ½è·¯å¾„è·¯ç”±æœºåˆ¶ï¼Œå°†ä½ç¼ºå¤±ç‡æ ·æœ¬å¯¼å‘é«˜æ•ˆçš„ç»Ÿè®¡æ’è¡¥(MICE)ï¼Œè€Œå°†å¤æ‚æ¨¡å¼å¯¼å‘ç»“åˆäº†æ—¶é—´åˆ†æçš„ç¥ç»ç½‘ç»œ(GAIN)ã€‚åŒæ—¶ï¼Œåˆ©ç”¨è·¨è·¯å¾„æ³¨æ„åŠ›èåˆ(cross-path attention fusion)å’Œç¼ºå¤±æ„ŸçŸ¥åµŒå…¥ï¼Œæ™ºèƒ½åœ°ç»“åˆä¸¤ä¸ªåˆ†æ”¯çš„ä¿¡æ¯ï¼Œå¹¶å®ç°äº†æ’è¡¥å‡†ç¡®åº¦ä¸ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½çš„ç«¯åˆ°ç«¯è”åˆä¼˜åŒ–ã€‚åœ¨MIMIC-IIIç­‰åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼ŒPI-NAIMè¾¾åˆ°äº†SOTAæ€§èƒ½ï¼Œå…¶RMSEé™è‡³0.108ï¼Œä¸”åœ¨æ­»äº¡ç‡é¢„æµ‹ä»»åŠ¡ä¸­å–å¾—äº†0.812çš„AUROCã€‚å…¶æ¨¡å—åŒ–è®¾è®¡ä½¿å…¶èƒ½å¤Ÿæ— ç¼é›†æˆåˆ°å¤„ç†ä¼ æ„Ÿå™¨æµ‹é‡ä¸å…¨æˆ–æ¨¡æ€ç¼ºå¤±çš„è§†è§‰æµæ°´çº¿ä¸­ï¼Œä¸ºç°å®åŒ»ç–—åœºæ™¯æä¾›äº†ç»Ÿä¸€çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11908v1",
      "published_date": "2025-11-14 22:38:40 UTC",
      "updated_date": "2025-11-14 22:38:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:13:12.027769+00:00"
    },
    {
      "arxiv_id": "2511.11907v2",
      "title": "KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference",
      "title_zh": "KVSwapï¼šé¢å‘é•¿ä¸Šä¸‹æ–‡ç«¯ä¾§æ¨ç†çš„ç£ç›˜æ„ŸçŸ¥ KV ç¼“å­˜å¸è½½",
      "authors": [
        "Huawei Zhang",
        "Chunwei Xia",
        "Zheng Wang"
      ],
      "abstract": "Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \\emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size. Existing KV-cache offloading schemes are designed to transfer cache data from GPU memory to CPU memory; however, they are not suitable for embedded and mobile systems, where the CPU and GPU (or NPU) typically share a unified memory and the non-volatile secondary storage (disk) offers limited I/O bandwidth. We present KVSwap, a software framework tailored for local devices that achieves high memory efficiency while effectively leveraging disk storage. KVSwap stores the full cache on disk, uses highly compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining generation quality over existing KV cache offloading schemes.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†KVSwapï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºæœ¬åœ°è®¾å¤‡è®¾è®¡çš„è½¯ä»¶æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³é•¿ä¸Šä¸‹æ–‡æ¨ç†åœ¨ç§»åŠ¨å’ŒåµŒå…¥å¼è®¾å¤‡ä¸­é¢ä¸´çš„å†…å­˜å®¹é‡å¢™(memory capacity wall)æŒ‘æˆ˜ã€‚é’ˆå¯¹è¿™äº›è®¾å¤‡ä¸­CPUä¸åŠ é€Ÿå™¨å…±äº«ç»Ÿä¸€å†…å­˜ä¸”ç£ç›˜I/Oå¸¦å®½æœ‰é™çš„ç°çŠ¶ï¼ŒKVSwapé€šè¿‡å°†å®Œæ•´çš„KV Cacheå­˜å‚¨åœ¨ç£ç›˜ä¸­ï¼Œå¹¶ä½¿ç”¨ç´§å‡‘çš„å†…å­˜å…ƒæ•°æ®é¢„æµ‹éœ€é¢„åŠ è½½çš„æ¡ç›®ï¼Œä»è€Œæ˜¾è‘—æé«˜å†…å­˜æ•ˆç‡ã€‚è¯¥æ¡†æ¶è¿›ä¸€æ­¥é€šè¿‡è®¡ç®—ä¸ç¡¬ä»¶æ„ŸçŸ¥ç£ç›˜è®¿é—®çš„é‡å ï¼Œä»¥åŠæ ¹æ®å­˜å‚¨è®¾å¤‡ç‰¹æ€§ä¼˜åŒ–è¯»å–æ¨¡å¼ï¼Œå®ç°äº†é«˜æ•ˆçš„æ•°æ®è°ƒåº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨ä¸¥è‹›çš„å†…å­˜é¢„ç®—ä¸‹ï¼ŒKVSwapåœ¨ä¸åŒè¯­è¨€æ¨¡å‹å’Œå­˜å‚¨ä»‹è´¨ä¸Šçš„ååé‡å‡ä¼˜äºç°æœ‰çš„KV Cacheå¸è½½æ–¹æ¡ˆï¼ŒåŒæ—¶ä¿è¯äº†æ¨¡å‹ç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI"
      ],
      "primary_category": "cs.DC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11907v2",
      "published_date": "2025-11-14 22:37:57 UTC",
      "updated_date": "2025-12-11 23:35:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:13:14.327825+00:00"
    },
    {
      "arxiv_id": "2511.11902v1",
      "title": "Robust Bidirectional Associative Memory via Regularization Inspired by the Subspace Rotation Algorithm",
      "title_zh": "åŸºäºå­ç©ºé—´æ—‹è½¬ç®—æ³•å¯å‘å¼æ­£åˆ™åŒ–çš„é²æ£’åŒå‘è”æƒ³è®°å¿†",
      "authors": [
        "Ci Lin",
        "Tet Yeap",
        "Iluju Kiringa",
        "Biwei Zhang"
      ],
      "abstract": "Bidirectional Associative Memory (BAM) trained with Bidirectional Backpropagation (B-BP) often suffers from poor robustness and high sensitivity to noise and adversarial attacks. To address these issues, we propose a novel gradient-free training algorithm, the Bidirectional Subspace Rotation Algorithm (B-SRA), which significantly improves the robustness and convergence behavior of BAM. Through comprehensive experiments, we identify two key principles -- orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA) -- as central to enhancing the robustness of BAM. Motivated by these findings, we introduce new regularization strategies into B-BP, resulting in models with greatly improved resistance to corruption and adversarial perturbations. We further conduct an ablation study across different training strategies to determine the most robust configuration and evaluate BAM's performance under a variety of attack scenarios and memory capacities, including 50, 100, and 200 associative pairs. Among all methods, the SAME configuration, which integrates both OWM and GPA, achieves the strongest resilience. Overall, our results demonstrate that B-SRA and the proposed regularization strategies lead to substantially more robust associative memories and open new directions for building resilient neural architectures.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ä½¿ç”¨Bidirectional Backpropagation (B-BP)è®­ç»ƒçš„åŒå‘è”æƒ³è®°å¿†(Bidirectional Associative Memory, BAM)åœ¨é¢å¯¹å™ªå£°å’Œå¯¹æŠ—æ”»å‡»æ—¶é²æ£’æ€§è¾ƒå·®çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å…¨æ–°çš„æ— æ¢¯åº¦è®­ç»ƒç®—æ³•â€”â€”Bidirectional Subspace Rotation Algorithm (B-SRA)ã€‚é€šè¿‡æ·±å…¥çš„å®éªŒåˆ†æï¼Œç ”ç©¶è¯†åˆ«å‡ºæ­£äº¤æƒé‡çŸ©é˜µ(Orthogonal Weight Matrices, OWM)å’Œæ¢¯åº¦æ¨¡å¼å¯¹é½(Gradient-Pattern Alignment, GPA)æ˜¯æå‡BAMé²æ£’æ€§çš„ä¸¤ä¸ªæ ¸å¿ƒåŸåˆ™ã€‚åŸºäºè¿™äº›å‘ç°ï¼Œç ”ç©¶äººå‘˜å°†æ–°çš„æ­£åˆ™åŒ–ç­–ç•¥å¼•å…¥ä¼ ç»Ÿçš„B-BPç®—æ³•ä¸­ï¼Œæ˜¾è‘—å¢å¼ºäº†æ¨¡å‹å¯¹æ•°æ®æŸåå’Œå¯¹æŠ—æ‰°åŠ¨çš„æŠµæŠ—åŠ›ã€‚è¯¥ç ”ç©¶åœ¨50ã€100åŠ200ä¸ªå…³è”å¯¹çš„ä¸åŒå­˜å‚¨å®¹é‡ä¸‹è¿›è¡Œäº†æ¶ˆèç ”ç©¶ï¼Œå¹¶åœ¨å¤šç§æ”»å‡»åœºæ™¯ä¸‹éªŒè¯äº†æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œèåˆäº†OWMå’ŒGPAçš„SAMEé…ç½®åœ¨æ‰€æœ‰æ–¹æ³•ä¸­å±•ç°å‡ºæœ€å¼ºçš„å¼¹æ€§ã€‚æ€»ä½“è€Œè¨€ï¼ŒB-SRAç®—æ³•å’Œæ‰€æå‡ºçš„æ­£åˆ™åŒ–ç­–ç•¥å¤§å¹…æå‡äº†è”æƒ³è®°å¿†çš„ç¨³å¥æ€§ï¼Œä¸ºæ„å»ºæ›´å…·éŸ§æ€§çš„ç¥ç»ç½‘ç»œæ¶æ„å¼€è¾Ÿäº†æ–°æ–¹å‘ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11902v1",
      "published_date": "2025-11-14 22:15:07 UTC",
      "updated_date": "2025-11-14 22:15:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:13:18.229163+00:00"
    },
    {
      "arxiv_id": "2511.11899v1",
      "title": "End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction",
      "title_zh": "é¢å‘æ‰‹æœ¯åŠ¨ä½œåºåˆ—è¯†åˆ«ä¸ä¸´åºŠç»“å±€é¢„æµ‹çš„ç«¯åˆ°ç«¯äººå·¥æ™ºèƒ½ç³»ç»Ÿ",
      "authors": [
        "Xi Li",
        "Nicholas Matsumoto",
        "Ujjwal Pasupulety",
        "Atharva Deo",
        "Cherine Yang",
        "Jay Moran",
        "Miguel E. Hernandez",
        "Peter Wager",
        "Jasmine Lin",
        "Jeanine Kim",
        "Alvin C. Goh",
        "Christian Wagner",
        "Geoffrey A. Sonn",
        "Andrew J. Hung"
      ],
      "abstract": "Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºFrame-to-Outcome (F2O)çš„ç«¯åˆ°ç«¯ç³»ç»Ÿï¼Œæ—¨åœ¨å°†æ‰‹æœ¯ç»„ç»‡å‰¥ç¦»è§†é¢‘è½¬åŒ–ä¸ºæ‰‹åŠ¿åºåˆ—ï¼Œå¹¶æ­ç¤ºå…¶ä¸æœ¯åä¸´åºŠç»“å±€(postoperative outcomes)ä¹‹é—´çš„å…³è”ã€‚F2Oåˆ©ç”¨åŸºäºtransformerçš„ç©ºé—´å’Œæ—¶é—´å»ºæ¨¡ä»¥åŠå¸§çº§åˆ†ç±»æŠ€æœ¯ï¼Œåœ¨æœºå™¨äººè¾…åŠ©æ ¹æ²»æ€§å‰åˆ—è…ºåˆ‡é™¤æœ¯(robot-assisted radical prostatectomy)çš„ç¥ç»ä¿æŠ¤æ­¥éª¤ä¸­å®ç°äº†å¯¹æ‰‹åŠ¿çš„é²æ£’æ£€æµ‹ï¼Œå…¶å¸§çº§åˆ«å’Œè§†é¢‘çº§åˆ«çš„AUCåˆ†åˆ«è¾¾åˆ°0.80å’Œ0.81ã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡åˆ†ææ‰‹åŠ¿é¢‘ç‡ã€æŒç»­æ—¶é—´å’Œè½¬æ¢ç­‰ç‰¹å¾ï¼ŒF2Oé¢„æµ‹æœ¯åç»“å±€çš„å‡†ç¡®ç‡ä¸º0.79ï¼Œä¼˜äºäººç±»æ ‡æ³¨çš„è¡¨ç°(0.75)ä¸”ä¸¤è€…å…·æœ‰æå¼ºçš„ç›¸å…³æ€§(r = 0.96)ã€‚æ­¤å¤–ï¼Œè¯¥ç³»ç»ŸæˆåŠŸæ•æ‰åˆ°äº†ä¸å‹ƒèµ·åŠŸèƒ½æ¢å¤(erectile function recovery)ç›¸å…³çš„å…³é”®æ‰‹æœ¯æ¨¡å¼ï¼Œä¾‹å¦‚ç»„ç»‡å‰¥ç¦»æ—¶é—´å»¶é•¿å’Œèƒ½é‡ä½¿ç”¨å‡å°‘ã€‚é€šè¿‡å®ç°è‡ªåŠ¨åŒ–çš„å¯è§£é‡Šæ€§è¯„ä¼°ï¼ŒF2Oä¸ºæ•°æ®é©±åŠ¨çš„æ‰‹æœ¯åé¦ˆå’Œå‰ç»æ€§ä¸´åºŠå†³ç­–æ”¯æŒ(clinical decision support)å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11899v1",
      "published_date": "2025-11-14 22:02:46 UTC",
      "updated_date": "2025-11-14 22:02:46 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:13:25.238123+00:00"
    },
    {
      "arxiv_id": "2511.11898v1",
      "title": "Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks",
      "title_zh": "Prompt Triageï¼šç»“æ„åŒ–ä¼˜åŒ–æå‡è§†è§‰è¯­è¨€æ¨¡å‹åœ¨åŒ»å­¦å½±åƒåŸºå‡†ä¸Šçš„æ€§èƒ½",
      "authors": [
        "Arnav Singhvi",
        "Vasiliki Bikia",
        "Asad Aali",
        "Akshay Chaudhari",
        "Roxana Daneshjou"
      ],
      "abstract": "Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at https://github.com/DaneshjouLab/prompt-triage-lab.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€åŸºç¡€æ¨¡å‹(VLMs)åœ¨åŒ»ç–—å½±åƒåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜ï¼ŒæŒ‡å‡ºæ¨¡å‹å¾®è°ƒ(finetuning)æˆæœ¬é«˜æ˜‚ä¸”æ‰‹åŠ¨æç¤ºè¯å·¥ç¨‹(manual prompt engineering)éš¾ä»¥æ¨å¹¿ã€‚ä¸ºæ­¤ï¼Œç ”ç©¶å›¢é˜Ÿé€‚é…äº†Declarative Self-improving Python (DSPy)æ¡†æ¶ï¼Œæå‡ºäº†ä¸€ç§ç”¨äºåŒ»ç–—è§†è§‰è¯­è¨€ç³»ç»Ÿçš„ç»“æ„åŒ–è‡ªåŠ¨åŒ–æç¤ºè¯ä¼˜åŒ–æ–¹æ³•ã€‚å®éªŒæ¶µç›–äº†æ”¾å°„å­¦ã€èƒƒè‚ ç—…å­¦å’Œçš®è‚¤ç—…å­¦ç­‰äº”ä¸ªé¢†åŸŸçš„åŒ»ç–—å½±åƒä»»åŠ¡ï¼Œå¹¶åœ¨10ä¸ªå¼€æºVLMsä¸Šå¯¹å››ç§ä¼˜åŒ–æŠ€æœ¯è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚ç»“æœæ˜¾ç¤ºï¼Œä¼˜åŒ–åçš„æµæ°´çº¿è¾ƒé›¶æ ·æœ¬æç¤º(zero-shot prompting)åŸºçº¿å®ç°äº†53%çš„ä¸­ä½ç›¸å¯¹æå‡ï¼Œåœ¨éƒ¨åˆ†æå…·æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ä¸­è¡¨ç°æ¶¨å¹…ç”šè‡³è¾¾åˆ°äº†3,400%ã€‚è¿™ä¸€æˆæœè¯æ˜äº†è‡ªåŠ¨åŒ–æç¤ºè¯ä¼˜åŒ–åœ¨æå‡åŒ»ç–—AIç³»ç»Ÿä¸´åºŠå½±åƒè§£è¯»å‡†ç¡®æ€§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ï¼Œä¸ä»…å…·æœ‰è‰¯å¥½çš„å¯æ‰©å±•æ€§å¹¶èƒ½ä¿æŠ¤æ•°æ®éšç§ï¼Œè¿˜æ˜¾è‘—å‡è½»äº†ä¸´åºŠåŒ»ç”Ÿå¯¹å¤æ‚æç¤ºè¯è®¾è®¡çš„ä¾èµ–ã€‚è¯¥ç ”ç©¶ä¸ä»…ä¸ºå¼€æºVLMsçš„æ€§èƒ½æå‡æä¾›äº†æ–°è·¯å¾„ï¼Œä¹Ÿä¸ºå¯é‡å¤çš„ä¸“ä¸šåŒ»ç–—ä»»åŠ¡ç ”ç©¶å¥ å®šäº†æŠ€æœ¯åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11898v1",
      "published_date": "2025-11-14 22:01:08 UTC",
      "updated_date": "2025-11-14 22:01:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:13:31.426257+00:00"
    },
    {
      "arxiv_id": "2511.11896v2",
      "title": "VULPO: Context-Aware Vulnerability Detection via On-Policy LLM Optimization",
      "title_zh": "VULPOï¼šåŸºäº On-Policy LLM ä¼˜åŒ–çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¼æ´æ£€æµ‹",
      "authors": [
        "Youpeng Li",
        "Fuxun Yu",
        "Xinda Wang"
      ],
      "abstract": "The widespread reliance on open-source software dramatically increases the risk of vulnerability exploitation, underscoring the need for effective and scalable vulnerability detection (VD). Existing VD techniques, whether traditional machine learning-based or LLM-based approaches like prompt engineering, supervised fine-tuning, or off-policy preference optimization, remain fundamentally limited in their ability to perform context-aware analysis: They depend on fixed inputs or static preference datasets, cannot adaptively explore repository-level dependencies, and are constrained by function-level benchmarks that overlook critical vulnerability context.\n  This paper introduces Vulnerability-Adaptive Policy Optimization (VULPO), an on-policy LLM reinforcement learning framework for context-aware VD. To support training and evaluation, we first construct ContextVul, a new dataset that augments high-quality function-level samples with lightweight method to extract repository-level context information. We then design multi-dimensional reward structuring that jointly captures prediction correctness, vulnerability localization accuracy, and the semantic relevance of vulnerability analysis, thereby guiding the model toward comprehensive contextual reasoning. To address the asymmetric difficulty of different vulnerability cases and mitigate reward hacking, VULPO incorporates label-level and sample-level difficulty-adaptive reward scaling, encouraging the model to explore challenging cases while maintaining balanced reward distribution. Extensive experiments demonstrate the superiority of our VULPO framework in context-aware VD: Our VULPO-4B substantially outperforms existing VD baselines based on prompt engineering and off-policy optimization, improving F1 by 85% over Qwen3-4B and achieving performance comparable to a 150x larger-scale model, DeepSeek-R1-0528.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VULPOï¼Œä¸€ç§æ—¨åœ¨å®ç°ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ¼æ´æ£€æµ‹(Vulnerability Detection, VD)çš„åœ¨çº¿ç­–ç•¥LLMå¼ºåŒ–å­¦ä¹ (on-policy LLM reinforcement learning)æ¡†æ¶ï¼Œä»¥è§£å†³ç°æœ‰æŠ€æœ¯åœ¨å¤„ç†ä»“åº“çº§ä¾èµ–å…³ç³»å’Œå¤æ‚ä¸Šä¸‹æ–‡åˆ†ææ—¶çš„å±€é™æ€§ã€‚ä¸ºæ”¯æŒè¯¥æ¡†æ¶ï¼Œç ”ç©¶è€…é¦–å…ˆæ„å»ºäº†ContextVulæ•°æ®é›†ï¼Œåˆ©ç”¨è½»é‡çº§æ–¹æ³•æå–å¹¶å¢å¼ºäº†é«˜è´¨é‡çš„ä»“åº“çº§ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚VULPOè®¾è®¡äº†æ¶µç›–é¢„æµ‹æ­£ç¡®æ€§ã€æ¼æ´å®šä½ç²¾åº¦åŠè¯­ä¹‰ç›¸å…³æ€§çš„å¤šç»´å¥–åŠ±ç»“æ„ï¼Œå¹¶å¼•å…¥æ ‡ç­¾ä¸æ ·æœ¬çº§åˆ«çš„éš¾åº¦è‡ªé€‚åº”å¥–åŠ±ç¼©æ”¾æœºåˆ¶ï¼Œä»¥å¼•å¯¼æ¨¡å‹è¿›è¡Œæ·±å…¥çš„ä¸Šä¸‹æ–‡æ¨ç†å¹¶ç¼“è§£å¥–åŠ±ä½œå¼Š(reward hacking)é—®é¢˜ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVULPO-4Båœ¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€æµ‹ä»»åŠ¡ä¸­æ˜¾è‘—ä¼˜äºåŸºäºæç¤ºå·¥ç¨‹å’Œç¦»çº¿ä¼˜åŒ–çš„åŸºå‡†æ¨¡å‹ï¼Œå…¶F1åˆ†æ•°è¾ƒQwen3-4Bæå‡äº†85%ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹ä»¥è¾ƒå°çš„å‚æ•°è§„æ¨¡å®ç°äº†ä¸DeepSeek-R1-0528ç­‰è¶…å¤§è§„æ¨¡æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨æ¼æ´æ£€æµ‹é¢†åŸŸçš„å“è¶Šæ•ˆèƒ½å’Œæ‰©å±•æ½œåŠ›ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SE"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11896v2",
      "published_date": "2025-11-14 21:57:48 UTC",
      "updated_date": "2025-11-18 18:53:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:13:27.028872+00:00"
    },
    {
      "arxiv_id": "2511.11894v1",
      "title": "Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design",
      "title_zh": "Chain-of-Generationï¼šé¢å‘æ–‡æœ¬å¼•å¯¼åˆ†å­è®¾è®¡çš„æ¸è¿›å¼æ½œæ‰©æ•£æ¨¡å‹",
      "authors": [
        "Lingxiao Li",
        "Haobo Zhang",
        "Bin Chen",
        "Jiayu Zhou"
      ],
      "abstract": "Text-conditioned molecular generation aims to translate natural-language descriptions into chemical structures, enabling scientists to specify functional groups, scaffolds, and physicochemical constraints without handcrafted rules. Diffusion-based models, particularly latent diffusion models (LDMs), have recently shown promise by performing stochastic search in a continuous latent space that compactly captures molecular semantics. Yet existing methods rely on one-shot conditioning, where the entire prompt is encoded once and applied throughout diffusion, making it hard to satisfy all the requirements in the prompt. We discuss three outstanding challenges of one-shot conditioning generation, including the poor interpretability of the generated components, the failure to generate all substructures, and the overambition in considering all requirements simultaneously. We then propose three principles to address those challenges, motivated by which we propose Chain-of-Generation (CoG), a training-free multi-stage latent diffusion framework. CoG decomposes each prompt into curriculum-ordered semantic segments and progressively incorporates them as intermediate goals, guiding the denoising trajectory toward molecules that satisfy increasingly rich linguistic constraints. To reinforce semantic guidance, we further introduce a post-alignment learning phase that strengthens the correspondence between textual and molecular latent spaces. Extensive experiments on benchmark and real-world tasks demonstrate that CoG yields higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that more faithfully reflect complex, compositional prompts while offering transparent insight into the generation process.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Chain-of-Generation (CoG)ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„å¤šé˜¶æ®µæ½œæ‰©æ•£æ¡†æ¶ (latent diffusion framework)ï¼Œæ—¨åœ¨é€šè¿‡è‡ªç„¶è¯­è¨€æè¿°ç²¾å‡†å¼•å¯¼åˆ†å­ç»“æ„çš„è®¾è®¡ã€‚é’ˆå¯¹ç°æœ‰æ½œæ‰©æ•£æ¨¡å‹ (LDMs) åœ¨å•æ¬¡è°ƒèŠ‚ (one-shot conditioning) ä¸‹éš¾ä»¥åŒæ—¶æ»¡è¶³å¤æ‚æŒ‡ä»¤ä¸­æ‰€æœ‰åŠŸèƒ½åŸºå›¢å’Œçº¦æŸæ¡ä»¶çš„å±€é™æ€§ï¼ŒCoG å°†é•¿æç¤ºè¯åˆ†è§£ä¸ºæŒ‰è¯¾ç¨‹æ’åºçš„è¯­ä¹‰ç‰‡æ®µã€‚è¯¥æ¡†æ¶é€šè¿‡åœ¨å»å™ªè½¨è¿¹ä¸­é€æ­¥å¼•å…¥è¿™äº›ç‰‡æ®µä½œä¸ºä¸­é—´ç›®æ ‡ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆæ›´ç¬¦åˆå¤æ‚è¯­è¨€çº¦æŸçš„åˆ†å­ã€‚ä¸ºäº†è¿›ä¸€æ­¥å¢å¼ºè¯­ä¹‰å¼•å¯¼ï¼Œç ”ç©¶è¿˜å¼•å…¥äº†åå¯¹é½å­¦ä¹ é˜¶æ®µ (post-alignment learning phase)ï¼Œä»¥å¼ºåŒ–æ–‡æœ¬ä¸åˆ†å­æ½œåœ¨ç©ºé—´ä¹‹é—´çš„å¯¹åº”å…³ç³»ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒCoG åœ¨è¯­ä¹‰å¯¹é½åº¦ã€ç”Ÿæˆå¤šæ ·æ€§å’Œå¯æ§æ€§æ–¹é¢æ˜¾è‘—ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´å¿ å®åœ°æ‰§è¡Œç»„åˆå¼æç¤ºè¯è¦æ±‚ã€‚è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†åˆ†å­ç”Ÿæˆçš„æˆåŠŸç‡ï¼Œè¿˜ä¸ºç”Ÿæˆè¿‡ç¨‹æä¾›äº†é€æ˜çš„è§£é‡Šæ€§ï¼Œä¸ºç ”ç©¶äººå‘˜æŒ‡å®šç‰©ç†åŒ–å­¦çº¦æŸæä¾›äº†æœ‰æ•ˆçš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "22 pages, 7 figures, 10 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.11894v1",
      "published_date": "2025-11-14 21:54:10 UTC",
      "updated_date": "2025-11-14 21:54:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:13:29.235833+00:00"
    },
    {
      "arxiv_id": "2511.11891v1",
      "title": "FLEX: Feature Importance from Layered Counterfactual Explanations",
      "title_zh": "FLEXï¼šåŸºäºåˆ†å±‚åäº‹å®è§£é‡Šçš„ç‰¹å¾é‡è¦æ€§",
      "authors": [
        "Nawid Keshtmand",
        "Roussel Desmond Nzoyem",
        "Jeffrey Nicholas Clark"
      ],
      "abstract": "Machine learning models achieve state-of-the-art performance across domains, yet their lack of interpretability limits safe deployment in high-stakes settings. Counterfactual explanations are widely used to provide actionable \"what-if\" recourse, but they typically remain instance-specific and do not quantify which features systematically drive outcome changes within coherent regions of the feature space or across an entire dataset. We introduce FLEX (Feature importance from Layered counterfactual EXplanations), a model- and domain-agnostic framework that converts sets of counterfactuals into feature change frequency scores at local, regional, and global levels. FLEX generalises local change-frequency measures by aggregating across instances and neighbourhoods, offering interpretable rankings that reflect how often each feature must change to flip predictions. The framework is compatible with different counterfactual generation methods, allowing users to emphasise characteristics such as sparsity, feasibility, or actionability, thereby tailoring the derived feature importances to practical constraints. We evaluate FLEX on two contrasting tabular tasks: traffic accident severity prediction and loan approval, and compare FLEX to SHAP- and LIME-derived feature importance values. Results show that (i) FLEX's global rankings correlate with SHAP while surfacing additional drivers, and (ii) regional analyses reveal context-specific factors that global summaries miss. FLEX thus bridges the gap between local recourse and global attribution, supporting transparent and intervention-oriented decision-making in risk-sensitive applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†FLEX (Feature importance from Layered counterfactual EXplanations)ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡å‹æ— å…³ä¸”é¢†åŸŸæ— å…³çš„è§£é‡Šæ¡†æ¶ï¼Œæ—¨åœ¨å¼¥è¡¥åäº‹å®è§£é‡Š(Counterfactual Explanations)åœ¨å±€éƒ¨è¡¥æ•‘ä¸å…¨å±€å½’å› ä¹‹é—´çš„å·®è·ã€‚FLEXé€šè¿‡å°†åäº‹å®ç”Ÿæˆçš„é›†åˆè½¬åŒ–ä¸ºå±€éƒ¨ã€åŒºåŸŸå’Œå…¨å±€å±‚é¢çš„ç‰¹å¾å˜åŒ–é¢‘ç‡åˆ†æ•°ï¼Œé‡åŒ–äº†å„ç‰¹å¾åœ¨æ”¹å˜æ¨¡å‹é¢„æµ‹ç»“æœä¸­çš„ç³»ç»Ÿæ€§é¢‘ç‡ã€‚è¯¥æ¡†æ¶å…¼å®¹å¤šç§åäº‹å®ç”Ÿæˆç®—æ³•ï¼Œå…è®¸ç”¨æˆ·æ ¹æ®ç¨€ç–æ€§(Sparsity)ã€å¯è¡Œæ€§(Feasibility)æˆ–å¯è¡ŒåŠ¨æ€§(Actionability)ç­‰çº¦æŸæ¥å®šåˆ¶ç‰¹å¾é‡è¦æ€§æ’åã€‚åœ¨äº¤é€šäº‹æ•…ä¸¥é‡ç¨‹åº¦é¢„æµ‹å’Œè´·æ¬¾å®¡æ‰¹ä»»åŠ¡ä¸­çš„å®éªŒè¡¨æ˜ï¼ŒFLEXçš„å…¨å±€æ’åä¸SHAPé«˜åº¦ç›¸å…³ï¼Œå¹¶èƒ½æŒ–æ˜å‡ºé¢å¤–çš„é©±åŠ¨å› ç´ ï¼Œå…¶åŒºåŸŸåˆ†æåŠŸèƒ½åˆ™æ­ç¤ºäº†å…¨å±€æ‘˜è¦å®¹æ˜“é—æ¼çš„ç‰¹å®šè¯­å¢ƒå› ç´ ã€‚é€šè¿‡èåˆå±€éƒ¨ recourse ä¸å…¨å±€ attributionï¼ŒFLEX ä¸ºé£é™©æ•æ„Ÿåº”ç”¨ä¸­çš„å¹²é¢„å¯¼å‘å†³ç­–æä¾›äº†é€æ˜æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "12 pages, 6 figures, 3 tables, 2 algorithms. Preprint under review",
      "pdf_url": "https://arxiv.org/pdf/2511.11891v1",
      "published_date": "2025-11-14 21:48:24 UTC",
      "updated_date": "2025-11-14 21:48:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:13:42.636410+00:00"
    },
    {
      "arxiv_id": "2511.11885v1",
      "title": "Flash-Fusion: Enabling Expressive, Low-Latency Queries on IoT Sensor Streams with LLMs",
      "title_zh": "Flash-Fusionï¼šåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°ç‰©è”ç½‘ä¼ æ„Ÿå™¨æµçš„é«˜è¡¨ç°åŠ›ã€ä½å»¶è¿ŸæŸ¥è¯¢",
      "authors": [
        "Kausar Patherya",
        "Ashutosh Dhekne",
        "Francisco Romero"
      ],
      "abstract": "Smart cities and pervasive IoT deployments have generated interest in IoT data analysis across transportation and urban planning. At the same time, Large Language Models offer a new interface for exploring IoT data - particularly through natural language. Users today face two key challenges when working with IoT data using LLMs: (1) data collection infrastructure is expensive, producing terabytes of low-level sensor readings that are too granular for direct use, and (2) data analysis is slow, requiring iterative effort and technical expertise. Directly feeding all IoT telemetry to LLMs is impractical due to finite context windows, prohibitive token costs at scale, and non-interactive latencies. What is missing is a system that first parses a user's query to identify the analytical task, then selects the relevant data slices, and finally chooses the right representation before invoking an LLM.\n  We present Flash-Fusion, an end-to-end edge-cloud system that reduces the IoT data collection and analysis burden on users. Two principles guide its design: (1) edge-based statistical summarization (achieving 73.5% data reduction) to address data volume, and (2) cloud-based query planning that clusters behavioral data and assembles context-rich prompts to address data interpretation. We deploy Flash-Fusion on a university bus fleet and evaluate it against a baseline that feeds raw data to a state-of-the-art LLM. Flash-Fusion achieves a 95% latency reduction and 98% decrease in token usage and cost while maintaining high-quality responses. It enables personas across disciplines - safety officers, urban planners, fleet managers, and data scientists - to efficiently iterate over IoT data without the burden of manual query authoring or preprocessing.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Flash-Fusionï¼Œä¸€ä¸ªç«¯åˆ°ç«¯çš„è¾¹ç¼˜-äº‘(edge-cloud)ç³»ç»Ÿï¼Œæ—¨åœ¨è§£å†³ç‰©è”ç½‘(IoT)ä¼ æ„Ÿå™¨æ•°æ®é‡å·¨å¤§ä¸”éš¾ä»¥é€šè¿‡å¤§è¯­è¨€æ¨¡å‹(LLMs)ç›´æ¥è¿›è¡Œä½å»¶è¿Ÿã€ä½æˆæœ¬åˆ†æçš„æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åŸºäºè¾¹ç¼˜çš„ç»Ÿè®¡æ‘˜è¦(statistical summarization)æŠ€æœ¯å®ç°äº† 73.5% çš„æ•°æ®ç¼©å‡ï¼Œå¹¶ç»“åˆäº‘ç«¯çš„æŸ¥è¯¢è§„åˆ’(query planning)å¯¹è¡Œä¸ºæ•°æ®è¿›è¡Œèšç±»ï¼Œä»è€Œæ„å»ºä¸Šä¸‹æ–‡ä¸°å¯Œçš„æç¤º(prompts)ä»¥ä¼˜åŒ–æ•°æ®è§£æã€‚åœ¨å¤§å­¦å…¬äº¤è½¦é˜Ÿçš„å®é™…éƒ¨ç½²è¯„ä¼°æ˜¾ç¤ºï¼Œä¸ç›´æ¥å‘ LLM è¾“å…¥åŸå§‹æ•°æ®çš„åŸºå‡†æ–¹æ³•ç›¸æ¯”ï¼ŒFlash-Fusion åœ¨ä¿æŒé«˜è´¨é‡å“åº”çš„åŒæ—¶ï¼Œå®ç°äº† 95% çš„å»¶è¿Ÿé™ä½ä»¥åŠ 98% çš„ä»¤ç‰Œ(Token)æ¶ˆè€—å’Œæˆæœ¬å‰Šå‡ã€‚è¯¥ç³»ç»Ÿä½¿å¾—åŸå¸‚è§„åˆ’å¸ˆã€è½¦é˜Ÿç®¡ç†è€…å’Œæ•°æ®ç§‘å­¦å®¶èƒ½å¤Ÿé«˜æ•ˆåœ°å¯¹ IoT æ•°æ®è¿›è¡Œè‡ªç„¶è¯­è¨€äº¤äº’å’Œè¿­ä»£æ¢ç´¢ï¼Œæ— éœ€æ‰¿æ‹…ç¹é‡çš„æ‰‹åŠ¨é¢„å¤„ç†æˆ–æŸ¥è¯¢ç¼–å†™è´Ÿæ‹…ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.DC",
      "comment": "12 pages, 5 figures. Under review",
      "pdf_url": "https://arxiv.org/pdf/2511.11885v1",
      "published_date": "2025-11-14 21:34:28 UTC",
      "updated_date": "2025-11-14 21:34:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:13:57.931228+00:00"
    },
    {
      "arxiv_id": "2511.11881v3",
      "title": "Better LLM Reasoning via Dual-Play",
      "title_zh": "é€šè¿‡åŒåšå¼ˆæå‡å¤§è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›",
      "authors": [
        "Zhengxin Zhang",
        "Chengyu Huang",
        "Aochong Oliver Li",
        "Claire Cardie"
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†PasoDobleï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°å‹çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)å¯¹æˆ˜(Dual-Play)æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡æ¨¡å‹é—´çš„å¯¹æŠ—æ€§å­¦ä¹ æå‡æ¨ç†èƒ½åŠ›å¹¶å‡å°‘å¯¹å¤–éƒ¨ç›‘ç£çš„ä¾èµ–ã€‚è¯¥æ¡†æ¶è®­ç»ƒä¸¤ä¸ªæºè‡ªåŒä¸€åŸºç¡€æ¨¡å‹çš„è§’è‰²ï¼šæè®®è€…(Proposer)åˆ©ç”¨é¢„è®­ç»ƒæ•°æ®é›†ç”Ÿæˆå…·æœ‰æ ‡å‡†ç­”æ¡ˆçš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œè€Œæ±‚è§£è€…(Solver)åˆ™å°è¯•è§£å†³è¿™äº›é—®é¢˜ã€‚ä¸ºäº†é˜²æ­¢å¥–åŠ±æ¬ºéª—(Reward Hacking)ï¼Œç³»ç»Ÿå¯¹äº§ç”Ÿèƒ½æŒ‘æˆ˜Solveræé™çš„æœ‰æ•ˆé—®é¢˜çš„Proposerå’Œèƒ½æ­£ç¡®è§£é¢˜çš„Solverè¿›è¡Œå¥–åŠ±ï¼Œå¹¶å®ç°ä¸¤è€…çš„è”åˆæ›´æ–°ã€‚ç ”ç©¶è¿˜å¼•å…¥äº†ä¸€ç§å¯é€‰çš„ç¦»çº¿æ¨¡å¼ï¼Œé€šè¿‡äº¤æ›¿æ›´æ–°Proposerå’ŒSolveræ¥å¢å¼ºè®­ç»ƒç¨³å®šæ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒPasoDobleåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ— éœ€é¢å¤–ç›‘ç£ï¼Œå³å¯æ˜¾è‘—æé«˜LLMsçš„æ¨ç†è¡¨ç°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.LG",
      "comment": "33 pages, 17 figures, 17 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.11881v3",
      "published_date": "2025-11-14 21:19:07 UTC",
      "updated_date": "2026-01-15 20:34:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:13:58.129850+00:00"
    },
    {
      "arxiv_id": "2511.11880v1",
      "title": "Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production",
      "title_zh": "æ£®æ—æ€»åˆçº§ç”Ÿäº§åŠ›ä¼°ç®—ï¼šTransformer ä¸å¾ªç¯æ¨¡å‹çš„å¯¹æ¯”ç ”ç©¶",
      "authors": [
        "David Montero",
        "Miguel D. Mahecha",
        "Francesco Martinuzzi",
        "CÃ©sar Aybar",
        "Anne Klosterhalfen",
        "Alexander Knohl",
        "JesÃºs Anaya",
        "Clemens Mosig",
        "Sebastian Wieneke"
      ],
      "abstract": "Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº† GPT-2 (Transformer æ¶æ„) ä¸ Long Short-Term Memory (LSTM, é€’å½’ç¥ç»ç½‘ç»œ) åœ¨åˆ©ç”¨å¤šå˜é‡è¾“å…¥é¢„æµ‹æ£®æ—æ€»åˆçº§ç”Ÿäº§åŠ› (Gross Primary Production, GPP) æ–¹é¢çš„æ€§èƒ½è¡¨ç°ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œä¸¤ç§æ¨¡å‹åœ¨æ•´ä½“é¢„æµ‹å‡†ç¡®æ€§ä¸Šè¡¨ç°ç›¸è¿‘ï¼Œä½† LSTM åœ¨å¸¸è§„è¡¨ç°ä¸Šç•¥èƒœä¸€ç­¹ï¼Œè€Œ GPT-2 åœ¨æ•æ‰æç«¯äº‹ä»¶ä¸­çš„åŠ¨æ€å˜åŒ–æ–¹é¢æ›´å…·ä¼˜åŠ¿ã€‚æ—¶é—´ä¸Šä¸‹æ–‡é•¿åº¦åˆ†ææ­ç¤ºï¼ŒLSTM èƒ½å¤Ÿä»¥æ˜¾è‘—çŸ­äº GPT-2 çš„è¾“å…¥çª—å£è¾¾åˆ°ç›¸å½“çš„ç²¾åº¦ï¼Œçªæ˜¾äº†ä¸¤ç§æ¶æ„åœ¨å‡†ç¡®æ€§ä¸æ•ˆç‡ä¹‹é—´çš„æƒè¡¡ã€‚ç‰¹å¾é‡è¦æ€§åˆ†æè¡¨æ˜ï¼ŒRadiation (è¾å°„) æ˜¯ GPP é¢„æµ‹ä¸­æœ€å…³é”®çš„å› å­ï¼Œå…¶æ¬¡æ˜¯ Sentinel-2ã€MODIS é™†é¢æ¸©åº¦åŠ Sentinel-1 æ•°æ®çš„è´¡çŒ®ã€‚æœ¬ç ”ç©¶é€šè¿‡å¯¹æ¯”ä¸åŒæ¶æ„ã€ä¸Šä¸‹æ–‡é•¿åº¦åŠå¤šæ¨¡æ€è¾“å…¥çš„æ•ˆæœï¼Œä¸ºæœªæ¥å¼€å‘ç”¨äºç›‘æµ‹é™†åœ°ç¢³å¾ªç¯åŠ¨æ€çš„æ·±åº¦å­¦ä¹ æ¡†æ¶æä¾›äº†å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11880v1",
      "published_date": "2025-11-14 21:18:01 UTC",
      "updated_date": "2025-11-14 21:18:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:14:08.238177+00:00"
    },
    {
      "arxiv_id": "2511.11857v1",
      "title": "Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection",
      "title_zh": "ä¸‰é˜¶æ®µå™äº‹åˆ†æï¼šæƒ…èŠ‚-æƒ…æ„Ÿåˆ†è§£ã€ç»“æ„å­¦ä¹ ä¸æ¦‚å¿µæ£€æµ‹",
      "authors": [
        "Taimur Khan",
        "Ramoza Ahsan",
        "Mohib Hameed"
      ],
      "abstract": "Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªä¸‰é˜¶æ®µå™äº‹åˆ†ææ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªç„¶è¯­è¨€ç†è§£(Natural Language Understanding)é¢†åŸŸä¸­æ•…äº‹ç†è§£ä¸åˆ†æçš„å¤æ‚æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡åˆ†æç”µå½±å‰§æœ¬çš„æƒ…æ„Ÿå¼§çº¿(Sentiment Arcs)å¹¶ç»“åˆè§’è‰²è¯­å¢ƒè¿›è¡Œæ‰©å±•åˆ†æï¼Œèƒ½å¤Ÿæœ‰æ•ˆæå–å™äº‹ä¸­æ‰€ä¼ è¾¾çš„é«˜å±‚å’Œåº•å±‚æ¦‚å¿µã€‚åœ¨æŠ€æœ¯å®ç°ä¸Šï¼Œç ”ç©¶é‡‡ç”¨äº†åŸºäºè¯å…¸çš„æƒ…æ„Ÿåˆ†ææ–¹æ³•ï¼Œåˆ©ç”¨LabMTsimpleæ¨¡å—ä»¥åŠåŸºäºNRC-VADæ•°æ®é›†ä¸­çš„æ•ˆä»·(Valence)ã€å”¤èµ·åº¦(Arousal)å’Œæ”¯é…åº¦(Dominance)åˆ†æ•°æ„å»ºäº†è‡ªå®šä¹‰è¯å…¸ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åˆ©ç”¨Wardå±‚æ¬¡èšç±»(Ward's hierarchical clustering)æŠ€æœ¯å¯¹ç›¸ä¼¼çš„æƒ…æ„Ÿå›¾è°±è¿›è¡Œèšç±»ï¼Œå®ç°äº†å™äº‹ç»“æ„çš„è‡ªåŠ¨åŒ–å­¦ä¹ ã€‚é’ˆå¯¹ç”µå½±æ•°æ®é›†çš„å®éªŒè¯„ä¼°è¯æ˜ï¼Œè¯¥åˆ†ææ¡†æ¶èƒ½å¤Ÿä¸ºè¯»è€…å’Œæ¶ˆè´¹è€…åœ¨é€‰æ‹©å™äº‹ä½œå“æˆ–æ•…äº‹æ—¶æä¾›å…·æœ‰å‚è€ƒä»·å€¼çš„è¾…åŠ©å»ºè®®ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.11857v1",
      "published_date": "2025-11-14 20:30:18 UTC",
      "updated_date": "2025-11-14 20:30:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:14:02.037733+00:00"
    },
    {
      "arxiv_id": "2511.11847v1",
      "title": "A Multimodal Manufacturing Safety Chatbot: Knowledge Base Design, Benchmark Development, and Evaluation of Multiple RAG Approaches",
      "title_zh": "å¤šæ¨¡æ€åˆ¶é€ ä¸šå®‰å…¨èŠå¤©æœºå™¨äººï¼šçŸ¥è¯†åº“è®¾è®¡ã€åŸºå‡†æ„å»ºåŠå¤šç§ RAG æ–¹æ³•è¯„ä¼°",
      "authors": [
        "Ryan Singh",
        "Austin Hamilton",
        "Amanda White",
        "Michael Wise",
        "Ibrahim Yousif",
        "Arthur Carvalho",
        "Zhe Shan",
        "Reza Abrisham Baf",
        "Mohammad Mayyas",
        "Lora A. Cavuoto",
        "Fadel M. Megahed"
      ],
      "abstract": "Ensuring worker safety remains a critical challenge in modern manufacturing environments. Industry 5.0 reorients the prevailing manufacturing paradigm toward more human-centric operations. Using a design science research methodology, we identify three essential requirements for next-generation safety training systems: high accuracy, low latency, and low cost. We introduce a multimodal chatbot powered by large language models that meets these design requirements. The chatbot uses retrieval-augmented generation to ground its responses in curated regulatory and technical documentation. To evaluate our solution, we developed a domain-specific benchmark of expert-validated question and answer pairs for three representative machines: a Bridgeport manual mill, a Haas TL-1 CNC lathe, and a Universal Robots UR5e collaborative robot. We tested 24 RAG configurations using a full-factorial design and assessed them with automated evaluations of correctness, latency, and cost. Our top 2 configurations were then evaluated by ten industry experts and academic researchers. Our results show that retrieval strategy and model configuration have a significant impact on performance. The top configuration (selected for chatbot deployment) achieved an accuracy of 86.66%, an average latency of 10.04 seconds, and an average cost of $0.005 per query. Overall, our work provides three contributions: an open-source, domain-grounded safety training chatbot; a validated benchmark for evaluating AI-assisted safety instruction; and a systematic methodology for designing and assessing AI-enabled instructional and immersive safety training systems for Industry 5.0 environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å·¥ä¸š5.0(Industry 5.0)ç¯å¢ƒä¸‹çš„äººæœºåä½œå®‰å…¨æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å¤šæ¨¡æ€åˆ¶é€ å®‰å…¨èŠå¤©æœºå™¨äººç³»ç»Ÿã€‚åŸºäºè®¾è®¡ç§‘å­¦ç ”ç©¶æ–¹æ³•ï¼Œè¯¥ç³»ç»Ÿåˆ©ç”¨æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯å°†å¤§è¯­è¨€æ¨¡å‹(LLMs)çš„å›ç­”æ¤æ ¹äºç›‘ç®¡å’ŒæŠ€æœ¯æ–‡æ¡£ä¸­ï¼Œæ—¨åœ¨æ»¡è¶³é«˜å‡†ç¡®ç‡ã€ä½å»¶è¿Ÿå’Œä½æˆæœ¬çš„å·¥ä¸šéœ€æ±‚ã€‚ä¸ºäº†è¯„ä¼°è¯¥æ–¹æ¡ˆï¼Œç ”ç©¶å›¢é˜Ÿå¼€å‘äº†ä¸€ä¸ªé¢†åŸŸç‰¹å®šçš„åŸºå‡†æµ‹è¯•(Benchmark)ï¼Œæ¶µç›–äº†æ‰‹åŠ¨é“£åºŠã€CNCè½¦åºŠå’Œåä½œæœºå™¨äººç­‰å…¸å‹è®¾å¤‡çš„ä¸“å®¶éªŒè¯é—®ç­”å¯¹ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å…¨æå› è®¾è®¡(Full-factorial design)æµ‹è¯•äº†24ç§RAGé…ç½®ï¼Œå¹¶ä»æ­£ç¡®æ€§ã€å»¶è¿Ÿå’Œæˆæœ¬ä¸‰ä¸ªç»´åº¦è¿›è¡Œäº†è‡ªåŠ¨åŒ–è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜æ£€ç´¢ç­–ç•¥å’Œæ¨¡å‹é…ç½®å¯¹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ï¼Œå…¶ä¸­æœ€ä¼˜é…ç½®å®ç°äº†86.66%çš„å‡†ç¡®ç‡ï¼Œä¸”å•æ¬¡æŸ¥è¯¢å¹³å‡æˆæœ¬ä»…ä¸º0.005ç¾å…ƒã€‚è¯¥å·¥ä½œçš„æ ¸å¿ƒè´¡çŒ®åŒ…æ‹¬ä¸€ä¸ªå¼€æºä¸”åŸºäºé¢†åŸŸçŸ¥è¯†çš„å®‰å…¨åŸ¹è®­èŠå¤©æœºå™¨äººã€ä¸€å¥—ç»éªŒè¯çš„åŸºå‡†æµ‹è¯•é›†ï¼Œä»¥åŠä¸€å¥—ä¸ºå·¥ä¸š5.0ç¯å¢ƒè®¾è®¡å’Œè¯„ä¼°AIè¾…åŠ©å®‰å…¨æ•™å­¦ç³»ç»Ÿçš„ç³»ç»Ÿæ€§æ–¹æ³•è®ºã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.IR",
      "comment": "25 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.11847v1",
      "published_date": "2025-11-14 20:10:23 UTC",
      "updated_date": "2025-11-14 20:10:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:14:09.328144+00:00"
    },
    {
      "arxiv_id": "2511.11845v1",
      "title": "Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture",
      "title_zh": "é¢å‘è‡ªé€‚åº”å¯¼èˆªçš„è‡ªä¸»æ°´ä¸‹è®¤çŸ¥ç³»ç»Ÿï¼šä¸€ç§é›†æˆ SLAM çš„è®¤çŸ¥æ¶æ„",
      "authors": [
        "K. A. I. N Jayarathne",
        "R. M. N. M. Rathnayaka",
        "D. P. S. S. Peiris"
      ],
      "abstract": "Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments. This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions. The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning. Unlike conventional SLAM systems, AUCS incorporates semantic understanding, adaptive sensor management, and memory-based learning to differentiate between dynamic and static objects, reducing false loop closures and enhancing long-term map consistency. The proposed architecture demonstrates a complete perception-cognition-action-learning loop, allowing autonomous underwater vehicles to sense, reason, and adapt intelligently. This work lays a foundation for next-generation cognitive submersible systems, improving safety, reliability, and autonomy in deep-sea exploration.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§è‡ªä¸»æ°´ä¸‹è®¤çŸ¥ç³»ç»Ÿ(AUCS)ï¼Œæ—¨åœ¨è§£å†³æ·±æµ·æ¢æµ‹ä¸­å› ç¯å¢ƒå¤æ‚å¯¼è‡´çš„å®šå‘éšœç¢ã€é€šä¿¡ä¸­æ–­åŠå¯¼èˆªå¤±è´¥ç­‰æŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿé€šè¿‡å°†åŒæ­¥å®šä½ä¸åœ°å›¾æ„å»º(SLAM)ä¸åŸºäºSoarçš„è®¤çŸ¥æ¶æ„ç›¸ç»“åˆï¼Œå®ç°äº†åœ¨å¤æ‚æµ·æ´‹æ¡ä»¶ä¸‹çš„è‡ªé€‚åº”å¯¼èˆªã€‚AUCSèåˆäº†æ¥è‡ªSONARã€LiDARã€IMUå’ŒDVLçš„å¤šä¼ æ„Ÿå™¨æ•°æ®ï¼Œå¹¶é›†æˆäº†æ„ŸçŸ¥ã€æ³¨æ„åŠ›ã€è§„åˆ’å’Œå­¦ä¹ ç­‰è®¤çŸ¥æ¨ç†æ¨¡å—ã€‚ä¸ä¼ ç»ŸSLAMç³»ç»Ÿä¸åŒï¼Œè¯¥æ¶æ„å¼•å…¥äº†è¯­ä¹‰ç†è§£ã€è‡ªé€‚åº”ä¼ æ„Ÿå™¨ç®¡ç†å’ŒåŸºäºè®°å¿†çš„å­¦ä¹ æœºåˆ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†åŠ¨æ€å’Œé™æ€ç‰©ä½“ã€‚è¿™ç§è®¾è®¡æ˜¾è‘—å‡å°‘äº†é”™è¯¯çš„å›ç¯æ£€æµ‹(loop closures)ï¼Œå¢å¼ºäº†åœ°å›¾çš„é•¿æœŸä¸€è‡´æ€§ï¼Œå¹¶æ„å»ºäº†å®Œæ•´çš„æ„ŸçŸ¥-è®¤çŸ¥-è¡ŒåŠ¨-å­¦ä¹ é—­ç¯ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥ç³»ç»Ÿæå‡äº†è‡ªä¸»æ°´ä¸‹èˆªè¡Œå™¨åœ¨æ·±æµ·æ¢æµ‹ä¸­çš„å®‰å…¨æ€§ã€å¯é æ€§ä¸è‡ªä¸»åŒ–æ°´å¹³ï¼Œä¸ºä¸‹ä¸€ä»£è®¤çŸ¥å‹æ½œæ°´ç³»ç»Ÿå¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.AR"
      ],
      "primary_category": "cs.RO",
      "comment": "6 pages, 2 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.11845v1",
      "published_date": "2025-11-14 20:07:56 UTC",
      "updated_date": "2025-11-14 20:07:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:14:13.025111+00:00"
    },
    {
      "arxiv_id": "2511.11836v1",
      "title": "Securing Generative AI in Healthcare: A Zero-Trust Architecture Powered by Confidential Computing on Google Cloud",
      "title_zh": "åŒ»ç–—é¢†åŸŸç”Ÿæˆå¼äººå·¥æ™ºèƒ½å®‰å…¨ä¿éšœï¼šåŸºäº Google Cloud æœºå¯†è®¡ç®—çš„é›¶ä¿¡ä»»æ¶æ„",
      "authors": [
        "Adaobi Amanna",
        "Ishana Shinde"
      ],
      "abstract": "The integration of Generative Artificial Intelligence (GenAI) in healthcare is impeded by significant security challenges unaddressed by traditional frameworks, precisely the data-in-use gap where sensitive patient data and proprietary AI models are exposed during active processing. To address this, the paper proposes the Confidential Zero-Trust Framework (CZF), a novel security paradigm that synergistically combines Zero-Trust Architecture for granular access control with the hardware-enforced data isolation of Confidential Computing. We detailed a multi-tiered architectural blueprint for implementing the CZF on Google Cloud and analyzed its efficacy against real-world threats. The CZF provides a defense-in-depth architecture where data remains encrypted while in-use within a hardware-based Trusted Execution Environment (TEE). The framework's use of remote attestation offers cryptographic proof of workload integrity, transforming compliance from a procedural exercise into a verifiable technical fact and enabling secure, multi-party collaborations previously blocked by security and intellectual property concerns. By closing the data-in-use gap and enforcing Zero-Trust principles, the CZF provides a robust and verifiable framework that establishes the necessary foundation of trust to enable the responsible adoption of transformative AI technologies in healthcare.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(GenAI)åœ¨åŒ»ç–—é¢†åŸŸé¢ä¸´çš„æ•°æ®ä½¿ç”¨ä¸­(data-in-use)å®‰å…¨æ¼æ´ï¼Œæå‡ºäº†æœºå¯†é›¶ä¿¡ä»»æ¡†æ¶(Confidential Zero-Trust Framework, CZF)ã€‚è¯¥æ¡†æ¶å°†ç”¨äºç»†ç²’åº¦è®¿é—®æ§åˆ¶çš„é›¶ä¿¡ä»»æ¶æ„(Zero-Trust Architecture)ä¸ç¡¬ä»¶å¼ºåŒ–çš„æœºå¯†è®¡ç®—(Confidential Computing)æ•°æ®éš”ç¦»æŠ€æœ¯ç›¸ç»“åˆï¼Œå¹¶åœ¨Google Cloudä¸Šå®ç°äº†å¤šå±‚æ¶æ„è“å›¾ã€‚CZFé€šè¿‡åœ¨ç¡¬ä»¶å—ä¿¡ä»»æ‰§è¡Œç¯å¢ƒ(Trusted Execution Environment, TEE)ä¸­ä¿æŒæ•°æ®åœ¨å¤„ç†è¿‡ç¨‹ä¸­çš„åŠ å¯†çŠ¶æ€ï¼Œå¹¶åˆ©ç”¨è¿œç¨‹éªŒè¯(remote attestation)æä¾›å·¥ä½œè´Ÿè½½å®Œæ•´æ€§çš„å¯†ç å­¦è¯æ˜ã€‚è¿™ç§æ–¹æ³•å°†åˆè§„æ€§ä»å•çº¯çš„ç¨‹åºæ€§æµç¨‹è½¬å˜ä¸ºå¯éªŒè¯çš„æŠ€æœ¯äº‹å®ï¼Œæœ‰æ•ˆè§£å†³äº†å› å®‰å…¨å’ŒçŸ¥è¯†äº§æƒæ‹…å¿§è€Œå—é˜»çš„å¤šæ–¹åä½œéš¾é¢˜ã€‚é€šè¿‡å¡«è¡¥æ•°æ®ä½¿ç”¨é˜¶æ®µçš„å®‰å…¨ç¼ºå£å¹¶å¼ºåŒ–é›¶ä¿¡ä»»åŸåˆ™ï¼ŒCZFä¸ºåŒ»ç–—æœºæ„è´Ÿè´£ä»»åœ°é‡‡ç”¨å˜é©æ€§AIæŠ€æœ¯æä¾›äº†ç¨³å›ºä¸”å¯éªŒè¯çš„ä¿¡ä»»åŸºç¡€ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "19 Pages, 1 Figure, 1 Table",
      "pdf_url": "https://arxiv.org/pdf/2511.11836v1",
      "published_date": "2025-11-14 19:56:52 UTC",
      "updated_date": "2025-11-14 19:56:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:14:12.525237+00:00"
    },
    {
      "arxiv_id": "2511.11834v1",
      "title": "Volatility in Certainty (VC): A Metric for Detecting Adversarial Perturbations During Inference in Neural Network Classifiers",
      "title_zh": "ç¡®å®šæ€§æ³¢åŠ¨ (VC)ï¼šä¸€ç§ç¥ç»ç½‘ç»œåˆ†ç±»å™¨æ¨ç†é˜¶æ®µçš„å¯¹æŠ—æ‰°åŠ¨æ£€æµ‹æŒ‡æ ‡",
      "authors": [
        "Vahid Hemmati",
        "Ahmad Mohammadi",
        "Abdul-Rauf Nuhu",
        "Reza Ahmari",
        "Parham Kebria",
        "Abdollah Homaifar"
      ],
      "abstract": "Adversarial robustness remains a critical challenge in deploying neural network classifiers, particularly in real-time systems where ground-truth labels are unavailable during inference. This paper investigates \\textit{Volatility in Certainty} (VC), a recently proposed, label-free metric that quantifies irregularities in model confidence by measuring the dispersion of sorted softmax outputs. Specifically, VC is defined as the average squared log-ratio of adjacent certainty values, capturing local fluctuations in model output smoothness. We evaluate VC as a proxy for classification accuracy and as an indicator of adversarial drift. Experiments are conducted on artificial neural networks (ANNs) and convolutional neural networks (CNNs) trained on MNIST, as well as a regularized VGG-like model trained on CIFAR-10. Adversarial examples are generated using the Fast Gradient Sign Method (FGSM) across varying perturbation magnitudes. In addition, mixed test sets are created by gradually introducing adversarial contamination to assess VC's sensitivity under incremental distribution shifts. Our results reveal a strong negative correlation between classification accuracy and log(VC) (correlation rho < -0.90 in most cases), suggesting that VC effectively reflects performance degradation without requiring labeled data. These findings position VC as a scalable, architecture-agnostic, and real-time performance metric suitable for early-warning systems in safety-critical applications.",
      "tldr_zh": "é’ˆå¯¹ç¥ç»ç½‘ç»œåˆ†ç±»å™¨åœ¨æ¨ç†é˜¶æ®µç¼ºä¹çœŸå®æ ‡ç­¾ä¸”æ˜“å—å¯¹æŠ—æ‰°åŠ¨(Adversarial Perturbations)å½±å“çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶æ¢è®¨äº†ä¸€ç§åä¸ºVolatility in Certainty (VC)çš„æ— æ ‡ç­¾åº¦é‡æŒ‡æ ‡ã€‚VCé€šè¿‡æµ‹é‡æ’åºåSoftmaxè¾“å‡ºçš„ç¦»æ•£ç¨‹åº¦æ¥é‡åŒ–æ¨¡å‹ç½®ä¿¡åº¦çš„ä¸è§„åˆ™æ€§ï¼Œå…·ä½“å®šä¹‰ä¸ºç›¸é‚»ç¡®å®šæ€§æ•°å€¼ä¹‹æ¯”çš„å¯¹æ•°å¹³æ–¹å‡å€¼ï¼Œæ—¨åœ¨æ•æ‰æ¨¡å‹è¾“å‡ºå¹³æ»‘åº¦çš„å±€éƒ¨æ³¢åŠ¨ã€‚ç ”ç©¶äººå‘˜åœ¨MNISTæ•°æ®é›†çš„ANNå’ŒCNNæ¨¡å‹ï¼Œä»¥åŠCIFAR-10æ•°æ®é›†çš„VGGç±»æ¨¡å‹ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶åˆ©ç”¨Fast Gradient Sign Method (FGSM)ç”Ÿæˆäº†ä¸åŒå¼ºåº¦çš„å¯¹æŠ—æ ·æœ¬ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œåˆ†ç±»å‡†ç¡®ç‡ä¸log(VC)ä¹‹é—´å­˜åœ¨æå¼ºçš„è´Ÿç›¸å…³æ€§ï¼ˆç›¸å…³ç³»æ•°rho < -0.90ï¼‰ï¼Œè¡¨æ˜VCèƒ½åœ¨æ— éœ€æ ‡æ³¨æ•°æ®çš„æƒ…å†µä¸‹æœ‰æ•ˆåæ˜ æ¨¡å‹æ€§èƒ½çš„ä¸‹é™ã€‚è¯¥ç ”ç©¶è¯å®äº†VCä½œä¸ºä¸€ç§æ¶æ„æ— å…³ä¸”å®æ—¶çš„æ€§èƒ½åº¦é‡å·¥å…·ï¼Œèƒ½å¤Ÿä¸ºå®‰å…¨å…³é”®å‹åº”ç”¨ä¸­çš„å¯¹æŠ—æ£€æµ‹æä¾›é«˜æ•ˆçš„æ—©æœŸé¢„è­¦ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11834v1",
      "published_date": "2025-11-14 19:51:04 UTC",
      "updated_date": "2025-11-14 19:51:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:14:16.634859+00:00"
    },
    {
      "arxiv_id": "2511.11831v1",
      "title": "TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models",
      "title_zh": "TopoPerceptionï¼šå¤§è§†è§‰è¯­è¨€æ¨¡å‹å…¨å±€è§†è§‰æ„ŸçŸ¥èƒ½åŠ›çš„æ— æ·å¾„è¯„ä¼°",
      "authors": [
        "Wenhao Zhou",
        "Hao Zheng",
        "Rong Zhao"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† TopoPerceptionï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ©ç”¨æ‹“æ‰‘å±æ€§ (topological properties) ä¸¥æ ¼è¯„ä¼°å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (LVLMs) å…¨å±€è§†è§‰æ„ŸçŸ¥èƒ½åŠ›çš„åŸºå‡†æµ‹è¯•ã€‚ç”±äºæ‹“æ‰‘ç»“æ„ä¾èµ–äºå›¾åƒçš„å…¨å±€ç»“æ„ä¸”å¯¹å±€éƒ¨ç‰¹å¾å…·æœ‰ä¸å˜æ€§ï¼ŒTopoPerception å®ç°äº†æ— æ·å¾„ (shortcut-free) çš„è¯„ä¼°ï¼Œæœ‰æ•ˆé¿å…äº†ä¼ ç»Ÿè¯­ä¹‰åŸºå‡†ä¸­å› å±€éƒ¨ç‰¹å¾å¯¼è‡´çš„æ¨¡å‹æ„ŸçŸ¥èƒ½åŠ›é«˜ä¼°é—®é¢˜ã€‚é€šè¿‡å¯¹å½“å‰æœ€å…ˆè¿›æ¨¡å‹çš„è¯„ä¼°å‘ç°ï¼Œå³ä¾¿åœ¨æœ€ç²—ç•¥çš„æ„ŸçŸ¥ç²’åº¦ä¸‹ï¼Œæ‰€æœ‰æ¨¡å‹çš„è¡¨ç°å‡ä¸ä¼˜äºéšæœºçŒœæµ‹ï¼Œè¡¨æ˜æ¨¡å‹åœ¨æ„ŸçŸ¥å…¨å±€è§†è§‰ç‰¹å¾æ–¹é¢å­˜åœ¨ä¸¥é‡ç¼ºé™·ã€‚ç ”ç©¶è¿›ä¸€æ­¥å‘ç°ï¼Œæ¨ç†èƒ½åŠ›æ›´å¼ºçš„æ¨¡å‹å…¶å‡†ç¡®ç‡åè€Œæ›´ä½ï¼Œè¿™æš—ç¤ºå•çº¯æ‰©å±•æ¨¡å‹è§„æ¨¡ä¸è¶³ä»¥è§£å†³è¯¥æ„ŸçŸ¥ç“¶é¢ˆï¼Œç”šè‡³å¯èƒ½åŠ å‰§é—®é¢˜ã€‚TopoPerception æ­ç¤ºäº†å½“å‰ LVLMs çš„æ ¸å¿ƒç¼ºé™·ï¼Œå¹¶ä¸ºå¼€å‘æ–°çš„è®­ç»ƒèŒƒå¼æˆ–æ¨¡å‹æ¶æ„ä»¥æå‡å…¨å±€è§†è§‰æ„ŸçŸ¥æä¾›äº†æŒ‡å¼•ã€‚",
      "categories": [
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11831v1",
      "published_date": "2025-11-14 19:45:56 UTC",
      "updated_date": "2025-11-14 19:45:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:14:26.136617+00:00"
    },
    {
      "arxiv_id": "2511.11829v1",
      "title": "Towards Autoformalization of LLM-generated Outputs for Requirement Verification",
      "title_zh": "è¿ˆå‘é¢å‘éœ€æ±‚éªŒè¯çš„å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆè¾“å‡ºè‡ªåŠ¨å½¢å¼åŒ–",
      "authors": [
        "Mihir Gupte",
        "Ramesh S"
      ],
      "abstract": "Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†é¢å‘éœ€æ±‚éªŒè¯çš„ LLM ç”Ÿæˆè¾“å‡ºçš„ Autoformalizationï¼ˆè‡ªåŠ¨å½¢å¼åŒ–ï¼‰ï¼Œæ—¨åœ¨è§£å†³å½“å‰ç¼ºä¹æ­£å¼æ–¹æ³•éªŒè¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä»è‡ªç„¶è¯­è¨€éœ€æ±‚ç”Ÿæˆçš„ç»“æ„åŒ–è¾“å‡ºï¼ˆå¦‚ Gherkin Scenariosï¼‰å‡†ç¡®æ€§çš„é—®é¢˜ã€‚ç ”ç©¶æå‡ºå¹¶æµ‹è¯•äº†ä¸€ä¸ªåŸºäº LLM çš„ç®€å• Autoformalizerï¼Œé€šè¿‡å°†éæ­£å¼å£°æ˜è½¬åŒ–ä¸ºå½¢å¼é€»è¾‘ï¼Œå¯¹æ¯”éªŒè¯ç”Ÿæˆå†…å®¹ä¸åŸå§‹éœ€æ±‚çš„ä¸€è‡´æ€§ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥å·¥å…·èƒ½å¤ŸæˆåŠŸè¯†åˆ«è¡¨è¿°ä¸åŒä½†é€»è¾‘ç­‰ä»·çš„è‡ªç„¶è¯­è¨€éœ€æ±‚ï¼Œå¹¶èƒ½æœ‰æ•ˆå‘ç° LLM ç”Ÿæˆè¾“å‡ºä¸éœ€æ±‚ä¹‹é—´çš„é€»è¾‘ä¸ä¸€è‡´ã€‚è¿™äº›åˆæ­¥å‘ç°è¯æ˜äº† Autoformalization åœ¨ç¡®ä¿ LLM è¾“å‡ºçš„å¿ å®åº¦ï¼ˆFidelityï¼‰å’Œé€»è¾‘ä¸€è‡´æ€§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚è¯¥å·¥ä½œä¸ºæœªæ¥åˆ©ç”¨è‡ªåŠ¨å½¢å¼åŒ–æŠ€æœ¯è¿›è¡Œå¤æ‚ç³»ç»Ÿçš„æ­£å¼éªŒè¯ï¼ˆFormal Verificationï¼‰å¥ å®šäº†é‡è¦åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.FL",
        "cs.LO"
      ],
      "primary_category": "cs.CL",
      "comment": "To be submitted for publication",
      "pdf_url": "https://arxiv.org/pdf/2511.11829v1",
      "published_date": "2025-11-14 19:45:17 UTC",
      "updated_date": "2025-11-14 19:45:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:14:27.333930+00:00"
    },
    {
      "arxiv_id": "2511.11828v1",
      "title": "Conformal Constrained Policy Optimization for Cost-Effective LLM Agents",
      "title_zh": "é¢å‘é«˜æ€§ä»·æ¯”å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“çš„ç¬¦åˆæ€§çº¦æŸç­–ç•¥ä¼˜åŒ–",
      "authors": [
        "Wenwen Si",
        "Sooyong Jang",
        "Insup Lee",
        "Osbert Bastani"
      ],
      "abstract": "While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)è®¡ç®—å’ŒAPIæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§ç»“åˆå¤šä¸ªä¸åŒæˆæœ¬ä¸å‡†ç¡®ç‡æƒè¡¡æ¨¡å‹çš„æ™ºèƒ½ä½“ç­–ç•¥ã€‚ç ”ç©¶è€…é€šè¿‡ç¼–æ’æ¨¡å‹(orchestration model)æŒ‰åºè¿è¡Œæ¨¡å‹å’Œå·¥å…·ï¼Œæ—¨åœ¨æ»¡è¶³ç”¨æˆ·å¯é æ€§éœ€æ±‚çš„åŒæ—¶å®ç°æˆæœ¬æœ€å°åŒ–ï¼Œå¹¶åˆ©ç”¨Conformal Predictionæä¾›å½¢å¼åŒ–çš„å¯é æ€§ä¿è¯ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡æå‡ºäº†Conformal Constrained Policy Optimization (CCPO)ï¼Œè¿™æ˜¯ä¸€ç§é›†æˆå—é™ç­–ç•¥ä¼˜åŒ–ã€ç¦»ç­–å¼ºåŒ–å­¦ä¹ (off-policy reinforcement learning)å’Œåœ¨çº¿Conformal Predictionçš„è®­ç»ƒèŒƒå¼ã€‚CCPOé€šè¿‡å…±åŒä¼˜åŒ–æˆæœ¬æ„ŸçŸ¥ç­–ç•¥(score function)å’Œè‡ªé€‚åº”é˜ˆå€¼ï¼Œåœ¨ä¸¤é¡¹å¤šè·³é—®ç­”åŸºå‡†æµ‹è¯•ä¸­æ¯”å…¶ä»–æˆæœ¬æ„ŸçŸ¥åŸºå‡†æ–¹æ³•é™ä½äº†é«˜è¾¾30%çš„æˆæœ¬ï¼Œä¸”æœªæŸå®³å¯é æ€§ã€‚è¯¥æˆæœä¸ºéƒ¨ç½²å…¼å…·é«˜æ€§ä»·æ¯”ä¸å¯é æ€§çš„LLMæ™ºèƒ½ä½“æä¾›äº†ä¸€ä¸ªåŸåˆ™æ€§ä¸”å®ç”¨çš„æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11828v1",
      "published_date": "2025-11-14 19:39:28 UTC",
      "updated_date": "2025-11-14 19:39:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:14:54.727049+00:00"
    },
    {
      "arxiv_id": "2511.11825v1",
      "title": "Real-Time Speech Enhancement via a Hybrid ViT: A Dual-Input Acoustic-Image Feature Fusion",
      "title_zh": "åŸºäºæ··åˆ ViT çš„å®æ—¶è¯­éŸ³å¢å¼ºï¼šåŒè¾“å…¥å£°å­¦-å›¾åƒç‰¹å¾èåˆ",
      "authors": [
        "Behnaz Bahmei",
        "Siamak Arzanpour",
        "Elina Birmingham"
      ],
      "abstract": "Speech quality and intelligibility are significantly degraded in noisy environments. This paper presents a novel transformer-based learning framework to address the single-channel noise suppression problem for real-time applications. Although existing deep learning networks have shown remarkable improvements in handling stationary noise, their performance often diminishes in real-world environments characterized by non-stationary noise (e.g., dog barking, baby crying). The proposed dual-input acoustic-image feature fusion using a hybrid ViT framework effectively models both temporal and spectral dependencies in noisy signals. Designed for real-world audio environments, the proposed framework is computationally lightweight and suitable for implementation on embedded devices. To evaluate its effectiveness, four standard and commonly used quality measurements, namely PESQ, STOI, Seg SNR, and LLR, are utilized. Experimental results obtained using the Librispeech dataset as the clean speech source and the UrbanSound8K and Google Audioset datasets as the noise sources, demonstrate that the proposed method significantly improves noise reduction, speech intelligibility, and perceptual quality compared to the noisy input signal, achieving performance close to the clean reference.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº Transformer çš„æ–°å‹å­¦ä¹ æ¡†æ¶ï¼Œä¸“é—¨ç”¨äºè§£å†³å®æ—¶åº”ç”¨ä¸­çš„å•é€šé“å™ªå£°æŠ‘åˆ¶ (noise suppression) é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å…·æœ‰æŒ‘æˆ˜æ€§çš„éå¹³ç¨³å™ªå£° (non-stationary noise) æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚è¯¥æ¡†æ¶é‡‡ç”¨äº†æ··åˆ ViT (Hybrid ViT) ç»“æ„ï¼Œåˆ©ç”¨åŒè¾“å…¥å£°å­¦-å›¾åƒç‰¹å¾èåˆ (dual-input acoustic-image feature fusion) æŠ€æœ¯æ¥åŒæ—¶å»ºæ¨¡å™ªå£°ä¿¡å·çš„æ—¶é—´ä¸é¢‘è°±ä¾èµ–å…³ç³»ã€‚æ¨¡å‹è®¾è®¡éµå¾ªè½»é‡åŒ–åŸåˆ™ï¼Œä½¿å…¶èƒ½å¤Ÿæ»¡è¶³åœ¨åµŒå…¥å¼è®¾å¤‡ (embedded devices) ä¸Šè¿›è¡Œå®æ—¶éƒ¨ç½²çš„éœ€æ±‚ã€‚ç ”ç©¶å›¢é˜Ÿåˆ©ç”¨ Librispeechã€UrbanSound8K å’Œ Google Audioset ç­‰æ•°æ®é›†ï¼Œé€šè¿‡ PESQã€STOIã€Seg SNR å’Œ LLR å››é¡¹æ ‡å‡†æŒ‡æ ‡å¯¹æ¨¡å‹è¿›è¡Œäº†å…¨é¢è¯„ä¼°ã€‚å®éªŒç»“æœè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨é™å™ªæ•ˆæœã€è¯­éŸ³å¯æ‡‚åº¦åŠæ„ŸçŸ¥è´¨é‡ä¸Šè¾ƒå¸¦å™ªè¾“å…¥æœ‰æ˜¾è‘—æå‡ï¼Œå…¶æ€§èƒ½è¡¨ç°å·²æ¥è¿‘çº¯å‡€çš„å‚è€ƒä¿¡å· (clean reference)ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11825v1",
      "published_date": "2025-11-14 19:27:42 UTC",
      "updated_date": "2025-11-14 19:27:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:14:44.128913+00:00"
    },
    {
      "arxiv_id": "2511.11821v1",
      "title": "Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis",
      "title_zh": "é¢å‘æ°´ç”µç›‘ç®¡ä¿¡æ¯æŠ½å–çš„å¼€æ”¾æƒé‡å¤§è¯­è¨€æ¨¡å‹è§„æ¨¡åŒ–ï¼šä¸€é¡¹ç³»ç»Ÿæ€§åˆ†æ",
      "authors": [
        "Hong-Jun Yoon",
        "Faisal Ashraf",
        "Thomas A. Ruggles",
        "Debjani Singh"
      ],
      "abstract": "Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.\n  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\\% F1 through appropriate validation, while smaller models plateau at 51\\%. Large-scale models approach 77\\% F1 but require enterprise infrastructure.\n  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.\n  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶å¯¹ä¸ƒç§è§„æ¨¡åœ¨0.6Bè‡³70Bå‚æ•°ä¹‹é—´çš„å¼€æºæƒé‡(Open-Weight)å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ï¼Œæ—¨åœ¨åˆ†æå…¶åœ¨æ°´ç”µç›‘ç®¡æ–‡ä»¶ä¿¡æ¯æå–ä»»åŠ¡ä¸­çš„æ€§èƒ½ä¸è®¡ç®—èµ„æºæƒè¡¡ã€‚ç ”ç©¶å‘ç°14Bå‚æ•°æ˜¯ä¸€ä¸ªå…³é”®é˜ˆå€¼ï¼Œæ¨¡å‹è§„æ¨¡è¶…è¿‡æ­¤ç‚¹åï¼ŒéªŒè¯æ–¹æ³•æ‰ä»æ— æ•ˆï¼ˆF1 < 0.15ï¼‰è½¬å˜ä¸ºå¯è¡Œï¼ˆF1 = 0.64ï¼‰ã€‚åœ¨é€‚å½“çš„éªŒè¯æœºåˆ¶ä¸‹ï¼Œé€‚åˆä¸ªäººæ¶ˆè´¹è€…éƒ¨ç½²çš„æ¨¡å‹å¯è¾¾åˆ°64%çš„F1åˆ†æ•°ï¼Œè€Œæ›´å¤§è§„æ¨¡çš„æ¨¡å‹è™½ç„¶èƒ½è¾¾åˆ°77%çš„F1åˆ†æ•°ï¼Œä½†éœ€è¦ä¼ä¸šçº§åŸºç¡€è®¾æ–½æ”¯æŒã€‚ç ”ç©¶è¿˜è¯†åˆ«äº†ç³»ç»Ÿæ€§çš„å¹»è§‰(Hallucination)æ¨¡å¼ï¼Œå‘ç°åœ¨è¾ƒå°æ¨¡å‹ä¸­ï¼Œå®Œç¾çš„å¬å›ç‡(Recall)å¾€å¾€é¢„ç¤ºç€æå–å¤±è´¥è€ŒéæˆåŠŸã€‚è¯¥é¡¹å·¥ä½œå»ºç«‹äº†é¦–ä¸ªé’ˆå¯¹ç›‘ç®¡è¯­å¢ƒä¸‹å¼€æºæ¨¡å‹ä¿¡æ¯æå–çš„èµ„æº-æ€§èƒ½æ˜ å°„å…³ç³»ï¼Œä¸ºæ°´ç”µåˆè§„é¢†åŸŸçš„æ¨¡å‹é€‰æ‹©æä¾›äº†å®è¯æŒ‡å¯¼ï¼Œå¹¶æ·±åŒ–äº†å¯¹å‚æ•°è§„æ¨¡æ•ˆåº”(Scaling effects)çš„ç†è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "18 pages, zero figures, Preprint submitted to Environmental Modeling and Software",
      "pdf_url": "https://arxiv.org/pdf/2511.11821v1",
      "published_date": "2025-11-14 19:23:25 UTC",
      "updated_date": "2025-11-14 19:23:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:14:44.928715+00:00"
    },
    {
      "arxiv_id": "2511.11816v1",
      "title": "Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy",
      "title_zh": "LLMs åœ¨è‡ªç„¶è¯­è¨€åˆ°ä¸€é˜¶é€»è¾‘ï¼ˆNL-FOLï¼‰ç¿»è¯‘ä¸­çœŸçš„è¡¨ç°ä¸ä½³å—ï¼Ÿé€šè¿‡ä¸€ç§æ–°é¢–çš„åŸºå‡†æµ‹è¯•ç­–ç•¥æ­ç¤ºå…¶çœŸå®èƒ½åŠ›",
      "authors": [
        "Andrea Brunello",
        "Luca Geatti",
        "Michele Mignani",
        "Angelo Montanari",
        "Nicola Saccomanno"
      ],
      "abstract": "Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è‡ªç„¶è¯­è¨€åˆ°ä¸€é˜¶é€»è¾‘(NL-FOL translation)è½¬æ¢ä»»åŠ¡ä¸­çš„çœŸå®è¡¨ç°ï¼Œé’ˆå¯¹ç°æœ‰æ–‡çŒ®ä¸­å…³äºæ¨¡å‹èƒ½åŠ›çš„çŸ›ç›¾ç»“è®ºè¿›è¡Œäº†æ·±å…¥åˆ†æã€‚ä½œè€…é¦–å…ˆå¯¹ç°æœ‰çš„æ•°æ®é›†å’Œè¯„ä¼°åè®®è¿›è¡Œäº†æ‰¹åˆ¤æ€§å®¡æŸ¥ï¼Œæ­ç¤ºäº†å¯èƒ½å¯¼è‡´LLMså®é™…èƒ½åŠ›è¢«è¯¯è§£çš„å…³é”®å±€é™æ€§ã€‚ä¸ºå…‹æœè¿™äº›ç¼ºé™·ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§å…¨æ–°çš„è¯„ä¼°åè®®ï¼Œæ—¨åœ¨åŒºåˆ†çœŸæ­£çš„è¯­ä¹‰çº§é€»è¾‘ç†è§£ä¸è¡¨é¢çš„æ¨¡å¼è¯†åˆ«ã€è®°å¿†æˆ–æ•°æ®é›†æ±¡æŸ“ã€‚é€šè¿‡è¯¥åè®®çš„æµ‹è¯•å‘ç°ï¼Œæœ€å…ˆè¿›çš„å¯¹è¯å¯¼å‘å‹LLMsåœ¨NL-FOL translationä¸Šå±•ç°äº†å‡ºè‰²çš„æŠ€èƒ½å’Œå¯¹é€»è¾‘çš„çœŸå®ç†è§£ï¼Œè€Œä»¥åµŒå…¥ä¸ºä¸­å¿ƒ(embedding-centric)çš„æ¨¡å‹è¡¨ç°åˆ™æ˜¾è‘—è¾ƒå·®ã€‚è¯¥ç ”ç©¶é€šè¿‡ç§‘å­¦çš„åŸºå‡†æµ‹è¯•ç­–ç•¥é‡æ–°è¯„ä¼°äº†LLMsçš„é€»è¾‘è½¬åŒ–èƒ½åŠ›ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚é€»è¾‘è¡¨è¾¾æ–¹é¢çš„å¼ºå¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Full version of the paper accepted for publication at The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)",
      "pdf_url": "https://arxiv.org/pdf/2511.11816v1",
      "published_date": "2025-11-14 19:11:41 UTC",
      "updated_date": "2025-11-14 19:11:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:14:52.535388+00:00"
    },
    {
      "arxiv_id": "2511.11810v1",
      "title": "On the Notion that Language Models Reason",
      "title_zh": "è®ºè¯­è¨€æ¨¡å‹å…·å¤‡æ¨ç†èƒ½åŠ›è¿™ä¸€è§‚ç‚¹",
      "authors": [
        "Bertram HÃ¸jer"
      ],
      "abstract": "Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \\textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are \"statistical pattern matchers\"\" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è¯­è¨€æ¨¡å‹ï¼ˆLanguage Models, LMsï¼‰æ˜¯å¦å…·å¤‡æ¨ç†èƒ½åŠ›è¿™ä¸€æ ¸å¿ƒå‘½é¢˜ï¼Œå¹¶æŒ‡å‡ºå½“å‰è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸå¯¹æ¨ç†çš„å®šä¹‰ä¸ LMs çš„è®­ç»ƒåŠä¿¡æ¯å¤„ç†æ–¹å¼å¹¶ä¸ä¸€è‡´ã€‚ä½œè€…æå‡ºå°†åŸºäº Transformer çš„è¯­è¨€æ¨¡å‹è§†ä¸ºå®ç°äº†ä¸€ä¸ªéšå¼çš„æœ‰é™é˜¶é©¬å°”å¯å¤«æ ¸ï¼ˆimplicit finite-order Markov kernelï¼‰ï¼Œè¯¥æ ¸è´Ÿè´£å°†ä¸Šä¸‹æ–‡æ˜ å°„åˆ°æ¡ä»¶æ ‡è®°åˆ†å¸ƒï¼ˆconditional token distributionsï¼‰ã€‚è®ºæ–‡è®¤ä¸ºï¼Œæ‰€è°“çš„æ¨ç†ç±»è¾“å‡ºå®é™…ä¸Šå¯¹åº”äºå­¦ä¹ æ ¸ä¸­çš„ç»Ÿè®¡è§„å¾‹å’Œè¿‘ä¼¼ç»Ÿè®¡ä¸å˜æ€§ï¼Œè€Œéæ˜¾å¼é€»è¾‘æœºåˆ¶ï¼ˆexplicit logical mechanismsï¼‰çš„å®ç°ã€‚è¿™ä¸€è§†è§’æ”¯æŒäº† LMs æœ¬è´¨ä¸Šæ˜¯â€œç»Ÿè®¡æ¨¡å¼åŒ¹é…å™¨â€ï¼ˆstatistical pattern matchersï¼‰è€ŒéçœŸå®æ¨ç†è€…çš„è®ºç‚¹ï¼Œå¹¶è§£é‡Šäº†ä¸ºä½• LMs èƒ½äº§ç”Ÿçœ‹ä¼¼æ¨ç†çš„è¾“å‡ºå´æ— æ³•ä¿è¯é€»è¾‘ä¸€è‡´æ€§ã€‚è¯¥ç ”ç©¶å¼ºè°ƒè¿™ç§åŒºåˆ†å¯¹äºè¯„ä¼° LMs çš„è®¤çŸ¥ä¸ç¡®å®šæ€§ï¼ˆepistemic uncertaintyï¼‰è‡³å…³é‡è¦ï¼Œå¹¶å‘¼åå­¦æœ¯ç•Œé‡æ–°å®¡è§†å¦‚ä½•æè¿° NLP ç³»ç»Ÿä¸­çš„è®¡ç®—è¿‡ç¨‹ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at the 1st Workshop on Epistemic Intelligence in Machine Learning, EurIPS 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.11810v1",
      "published_date": "2025-11-14 19:04:24 UTC",
      "updated_date": "2025-11-14 19:04:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:15:48.531752+00:00"
    },
    {
      "arxiv_id": "2511.11569v2",
      "title": "Private Frequency Estimation Via Residue Number Systems",
      "title_zh": "åŸºäºä½™æ•°ç³»ç»Ÿçš„éšç§é¢‘ç‡ä¼°è®¡",
      "authors": [
        "HÃ©ber H. Arcolezi"
      ],
      "abstract": "We present \\textsf{ModularSubsetSelection} (MSS), a new algorithm for locally differentially private (LDP) frequency estimation. Given a universe of size $k$ and $n$ users, our $\\varepsilon$-LDP mechanism encodes each input via a Residue Number System (RNS) over $\\ell$ pairwise-coprime moduli $m_0, \\ldots, m_{\\ell-1}$, and reports a randomly chosen index $j \\in [\\ell]$ along with the perturbed residue using the statistically optimal \\textsf{SubsetSelection} (SS) (Wang et al. 2016). This design reduces the user communication cost from $Î˜\\bigl(Ï‰\\log_2(k/Ï‰)\\bigr)$ bits required by standard SS (with $Ï‰\\approx k/(e^\\varepsilon+1)$) down to $\\lceil \\log_2 \\ell \\rceil + \\lceil \\log_2 m_j \\rceil$ bits, where $m_j < k$. Server-side decoding runs in $Î˜(n + r k \\ell)$ time, where $r$ is the number of LSMR (Fong and Saunders 2011) iterations. In practice, with well-conditioned moduli (\\textit{i.e.}, constant $r$ and $\\ell = Î˜(\\log k)$), this becomes $Î˜(n + k \\log k)$. We prove that MSS achieves worst-case MSE within a constant factor of state-of-the-art protocols such as SS and \\textsf{ProjectiveGeometryResponse} (PGR) (Feldman et al. 2022) while avoiding the algebraic prerequisites and dynamic-programming decoder required by PGR. Empirically, MSS matches the estimation accuracy of SS, PGR, and \\textsf{RAPPOR} (Erlingsson, Pihur, and Korolova 2014) across realistic $(k, \\varepsilon)$ settings, while offering faster decoding than PGR and shorter user messages than SS. Lastly, by sampling from multiple moduli and reporting only a single perturbed residue, MSS achieves the lowest reconstruction-attack success rate among all evaluated LDP protocols.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ModularSubsetSelection (MSS)ï¼Œä¸€ç§ç”¨äºæœ¬åœ°å·®åˆ†éšç§(Locally Differentially Private, LDP)é¢‘ç‡ä¼°è®¡çš„æ–°ç®—æ³•ã€‚è¯¥ç®—æ³•åˆ©ç”¨ä½™æ•°ç³»ç»Ÿ(Residue Number System, RNS)é€šè¿‡å¤šä¸ªäº’è´¨æ¨¡æ•°å¯¹è¾“å…¥è¿›è¡Œç¼–ç ï¼Œå¹¶ç»“åˆç»Ÿè®¡æœ€ä¼˜çš„SubsetSelection (SS)æœºåˆ¶æŠ¥å‘Šéšæœºé€‰æ‹©çš„æ‰°åŠ¨ä½™æ•°ã€‚è¿™ç§è®¾è®¡å°†ç”¨æˆ·é€šä¿¡æˆæœ¬ä»æ ‡å‡†SSçš„å¤æ‚åº¦æ˜¾è‘—é™ä½ï¼ŒåŒæ—¶åœ¨æœåŠ¡ç«¯å®ç°äº†é«˜æ•ˆçš„è§£ç æ•ˆç‡ï¼Œå®é™…è¿è¡Œæ—¶é—´å¯è¾¾$Î˜(n + k \\log k)$ã€‚ç†è®ºè¯æ˜MSSåœ¨æœ€åæƒ…å†µä¸‹çš„å‡æ–¹è¯¯å·®(MSE)ä¸ProjectiveGeometryResponse (PGR)ç­‰å°–ç«¯åè®®ç›¸å½“ï¼Œä½†æœ‰æ•ˆé¿å¼€äº†PGRå¤æ‚çš„ä»£æ•°é¢„å¤„ç†å’ŒåŠ¨æ€è§„åˆ’è§£ç å™¨ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMSSåœ¨ä¿æŒä¸SSã€PGRå’ŒRAPPORåŒç­‰ä¼°è®¡ç²¾åº¦çš„åŒæ—¶ï¼Œæä¾›äº†æ›´å¿«çš„è§£ç é€Ÿåº¦å’Œæ›´çŸ­çš„ç”¨æˆ·æ¶ˆæ¯é•¿åº¦ã€‚æ­¤å¤–ï¼Œç”±äºé‡‡ç”¨äº†å¤šæ¨¡æ•°é‡‡æ ·æœºåˆ¶ï¼ŒMSSåœ¨æ‰€æœ‰è¯„ä¼°çš„LDPåè®®ä¸­è¡¨ç°å‡ºæœ€ä½çš„é‡æ„æ”»å‡»æˆåŠŸç‡ï¼Œä¸ºéšç§ä¿æŠ¤æ•°æ®ç»Ÿè®¡æä¾›äº†é«˜æ•ˆä¸”å®‰å…¨çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.11569v2",
      "published_date": "2025-11-14 18:58:41 UTC",
      "updated_date": "2025-11-17 10:42:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:14:58.739143+00:00"
    },
    {
      "arxiv_id": "2511.11560v2",
      "title": "A Unified Convergence Analysis for Semi-Decentralized Learning: Sampled-to-Sampled vs. Sampled-to-All Communication",
      "title_zh": "åŠå»ä¸­å¿ƒåŒ–å­¦ä¹ ä¸­é‡‡æ ·å¯¹é‡‡æ ·ä¸é‡‡æ ·å¯¹å…¨ä½“é€šä¿¡çš„ç»Ÿä¸€æ”¶æ•›æ€§åˆ†æ",
      "authors": [
        "Angelo Rodio",
        "Giovanni Neglia",
        "Zheng Chen",
        "Erik G. Larsson"
      ],
      "abstract": "In semi-decentralized federated learning, devices primarily rely on device-to-device communication but occasionally interact with a central server. Periodically, a sampled subset of devices uploads their local models to the server, which computes an aggregate model. The server can then either (i) share this aggregate model only with the sampled clients (sampled-to-sampled, S2S) or (ii) broadcast it to all clients (sampled-to-all, S2A). Despite their practical significance, a rigorous theoretical and empirical comparison of these two strategies remains absent. We address this gap by analyzing S2S and S2A within a unified convergence framework that accounts for key system parameters: sampling rate, server aggregation frequency, and network connectivity. Our results, both analytical and experimental, reveal distinct regimes where one strategy outperforms the other, depending primarily on the degree of data heterogeneity across devices. These insights lead to concrete design guidelines for practical semi-decentralized FL deployments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŠå»ä¸­å¿ƒåŒ–è”é‚¦å­¦ä¹  (Semi-Decentralized Federated Learning) ä¸­ï¼Œè®¾å¤‡é—´é€šä¿¡ä¸æœåŠ¡å™¨å¶å°”äº¤äº’çš„ä¸¤ç§ä¸»è¦ç­–ç•¥â€”â€”é‡‡æ ·åˆ°é‡‡æ · (Sampled-to-Sampled, S2S) å’Œé‡‡æ ·åˆ°å…¨éƒ¨ (Sampled-to-All, S2A) ç¼ºä¹ä¸¥æ ¼ç†è®ºä¸å®è¯å¯¹æ¯”çš„é—®é¢˜è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚ä½œè€…æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„æ”¶æ•›åˆ†ææ¡†æ¶ï¼Œå°†é‡‡æ ·ç‡ã€æœåŠ¡å™¨èšåˆé¢‘ç‡ä»¥åŠç½‘ç»œè¿é€šæ€§ç­‰å…³é”®ç³»ç»Ÿå‚æ•°çº³å…¥è€ƒé‡ã€‚ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒS2S å’Œ S2A çš„æ€§èƒ½è¡¨ç°é«˜åº¦ä¾èµ–äºè®¾å¤‡é—´çš„æ•°æ®å¼‚æ„æ€§ (Data Heterogeneity) ç¨‹åº¦ï¼Œå¹¶åœ¨ä¸åŒçš„å‚æ•°åŒºé—´å†…å‘ˆç°å‡ºæ˜æ˜¾çš„ä¼˜åŠ£å·®å¼‚ã€‚è¿™é¡¹å·¥ä½œé€šè¿‡ç†è®ºä¸å®éªŒçš„ç»“åˆï¼Œä¸ä»…å¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç†è®ºç©ºç™½ï¼Œè¿˜ä¸ºå®é™…åº”ç”¨ä¸­åŠå»ä¸­å¿ƒåŒ–å­¦ä¹ ç³»ç»Ÿçš„é€šä¿¡æ¶æ„è®¾è®¡æä¾›äº†å…·ä½“çš„æŒ‡å¯¼å‡†åˆ™ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as a conference paper at AAAI 2026 (oral presentation). This is the extended version including the appendix",
      "pdf_url": "https://arxiv.org/pdf/2511.11560v2",
      "published_date": "2025-11-14 18:53:37 UTC",
      "updated_date": "2025-11-17 13:43:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:14:58.229470+00:00"
    },
    {
      "arxiv_id": "2511.11558v1",
      "title": "Human-AI collaborative autonomous synthesis with pulsed laser deposition for remote epitaxy",
      "title_zh": "é¢å‘è¿œç¨‹å¤–å»¶çš„äººæœºåä½œè„‰å†²æ¿€å…‰æ²‰ç§¯è‡ªä¸»åˆæˆ",
      "authors": [
        "Asraful Haque",
        "Daniel T. Yimam",
        "Jawad Chowdhury",
        "Ralph Bulanadi",
        "Ivan Vlassiouk",
        "John Lasseter",
        "Sujoy Ghosh",
        "Christopher M. Rouleau",
        "Kai Xiao",
        "Yongtao Liu",
        "Eva Zarkadoula",
        "Rama K. Vasudevan",
        "Sumner B. Harris"
      ],
      "abstract": "Autonomous laboratories typically rely on data-driven decision-making, occasionally with human-in-the-loop oversight to inject domain expertise. Fully leveraging AI agents, however, requires tightly coupled, collaborative workflows spanning hypothesis generation, experimental planning, execution, and interpretation. To address this, we develop and deploy a human-AI collaborative (HAIC) workflow that integrates large language models for hypothesis generation and analysis, with collaborative policy updates driving autonomous pulsed laser deposition (PLD) experiments for remote epitaxy of BaTiO$_3$/graphene. HAIC accelerated the hypothesis formation and experimental design and efficiently mapped the growth space to graphene-damage. In situ Raman spectroscopy reveals that chemistry drives degradation while the highest energy plume components seed defects, identifying a low-O$_2$ pressure low-temperature synthesis window that preserves graphene but is incompatible with optimal BaTiO$_3$ growth. Thus, we show a two-step Ar/O$_2$ deposition is required to exfoliate ferroelectric BaTiO$_3$ while maintaining a monolayer graphene interlayer. HAIC stages human insight with AI reasoning between autonomous batches to drive rapid scientific progress, providing an evolution to many existing human-in-the-loop autonomous workflows.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼€å‘å¹¶éƒ¨ç½²äº†ä¸€ç§äººæœºåä½œï¼ˆHuman-AI Collaborative, HAICï¼‰å·¥ä½œæµç¨‹ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆå¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelsï¼‰è¿›è¡Œå‡è®¾ç”Ÿæˆä¸åˆ†æï¼Œæå‡è‡ªä¸»å®éªŒå®¤çš„ç§‘ç ”æ•ˆç‡ã€‚è¯¥å·¥ä½œæµç¨‹ç»“åˆäº†åä½œç­–ç•¥æ›´æ–°ï¼Œé©±åŠ¨è‡ªä¸»è„‰å†²æ¿€å…‰æ²‰ç§¯ï¼ˆPulsed Laser Deposition, PLDï¼‰å®éªŒï¼Œç”¨äºå®ç° BaTiO$_3$/graphene çš„è¿œç¨‹å¤–å»¶ï¼ˆRemote Epitaxyï¼‰ç”Ÿé•¿ã€‚é€šè¿‡åŸä½æ‹‰æ›¼å…‰è°±ï¼ˆIn situ Raman spectroscopyï¼‰åˆ†æï¼Œç ”ç©¶å‘ç°åŒ–å­¦ä½œç”¨æ˜¯å¯¼è‡´é™è§£çš„ä¸»è¦é©±åŠ¨åŠ›ï¼Œè€Œé«˜èƒ½ç­‰ç¦»å­ä½“ç¾½è¾‰ï¼ˆPlume componentsï¼‰ç»„åˆ†åˆ™ä¼šè¯±å¯¼ç¼ºé™·äº§ç”Ÿã€‚å®éªŒç¡®å®šäº†ä¸€ä¸ªä½æ°§å‹ã€ä½æ¸©çš„åˆæˆçª—å£ä»¥ä¿æŠ¤çŸ³å¢¨çƒ¯ï¼Œä½†è¯¥ç¯å¢ƒå¹¶ä¸å…¼å®¹ BaTiO$_3$ çš„æœ€ä½³ç”Ÿé•¿æ¡ä»¶ã€‚ä¸ºäº†å…‹æœè¿™ä¸€æŒ‘æˆ˜ï¼Œç ”ç©¶æå‡ºå¹¶éªŒè¯äº†ä¸€ç§ä¸¤æ­¥ Ar/O$_2$ æ²‰ç§¯æ³•ï¼ŒæˆåŠŸåœ¨ä¿æŒå•å±‚çŸ³å¢¨çƒ¯ä¸­é—´å±‚çš„åŒæ—¶ï¼Œå‰¥ç¦»å‡ºé«˜è´¨é‡çš„é“ç”µ BaTiO$_3$ è–„è†œã€‚HAIC æ¡†æ¶é€šè¿‡åœ¨è‡ªä¸»å®éªŒæ‰¹æ¬¡é—´æœ‰æ•ˆç»“åˆäººç±»æ´å¯Ÿä¸ AI æ¨ç†ï¼Œæ˜¾è‘—åŠ é€Ÿäº†ç§‘å­¦å‘ç°è¿›ç¨‹ï¼Œä¸ºç°æœ‰çš„äººæœºå›ç¯ï¼ˆHuman-in-the-loopï¼‰è‡ªä¸»å·¥ä½œæµæä¾›äº†é‡è¦æ¼”è¿›ã€‚",
      "categories": [
        "cond-mat.mtrl-sci",
        "cs.AI"
      ],
      "primary_category": "cond-mat.mtrl-sci",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11558v1",
      "published_date": "2025-11-14 18:48:52 UTC",
      "updated_date": "2025-11-14 18:48:52 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:15:04.534061+00:00"
    },
    {
      "arxiv_id": "2511.11551v3",
      "title": "Aligning Machiavellian Agents: Behavior Steering via Test-Time Policy Shaping",
      "title_zh": "å¯¹é½é©¬åŸºé›…ç»´åˆ©å¼æ™ºèƒ½ä½“ï¼šåŸºäºæµ‹è¯•æ—¶ç­–ç•¥å¡‘é€ çš„è¡Œä¸ºå¼•å¯¼",
      "authors": [
        "Dena Mujtaba",
        "Brian Hu",
        "Anthony Hoogs",
        "Arslan Basharat"
      ],
      "abstract": "The deployment of decision-making AI agents presents a critical challenge in maintaining alignment with human values or guidelines while operating in complex, dynamic environments. Agents trained solely to achieve their objectives may adopt harmful behavior, exposing a key trade-off between maximizing the reward function and maintaining alignment. For pre-trained agents, ensuring alignment is particularly challenging, as retraining can be a costly and slow process. This is further complicated by the diverse and potentially conflicting attributes representing the ethical values for alignment. To address these challenges, we propose a test-time alignment technique based on model-guided policy shaping. Our method allows precise control over individual behavioral attributes, generalizes across diverse reinforcement learning (RL) environments, and facilitates a principled trade-off between ethical alignment and reward maximization without requiring agent retraining. We evaluate our approach using the MACHIAVELLI benchmark, which comprises 134 text-based game environments and thousands of annotated scenarios involving ethical decisions. The RL agents are first trained to maximize the reward in their respective games. At test time, we apply policy shaping via scenario-action attribute classifiers to ensure decision alignment with ethical attributes. We compare our approach against prior training-time methods and general-purpose agents, as well as study several types of ethical violations and power-seeking behavior. Our results demonstrate that test-time policy shaping provides an effective and scalable solution for mitigating unethical behavior across diverse environments and alignment attributes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å†³ç­– AI æ™ºèƒ½ä½“åœ¨è¿½æ±‚ç›®æ ‡æ—¶å¯èƒ½äº§ç”Ÿæœ‰å®³è¡Œä¸ºä¸”é‡æ–°è®­ç»ƒæˆæœ¬é«˜æ˜‚çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ¨¡å‹å¼•å¯¼çš„ç­–ç•¥å¡‘é€  (test-time policy shaping) çš„æµ‹è¯•æ—¶å¯¹é½æŠ€æœ¯ã€‚è¯¥æ–¹æ³•æ— éœ€å¯¹é¢„è®­ç»ƒæ™ºèƒ½ä½“è¿›è¡Œé‡æ–°è®­ç»ƒï¼Œé€šè¿‡æƒ…å¢ƒ-åŠ¨ä½œå±æ€§åˆ†ç±»å™¨ (scenario-action attribute classifiers) å®ç°å¯¹ä¸ªä½“è¡Œä¸ºå±æ€§çš„ç²¾ç¡®æ§åˆ¶ã€‚è¿™ç§æ–¹æ³•ä¸ä»…èƒ½æ³›åŒ–è‡³å¤šç§å¼ºåŒ–å­¦ä¹  (RL) ç¯å¢ƒï¼Œè¿˜èƒ½åœ¨ä¼¦ç†å¯¹é½ä¸å¥–åŠ±æœ€å¤§åŒ–ä¹‹é—´è¾¾æˆåŸåˆ™æ€§çš„æƒè¡¡ã€‚ç ”ç©¶äººå‘˜åœ¨åŒ…å« 134 ä¸ªæ–‡æœ¬æ¸¸æˆçš„ MACHIAVELLI åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œåˆ†æäº†æ•°åƒä¸ªæ¶‰åŠä¼¦ç†å†³ç­–å’ŒæƒåŠ›å¯»æ±‚è¡Œä¸º (power-seeking behavior) çš„åœºæ™¯ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨å‡è½»ä¸é“å¾·è¡Œä¸ºæ–¹é¢ä¼˜äºä¼ ç»Ÿçš„è®­ç»ƒæ—¶å¯¹é½æ–¹æ³•ã€‚ç ”ç©¶è¯æ˜äº†æµ‹è¯•æ—¶ç­–ç•¥å¡‘é€ æ˜¯ä¸€ç§åœ¨å¤šå…ƒç¯å¢ƒå’Œä¸åŒå¯¹é½å±æ€§ä¸‹æå…·æ‰©å±•æ€§çš„å¯¹é½è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted to AAAI 2026 AI Alignment Track",
      "pdf_url": "https://arxiv.org/pdf/2511.11551v3",
      "published_date": "2025-11-14 18:42:18 UTC",
      "updated_date": "2025-12-08 18:05:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:15:12.929347+00:00"
    },
    {
      "arxiv_id": "2511.17571v1",
      "title": "An improved clustering-based multi-swarm PSO using local diversification and topology information",
      "title_zh": "èåˆå±€éƒ¨å¤šæ ·åŒ–ä¸æ‹“æ‰‘ä¿¡æ¯çš„æ”¹è¿›å‹èšç±»å¤šç§ç¾¤ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³•",
      "authors": [
        "Yves Matanga",
        "Yanxia Sun",
        "Zenghui Wang"
      ],
      "abstract": "Multi-swarm particle optimisation algorithms are gaining popularity due to their ability to locate multiple optimum points concurrently. In this family of algorithms, clustering-based multi-swarm algorithms are among the most effective techniques that join the closest particles together to form independent niche swarms that exploit potential promising regions. However, most clustering-based multi-swarms are Euclidean distance-based and only inquire about the potential of one peak within a cluster and thus can lose multiple peaks due to poor resolution. In a bid to improve the peak detection ratio, the current study proposes two enhancements. First, a preliminary local search across initial particles is proposed to ensure that each local region is sufficiently scouted prior to particle collaboration. Secondly, an investigative clustering approach that performs concavity analysis is proposed to evaluate the potential for several sub-niches within a single cluster. An improved clustering-based multi-swarm PSO (TImPSO) has resulted from these enhancements and has been tested against three competing algorithms in the same family using the IEEE CEC2013 niching datasets, resulting in an improved peak ratio for almost all the test functions.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šç¾¤ä½“ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³• (Multi-swarm PSO) åœ¨å¤„ç†å¤šå³°å€¼ä¼˜åŒ–é—®é¢˜æ—¶ï¼Œä¼ ç»Ÿèšç±»æ–¹æ³•å› è¿‡åº¦ä¾èµ–æ¬§å‡ é‡Œå¾—è·ç¦»è€Œå®¹æ˜“å¿½ç•¥ç°‡å†…å¤šä¸ªå³°å€¼çš„å±€é™æ€§ï¼Œæå‡ºäº†æ”¹è¿›å‹ç®—æ³• TImPSOã€‚è¯¥æ¡†æ¶å¼•å…¥äº†ä¸¤é¡¹å…³é”®å¢å¼ºæªæ–½ï¼šé¦–å…ˆé€šè¿‡å¯¹åˆå§‹ç²’å­è¿›è¡Œåˆæ­¥çš„å±€éƒ¨æœç´¢ (Local search) ä»¥ç¡®ä¿å±€éƒ¨åŒºåŸŸåœ¨åä½œå‰å¾—åˆ°å……åˆ†æ¢ç´¢ï¼›å…¶æ¬¡é‡‡ç”¨ä¸€ç§åŸºäºå‡¹åº¦åˆ†æ (Concavity analysis) çš„è°ƒæŸ¥æ€§èšç±»æ–¹æ³•ï¼Œåˆ©ç”¨æ‹“æ‰‘ä¿¡æ¯è¯†åˆ«å•ä¸ªç°‡å†…çš„å¤šä¸ªå­åˆ©åŸº (Sub-niches)ã€‚å®éªŒåˆ©ç”¨ IEEE CEC2013 åˆ©åŸºä¼˜åŒ–æ•°æ®é›†è¿›è¡ŒéªŒè¯ï¼Œç»“æœæ˜¾ç¤º TImPSO åœ¨å‡ ä¹æ‰€æœ‰æµ‹è¯•å‡½æ•°ä¸Šçš„å³°å€¼æ£€æµ‹ç‡å‡ä¼˜äºå¯¹æ¯”ç®—æ³•ã€‚è¯¥ç ”ç©¶æœ‰æ•ˆæå‡äº†èšç±»å¤šç¾¤ä½“ç®—æ³•åœ¨å¤æ‚å¤šå³°ç¯å¢ƒä¸‹çš„åˆ†è¾¨ç‡ä¸æœç´¢æ•ˆèƒ½ã€‚",
      "categories": [
        "cs.NE",
        "cs.AI"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17571v1",
      "published_date": "2025-11-14 18:40:40 UTC",
      "updated_date": "2025-11-14 18:40:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:15:10.540490+00:00"
    },
    {
      "arxiv_id": "2511.11790v1",
      "title": "Differences in the Moral Foundations of Large Language Models",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹é“å¾·åŸºç¡€çš„å·®å¼‚",
      "authors": [
        "Peter Kirgis"
      ],
      "abstract": "Large language models are increasingly being used in critical domains of politics, business, and education, but the nature of their normative ethical judgment remains opaque. Alignment research has, to date, not sufficiently utilized perspectives and insights from the field of moral psychology to inform training and evaluation of frontier models. I perform a synthetic experiment on a wide range of models from most major model providers using Jonathan Haidt's influential moral foundations theory (MFT) to elicit diverse value judgments from LLMs. Using multiple descriptive statistical approaches, I document the bias and variance of large language model responses relative to a human baseline in the original survey. My results suggest that models rely on different moral foundations from one another and from a nationally representative human baseline, and these differences increase as model capabilities increase. This work seeks to spur further analysis of LLMs using MFT, including finetuning of open-source models, and greater deliberation by policymakers on the importance of moral foundations for LLM alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨å…³é”®é¢†åŸŸä¸­ä¸é€æ˜çš„è§„èŒƒæ€§é“å¾·åˆ¤æ–­é—®é¢˜ï¼Œæ—¨åœ¨å¡«è¡¥é“å¾·å¿ƒç†å­¦è§†è§’åœ¨æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°ä¸­çš„ç©ºç™½ã€‚ä½œè€…åˆ©ç”¨ Jonathan Haidt çš„é“å¾·åŸºç¡€ç†è®º(Moral Foundations Theory, MFT)å¯¹å„å¤§ä¸»æµæ¨¡å‹æä¾›å•†çš„å‰æ²¿æ¨¡å‹è¿›è¡Œäº†ç»¼åˆæ€§å®éªŒï¼Œä»¥è¯±å¯¼ LLMs äº§ç”Ÿå¤šæ ·åŒ–çš„ä»·å€¼åˆ¤æ–­ã€‚é€šè¿‡å¤šç§ç»Ÿè®¡å­¦æ–¹æ³•ï¼Œç ”ç©¶è®°å½•äº†å¤§è¯­è¨€æ¨¡å‹ç›¸å¯¹äºäººç±»åŸºå‡†åœ¨é“å¾·å“åº”ä¸Šçš„åå·®(bias)ä¸æ–¹å·®(variance)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œæ¨¡å‹ä¹‹é—´ä»¥åŠæ¨¡å‹ä¸å…·æœ‰ä»£è¡¨æ€§çš„äººç±»åŸºå‡†ä¹‹é—´åœ¨é“å¾·åŸºç¡€ä¸Šå­˜åœ¨æ˜¾è‘—å·®å¼‚ï¼Œä¸”è¿™ç§å·®å¼‚éšç€æ¨¡å‹èƒ½åŠ›çš„å¢å¼ºè€Œè¿›ä¸€æ­¥æ‰©å¤§ã€‚è¯¥å·¥ä½œä¸ä»…æå€¡åˆ©ç”¨ MFT å¯¹å¼€æºæ¨¡å‹è¿›è¡Œå¾®è°ƒ(finetuning)ï¼Œè¿˜å‘¼åæ”¿ç­–åˆ¶å®šè€…åœ¨ LLM å¯¹é½(alignment)è¿‡ç¨‹ä¸­æ›´æ·±å…¥åœ°å®¡è§†é“å¾·åŸºç¡€çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11790v1",
      "published_date": "2025-11-14 18:21:22 UTC",
      "updated_date": "2025-11-14 18:21:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:16:12.224488+00:00"
    },
    {
      "arxiv_id": "2511.11789v1",
      "title": "From Single to Societal: Analyzing Persona-Induced Bias in Multi-Agent Interactions",
      "title_zh": "ä»ä¸ªä½“åˆ°ç¤¾ä¼šï¼šå¤šæ™ºèƒ½ä½“äº¤äº’ä¸­äººæ ¼è¯±å‘çš„åè§åˆ†æ",
      "authors": [
        "Jiayi Li",
        "Xiao Liu",
        "Yansong Feng"
      ],
      "abstract": "Large Language Model (LLM)-based multi-agent systems are increasingly used to simulate human interactions and solve collaborative tasks. A common practice is to assign agents with personas to encourage behavioral diversity. However, this raises a critical yet underexplored question: do personas introduce biases into multi-agent interactions? This paper presents a systematic investigation into persona-induced biases in multi-agent interactions, with a focus on social traits like trustworthiness (how an agent's opinion is received by others) and insistence (how strongly an agent advocates for its opinion). Through a series of controlled experiments in collaborative problem-solving and persuasion tasks, we reveal that (1) LLM-based agents exhibit biases in both trustworthiness and insistence, with personas from historically advantaged groups (e.g., men and White individuals) perceived as less trustworthy and demonstrating less insistence; and (2) agents exhibit significant in-group favoritism, showing a higher tendency to conform to others who share the same persona. These biases persist across various LLMs, group sizes, and numbers of interaction rounds, highlighting an urgent need for awareness and mitigation to ensure the fairness and reliability of multi-agent systems.",
      "tldr_zh": "è¯¥ç ”ç©¶ç³»ç»Ÿæ¢è®¨äº†åœ¨å¤§è¯­è¨€æ¨¡å‹(LLM)å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œä¸ºæ™ºèƒ½ä½“åˆ†é…çš„äººæ ¼è®¾å®š(personas)æ˜¯å¦ä¼šå¼•å…¥åå·®ã€‚ç ”ç©¶é‡ç‚¹åˆ†æäº†ä¿¡ä»»åº¦(trustworthiness)å’ŒåšæŒåº¦(insistence)ç­‰ç¤¾äº¤ç‰¹å¾ï¼Œå³å…¶ä»–æ™ºèƒ½ä½“å¦‚ä½•æ¥æ”¶å…¶è§‚ç‚¹ä»¥åŠæ™ºèƒ½ä½“è‡ªèº«å€¡å¯¼è§‚ç‚¹çš„åŠ›åº¦ã€‚é€šè¿‡åœ¨åä½œè§£å†³é—®é¢˜å’Œè¯´æœä»»åŠ¡ä¸­çš„å—æ§å®éªŒï¼Œç ”ç©¶æ­ç¤ºäº†åŸºäºLLMçš„æ™ºèƒ½ä½“åœ¨ä¸Šè¿°ç¤¾äº¤ç‰¹å¾ä¸Šå­˜åœ¨æ˜¾è‘—åå·®ï¼Œå…¶ä¸­æ¥è‡ªå†å²ä¸Šä¼˜åŠ¿ç¾¤ä½“ï¼ˆå¦‚ç”·æ€§å’Œç™½äººï¼‰çš„äººæ ¼è®¾å®šè¢«æ„ŸçŸ¥ä¸ºä¿¡ä»»åº¦è¾ƒä½ä¸”åšæŒåº¦è¾ƒå¼±ã€‚æ­¤å¤–ï¼Œæ™ºèƒ½ä½“è¡¨ç°å‡ºæ˜¾è‘—çš„å†…ç¾¤ä½“åå¥½(in-group favoritism)ï¼Œæ›´å€¾å‘äºé¡ºä»å…·æœ‰ç›¸åŒäººæ ¼è®¾å®šçš„æˆå‘˜ã€‚è¿™äº›åå·®åœ¨ä¸åŒçš„LLMã€ç¾¤ä½“è§„æ¨¡å’Œäº¤äº’è½®æ¬¡ä¸­è¡¨ç°å‡ºæŒç»­æ€§ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¼€å‘ä¸­è¯†åˆ«å¹¶å‡è½»äººæ ¼è¯±å‘åå·®çš„ç´§è¿«æ€§ï¼Œä»¥ç¡®ä¿ç³»ç»Ÿçš„å…¬å¹³æ€§ä¸å¯é æ€§ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "AAAI-2026",
      "pdf_url": "https://arxiv.org/pdf/2511.11789v1",
      "published_date": "2025-11-14 18:19:28 UTC",
      "updated_date": "2025-11-14 18:19:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:16:07.129723+00:00"
    },
    {
      "arxiv_id": "2511.11533v1",
      "title": "Volumetric Ergodic Control",
      "title_zh": "ä½“ç§¯éå†æ§åˆ¶",
      "authors": [
        "Jueun Kwon",
        "Max M. Sun",
        "Todd Murphey"
      ],
      "abstract": "Ergodic control synthesizes optimal coverage behaviors over spatial distributions for nonlinear systems. However, existing formulations model the robot as a non-volumetric point, but in practice a robot interacts with the environment through its body and sensors with physical volume. In this work, we introduce a new ergodic control formulation that optimizes spatial coverage using a volumetric state representation. Our method preserves the asymptotic coverage guarantees of ergodic control, adds minimal computational overhead for real-time control, and supports arbitrary sample-based volumetric models. We evaluate our method across search and manipulation tasks -- with multiple robot dynamics and end-effector geometries or sensor models -- and show that it improves coverage efficiency by more than a factor of two while maintaining a 100% task completion rate across all experiments, outperforming the standard ergodic control method. Finally, we demonstrate the effectiveness of our method on a robot arm performing mechanical erasing tasks.",
      "tldr_zh": "è¯¥é¡¹ç ”ç©¶é’ˆå¯¹ç°æœ‰ Ergodic control ç®—æ³•å°†æœºå™¨äººç®€åŒ–ä¸ºæ— ä½“ç§¯ç‚¹è€Œå¿½è§†å…¶ç‰©ç†ä½“ç§¯ä¸ç¯å¢ƒäº¤äº’çš„é—®é¢˜ï¼Œæå‡ºäº† Volumetric Ergodic Control æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡å¼•å…¥ä½“ç§¯çŠ¶æ€è¡¨ç¤ºï¼ˆvolumetric state representationï¼‰æ¥ä¼˜åŒ–ç©ºé—´è¦†ç›–è¡Œä¸ºï¼Œèƒ½å¤Ÿæ”¯æŒä»»æ„åŸºäºé‡‡æ ·çš„ä½“ç§¯æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒäº† Ergodic control çš„æ¸è¿‘è¦†ç›–ä¿è¯ï¼Œä¸”ä»…å¢åŠ æå°çš„å®æ—¶æ§åˆ¶è®¡ç®—å¼€é”€ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨æ¶‰åŠå¤šç§æœºå™¨äººåŠ¨åŠ›å­¦ã€æœ«ç«¯æ‰§è¡Œå™¨å‡ ä½•å½¢çŠ¶åŠä¼ æ„Ÿå™¨æ¨¡å‹çš„æœç´¢ä¸æ“ä½œä»»åŠ¡ä¸­è¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒ 100% ä»»åŠ¡å®Œæˆç‡çš„åŒæ—¶ï¼Œå°†è¦†ç›–æ•ˆç‡æå‡äº†ä¸¤å€ä»¥ä¸Šï¼Œæ˜¾è‘—ä¼˜äºä¼ ç»Ÿçš„ Ergodic control æ–¹æ³•ã€‚æœ€åï¼Œç ”ç©¶è€…é€šè¿‡æœºæ¢°è‡‚æ‰§è¡Œæœºæ¢°æ“¦é™¤ä»»åŠ¡çš„å®éªŒï¼Œè¿›ä¸€æ­¥è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤„ç†å…·æœ‰ç‰©ç†ä½“ç§¯çº¦æŸçš„çœŸå®ç¯å¢ƒäº¤äº’ä¸­çš„æœ‰æ•ˆæ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "8 pages, 8 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.11533v1",
      "published_date": "2025-11-14 18:10:40 UTC",
      "updated_date": "2025-11-14 18:10:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:16:06.426312+00:00"
    },
    {
      "arxiv_id": "2511.11788v1",
      "title": "MALBO: Optimizing LLM-Based Multi-Agent Teams via Multi-Objective Bayesian Optimization",
      "title_zh": "MALBOï¼šåŸºäºå¤šç›®æ ‡è´å¶æ–¯ä¼˜åŒ–çš„å¤§è¯­è¨€æ¨¡å‹å¤šæ™ºèƒ½ä½“å›¢é˜Ÿä¼˜åŒ–",
      "authors": [
        "Antonio Sabbatella"
      ],
      "abstract": "The optimal assignment of Large Language Models (LLMs) to specialized roles in multi-agent systems is a significant challenge, defined by a vast combinatorial search space, expensive black-box evaluations, and an inherent trade-off between performance and cost. Current optimization methods focus on single-agent settings and lack a principled framework for this multi-agent, multi-objective problem.\n  This thesis introduces MALBO (Multi-Agent LLM Bayesian Optimization), a systematic framework designed to automate the efficient composition of LLM-based agent teams. We formalize the assignment challenge as a multi-objective optimization problem, aiming to identify the Pareto front of configurations between task accuracy and inference cost. The methodology employs multi-objective Bayesian Optimization (MOBO) with independent Gaussian Process surrogate models. By searching over a continuous feature-space representation of the LLMs, this approach performs a sample-efficient exploration guided by the expected hypervolume improvement.\n  The primary contribution is a principled and automated methodology that yields a Pareto front of optimal team configurations. Our results demonstrate that the Bayesian optimization phase, compared to an initial random search, maintained a comparable average performance while reducing the average configuration cost by over 45%. Furthermore, MALBO identified specialized, heterogeneous teams that achieve cost reductions of up to 65.8% compared to homogeneous baselines, all while maintaining maximum performance. The framework thus provides a data-driven tool for deploying cost-effective and highly specialized multi-agent AI systems.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº† MALBO (Multi-Agent LLM Bayesian Optimization)ï¼Œä¸€ä¸ªæ—¨åœ¨è‡ªåŠ¨åŒ–å’Œä¼˜åŒ–åŸºäº Large Language Models (LLMs) çš„å¤šæ™ºèƒ½ä½“å›¢é˜Ÿé…ç½®çš„ç³»ç»Ÿæ€§æ¡†æ¶ã€‚è¯¥æ¡†æ¶å°†æ™ºèƒ½ä½“è§’è‰²åˆ†é…è§†ä¸ºä¸€ä¸ªå¤šç›®æ ‡ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡ Multi-Objective Bayesian Optimization (MOBO) å’Œç‹¬ç«‹çš„ Gaussian Process ä»£ç†æ¨¡å‹ï¼Œåœ¨ä»»åŠ¡å‡†ç¡®ç‡ä¸æ¨ç†æˆæœ¬ä¹‹é—´å¯»æ‰¾ Pareto frontã€‚MALBO åˆ©ç”¨ LLMs çš„è¿ç»­ç‰¹å¾ç©ºé—´è¡¨ç¤ºï¼Œé€šè¿‡ Expected Hypervolume Improvement å¼•å¯¼è¿›è¡Œé«˜æ•ˆçš„æ ·æœ¬æ¢ç´¢ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸åˆå§‹éšæœºæœç´¢ç›¸æ¯”ï¼ŒMALBO åœ¨ä¿æŒç›¸å½“æ€§èƒ½çš„åŒæ—¶å°†å¹³å‡é…ç½®æˆæœ¬é™ä½äº† 45% ä»¥ä¸Šã€‚æ­¤å¤–ï¼ŒMALBO èƒ½å¤Ÿè¯†åˆ«å‡ºé«˜åº¦ä¸“ä¸šåŒ–çš„å¼‚æ„å›¢é˜Ÿï¼Œåœ¨ç»´æŒæœ€é«˜æ€§èƒ½çš„å‰æä¸‹ï¼Œç›¸æ¯”åŒè´¨åŒ–åŸºå‡†æ¨¡å‹å®ç°äº†é«˜è¾¾ 65.8% çš„æˆæœ¬å‰Šå‡ã€‚è¯¥æ¡†æ¶ä¸ºéƒ¨ç½²é«˜æˆæœ¬æ•ˆç›Šä¸”é«˜åº¦ä¸“ä¸šåŒ–çš„å¤šæ™ºèƒ½ä½“äººå·¥æ™ºèƒ½ç³»ç»Ÿæä¾›äº†ä¸€ç§æ•°æ®é©±åŠ¨çš„å·¥å…·ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "Master's Thesis, University of Milano-Bicocca, 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.11788v1",
      "published_date": "2025-11-14 18:01:08 UTC",
      "updated_date": "2025-11-14 18:01:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:16:09.633511+00:00"
    },
    {
      "arxiv_id": "2511.11519v1",
      "title": "Experience-Guided Adaptation of Inference-Time Reasoning Strategies",
      "title_zh": "ç»éªŒå¼•å¯¼çš„æ¨ç†æ—¶æ¨ç†ç­–ç•¥è‡ªé€‚åº”",
      "authors": [
        "Adam Stein",
        "Matthew Trager",
        "Benjamin Bowman",
        "Michael Kleinman",
        "Aditya Chattopadhyay",
        "Wei Xia",
        "Stefano Soatto"
      ],
      "abstract": "Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Experience-Guided Reasoner (EGuR)ï¼Œä¸€ç§æ—¨åœ¨ä½¿ Agentic AI ç³»ç»Ÿèƒ½å¤Ÿæ ¹æ®è®­ç»ƒåçš„äº¤äº’ç»éªŒåŠ¨æ€è°ƒæ•´æ¨ç†ç­–ç•¥çš„æ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰ç³»ç»Ÿæ— æ³•åœ¨æ¨ç†é˜¶æ®µçµæ´»æ›´æ”¹é‡‡æ ·å‚æ•°ã€å·¥å…·é…ç½®æˆ–æ§åˆ¶é€»è¾‘çš„å±€é™ï¼ŒEGuR åˆ©ç”¨åŸºäº LLM çš„ Meta-strategy åœ¨æ¨ç†æ—¶ç”Ÿæˆæ¶µç›–æç¤ºè¯ã€å‚æ•°å’Œæ§åˆ¶é€»è¾‘çš„å®šåˆ¶åŒ–è®¡ç®—ç¨‹åºã€‚è¯¥ç³»ç»Ÿé€šè¿‡è´Ÿè´£ç”Ÿæˆå€™é€‰ç­–ç•¥çš„ Guide ä»¥åŠæ•´åˆæ‰§è¡Œåé¦ˆä»¥ä¼˜åŒ–æœªæ¥ç”Ÿæˆçš„ Consolidator ååŒå·¥ä½œï¼Œå®ç°äº†ç­–ç•¥çš„åŠ¨æ€ç”Ÿæˆã€ç¼“å­˜ä¸é«˜æ•ˆæ‰§è¡Œã€‚å®éªŒè¡¨æ˜ï¼Œåœ¨ AIME 2025ã€3-SAT å’Œ Big Bench Extra Hard ç­‰æŒ‘æˆ˜æ€§åŸºå‡†æµ‹è¯•ä¸­ï¼ŒEGuR ç›¸æ¯”æœ€å¼ºåŸºçº¿æ¨¡å‹å®ç°äº†é«˜è¾¾ 14% çš„å‡†ç¡®ç‡æå‡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶åœ¨æ˜¾è‘—æé«˜æ€§èƒ½çš„åŒæ—¶å°†è®¡ç®—æˆæœ¬é™ä½äº†å¤šè¾¾ 111 å€ï¼Œä¸”ç³»ç»Ÿè¡¨ç°éšç»éªŒç§¯ç´¯å±•ç°å‡ºæŒç»­çš„è¿›åŒ–èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "29 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.11519v1",
      "published_date": "2025-11-14 17:45:28 UTC",
      "updated_date": "2025-11-14 17:45:28 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:16:12.630913+00:00"
    },
    {
      "arxiv_id": "2511.11502v1",
      "title": "PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models",
      "title_zh": "PASï¼šç”¨äºæ£€æµ‹å¤§è§†è§‰è¯­è¨€æ¨¡å‹ç‰©ä½“å¹»è§‰çš„å‰å¯¼æ³¨æ„åŠ›è¯„åˆ†",
      "authors": [
        "Nhat Hoang-Xuan",
        "Minh Vu",
        "My T. Thai",
        "Manish Bhattarai"
      ],
      "abstract": "Large vision-language models (LVLMs) are powerful, yet they remain unreliable due to object hallucinations. In this work, we show that in many hallucinatory predictions the LVLM effectively ignores the image and instead relies on previously generated output (prelim) tokens to infer new objects. We quantify this behavior via the mutual information between the image and the predicted object conditioned on the prelim, demonstrating that weak image dependence strongly correlates with hallucination. Building on this finding, we introduce the Prelim Attention Score (PAS), a lightweight, training-free signal computed from attention weights over prelim tokens. PAS requires no additional forward passes and can be computed on the fly during inference. Exploiting this previously overlooked signal, PAS achieves state-of-the-art object-hallucination detection across multiple models and datasets, enabling real-time filtering and intervention.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ (Large Vision-Language Models, LVLMs) ä¸­å­˜åœ¨çš„ç‰©ä½“å¹»è§‰ (object hallucinations) é—®é¢˜ï¼Œæ­ç¤ºäº†æ¨¡å‹åœ¨äº§ç”Ÿå¹»è§‰æ—¶å¾€å¾€ä¼šå¿½ç•¥å›¾åƒå†…å®¹ï¼Œè½¬è€Œè¿‡åº¦ä¾èµ–å·²ç”Ÿæˆçš„æ–‡æœ¬æ ‡è®° (prelim tokens) æ¥æ¨æ–­æ–°ç‰©ä½“ã€‚ç ”ç©¶äººå‘˜é€šè¿‡é‡åŒ–åœ¨ç»™å®š prelim æƒ…å†µä¸‹å›¾åƒä¸é¢„æµ‹ç‰©ä½“ä¹‹é—´çš„äº’ä¿¡æ¯ (mutual information)ï¼Œè¯å®äº†å¼±å›¾åƒä¾èµ–æ€§ä¸å¹»è§‰ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ã€‚åŸºäºè¿™ä¸€å‘ç°ï¼Œè®ºæ–‡æå‡ºäº†åˆæ­¥æ³¨æ„åŠ›åˆ†æ•° (Prelim Attention Score, PAS)ï¼Œè¿™æ˜¯ä¸€ç§åˆ©ç”¨ prelim tokens çš„æ³¨æ„åŠ›æƒé‡è®¡ç®—å‡ºçš„è½»é‡çº§ã€æ— éœ€è®­ç»ƒçš„ä¿¡å·ã€‚PAS æ— éœ€é¢å¤–çš„æ¨¡å‹å‰å‘ä¼ æ’­ (forward passes)ï¼Œèƒ½å¤Ÿåœ¨æ¨ç†è¿‡ç¨‹ä¸­å®æ—¶è®¡ç®—ï¼Œå¹¶åˆ©ç”¨è¿™ä¸€æ­¤å‰è¢«å¿½è§†çš„ä¿¡å·åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šå®ç°äº†æœ€å…ˆè¿› (state-of-the-art) çš„ç‰©ä½“å¹»è§‰æ£€æµ‹æ€§èƒ½ã€‚è¯¥æŠ€æœ¯ä¸ºå®ç°è§†è§‰è¯­è¨€æ¨¡å‹çš„å®æ—¶å¹»è§‰è¿‡æ»¤ä¸å¹²é¢„æä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11502v1",
      "published_date": "2025-11-14 17:23:55 UTC",
      "updated_date": "2025-11-14 17:23:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:16:21.076469+00:00"
    },
    {
      "arxiv_id": "2511.11490v1",
      "title": "Intrinsic Dimension Estimation for Radio Galaxy Zoo using Diffusion Models",
      "title_zh": "åŸºäºæ‰©æ•£æ¨¡å‹çš„å°„ç”µæ˜Ÿç³»åŠ¨ç‰©å›­æœ¬å¾ç»´åº¦ä¼°è®¡",
      "authors": [
        "Joan Font-Quer Roset",
        "Devina Mohan",
        "Anna Scaife"
      ],
      "abstract": "In this work, we estimate the intrinsic dimension (iD) of the Radio Galaxy Zoo (RGZ) dataset using a score-based diffusion model. We examine how the iD estimates vary as a function of Bayesian neural network (BNN) energy scores, which measure how similar the radio sources are to the MiraBest subset of the RGZ dataset. We find that out-of-distribution sources exhibit higher iD values, and that the overall iD for RGZ exceeds those typically reported for natural image datasets. Furthermore, we analyse how iD varies across Fanaroff-Riley (FR) morphological classes and as a function of the signal-to-noise ratio (SNR). While no relationship is found between FR I and FR II classes, a weak trend toward higher SNR at lower iD. Future work using the RGZ dataset could make use of the relationship between iD and energy scores to quantitatively study and improve the representations learned by various self-supervised learning algorithms.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨ score-based diffusion model å¯¹ Radio Galaxy Zoo (RGZ) æ•°æ®é›†çš„ intrinsic dimension (iD) è¿›è¡Œäº†ä¼°ç®—ã€‚é€šè¿‡åˆ†æ iD ä¼°è®¡å€¼ä¸ Bayesian neural network (BNN) energy scores ä¹‹é—´çš„å…³ç³»ï¼Œç ”ç©¶å‘ç° out-of-distribution å°„ç”µæºå…·æœ‰æ›´é«˜çš„ iD å€¼ï¼Œä¸” RGZ çš„æ•´ä½“ iD æ™®éé«˜äºè‡ªç„¶å›¾åƒæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œç ”ç©¶æ¢è®¨äº† iD åœ¨ Fanaroff-Riley (FR) å½¢æ€åˆ†ç±»åŠ signal-to-noise ratio (SNR) å½±å“ä¸‹çš„å˜åŒ–ï¼Œç»“æœæ˜¾ç¤º FR I ä¸ FR II ç±»åˆ«é—´æ— æ˜æ˜¾å…³è”ï¼Œä½†åœ¨ä½ iD å¤„å‘ˆç°å‡º SNR å¢é«˜çš„å¾®å¼±è¶‹åŠ¿ã€‚è¯¥æˆæœæ­ç¤ºäº† iD ä¸ energy scores ä¹‹é—´çš„å®šé‡è”ç³»ï¼Œä¸ºæœªæ¥åˆ©ç”¨è¯¥å…³ç³»æ”¹è¿› self-supervised learning ç®—æ³•åœ¨å¤©æ–‡æ•°æ®é›†ä¸Šçš„è¡¨å¾å­¦ä¹ èƒ½åŠ›æä¾›äº†é‡è¦ä¾æ®ã€‚",
      "categories": [
        "cs.LG",
        "astro-ph.IM",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "9 pages, 5 figures, 2 tables, submitted to NeurIPS 2025 ML4PS Workshop",
      "pdf_url": "https://arxiv.org/pdf/2511.11490v1",
      "published_date": "2025-11-14 17:09:01 UTC",
      "updated_date": "2025-11-14 17:09:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:16:23.184046+00:00"
    },
    {
      "arxiv_id": "2511.11483v2",
      "title": "ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation",
      "title_zh": "ImAgentï¼šé¢å‘æµ‹è¯•æ—¶å¯æ‰©å±•å›¾åƒç”Ÿæˆçš„ç»Ÿä¸€å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Kaishen Wang",
        "Ruibo Chen",
        "Tong Zheng",
        "Heng Huang"
      ],
      "abstract": "Recent text-to-image (T2I) models have made remarkable progress in generating visually realistic and semantically coherent images. However, they still suffer from randomness and inconsistency with the given prompts, particularly when textual descriptions are vague or underspecified. Existing approaches, such as prompt rewriting, best-of-N sampling, and self-refinement, can mitigate these issues but usually require additional modules and operate independently, hindering test-time scaling efficiency and increasing computational overhead. In this paper, we introduce ImAgent, a training-free unified multimodal agent that integrates reasoning, generation, and self-evaluation within a single framework for efficient test-time scaling. Guided by a policy controller, multiple generation actions dynamically interact and self-organize to enhance image fidelity and semantic alignment without relying on external models. Extensive experiments on image generation and editing tasks demonstrate that ImAgent consistently improves over the backbone and even surpasses other strong baselines where the backbone model fails, highlighting the potential of unified multimodal agents for adaptive and efficient image generation under test-time scaling.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† ImAgentï¼Œè¿™æ˜¯ä¸€ä¸ªæ— éœ€è®­ç»ƒçš„ç»Ÿä¸€å¤šæ¨¡æ€æ™ºèƒ½ä½“æ¡†æ¶ (Unified Multimodal Agent Framework)ï¼Œæ—¨åœ¨è§£å†³æ–‡æœ¬ç”Ÿæˆå›¾åƒ (text-to-image) æ¨¡å‹åœ¨é¢å¯¹æ¨¡ç³Šæç¤ºè¯æ—¶å­˜åœ¨çš„éšæœºæ€§ä¸ä¸ä¸€è‡´æ€§é—®é¢˜ã€‚è¯¥æ¡†æ¶åœ¨å•ä¸€ä½“ç³»å†…é›†æˆäº†æ¨ç† (reasoning)ã€ç”Ÿæˆ (generation) å’Œè‡ªæˆ‘è¯„ä¼° (self-evaluation)ï¼Œé€šè¿‡æµ‹è¯•æ—¶æ‰©å±• (test-time scaling) æ˜¾è‘—æå‡äº†ç”Ÿæˆæ•ˆç‡å¹¶é™ä½äº†è®¡ç®—å¼€é”€ã€‚åœ¨ç­–ç•¥æ§åˆ¶å™¨ (policy controller) çš„å¼•å¯¼ä¸‹ï¼Œå¤šä¸ªç”ŸæˆåŠ¨ä½œèƒ½å¤Ÿè¿›è¡ŒåŠ¨æ€äº¤äº’ä¸è‡ªç»„ç»‡ (self-organize)ï¼Œä»è€Œåœ¨ä¸ä¾èµ–å¤–éƒ¨æ¨¡å‹çš„æƒ…å†µä¸‹å¢å¼ºå›¾åƒçš„ä¿çœŸåº¦ (fidelity) å’Œè¯­ä¹‰å¯¹é½ (semantic alignment)ã€‚å¹¿æ³›çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒImAgent åœ¨å›¾åƒç”Ÿæˆä¸ç¼–è¾‘ä»»åŠ¡ä¸­å‡ä¼˜äºåŸºçº¿æ¨¡å‹ï¼Œå³ä½¿åœ¨éª¨å¹²æ¨¡å‹å¤±æ•ˆçš„æƒ…å†µä¸‹ä¹Ÿèƒ½å±•ç°å‡ºå“è¶Šæ€§èƒ½ã€‚è¯¥ç ”ç©¶çªæ˜¾äº†ç»Ÿä¸€å¤šæ¨¡æ€æ™ºèƒ½ä½“åœ¨å®ç°è‡ªé€‚åº”ä¸”é«˜æ•ˆçš„å›¾åƒç”Ÿæˆé¢†åŸŸçš„å·¨å¤§åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "12 pages, 5 tables, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.11483v2",
      "published_date": "2025-11-14 17:00:29 UTC",
      "updated_date": "2025-11-24 02:28:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:16:25.867387+00:00"
    },
    {
      "arxiv_id": "2511.11480v2",
      "title": "Inferring response times of perceptual decisions with Poisson variational autoencoders",
      "title_zh": "åˆ©ç”¨æ³Šæ¾å˜åˆ†è‡ªç¼–ç å™¨æ¨æ–­çŸ¥è§‰å†³ç­–çš„ååº”æ—¶é—´",
      "authors": [
        "Hayden R. Johnson",
        "Anastasia N. Krouglova",
        "Hadi Vafaii",
        "Jacob L. Yates",
        "Pedro J. GonÃ§alves"
      ],
      "abstract": "Many properties of perceptual decision making are well-modeled by deep neural networks. However, such architectures typically treat decisions as instantaneous readouts, overlooking the temporal dynamics of the decision process. We present an image-computable model of perceptual decision making in which choices and response times arise from efficient sensory encoding and Bayesian decoding of neural spiking activity. We use a Poisson variational autoencoder to learn unsupervised representations of visual stimuli in a population of rate-coded neurons, modeled as independent homogeneous Poisson processes. A task-optimized decoder then continually infers an approximate posterior over actions conditioned on incoming spiking activity. Combining these components with an entropy-based stopping rule yields a principled and image-computable model of perceptual decisions capable of generating trial-by-trial patterns of choices and response times. Applied to MNIST digit classification, the model reproduces key empirical signatures of perceptual decision making, including stochastic variability, right-skewed response time distributions, logarithmic scaling of response times with the number of alternatives (Hick's law), and speed-accuracy trade-offs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå›¾åƒè®¡ç®—(image-computable)æ¨¡å‹ç”¨äºæ¨æ–­æ„ŸçŸ¥å†³ç­–ä¸­çš„ååº”æ—¶é—´ï¼Œæ—¨åœ¨è§£å†³ä¼ ç»Ÿæ·±åº¦ç¥ç»ç½‘ç»œå°†å†³ç­–è§†ä¸ºç¬æ—¶è¾“å‡ºè€Œå¿½ç•¥æ—¶é—´åŠ¨æ€çš„é—®é¢˜ã€‚è¯¥æ¶æ„åˆ©ç”¨Poisson variational autoencoder (PVAE) åœ¨ç‡ç¼–ç ç¥ç»å…ƒ(rate-coded neurons)ç¾¤ä½“ä¸­å­¦ä¹ è§†è§‰åˆºæ¿€çš„æ— ç›‘ç£è¡¨å¾ï¼Œå¹¶å°†ç¥ç»æ´»åŠ¨å»ºæ¨¡ä¸ºç‹¬ç«‹é½æ¬¡æ³Šæ¾è¿‡ç¨‹ã€‚é€šè¿‡ç»“åˆä»»åŠ¡ä¼˜åŒ–è§£ç å™¨ä¸åŸºäºç†µ(entropy-based)çš„åœæ­¢è§„åˆ™ï¼Œæ¨¡å‹èƒ½å¤Ÿæ ¹æ®æŒç»­è¾“å…¥çš„è„‰å†²æ´»åŠ¨è¿›è¡Œè´å¶æ–¯è§£ç ï¼Œä»è€Œäº§ç”Ÿå•è¯•æ¬¡æ°´å¹³çš„é€‰æ‹©å’Œååº”æ—¶é—´æ¨¡å¼ã€‚åœ¨MNISTæ•°å­—åˆ†ç±»ä»»åŠ¡çš„å®éªŒä¸­ï¼Œè¯¥æ¨¡å‹æˆåŠŸé‡ç°äº†æ„ŸçŸ¥å†³ç­–çš„å…³é”®ç»éªŒç‰¹å¾ï¼ŒåŒ…æ‹¬éšæœºå˜å¼‚æ€§(stochastic variability)ã€å³åååº”æ—¶é—´åˆ†å¸ƒã€ç¬¦åˆHick's lawçš„å¯¹æ•°ç¼©æ”¾æ•ˆåº”ä»¥åŠé€Ÿåº¦-å‡†ç¡®åº¦æƒè¡¡(speed-accuracy trade-offs)ã€‚",
      "categories": [
        "q-bio.NC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "q-bio.NC",
      "comment": "To appear at the NeurIPS 2025 Workshop on Data on the Brain \\& Mind",
      "pdf_url": "https://arxiv.org/pdf/2511.11480v2",
      "published_date": "2025-11-14 16:58:04 UTC",
      "updated_date": "2025-11-24 12:53:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:16:35.579194+00:00"
    },
    {
      "arxiv_id": "2511.11476v1",
      "title": "Context-aware Adaptive Visualizations for Critical Decision Making",
      "title_zh": "é¢å‘å…³é”®å†³ç­–çš„è¯­å¢ƒæ„ŸçŸ¥è‡ªé€‚åº”å¯è§†åŒ–",
      "authors": [
        "Angela Lopez-Cardona",
        "Mireia Masias Bruns",
        "Nuwan T. Attygalle",
        "Sebastian Idesis",
        "Matteo Salvatori",
        "Konstantinos Raftopoulos",
        "Konstantinos Oikonomou",
        "Saravanakumar Duraisamy",
        "Parvin Emami",
        "Nacera Latreche",
        "Alaa Eddine Anis Sahraoui",
        "Michalis Vakallelis",
        "Jean Vanderdonckt",
        "Ioannis Arapakis",
        "Luis A. Leiva"
      ],
      "abstract": "Effective decision-making often relies on timely insights from complex visual data. While Information Visualization (InfoVis) dashboards can support this process, they rarely adapt to users' cognitive state, and less so in real time. We present Symbiotik, an intelligent, context-aware adaptive visualization system that leverages neurophysiological signals to estimate mental workload (MWL) and dynamically adapt visual dashboards using reinforcement learning (RL). Through a user study with 120 participants and three visualization types, we demonstrate that our approach improves task performance and engagement. Symbiotik offers a scalable, real-time adaptation architecture, and a validated methodology for neuroadaptive user interfaces.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Symbiotikï¼Œä¸€ä¸ªæ™ºèƒ½ä¸”å…·æœ‰ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›çš„è‡ªé€‚åº”å¯è§†åŒ–ç³»ç»Ÿï¼Œæ—¨åœ¨æå‡å…³é”®å†³ç­–ä¸­çš„ä¿¡æ¯è·å–æ•ˆç‡ã€‚è¯¥ç³»ç»Ÿé€šè¿‡ç›‘æµ‹ç¥ç»ç”Ÿç†ä¿¡å·æ¥å®æ—¶è¯„ä¼°ç”¨æˆ·çš„å¿ƒç†å·¥ä½œè´Ÿè·(Mental Workload, MWL)ï¼Œå¹¶åˆ©ç”¨å¼ºåŒ–å­¦ä¹ (Reinforcement Learning, RL)ç®—æ³•åŠ¨æ€è°ƒæ•´å¯è§†åŒ–ä»ªè¡¨æ¿ã€‚åœ¨ä¸€é¡¹åŒ…å«120åå‚ä¸è€…å’Œä¸‰ç§å¯è§†åŒ–ç±»å‹çš„ç”¨æˆ·ç ”ç©¶ä¸­ï¼ŒSymbiotikè¢«è¯å®èƒ½å¤Ÿæœ‰æ•ˆæ”¹å–„ä»»åŠ¡è¡¨ç°å¹¶å¢å¼ºç”¨æˆ·å‚ä¸åº¦ã€‚è¯¥æˆæœä¸ºç¥ç»è‡ªé€‚åº”ç”¨æˆ·ç•Œé¢(Neuroadaptive User Interfaces)æä¾›äº†ä¸€å¥—å¯æ‰©å±•çš„å®æ—¶é€‚åº”æ¶æ„ï¼Œå¹¶å»ºç«‹äº†ä¸€å¥—ç»è¿‡éªŒè¯çš„ç³»ç»Ÿæ€§æ–¹æ³•è®ºã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11476v1",
      "published_date": "2025-11-14 16:53:15 UTC",
      "updated_date": "2025-11-14 16:53:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:16:28.581098+00:00"
    },
    {
      "arxiv_id": "2511.11468v1",
      "title": "Benchmarking Visual LLMs Resilience to Unanswerable Questions on Visually Rich Documents",
      "title_zh": "è§†è§‰å¤§è¯­è¨€æ¨¡å‹å¤„ç†å¯Œè§†è§‰æ–‡æ¡£ä¸­ä¸å¯å›ç­”é—®é¢˜çš„éŸ§æ€§åŸºå‡†è¯„ä¼°",
      "authors": [
        "Davide Napolitano",
        "Luca Cagliero",
        "Fabrizio Battiloro"
      ],
      "abstract": "The evolution of Visual Large Language Models (VLLMs) has revolutionized the automatic understanding of Visually Rich Documents (VRDs), which contain both textual and visual elements. Although VLLMs excel in Visual Question Answering (VQA) on multi-page VRDs, their ability to detect unanswerable questions is still an open research question. Our research delves into the robustness of the VLLMs to plausible yet unanswerable questions, i.e., questions that appear valid but cannot be answered due to subtle corruptions caused by swaps between related concepts or plausible question formulations. Corruptions are generated by replacing the original natural language entities with other ones of the same type, belonging to different document elements, and in different layout positions or pages of the related document. To this end, we present VRD-UQA (VISUALLY RICH DOCUMENT UNANSWERABLE QUESTION ANSWERING), a benchmark for evaluating VLLMs' resilience to plausible yet unanswerable questions across multiple dimensions. It automatically alters the questions of existing VQA datasets consisting of multi-page VRDs, verifies their unanswerability using a VLLM-as-a-judge approach, and then thoroughly evaluates VLLMs' performance. Experiments, run on 12 models, analyze: (1) The VLLMs' accuracy in detecting unanswerable questions at both page and document levels; (2) The effect of different types of corruption (NLP entity, document element, layout); (3) The effectiveness of different knowledge injection strategies based on in-context learning (OCR, multi-page selection, or the possibility of unanswerability). Our findings reveal VLLMs' limitations and demonstrate that VRD-UQA can serve as an evaluation framework for developing resilient document VQA systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰å¤§è¯­è¨€æ¨¡å‹(Visual Large Language Models, VLLMs)åœ¨å¤„ç†å¯Œè§†è§‰æ–‡æ¡£(Visually Rich Documents, VRDs)æ—¶ï¼Œæ£€æµ‹çœ‹ä¼¼åˆç†ä½†å®é™…ä¸Šæ— æ³•å›ç­”çš„é—®é¢˜(unanswerable questions)çš„é²æ£’æ€§ã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº†VRD-UQA(VISUALLY RICH DOCUMENT UNANSWERABLE QUESTION ANSWERING)åŸºå‡†æµ‹è¯•ï¼Œæ—¨åœ¨ä»å¤šä¸ªç»´åº¦è¯„ä¼°VLLMså¯¹è¿™ç±»é—®é¢˜çš„è¯†åˆ«èƒ½åŠ›ã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªåŠ¨æ›¿æ¢æ–‡æ¡£ä¸­çš„è‡ªç„¶è¯­è¨€å®ä½“ã€å…ƒç´ æˆ–å¸ƒå±€ä½ç½®æ¥ç”Ÿæˆå—æŸé—®é¢˜ï¼Œå¹¶é‡‡ç”¨VLLM-as-a-judgeæ–¹æ³•éªŒè¯å…¶ä¸å¯å›ç­”æ€§ã€‚å®éªŒå¯¹12ä¸ªæ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œæ·±å…¥åˆ†æäº†é¡µé¢ä¸æ–‡æ¡£å±‚é¢çš„æ£€æµ‹å‡†ç¡®ç‡ã€ä¸åŒæŸåç±»å‹çš„å½±å“ä»¥åŠåŸºäºä¸Šä¸‹æ–‡å­¦ä¹ (in-context learning)çš„çŸ¥è¯†æ³¨å…¥ç­–ç•¥ã€‚ç ”ç©¶å‘ç°æ­ç¤ºäº†å½“å‰VLLMsåœ¨åº”å¯¹æ­¤ç±»å¤æ‚è™šå‡é—®é¢˜æ—¶çš„å±€é™æ€§ï¼Œè¯æ˜VRD-UQAå¯ä½œä¸ºå¼€å‘é«˜éŸ§æ€§æ–‡æ¡£è§†è§‰é—®ç­”(Visual Question Answering, VQA)ç³»ç»Ÿçš„é‡è¦è¯„ä¼°æ¡†æ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11468v1",
      "published_date": "2025-11-14 16:41:10 UTC",
      "updated_date": "2025-11-14 16:41:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:16:48.784300+00:00"
    },
    {
      "arxiv_id": "2511.11461v1",
      "title": "Epistemic Error Decomposition for Multi-step Time Series Forecasting: Rethinking Bias-Variance in Recursive and Direct Strategies",
      "title_zh": "å¤šæ­¥æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„è®¤çŸ¥è¯¯å·®åˆ†è§£ï¼šé‡æ–°å®¡è§†é€’å½’ä¸ç›´æ¥ç­–ç•¥ä¸­çš„åå·®-æ–¹å·®é—®é¢˜",
      "authors": [
        "Riku Green",
        "Huw Day",
        "Zahraa S. Abdallah",
        "Telmo M. Silva Filho"
      ],
      "abstract": "Multi-step forecasting is often described through a simple rule of thumb: recursive strategies are said to have high bias and low variance, while direct strategies are said to have low bias and high variance. We revisit this belief by decomposing the expected multi-step forecast error into three parts: irreducible noise, a structural approximation gap, and an estimation-variance term. For linear predictors we show that the structural gap is identically zero for any dataset. For nonlinear predictors, however, the repeated composition used in recursion can increase model expressivity, making the structural gap depend on both the model and the data. We further show that the estimation variance of the recursive strategy at any horizon can be written as the one-step variance multiplied by a Jacobian-based amplification factor that measures how sensitive the composed predictor is to parameter error. This perspective explains when recursive forecasting may simultaneously have lower bias and higher variance than direct forecasting. Experiments with multilayer perceptrons on the ETTm1 dataset confirm these findings. The results offer practical guidance for choosing between recursive and direct strategies based on model nonlinearity and noise characteristics, rather than relying on traditional bias-variance intuition.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤šæ­¥æ—¶é—´åºåˆ—é¢„æµ‹ä¸­çš„è®¤è¯†é”™è¯¯åˆ†è§£(Epistemic Error Decomposition)ï¼Œé‡æ–°å®¡è§†äº†é€’å½’ç­–ç•¥(Recursive Strategies)ä¸ç›´æ¥ç­–ç•¥(Direct Strategies)åœ¨åå·®-æ–¹å·®(Bias-Variance)æƒè¡¡ä¸Šçš„ä¼ ç»Ÿè§‚ç‚¹ã€‚ç ”ç©¶è€…å°†é¢„æœŸçš„å¤šæ­¥é¢„æµ‹è¯¯å·®åˆ†è§£ä¸ºä¸å¯çº¦å™ªå£°ã€ç»“æ„è¿‘ä¼¼å·®è·(Structural Approximation Gap)å’Œä¼°è®¡æ–¹å·®é¡¹ä¸‰éƒ¨åˆ†ã€‚åˆ†æå‘ç°ï¼Œçº¿æ€§é¢„æµ‹å™¨çš„ç»“æ„å·®è·åœ¨ä»»ä½•æ•°æ®é›†ä¸­å‡ä¸ºé›¶ï¼Œè€Œå¯¹äºéçº¿æ€§é¢„æµ‹å™¨ï¼Œé€’å½’ç»„åˆä¼šå¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼Œä½¿ç»“æ„å·®è·å–å†³äºæ¨¡å‹å’Œæ•°æ®ã€‚ç ”ç©¶è¿›ä¸€æ­¥æ¨å¯¼å‡ºé€’å½’ç­–ç•¥çš„ä¼°è®¡æ–¹å·®å¯è¡¨ç¤ºä¸ºå•æ­¥æ–¹å·®ä¸é›…å¯æ¯”(Jacobian-based)æ”¾å¤§å› å­çš„ä¹˜ç§¯ï¼Œç”¨ä»¥è¡¡é‡é¢„æµ‹å™¨å¯¹å‚æ•°è¯¯å·®çš„æ•æ„Ÿæ€§ã€‚è¿™ä¸€è§†è§’è§£é‡Šäº†é€’å½’é¢„æµ‹åœ¨ç‰¹å®šæƒ…å†µä¸‹å¯èƒ½åŒæ—¶è¡¨ç°å‡ºæ›´ä½åå·®å’Œæ›´é«˜æ–¹å·®çš„åŸå› ã€‚é€šè¿‡åœ¨ETTm1æ•°æ®é›†ä¸Šä½¿ç”¨å¤šå±‚æ„ŸçŸ¥æœº(MLP)çš„å®éªŒï¼Œè¯¥ç ”ç©¶éªŒè¯äº†ç†è®ºæ¨å¯¼ï¼Œå¹¶ä¸ºæ ¹æ®æ¨¡å‹éçº¿æ€§å’Œå™ªå£°ç‰¹å¾é€‰æ‹©é¢„æµ‹ç­–ç•¥æä¾›äº†è¶…è¶Šä¼ ç»Ÿç›´è§‰çš„å®è·µæŒ‡å¯¼ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "2025 EIML Eurips Workshop, 6 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.11461v1",
      "published_date": "2025-11-14 16:32:42 UTC",
      "updated_date": "2025-11-14 16:32:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:16:50.376095+00:00"
    },
    {
      "arxiv_id": "2511.11439v1",
      "title": "Retrofit: Continual Learning with Bounded Forgetting for Security Applications",
      "title_zh": "Retrofitï¼šé¢å‘å®‰å…¨åº”ç”¨çš„å…·æœ‰æœ‰ç•Œé—å¿˜ç‰¹æ€§çš„æŒç»­å­¦ä¹ ",
      "authors": [
        "Yiling He",
        "Junchi Lei",
        "Hongyu She",
        "Shuo Shao",
        "Xinran Zheng",
        "Yiping Liu",
        "Zhan Qin",
        "Lorenzo Cavallaro"
      ],
      "abstract": "Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference.\n  We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2% to 38.6% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RETROFITï¼Œä¸€ç§æ— éœ€æ•°æ®å›æº¯çš„æŒç»­å­¦ä¹  (Continual Learning) æ–¹æ³•ï¼Œä¸“é—¨ç”¨äºè§£å†³å®‰å…¨åº”ç”¨ä¸­æ·±åº¦å­¦ä¹ æ¨¡å‹å› å¨èƒç¯å¢ƒæ¼”åŒ–è€Œäº§ç”Ÿçš„æ€§èƒ½è¡°å‡é—®é¢˜ã€‚é’ˆå¯¹æ•°æ®æ•æ„Ÿç¯å¢ƒä¸‹æ— æ³•è·å–å†å²æ•°æ®ä»¥åŠæ–°æ—§çŸ¥è¯†æ•´åˆå­˜åœ¨å¹²æ‰°çš„æŒ‘æˆ˜ï¼ŒRETROFIT é€šè¿‡å‚æ•°çº§åˆå¹¶ (Parameter-level Merging) æŠ€æœ¯æ•´åˆæ—§æ¨¡å‹ä¸æ–°å¾®è°ƒæ¨¡å‹çš„çŸ¥è¯†ï¼Œå®ç°äº†æ— éœ€æ—§æ•°æ®çš„çŸ¥è¯†è½¬ç§»ã€‚è¯¥æ–¹æ³•é‡‡ç”¨ä½ç§©å’Œç¨€ç–æ›´æ–° (Low-rank and Sparse Updates) å°†å‚æ•°å˜æ›´é™åˆ¶åœ¨ç‹¬ç«‹å­ç©ºé—´å†…ä»¥å‡å°‘å¹²æ‰°ï¼Œå¹¶å¼•å…¥çŸ¥è¯†ä»²è£ (Knowledge Arbitration) æœºåˆ¶æ ¹æ®æ¨¡å‹ç½®ä¿¡åº¦åŠ¨æ€å¹³è¡¡æ–°æ—§çŸ¥è¯†çš„è´¡çŒ®ã€‚åœ¨æ¶æ„è½¯ä»¶æ£€æµ‹å’ŒäºŒè¿›åˆ¶æ‘˜è¦ (Binary Summarization) ä»»åŠ¡çš„è¯„ä¼°ä¸­ï¼Œå®éªŒè¯æ˜ RETROFIT åœ¨ç¼“è§£é—å¿˜çš„åŒæ—¶ä¿æŒäº†æé«˜çš„é€‚åº”æ€§ï¼Œå…¶æ¶æ„è½¯ä»¶æ£€æµ‹ç•™å­˜è¯„åˆ†ä» 20.2% æå‡è‡³ 38.6%ï¼Œä¸”åœ¨è·¨è¡¨ç¤ºæ³›åŒ–æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰åŸºå‡†æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11439v1",
      "published_date": "2025-11-14 16:07:03 UTC",
      "updated_date": "2025-11-14 16:07:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:16:57.072899+00:00"
    },
    {
      "arxiv_id": "2511.20672v1",
      "title": "MindSET: Advancing Mental Health Benchmarking through Large-Scale Social Media Data",
      "title_zh": "MindSETï¼šé€šè¿‡å¤§è§„æ¨¡ç¤¾äº¤åª’ä½“æ•°æ®æ¨è¿›å¿ƒç†å¥åº·åŸºå‡†ç ”ç©¶",
      "authors": [
        "Saad Mankarious",
        "Ayah Zirikly",
        "Daniel Wiechmann",
        "Elma Kerz",
        "Edward Kempa",
        "Yu Qiao"
      ],
      "abstract": "Social media data has become a vital resource for studying mental health, offering real-time insights into thoughts, emotions, and behaviors that traditional methods often miss. Progress in this area has been facilitated by benchmark datasets for mental health analysis; however, most existing benchmarks have become outdated due to limited data availability, inadequate cleaning, and the inherently diverse nature of social media content (e.g., multilingual and harmful material). We present a new benchmark dataset, \\textbf{MindSET}, curated from Reddit using self-reported diagnoses to address these limitations. The annotated dataset contains over \\textbf{13M} annotated posts across seven mental health conditions, more than twice the size of previous benchmarks. To ensure data quality, we applied rigorous preprocessing steps, including language filtering, and removal of Not Safe for Work (NSFW) and duplicate content. We further performed a linguistic analysis using LIWC to examine psychological term frequencies across the eight groups represented in the dataset. To demonstrate the dataset utility, we conducted binary classification experiments for diagnosis detection using both fine-tuned language models and Bag-of-Words (BoW) features. Models trained on MindSET consistently outperformed those trained on previous benchmarks, achieving up to an \\textbf{18-point} improvement in F1 for Autism detection. Overall, MindSET provides a robust foundation for researchers exploring the intersection of social media and mental health, supporting both early risk detection and deeper analysis of emerging psychological trends.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MindSETï¼Œè¿™æ˜¯ä¸€ä¸ªä» Reddit å¹³å°æ•´ç†çš„å¤§è§„æ¨¡å¿ƒç†å¥åº·åˆ†æåŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ•°æ®é›†é™ˆæ—§ã€è§„æ¨¡æœ‰é™åŠæ•°æ®æ¸…æ´—ä¸è¶³ç­‰æŒ‘æˆ˜ã€‚è¯¥æ•°æ®é›†åŒ…å«è¶…è¿‡ 1300 ä¸‡æ¡æ ‡æ³¨å¸–å­ï¼Œæ¶µç›–ä¸ƒç§ä¸åŒçš„å¿ƒç†å¥åº·çŠ¶å†µï¼Œå…¶è§„æ¨¡æ˜¯æ­¤å‰ç±»ä¼¼åŸºå‡†çš„ä¸¤å€ä»¥ä¸Šã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡è¯­è¨€è¿‡æ»¤ã€NSFW å†…å®¹ç§»é™¤åŠé‡å¤æ¸…ç†ç­‰ä¸¥æ ¼é¢„å¤„ç†ç¡®ä¿æ•°æ®è´¨é‡ï¼Œå¹¶åˆ©ç”¨ LIWC å·¥å…·è¿›è¡Œäº†å¿ƒç†å­¦æœ¯è¯­çš„è¯­è¨€åˆ†æã€‚åœ¨è¯Šæ–­æ£€æµ‹çš„äºŒåˆ†ç±»å®éªŒä¸­ï¼ŒåŸºäº MindSET è®­ç»ƒçš„å¾®è°ƒè¯­è¨€æ¨¡å‹ï¼ˆfine-tuned language modelsï¼‰è¡¨ç°æ˜¾è‘—ä¼˜äºæ—§æœ‰åŸºå‡†ï¼Œå…¶ä¸­åœ¨å­¤ç‹¬ç—‡ï¼ˆAutismï¼‰æ£€æµ‹ä»»åŠ¡ä¸­çš„ F1 åˆ†æ•°æå‡äº†é«˜è¾¾ 18 ä¸ªç™¾åˆ†ç‚¹ã€‚æ€»ä½“è€Œè¨€ï¼ŒMindSET ä¸ºç¤¾äº¤åª’ä½“ä¸å¿ƒç†å¥åº·äº¤å‰é¢†åŸŸçš„æ·±åº¦ç ”ç©¶æä¾›äº†å¼ºæœ‰åŠ›çš„åŸºç¡€ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ”¯æŒæ—©æœŸé£é™©è¯†åˆ«åŠå¿ƒç†è¶‹åŠ¿åˆ†æã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.20672v1",
      "published_date": "2025-11-14 16:06:04 UTC",
      "updated_date": "2025-11-14 16:06:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:16:56.890771+00:00"
    },
    {
      "arxiv_id": "2511.13767v1",
      "title": "Dynamic Temperature Scheduler for Knowledge Distillation",
      "title_zh": "çŸ¥è¯†è’¸é¦çš„åŠ¨æ€æ¸©åº¦è°ƒåº¦å™¨",
      "authors": [
        "Sibgat Ul Islam",
        "Jawad Ibn Ahad",
        "Fuad Rahman",
        "Mohammad Ruhul Amin",
        "Nabeel Mohammed",
        "Shafin Rahman"
      ],
      "abstract": "Knowledge Distillation (KD) trains a smaller student model using a large, pre-trained teacher model, with temperature as a key hyperparameter controlling the softness of output probabilities. Traditional methods use a fixed temperature throughout training, which is suboptimal. Moreover, architectural differences between teacher and student often result in mismatched logit magnitudes. We demonstrate that students benefit from softer probabilities early in training but require sharper probabilities in later stages. We introduce Dynamic Temperature Scheduler (DTS), which adjusts temperature dynamically based on the cross-entropy loss gap between teacher and student. To our knowledge, this is the first temperature scheduling method that adapts based on the divergence between teacher and student distributions. Our method integrates seamlessly with existing KD frameworks. We validate DTS across multiple KD strategies on vision (CIFAR-100, Tiny-ImageNet) and NLP tasks (GLUE, Dolly, SelfIns, UnNI, S-NI), consistently outperforming static-temperature baselines. Code is available at https://github.com/Sibgat-Ul/DTS.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Knowledge Distillation (KD) ä¸­å›ºå®šæ¸©åº¦è¶…å‚æ•°å¯¼è‡´çš„æ€§èƒ½ç“¶é¢ˆï¼Œæå‡ºäº† Dynamic Temperature Scheduler (DTS) æ¡†æ¶ã€‚ç ”ç©¶å‘ç°å­¦ç”Ÿæ¨¡å‹åœ¨è®­ç»ƒåˆæœŸå—ç›Šäº softer æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œåœ¨åæœŸåˆ™éœ€è¦ sharper åˆ†å¸ƒï¼Œä»¥è§£å†³æ•™å¸ˆä¸å­¦ç”Ÿæ¨¡å‹é—´å› æ¶æ„å·®å¼‚äº§ç”Ÿçš„ logit å¹…åº¦ä¸åŒ¹é…é—®é¢˜ã€‚DTS åˆ›æ–°æ€§åœ°æ ¹æ®æ•™å¸ˆä¸å­¦ç”Ÿæ¨¡å‹é—´çš„ cross-entropy loss å·®è·åŠ¨æ€è°ƒæ•´æ¸©åº¦ï¼Œæ˜¯é¦–ä¸ªåŸºäºåˆ†å¸ƒæ•£åº¦è¿›è¡Œè‡ªé€‚åº”çš„è°ƒåº¦æ–¹æ³•ã€‚è¯¥æ–¹æ¡ˆèƒ½å¤Ÿæ— ç¼é›†æˆåˆ°ç°æœ‰çš„ KD æ¡†æ¶ä¸­ï¼Œå¹¶åœ¨ CIFAR-100ã€Tiny-ImageNet åŠ GLUE ç­‰å¤šé¡¹ vision å’Œ NLP ä»»åŠ¡ä¸­è¿›è¡Œäº†éªŒè¯ã€‚å®éªŒç»“æœè¯å®ï¼ŒDTS åœ¨å„æ•°æ®é›†ä¸Šå‡æŒç»­ä¼˜äºå›ºå®šæ¸©åº¦çš„åŸºå‡†æ–¹æ³•ï¼Œæ˜¾è‘—å¢å¼ºäº†è’¸é¦æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13767v1",
      "published_date": "2025-11-14 16:03:22 UTC",
      "updated_date": "2025-11-14 16:03:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:17:18.184329+00:00"
    },
    {
      "arxiv_id": "2511.11435v1",
      "title": "The Persistence of Cultural Memory: Investigating Multimodal Iconicity in Diffusion Models",
      "title_zh": "æ–‡åŒ–è®°å¿†çš„æŒä¹…æ€§ï¼šæ¢ç©¶æ‰©æ•£æ¨¡å‹ä¸­çš„å¤šæ¨¡æ€åƒä¼¼æ€§",
      "authors": [
        "Maria-Teresa De Rosa Palmini",
        "Eva Cetinic"
      ],
      "abstract": "Our work addresses the ambiguity between generalization and memorization in text-to-image diffusion models, focusing on a specific case we term multimodal iconicity. This refers to instances where images and texts evoke culturally shared associations, such as when a title recalls a familiar artwork or film scene. While prior research on memorization and unlearning emphasizes forgetting, we examine what is remembered and how, focusing on the balance between recognizing cultural references and reproducing them. We introduce an evaluation framework that separates recognition, whether a model identifies a reference, from realization, how it depicts it through replication or reinterpretation, quantified through measures capturing both dimensions. By evaluating five diffusion models across 767 Wikidata-derived cultural references spanning static and dynamic imagery, we show that our framework distinguishes replication from transformation more effectively than existing similarity-based methods. To assess linguistic sensitivity, we conduct prompt perturbation experiments using synonym substitutions and literal image descriptions, finding that models often reproduce iconic visual structures even when textual cues are altered. Finally, our analysis shows that cultural alignment correlates not only with training data frequency, but also textual uniqueness, reference popularity, and creation date. Our work reveals that the value of diffusion models lies not only in what they reproduce but in how they transform and recontextualize cultural knowledge, advancing evaluation beyond simple text-image matching toward richer contextual understanding.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ–‡ç”Ÿå›¾æ‰©æ•£æ¨¡å‹(text-to-image diffusion models)ä¸­æ³›åŒ–ä¸è®°å¿†ä¹‹é—´çš„æ¨¡ç³Šè¾¹ç•Œï¼Œé‡ç‚¹åˆ†æäº†æ¨¡å‹å¤„ç†å…·æœ‰æ–‡åŒ–å…±äº«å…³è”çš„â€œå¤šæ¨¡æ€å›¾ç¬¦æ€§â€(multimodal iconicity)ç°è±¡ã€‚ä½œè€…æå‡ºäº†ä¸€å¥—å…¨æ–°çš„è¯„ä¼°æ¡†æ¶ï¼Œé€šè¿‡åŒºåˆ†â€œè¯†åˆ«â€(recognition)ä¸â€œå®ç°â€(realization)æ¥é‡åŒ–æ¨¡å‹å¯¹æ–‡åŒ–å‚è€ƒå†…å®¹çš„å¤åˆ¶(replication)æˆ–é‡æ„(transformation)ç¨‹åº¦ã€‚åœ¨å¯¹äº”ä¸ªæ‰©æ•£æ¨¡å‹åŠ767ä¸ªæºè‡ªWikidataçš„æ–‡åŒ–å‚è€ƒæ¡ˆä¾‹è¿›è¡Œè¯„ä¼°åï¼Œç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶åœ¨åŒºåˆ†å¤åˆ¶ä¸è½¬åŒ–æ–¹é¢ä¼˜äºä¼ ç»Ÿçš„ç›¸ä¼¼æ€§æµ‹é‡æ–¹æ³•ã€‚å®éªŒé€šè¿‡æç¤ºè¯æ‰°åŠ¨(prompt perturbation)å‘ç°ï¼Œå³ä½¿åœ¨æ”¹å˜æ–‡æœ¬æç¤ºçš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä»å€¾å‘äºé‡ç°æ ‡å¿—æ€§çš„è§†è§‰ç»“æ„ã€‚åˆ†æè¡¨æ˜ï¼Œæ–‡åŒ–å¯¹é½(cultural alignment)ä¸ä»…å—è®­ç»ƒé¢‘ç‡å½±å“ï¼Œè¿˜ä¸æ–‡æœ¬å”¯ä¸€æ€§ã€å‚è€ƒçŸ¥ååº¦åŠåˆ›ä½œæ—¥æœŸå¯†åˆ‡ç›¸å…³ã€‚è¯¥å·¥ä½œæ­ç¤ºäº†æ‰©æ•£æ¨¡å‹åœ¨é‡æ„æ–‡åŒ–çŸ¥è¯†æ–¹é¢çš„æ½œåŠ›ï¼Œæ¨åŠ¨äº†è¯„ä¼°æ–¹æ³•ä»ç®€å•çš„æ–‡æœ¬-å›¾åƒåŒ¹é…å‘æ›´æ·±å±‚çš„è¯­å¢ƒç†è§£æ¼”è¿›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11435v1",
      "published_date": "2025-11-14 16:03:10 UTC",
      "updated_date": "2025-11-14 16:03:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:17:11.066801+00:00"
    },
    {
      "arxiv_id": "2511.11423v1",
      "title": "CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction",
      "title_zh": "CURENetï¼šèåˆç»Ÿä¸€è¡¨å¾çš„é«˜æ•ˆæ…¢æ€§ç—…é¢„æµ‹",
      "authors": [
        "Cong-Tinh Dao",
        "Nguyen Minh Thao Phan",
        "Jun-En Ding",
        "Chenwei Wu",
        "David Restrepo",
        "Dongsheng Luo",
        "Fanyi Zhao",
        "Chun-Chieh Liao",
        "Wen-Chih Peng",
        "Chi-Te Wang",
        "Pei-Fu Chen",
        "Ling Chen",
        "Xinglong Ju",
        "Feng Liu",
        "Fang-Ming Hung"
      ],
      "abstract": "Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µå­å¥åº·è®°å½• (Electronic Health Records, EHR) ä¸­å¤šæ¨¡æ€æ•°æ®äº¤äº’åŠæ—¶é—´æ¨¡å¼æ•æ‰ä¸è¶³çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† CURENet æ¨¡å‹ã€‚è¯¥æ¨¡å‹æ—¨åœ¨é€šè¿‡æ•´åˆéç»“æ„åŒ–ä¸´åºŠç¬”è®°ã€å®éªŒå®¤æ£€æŸ¥å’Œæ‚£è€…çš„æ—¶é—´åºåˆ—å°±è¯Šæ•°æ®ï¼Œå®ç°å¯¹æ…¢æ€§ç—…çš„é«˜æ•ˆé¢„æµ‹ã€‚CURENet åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (Large Language Models, LLMs) å¤„ç†ä¸´åºŠæ–‡æœ¬å’Œå®éªŒå®¤æ•°æ®ï¼Œå¹¶ç»“åˆ transformer encoders å¯¹çºµå‘åºåˆ—å°±è¯Šè¿›è¡Œå»ºæ¨¡ï¼Œä»¥æ•æ‰ä¸´åºŠæ•°æ®é—´å¤æ‚çš„ç›¸äº’ä½œç”¨ã€‚å®éªŒåœ¨å…¬å¼€çš„ MIMIC-III å’Œç§æœ‰ FEMH æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤º CURENet åœ¨å¤šæ ‡ç­¾æ¡†æ¶ä¸‹é¢„æµ‹å‰ 10 ç§æ…¢æ€§ç—…çš„å‡†ç¡®ç‡è¶…è¿‡äº† 94%ã€‚è¯¥ç ”ç©¶è¯æ˜äº†å¤šæ¨¡æ€ EHR æ•´åˆåœ¨å¢å¼ºä¸´åºŠå†³ç­–æ”¯æŒå’Œæ”¹å–„æ‚£è€…é¢„åæ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11423v1",
      "published_date": "2025-11-14 15:52:22 UTC",
      "updated_date": "2025-11-14 15:52:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:17:07.077245+00:00"
    },
    {
      "arxiv_id": "2511.11397v1",
      "title": "Variational Quantum Algorithms for Particle Track Reconstruction",
      "title_zh": "ç”¨äºç²’å­å¾„è¿¹é‡å»ºçš„å˜åˆ†é‡å­ç®—æ³•",
      "authors": [
        "Vincenzo Lipardi",
        "Xenofon Chiotopoulos",
        "Jacco A. de Vries",
        "Domenica Dibenedetto",
        "Kurt Driessens",
        "Marcel Merk",
        "Mark H. M. Winands"
      ],
      "abstract": "Quantum Computing is a rapidly developing field with the potential to tackle the increasing computational challenges faced in high-energy physics. In this work, we explore the potential and limitations of variational quantum algorithms in solving the particle track reconstruction problem. We present an analysis of two distinct formulations for identifying straight-line tracks in a multilayer detection system, inspired by the LHCb vertex detector. The first approach is formulated as a ground-state energy problem, while the second approach is formulated as a system of linear equations. This work addresses one of the main challenges when dealing with variational quantum algorithms on general problems, namely designing an expressive and efficient quantum ansatz working on tracking events with fixed detector geometry. For this purpose, we employed a quantum architecture search method based on Monte Carlo Tree Search to design the quantum circuits for different problem sizes. We provide experimental results to test our approach on both formulations for different problem sizes in terms of performance and computational cost.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†Variational Quantum Algorithmsåœ¨è§£å†³é«˜èƒ½ç‰©ç†ä¸­ç²’å­å¾„è¿¹é‡å»º(Particle Track Reconstruction)é—®é¢˜çš„æ½œåŠ›å’Œå±€é™æ€§ã€‚é’ˆå¯¹å—LHCbé¡¶ç‚¹æ¢æµ‹å™¨å¯å‘çš„æ¢æµ‹ç³»ç»Ÿï¼Œä½œè€…æå‡ºäº†ä¸¤ç§è¯†åˆ«ç›´çº¿å¾„è¿¹çš„æ•°å­¦è¡¨è¿°ï¼Œåˆ†åˆ«å°†å…¶å»ºæ¨¡ä¸ºåŸºæ€èƒ½é‡é—®é¢˜(Ground-state energy problem)å’Œçº¿æ€§æ–¹ç¨‹ç»„(System of linear equations)ã€‚ä¸ºäº†è§£å†³å˜åˆ†é‡å­ç®—æ³•åœ¨å¤„ç†å›ºå®šæ¢æµ‹å™¨å‡ ä½•å½¢çŠ¶æ—¶é¢ä¸´çš„Ansatzè®¾è®¡éš¾é¢˜ï¼Œç ”ç©¶å¼•å…¥äº†åŸºäºMonte Carlo Tree Searchçš„é‡å­æ¶æ„æœç´¢(Quantum Architecture Search)æ–¹æ³•ï¼Œä»¥è‡ªåŠ¨ç”Ÿæˆé«˜æ•ˆä¸”è¡¨è¾¾åŠ›å¼ºçš„é‡å­ç”µè·¯ã€‚é€šè¿‡åœ¨ä¸åŒè§„æ¨¡é—®é¢˜ä¸Šè¿›è¡Œçš„å®éªŒæµ‹è¯•ï¼Œè¯¥ç ”ç©¶è¯¦ç»†åˆ†æäº†ä¸¤ç§è¡¨è¿°æ–¹æ¡ˆåœ¨æ€§èƒ½å’Œè®¡ç®—æˆæœ¬ä¸Šçš„è¡¨ç°ï¼Œä¸ºåˆ©ç”¨é‡å­è®¡ç®—åº”å¯¹é«˜èƒ½ç‰©ç†é¢†åŸŸæ—¥ç›Šå¢é•¿çš„è®¡ç®—æŒ‘æˆ˜æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "quant-ph",
        "cs.AI"
      ],
      "primary_category": "quant-ph",
      "comment": "17 pages, 5 figures, 2 tables, pre-proceedings BNAIC 2024",
      "pdf_url": "https://arxiv.org/pdf/2511.11397v1",
      "published_date": "2025-11-14 15:24:59 UTC",
      "updated_date": "2025-11-14 15:24:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:17:10.570971+00:00"
    },
    {
      "arxiv_id": "2511.11393v1",
      "title": "Robust and Efficient Communication in Multi-Agent Reinforcement Learning",
      "title_zh": "å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„é²æ£’é«˜æ•ˆé€šä¿¡",
      "authors": [
        "Zejiao Liu",
        "Yi Li",
        "Jiali Wang",
        "Junqi Tu",
        "Yitian Hong",
        "Fangfei Li",
        "Yang Liu",
        "Toshiharu Sugawara",
        "Yang Tang"
      ],
      "abstract": "Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿå›é¡¾äº†å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹  (Multi-Agent Reinforcement Learning, MARL) åœ¨ç°å®çº¦æŸä¸‹å®ç°ç¨³å¥ä¸”é«˜æ•ˆé€šä¿¡ç­–ç•¥çš„æœ€æ–°è¿›å±•ã€‚è¯¥ç ”ç©¶é’ˆå¯¹ç°æœ‰ç†è®ºæ¨¡å‹ä¸­é€šä¿¡ç†æƒ³åŒ–ä¸å®é™…éƒ¨ç½²ä¸­æ¶ˆæ¯æ‰°åŠ¨ (message perturbations)ã€ä¼ è¾“å»¶è¿Ÿ (transmission delays) åŠå¸¦å®½é™åˆ¶ (limited bandwidth) ä¹‹é—´çš„çŸ›ç›¾è¿›è¡Œäº†æ·±å…¥æ¢è®¨ã€‚æ–‡ç« é‡ç‚¹åˆ†æäº†åä½œå¼è‡ªåŠ¨é©¾é©¶ (cooperative autonomous driving)ã€åˆ†å¸ƒå¼åŒæ­¥å®šä½ä¸åœ°å›¾æ„å»º (SLAM) ä»¥åŠè”é‚¦å­¦ä¹  (federated learning) ç­‰å…³é”®åº”ç”¨ä¸­çš„ä½å»¶è¿Ÿå¯é æ€§ä¸éšç§æƒè¡¡é—®é¢˜ã€‚æœ€åï¼Œä½œè€…æŒ‡å‡ºäº†è¯¥é¢†åŸŸçš„å…¬å¼€æŒ‘æˆ˜å¹¶æ˜ç¡®äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ï¼Œå€¡å¯¼é€šè¿‡é€šä¿¡ã€å­¦ä¹ ä¸ç¨³å¥æ€§çš„ååŒè®¾è®¡æ¥ç¼©å°ç†è®ºæ¨¡å‹ä¸å®é™…åº”ç”¨ä¹‹é—´çš„å·®è·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11393v1",
      "published_date": "2025-11-14 15:23:11 UTC",
      "updated_date": "2025-11-14 15:23:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:17:12.288090+00:00"
    },
    {
      "arxiv_id": "2511.13766v1",
      "title": "Credal Ensemble Distillation for Uncertainty Quantification",
      "title_zh": "ç”¨äºä¸ç¡®å®šæ€§é‡åŒ–çš„ä¿¡åº¦é›†æˆè’¸é¦",
      "authors": [
        "Kaizheng Wang",
        "Fabio Cuzzolin",
        "David Moens",
        "Hans Hallez"
      ],
      "abstract": "Deep ensembles (DE) have emerged as a powerful approach for quantifying predictive uncertainty and distinguishing its aleatoric and epistemic components, thereby enhancing model robustness and reliability. However, their high computational and memory costs during inference pose significant challenges for wide practical deployment. To overcome this issue, we propose credal ensemble distillation (CED), a novel framework that compresses a DE into a single model, CREDIT, for classification tasks. Instead of a single softmax probability distribution, CREDIT predicts class-wise probability intervals that define a credal set, a convex set of probability distributions, for uncertainty quantification. Empirical results on out-of-distribution detection benchmarks demonstrate that CED achieves superior or comparable uncertainty estimation compared to several existing baselines, while substantially reducing inference overhead compared to DE.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ·±åº¦é›†æˆ(Deep Ensembles, DE)åœ¨æ¨ç†é˜¶æ®µé¢ä¸´çš„é«˜è®¡ç®—å’Œå†…å­˜æˆæœ¬æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¿¡åº¦é›†æˆè’¸é¦(Credal Ensemble Distillation, CED)æ¡†æ¶ã€‚è¯¥æ¡†æ¶æ—¨åœ¨å°†DEå‹ç¼©ä¸ºåä¸ºCREDITçš„å•ä¸ªåˆ†ç±»æ¨¡å‹ï¼Œé€šè¿‡é¢„æµ‹ç±»åˆ«æ¦‚ç‡åŒºé—´æ¥å®šä¹‰ä¿¡åº¦é›†(Credal Set)ï¼Œå³æ¦‚ç‡åˆ†å¸ƒçš„å‡¸é›†ã€‚è¿™ç§æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåŒºåˆ†å¶ç„¶ä¸ç¡®å®šæ€§å’Œè®¤çŸ¥ä¸ç¡®å®šæ€§ï¼Œæä¾›æ›´ç¨³å¥çš„é¢„æµ‹ç»“æœã€‚åœ¨åˆ†å¸ƒå¤–æ£€æµ‹(Out-of-Distribution, OOD)åŸºå‡†æµ‹è¯•ä¸­çš„å®è¯ç»“æœæ˜¾ç¤ºï¼ŒCEDåœ¨ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹é¢è¾¾åˆ°äº†ä¸å¤šä¸ªåŸºå‡†æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜çš„æ°´å¹³ã€‚ä¸åŸå§‹çš„DEç›¸æ¯”ï¼ŒCEDåœ¨ç¡®ä¿æ¨¡å‹å¯é æ€§çš„åŒæ—¶æ˜¾è‘—é™ä½äº†æ¨ç†å¼€é”€ï¼Œä¸ºä¸ç¡®å®šæ€§é‡åŒ–æŠ€æœ¯çš„å®é™…éƒ¨ç½²æä¾›äº†é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "An extended version for Credal Ensemble Distillation for Uncertainty Quantification, which has been accepted for publication at AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.13766v1",
      "published_date": "2025-11-14 14:53:42 UTC",
      "updated_date": "2025-11-14 14:53:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:17:18.665569+00:00"
    },
    {
      "arxiv_id": "2511.11373v1",
      "title": "MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism",
      "title_zh": "MarsRLï¼šé€šè¿‡æ™ºèƒ½ä½“æµæ°´çº¿å¹¶è¡Œå¼ºåŒ–å­¦ä¹ æå‡å¤šæ™ºèƒ½ä½“æ¨ç†ç³»ç»Ÿ",
      "authors": [
        "Shulin Liu",
        "Dong Du",
        "Tao Yang",
        "Yang Li",
        "Boyu Qiu"
      ],
      "abstract": "Recent progress in large language models (LLMs) has been propelled by reinforcement learning with verifiable rewards (RLVR) and test-time scaling. However, the limited output length of LLMs constrains the depth of reasoning attainable in a single inference process. Multi-agent reasoning systems offer a promising alternative by employing multiple agents including Solver, Verifier, and Corrector, to iteratively refine solutions. While effective in closed-source models like Gemini 2.5 Pro, they struggle to generalize to open-source models due to insufficient critic and correction capabilities. To address this, we propose MarsRL, a novel reinforcement learning framework with agentic pipeline parallelism, designed to jointly optimize all agents in the system. MarsRL introduces agent-specific reward mechanisms to mitigate reward noise and employs pipeline-inspired training to enhance efficiency in handling long trajectories. Applied to Qwen3-30B-A3B-Thinking-2507, MarsRL improves AIME2025 accuracy from 86.5% to 93.3% and BeyondAIME from 64.9% to 73.8%, even surpassing Qwen3-235B-A22B-Thinking-2507. These findings highlight the potential of MarsRL to advance multi-agent reasoning systems and broaden their applicability across diverse reasoning tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MarsRLï¼Œä¸€ç§ç»“åˆæ™ºèƒ½ä½“æµæ°´çº¿å¹¶è¡Œ (Agentic Pipeline Parallelism) çš„æ–°å‹å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œæ—¨åœ¨ååŒä¼˜åŒ–å¤šæ™ºèƒ½ä½“æ¨ç†ç³»ç»Ÿä¸­çš„ Solverã€Verifier å’Œ Correctorã€‚é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) å•æ¬¡æ¨ç†é•¿åº¦å—é™ä»¥åŠå¼€æºæ¨¡å‹åœ¨åˆ¤åˆ«ä¸çº é”™èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼ŒMarsRL å¼•å…¥äº†ç‰¹å®šäºæ™ºèƒ½ä½“çš„å¥–åŠ±æœºåˆ¶ä»¥å‡å°‘å¥–åŠ±å™ªå£°ï¼Œå¹¶åˆ©ç”¨æµæ°´çº¿å¼è®­ç»ƒæå‡å¤„ç†é•¿è½¨è¿¹çš„æ•ˆç‡ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¡†æ¶å°† Qwen3-30B-A3B-Thinking-2507 çš„ AIME2025 å‡†ç¡®ç‡ä» 86.5% æå‡è‡³ 93.3%ï¼Œå¹¶åœ¨ BeyondAIME åŸºå‡†ä¸Šå®ç°äº†ä» 64.9% åˆ° 73.8% çš„å¢é•¿ã€‚MarsRL çš„æ¨ç†æ€§èƒ½ç”šè‡³è¶…è¶Šäº†è§„æ¨¡æ›´å¤§çš„ Qwen3-235B-A22B-Thinking-2507 æ¨¡å‹ã€‚è¿™äº›ç»“æœå……åˆ†è¯æ˜äº† MarsRL åœ¨æå‡å¤šæ™ºèƒ½ä½“æ¨ç†ç³»ç»Ÿæ€§èƒ½åŠæ‰©å±•å¤æ‚æ¨ç†ä»»åŠ¡é€‚ç”¨æ€§æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.11373v1",
      "published_date": "2025-11-14 14:52:34 UTC",
      "updated_date": "2025-11-14 14:52:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:17:38.079320+00:00"
    },
    {
      "arxiv_id": "2511.16688v1",
      "title": "Prompt-Based Value Steering of Large Language Models",
      "title_zh": "åŸºäºæç¤ºçš„å¤§è¯­è¨€æ¨¡å‹ä»·å€¼å¼•å¯¼",
      "authors": [
        "Giulio Antonio Abbo",
        "Tony Belpaeme"
      ],
      "abstract": "Large language models are increasingly used in applications where alignment with human values is critical. While model fine-tuning is often employed to ensure safe responses, this technique is static and does not lend itself to everyday situations involving dynamic values and preferences. In this paper, we present a practical, reproducible, and model-agnostic procedure to evaluate whether a prompt candidate can effectively steer generated text toward specific human values, formalising a scoring method to quantify the presence and gain of target values in generated responses. We apply our method to a variant of the Wizard-Vicuna language model, using Schwartz's theory of basic human values and a structured evaluation through a dialogue dataset. With this setup, we compare a baseline prompt to one explicitly conditioned on values, and show that value steering is possible even without altering the model or dynamically optimising prompts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨äººç±»ä»·å€¼è§‚å¯¹é½ä¸­é¢ä¸´çš„é™æ€å¾®è°ƒï¼ˆFine-tuningï¼‰å±€é™æ€§ï¼Œæå‡ºäº†ä¸€ç§å®ç”¨ã€å¯é‡ç°ä¸”ä¸æ¨¡å‹æ— å…³çš„è¯„ä¼°ç¨‹åºï¼Œæ—¨åœ¨éªŒè¯æç¤ºè¯ï¼ˆPromptï¼‰å€™é€‰è€…æ˜¯å¦èƒ½æœ‰æ•ˆå¼•å¯¼ç”Ÿæˆæ–‡æœ¬è¶‹å‘ç‰¹å®šäººç±»ä»·å€¼è§‚ã€‚ç ”ç©¶è€…å½¢å¼åŒ–äº†ä¸€å¥—è¯„åˆ†æ–¹æ³•ï¼Œç”¨ä»¥é‡åŒ–ç”Ÿæˆå›å¤ä¸­ç›®æ ‡ä»·å€¼è§‚çš„å­˜åœ¨åŠå…¶å¢ç›Šæƒ…å†µã€‚åœ¨å®éªŒé˜¶æ®µï¼Œè¯¥ç ”ç©¶åŸºäº Schwartz çš„åŸºæœ¬äººç±»ä»·å€¼è§‚ç†è®ºï¼ˆSchwartz's theory of basic human valuesï¼‰ï¼Œå¯¹ Wizard-Vicuna æ¨¡å‹çš„å˜ä½“åœ¨å¯¹è¯æ•°æ®é›†ä¸Šè¿›è¡Œäº†ç»“æ„åŒ–è¯„ä¼°ã€‚é€šè¿‡å°†åŸºå‡†æç¤ºè¯ä¸æ˜¾å¼åŒ…å«ä»·å€¼è§‚æ¡ä»¶çš„æç¤ºè¯è¿›è¡Œå¯¹æ¯”ï¼Œå®éªŒç»“æœè¯æ˜äº†åœ¨ä¸æ”¹å˜æ¨¡å‹æ¶æ„æˆ–è¿›è¡ŒåŠ¨æ€æç¤ºè¯ä¼˜åŒ–çš„æƒ…å†µä¸‹ï¼Œå®ç° Prompt-Based Value Steering æ˜¯åˆ‡å®å¯è¡Œçš„ã€‚è¿™ä¸€å‘ç°ä¸ºåœ¨åŠ¨æ€åº”ç”¨åœºæ™¯ä¸­çµæ´»è°ƒæ•´æ¨¡å‹çš„ä»·å€¼åå¥½æä¾›äº†é‡è¦çš„å‚è€ƒè·¯å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "9 pages, 1 figure, 4 tables. Presented at the 3rd International Workshop on Value Engineering in AI (VALE 2025), 28th European Conference on AI. To appear in Springer LNCS",
      "pdf_url": "https://arxiv.org/pdf/2511.16688v1",
      "published_date": "2025-11-14 14:45:41 UTC",
      "updated_date": "2025-11-14 14:45:41 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:17:33.486278+00:00"
    },
    {
      "arxiv_id": "2511.11357v2",
      "title": "KarmaTS: A Universal Simulation Platform for Multivariate Time Series with Functional Causal Dynamics",
      "title_zh": "KarmaTSï¼šå…·æœ‰å‡½æ•°å¼å› æœåŠ¨åŠ›å­¦çš„å¤šå…ƒæ—¶é—´åºåˆ—é€šç”¨ä»¿çœŸå¹³å°",
      "authors": [
        "Haixin Li",
        "Yanke Li",
        "Diego Paez-Granados"
      ],
      "abstract": "We introduce KarmaTS, an interactive framework for constructing lag-indexed, executable spatiotemporal causal graphical models for multivariate time series (MTS) simulation. Motivated by the challenge of access-restricted physiological data, KarmaTS generates synthetic MTS with known causal dynamics and augments real-world datasets with expert knowledge. The system constructs a discrete-time structural causal process (DSCP) by combining expert knowledge and algorithmic proposals in a mixed-initiative, human-in-the-loop workflow. The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts. KarmaTS handles mixed variable types, contemporaneous and lagged edges, and modular edge functionals ranging from parameterizable templates to neural network models. Together, these features enable flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†KarmaTSï¼Œä¸€ä¸ªç”¨äºå¤šå˜é‡æ—¶é—´åºåˆ—ï¼ˆMultivariate Time Series, MTSï¼‰æ¨¡æ‹Ÿçš„é€šç”¨äº¤äº’å¼æ¡†æ¶ã€‚é’ˆå¯¹ç”Ÿç†æ•°æ®è·å–å—é™ç­‰æŒ‘æˆ˜ï¼ŒKarmaTSé€šè¿‡æ„å»ºå¸¦æœ‰æ»åç´¢å¼•çš„å¯æ‰§è¡Œæ—¶ç©ºå› æœå›¾æ¨¡å‹ï¼Œç”Ÿæˆå…·æœ‰å·²çŸ¥å› æœåŠ¨æ€çš„åˆæˆæ•°æ®å¹¶åˆ©ç”¨ä¸“å®¶çŸ¥è¯†å¢å¼ºçœŸå®æ•°æ®é›†ã€‚ç³»ç»Ÿæ ¸å¿ƒé‡‡ç”¨ç¦»æ•£æ—¶é—´ç»“æ„å› æœè¿‡ç¨‹ï¼ˆDiscrete-time Structural Causal Process, DSCPï¼‰ï¼Œåœ¨äººæœºåä½œï¼ˆHuman-in-the-loopï¼‰çš„å·¥ä½œæµä¸­ç»“åˆäº†ä¸“å®¶æ´å¯Ÿä¸ç®—æ³•å»ºè®®ã€‚KarmaTSä¸ä»…æ”¯æŒå› æœå¹²é¢„å’Œç‰¹å®šåˆ†å¸ƒåç§»ä¸‹çš„æ•°æ®æ¨¡æ‹Ÿï¼Œè¿˜èƒ½å¤„ç†æ··åˆå˜é‡ç±»å‹ä»¥åŠä»å‚æ•°åŒ–æ¨¡æ¿åˆ°ç¥ç»ç½‘ç»œæ¨¡å‹çš„æ¨¡å—åŒ–è¾¹ç¼˜æ³›å‡½ã€‚è¿™äº›ç‰¹æ€§ä½¿å¾—è¯¥å¹³å°èƒ½å¤Ÿä¸ºå› æœå‘ç°ç®—æ³•ï¼ˆCausal Discovery Algorithmsï¼‰çš„éªŒè¯å’ŒåŸºå‡†æµ‹è¯•æä¾›çµæ´»ä¸”å¯é çš„æ¨¡æ‹Ÿç¯å¢ƒã€‚",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11357v2",
      "published_date": "2025-11-14 14:44:14 UTC",
      "updated_date": "2025-12-18 16:47:05 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:17:41.290425+00:00"
    },
    {
      "arxiv_id": "2511.11784v2",
      "title": "NegBLEURT Forest: Leveraging Inconsistencies for Detecting Jailbreak Attacks",
      "title_zh": "NegBLEURT Forestï¼šåˆ©ç”¨ä¸ä¸€è‡´æ€§æ£€æµ‹è¶Šç‹±æ”»å‡»",
      "authors": [
        "Lama Sleem",
        "Jerome Francois",
        "Lujun Li",
        "Nathan Foucher",
        "Niccolo Gentile",
        "Radu State"
      ],
      "abstract": "Jailbreak attacks designed to bypass safety mechanisms pose a serious threat by prompting LLMs to generate harmful or inappropriate content, despite alignment with ethical guidelines. Crafting universal filtering rules remains difficult due to their inherent dependence on specific contexts. To address these challenges without relying on threshold calibration or model fine-tuning, this work introduces a semantic consistency analysis between successful and unsuccessful responses, demonstrating that a negation-aware scoring approach captures meaningful patterns. Building on this insight, a novel detection framework called NegBLEURT Forest is proposed to evaluate the degree of alignment between outputs elicited by adversarial prompts and expected safe behaviors. It identifies anomalous responses using the Isolation Forest algorithm, enabling reliable jailbreak detection. Experimental results show that the proposed method consistently achieves top-tier performance, ranking first or second in accuracy across diverse models using the crafted dataset, while competing approaches exhibit notable sensitivity to model and data variations.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ LLMs é¢ä¸´çš„ç»•è¿‡å®‰å…¨æœºåˆ¶ç”Ÿæˆæœ‰å®³å†…å®¹çš„ Jailbreak æ”»å‡»ï¼Œæå‡ºäº†ä¸€ç§æ— éœ€é˜ˆå€¼æ ¡å‡†æˆ–æ¨¡å‹å¾®è°ƒçš„æ–°å‹æ£€æµ‹æ–¹æ³•ã€‚é€šè¿‡å¯¹æˆåŠŸä¸å¤±è´¥å“åº”è¿›è¡Œè¯­ä¹‰ä¸€è‡´æ€§åˆ†æï¼Œç ”ç©¶è¯æ˜é‡‡ç”¨ negation-aware è¯„åˆ†æ–¹å¼èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰åˆ°æœ‰æ„ä¹‰çš„å¼‚å¸¸æ¨¡å¼ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œä½œè€…æå‡ºäº†åä¸º NegBLEURT Forest çš„æ£€æµ‹æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å¯¹æŠ—æ€§æç¤ºè¯±å‘çš„è¾“å‡ºä¸é¢„æœŸå®‰å…¨è¡Œä¸ºä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ Isolation Forest ç®—æ³•è¯†åˆ«å¼‚å¸¸å“åº”ï¼Œä»è€Œå®ç°å¯é çš„ Jailbreak æ£€æµ‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šç§æ¨¡å‹å’Œæ•°æ®é›†ä¸Šå‡è¡¨ç°å“è¶Šï¼Œå‡†ç¡®ç‡ç¨³å±…å‰ä¸¤ä½ï¼Œä¸”åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®å˜åŒ–ä¸‹å±•ç°å‡ºæ¯”ç°æœ‰ç«äº‰æ–¹æ³•æ›´å¼ºçš„ç¨³å®šæ€§ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "This paper has been accepted in IEEE Consumer Communications & Networking Conference 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.11784v2",
      "published_date": "2025-11-14 14:43:54 UTC",
      "updated_date": "2025-11-28 18:49:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:17:37.185187+00:00"
    },
    {
      "arxiv_id": "2511.13765v1",
      "title": "PROF: An LLM-based Reward Code Preference Optimization Framework for Offline Imitation Learning",
      "title_zh": "PROFï¼šåŸºäº LLM çš„ç¦»çº¿æ¨¡ä»¿å­¦ä¹ å¥–åŠ±ä»£ç åå¥½ä¼˜åŒ–æ¡†æ¶",
      "authors": [
        "Shengjie Sun",
        "Jiafei Lyu",
        "Runze Liu",
        "Mengbei Yan",
        "Bo Liu",
        "Deheng Ye",
        "Xiu Li"
      ],
      "abstract": "Offline imitation learning (offline IL) enables training effective policies without requiring explicit reward annotations. Recent approaches attempt to estimate rewards for unlabeled datasets using a small set of expert demonstrations. However, these methods often assume that the similarity between a trajectory and an expert demonstration is positively correlated with the reward, which oversimplifies the underlying reward structure. We propose PROF, a novel framework that leverages large language models (LLMs) to generate and improve executable reward function codes from natural language descriptions and a single expert trajectory. We propose Reward Preference Ranking (RPR), a novel reward function quality assessment and ranking strategy without requiring environment interactions or RL training. RPR calculates the dominance scores of the reward functions, where higher scores indicate better alignment with expert preferences. By alternating between RPR and text-based gradient optimization, PROF fully automates the selection and refinement of optimal reward functions for downstream policy learning. Empirical results on D4RL demonstrate that PROF surpasses or matches recent strong baselines across numerous datasets and domains, highlighting the effectiveness of our approach.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿æ¨¡ä»¿å­¦ä¹  (Offline IL) ä¸­å¥–åŠ±ç»“æ„è¿‡äºç®€åŒ–çš„æŒ‘æˆ˜ï¼Œæå‡ºäº† PROF æ¡†æ¶ï¼Œåˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ (LLMs) ä»è‡ªç„¶è¯­è¨€æè¿°å’Œå•æ¡ä¸“å®¶è½¨è¿¹ä¸­ç”Ÿæˆå¹¶ä¼˜åŒ–å¯æ‰§è¡Œçš„å¥–åŠ±å‡½æ•°ä»£ç ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å¥–åŠ±åå¥½æ’åº (Reward Preference Ranking, RPR) ç­–ç•¥ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€ç¯å¢ƒäº¤äº’æˆ–å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è®¡ç®—ä¼˜åŠ¿å¾—åˆ†æ¥è¯„ä¼°å¥–åŠ±å‡½æ•°ä¸ä¸“å®¶åå¥½çš„å¯¹é½ç¨‹åº¦ã€‚é€šè¿‡äº¤æ›¿æ‰§è¡Œ RPR ä¸åŸºäºæ–‡æœ¬çš„æ¢¯åº¦ä¼˜åŒ– (text-based gradient optimization)ï¼ŒPROF å®ç°äº†æœ€ä¼˜å¥–åŠ±å‡½æ•°çš„å…¨è‡ªåŠ¨ç­›é€‰ä¸è¿­ä»£ã€‚åœ¨ D4RL åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒPROF åœ¨å¤šä¸ªæ•°æ®é›†å’Œé¢†åŸŸä¸­å‡è¾¾åˆ°æˆ–è¶…è¿‡äº†ç°æœ‰çš„å¼ºåŸºå‡†æ¨¡å‹æ°´å¹³ï¼Œå……åˆ†éªŒè¯äº†è¯¥æ–¹æ³•åœ¨è‡ªåŠ¨åŒ–å¥–åŠ±å»ºæ¨¡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13765v1",
      "published_date": "2025-11-14 14:38:02 UTC",
      "updated_date": "2025-11-14 14:38:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:17:42.274474+00:00"
    },
    {
      "arxiv_id": "2511.11347v2",
      "title": "Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions",
      "title_zh": "åŒ»ç–—èŠå¤©æœºå™¨äººä¸­æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å¢å¼ºå‹å¤§è¯­è¨€æ¨¡å‹çš„éšç§æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆï¼šåº”ç”¨ã€é£é™©åŠæœªæ¥æ–¹å‘ç»¼è¿°",
      "authors": [
        "Shaowei Guan",
        "Hin Chi Kwok",
        "Ngai Fong Law",
        "Gregor Stiglic",
        "Harry Qin",
        "Vivian Hui"
      ],
      "abstract": "Retrieval-augmented generation (RAG) has rapidly emerged as a transformative approach for integrating large language models into clinical and biomedical workflows. However, privacy risks, such as protected health information (PHI) exposure, remain inconsistently mitigated. This review provides a thorough analysis of the current landscape of RAG applications in healthcare, including (i) sensitive data type across clinical scenarios, (ii) the associated privacy risks, (iii) current and emerging data-privacy protection mechanisms and (iv) future direction for patient data privacy protection. We synthesize 23 articles on RAG applications in healthcare and systematically analyze privacy challenges through a pipeline-structured framework encompassing data storage, transmission, retrieval and generation stages, delineating potential failure modes, their underlying causes in threat models and system mechanisms, and their practical implications. Building on this analysis, we critically review 17 articles on privacy-preserving strategies for RAG systems. Our evaluation reveals critical gaps, including insufficient clinical validation, absence of standardized evaluation frameworks, and lack of automated assessment tools. We propose actionable directions based on these limitations and conclude with a call to action. This review provides researchers and practitioners with a structured framework for understanding privacy vulnerabilities in healthcare RAG and offers a roadmap toward developing systems that achieve both clinical effectiveness and robust privacy preservation.",
      "tldr_zh": "è¯¥ç»¼è¿°æ·±å…¥æ¢è®¨äº†æ£€ç´¢å¢å¼ºç”Ÿæˆ(Retrieval-Augmented Generation, RAG)åœ¨åŒ»ç–—èŠå¤©æœºå™¨äººä¸­çš„åº”ç”¨åŠå…¶é¢ä¸´çš„éšç§æŒ‘æˆ˜ï¼Œç‰¹åˆ«æ˜¯å—ä¿æŠ¤å¥åº·ä¿¡æ¯(Protected Health Information, PHI)çš„æ³„éœ²é£é™©ã€‚ç ”ç©¶é€šè¿‡åˆ†æ23ç¯‡åŒ»ç–—RAGåº”ç”¨æ–‡çŒ®å’Œ17ç¯‡éšç§ä¿æŠ¤ç­–ç•¥æ–‡çŒ®ï¼Œæ„å»ºäº†ä¸€ä¸ªæ¶µç›–æ•°æ®å­˜å‚¨ã€ä¼ è¾“ã€æ£€ç´¢å’Œç”Ÿæˆå…¨è¿‡ç¨‹çš„æµæ°´çº¿ç»“æ„åŒ–æ¡†æ¶(Pipeline-structured Framework)æ¥è¯†åˆ«ç³»ç»Ÿæ¼æ´ã€‚æ–‡ç« è¯¦ç»†å½’çº³äº†ä¸´åºŠåœºæ™¯ä¸­çš„æ•æ„Ÿæ•°æ®ç±»å‹ã€å¨èƒæ¨¡å‹(Threat Models)ä»¥åŠéšç§ä¿æŠ¤æœºåˆ¶çš„ç°çŠ¶ã€‚è¯„ä¼°ç»“æœæ­ç¤ºäº†ä¸´åºŠéªŒè¯ä¸è¶³ã€ç¼ºä¹æ ‡å‡†åŒ–è¯„ä¼°æ¡†æ¶(Standardized Evaluation Frameworks)åŠè‡ªåŠ¨åŒ–è¯„ä¼°å·¥å…·ç­‰å…³é”®ç¼ºå£ã€‚åŸºäºæ­¤ï¼Œç ”ç©¶æå‡ºäº†æœªæ¥å‘å±•çš„è¡ŒåŠ¨å»ºè®®ï¼Œæ—¨åœ¨ä¸ºå¼€å‘è€…æä¾›ä¸€å¥—åœ¨ç¡®ä¿ä¸´åºŠæ•ˆèƒ½çš„åŒæ—¶å®ç°ç¨³å¥éšç§ä¿æŠ¤çš„æŠ€æœ¯è·¯çº¿å›¾ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "23 pages, 2 figures; Corrected typos and 2 references format, added a co-author",
      "pdf_url": "https://arxiv.org/pdf/2511.11347v2",
      "published_date": "2025-11-14 14:33:58 UTC",
      "updated_date": "2025-11-17 03:23:50 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:17:49.888972+00:00"
    },
    {
      "arxiv_id": "2512.00042v1",
      "title": "Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the Standardized Exam Questions",
      "title_zh": "å¼¥åˆå·®è·ï¼šé¢å‘æ ‡å‡†åŒ–è€ƒè¯•è¯•é¢˜çš„è§†è§‰è¯­è¨€æ¨¡å‹ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„å¾®è°ƒ",
      "authors": [
        "Egemen Sert",
        "Åeyda Ertekin"
      ],
      "abstract": "Multimodal reasoning has become a cornerstone of modern AI research. Standardized exam questions offer a uniquely rigorous testbed for such reasoning, providing structured visual contexts and verifiable answers. While recent progress has largely focused on algorithmic advances such as reinforcement learning (e.g., GRPO, DPO), the data centric foundations of vision language reasoning remain less explored.\n  We show that supervised fine-tuning (SFT) with high-quality data can rival proprietary approaches. To this end, we compile a 161.4 million token multimodal dataset combining textbook question-solution pairs, curriculum aligned diagrams, and contextual materials, and fine-tune Qwen-2.5VL-32B using an optimized reasoning syntax (QMSA). The resulting model achieves 78.6% accuracy, only 1.0% below Gemini 2.0 Flash, on our newly released benchmark YKSUniform, which standardizes 1,854 multimodal exam questions across 309 curriculum topics.\n  Our results reveal that data composition and representational syntax play a decisive role in multimodal reasoning. This work establishes a data centric framework for advancing open weight vision language models, demonstrating that carefully curated and curriculum-grounded multimodal data can elevate supervised fine-tuning to near state-of-the-art performance.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨æ ‡å‡†åŒ–è€ƒè¯•é¢˜ç›®ä¸­çš„å¤šæ¨¡æ€æ¨ç†èƒ½åŠ›ï¼Œæå‡ºäº†ä¸€ç§ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„å¾®è°ƒæ–¹æ³•ã€‚ç ”ç©¶è€…é€šè¿‡ç¼–è¯‘åŒ…å«1.614äº¿Tokençš„å¤šæ¨¡æ€æ•°æ®é›†ï¼Œç»“åˆæ•™ç§‘ä¹¦é—®ç­”å¯¹ã€è¯¾ç¨‹å…³è”å›¾è¡¨åŠä¸Šä¸‹æ–‡ææ–™ï¼Œå¯¹Qwen-2.5VL-32Bæ¨¡å‹è¿›è¡Œäº†ç›‘ç£å¾®è°ƒ(SFT)ã€‚å®éªŒé‡‡ç”¨äº†ä¼˜åŒ–çš„æ¨ç†è¯­æ³•QMSAï¼Œè¯æ˜äº†é«˜è´¨é‡æ•°æ®é©±åŠ¨çš„å¾®è°ƒæ•ˆæœå¯ä»¥åª²ç¾é—­æºå•†ä¸šæ¨¡å‹ã€‚åœ¨åŒ…å«309ä¸ªè¯¾ç¨‹ä¸»é¢˜çš„æ–°åŸºå‡†æµ‹è¯•é›†YKSUniformä¸Šï¼Œè¯¥æ¨¡å‹å–å¾—äº†78.6%çš„å‡†ç¡®ç‡ï¼Œä¸Gemini 2.0 Flashçš„æ€§èƒ½å·®è·ä»…ä¸º1.0%ã€‚ç ”ç©¶ç»“æœæ­ç¤ºäº†æ•°æ®ç»„æˆå’Œè¡¨å¾è¯­æ³•åœ¨å¤šæ¨¡æ€æ¨ç†ä¸­èµ·ç€å†³å®šæ€§ä½œç”¨ï¼Œä¸ºæå‡å¼€æºè§†è§‰è¯­è¨€æ¨¡å‹æ€§èƒ½å»ºç«‹äº†ä¸€ä¸ªä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„ç ”ç©¶æ¡†æ¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2512.00042v1",
      "published_date": "2025-11-14 14:28:14 UTC",
      "updated_date": "2025-11-14 14:28:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:17:49.096054+00:00"
    },
    {
      "arxiv_id": "2511.11340v1",
      "title": "M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text",
      "title_zh": "M-DAIGTï¼šäººå·¥æ™ºèƒ½ç”Ÿæˆæ–‡æœ¬å¤šé¢†åŸŸæ£€æµ‹å…±äº«ä»»åŠ¡",
      "authors": [
        "Salima Lamsiyah",
        "Saad Ezzini",
        "Abdelkader El Mahdaouy",
        "Hamza Alami",
        "Abdessamad Benlahbib",
        "Samir El Amrany",
        "Salmane Chafik",
        "Hicham Hammouchi"
      ],
      "abstract": "The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) ç”Ÿæˆå†…å®¹å¯¹ä¿¡æ¯å®Œæ•´æ€§å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†å¤šé¢†åŸŸ AI ç”Ÿæˆæ–‡æœ¬æ£€æµ‹ (Multi-Domain Detection of AI-Generated Text, M-DAIGT) å…±äº«ä»»åŠ¡ã€‚è¯¥ä»»åŠ¡ä¸‹è®¾æ–°é—»æ–‡ç« æ£€æµ‹ (News Article Detection, NAD) å’Œå­¦æœ¯å†™ä½œæ£€æµ‹ (Academic Writing Detection, AWD) ä¸¤ä¸ªäºŒå…ƒåˆ†ç±»å­ä»»åŠ¡ã€‚ä¸ºæ”¯æŒç ”ç©¶ï¼Œå›¢é˜Ÿå‘å¸ƒäº†ä¸€ä¸ªåŒ…å« 30,000 ä¸ªæ ·æœ¬çš„å¤§è§„æ¨¡åŸºå‡†æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬ç”± GPT-4 å’Œ Claude ç­‰å¤šç§ LLMs é€šè¿‡ä¸åŒæç¤ºç­–ç•¥ç”Ÿæˆçš„é«˜è´¨é‡æ–‡æœ¬ã€‚åœ¨ 46 æ”¯æ³¨å†Œå›¢é˜Ÿä¸­ï¼Œæœ‰ 4 æ”¯å›¢é˜Ÿå®Œæˆäº†å…¨éƒ¨å­ä»»åŠ¡çš„è¯„æµ‹ã€‚è®ºæ–‡è¯¦ç»†é˜è¿°äº†å„å‚ä¸å›¢é˜Ÿæ‰€é‡‡ç”¨çš„æŠ€æœ¯æ–¹æ³•ï¼Œå¹¶æ¢è®¨äº† M-DAIGT åœ¨æå‡ AI ç”Ÿæˆæ–‡æœ¬æ£€æµ‹èƒ½åŠ›æ–¹é¢çš„æœªæ¥ç ”ç©¶æ–¹å‘ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11340v1",
      "published_date": "2025-11-14 14:26:31 UTC",
      "updated_date": "2025-11-14 14:26:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:17:51.781522+00:00"
    },
    {
      "arxiv_id": "2511.11324v1",
      "title": "NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery",
      "title_zh": "NOVAï¼šé¢å‘è‡ªåŠ¨åŒ–ç»„ç»‡ç—…ç†å­¦åˆ†æä¸å‘ç°çš„æ™ºèƒ½ä½“æ¡†æ¶",
      "authors": [
        "Anurag J. Vaidya",
        "Felix Meissen",
        "Daniel C. Castro",
        "Shruthi Bannur",
        "Tristan Lazard",
        "Drew F. K. Williamson",
        "Faisal Mahmood",
        "Javier Alvarez-Valle",
        "Stephanie L. Hyland",
        "Kenza Bouzid"
      ],
      "abstract": "Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† NOVAï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è‡ªåŠ¨åŒ–ç»„ç»‡ç—…ç†å­¦(histopathology)åˆ†æä¸å‘ç°çš„æ™ºèƒ½ä½“æ¡†æ¶(agentic framework)ã€‚NOVA é€šè¿‡è¿­ä»£ç”Ÿæˆå¹¶è¿è¡Œ Python ä»£ç ï¼Œå°†ç§‘å­¦æŸ¥è¯¢è½¬åŒ–ä¸ºå¯æ‰§è¡Œçš„åˆ†ææµç¨‹ï¼Œä»è€Œè§£å†³äº†æ•°å­—åŒ–ç—…ç†åˆ†æä¸­æµç¨‹å¤æ‚ä¸”è€—æ—¶çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶é›†æˆäº† 49 ç§åŸºäºå¼€æºè½¯ä»¶çš„é¢†åŸŸä¸“ç”¨å·¥å…·(domain-specific tools)ï¼Œå¦‚ç»†èƒæ ¸åˆ†å‰²(nuclei segmentation)å’Œå…¨åˆ‡ç‰‡ç¼–ç (whole-slide encoding)ï¼Œå¹¶æ”¯æŒæŒ‰éœ€åˆ›å»ºæ–°å·¥å…·ã€‚ä¸ºäº†å…¨é¢è¯„ä¼°æ­¤ç±»ç³»ç»Ÿï¼Œç ”ç©¶è€…æå‡ºäº†åŒ…å« 90 ä¸ªé—®é¢˜çš„ SlideQuest åŸºå‡†æµ‹è¯•ï¼Œæ¶µç›–äº†ä»æ•°æ®å¤„ç†åˆ°å‡è®¾æµ‹è¯•çš„å¤šæ­¥æ¨ç†ä»»åŠ¡ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒNOVA åœ¨æ€§èƒ½ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰çš„ä»£ç æ™ºèƒ½ä½“åŸºçº¿(coding-agent baselines)ã€‚æ­¤å¤–ï¼Œä¸€é¡¹ç»ç—…ç†å­¦å®¶éªŒè¯çš„æ¡ˆä¾‹ç ”ç©¶æˆåŠŸå°†å½¢æ€å­¦ç‰¹å¾ä¸å…·æœ‰é¢„åæ„ä¹‰çš„ PAM50 äºšå‹è”ç³»èµ·æ¥ï¼Œå±•ç¤ºäº† NOVA åœ¨å¤§è§„èŒƒç§‘å­¦å‘ç°æ–¹é¢çš„å·¨å¤§æ½œåŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11324v1",
      "published_date": "2025-11-14 14:01:18 UTC",
      "updated_date": "2025-11-14 14:01:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:17:55.294397+00:00"
    },
    {
      "arxiv_id": "2511.11323v1",
      "title": "RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms",
      "title_zh": "RLSLMï¼šä¸€ç§ä½¿åŸºäºè§„åˆ™çš„ç¤¾äº¤è¿åŠ¨æ¨¡å‹ä¸äººç±»ç¤¾ä¼šè§„èŒƒå¯¹é½çš„æ··åˆå¼ºåŒ–å­¦ä¹ æ¡†æ¶",
      "authors": [
        "Yitian Kou",
        "Yihe Gu",
        "Chen Zhou",
        "DanDan Zhu",
        "Shuguang Kuai"
      ],
      "abstract": "Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† RLSLMï¼Œä¸€ç§å°†åŸºäºè§„åˆ™çš„ç¤¾ä¼šè¿åŠ¨æ¨¡å‹ä¸äººç±»ç¤¾ä¼šè§„èŒƒå¯¹é½çš„æ··åˆ Reinforcement Learning æ¡†æ¶ã€‚ä¸ºäº†å…‹æœ rule-based æ–¹æ³•çµæ´»æ€§ä¸è¶³ä»¥åŠæ•°æ®é©±åŠ¨æ–¹æ³•ç¼ºä¹ interpretability çš„ç¼ºé™·ï¼ŒRLSLM å°†åŸºäºç»éªŒè¡Œä¸ºå®éªŒçš„ Social Locomotion Model é›†æˆåˆ°äº†å¼ºåŒ–å­¦ä¹ çš„å¥–åŠ±å‡½æ•°ä¸­ã€‚è¯¥æ¡†æ¶ç”Ÿæˆä¸€ä¸ªå¯¹æ–¹å‘æ•æ„Ÿçš„ social comfort fieldï¼Œèƒ½å¤Ÿç²¾ç¡®é‡åŒ–ç©ºé—´ä¸­çš„äººç±»èˆ’é€‚åº¦ï¼Œå¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ ç¤¾äº¤ä¸€è‡´çš„å¯¼èˆªç­–ç•¥ã€‚RLSLM é€šè¿‡è”åˆä¼˜åŒ–æœºæ¢°èƒ½ä¸ç¤¾äº¤èˆ’é€‚åº¦ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿæœ‰æ•ˆé¿å…ä¾µå…¥ä¸ªäººæˆ–ç¾¤ä½“ç©ºé—´ã€‚åœ¨æ²‰æµ¸å¼ VR ç¯å¢ƒä¸­è¿›è¡Œçš„äººæœºäº¤äº’å®éªŒè¡¨æ˜ï¼ŒRLSLM åœ¨ç”¨æˆ·ä½“éªŒæ–¹é¢ä¼˜äºæœ€å…ˆè¿›çš„ rule-based æ¨¡å‹ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œè¯¥æ–¹æ³•æ¯”ä¼ ç»Ÿæ•°æ®é©±åŠ¨æ–¹æ³•å…·æœ‰æ˜¾è‘—æå‡çš„è§£é‡Šæ€§ï¼Œä¸ºç¤¾äº¤æ„ŸçŸ¥æœºå™¨äººå¯¼èˆªæä¾›äº†å¯æ‰©å±•ä¸”ä»¥äººä¸ºæœ¬çš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.11323v1",
      "published_date": "2025-11-14 13:59:40 UTC",
      "updated_date": "2025-11-14 13:59:40 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:18:01.787082+00:00"
    },
    {
      "arxiv_id": "2511.11315v2",
      "title": "LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models",
      "title_zh": "LAETï¼šä¸€ç§é¢å‘é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„é€å±‚è‡ªé€‚åº”é›†æˆå¾®è°ƒæ¡†æ¶",
      "authors": [
        "Jawad Ibn Ahad",
        "Muhammad Rafsan Kabir",
        "Robin Krambroeckers",
        "Sifat Momen",
        "Nabeel Mohammed",
        "Shafin Rahman"
      ],
      "abstract": "Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† Layer-wise Adaptive Ensemble Tuning (LAET)ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹é¢„è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) çš„æ–°å‹å¾®è°ƒç­–ç•¥ï¼Œæ—¨åœ¨è§£å†³é‡‘èè‡ªç„¶è¯­è¨€å¤„ç† (Financial NLP) ä»»åŠ¡ä¸­è®¡ç®—æˆæœ¬è¿‡é«˜çš„é—®é¢˜ã€‚LAET é€šè¿‡åˆ†æéšè—çŠ¶æ€è¡¨ç¤º (Hidden State Representations) æ¥é€‰æ‹©æ€§åœ°å¾®è°ƒæœ€æœ‰æ•ˆçš„æ¨¡å‹å±‚ï¼Œå¹¶å†»ç»“éå…³é”®å±‚ï¼Œä»è€Œåœ¨æ˜¾è‘—é™ä½è®¡ç®—å¼€é”€çš„åŒæ—¶å¢å¼ºç‰¹å®šä»»åŠ¡çš„æ€§èƒ½ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨å¤šé¡¹é‡‘è NLP ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå³ä½¿æ˜¯å‚æ•°é‡çº¦ä¸º 3B çš„å°å‹æ¨¡å‹ï¼Œåœ¨åº”ç”¨ LAET åä¹Ÿèƒ½è¶…è¶Š GPT-4 ç­‰æœ€å…ˆè¿›çš„æ¨¡å‹å’Œç°æœ‰åŸºå‡†ã€‚è¿™é¡¹å·¥ä½œä¸ºé‡‘èé¢†åŸŸçš„å®é™…åº”ç”¨æä¾›äº†é«˜æ•ˆä¸”å¯æ‰©å±•çš„è§£å†³æ–¹æ¡ˆï¼ŒæˆåŠŸå¡«è¡¥äº†å‰æ²¿å­¦æœ¯ç ”ç©¶ä¸ç°å®åœºæ™¯éƒ¨ç½²ä¹‹é—´çš„æŠ€æœ¯é¸¿æ²Ÿã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CE"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11315v2",
      "published_date": "2025-11-14 13:57:46 UTC",
      "updated_date": "2025-12-04 19:17:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:18:13.759712+00:00"
    },
    {
      "arxiv_id": "2511.11311v2",
      "title": "Large-scale modality-invariant foundation models for brain MRI analysis: Application to lesion segmentation",
      "title_zh": "ç”¨äºè„‘éƒ¨ MRI åˆ†æçš„å¤§è§„æ¨¡æ¨¡æ€ä¸å˜åŸºç¡€æ¨¡å‹ï¼šåœ¨ç—…ç¶åˆ†å‰²ä¸­çš„åº”ç”¨",
      "authors": [
        "Petros Koutsouvelis",
        "Matej Gazda",
        "Leroy Volmer",
        "Sina Amirrajab",
        "Kamil Barbierik",
        "Branislav Setlak",
        "Jakub Gazda",
        "Peter Drotar"
      ],
      "abstract": "The field of computer vision is undergoing a paradigm shift toward large-scale foundation model pre-training via self-supervised learning (SSL). Leveraging large volumes of unlabeled brain MRI data, such models can learn anatomical priors that improve few-shot performance in diverse neuroimaging tasks. However, most SSL frameworks are tailored to natural images, and their adaptation to capture multi-modal MRI information remains underexplored. This work proposes a modality-invariant representation learning setup and evaluates its effectiveness in stroke and epilepsy lesion segmentation, following large-scale pre-training. Experimental results suggest that despite successful cross-modality alignment, lesion segmentation primarily benefits from preserving fine-grained modality-specific features. Model checkpoints and code are made publicly available.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸå‘è‡ªç›‘ç£å­¦ä¹ (Self-supervised learning, SSL)å¤§è§„æ¨¡åŸºç¡€æ¨¡å‹é¢„è®­ç»ƒè½¬å‹çš„è¶‹åŠ¿ï¼Œé’ˆå¯¹å¤šæ¨¡æ€MRIä¿¡æ¯æ•æ‰ä¸è¶³çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ¨¡æ€ä¸å˜è¡¨ç¤ºå­¦ä¹ (Modality-invariant representation learning)æ¡†æ¶ã€‚é€šè¿‡åœ¨å¤§é‡æœªæ ‡æ³¨è„‘éƒ¨MRIæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ åˆ°æœ‰åŠ©äºæå‡å°‘æ ·æœ¬ç¥ç»å½±åƒä»»åŠ¡è¡¨ç°çš„è§£å‰–å­¦å…ˆéªŒã€‚ç ”ç©¶äººå‘˜åœ¨ä¸­é£(Stroke)å’Œç™«ç—«(Epilepsy)ç—…ç¶åˆ†å‰²ä»»åŠ¡ä¸­è¯„ä¼°äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå®éªŒç»“æœæ˜¾ç¤ºï¼Œå°½ç®¡è·¨æ¨¡æ€å¯¹é½å–å¾—äº†æˆåŠŸï¼Œä½†ç—…ç¶åˆ†å‰²çš„å‡†ç¡®æ€§ä¸»è¦å—ç›Šäºä¿ç•™ç»†ç²’åº¦çš„æ¨¡æ€ç‰¹å®šç‰¹å¾(Modality-specific features)ã€‚è¯¥å·¥ä½œä¸ä»…æ·±å…¥åˆ†æäº†æ¨¡æ€ä¸å˜æ€§åœ¨è„‘éƒ¨å›¾åƒåˆ†æä¸­çš„åº”ç”¨æ•ˆæœï¼Œè¿˜å…¬å¼€äº†æ¨¡å‹æƒé‡å’Œä»£ç ï¼Œä¸ºå¤šæ¨¡æ€åŒ»ç–—å½±åƒåŸºç¡€æ¨¡å‹çš„ç ”ç©¶æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "eess.IV",
      "comment": "Submitted to IEEE ISBI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.11311v2",
      "published_date": "2025-11-14 13:56:07 UTC",
      "updated_date": "2026-01-14 13:37:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:18:17.990760+00:00"
    },
    {
      "arxiv_id": "2511.11306v2",
      "title": "iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference",
      "title_zh": "iMADï¼šé¢å‘é«˜æ•ˆå‡†ç¡® LLM æ¨ç†çš„æ™ºèƒ½å¤šæ™ºèƒ½ä½“è¾©è®º",
      "authors": [
        "Wei Fan",
        "JinYi Yoon",
        "Bo Ji"
      ],
      "abstract": "Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†iMAD (intelligent Multi-Agent Debate)ï¼Œè¿™æ˜¯ä¸€ä¸ª token-efficient çš„æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³ Multi-Agent Debate (MAD) åœ¨ Large Language Model (LLM) æ¨ç†ä¸­å­˜åœ¨çš„è®¡ç®—æˆæœ¬é«˜å’Œå¯èƒ½é™ä½å‡†ç¡®ç‡çš„é—®é¢˜ã€‚iMAD çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€‰æ‹©æ€§åœ°è§¦å‘è¾©è®ºæœºåˆ¶ï¼Œä»…åœ¨è¾©è®ºå¯èƒ½çº æ­£åˆå§‹é”™è¯¯ç­”æ¡ˆæ—¶æ‰å¯åŠ¨ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ¡†æ¶é¦–å…ˆå¼•å¯¼å•ä¸ªæ™ºèƒ½ä½“ç”Ÿæˆç»“æ„åŒ–çš„ self-critique å“åº”ï¼Œå¹¶ä»ä¸­æå–41ä¸ªæ•æ‰çŠ¹è±«ä¿¡å·çš„è¯­è¨€å’Œè¯­ä¹‰ç‰¹å¾ã€‚éšåï¼ŒiMAD åˆ©ç”¨ä¸€ä¸ªåŸºäº FocusCal loss è®­ç»ƒçš„è½»é‡åŒ–è¾©è®ºå†³ç­–åˆ†ç±»å™¨æ¥å†³å®šæ˜¯å¦è§¦å‘ MADï¼Œå®ç°äº†æ— éœ€ç‰¹å®šæ•°æ®é›†å¾®è°ƒçš„ç¨³å¥å†³ç­–ã€‚åœ¨å…­ä¸ªè§†è§‰åŠæ–‡æœ¬é—®ç­”æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒiMAD åœ¨æå‡æœ€ç»ˆç­”æ¡ˆå‡†ç¡®ç‡ï¼ˆæœ€é«˜è¾¾13.5%ï¼‰çš„åŒæ—¶ï¼Œæ˜¾è‘—é™ä½äº†é«˜è¾¾92%çš„ token ä½¿ç”¨é‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted in AAAI 2026 (Oral)",
      "pdf_url": "https://arxiv.org/pdf/2511.11306v2",
      "published_date": "2025-11-14 13:50:51 UTC",
      "updated_date": "2025-12-02 14:13:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:18:20.676411+00:00"
    },
    {
      "arxiv_id": "2511.11305v2",
      "title": "MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising",
      "title_zh": "MOON Embeddingï¼šé¢å‘ç”µå•†æœç´¢å¹¿å‘Šçš„å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ",
      "authors": [
        "Chenghan Fu",
        "Daoze Zhang",
        "Yukang Lin",
        "Zhanheng Nie",
        "Xiang Zhang",
        "Jianyu Liu",
        "Yueran Liu",
        "Wanxian Guan",
        "Pengjie Wang",
        "Jian Xu",
        "Bo Zheng"
      ],
      "abstract": "We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of \"Pretraining, Post-training, and Application\", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MOONï¼Œä¸€å¥—ä¸“ä¸ºç”µå­å•†åŠ¡æœç´¢å¹¿å‘Šè®¾è®¡çš„å¯æŒç»­è¿­ä»£å¤šæ¨¡æ€è¡¨ç¤ºå­¦ä¹ ï¼ˆMultimodal Representation Learningï¼‰æ¡†æ¶ï¼Œç°å·²å…¨é¢éƒ¨ç½²äºæ·˜å®æœç´¢å¹¿å‘Šç³»ç»Ÿçš„æ£€ç´¢ã€ç›¸å…³æ€§å’Œæ’åºç­‰ç¯èŠ‚ã€‚è¯¥ç³»ç»Ÿåœ¨ç‚¹å‡»ç‡ï¼ˆCTRï¼‰é¢„æµ‹ä»»åŠ¡ä¸­å®ç°äº† 20.00% çš„åœ¨çº¿æå‡ï¼Œæ˜¯è¯¥é¡¹ç›®è¿‡å»ä¸‰å¹´ä¸­åœ¨ CTR é¢„æµ‹ä»»åŠ¡ä¸Šå–å¾—çš„æœ€å¤§æ€§èƒ½å¢é•¿ã€‚MOON é‡‡ç”¨äº†â€œPretrainingã€Post-training å’Œ Applicationâ€ä¸‰é˜¶æ®µè®­ç»ƒèŒƒå¼ï¼Œæœ‰æ•ˆå®ç°äº†å¤šæ¨¡æ€è¡¨ç¤ºä¸ä¸‹æ¸¸ä»»åŠ¡çš„æ·±åº¦æ•´åˆã€‚ç ”ç©¶é€šè¿‡å¼•å…¥â€œexchange rateâ€æ¦‚å¿µé‡åŒ–äº†ä¸­é—´æŒ‡æ ‡å‘ä¸‹æ¸¸å¢ç›Šçš„è½¬åŒ–æ•ˆç‡ï¼Œå¹¶ç¡®å®š image-based search recall ä¸ºä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹çš„å…³é”®æŒ‡å¯¼æŒ‡æ ‡ã€‚åœ¨å†ç»äº”æ¬¡å…¨è§„æ¨¡è¿­ä»£çš„è¿‡ç¨‹ä¸­ï¼ŒMOON åœ¨æ•°æ®å¤„ç†ã€è®­ç»ƒç­–ç•¥ã€æ¨¡å‹æ¶æ„å’Œä¸‹æ¸¸åº”ç”¨å››ä¸ªæ ¸å¿ƒç»´åº¦æŒç»­æ¼”è¿›ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜ç³»ç»Ÿæ€§åœ°æ¢è®¨äº†ç”µå­å•†åŠ¡é¢†åŸŸå¤šæ¨¡æ€å­¦ä¹ çš„ Scaling Lawsï¼Œåˆ†æäº†è®­ç»ƒ token æ•°é‡ã€è´Ÿæ ·æœ¬åŠç”¨æˆ·è¡Œä¸ºåºåˆ—é•¿åº¦ç­‰å› ç´ å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œä¸ºå­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œæä¾›äº†å®è´µçš„å®è·µè§è§£ä¸ç»éªŒã€‚",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "primary_category": "cs.IR",
      "comment": "31 pages, 12 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.11305v2",
      "published_date": "2025-11-14 13:49:56 UTC",
      "updated_date": "2025-11-18 16:56:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:18:23.291165+00:00"
    },
    {
      "arxiv_id": "2511.11301v1",
      "title": "EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment",
      "title_zh": "EcoAlignï¼šé¢å‘é«˜æ•ˆ LVLM å¯¹é½çš„ç»æµç†æ€§æ¡†æ¶",
      "authors": [
        "Ruoxi Cheng",
        "Haoxuan Ma",
        "Teng Ma",
        "Hongyi Zhang"
      ],
      "abstract": "Large Vision-Language Models (LVLMs) exhibit powerful reasoning capabilities but suffer sophisticated jailbreak vulnerabilities. Fundamentally, aligning LVLMs is not just a safety challenge but a problem of economic efficiency. Current alignment methods struggle with the trade-off between safety, utility, and operational costs. Critically, a focus solely on final outputs (process-blindness) wastes significant computational budget on unsafe deliberation. This flaw allows harmful reasoning to be disguised with benign justifications, thereby circumventing simple additive safety scores. To address this, we propose EcoAlign, an inference-time framework that reframes alignment as an economically rational search by treating the LVLM as a boundedly rational agent. EcoAlign incrementally expands a thought graph and scores actions using a forward-looking function (analogous to net present value) that dynamically weighs expected safety, utility, and cost against the remaining budget. To prevent deception, path safety is enforced via the weakest-link principle. Extensive experiments across 3 closed-source and 2 open-source models on 6 datasets show that EcoAlign matches or surpasses state-of-the-art safety and utility at a lower computational cost, thereby offering a principled, economical pathway to robust LVLM alignment.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€è§†è§‰æ¨¡å‹(LVLMs)åœ¨å¯¹é½(alignment)è¿‡ç¨‹ä¸­é¢ä¸´çš„è¶Šç‹±(jailbreak)æ¼æ´ä»¥åŠå®‰å…¨æ€§ã€å®ç”¨æ€§ä¸æˆæœ¬ä¹‹é—´çš„æƒè¡¡éš¾é¢˜ï¼Œæå‡ºäº†EcoAlignæ¡†æ¶ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„è¿‡ç¨‹ç›²è§†(process-blindness)é—®é¢˜å¯¼è‡´åœ¨ä¸å®‰å…¨æ¨ç†ä¸Šæµªè´¹äº†è¿‡å¤šè®¡ç®—èµ„æºï¼Œä¸”æœ‰å®³æ¨ç†å®¹æ˜“é€šè¿‡ä¼ªè£…çš„è‰¯æ€§è¾©è§£è§„é¿ç›‘ç®¡ã€‚EcoAlignå°†å¯¹é½é‡æ„ä¸ºä¸€ç§ç»æµç†æ€§æœç´¢ï¼Œå°†LVLMè§†ä¸ºæœ‰é™ç†æ€§æ™ºèƒ½ä½“(boundedly rational agent)ï¼Œé€šè¿‡é€æ­¥æ‰©å±•æ€ç»´å›¾(thought graph)å¹¶åˆ©ç”¨ç±»ä¼¼äºå‡€ç°å€¼(net present value)çš„å‰ç»æ€§å‡½æ•°å¯¹åŠ¨ä½œè¿›è¡ŒåŠ¨æ€è¯„åˆ†ã€‚ä¸ºäº†é˜²æ­¢æ¬ºéª—ï¼Œè¯¥æ¡†æ¶é‡‡ç”¨æœ€å¼±ç¯èŠ‚åŸåˆ™(weakest-link principle)æ¥ç¡®ä¿è·¯å¾„å®‰å…¨æ€§ã€‚åœ¨5ä¸ªæ¨¡å‹å’Œ6ä¸ªæ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒEcoAlignèƒ½ä»¥æ›´ä½çš„è®¡ç®—æˆæœ¬è¾¾åˆ°æˆ–è¶…è¶Šæœ€å…ˆè¿›(state-of-the-art)çš„å®‰å…¨ä¸å®ç”¨æ€§èƒ½ã€‚è¯¥ç ”ç©¶ä¸ºå®ç°é²æ£’ä¸”é«˜æ•ˆçš„LVLMå¯¹é½æä¾›äº†ä¸€æ¡å…·æœ‰ç»æµç†æ€§çš„æŠ€æœ¯è·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11301v1",
      "published_date": "2025-11-14 13:38:13 UTC",
      "updated_date": "2025-11-14 13:38:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:18:25.683737+00:00"
    },
    {
      "arxiv_id": "2511.11299v1",
      "title": "AUVIC: Adversarial Unlearning of Visual Concepts for Multi-modal Large Language Models",
      "title_zh": "AUVICï¼šé¢å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰æ¦‚å¿µå¯¹æŠ—æ€§é—å¿˜",
      "authors": [
        "Haokun Chen",
        "Jianing Li",
        "Yao Zhang",
        "Jinhe Bi",
        "Yan Xia",
        "Jindong Gu",
        "Volker Tresp"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) achieve impressive performance once optimized on massive datasets. Such datasets often contain sensitive or copyrighted content, raising significant data privacy concerns. Regulatory frameworks mandating the 'right to be forgotten' drive the need for machine unlearning. This technique allows for the removal of target data without resource-consuming retraining. However, while well-studied for text, visual concept unlearning in MLLMs remains underexplored. A primary challenge is precisely removing a target visual concept without disrupting model performance on related entities. To address this, we introduce AUVIC, a novel visual concept unlearning framework for MLLMs. AUVIC applies adversarial perturbations to enable precise forgetting. This approach effectively isolates the target concept while avoiding unintended effects on similar entities. To evaluate our method, we construct VCUBench. It is the first benchmark designed to assess visual concept unlearning in group contexts. Experimental results demonstrate that AUVIC achieves state-of-the-art target forgetting rates while incurs minimal performance degradation on non-target concepts.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLMs)åœ¨è®­ç»ƒæ•°æ®ä¸­æ¶‰åŠéšç§å’Œç‰ˆæƒå†…å®¹çš„é—®é¢˜ï¼Œæå‡ºäº†AUVICï¼Œä¸€ç§æ–°å‹çš„è§†è§‰æ¦‚å¿µé—å¿˜(Visual Concept Unlearning)æ¡†æ¶ã€‚é’ˆå¯¹åœ¨ç§»é™¤ç‰¹å®šè§†è§‰æ¦‚å¿µæ—¶å®¹æ˜“ç ´åç›¸å…³å®ä½“æ€§èƒ½çš„æŒ‘æˆ˜ï¼ŒAUVICé€šè¿‡å¼•å…¥å¯¹æŠ—æ‰°åŠ¨(Adversarial Perturbations)æŠ€æœ¯å®ç°äº†ç²¾ç¡®é—å¿˜ï¼Œèƒ½å¤Ÿæœ‰æ•ˆéš”ç¦»ç›®æ ‡æ¦‚å¿µå¹¶é¿å…å¯¹ç›¸ä¼¼å®ä½“äº§ç”Ÿå‰¯ä½œç”¨ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜æ„å»ºäº†é¦–ä¸ªç”¨äºè¯„ä¼°ç¾¤ä½“èƒŒæ™¯ä¸‹è§†è§‰æ¦‚å¿µé—å¿˜çš„åŸºå‡†æµ‹è¯•é›†VCUBenchã€‚å®éªŒç»“æœè¯æ˜ï¼ŒAUVICåœ¨å®ç°æœ€å…ˆè¿›çš„ç›®æ ‡é—å¿˜ç‡çš„åŒæ—¶ï¼Œå¯¹éç›®æ ‡æ¦‚å¿µçš„æ€§èƒ½ä¸‹é™å½±å“æå°ï¼Œä¸ºæ¨¡å‹æ»¡è¶³â€œè¢«é—å¿˜æƒâ€ç­‰ç›‘ç®¡è¦æ±‚æä¾›äº†é«˜æ•ˆä¸”ç²¾å‡†çš„æŠ€æœ¯æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI 2026. Code: https://github.com/HaokunChen245/AUVIC",
      "pdf_url": "https://arxiv.org/pdf/2511.11299v1",
      "published_date": "2025-11-14 13:35:32 UTC",
      "updated_date": "2025-11-14 13:35:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:18:30.096654+00:00"
    },
    {
      "arxiv_id": "2511.11298v1",
      "title": "Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation",
      "title_zh": "æœºå™¨äººæ“æ§è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹åŸºå‡†æµ‹è¯•çš„å®è·µç»éªŒ",
      "authors": [
        "Yihao Zhang",
        "Yuankai Qi",
        "Xi Zheng"
      ],
      "abstract": "Foundation models applied in robotics, particularly \\textbf{Vision--Language--Action (VLA)} models, hold great promise for achieving general-purpose manipulation. Yet, systematic real-world evaluations and cross-model comparisons remain scarce. This paper reports our \\textbf{empirical experiences} from benchmarking four representative VLAs -- \\textbf{ACT}, \\textbf{OpenVLA--OFT}, \\textbf{RDT-1B}, and \\boldmath{$Ï€_0$} -- across four manipulation tasks conducted in both simulation and on the \\textbf{ALOHA Mobile} platform. We establish a \\textbf{standardized evaluation framework} that measures performance along three key dimensions: (1) \\textit{accuracy and efficiency} (success rate and time-to-success), (2) \\textit{adaptability} across in-distribution, spatial out-of-distribution, and instance-plus-spatial out-of-distribution settings, and (3) \\textit{language instruction-following accuracy}. Through this process, we observe that \\boldmath{$Ï€_0$} demonstrates superior adaptability in out-of-distribution scenarios, while \\textbf{ACT} provides the highest stability in-distribution. Further analysis highlights differences in computational demands, data-scaling behavior, and recurring failure modes such as near-miss grasps, premature releases, and long-horizon state drift. These findings reveal practical trade-offs among VLA model architectures in balancing precision, generalization, and deployment cost, offering actionable insights for selecting and deploying VLAs in real-world robotic manipulation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æœºå™¨äººæ“çºµé¢†åŸŸä¸­è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹(Vision-Language-Action, VLA)ç¼ºä¹ç³»ç»Ÿæ€§çœŸå®ä¸–ç•Œè¯„ä¼°çš„é—®é¢˜ï¼Œå»ºç«‹äº†ä¸€ä¸ªæ ‡å‡†åŒ–çš„è¯„ä¼°æ¡†æ¶ã€‚é€šè¿‡åœ¨ä»¿çœŸç¯å¢ƒå’ŒALOHA Mobileå¹³å°ä¸Šå¯¹ACTã€OpenVLA-OFTã€RDT-1Bå’Œ$\\pi_0$å››ç§ä»£è¡¨æ€§æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œç ”ç©¶ä»å‡†ç¡®æ€§ã€é€‚åº”æ€§åŠæŒ‡ä»¤éµå¾ªèƒ½åŠ›ç­‰ç»´åº¦è¿›è¡Œäº†æ·±å…¥å¯¹æ¯”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œ$\\pi_0$åœ¨åˆ†å¸ƒå¤–(Out-of-Distribution)åœºæ™¯ä¸­å±•ç°å‡ºä¼˜å¼‚çš„é€‚åº”æ€§ï¼Œè€ŒACTåœ¨åˆ†å¸ƒå†…(In-Distribution)ä»»åŠ¡ä¸­åˆ™è¡¨ç°å‡ºæœ€é«˜çš„ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶è¯¦ç»†åˆ†æäº†å„æ¨¡å‹åœ¨è®¡ç®—éœ€æ±‚ã€æ•°æ®ç¼©æ”¾ä»¥åŠå…¸å‹å¤±è´¥æ¨¡å¼ï¼ˆå¦‚æŠ“å–å¤±è¯¯å’Œé•¿ç¨‹çŠ¶æ€æ¼‚ç§»ï¼‰æ–¹é¢çš„å·®å¼‚ã€‚è¿™äº›å‘ç°æ­ç¤ºäº†VLAæ¶æ„åœ¨ç²¾åº¦ã€æ³›åŒ–ä¸éƒ¨ç½²æˆæœ¬ä¹‹é—´çš„æƒè¡¡ï¼Œä¸ºæœºå™¨äººæ“çºµä»»åŠ¡ä¸­æ¨¡å‹çš„é€‰æ‹©ä¸éƒ¨ç½²æä¾›äº†å…³é”®çš„å®è·µæŒ‡å—ã€‚",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "primary_category": "cs.RO",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11298v1",
      "published_date": "2025-11-14 13:35:30 UTC",
      "updated_date": "2025-11-14 13:35:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:18:32.288089+00:00"
    },
    {
      "arxiv_id": "2511.14791v1",
      "title": "Enabling Predictive Maintenance in District Heating Substations: A Labelled Dataset and Fault Detection Evaluation Framework based on Service Data",
      "title_zh": "å®ç°åŒºåŸŸä¾›çƒ­çƒ­åŠ›ç«™çš„é¢„æµ‹æ€§ç»´æŠ¤ï¼šåŸºäºæœåŠ¡æ•°æ®çš„æ ‡æ³¨æ•°æ®é›†ä¸æ•…éšœæ£€æµ‹è¯„ä¼°æ¡†æ¶",
      "authors": [
        "Cyriana M. A. Roelofs",
        "Edison Guevara Bastidas",
        "Thomas Hugo",
        "Stefan Faulstich",
        "Anna Cadenbach"
      ],
      "abstract": "Early detection of faults in district heating substations is imperative to reduce return temperatures and enhance efficiency. However, progress in this domain has been hindered by the limited availability of public, labelled datasets. We present an open source framework combining a service report validated public dataset, an evaluation method based on Accuracy, Reliability, and Earliness, and baseline results implemented with EnergyFaultDetector, an open source Python framework.\n  The dataset contains time series of operational data from 93 substations across two manufacturers, annotated with a list of disturbances due to faults and maintenance actions, a set of normal-event examples and detailed fault metadata. We evaluate the EnergyFaultDetector using three metrics: Accuracy for recognising normal behaviour, an eventwise F Score for reliable fault detection with few false alarms, and Earliness for early detection. The framework also supports root cause analysis using ARCANA. We demonstrate three use cases to assist operators in interpreting anomalies and identifying underlying faults. The models achieve high normal-behaviour accuracy (0.98) and eventwise F-score (beta=0.5) of 0.83, detecting 60% of the faults in the dataset before the customer reports a problem, with an average lead time of 3.9 days.\n  Integrating an open dataset, metrics, open source code, and baselines establishes a reproducible, fault centric benchmark with operationally meaningful evaluation, enabling consistent comparison and development of early fault detection and diagnosis methods for district heating substations.",
      "tldr_zh": "æœ¬ç ”ç©¶é’ˆå¯¹åŒºåŸŸä¾›çƒ­å˜ç”µç«™(district heating substations)æ•…éšœæ£€æµ‹é¢†åŸŸç¼ºä¹å…¬å¼€æ ‡æ³¨æ•°æ®é›†çš„ç°çŠ¶ï¼Œæå‡ºäº†ä¸€ä¸ªåŒ…å«å…¬å¼€æ•°æ®é›†ã€è¯„ä¼°æ–¹æ³•åŠåŸºçº¿ç»“æœçš„å¼€æºæ¡†æ¶ã€‚è¯¥æ•°æ®é›†æ¶µç›–äº†æ¥è‡ªä¸¤å®¶åˆ¶é€ å•†çš„93ä¸ªå˜ç”µç«™çš„è¿è¡Œæ—¶é—´åºåˆ—æ•°æ®ï¼Œå¹¶ç»“åˆæœåŠ¡æŠ¥å‘Šå¯¹æ•…éšœã€ç»´æŠ¤åŠ¨ä½œåŠå…ƒæ•°æ®è¿›è¡Œäº†è¯¦ç»†æ ‡æ³¨ã€‚ç ”ç©¶åˆ©ç”¨å¼€æºPythonæ¡†æ¶EnergyFaultDetectorå»ºç«‹äº†åŸºçº¿æ¨¡å‹ï¼Œå¹¶é‡‡ç”¨å‡†ç¡®åº¦(Accuracy)ã€å¯é æ€§(Reliability)å’Œæå‰é‡(Earliness)ä¸‰ä¸ªç»´åº¦è¿›è¡Œç»¼åˆè¯„ä»·ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶æ”¯æŒåˆ©ç”¨ARCANAè¿›è¡Œæ ¹å› åˆ†æ(root cause analysis)ï¼Œé€šè¿‡å…·ä½“ç”¨ä¾‹å±•ç¤ºäº†å…¶åœ¨ååŠ©æ“ä½œå‘˜è¯†åˆ«åº•å±‚æ•…éšœæ–¹é¢çš„æ½œåŠ›ã€‚å®éªŒè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨æ­£å¸¸è¡Œä¸ºè¯†åˆ«ä¸Šè¾¾åˆ°0.98çš„å‡†ç¡®ç‡ï¼Œä¸”èƒ½åœ¨ç”¨æˆ·æŠ¥å‘Šé—®é¢˜å‰3.9å¤©æ£€æµ‹å‡º60%çš„æ•…éšœã€‚è¿™ä¸€é›†æˆæ•°æ®é›†ã€æŒ‡æ ‡å’Œä»£ç çš„æ¡†æ¶ä¸ºåŒºåŸŸä¾›çƒ­ç³»ç»Ÿçš„é¢„æµ‹æ€§ç»´æŠ¤(predictive maintenance)æä¾›äº†å¯å¤ç°çš„åŸºå‡†ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "30 pages, 6 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.14791v1",
      "published_date": "2025-11-14 13:31:24 UTC",
      "updated_date": "2025-11-14 13:31:24 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:18:33.866817+00:00"
    },
    {
      "arxiv_id": "2511.20669v1",
      "title": "Structured Definitions and Segmentations for Legal Reasoning in LLMs: A Study on Indian Legal Data",
      "title_zh": "LLM æ³•å¾‹æ¨ç†ä¸­çš„ç»“æ„åŒ–å®šä¹‰ä¸æ–‡æœ¬åˆ†æ®µï¼šåŸºäºå°åº¦æ³•å¾‹æ•°æ®çš„ç ”ç©¶",
      "authors": [
        "Mann Khatri",
        "Mirza Yusuf",
        "Rajiv Ratn Shah",
        "Ponnurangam Kumaraguru"
      ],
      "abstract": "Large Language Models (LLMs), trained on extensive datasets from the web, exhibit remarkable general reasoning skills. Despite this, they often struggle in specialized areas like law, mainly because they lack domain-specific pretraining. The legal field presents unique challenges, as legal documents are generally long and intricate, making it hard for models to process the full text efficiently. Previous studies have examined in-context approaches to address the knowledge gap, boosting model performance in new domains without full domain alignment. In our paper, we analyze model behavior on legal tasks by conducting experiments in three areas: (i) reorganizing documents based on rhetorical roles to assess how structured information affects long context processing and model decisions, (ii) defining rhetorical roles to familiarize the model with legal terminology, and (iii) emulating the step-by-step reasoning of courts regarding rhetorical roles to enhance model reasoning. These experiments are conducted in a zero-shot setting across three Indian legal judgment prediction datasets. Our results reveal that organizing data or explaining key legal terms significantly boosts model performance, with a minimum increase of ~1.5% and a maximum improvement of 4.36% in F1 score compared to the baseline.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•é€šè¿‡ç»“æ„åŒ–å®šä¹‰å’Œåˆ†æ®µæå‡å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ³•å¾‹æ¨ç†ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«é’ˆå¯¹å¤„ç†å¤æ‚å†—é•¿çš„å°åº¦æ³•å¾‹æ–‡æ¡£ã€‚ä½œè€…åœ¨ä¸‰ä¸ªå°åº¦æ³•å¾‹åˆ¤å†³é¢„æµ‹æ•°æ®é›†ä¸Šå¼€å±•äº†é›¶æ ·æœ¬(zero-shot)å®éªŒï¼Œä»é‡æ–°ç»„ç»‡ä¿®è¾è§’è‰²(rhetorical roles)ã€å®šä¹‰ä¿®è¾è§’è‰²æœ¯è¯­ä»¥åŠæ¨¡æ‹Ÿæ³•é™¢é€æ­¥æ¨ç†è¿‡ç¨‹ä¸‰ä¸ªç»´åº¦è¯„ä¼°æ¨¡å‹è¡Œä¸ºã€‚ç ”ç©¶å‘ç°ï¼Œé€šè¿‡ç»“æ„åŒ–ç»„ç»‡æ–‡æ¡£ä¿¡æ¯æˆ–è§£é‡Šæ ¸å¿ƒæ³•å¾‹æœ¯è¯­ï¼Œå¯ä»¥æœ‰æ•ˆå¼¥è¡¥LLMsåœ¨æ³•å¾‹ç‰¹å®šé¢†åŸŸé¢„è®­ç»ƒä¸è¶³çš„ç¼ºé™·ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•ä½¿æ¨¡å‹çš„ F1 score ç›¸æ¯”åŸºçº¿æå‡äº†1.5%è‡³4.36%ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†åœ¨æ— éœ€å…¨é¢†åŸŸå¯¹é½çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡ä¼˜åŒ–å³å¯æ˜¾è‘—å¢å¼ºæ¨¡å‹å¤„ç†æ³•å¾‹é•¿æ–‡æœ¬åŠå¤æ‚æ¨ç†ä»»åŠ¡çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted at BDA 2025 as short paper; This paper is long version",
      "pdf_url": "https://arxiv.org/pdf/2511.20669v1",
      "published_date": "2025-11-14 13:24:00 UTC",
      "updated_date": "2025-11-14 13:24:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:18:38.188141+00:00"
    },
    {
      "arxiv_id": "2511.11287v1",
      "title": "Building the Web for Agents: A Declarative Framework for Agent-Web Interaction",
      "title_zh": "ä¸ºæ™ºèƒ½ä½“æ„å»º Webï¼šä¸€ç§æ™ºèƒ½ä½“-Web äº¤äº’çš„å£°æ˜å¼æ¡†æ¶",
      "authors": [
        "Sven Schultze",
        "Meike Verena Kietzmann",
        "Nils-Lucas SchÃ¶nfeld",
        "Ruth Stock-Homburg"
      ],
      "abstract": "The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces <tool> and <context> tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ AI æ™ºèƒ½ä½“åœ¨å¤„ç†é¢å‘äººç±»çš„ç½‘é¡µç•Œé¢æ—¶å­˜åœ¨çš„äº¤äº’è„†å¼±å’Œå®‰å…¨æ€§é—®é¢˜ï¼Œæå‡ºäº† VOIX è¿™ä¸€ Web åŸç”Ÿæ¡†æ¶ã€‚VOIX é€šè¿‡å¼•å…¥ `<tool>` å’Œ `<context>` ç­‰å£°æ˜å¼ HTML æ ‡ç­¾ï¼Œå…è®¸ç½‘ç«™å¼€å‘è€…æ˜¾å¼å®šä¹‰æ™ºèƒ½ä½“å¯ç”¨çš„æ“ä½œå’Œç›¸å…³çŠ¶æ€ï¼Œä»è€Œå»ºç«‹æ¸…æ™°çš„æœºå™¨å¯è¯»å¥‘çº¦ã€‚è¿™ç§æ–¹æ³•åœ¨å°†æ§åˆ¶æƒäº¤è¿˜ç»™å¼€å‘è€…çš„åŒæ—¶ï¼Œé€šè¿‡éš”ç¦»å¯¹è¯äº¤äº’ä¸ç½‘ç«™å†…å®¹ï¼Œæœ‰æ•ˆä¿æŠ¤äº†ç”¨æˆ·éšç§ã€‚åœ¨ 16 åå¼€å‘è€…å‚ä¸çš„é»‘å®¢æ¾ç ”ç©¶ä¸­ï¼Œå®éªŒç»“æœè¯æ˜äº† VOIX å…·æœ‰è‰¯å¥½çš„å®ç”¨æ€§å’Œæ˜“å­¦æ€§ï¼Œä¸åŒèƒŒæ™¯çš„å¼€å‘è€…å‡èƒ½å¿«é€Ÿæ„å»ºåŠŸèƒ½ä¸°å¯Œçš„ Agentic Web åº”ç”¨ã€‚è¯¥é¡¹å·¥ä½œä¸ºå®ç°æ— ç¼ã€å®‰å…¨çš„æ™ºèƒ½ä½“ç½‘ç»œå¥ å®šäº†åŸºç¡€æ€§æœºåˆ¶ï¼Œæ˜¯æ¨åŠ¨äº’è”ç½‘äººæœºåä½œæœªæ¥å‘å±•çš„å…³é”®ä¸€æ­¥ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.MA"
      ],
      "primary_category": "cs.HC",
      "comment": "for associated documentation, see https://svenschultze.github.io/VOIX/",
      "pdf_url": "https://arxiv.org/pdf/2511.11287v1",
      "published_date": "2025-11-14 13:23:34 UTC",
      "updated_date": "2025-11-14 13:23:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:18:39.180009+00:00"
    },
    {
      "arxiv_id": "2511.11286v1",
      "title": "D-GAP: Improving Out-of-Domain Robustness via Dataset-Agnostic and Gradient-Guided Augmentation in Amplitude and Pixel Spaces",
      "title_zh": "D-GAPï¼šé€šè¿‡å¹…åº¦ä¸åƒç´ ç©ºé—´ä¸­æ•°æ®é›†æ— å…³ä¸”æ¢¯åº¦å¼•å¯¼çš„å¢å¼ºæå‡åŸŸå¤–é²æ£’æ€§",
      "authors": [
        "Ruoqi Wang",
        "Haitao Wang",
        "Shaojie Guo",
        "Qiong Luo"
      ],
      "abstract": "Out-of-domain (OOD) robustness is challenging to achieve in real-world computer vision applications, where shifts in image background, style, and acquisition instruments always degrade model performance. Generic augmentations show inconsistent gains under such shifts, whereas dataset-specific augmentations require expert knowledge and prior analysis. Moreover, prior studies show that neural networks adapt poorly to domain shifts because they exhibit a learning bias to domain-specific frequency components. Perturbing frequency values can mitigate such bias but overlooks pixel-level details, leading to suboptimal performance. To address these problems, we propose D-GAP (Dataset-agnostic and Gradient-guided augmentation in Amplitude and Pixel spaces), improving OOD robustness by introducing targeted augmentation in both the amplitude space (frequency space) and pixel space. Unlike conventional handcrafted augmentations, D-GAP computes sensitivity maps in the frequency space from task gradients, which reflect how strongly the model responds to different frequency components, and uses the maps to adaptively interpolate amplitudes between source and target samples. This way, D-GAP reduces the learning bias in frequency space, while a complementary pixel-space blending procedure restores fine spatial details. Extensive experiments on four real-world datasets and three domain-adaptation benchmarks show that D-GAP consistently outperforms both generic and dataset-specific augmentations, improving average OOD performance by +5.3% on real-world datasets and +1.8% on benchmark datasets.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç°å®ä¸­è®¡ç®—æœºè§†è§‰æ¨¡å‹åœ¨é¢å¯¹èƒŒæ™¯ã€é£æ ¼æˆ–é‡‡é›†ä»ªå™¨å˜åŒ–æ—¶ Out-of-Domain (OOD) é²æ£’æ€§æ˜¾è‘—ä¸‹é™çš„é—®é¢˜ï¼Œæå‡ºäº† D-GAP æ¡†æ¶ã€‚ç”±äºç¥ç»ç½‘ç»œå¯¹ç‰¹å®šé¢†åŸŸçš„é¢‘ç‡ç»„ä»¶å­˜åœ¨å­¦ä¹ åå·®ï¼Œä¸”ç°æœ‰å¢å¼ºæŠ€æœ¯å¾€å¾€éš¾ä»¥å…¼é¡¾åƒç´ ç»†èŠ‚ä¸é¢‘ç‡ç‰¹æ€§çš„å±€é™æ€§ï¼ŒD-GAP åœ¨æŒ¯å¹…ç©ºé—´ï¼ˆAmplitude Spaceï¼‰å’Œåƒç´ ç©ºé—´åŒæ—¶å¼•å…¥äº†é’ˆå¯¹æ€§çš„å¢å¼ºæœºåˆ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨ä»»åŠ¡æ¢¯åº¦åœ¨é¢‘ç‡ç©ºé—´è®¡ç®—æ•æ„Ÿåº¦å›¾ï¼Œå¹¶æ®æ­¤åœ¨æºæ ·æœ¬ä¸ç›®æ ‡æ ·æœ¬ä¹‹é—´è¿›è¡Œè‡ªé€‚åº”æŒ¯å¹…æ’å€¼ï¼Œä»¥æœ‰æ•ˆå‡è½»é¢‘ç‡ç©ºé—´çš„å­¦ä¹ åå·®ã€‚æ­¤å¤–ï¼ŒD-GAP ç»“åˆäº†åƒç´ ç©ºé—´æ··åˆç¨‹åºæ¥æ¢å¤ç²¾ç»†çš„ç©ºé—´ç»†èŠ‚ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰å¤æ‚çš„è§†è§‰ç‰¹å¾ã€‚å¤§é‡å®éªŒè¯æ˜ï¼ŒD-GAP åœ¨å››ä¸ªçœŸå®ä¸–ç•Œæ•°æ®é›†å’Œä¸‰ä¸ªé¢†åŸŸè‡ªé€‚åº”åŸºå‡†ä¸Šå‡ä¼˜äºé€šç”¨åŠç‰¹å®šé¢†åŸŸçš„å¢å¼ºæ–¹æ³•ï¼Œåˆ†åˆ«åœ¨å¹³å‡ OOD æ€§èƒ½ä¸Šå®ç°äº† 5.3% å’Œ 1.8% çš„æå‡ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11286v1",
      "published_date": "2025-11-14 13:22:10 UTC",
      "updated_date": "2025-11-14 13:22:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:18:54.468491+00:00"
    },
    {
      "arxiv_id": "2511.11281v1",
      "title": "Can You Tell the Difference? Contrastive Explanations for ABox Entailments",
      "title_zh": "ä½ èƒ½åˆ†è¾¨å‡ºå·®å¼‚å—ï¼Ÿé’ˆå¯¹ ABox è•´å«çš„å¯¹æ¯”æ€§è§£é‡Š",
      "authors": [
        "Patrick Koopmann",
        "Yasir Mahmood",
        "Axel-Cyrille Ngonga Ngomo",
        "Balram Tiwari"
      ],
      "abstract": "We introduce the notion of contrastive ABox explanations to answer questions of the type \"Why is a an instance of C, but b is not?\". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº†å¯¹æ¯”æ€§ ABox è§£é‡Šï¼ˆcontrastive ABox explanationsï¼‰çš„æ¦‚å¿µï¼Œæ—¨åœ¨è§£å†³å¦‚â€œä¸ºä»€ä¹ˆ a æ˜¯ C çš„å®ä¾‹ï¼Œè€Œ b ä¸æ˜¯â€è¿™ç±»å¯¹æ¯”æ€§é€»è¾‘æŸ¥è¯¢é—®é¢˜ã€‚ä¸åŒäºä»¥å¾€å­¤ç«‹å¤„ç†æ­£å‘è•´å«ï¼ˆpositive entailmentsï¼‰æˆ–ç¼ºå¤±è•´å«ï¼ˆmissing entailmentsï¼‰çš„æ–¹æ³•ï¼Œè¯¥æ¡†æ¶é€šè¿‡åŒæ—¶åˆ†æä¸¤è€…çš„å…±æ€§ä¸å·®å¼‚ï¼Œæä¾›äº†æ›´å…·é’ˆå¯¹æ€§çš„è§£é‡Šã€‚ç ”ç©¶é’ˆå¯¹æè¿°é€»è¾‘ï¼ˆDescription Logicï¼‰æœ¬ä½“ä¸­çš„ ABox æ¨ç†åˆ¶å®šäº†ä¸¥è°¨çš„å®šä¹‰ï¼Œå¹¶æ·±å…¥åˆ†æäº†åœ¨è½»é‡çº§ä¸å¼ºè¡¨è¾¾åŠ›æè¿°é€»è¾‘ä¸‹ï¼Œä¸åŒæœ€ä¼˜æ€§å‡†åˆ™å˜ä½“çš„è®¡ç®—å¤æ‚åº¦ã€‚æœ€åï¼Œä½œè€…å®ç°äº†ä¸€ç§ç”¨äºè®¡ç®—å¯¹æ¯”è§£é‡Šçš„æ–¹æ³•ï¼Œå¹¶åœ¨åŸºäºçœŸå®çŸ¥è¯†åº“çš„å®éªŒä¸­éªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.AI",
        "cs.LO"
      ],
      "primary_category": "cs.AI",
      "comment": "Technical report to the paper accepted at AAAI-2026",
      "pdf_url": "https://arxiv.org/pdf/2511.11281v1",
      "published_date": "2025-11-14 13:16:43 UTC",
      "updated_date": "2025-11-14 13:16:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:18:55.967662+00:00"
    },
    {
      "arxiv_id": "2511.11275v2",
      "title": "A Workflow for Full Traceability of AI Decisions",
      "title_zh": "äººå·¥æ™ºèƒ½å†³ç­–å…¨æµç¨‹å¯è¿½æº¯å·¥ä½œæµ",
      "authors": [
        "Julius Wenzel",
        "Syeda Umaima Alam",
        "Andreas Schmidt",
        "Hanwei Zhang",
        "Holger Hermanns"
      ],
      "abstract": "An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.\n  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹äººå·¥æ™ºèƒ½(AI)åœ¨é«˜é£é™©å†³ç­–ä¸­ç¼ºä¹æ–‡æ¡£è®°å½•ã€å¯¼è‡´è´£ä»»é“¾éš¾ä»¥è¿½æº¯åŠæ³•å¾‹åˆè§„æ€§å—é˜»çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§å®ç°AIå†³ç­–å…¨æµç¨‹è¿½æº¯çš„å·¥ä½œæµæ–¹æ¡ˆã€‚è¿™æ˜¯é¦–ä¸ªèƒ½å¤Ÿç”Ÿæˆé˜²ç¯¡æ”¹ã€å¯éªŒè¯ä¸”è¯¦å°½è¿½è¸ªè®°å½•çš„è¿è¡Œç³»ç»Ÿï¼Œæ—¨åœ¨å¼ºåˆ¶è®°å½•å‚ä¸æ¨¡å‹è®­ç»ƒä¸æ¨ç†çš„æ¯ä¸€ä¸ªç»„ä»¶ã€‚é€šè¿‡æ‰©å±•DBOM (Digital Bill of Materials) æ¦‚å¿µå¹¶æ•´åˆæœºå¯†è®¡ç®— (confidential computing) æŠ€æœ¯ï¼Œè¯¥ç ”ç©¶å°†ç†è®ºè½¬åŒ–ä¸ºå®é™…å¯æ“ä½œçš„å·¥ä½œæµã€‚ä¸ºäº†éªŒè¯å…¶æœ‰æ•ˆæ€§ï¼Œç ”ç©¶è€…å¼€å‘äº†ä¸€ä¸ªåŒºåˆ†æœ‰æ¯’ä¸é£Ÿç”¨è˜‘è‡çš„å†³ç­–æ”¯æŒåº”ç”¨ï¼Œç›´è§‚å±•ç¤ºäº†è¯¥å·¥ä½œæµåœ¨å¤„ç†å…³é”®å†³ç­–æ—¶çš„å†…éƒ¨è¿ä½œæœºåˆ¶ã€‚è¯¥æˆæœä¸ºæ„å»ºå¯ä¿¡ã€å¯å®¡è®¡çš„AIç³»ç»Ÿæä¾›äº†é‡è¦çš„æŠ€æœ¯æ”¯æ’‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "10 pages, 10 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.11275v2",
      "published_date": "2025-11-14 13:10:45 UTC",
      "updated_date": "2025-11-17 10:24:15 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:18:56.866763+00:00"
    },
    {
      "arxiv_id": "2511.11265v1",
      "title": "SQuaD: The Software Quality Dataset",
      "title_zh": "SQuaDï¼šè½¯ä»¶è´¨é‡æ•°æ®é›†",
      "authors": [
        "Mikel Robredo",
        "Matteo Esposito",
        "Davide Taibi",
        "Rafael PeÃ±aloza",
        "Valentina Lenarduzzi"
      ],
      "abstract": "Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†Software Quality Dataset (SQuaD)ï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹è½¯ä»¶ç³»ç»Ÿäº§å“å’Œè¿‡ç¨‹ç»´åº¦çš„å¤šç»´åº¦ã€æ—¶é—´æ„ŸçŸ¥çš„è½¯ä»¶è´¨é‡æŒ‡æ ‡æ•°æ®é›†ã€‚ç ”ç©¶å›¢é˜Ÿä»åŒ…æ‹¬Apacheã€Mozillaå’ŒLinux kernelåœ¨å†…çš„450ä¸ªæˆç†Ÿå¼€æºé¡¹ç›®ä¸­æå–æ•°æ®ï¼Œæ¶µç›–äº†63,586ä¸ªé¡¹ç›®å‘å¸ƒç‰ˆæœ¬ã€‚é€šè¿‡é›†æˆSonarQubeã€PMDå’ŒRefactoringMinerç­‰ä¹ç§å…ˆè¿›çš„é™æ€åˆ†æå·¥å…·ï¼ŒSQuaDåœ¨æ–¹æ³•ã€ç±»ã€æ–‡ä»¶å’Œé¡¹ç›®çº§åˆ«ç»Ÿä¸€äº†è¶…è¿‡700ä¸ªç‹¬ç‰¹æŒ‡æ ‡ã€‚é™¤äº†è´¨é‡åº¦é‡ï¼Œæ•°æ®é›†è¿˜é›†æˆäº†ç‰ˆæœ¬æ§åˆ¶ã€é—®é¢˜è·Ÿè¸ªå†å²ã€è½¯ä»¶æ¼æ´(CVE/CWE)ä»¥åŠæ—¨åœ¨å¢å¼ºåŠæ—¶ç¼ºé™·é¢„æµ‹(Just-In-Time defect prediction)çš„è¿‡ç¨‹æŒ‡æ ‡ã€‚è¯¥æ•°æ®é›†ä¸ºå¯ç»´æŠ¤æ€§(Maintainability)ã€æŠ€æœ¯å€º(Technical Debt)å’Œè½¯ä»¶æ¼”åŒ–(Software Evolution)çš„å®è¯ç ”ç©¶æä¾›äº†å‰æ‰€æœªæœ‰çš„è§„æ¨¡æ”¯æŒã€‚SQuaDå·²åœ¨ZENODOå¹³å°ä¸Šå…¬å¼€ï¼Œæ—¨åœ¨æ¨åŠ¨è‡ªåŠ¨åŒ–æ•°æ®é›†æ›´æ–°å’Œè·¨é¡¹ç›®è´¨é‡å»ºæ¨¡ç­‰é¢†åŸŸçš„ç ”ç©¶ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.CR",
        "cs.IR"
      ],
      "primary_category": "cs.SE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11265v1",
      "published_date": "2025-11-14 12:57:22 UTC",
      "updated_date": "2025-11-14 12:57:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:18:59.086775+00:00"
    },
    {
      "arxiv_id": "2511.11258v1",
      "title": "KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement",
      "title_zh": "KGQuestï¼šåŸºäºæ¨¡æ¿é©±åŠ¨ä¸å¤§è¯­è¨€æ¨¡å‹ç²¾ç‚¼çš„çŸ¥è¯†å›¾è°±é—®ç­”ç”Ÿæˆ",
      "authors": [
        "Sania Nayab",
        "Marco Simoni",
        "Giulio Rossolini",
        "Andrea Saracino"
      ],
      "abstract": "The generation of questions and answers (QA) from knowledge graphs (KG) plays a crucial role in the development and testing of educational platforms, dissemination tools, and large language models (LLM). However, existing approaches often struggle with scalability, linguistic quality, and factual consistency. This paper presents a scalable and deterministic pipeline for generating natural language QA from KGs, with an additional refinement step using LLMs to further enhance linguistic quality. The approach first clusters KG triplets based on their relations, creating reusable templates through natural language rules derived from the entity types of objects and relations. A module then leverages LLMs to refine these templates, improving clarity and coherence while preserving factual accuracy. Finally, the instantiation of answer options is achieved through a selection strategy that introduces distractors from the KG. Our experiments demonstrate that this hybrid approach efficiently generates high-quality QA pairs, combining scalability with fluency and linguistic precision.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† KGQuestï¼Œä¸€ä¸ªå¯æ‰©å±•ä¸”ç¡®å®šæ€§çš„æµæ°´çº¿æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ Knowledge Graphs (KG) è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡çš„è‡ªç„¶è¯­è¨€é—®ç­”å¯¹ (QA pairs)ã€‚é’ˆå¯¹ç°æœ‰æ–¹æ³•åœ¨å¯æ‰©å±•æ€§ã€è¯­è¨€è´¨é‡å’Œäº‹å®ä¸€è‡´æ€§æ–¹é¢çš„å±€é™ï¼ŒKGQuest é‡‡ç”¨äº†ä¸€ç§ç»“åˆæ¨¡æ¿é©±åŠ¨ç”Ÿæˆä¸ Large Language Models (LLM) ç»†åŒ–ä¼˜åŒ–çš„æ··åˆæ–¹æ¡ˆã€‚è¯¥æµç¨‹é¦–å…ˆæ ¹æ®å…³ç³»å¯¹ KG ä¸‰å…ƒç»„è¿›è¡Œèšç±»ï¼Œå¹¶ä¾æ®å®ä½“ç±»å‹å’Œå…³ç³»è§„åˆ™æ„å»ºå¯é‡ç”¨çš„è‡ªç„¶è¯­è¨€æ¨¡æ¿ã€‚éšåï¼Œåˆ©ç”¨ LLM å¯¹ç”Ÿæˆçš„æ¨¡æ¿è¿›è¡Œç²¾ç‚¼ï¼Œåœ¨ä¿æŒäº‹å®å‡†ç¡®æ€§çš„åŒæ—¶å¢å¼ºè¯­è¨€çš„æ¸…æ™°åº¦ä¸è¿è´¯æ€§ã€‚åœ¨ç­”æ¡ˆå®ä¾‹åŒ–é˜¶æ®µï¼Œæ¡†æ¶é€šè¿‡ç‰¹å®šç­–ç•¥ä» KG ä¸­å¼•å…¥å¹²æ‰°é¡¹ (distractors) ä»¥å®Œå–„é€‰é¡¹ã€‚å®éªŒè¯æ˜ï¼ŒKGQuest èƒ½å¤Ÿé«˜æ•ˆç”Ÿæˆå…·å¤‡é«˜åº¦è¯­è¨€ç²¾å‡†åº¦å’Œæµç•…æ€§çš„é—®ç­”æ•°æ®ï¼ŒæˆåŠŸå¹³è¡¡äº†ç”Ÿæˆæ•ˆç‡ä¸æ–‡æœ¬è´¨é‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11258v1",
      "published_date": "2025-11-14 12:54:01 UTC",
      "updated_date": "2025-11-14 12:54:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:19:05.183883+00:00"
    },
    {
      "arxiv_id": "2511.11257v1",
      "title": "AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery",
      "title_zh": "AIonopediaï¼šç”¨äºç¦»å­æ¶²ä½“å‘ç°çš„ååŒå¤šæ¨¡æ€å­¦ä¹ å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“",
      "authors": [
        "Yuqi Yin",
        "Yibo Fu",
        "Siyuan Wang",
        "Peng Sun",
        "Hongyu Wang",
        "Xiaohui Wang",
        "Lei Zheng",
        "Zhiyong Li",
        "Zhirong Liu",
        "Jianji Wang",
        "Zhaoxi Sun"
      ],
      "abstract": "The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AIonopediaï¼Œè¿™æ˜¯é¦–ä¸ªç”¨äºç¦»å­æ¶²ä½“(Ionic Liquids, ILs)å‘ç°çš„å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“(LLM agent)ï¼Œæ—¨åœ¨è§£å†³è¯¥é¢†åŸŸå±æ€§é¢„æµ‹ä¸­æ•°æ®æœ‰é™ã€æ¨¡å‹å‡†ç¡®æ€§å·®å’Œå·¥ä½œæµç¢ç‰‡åŒ–ç­‰æ ¸å¿ƒæŒ‘æˆ˜ã€‚è¯¥ç³»ç»Ÿç”±ä¸€ä¸ªLLMå¢å¼ºçš„å¤šæ¨¡æ€é¢†åŸŸåŸºç¡€æ¨¡å‹(LLM-augmented multimodal domain foundation model)é©±åŠ¨ï¼Œä¸ä»…èƒ½å®ç°ç²¾ç¡®çš„å±æ€§é¢„æµ‹ï¼Œè¿˜é›†æˆäº†ä¸€å¥—ç”¨äºåˆ†å­ç­›é€‰ä¸è®¾è®¡çš„å±‚æ¬¡åŒ–æœç´¢æ¶æ„(hierarchical search architecture)ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨å…¨æ–°ç­–åˆ’çš„å…¨é¢ç¦»å­æ¶²ä½“æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†è®­ç»ƒä¸è¯„ä¼°ï¼Œç»“æœè¯æ˜å…¶å…·æœ‰å“è¶Šçš„æ€§èƒ½å’Œæœ‰æ•ˆçš„åˆ†å­æ”¹æ€§èƒ½åŠ›ã€‚é€šè¿‡çœŸå®çš„æ¹¿å®éªŒå®¤(wet-lab)éªŒè¯ï¼ŒAIonopediaåœ¨å¤„ç†æŒ‘æˆ˜æ€§çš„åˆ†å¸ƒå¤–(out-of-distribution)ä»»åŠ¡æ—¶å±•ç°äº†æå¼ºçš„æ³›åŒ–èƒ½åŠ›ã€‚è¿™é¡¹å·¥ä½œæˆåŠŸè¯å®äº†LLMæ™ºèƒ½ä½“åœ¨åŠ é€Ÿç°å®ä¸–ç•Œç¦»å­æ¶²ä½“å‘ç°å’Œææ–™è®¾è®¡æ–¹é¢çš„å®ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.CE",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11257v1",
      "published_date": "2025-11-14 12:53:57 UTC",
      "updated_date": "2025-11-14 12:53:57 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:19:06.787027+00:00"
    },
    {
      "arxiv_id": "2511.11252v1",
      "title": "UAVBench: An Open Benchmark Dataset for Autonomous and Agentic AI UAV Systems via LLM-Generated Flight Scenarios",
      "title_zh": "UAVBenchï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆé£è¡Œåœºæ™¯çš„è‡ªä¸»ä¸æ™ºèƒ½ä½“ AI æ— äººæœºç³»ç»Ÿå¼€æºåŸºå‡†æ•°æ®é›†",
      "authors": [
        "Mohamed Amine Ferrag",
        "Abderrahmane Lakas",
        "Merouane Debbah"
      ],
      "abstract": "Autonomous aerial systems increasingly rely on large language models (LLMs) for mission planning, perception, and decision-making, yet the lack of standardized and physically grounded benchmarks limits systematic evaluation of their reasoning capabilities. To address this gap, we introduce UAVBench, an open benchmark dataset comprising 50,000 validated UAV flight scenarios generated through taxonomy-guided LLM prompting and multi-stage safety validation. Each scenario is encoded in a structured JSON schema that includes mission objectives, vehicle configuration, environmental conditions, and quantitative risk labels, providing a unified representation of UAV operations across diverse domains. Building on this foundation, we present UAVBench_MCQ, a reasoning-oriented extension containing 50,000 multiple-choice questions spanning ten cognitive and ethical reasoning styles, ranging from aerodynamics and navigation to multi-agent coordination and integrated reasoning. This framework enables interpretable and machine-checkable assessment of UAV-specific cognition under realistic operational contexts. We evaluate 32 state-of-the-art LLMs, including GPT-5, ChatGPT-4o, Gemini 2.5 Flash, DeepSeek V3, Qwen3 235B, and ERNIE 4.5 300B, and find strong performance in perception and policy reasoning but persistent challenges in ethics-aware and resource-constrained decision-making. UAVBench establishes a reproducible and physically grounded foundation for benchmarking agentic AI in autonomous aerial systems and advancing next-generation UAV reasoning intelligence. To support open science and reproducibility, we release the UAVBench dataset, the UAVBench_MCQ benchmark, evaluation scripts, and all related materials on GitHub at https://github.com/maferrag/UAVBench",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†UAVBenchï¼Œä¸€ä¸ªåŒ…å«5ä¸‡ä¸ªç»è¿‡éªŒè¯çš„æ— äººæœºé£è¡Œåœºæ™¯çš„å¼€æºåŸºå‡†æ•°æ®é›†ï¼Œæ—¨åœ¨è§£å†³è‡ªä¸»ç©ºä¸­ç³»ç»Ÿåœ¨æ¨ç†èƒ½åŠ›è¯„ä¼°æ–¹é¢ç¼ºä¹æ ‡å‡†åŒ–å’Œç‰©ç†æ¥åœ°(physically grounded)åŸºå‡†çš„é—®é¢˜ã€‚è¯¥æ•°æ®é›†é€šè¿‡åˆ†ç±»æ³•å¼•å¯¼çš„LLMæç¤ºå’Œå¤šé˜¶æ®µå®‰å…¨éªŒè¯ç”Ÿæˆï¼Œé‡‡ç”¨ç»“æ„åŒ–çš„JSON schemaè®°å½•ä»»åŠ¡ç›®æ ‡ã€ç¯å¢ƒæ¡ä»¶å’Œå®šé‡é£é™©æ ‡ç­¾ã€‚æ­¤å¤–ï¼Œç ”ç©¶è¿˜æ¨å‡ºäº†UAVBench_MCQæ‰©å±•åŒ…ï¼ŒåŒ…å«5ä¸‡é“æ¶µç›–ç©ºæ°”åŠ¨åŠ›å­¦ã€å¯¼èˆªå’Œå¤šæ™ºèƒ½ä½“åä½œç­‰10ç§æ¨ç†é£æ ¼çš„é€‰æ‹©é¢˜ï¼Œç”¨äºè¯„ä¼°æ— äººæœºç‰¹å®šè®¤çŸ¥ã€‚é€šè¿‡å¯¹32ä¸ªæœ€å…ˆè¿›çš„LLMï¼ˆå¦‚GPT-5ã€Gemini 2.5 Flashå’ŒDeepSeek V3ç­‰ï¼‰è¿›è¡Œæµ‹è¯•ï¼Œå‘ç°è¿™äº›æ¨¡å‹åœ¨æ„ŸçŸ¥å’Œç­–ç•¥æ¨ç†æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨ä¼¦ç†æ„è¯†å’Œèµ„æºå—é™å†³ç­–æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ã€‚UAVBenchä¸ºè‡ªä¸»ç©ºä¸­ç³»ç»Ÿçš„agentic AIåŸºå‡†æµ‹è¯•æä¾›äº†å¯å¤åˆ¶çš„åŸºç¡€ï¼Œå¯¹æ¨åŠ¨ä¸‹ä¸€ä»£æ— äººæœºæ¨ç†æ™ºèƒ½å…·æœ‰é‡è¦æ„ä¹‰ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "18 pages, 5 Figures",
      "pdf_url": "https://arxiv.org/pdf/2511.11252v1",
      "published_date": "2025-11-14 12:51:48 UTC",
      "updated_date": "2025-11-14 12:51:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:19:08.864238+00:00"
    },
    {
      "arxiv_id": "2511.11244v1",
      "title": "Toward Gaze Target Detection of Young Autistic Children",
      "title_zh": "é¢å‘å­¤ç‹¬ç—‡å¹¼å„¿çš„æ³¨è§†ç›®æ ‡æ£€æµ‹",
      "authors": [
        "Shijian Deng",
        "Erin E. Kosloski",
        "Siva Sai Nagender Vasireddy",
        "Jia Li",
        "Randi Sierra Sherwood",
        "Feroz Mohamed Hatha",
        "Siddhi Patel",
        "Pamela R Rollins",
        "Yapeng Tian"
      ],
      "abstract": "The automatic detection of gaze targets in autistic children through artificial intelligence can be impactful, especially for those who lack access to a sufficient number of professionals to improve their quality of life. This paper introduces a new, real-world AI application for gaze target detection in autistic children, which predicts a child's point of gaze from an activity image. This task is foundational for building automated systems that can measure joint attention-a core challenge in Autism Spectrum Disorder (ASD). To facilitate the study of this challenging application, we collected the first-ever Autism Gaze Target (AGT) dataset. We further propose a novel Socially Aware Coarse-to-Fine (SACF) gaze detection framework that explicitly leverages the social context of a scene to overcome the class imbalance common in autism datasets-a consequence of autistic children's tendency to show reduced gaze to faces. It utilizes a two-pathway architecture with expert models specialized in social and non-social gaze, guided by a context-awareness gate module. The results of our comprehensive experiments demonstrate that our framework achieves new state-of-the-art performance for gaze target detection in this population, significantly outperforming existing methods, especially on the critical minority class of face-directed gaze.",
      "tldr_zh": "è¯¥ç ”ç©¶æ—¨åœ¨é€šè¿‡äººå·¥æ™ºèƒ½å®ç°å¯¹å­¤ç‹¬ç—‡(ASD)å„¿ç«¥è§†çº¿ç›®æ ‡(gaze targets)çš„è‡ªåŠ¨æ£€æµ‹ï¼Œè¿™å¯¹äºè¯„ä¼°å…¶å…±åŒæ³¨æ„(joint attention)è¿™ä¸€æ ¸å¿ƒæŒ‘æˆ˜å…·æœ‰é‡è¦æ„ä¹‰ã€‚ä¸ºäº†æ”¯æ’‘è¯¥é¢†åŸŸçš„ç ”ç©¶ï¼Œä½œè€…æ„å»ºäº†é¦–ä¸ªå­¤ç‹¬ç—‡è§†çº¿ç›®æ ‡(AGT)æ•°æ®é›†ã€‚é’ˆå¯¹å­¤ç‹¬ç—‡å„¿ç«¥è¾ƒå°‘å…³æ³¨é¢éƒ¨å¯¼è‡´çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œç ”ç©¶æå‡ºäº†ä¸€ç§ç¤¾äº¤æ„ŸçŸ¥ç²—åˆ°ç²¾(SACF)è§†çº¿æ£€æµ‹æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨åŒè·¯å¾„æ¶æ„ï¼Œé€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥é—¨æ§æ¨¡å—å¼•å¯¼ä¸“é—¨è´Ÿè´£ç¤¾äº¤å’Œéç¤¾äº¤è§†çº¿çš„ä¸“å®¶æ¨¡å‹ï¼Œä»è€Œæ˜¾å¼åœ°æ•æ‰ç¤¾äº¤è¯­å¢ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒSACFæ¡†æ¶åœ¨è§†çº¿ç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›(SOTA)çš„æ€§èƒ½æ°´å¹³ã€‚ç‰¹åˆ«æ˜¯åœ¨é¢éƒ¨å®šå‘æ³¨è§†è¿™ä¸€å…³é”®å°‘æ•°ç±»åˆ«ä¸Šï¼Œå…¶è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼Œä¸ºè‡ªåŠ¨åŒ–è¾…åŠ©è¯Šæ–­ç³»ç»Ÿçš„æ„å»ºå¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "AAAI 2026 Artificial Intelligence for Social Impact Track",
      "pdf_url": "https://arxiv.org/pdf/2511.11244v1",
      "published_date": "2025-11-14 12:44:06 UTC",
      "updated_date": "2025-11-14 12:44:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:19:14.188242+00:00"
    },
    {
      "arxiv_id": "2511.11240v1",
      "title": "HealSplit: Towards Self-Healing through Adversarial Distillation in Split Federated Learning",
      "title_zh": "HealSplitï¼šé€šè¿‡æ‹†åˆ†è”é‚¦å­¦ä¹ ä¸­çš„å¯¹æŠ—è’¸é¦å®ç°è‡ªä¿®å¤",
      "authors": [
        "Yuhan Xie",
        "Chen Lyu"
      ],
      "abstract": "Split Federated Learning (SFL) is an emerging paradigm for privacy-preserving distributed learning. However, it remains vulnerable to sophisticated data poisoning attacks targeting local features, labels, smashed data, and model weights. Existing defenses, primarily adapted from traditional Federated Learning (FL), are less effective under SFL due to limited access to complete model updates. This paper presents HealSplit, the first unified defense framework tailored for SFL, offering end-to-end detection and recovery against five sophisticated types of poisoning attacks. HealSplit comprises three key components: (1) a topology-aware detection module that constructs graphs over smashed data to identify poisoned samples via topological anomaly scoring (TAS); (2) a generative recovery pipeline that synthesizes semantically consistent substitutes for detected anomalies, validated by a consistency validation student; and (3) an adversarial multi-teacher distillation framework trains the student using semantic supervision from a Vanilla Teacher and anomaly-aware signals from an Anomaly-Influence Debiasing (AD) Teacher, guided by the alignment between topological and gradient-based interaction matrices. Extensive experiments on four benchmark datasets demonstrate that HealSplit consistently outperforms ten state-of-the-art defenses, achieving superior robustness and defense effectiveness across diverse attack scenarios.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†é¦–ä¸ªä¸“ä¸ºæ‹†åˆ†è”é‚¦å­¦ä¹  (Split Federated Learning, SFL) è®¾è®¡çš„ç»Ÿä¸€é˜²å¾¡æ¡†æ¶ HealSplitï¼Œæ—¨åœ¨è§£å†³å…¶åœ¨åº”å¯¹é’ˆå¯¹å±€éƒ¨ç‰¹å¾ã€æ ‡ç­¾ã€ç²‰ç¢æ•°æ® (smashed data) å’Œæ¨¡å‹æƒé‡çš„å„ç§æŠ•æ¯’æ”»å‡»æ—¶çš„è„†å¼±æ€§ã€‚è¯¥æ¡†æ¶é€šè¿‡æ‹“æ‰‘æ„ŸçŸ¥æ£€æµ‹æ¨¡å—åˆ©ç”¨æ‹“æ‰‘å¼‚å¸¸è¯„åˆ† (Topological Anomaly Scoring, TAS) è¯†åˆ«æŠ•æ¯’æ ·æœ¬ï¼Œå¹¶ç»“åˆç”Ÿæˆå¼æ¢å¤æµæ°´çº¿ä¸ºå¼‚å¸¸æ•°æ®åˆæˆè¯­ä¹‰ä¸€è‡´çš„æ›¿ä»£å“ã€‚æ­¤å¤–ï¼ŒHealSplit é‡‡ç”¨äº†å¯¹æŠ—æ€§å¤šæ•™å¸ˆè’¸é¦æ¡†æ¶ï¼Œé€šè¿‡ Vanilla Teacher å’Œ Anomaly-Influence Debiasing (AD) Teacher çš„ä¿¡å·ååŒå¼•å¯¼å­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒã€‚å¹¿æ³›çš„å®éªŒè¯æ˜ï¼ŒHealSplit åœ¨å››ä¸ªåŸºå‡†æ•°æ®é›†ä¸Šçš„é²æ£’æ€§å’Œæœ‰æ•ˆæ€§å‡æ˜¾è‘—ä¼˜äºåç§æœ€å…ˆè¿› (State-of-the-art) çš„é˜²å¾¡æ–¹æ³•ï¼Œå®ç°äº†é’ˆå¯¹äº”ç§å¤æ‚æŠ•æ¯’æ”»å‡»çš„ç«¯åˆ°ç«¯æ£€æµ‹ä¸è‡ªæˆ‘ä¿®å¤ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.11240v1",
      "published_date": "2025-11-14 12:42:11 UTC",
      "updated_date": "2025-11-14 12:42:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:19:19.487550+00:00"
    },
    {
      "arxiv_id": "2511.11238v2",
      "title": "Virtual Width Networks",
      "title_zh": "è™šæ‹Ÿå®½åº¦ç½‘ç»œ",
      "authors": [
        "Seed",
        "Baisheng Li",
        "Banggu Wu",
        "Bole Ma",
        "Bowen Xiao",
        "Chaoyi Zhang",
        "Cheng Li",
        "Chengyi Wang",
        "Chengyin Xu",
        "Chi Zhang",
        "Chong Hu",
        "Daoguang Zan",
        "Defa Zhu",
        "Dongyu Xu",
        "Du Li",
        "Faming Wu",
        "Fan Xia",
        "Ge Zhang",
        "Guang Shi",
        "Haobin Chen",
        "Hongyu Zhu",
        "Hongzhi Huang",
        "Huan Zhou",
        "Huanzhang Dou",
        "Jianhui Duan",
        "Jianqiao Lu",
        "Jianyu Jiang",
        "Jiayi Xu",
        "Jiecao Chen",
        "Jin Chen",
        "Jin Ma",
        "Jing Su",
        "Jingji Chen",
        "Jun Wang",
        "Jun Yuan",
        "Juncai Liu",
        "Jundong Zhou",
        "Kai Hua",
        "Kai Shen",
        "Kai Xiang",
        "Kaiyuan Chen",
        "Kang Liu",
        "Ke Shen",
        "Liang Xiang",
        "Lin Yan",
        "Lishu Luo",
        "Mengyao Zhang",
        "Ming Ding",
        "Mofan Zhang",
        "Nianning Liang",
        "Peng Li",
        "Penghao Huang",
        "Pengpeng Mu",
        "Qi Huang",
        "Qianli Ma",
        "Qiyang Min",
        "Qiying Yu",
        "Renming Pang",
        "Ru Zhang",
        "Shen Yan",
        "Shen Yan",
        "Shixiong Zhao",
        "Shuaishuai Cao",
        "Shuang Wu",
        "Siyan Chen",
        "Siyu Li",
        "Siyuan Qiao",
        "Tao Sun",
        "Tian Xin",
        "Tiantian Fan",
        "Ting Huang",
        "Ting-Han Fan",
        "Wei Jia",
        "Wenqiang Zhang",
        "Wenxuan Liu",
        "Xiangzhong Wu",
        "Xiaochen Zuo",
        "Xiaoying Jia",
        "Ximing Yang",
        "Xin Liu",
        "Xin Yu",
        "Xingyan Bin",
        "Xintong Hao",
        "Xiongcai Luo",
        "Xujing Li",
        "Xun Zhou",
        "Yanghua Peng",
        "Yangrui Chen",
        "Yi Lin",
        "Yichong Leng",
        "Yinghao Li",
        "Yingshuan Song",
        "Yiyuan Ma",
        "Yong Shan",
        "Yongan Xiang",
        "Yonghui Wu",
        "Yongtao Zhang",
        "Yongzhen Yao",
        "Yu Bao",
        "Yuehang Yang",
        "Yufeng Yuan",
        "Yunshui Li",
        "Yuqiao Xian",
        "Yutao Zeng",
        "Yuxuan Wang",
        "Zehua Hong",
        "Zehua Wang",
        "Zengzhi Wang",
        "Zeyu Yang",
        "Zhengqiang Yin",
        "Zhenyi Lu",
        "Zhexi Zhang",
        "Zhi Chen",
        "Zhi Zhang",
        "Zhiqi Lin",
        "Zihao Huang",
        "Zilin Xu",
        "Ziyun Wei",
        "Zuo Wang"
      ],
      "abstract": "We introduce Virtual Width Networks (VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size. VWN decouples representational width from backbone width, expanding the embedding space while keeping backbone compute nearly constant. In our large-scale experiment, an 8-times expansion accelerates optimization by over 2 times for next-token and 3 times for next-2-token prediction. The advantage amplifies over training as both the loss gap grows and the convergence-speedup ratio increases, showing that VWN is not only token-efficient but also increasingly effective with scale. Moreover, we identify an approximately log-linear scaling relation between virtual width and loss reduction, offering an initial empirical basis and motivation for exploring virtual-width scaling as a new dimension of large-model efficiency.",
      "tldr_zh": "è¯¥ç ”ç©¶å¼•å…¥äº† Virtual Width Networks (VWN) æ¡†æ¶ï¼Œæ—¨åœ¨æä¾›æ›´å®½è¡¨ç¤ºå¸¦æ¥çš„æ€§èƒ½ä¼˜åŠ¿ï¼ŒåŒæ—¶é¿å…å¢åŠ éšè—å±‚ç»´åº¦æ‰€äº§ç”Ÿçš„äºŒæ¬¡æ–¹è®¡ç®—æˆæœ¬ã€‚VWN é€šè¿‡å°†è¡¨ç¤ºå®½åº¦ (representational width) ä¸éª¨å¹²å®½åº¦ (backbone width) è§£è€¦ï¼Œåœ¨ä¿æŒéª¨å¹²è®¡ç®—é‡å‡ ä¹æ’å®šçš„å‰æä¸‹æ˜¾è‘—æ‰©å±•äº†åµŒå…¥ç©ºé—´ã€‚åœ¨å¤§è§„æ¨¡å®éªŒä¸­ï¼Œ8å€çš„å®½åº¦æ‰©å±•ä½¿ next-token é¢„æµ‹çš„ä¼˜åŒ–é€Ÿåº¦æå‡äº†2å€ä»¥ä¸Šï¼Œå¹¶å°† next-2-token é¢„æµ‹çš„ä¼˜åŒ–é€Ÿåº¦æå‡äº†3å€ã€‚éšç€è®­ç»ƒè¿›ç¨‹çš„æ¨è¿›ï¼ŒVWN çš„ä¼˜åŠ¿æ„ˆå‘æ˜¾è‘—ï¼Œè¡¨ç°ä¸ºæŸå¤±å·®è·çš„æ‰©å¤§å’Œæ”¶æ•›åŠ é€Ÿæ¯”çš„æŒç»­æå‡ï¼Œè¯æ˜äº†å…¶åœ¨å¤§è§„æ¨¡åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œè¯¥ç ”ç©¶å‘ç°è™šæ‹Ÿå®½åº¦ä¸æŸå¤±å‡å°‘ä¹‹é—´å­˜åœ¨è¿‘ä¼¼å¯¹æ•°çº¿æ€§ (log-linear) çš„æ‰©å±•å…³ç³»ï¼Œä¸ºå°†è™šæ‹Ÿå®½åº¦æ‰©å±• (virtual-width scaling) ä½œä¸ºå¤§æ¨¡å‹æ•ˆç‡ä¼˜åŒ–çš„æ–°ç»´åº¦æä¾›äº†åšå®çš„ç»éªŒåŸºç¡€ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11238v2",
      "published_date": "2025-11-14 12:41:57 UTC",
      "updated_date": "2025-11-17 16:00:09 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:19:18.275143+00:00"
    },
    {
      "arxiv_id": "2511.11233v1",
      "title": "STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models",
      "title_zh": "STaRï¼šé€šè¿‡æ…¢æ€è€ƒå¤§è¯­è¨€æ¨¡å‹è¿ˆå‘è®¤çŸ¥è¡¨æ ¼æ¨ç†",
      "authors": [
        "Huajian Zhang",
        "Mingyue Cheng",
        "Yucong Luo",
        "Xiaoyu Tao"
      ],
      "abstract": "Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† STaR (slow-thinking for table reasoning) æ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡èµ‹äºˆå¤§è¯­è¨€æ¨¡å‹ (LLMs) â€œæ…¢æ€è€ƒâ€èƒ½åŠ›ï¼Œè§£å†³è¡¨æ ¼æ¨ç† (table reasoning) ä¸­æ¨ç†æ·±åº¦ä¸è¶³åŠè¿‡ç¨‹ä¸ç¨³å®šç­‰å…³é”®å±€é™ã€‚è¯¥æ¡†æ¶é€šè¿‡æ˜¾å¼å»ºæ¨¡åˆ†æ­¥æ€è€ƒå’Œä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¨ç†ï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨ä¸¤é˜¶æ®µéš¾åº¦æ„ŸçŸ¥å¼ºåŒ–å­¦ä¹  (difficulty-aware reinforcement learning, DRL)ï¼Œä½¿æ¨¡å‹èƒ½ä»ç®€å•åˆ°å¤æ‚é€æ­¥å­¦ä¹ å¤åˆå¥–åŠ±ä¸‹çš„æ¨ç†ç­–ç•¥ã€‚åœ¨æ¨ç†é˜¶æ®µï¼ŒSTaR æ•´åˆäº† Token çº§åˆ«ç½®ä¿¡åº¦ä¸ç­”æ¡ˆä¸€è‡´æ€§ï¼Œé€šè¿‡è½¨è¿¹çº§çš„ä¸ç¡®å®šæ€§é‡åŒ– (uncertainty quantification) ç­›é€‰å‡ºæœ€å…·å…¬ä¿¡åŠ›çš„æ¨ç†è·¯å¾„ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒSTaR åœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸­å‡å–å¾—äº†å“è¶Šçš„æ€§èƒ½å¹¶æ˜¾è‘—å¢å¼ºäº†æ¨ç†ç¨³å®šæ€§ã€‚æ­¤å¤–ï¼Œè¯¥æ¨¡å‹åœ¨åŸŸå¤–æ•°æ®é›†ä¸Šå±•ç°å‡ºçš„å¼ºå¤§æ³›åŒ–èƒ½åŠ› (generalization)ï¼Œè¿›ä¸€æ­¥éªŒè¯äº†å…¶ä½œä¸ºè®¤çŸ¥å¯å‘å¼è¡¨æ ¼æ¨ç†æ–¹æ¡ˆçš„å¯é æ€§ä¸æ½œåŠ›ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11233v1",
      "published_date": "2025-11-14 12:34:17 UTC",
      "updated_date": "2025-11-14 12:34:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:19:33.777622+00:00"
    },
    {
      "arxiv_id": "2511.11231v1",
      "title": "3D Gaussian and Diffusion-Based Gaze Redirection",
      "title_zh": "åŸºäº 3D é«˜æ–¯ä¸æ‰©æ•£æ¨¡å‹çš„è§†çº¿é‡å®šå‘",
      "authors": [
        "Abiram Panchalingam",
        "Indu Bodala",
        "Stuart Middleton"
      ],
      "abstract": "High-fidelity gaze redirection is critical for generating augmented data to improve the generalization of gaze estimators. 3D Gaussian Splatting (3DGS) models like GazeGaussian represent the state-of-the-art but can struggle with rendering subtle, continuous gaze shifts. In this paper, we propose DiT-Gaze, a framework that enhances 3D gaze redirection models using a novel combination of Diffusion Transformer (DiT), weak supervision across gaze angles, and an orthogonality constraint loss. DiT allows higher-fidelity image synthesis, while our weak supervision strategy using synthetically generated intermediate gaze angles provides a smooth manifold of gaze directions during training. The orthogonality constraint loss mathematically enforces the disentanglement of internal representations for gaze, head pose, and expression. Comprehensive experiments show that DiT-Gaze sets a new state-of-the-art in both perceptual quality and redirection accuracy, reducing the state-of-the-art gaze error by 4.1% to 6.353 degrees, providing a superior method for creating synthetic training data. Our code and models will be made available for the research community to benchmark against.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†DiT-Gazeæ¡†æ¶ï¼Œæ—¨åœ¨é€šè¿‡ç»“åˆDiffusion Transformer (DiT)ã€è·¨è§†çº¿è§’åº¦çš„å¼±ç›‘ç£(Weak Supervision)å’Œæ­£äº¤çº¦æŸæŸå¤±(Orthogonality Constraint Loss)æ¥æ”¹è¿›3Dè§†çº¿é‡å®šå‘(Gaze Redirection)æ¨¡å‹ã€‚é’ˆå¯¹3D Gaussian Splatting (3DGS)åœ¨æ¸²æŸ“è¿ç»­è§†çº¿åç§»æ—¶çš„å±€é™æ€§ï¼ŒDiTè¢«ç”¨äºå®ç°æ›´é«˜ä¿çœŸçš„å›¾åƒåˆæˆï¼Œè€Œå¼±ç›‘ç£ç­–ç•¥åˆ™é€šè¿‡åˆæˆä¸­é—´è§’åº¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æä¾›å¹³æ»‘çš„è§†çº¿æ–¹å‘æµå½¢ã€‚æ­£äº¤çº¦æŸæŸå¤±åœ¨æ•°å­¦ä¸Šå¼ºåŒ–äº†è§†çº¿ã€å¤´éƒ¨å§¿æ€å’Œè¡¨æƒ…å†…éƒ¨è¡¨ç¤ºçš„è§£è€¦(Disentanglement)ï¼Œç¡®ä¿äº†ç”Ÿæˆçš„ç²¾ç¡®æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒDiT-Gazeåœ¨æ„ŸçŸ¥è´¨é‡å’Œé‡å®šå‘ç²¾åº¦ä¸Šå‡è¾¾åˆ°äº†æ–°çš„å…ˆè¿›æ°´å¹³(State-of-the-Art)ï¼ŒæˆåŠŸå°†è§†çº¿è¯¯å·®é™ä½äº†4.1%è‡³6.353åº¦ã€‚è¯¥æ¡†æ¶ä¸ºç”Ÿæˆè§†çº¿ä¼°è®¡å™¨çš„é«˜è´¨é‡åˆæˆè®­ç»ƒæ•°æ®æä¾›äº†ä¼˜è¶Šæ‰‹æ®µã€‚ç ”ç©¶å›¢é˜Ÿè®¡åˆ’å…¬å¼€å…¶ä»£ç å’Œæ¨¡å‹ï¼Œä»¥ä¾¿ç ”ç©¶ç¤¾åŒºè¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11231v1",
      "published_date": "2025-11-14 12:32:22 UTC",
      "updated_date": "2025-11-14 12:32:22 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:19:47.481502+00:00"
    },
    {
      "arxiv_id": "2511.11182v1",
      "title": "Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning",
      "title_zh": "å¤šæ™ºèƒ½ä½“å§åº•åšå¼ˆï¼šåŸºäºåäº‹å®æµ‹è¯•çš„å¤šæ¨¡æ€æ¨ç†å¹»è§‰æ¶ˆé™¤",
      "authors": [
        "Dayong Liang",
        "Xiao-Yong Wei",
        "Changmeng Zheng"
      ],
      "abstract": "Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like \"Who is Undercover?\". MUG reframes MAD as a process of detecting \"undercover\" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜åœ¨çš„å¹»è§‰(Hallucination)æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºMulti-agent Undercover Gaming (MUG)çš„åè®®ã€‚å—â€œè°æ˜¯å§åº•â€ç¤¾äº¤æ¸¸æˆçš„å¯å‘ï¼ŒMUGå°†å¤šæ™ºèƒ½ä½“è¾©è®º(Multi-Agent Debate)é‡æ„ä¸ºé€šè¿‡å¤šæ¨¡æ€åäº‹å®æµ‹è¯•(Counterfactual Test)æ£€æµ‹â€œå§åº•â€æ™ºèƒ½ä½“ï¼ˆå³äº§ç”Ÿå¹»è§‰è€…ï¼‰çš„è¿‡ç¨‹ã€‚è¯¥æ¡†æ¶é€šè¿‡åŠ¨æ€ä¿®æ”¹å›¾åƒå¼•å…¥åäº‹å®è¯æ®ï¼Œå¹¶è§‚å¯Ÿæ™ºèƒ½ä½“èƒ½å¦è¯†åˆ«å˜åŒ–ï¼Œä»è€Œä¸ºè¯†åˆ«å¹»è§‰æä¾›Ground-truthï¼Œç¡®ä¿å¤šæ¨¡æ€æ¨ç†çš„ç¨³å¥æ€§ã€‚MUGä¸ä»…å®ç°äº†è¶…è¶Šç»Ÿè®¡å…±è¯†çš„äº‹å®æ ¡éªŒï¼Œè¿˜å¼•å…¥äº†è·¨è¯æ®æ¨ç†å¹¶ä¿ƒè¿›äº†æ™ºèƒ½ä½“é—´çš„ä¸»åŠ¨æ¢æµ‹æ€§è®¨è®ºã€‚è¿™äº›åˆ›æ–°å…±åŒä¸ºå¤§è¯­è¨€æ¨¡å‹çš„å¤šæ¨¡æ€æ¨ç†æä¾›äº†ä¸€ä¸ªæ¯”ä¼ ç»Ÿé™æ€è¾“å…¥æ›´å¯é ã€æ›´æœ‰æ•ˆçš„æŠ€æœ¯æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.MA",
        "cs.MM"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.11182v1",
      "published_date": "2025-11-14 11:27:55 UTC",
      "updated_date": "2025-11-14 11:27:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:19:54.980071+00:00"
    },
    {
      "arxiv_id": "2511.11172v1",
      "title": "Enhancing Group Recommendation using Soft Impute Singular Value Decomposition",
      "title_zh": "åˆ©ç”¨ Soft-Impute å¥‡å¼‚å€¼åˆ†è§£å¢å¼ºç¾¤ç»„æ¨è",
      "authors": [
        "Mubaraka Sani Ibrahim",
        "Isah Charles Saidu",
        "Lehel Csato"
      ],
      "abstract": "The growing popularity of group activities increased the need to develop methods for providing recommendations to a group of users based on the collective preferences of the group members. Several group recommender systems have been proposed, but these methods often struggle due to sparsity and high-dimensionality of the available data, common in many real-world applications. In this paper, we propose a group recommender system called Group Soft-Impute SVD, which leverages soft-impute singular value decomposition to enhance group recommendations. This approach addresses the challenge of sparse high-dimensional data using low-rank matrix completion. We compared the performance of Group Soft-Impute SVD with Group MF based approaches and found that our method outperforms the baselines in recall for small user groups while achieving comparable results across all group sizes when tasked on Goodbooks, Movielens, and Synthetic datasets. Furthermore, our method recovers lower matrix ranks than the baselines, demonstrating its effectiveness in handling high-dimensional data.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¾¤ç»„æ¨èç³»ç»Ÿä¸­å¸¸è§çš„æ•°æ®ç¨€ç–ï¼ˆsparsityï¼‰å’Œé«˜ç»´ï¼ˆhigh-dimensionalityï¼‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º Group Soft-Impute SVD çš„æ–°å‹æ¨èæ¡†æ¶ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ soft-impute singular value decomposition æŠ€æœ¯ï¼Œé€šè¿‡ä½ç§©çŸ©é˜µè¡¥å…¨ï¼ˆlow-rank matrix completionï¼‰æ¥æœ‰æ•ˆæ•æ‰ç¾¤ä½“æˆå‘˜çš„é›†ä½“åå¥½ã€‚ç ”ç©¶åœ¨ Goodbooksã€Movielens åŠåˆæˆæ•°æ®é›†ï¼ˆSynthetic datasetsï¼‰ä¸Šè¿›è¡Œäº†å®éªŒï¼Œç»“æœæ˜¾ç¤ºè¯¥æ–¹æ³•åœ¨å°è§„æ¨¡ç”¨æˆ·ç¾¤ç»„çš„ recall æŒ‡æ ‡ä¸Šä¼˜äºä¼ ç»Ÿçš„ Group MF æ–¹æ³•ï¼Œä¸”åœ¨ä¸åŒè§„æ¨¡çš„ç¾¤ç»„ä¸­å‡è¡¨ç°ç¨³å¥ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿå®ç°æ›´ä½çš„çŸ©é˜µç§©ï¼ˆmatrix ranksï¼‰æ¢å¤ï¼Œå……åˆ†éªŒè¯äº†å…¶åœ¨å¤„ç†å¤æ‚é«˜ç»´æ•°æ®æ—¶çš„ä¼˜è¶Šæ€§ä¸æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "primary_category": "cs.IR",
      "comment": "((1) African University of Science and Technology (Abuja, Nigeria), (2) Baze University (Abuja, Nigeria), (3) Babes-Bolyai University (Cluj-Napoca, Romania))",
      "pdf_url": "https://arxiv.org/pdf/2511.11172v1",
      "published_date": "2025-11-14 11:13:56 UTC",
      "updated_date": "2025-11-14 11:13:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:19:45.170834+00:00"
    },
    {
      "arxiv_id": "2511.11169v1",
      "title": "Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA",
      "title_zh": "ç²¾ç‚¼ä¸å¯¹é½ï¼šè§†è§‰é—®ç­”ä¸­åŸºäºå¤šæ™ºèƒ½ä½“äº¤äº’çš„ç½®ä¿¡åº¦æ ¡å‡†",
      "authors": [
        "Ayush Pandey",
        "Jai Bardhan",
        "Ishita Jain",
        "Ramya S Hebbalaguppe",
        "Rohan Raju Dhanakshirur",
        "Lovekesh Vig"
      ],
      "abstract": "In the context of Visual Question Answering (VQA) and Agentic AI, calibration refers to how closely an AI system's confidence in its answers reflects their actual correctness. This aspect becomes especially important when such systems operate autonomously and must make decisions under visual uncertainty. While modern VQA systems, powered by advanced vision-language models (VLMs), are increasingly used in high-stakes domains like medical diagnostics and autonomous navigation due to their improved accuracy, the reliability of their confidence estimates remains under-examined. Particularly, these systems often produce overconfident responses. To address this, we introduce AlignVQA, a debate-based multi-agent framework, in which diverse specialized VLM -- each following distinct prompting strategies -- generate candidate answers and then engage in two-stage interaction: generalist agents critique, refine and aggregate these proposals. This debate process yields confidence estimates that more accurately reflect the model's true predictive performance. We find that more calibrated specialized agents produce better aligned confidences. Furthermore, we introduce a novel differentiable calibration-aware loss function called aligncal designed to fine-tune the specialized agents by minimizing an upper bound on the calibration error. This objective explicitly improves the fidelity of each agent's confidence estimates. Empirical results across multiple benchmark VQA datasets substantiate the efficacy of our approach, demonstrating substantial reductions in calibration discrepancies. Furthermore, we propose a novel differentiable calibration-aware loss to fine-tune the specialized agents and improve the quality of their individual confidence estimates based on minimising upper bound calibration error.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†è§‰é—®ç­” (Visual Question Answering, VQA) ç³»ç»Ÿåœ¨è‡ªä¸»å†³ç­–ä¸­æ™®éå­˜åœ¨çš„è¿‡åº¦è‡ªä¿¡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸º AlignVQA çš„åŸºäºè¾©è®ºæœºåˆ¶çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶ã€‚è¯¥æ¡†æ¶åˆ©ç”¨é‡‡ç”¨ä¸åŒæç¤ºç­–ç•¥çš„ä¸“ä¸šåŒ–è§†è§‰è¯­è¨€æ¨¡å‹ (VLMs) ç”Ÿæˆå€™é€‰ç­”æ¡ˆï¼Œå¹¶é€šè¿‡é€šç”¨æ™ºèƒ½ä½“çš„ä¸¤é˜¶æ®µäº¤äº’å¯¹ææ¡ˆè¿›è¡Œæ‰¹åˆ¤ã€ç»†åŒ–å’Œèšåˆã€‚è¿™ç§è¾©è®ºè¿‡ç¨‹äº§ç”Ÿçš„ç½®ä¿¡åº¦ä¼°è®¡èƒ½æ›´å‡†ç¡®åœ°åæ˜ æ¨¡å‹çš„çœŸå®é¢„æµ‹æ€§èƒ½ï¼Œä¸”ç ”ç©¶å‘ç°æ ¡å‡†è‰¯å¥½çš„ä¸“ä¸šæ™ºèƒ½ä½“èƒ½äº§ç”Ÿæ›´å¥½çš„å¯¹é½ç½®ä¿¡åº¦ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§åä¸º aligncal çš„æ–°å‹å¯å¾®æ ¡å‡†æ„ŸçŸ¥æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨é€šè¿‡æœ€å°åŒ–æ ¡å‡†è¯¯å·®ä¸Šç•Œæ¥å¾®è°ƒä¸“ä¸šæ™ºèƒ½ä½“å¹¶æå‡å…¶ç½®ä¿¡åº¦ä¼°è®¡çš„å¿ å®åº¦ã€‚åœ¨å¤šä¸ªåŸºå‡† VQA æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•æ˜¾è‘—å‡å°‘äº†æ ¡å‡†å·®å¼‚ï¼Œæœ‰æ•ˆå¢å¼ºäº†å¤æ‚è§†è§‰ç¯å¢ƒä¸‹æ™ºèƒ½ä½“ç³»ç»Ÿçš„å¯é æ€§ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "17 pages, 6 figures, 5 tables. Accepted to Special Track on AI Alignment, AAAI 2026. Project Page- https://refine-align.github.io/",
      "pdf_url": "https://arxiv.org/pdf/2511.11169v1",
      "published_date": "2025-11-14 11:08:21 UTC",
      "updated_date": "2025-11-14 11:08:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:19:52.780691+00:00"
    },
    {
      "arxiv_id": "2511.11162v1",
      "title": "OT-ALD: Aligning Latent Distributions with Optimal Transport for Accelerated Image-to-Image Translation",
      "title_zh": "OT-ALDï¼šåˆ©ç”¨æœ€ä¼˜ä¼ è¾“å¯¹é½æ½œåœ¨åˆ†å¸ƒï¼Œå®ç°å›¾åƒåˆ°å›¾åƒåŠ é€Ÿè½¬æ¢",
      "authors": [
        "Zhanpeng Wang",
        "Shuting Cao",
        "Yuhang Lu",
        "Yuhan Li",
        "Na Lei",
        "Zhongxuan Luo"
      ],
      "abstract": "The Dual Diffusion Implicit Bridge (DDIB) is an emerging image-to-image (I2I) translation method that preserves cycle consistency while achieving strong flexibility. It links two independently trained diffusion models (DMs) in the source and target domains by first adding noise to a source image to obtain a latent code, then denoising it in the target domain to generate the translated image. However, this method faces two key challenges: (1) low translation efficiency, and (2) translation trajectory deviations caused by mismatched latent distributions. To address these issues, we propose a novel I2I translation framework, OT-ALD, grounded in optimal transport (OT) theory, which retains the strengths of DDIB-based approach. Specifically, we compute an OT map from the latent distribution of the source domain to that of the target domain, and use the mapped distribution as the starting point for the reverse diffusion process in the target domain. Our error analysis confirms that OT-ALD eliminates latent distribution mismatches. Moreover, OT-ALD effectively balances faster image translation with improved image quality. Experiments on four translation tasks across three high-resolution datasets show that OT-ALD improves sampling efficiency by 20.29% and reduces the FID score by 2.6 on average compared to the top-performing baseline models.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Dual Diffusion Implicit Bridge (DDIB) åœ¨å›¾åƒåˆ°å›¾åƒ (Image-to-Image) ç¿»è¯‘ä¸­é¢ä¸´çš„æ•ˆç‡ä½ä¸‹åŠç”±äºæ½œåœ¨åˆ†å¸ƒ (latent distributions) ä¸åŒ¹é…å¯¼è‡´çš„ç¿»è¯‘è½¨è¿¹åå·®ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸º OT-ALD çš„æ–°å‹ç¿»è¯‘æ¡†æ¶ã€‚è¯¥æ¡†æ¶ç«‹è¶³äºæœ€ä¼˜ä¼ è¾“ (Optimal Transport) ç†è®ºï¼Œé€šè¿‡è®¡ç®—ä»æºåŸŸåˆ°ç›®æ ‡åŸŸæ½œåœ¨åˆ†å¸ƒçš„ OT mapï¼Œå¹¶åˆ©ç”¨æ˜ å°„åçš„åˆ†å¸ƒä½œä¸ºç›®æ ‡åŸŸåå‘æ‰©æ•£è¿‡ç¨‹çš„èµ·å§‹ç‚¹ã€‚è¯¯å·®åˆ†æè¡¨æ˜ï¼ŒOT-ALD èƒ½å¤Ÿæœ‰æ•ˆæ¶ˆé™¤æ½œåœ¨åˆ†å¸ƒçš„ä¸åŒ¹é…ç°è±¡ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æˆåŠŸå¹³è¡¡äº†æ›´å¿«çš„å›¾åƒç¿»è¯‘é€Ÿåº¦ä¸æ›´é«˜çš„å›¾åƒè´¨é‡ã€‚å®éªŒåœ¨å››ä¸ªç¿»è¯‘ä»»åŠ¡å’Œä¸‰ä¸ªé«˜åˆ†è¾¨ç‡æ•°æ®é›†ä¸Šå±•å¼€ï¼Œç»“æœæ˜¾ç¤º OT-ALD ç›¸æ¯”æœ€å…ˆè¿›çš„åŸºçº¿æ¨¡å‹ï¼Œé‡‡æ ·æ•ˆç‡æå‡äº† 20.29%ï¼Œä¸” FID è¯„åˆ†å¹³å‡é™ä½äº† 2.6ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11162v1",
      "published_date": "2025-11-14 10:57:21 UTC",
      "updated_date": "2025-11-14 10:57:21 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:19:47.066296+00:00"
    },
    {
      "arxiv_id": "2511.13762v1",
      "title": "Gene Incremental Learning for Single-Cell Transcriptomics",
      "title_zh": "é¢å‘å•ç»†èƒè½¬å½•ç»„å­¦çš„åŸºå› å¢é‡å­¦ä¹ ",
      "authors": [
        "Jiaxin Qi",
        "Yan Cui",
        "Jianqiang Huang",
        "Gaogang Xie"
      ],
      "abstract": "Classes, as fundamental elements of Computer Vision, have been extensively studied within incremental learning frameworks. In contrast, tokens, which play essential roles in many research fields, exhibit similar characteristics of growth, yet investigations into their incremental learning remain significantly scarce. This research gap primarily stems from the holistic nature of tokens in language, which imposes significant challenges on the design of incremental learning frameworks for them. To overcome this obstacle, in this work, we turn to a type of token, gene, for a large-scale biological dataset--single-cell transcriptomics--to formulate a pipeline for gene incremental learning and establish corresponding evaluations. We found that the forgetting problem also exists in gene incremental learning, thus we adapted existing class incremental learning methods to mitigate the forgetting of genes. Through extensive experiments, we demonstrated the soundness of our framework design and evaluations, as well as the effectiveness of our method adaptations. Finally, we provide a complete benchmark for gene incremental learning in single-cell transcriptomics.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å•ç»†èƒè½¬å½•ç»„å­¦ (single-cell transcriptomics) ä¸­çš„åŸºå› å¢é‡å­¦ä¹  (gene incremental learning) é—®é¢˜ã€‚ä½œè€…æŒ‡å‡ºï¼Œè™½ç„¶è®¡ç®—æœºè§†è§‰ä¸­çš„ç±»å¢é‡å­¦ä¹  (class incremental learning) å·²æœ‰æ·±å…¥ç ”ç©¶ï¼Œä½†é’ˆå¯¹å¦‚â€œåŸºå› â€è¿™ç±»å…·æœ‰å¢é•¿ç‰¹æ€§çš„æ ‡è®° (tokens) çš„å¢é‡å­¦ä¹ ç ”ç©¶ä»è¾ƒä¸ºåŒ®ä¹ã€‚ä¸ºæ­¤ï¼Œè¯¥å·¥ä½œé’ˆå¯¹å¤§è§„æ¨¡ç”Ÿç‰©æ•°æ®é›†æ„å»ºäº†åŸºå› å¢é‡å­¦ä¹ çš„æµæ°´çº¿åŠç›¸åº”çš„è¯„ä¼°ä½“ç³»ã€‚ç ”ç©¶å‘ç°åŸºå› å¢é‡å­¦ä¹ ä¸­åŒæ ·å­˜åœ¨é—å¿˜é—®é¢˜ï¼Œå› æ­¤ä½œè€…é€‚é…å¹¶æ”¹è¿›äº†ç°æœ‰çš„ç±»å¢é‡å­¦ä¹ æ–¹æ³•ï¼Œä»¥ç¼“è§£æ¨¡å‹å¯¹æ—¢æœ‰åŸºå› çŸ¥è¯†çš„é—å¿˜ã€‚é€šè¿‡å¹¿æ³›çš„å®éªŒï¼Œè¯¥ç ”ç©¶éªŒè¯äº†æ‰€ææ¡†æ¶è®¾è®¡çš„åˆç†æ€§ä»¥åŠæ–¹æ³•é€‚é…çš„æœ‰æ•ˆæ€§ã€‚æœ€åï¼Œè¯¥å·¥ä½œä¸ºå•ç»†èƒè½¬å½•ç»„å­¦é¢†åŸŸçš„åŸºå› å¢é‡å­¦ä¹ æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„åŸºå‡† (benchmark)ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸçš„ç ”ç©¶ç©ºç™½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.GN"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.13762v1",
      "published_date": "2025-11-14 10:54:03 UTC",
      "updated_date": "2025-11-14 10:54:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:19:58.470171+00:00"
    },
    {
      "arxiv_id": "2511.13761v1",
      "title": "What happens when nanochat meets DiLoCo?",
      "title_zh": "å½“ nanochat é‡ä¸Š DiLoCoï¼šé€šä¿¡å—é™ä¸‹çš„åˆ†å¸ƒå¼è®­ç»ƒæ¢ç©¶",
      "authors": [
        "Alexander Acker",
        "Soeren Becker",
        "Sasho Nedelkoski",
        "Dominik Scheinert",
        "Odej Kao",
        "Philipp Wiesner"
      ],
      "abstract": "Although LLM training is typically centralized with high-bandwidth interconnects and large compute budgets, emerging methods target communication-constrained training in distributed environments. The model trade-offs introduced by this shift remain underexplored, and our goal is to study them.\n  We use the open-source nanochat project, a compact 8K-line full-stack ChatGPT-like implementation containing tokenization, pretraining, fine-tuning, and serving, as a controlled baseline. We implement the DiLoCo algorithm as a lightweight wrapper over nanochat's training loop, performing multiple local steps per worker before synchronization with an outer optimizer, effectively reducing communication by orders of magnitude. This inner-outer training is compared against a standard data-parallel (DDP) setup. Because nanochat is small and inspectable, it enables controlled pipeline adaptations and allows direct comparison with the conventional centralized baseline.\n  DiLoCo achieves stable convergence and competitive loss in pretraining but yields worse MMLU, GSM8K, and HumanEval scores after mid-training and SFT. We discover that using DiLoCo-pretrained weights and running mid- and post-training with DDP fails to recover performance, revealing irreversible representation drift from asynchronous updates that impairs downstream alignment. We provide this implementation as an official fork of nanochat on GitHub.",
      "tldr_zh": "è¯¥ç ”ç©¶åˆ©ç”¨ç´§å‡‘å‹å…¨æ ˆé¡¹ç›® nanochat æ¢è®¨äº†åœ¨é€šä¿¡å—é™ç¯å¢ƒä¸‹åº”ç”¨ DiLoCo ç®—æ³•è¿›è¡Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æ¨¡å‹æƒè¡¡ã€‚ç ”ç©¶è€…å°† DiLoCo å®ç°ä¸ºè®­ç»ƒå¾ªç¯çš„è½»é‡çº§åŒ…è£…å™¨ï¼Œé€šè¿‡åœ¨åŒæ­¥å‰æ‰§è¡Œå¤šæ¬¡å±€éƒ¨æ›´æ–°æ¥æ˜¾è‘—é™ä½é€šä¿¡é¢‘ç‡ï¼Œå¹¶å°†å…¶ä¸æ ‡å‡†çš„åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ (DDP) æ–¹æ¡ˆè¿›è¡Œäº†ç›´æ¥å¯¹æ¯”ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå°½ç®¡ DiLoCo åœ¨é¢„è®­ç»ƒ (pretraining) é˜¶æ®µè¡¨ç°å‡ºç¨³å®šçš„æ”¶æ•›å’Œç«äº‰æ€§çš„æŸå¤±å€¼ï¼Œä½†åœ¨ç»è¿‡ä¸­æœŸè®­ç»ƒ (mid-training) å’Œæœ‰ç›‘ç£å¾®è°ƒ (SFT) åï¼Œæ¨¡å‹åœ¨ MMLUã€GSM8K å’Œ HumanEval ç­‰ä¸‹æ¸¸è¯„ä¼°ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜¾è‘—å¼±äºåŸºå‡†æ¨¡å‹ã€‚è¿›ä¸€æ­¥åˆ†æå‘ç°ï¼ŒDiLoCo çš„å¼‚æ­¥æ›´æ–°ä¼šå¯¼è‡´ä¸å¯é€†çš„è¡¨å¾æ¼‚ç§» (irreversible representation drift)ï¼Œè¿™ç§æ¼‚ç§»å³ä¾¿åœ¨åç»­é˜¶æ®µåˆ‡æ¢å› DDP æ¨¡å¼ä¹Ÿæ— æ³•æ¢å¤ï¼Œä»è€Œä¸¥é‡æŸå®³äº†æ¨¡å‹çš„ä¸‹æ¸¸å¯¹é½æ€§èƒ½ã€‚è¯¥é¡¹å·¥ä½œæ­ç¤ºäº†ä½é€šä¿¡åˆ†å¸ƒå¼è®­ç»ƒå¯¹æ¨¡å‹æ·±å±‚ç‰¹å¾çš„å½±å“ï¼Œå¹¶ä½œä¸º nanochat çš„å®˜æ–¹åˆ†æ”¯å¼€æºäº†ç›¸å…³å®ç°ã€‚",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.DC",
      "comment": "8pages, 3 figures, technical report",
      "pdf_url": "https://arxiv.org/pdf/2511.13761v1",
      "published_date": "2025-11-14 10:30:04 UTC",
      "updated_date": "2025-11-14 10:30:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:20:08.485285+00:00"
    },
    {
      "arxiv_id": "2511.13760v1",
      "title": "MoETTA: Test-Time Adaptation Under Mixed Distribution Shifts with MoE-LayerNorm",
      "title_zh": "MoETTAï¼šåŸºäº MoE-LayerNorm çš„æ··åˆåˆ†å¸ƒåç§»æµ‹è¯•æ—¶è‡ªé€‚åº”",
      "authors": [
        "Xiao Fan",
        "Jingyan Jiang",
        "Zhaoru Chen",
        "Fanding Huang",
        "Xiao Chen",
        "Qinting Jiang",
        "Bowen Zhang",
        "Xing Tang",
        "Zhi Wang"
      ],
      "abstract": "Test-Time adaptation (TTA) has proven effective in mitigating performance drops under single-domain distribution shifts by updating model parameters during inference. However, real-world deployments often involve mixed distribution shifts, where test samples are affected by diverse and potentially conflicting domain factors, posing significant challenges even for SOTA TTA methods. A key limitation in existing approaches is their reliance on a unified adaptation path, which fails to account for the fact that optimal gradient directions can vary significantly across different domains. Moreover, current benchmarks focus only on synthetic or homogeneous shifts, failing to capture the complexity of real-world heterogeneous mixed distribution shifts. To address this, we propose MoETTA, a novel entropy-based TTA framework that integrates the Mixture-of-Experts (MoE) architecture. Rather than enforcing a single parameter update rule for all test samples, MoETTA introduces a set of structurally decoupled experts, enabling adaptation along diverse gradient directions. This design allows the model to better accommodate heterogeneous shifts through flexible and disentangled parameter updates. To simulate realistic deployment conditions, we introduce two new benchmarks: potpourri and potpourri+. While classical settings focus solely on synthetic corruptions, potpourri encompasses a broader range of domain shifts--including natural, artistic, and adversarial distortions--capturing more realistic deployment challenges. Additionally, potpourri+ further includes source-domain samples to evaluate robustness against catastrophic forgetting. Extensive experiments across three mixed distribution shifts settings show that MoETTA consistently outperforms strong baselines, establishing SOTA performance and highlighting the benefit of modeling multiple adaptation directions via expert-level diversity.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æµ‹è¯•æ—¶è‡ªé€‚åº”(Test-Time adaptation, TTA)åœ¨å¤„ç†æ··åˆåˆ†å¸ƒåç§»(mixed distribution shifts)æ—¶ï¼Œå› ä¾èµ–ç»Ÿä¸€é€‚é…è·¯å¾„è€Œæ— æ³•åº”å¯¹å¤šæ ·åŒ–é¢†åŸŸå› ç´ æŒ‘æˆ˜çš„é—®é¢˜ï¼Œæå‡ºäº†MoETTAæ¡†æ¶ã€‚è¯¥æ¡†æ¶æ˜¯ä¸€ç§é›†æˆæ··åˆä¸“å®¶(Mixture-of-Experts, MoE)æ¶æ„çš„æ–°å‹åŸºäºç†µçš„TTAç³»ç»Ÿï¼Œé€šè¿‡å¼•å…¥ä¸€ç»„ç»“æ„è§£è€¦çš„ä¸“å®¶ï¼Œå®ç°äº†æ²¿å¤šæ ·åŒ–æ¢¯åº¦æ–¹å‘çš„çµæ´»ä¸”è§£è€¦çš„å‚æ•°æ›´æ–°ã€‚ä¸ºäº†æ›´çœŸå®åœ°è¯„ä¼°æ€§èƒ½ï¼Œç ”ç©¶è€…è¿˜å¼•å…¥äº†potpourriå’Œpotpourri+ä¸¤ä¸ªæ–°åŸºå‡†ï¼Œæ¶µç›–äº†è‡ªç„¶ã€è‰ºæœ¯ã€å¯¹æŠ—æ€§å¤±çœŸåŠæºåŸŸæ ·æœ¬ï¼Œç”¨ä»¥è€ƒå¯Ÿæ¨¡å‹åœ¨å¼‚æ„åç§»ä¸‹çš„é²æ£’æ€§åŠå¯¹ç¾éš¾æ€§é—å¿˜(catastrophic forgetting)çš„æŠµæŠ—åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒMoETTAåœ¨å¤šç§æ··åˆåˆ†å¸ƒåç§»è®¾ç½®ä¸‹å‡æ˜¾è‘—ä¼˜äºç°æœ‰çš„å¼ºåŸºçº¿æ¨¡å‹ï¼Œè¾¾åˆ°äº†SOTAæ°´å¹³ï¼Œå……åˆ†è¯æ˜äº†é€šè¿‡ä¸“å®¶çº§å¤šæ ·æ€§å»ºæ¨¡å¤šä¸ªè‡ªé€‚åº”æ–¹å‘çš„ä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by AAAI 2026 Main Technical Track",
      "pdf_url": "https://arxiv.org/pdf/2511.13760v1",
      "published_date": "2025-11-14 10:24:06 UTC",
      "updated_date": "2025-11-14 10:24:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:20:03.508826+00:00"
    },
    {
      "arxiv_id": "2511.11144v1",
      "title": "Specification, Application, and Operationalization of a Metamodel of Fairness",
      "title_zh": "å…¬å¹³æ€§å…ƒæ¨¡å‹çš„è§„èŒƒã€åº”ç”¨ä¸æ“ä½œåŒ–\n\n---\n\næˆ‘å·²ç»ä¸ºæ‚¨å®Œæˆäº†è®ºæ–‡æ ‡é¢˜çš„ç¿»è¯‘ã€‚è¯¥è¯‘æ–‡å‡†ç¡®æ•æ‰äº†åŸæ–‡ä¸­ä»ç†è®ºå®šä¹‰ï¼ˆSpecificationï¼‰ã€å®é™…æ¡ˆä¾‹ï¼ˆApplicationï¼‰åˆ°æŠ€æœ¯è½åœ°ï¼ˆOperationalizationï¼‰çš„é€»è¾‘å±‚æ¬¡ï¼Œå¹¶ä½¿ç”¨äº†ç¬¦åˆä¸­æ–‡å­¦æœ¯è§„èŒƒçš„æœ¯è¯­ã€‚\n\nå¦‚æœæ‚¨è¿˜æœ‰å…¶ä»– arXiv è®ºæ–‡æ ‡é¢˜éœ€è¦ç¿»è¯‘ï¼Œæˆ–è€…å¸Œæœ›æˆ‘ä¸ºæ‚¨è¯¦ç»†è§£ææ‘˜è¦ä¸­æåˆ°çš„ **Tiles æ¡†æ¶** åŠå…¶å¯¹ **å…¬å¹³ä¸å¹³ç­‰ï¼ˆEquity vs Equalityï¼‰** çš„å½¢å¼åŒ–å®šä¹‰ï¼Œæ¬¢è¿éšæ—¶ä¸æˆ‘äº¤æµï¼âœ¨",
      "authors": [
        "Julian Alfredo Mendez",
        "Timotheus Kampik"
      ],
      "abstract": "This paper presents the AR fairness metamodel, aimed at formally representing, analyzing, and comparing fairness scenarios. The metamodel provides an abstract representation of fairness, enabling the formal definition of fairness notions. We instantiate the metamodel through several examples, with a particular focus on comparing the notions of equity and equality.\n  We use the Tiles framework, which offers modular components that can be interconnected to represent various definitions of fairness. Its primary objective is to support the operationalization of AR-based fairness definitions in a range of scenarios, providing a robust method for defining, comparing, and evaluating fairness.\n  Tiles has an open-source implementation for fairness modeling and evaluation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AR fairness metamodelï¼Œæ—¨åœ¨æ­£å¼è¡¨ç¤ºã€åˆ†æå’Œæ¯”è¾ƒå„ç§å…¬å¹³æ€§åœºæ™¯ã€‚è¯¥å…ƒæ¨¡å‹é€šè¿‡æä¾›å…¬å¹³æ€§çš„æŠ½è±¡è¡¨ç¤ºï¼Œå®ç°äº†å…¬å¹³æ€§æ¦‚å¿µçš„æ­£å¼å®šä¹‰ã€‚ç ”ç©¶äººå‘˜é€šè¿‡å¤šä¸ªå®ä¾‹å¯¹è¯¥å…ƒæ¨¡å‹è¿›è¡Œäº†å®ä¾‹åŒ–ï¼Œé‡ç‚¹æ¢è®¨å¹¶æ¯”è¾ƒäº† equity å’Œ equality è¿™ä¸¤ä¸ªæ ¸å¿ƒæ¦‚å¿µã€‚è®ºæ–‡è¿›ä¸€æ­¥å¼•å…¥äº† Tiles æ¡†æ¶ï¼Œåˆ©ç”¨å…¶æ¨¡å—åŒ–ç»„ä»¶çš„äº’è”æ¥è¡¨ç¤ºå¤šå…ƒåŒ–çš„å…¬å¹³æ€§å®šä¹‰ã€‚è¯¥ç ”ç©¶çš„ä¸»è¦ç›®æ ‡æ˜¯æ”¯æŒåŸºäº AR çš„å…¬å¹³æ€§å®šä¹‰åœ¨ä¸åŒåœºæ™¯ä¸‹çš„æ“ä½œåŒ– (operationalization)ï¼Œä»è€Œä¸ºå…¬å¹³æ€§çš„å®šä¹‰ã€å¯¹æ¯”å’Œè¯„ä¼°æä¾›äº†ä¸€å¥—ç¨³å¥çš„æ–¹æ³•ã€‚ç›®å‰ï¼ŒTiles æ¡†æ¶å·²å…·å¤‡å¼€æºå®ç°ï¼Œå¯ç›´æ¥ç”¨äºå…¬å¹³æ€§å»ºæ¨¡ä¸è¯„ä¼°å·¥ä½œã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11144v1",
      "published_date": "2025-11-14 10:21:07 UTC",
      "updated_date": "2025-11-14 10:21:07 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:20:26.484793+00:00"
    },
    {
      "arxiv_id": "2511.11134v2",
      "title": "GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models",
      "title_zh": "GGBenchï¼šé¢å‘ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹çš„å‡ ä½•ç”Ÿæˆå¼æ¨ç†åŸºå‡†",
      "authors": [
        "Jingxuan Wei",
        "Caijun Jia",
        "Xi Bai",
        "Xinglong Xu",
        "Siyuan Li",
        "Linzhuang Sun",
        "Bihui Yu",
        "Conghui He",
        "Lijun Wu",
        "Cheng Tan"
      ],
      "abstract": "The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.",
      "tldr_zh": "è¯¥ç ”ç©¶æŒ‡å‡ºç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹ (Unified Multimodal Models, UMMs) æ­£ä»è¢«åŠ¨æ„ŸçŸ¥è½¬å‘è·¨æ¨¡æ€ç”Ÿæˆï¼Œä½†ç°æœ‰åŸºå‡†æµ‹è¯•ä¸»è¦ä¾§é‡äºåˆ¤åˆ«å¼ç†è§£æˆ–æ— çº¦æŸå›¾åƒç”Ÿæˆï¼Œç¼ºä¹å¯¹ç”Ÿæˆå¼æ¨ç† (Generative Reasoning) æ•´åˆè®¤çŸ¥è¿‡ç¨‹çš„æœ‰æ•ˆè¯„ä¼°ã€‚ä¸ºå¡«è¡¥è¿™ä¸€ç©ºç™½ï¼Œç ”ç©¶å›¢é˜Ÿæå‡ºå°†å‡ ä½•æ„é€  (Geometric Construction) ä½œä¸ºç†æƒ³çš„æµ‹è¯•å¹³å°ï¼Œå› ä¸ºå®ƒæœ¬è´¨ä¸Šè¦æ±‚è¯­è¨€ç†è§£ä¸ç²¾ç¡®è§†è§‰ç”Ÿæˆçš„æ·±åº¦èåˆã€‚åŸºäºæ­¤ï¼Œç ”ç©¶è€…æ¨å‡ºäº† GGBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè¯„ä¼°å‡ ä½•ç”Ÿæˆå¼æ¨ç†è€Œè®¾è®¡çš„åŸºå‡†æµ‹è¯•æ¡†æ¶ã€‚è¯¥åŸºå‡†èƒ½å¤Ÿç³»ç»Ÿåœ°è¯Šæ–­æ¨¡å‹ä¸ä»…åœ¨ç†è§£å’Œæ¨ç†æ–¹é¢çš„è¡¨ç°ï¼Œè¿˜åŒ…æ‹¬å…¶ä¸»åŠ¨æ„å»ºè§£å†³æ–¹æ¡ˆçš„èƒ½åŠ›ï¼Œä»è€Œä¸ºä¸‹ä¸€ä»£æ™ºèƒ½ç³»ç»Ÿè®¾å®šäº†æ›´ä¸¥è‹›çš„æ ‡å‡†ã€‚GGBench çš„å»ºç«‹ä¸ºè¯„ä¼°å¤šæ¨¡æ€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„è®¤çŸ¥ä¸ç”Ÿæˆä¸€è‡´æ€§æä¾›äº†å…³é”®å·¥å…·ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "35 pages, 22 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.11134v2",
      "published_date": "2025-11-14 10:07:53 UTC",
      "updated_date": "2026-01-14 08:29:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:20:37.978840+00:00"
    },
    {
      "arxiv_id": "2511.11125v1",
      "title": "Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs",
      "title_zh": "åº”ç”¨å¤§è¯­è¨€æ¨¡å‹å®ç°å·¥ä¸šè¿‡ç¨‹è‡ªåŠ¨åŒ–ï¼šä»¥ RAPID ç¨‹åºä¿®æ”¹ä¸ºä¾‹",
      "authors": [
        "Salim Fares",
        "Steffen Herbold"
      ],
      "abstract": "How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨ Large Language Models (LLMs) è¿›è¡Œå·¥ä¸šè¿‡ç¨‹è‡ªåŠ¨åŒ–çš„æ½œåŠ›ï¼Œé‡ç‚¹å…³æ³¨äºä¿®æ”¹ RAPID ç¨‹åºçš„æ¡ˆä¾‹ç ”ç©¶ã€‚é’ˆå¯¹å·¥ä¸šé¢†åŸŸä¸­é«˜åº¦ä¸“ä¸šåŒ–ä¸”é€šå¸¸ä»…ç”¨äºä¸“æœ‰ç¯å¢ƒçš„ domain-specific languagesï¼Œä½œè€…ç ”ç©¶äº†ä¼ä¸šå¦‚ä½•åœ¨ä¸æŠ•å…¥å¤§é‡ç²¾åŠ›è¿›è¡Œæ¨¡å‹è®­ç»ƒçš„æƒ…å†µä¸‹æœ‰æ•ˆåº”ç”¨ LLMsã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé‡‡ç”¨ few-shot prompting æ–¹æ³•è¶³ä»¥åœ¨ LLM åŸç”Ÿæ”¯æŒè¾ƒå¼±çš„ç¼–ç¨‹è¯­è¨€ä¸­è§£å†³ç®€å•é—®é¢˜ã€‚æ­¤å¤–ï¼Œè¯¥æ–¹æ³•æ”¯æŒåœ¨æœ¬åœ° (on-premise) éƒ¨ç½²ï¼Œä»è€Œç¡®ä¿äº†ä¼ä¸šæ•æ„Ÿæ•°æ®çš„å®‰å…¨æ€§ã€‚è¿™ä¸€å‘ç°è¯æ˜äº†åœ¨å·¥ä¸šè‡ªåŠ¨åŒ–èƒŒæ™¯ä¸‹ï¼Œæ— éœ€å¤§è§„æ¨¡é¢†åŸŸç‰¹å®šè®­ç»ƒå³å¯å®ç°è½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„åˆæ­¥è‡ªåŠ¨åŒ–ã€‚",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "primary_category": "cs.SE",
      "comment": "Submitted to the International Conference on Software Engineering (ICSE) track Software Engineering in Practice (SEIP) 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.11125v1",
      "published_date": "2025-11-14 09:56:55 UTC",
      "updated_date": "2025-11-14 09:56:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:20:41.781296+00:00"
    },
    {
      "arxiv_id": "2511.11124v1",
      "title": "AV-Dialog: Spoken Dialogue Models with Audio-Visual Input",
      "title_zh": "AV-Dialogï¼šèåˆè§†å¬è¾“å…¥çš„å£è¯­å¯¹è¯æ¨¡å‹",
      "authors": [
        "Tuochao Chen",
        "Bandhav Veluri",
        "Hongyu Gong",
        "Shyamnath Gollakota"
      ],
      "abstract": "Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker, predict turn-taking, and generate coherent responses. By combining acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets, AV-Dialog achieves robust streaming transcription, semantically grounded turn-boundary detection and accurate responses, resulting in a natural conversational flow. Experiments show that AV-Dialog outperforms audio-only models under interference, reducing transcription errors, improving turn-taking prediction, and enhancing human-rated dialogue quality. These results highlight the power of seeing as well as hearing for speaker-aware interaction, paving the way for {spoken} dialogue agents that perform {robustly} in real-world, noisy environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†AV-Dialogï¼Œè¿™æ˜¯é¦–ä¸ªåŒæ—¶åˆ©ç”¨éŸ³é¢‘å’Œè§†è§‰çº¿ç´¢è¿›è¡Œç›®æ ‡å‘è¨€äººè¿½è¸ªã€è½®æ¢é¢„æµ‹(Turn-taking)åŠç”Ÿæˆè¿è´¯å“åº”çš„å¤šæ¨¡æ€å¯¹è¯æ¡†æ¶ã€‚è¯¥æ¡†æ¶é€šè¿‡å£°å­¦æ ‡è®°åŒ–(Acoustic tokenization)ç»“åˆå¤šä»»åŠ¡ã€å¤šé˜¶æ®µè®­ç»ƒï¼Œåœ¨å¤šç§è§†å¬å¯¹è¯æ•°æ®é›†ä¸Šå®ç°äº†ç¨³å¥çš„æµå¼è½¬å½•(Streaming transcription)å’Œè¯­ä¹‰å…³è”çš„è½®æ¢è¾¹ç•Œæ£€æµ‹(Turn-boundary detection)ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒAV-Dialog åœ¨å­˜åœ¨å¹²æ‰°çš„ç¯å¢ƒä¸‹æ˜¾è‘—ä¼˜äºçº¯éŸ³é¢‘æ¨¡å‹ï¼Œä¸ä»…é™ä½äº†è½¬å½•é”™è¯¯ï¼Œè¿˜å¤§å¹…æå‡äº†äººå·¥è¯„ä»·çš„å¯¹è¯è´¨é‡ä¸äº¤äº’è‡ªç„¶åº¦ã€‚è¯¥ç ”ç©¶å¼ºè°ƒäº†è§†å¬ç»“åˆåœ¨å‘è¨€äººæ„ŸçŸ¥äº¤äº’(Speaker-aware interaction)ä¸­çš„å…³é”®ä½œç”¨ï¼Œä¸ºå¼€å‘åœ¨ç°å®å¤æ‚å™ªå£°ç¯å¢ƒä¸­è¡¨ç°ç¨³å¥çš„å£è¯­å¯¹è¯æ™ºèƒ½ä½“(Spoken dialogue agents)å¥ å®šäº†åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV",
        "cs.MM",
        "cs.SD"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11124v1",
      "published_date": "2025-11-14 09:56:26 UTC",
      "updated_date": "2025-11-14 09:56:26 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:20:50.770631+00:00"
    },
    {
      "arxiv_id": "2511.11773v1",
      "title": "On the Measure of a Model: From Intelligence to Generality",
      "title_zh": "è®ºæ¨¡å‹çš„åº¦é‡ï¼šä»æ™ºèƒ½åˆ°é€šç”¨æ€§",
      "authors": [
        "Ruchira Dhar",
        "Ninell Oldenburg",
        "Anders Soegaard"
      ],
      "abstract": "Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ARCã€Ravenç­‰å¹¿æ³›ç”¨äºè¯„ä¼°å¤§è¯­è¨€æ¨¡å‹(LLMs)â€œæ™ºèƒ½â€çš„åŸºå‡†æµ‹è¯•ï¼Œå¹¶æŒ‡å‡ºâ€œæ™ºèƒ½â€è¿™ä¸€æ¦‚å¿µå› ç¼ºä¹ç¨³å®šå®šä¹‰è€Œéš¾ä»¥é¢„æµ‹æ¨¡å‹åœ¨å®é™…ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚é€šè¿‡å¯¹æ”¯æ’‘æ™ºèƒ½è¯„ä¼°çš„generalityã€stabilityå’Œrealismä¸‰ä¸ªå‡è®¾è¿›è¡Œç³»ç»Ÿåˆ†æï¼Œä½œè€…å‘ç°ä»…æœ‰generalityèƒ½ç»å—ä½æ¦‚å¿µä¸å®è¯çš„æ¨æ•²ã€‚è®ºæ–‡æå‡ºè¯„ä»·æ¨¡å‹åº”æ¤æ ¹äºgeneralityè€ŒéæŠ½è±¡çš„æ™ºèƒ½ï¼Œå¹¶å°†å…¶é‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªå¤šä»»åŠ¡å­¦ä¹ (multitask learning)é—®é¢˜ã€‚è¿™ç§è§†è§’ç›´æ¥å°†è¯„ä¼°ä¸å¯è¡¡é‡çš„æ€§èƒ½å¹¿åº¦åŠå¯é æ€§è”ç³»èµ·æ¥ï¼Œä¸ºè¡¡é‡äººå·¥æ™ºèƒ½çš„è¿›æ­¥æä¾›äº†æ›´ç¨³å®šçš„ç†è®ºåŸºç¡€ã€‚è¿™ä¸€è½¬å˜æœ‰åŠ©äºå°†æ¨¡å‹è¯„ä¼°ä¸ç°å®ä¸–ç•Œçš„å®ç”¨æ€§é‡æ–°å¯¹é½ï¼Œä»è€Œæ›´å‡†ç¡®åœ°è¡¡é‡è·¨å¤šæ ·åŒ–ä»»åŠ¡çš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "Accepted at EurIPS Workshop on \"The Science of Benchmarking and Evaluating AI\"",
      "pdf_url": "https://arxiv.org/pdf/2511.11773v1",
      "published_date": "2025-11-14 09:46:48 UTC",
      "updated_date": "2025-11-14 09:46:48 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:20:47.583170+00:00"
    },
    {
      "arxiv_id": "2511.11772v2",
      "title": "Scaling Equitable Reflection Assessment in Education via Large Language Models and Role-Based Feedback Agents",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹ä¸åŸºäºè§’è‰²çš„åé¦ˆæ™ºèƒ½ä½“å®ç°æ•™è‚²ä¸­å…¬å¹³åæ€è¯„ä¼°çš„è§„æ¨¡åŒ–",
      "authors": [
        "Chenyu Zhang",
        "Xiaohang Luo"
      ],
      "abstract": "Formative feedback is widely recognized as one of the most effective drivers of student learning, yet it remains difficult to implement equitably at scale. In large or low-resource courses, instructors often lack the time, staffing, and bandwidth required to review and respond to every student reflection, creating gaps in support precisely where learners would benefit most. This paper presents a theory-grounded system that uses five coordinated role-based LLM agents (Evaluator, Equity Monitor, Metacognitive Coach, Aggregator, and Reflexion Reviewer) to score learner reflections with a shared rubric and to generate short, bias-aware, learner-facing comments. The agents first produce structured rubric scores, then check for potentially biased or exclusionary language, add metacognitive prompts that invite students to think about their own thinking, and finally compose a concise feedback message of at most 120 words. The system includes simple fairness checks that compare scoring error across lower and higher scoring learners, enabling instructors to monitor and bound disparities in accuracy. We evaluate the pipeline in a 12-session AI literacy program with adult learners. In this setting, the system produces rubric scores that approach expert-level agreement, and trained graders rate the AI-generated comments as helpful, empathetic, and well aligned with instructional goals. Taken together, these results show that multi-agent LLM systems can deliver equitable, high-quality formative feedback at a scale and speed that would be impossible for human graders alone. More broadly, the work points toward a future where feedback-rich learning becomes feasible for any course size or context, advancing long-standing goals of equity, access, and instructional capacity in education.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ•™è‚²é¢†åŸŸå¤§è§„æ¨¡å…¬å¹³å®æ–½å½¢æˆæ€§åé¦ˆ(Formative feedback)é¢ä¸´çš„èµ„æºåŒ®ä¹æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)å’Œè§’è‰²åŒ–æ™ºèƒ½ä½“çš„è‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿç”±äº”ä¸ªååŒçš„æ™ºèƒ½ä½“ç»„æˆï¼ŒåŒ…æ‹¬è¯„ä¼°è€…(Evaluator)ã€å…¬å¹³ç›‘æµ‹å‘˜(Equity Monitor)ã€å…ƒè®¤çŸ¥æ•™ç»ƒ(Metacognitive Coach)ã€èšåˆå™¨(Aggregator)å’Œåæ€å®¡æŸ¥å‘˜(Reflexion Reviewer)ï¼Œå…±åŒè´Ÿè´£å¯¹å­¦ä¹ è€…çš„åæ€æŠ¥å‘Šè¿›è¡Œè¯„åˆ†å¹¶ç”Ÿæˆå…·æœ‰åè§æ„ŸçŸ¥å’ŒåŒç†å¿ƒçš„ç®€çŸ­åé¦ˆã€‚æ™ºèƒ½ä½“é¦–å…ˆæ ¹æ®å…±äº«è¯„åˆ†æ ‡å‡†(Rubric)ç”Ÿæˆåˆ†æ•°ï¼Œéšåé€šè¿‡å†…ç½®çš„å…¬å¹³æ€§æ£€æŸ¥æœºåˆ¶ç›‘æµ‹å¹¶é™åˆ¶ä¸åŒè¯„åˆ†æ°´å¹³å­¦ä¹ è€…ä¹‹é—´çš„å‡†ç¡®æ€§å·®å¼‚ã€‚åœ¨AIç´ å…»é¡¹ç›®çš„å®éªŒä¸­ï¼Œè¯¥ç³»ç»Ÿè¡¨ç°å‡ºæ¥è¿‘ä¸“å®¶æ°´å¹³çš„è¯„åˆ†ä¸€è‡´æ€§ï¼Œä¸”ç”Ÿæˆçš„åé¦ˆåœ¨å®ç”¨æ€§ã€åŒç†å¿ƒå’Œæ•™å­¦ç›®æ ‡å¯¹é½æ–¹é¢è·å¾—äº†é«˜åº¦è¯„ä»·ã€‚è¯¥æˆæœè¯æ˜äº†å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æå‡æ•™è‚²å…¬å¹³ã€æ‰©å¤§æ•™å­¦å®¹é‡æ–¹é¢çš„æ½œåŠ›ï¼Œä½¿ä»»ä½•è§„æ¨¡çš„è¯¾ç¨‹éƒ½èƒ½å®ç°é«˜è´¨é‡çš„åé¦ˆäº’åŠ¨ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "Accepted to AAAI-26 AISI Track",
      "pdf_url": "https://arxiv.org/pdf/2511.11772v2",
      "published_date": "2025-11-14 09:46:21 UTC",
      "updated_date": "2025-11-27 19:00:20 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:20:51.876991+00:00"
    },
    {
      "arxiv_id": "2511.11113v1",
      "title": "VIDEOP2R: Video Understanding from Perception to Reasoning",
      "title_zh": "VIDEOP2Rï¼šä»æ„ŸçŸ¥åˆ°æ¨ç†çš„è§†é¢‘ç†è§£",
      "authors": [
        "Yifan Jiang",
        "Yueying Wang",
        "Rui Zhao",
        "Toufiq Parag",
        "Zhimin Chen",
        "Zhenyu Liao",
        "Jayakrishnan Unnikrishnan"
      ],
      "abstract": "Reinforcement fine-tuning (RFT), a two-stage framework consisting of supervised fine-tuning (SFT) and reinforcement learning (RL) has shown promising results on improving reasoning ability of large language models (LLMs). Yet extending RFT to large video language models (LVLMs) remains challenging. We propose VideoP2R, a novel process-aware video RFT framework that enhances video reasoning by modeling perception and reasoning as distinct processes. In the SFT stage, we develop a three-step pipeline to generate VideoP2R-CoT-162K, a high-quality, process-aware chain-of-thought (CoT) dataset for perception and reasoning. In the RL stage, we introduce a novel process-aware group relative policy optimization (PA-GRPO) algorithm that supplies separate rewards for perception and reasoning. Extensive experiments show that VideoP2R achieves state-of-the-art (SotA) performance on six out of seven video reasoning and understanding benchmarks. Ablation studies further confirm the effectiveness of our process-aware modeling and PA-GRPO and demonstrate that model's perception output is information-sufficient for downstream reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†VideoP2Rï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§è§†é¢‘è¯­è¨€æ¨¡å‹(LVLMs)çš„è¿‡ç¨‹æ„ŸçŸ¥è§†é¢‘å¼ºåŒ–å¾®è°ƒ(RFT)æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³å°†å¼ºåŒ–å­¦ä¹ æ‰©å±•åˆ°è§†é¢‘æ¨ç†é¢†åŸŸçš„æŒ‘æˆ˜ã€‚åœ¨ç›‘ç£å¾®è°ƒ(SFT)é˜¶æ®µï¼Œè¯¥æ¡†æ¶é€šè¿‡ä¸‰æ­¥æµæ°´çº¿å¼€å‘äº†VideoP2R-CoT-162Kæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«16.2ä¸‡æ¡é«˜è´¨é‡ã€å…·å¤‡è¿‡ç¨‹æ„ŸçŸ¥ç‰¹æ€§çš„æ€ç»´é“¾(CoT)æ•°æ®ã€‚åœ¨å¼ºåŒ–å­¦ä¹ (RL)é˜¶æ®µï¼Œç ”ç©¶å¼•å…¥äº†ä¸€ç§æ–°å‹çš„è¿‡ç¨‹æ„ŸçŸ¥ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–(PA-GRPO)ç®—æ³•ï¼Œé€šè¿‡ä¸ºæ„ŸçŸ¥å’Œæ¨ç†è¿‡ç¨‹åˆ†åˆ«æä¾›å¥–åŠ±æ¥å¢å¼ºè§†é¢‘æ¨ç†èƒ½åŠ›ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒVideoP2Råœ¨ä¸ƒä¸ªè§†é¢‘æ¨ç†å’Œç†è§£åŸºå‡†æµ‹è¯•ä¸­çš„å…­ä¸ªè¾¾åˆ°äº†å½“å‰å…ˆè¿›æ°´å¹³(SotA)ã€‚æ¶ˆèç ”ç©¶è¿›ä¸€æ­¥è¯å®äº†è¿‡ç¨‹æ„ŸçŸ¥å»ºæ¨¡ä¸PA-GRPOç®—æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶è¯æ˜æ¨¡å‹çš„æ„ŸçŸ¥è¾“å‡ºä¸ºä¸‹æ¸¸æ¨ç†æä¾›äº†å……åˆ†çš„ä¿¡æ¯æ”¯æŒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11113v1",
      "published_date": "2025-11-14 09:42:42 UTC",
      "updated_date": "2025-11-14 09:42:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:20:53.376230+00:00"
    },
    {
      "arxiv_id": "2511.11095v1",
      "title": "Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)",
      "title_zh": "åŸºäºç›®æ ‡å›å½’çš„æ»¡è¶³å¼ä¸æœ€ä¼˜é€šç”¨è§„åˆ’ï¼ˆæ‰©å±•ç‰ˆï¼‰",
      "authors": [
        "Dillon Z. Chen",
        "Till Hofmann",
        "Toryn Q. Klassen",
        "Sheila A. McIlraith"
      ],
      "abstract": "Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\\textit{Condition} \\rightarrow \\textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹é€šç”¨è§„åˆ’ (Generalised planning) ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨ç›®æ ‡å›å½’ (Goal Regression) åˆæˆé€šç”¨ç¨‹åºçš„åˆ›æ–°æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡ä¸ºè®­ç»ƒé—®é¢˜ä¸­çš„ç›®æ ‡åŸå­è®¡ç®—æœ€ä¼˜è§„åˆ’ï¼Œå¹¶å¯¹å…¶å®æ–½ç›®æ ‡å›å½’å’Œæå‡å¤„ç†ï¼Œä»è€Œç”Ÿæˆä¸€ç»„ä¸€é˜¶ Condition -> Actions è§„åˆ™ã€‚è¿™äº›è§„åˆ™å…±åŒæ„æˆäº†ä¸€ä¸ªå¯ä»¥ç›´æ¥æ‰§è¡Œçš„é€šç”¨è§„åˆ’ï¼Œæˆ–è€…ä½œä¸ºå…¬ç†ç”¨äºå‰ªææœç´¢ç©ºé—´ã€‚ä½œè€…å½¢å¼åŒ–å¹¶è¯æ˜äº†è¯¥æ–¹æ³•ä¿è¯å­¦ä¹ åˆ°æœ‰æ•ˆé€šç”¨è§„åˆ’å’Œå‰ªæå…¬ç†çš„å‰ææ¡ä»¶ã€‚åœ¨å¤šä¸ªç»å…¸åŠæ•°å€¼è§„åˆ’é¢†åŸŸçš„å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨åˆæˆæˆæœ¬ (Synthesis cost)ã€è§„åˆ’è¦†ç›–ç‡ (Planning coverage) ä»¥åŠè§£çš„è´¨é‡ (Solution quality) æ–¹é¢å‡æ˜¾è‘—ä¼˜äºç›®å‰çš„å…ˆè¿›è§„åˆ’å™¨ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Extended version of AAAI 2026 paper",
      "pdf_url": "https://arxiv.org/pdf/2511.11095v1",
      "published_date": "2025-11-14 09:16:32 UTC",
      "updated_date": "2025-11-14 09:16:32 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:20:59.102624+00:00"
    },
    {
      "arxiv_id": "2511.11083v3",
      "title": "Efficient Reinforcement Learning for Zero-Shot Coordination in Evolving Games",
      "title_zh": "æ¼”åŒ–åšå¼ˆä¸­é›¶æ ·æœ¬åä½œçš„é«˜æ•ˆå¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Bingyu Hui",
        "Lebin Yu",
        "Quanming Yao",
        "Yunpeng Qu",
        "Xudong Zhang",
        "Jian Wang"
      ],
      "abstract": "Zero-shot coordination(ZSC), a key challenge in multi-agent game theory, has become a hot topic in reinforcement learning (RL) research recently, especially in complex evolving games. It focuses on the generalization ability of agents, requiring them to coordinate well with collaborators from a diverse, potentially evolving, pool of partners that are not seen before without any fine-tuning. Population-based training, which approximates such an evolving partner pool, has been proven to provide good zero-shot coordination performance; nevertheless, existing methods are limited by computational resources, mainly focusing on optimizing diversity in small populations while neglecting the potential performance gains from scaling population size. To address this issue, this paper proposes the Scalable Population Training (ScaPT), an efficient RL training framework comprising two key components: a meta-agent that efficiently realizes a population by selectively sharing parameters across agents, and a mutual information regularizer that guarantees population diversity. To empirically validate the effectiveness of ScaPT, this paper evaluates it along with representational frameworks in Hanabi cooperative game and confirms its superiority.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤šæ™ºèƒ½ä½“åšå¼ˆè®ºä¸­çš„é›¶æ ·æœ¬åè°ƒ (Zero-shot coordination, ZSC) éš¾é¢˜ï¼Œæå‡ºäº†åä¸º Scalable Population Training (ScaPT) çš„é«˜æ•ˆå¼ºåŒ–å­¦ä¹  (Reinforcement Learning) è®­ç»ƒæ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰åŸºäºç§ç¾¤çš„è®­ç»ƒæ–¹æ³•å—é™äºè®¡ç®—èµ„æºä¸”éš¾ä»¥æ‰©å±•ç§ç¾¤è§„æ¨¡çš„é—®é¢˜ï¼ŒScaPT å¼•å…¥äº†ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶ä»¥æå‡æ€§èƒ½ã€‚é¦–å…ˆï¼Œè¯¥æ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªå…ƒæ™ºèƒ½ä½“ (meta-agent)ï¼Œé€šè¿‡åœ¨æ™ºèƒ½ä½“é—´é€‰æ‹©æ€§åœ°å…±äº«å‚æ•°ï¼Œå®ç°äº†ç§ç¾¤è§„æ¨¡çš„é«˜æ•ˆæ‰©å±•ã€‚å…¶æ¬¡ï¼Œç ”ç©¶é‡‡ç”¨äº’ä¿¡æ¯æ­£åˆ™åŒ–é¡¹ (mutual information regularizer) æ¥ç¡®ä¿ç§ç¾¤å†…éƒ¨çš„å¤šæ ·æ€§ï¼Œä»è€Œå¢å¼ºæ™ºèƒ½ä½“çš„æ³›åŒ–èƒ½åŠ›ã€‚åœ¨ Hanabi åä½œåšå¼ˆå®éªŒä¸­ï¼ŒScaPT å±•ç°å‡ºäº†ä¼˜äºç°æœ‰ä»£è¡¨æ€§æ¡†æ¶çš„æ€§èƒ½ï¼Œè¯æ˜äº†å…¶åœ¨å¤„ç†å¤æ‚æ¼”åŒ–åšå¼ˆç¯å¢ƒä¸­çš„æœ‰æ•ˆæ€§å’Œä¼˜è¶Šæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11083v3",
      "published_date": "2025-11-14 08:59:22 UTC",
      "updated_date": "2025-11-18 10:20:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:21:00.691872+00:00"
    },
    {
      "arxiv_id": "2511.11079v2",
      "title": "ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving",
      "title_zh": "ARCTrajï¼šé¢å‘æŠ½è±¡é—®é¢˜è§£å†³çš„äººç±»æ¨ç†è½¨è¿¹æ•°æ®é›†ä¸åŸºå‡†",
      "authors": [
        "Sejin Kim",
        "Hayan Choi",
        "Seokki Lee",
        "Sundong Kim"
      ],
      "abstract": "We present ARCTraj, a dataset and methodological framework for modeling human reasoning through complex visual tasks in the Abstraction and Reasoning Corpus (ARC). While ARC has inspired extensive research on abstract reasoning, most existing approaches rely on static input--output supervision, which limits insight into how reasoning unfolds over time. ARCTraj addresses this gap by recording temporally ordered, object-level actions that capture how humans iteratively transform inputs into outputs, revealing intermediate reasoning steps that conventional datasets overlook. Collected via the O2ARC web interface, it contains around 10,000 trajectories annotated with task identifiers, timestamps, and success labels across 400 training tasks from the ARC-AGI-1 benchmark. It further defines a unified reasoning pipeline encompassing data collection, action abstraction, Markov decision process (MDP) formulation, and downstream learning, enabling integration with reinforcement learning, generative modeling, and sequence modeling methods such as PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers. Analyses of spatial selection, color attribution, and strategic convergence highlight the structure and diversity of human reasoning. Together, these contributions position ARCTraj as a structured and interpretable foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¨å‡ºäº†ARCTrajï¼Œä¸€ä¸ªæ—¨åœ¨ä¸ºæŠ½è±¡æ¨ç†è¯­æ–™åº“(ARC)ä¸­çš„å¤æ‚è§†è§‰ä»»åŠ¡å»ºç«‹äººç±»æ¨ç†æ¨¡å‹çš„æ•°æ®é›†å’Œæ–¹æ³•è®ºæ¡†æ¶ã€‚é’ˆå¯¹ç°æœ‰ARCç ”ç©¶ä¸»è¦ä¾èµ–é™æ€è¾“å…¥è¾“å‡ºç›‘ç£è€Œç¼ºä¹æ¨ç†è¿‡ç¨‹æ´å¯Ÿçš„é—®é¢˜ï¼ŒARCTrajé€šè¿‡è®°å½•æ—¶é—´æ’åºçš„å¯¹è±¡çº§åŠ¨ä½œï¼Œæ•æ‰äº†äººç±»å°†è¾“å…¥è½¬åŒ–ä¸ºè¾“å‡ºçš„è¿­ä»£è¿‡ç¨‹åŠä¸­é—´æ¨ç†æ­¥éª¤ã€‚è¯¥æ•°æ®é›†é€šè¿‡O2ARCç½‘ç»œç•Œé¢æ”¶é›†ï¼ŒåŒ…å«é’ˆå¯¹ARC-AGI-1åŸºå‡†æµ‹è¯•ä¸­400ä¸ªè®­ç»ƒä»»åŠ¡çš„çº¦10,000æ¡æ¨ç†è½¨è¿¹ï¼Œå¹¶å®šä¹‰äº†ä¸€ä¸ªæ¶µç›–æ•°æ®é‡‡é›†ã€åŠ¨ä½œæŠ½è±¡ã€é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(MDP)å»ºæ¨¡åŠä¸‹æ¸¸å­¦ä¹ çš„ç»Ÿä¸€æµæ°´çº¿ã€‚è¯¥æ¡†æ¶èƒ½å¤Ÿä¸å¼ºåŒ–å­¦ä¹ ã€ç”Ÿæˆå»ºæ¨¡å’Œåºåˆ—å»ºæ¨¡æ–¹æ³•ï¼ˆå¦‚PPOã€World Modelsã€GFlowNetsã€Diffusion agentså’ŒDecision Transformersï¼‰æ— ç¼é›†æˆã€‚é€šè¿‡å¯¹ç©ºé—´é€‰æ‹©ã€é¢œè‰²å½’å› å’Œç­–ç•¥æ”¶æ•›çš„åˆ†æï¼ŒARCTrajæ­ç¤ºäº†äººç±»æ¨ç†çš„ç»“æ„ä¸å¤šæ ·æ€§ã€‚æ€»ä¹‹ï¼ŒARCTrajä¸ºç ”ç©¶ç±»äººæ¨ç†æä¾›äº†ç»“æ„åŒ–ä¸”å¯è§£é‡Šçš„åŸºç¡€ï¼Œæœ‰æ•ˆæ¨åŠ¨äº†äººå·¥æ™ºèƒ½çš„å¯è§£é‡Šæ€§ã€å¯¹é½å’Œæ³›åŒ–æ™ºèƒ½çš„å‘å±•ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11079v2",
      "published_date": "2025-11-14 08:52:53 UTC",
      "updated_date": "2025-11-17 03:11:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:21:05.989673+00:00"
    },
    {
      "arxiv_id": "2511.11770v1",
      "title": "Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction",
      "title_zh": "å­¦ä¹ æç‚¼ï¼šä¸€ç§é¢å‘è¿­ä»£å¼ SPARQL æŸ¥è¯¢æ„å»ºçš„æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ–¹æ³•",
      "authors": [
        "Floris Vossebeld",
        "Shenghui Wang"
      ],
      "abstract": "Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çŸ¥è¯†å›¾è°±é—®ç­”(KGQA)ä¸­å¤æ‚å¤šè·³(multi-hop)é—®é¢˜çš„SPARQLæŸ¥è¯¢æ„é€ éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºå¼ºåŒ–å­¦ä¹ (RL)çš„è¿­ä»£å¼æ™ºèƒ½ä½“æ¡†æ¶ã€‚ç°æœ‰çš„å•æ¬¡ç”Ÿæˆæ–¹æ³•åœ¨å¤„ç†å¤æ‚é€»è¾‘æ—¶å®¹æ˜“å‡ºé”™ï¼Œä¸”ç¼ºä¹æ ¹æ®å®æ—¶æ‰§è¡Œåé¦ˆè¿›è¡ŒåŠ¨æ€è°ƒè¯•çš„èƒ½åŠ›ã€‚ä½œè€…é€šè¿‡ç»“æœé©±åŠ¨çš„å¼ºåŒ–å­¦ä¹ (GRPO)ç®—æ³•ï¼Œåœ¨æ— éœ€ç›‘ç£å¾®è°ƒ(SFT)çš„æƒ…å†µä¸‹è®­ç»ƒäº†ä¸€ä¸ª3Bå‚æ•°çš„å°å‹æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åˆ°ä»æ‰§è¡Œé”™è¯¯ä¸­æ¢å¤å¹¶é€æ­¥å®Œå–„æŸ¥è¯¢çš„æœ‰æ•ˆç­–ç•¥ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ™ºèƒ½ä½“åœ¨LC-QuAD 2.0æ•°æ®é›†çš„å•ç­”æ¡ˆå­é›†ä¸Šè¾¾åˆ°äº†49.7%çš„å‡†ç¡®ç‡ï¼Œè¾ƒæœ€å¼ºçš„é›¶æ ·æœ¬è¿­ä»£åŸºçº¿æ˜¾è‘—æå‡äº†17.5ä¸ªç™¾åˆ†ç‚¹ã€‚æ­¤å¤–ï¼Œåˆ†æè¡¨æ˜æ˜¾å¼çš„å®¡æ…æ¨ç†(deliberative reasoning)æ­¥éª¤èµ·åˆ°äº†è®¤çŸ¥æ”¯æ¶çš„ä½œç”¨ï¼Œæ˜¾è‘—å¢å¼ºäº†ç­–ç•¥çš„ç²¾ç¡®åº¦ã€‚è¿™é¡¹å·¥ä½œä¸ºå¤§è¯­è¨€æ¨¡å‹(LLMs)é€šè¿‡äº¤äº’å­¦ä¹ æŒæ¡æ­£å¼ç¬¦å·å·¥å…·å¹¶å¼¥åˆç»“æ„åŒ–çŸ¥è¯†å›¾è°±ä¹‹é—´çš„é¸¿æ²Ÿæä¾›äº†é€šç”¨çš„è“å›¾ã€‚",
      "categories": [
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11770v1",
      "published_date": "2025-11-14 08:44:58 UTC",
      "updated_date": "2025-11-14 08:44:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:21:07.069068+00:00"
    },
    {
      "arxiv_id": "2511.11066v1",
      "title": "S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation",
      "title_zh": "S2D-ALIGNï¼šé¢å‘è§£å‰–å­¦å¯¹é½æ”¾å°„æŠ¥å‘Šç”Ÿæˆçš„ç”±æµ…å…¥æ·±è¾…åŠ©å­¦ä¹ ",
      "authors": [
        "Jiechao Gao",
        "Chang Liu",
        "Yuangang Li"
      ],
      "abstract": "Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \\textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \\textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \\textsc{MIMIC-CXR} and \\textsc{IU X-Ray} benchmarks, where \\textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†S2D-ALIGNï¼Œä¸€ç§ç”¨äºæ”¾å°„æŠ¥å‘Šç”Ÿæˆ(Radiology Report Generation, RRG)çš„æ–°å‹ç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning, SFT)èŒƒå¼ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰æ–¹æ³•åœ¨è§£å‰–å­¦å…³è”å¯¹é½(anatomically-grounded alignment)æ–¹é¢çš„ä¸è¶³ã€‚S2D-ALIGNé‡‡ç”¨äº†ç”±æµ…å…¥æ·±çš„è¾…åŠ©å­¦ä¹ ç­–ç•¥ï¼Œé€šè¿‡æ•´åˆç²—ç²’åº¦å›¾åƒæŠ¥å‘Šé…å¯¹ã€å®ä¾‹çº§å‚è€ƒæŠ¥å‘ŠæŒ‡å¯¼ä»¥åŠç»†ç²’åº¦å…³é”®çŸ­è¯­ï¼Œé€æ­¥å¢å¼ºè·¨æ¨¡æ€å¯¹é½è¿‡ç¨‹ã€‚ä¸ºäº†è¡”æ¥ä¸åŒçš„å¯¹é½é˜¶æ®µï¼Œè¯¥æ–¹æ³•å¼•å…¥äº†åŸºäºè®°å¿†çš„é€‚é…å™¨(memory-based adapter)ä»¥å®ç°ç‰¹å¾å…±äº«å¹¶æ•´åˆå¤šç²’åº¦æŒ‡å¯¼ä¿¡æ¯ã€‚åœ¨MIMIC-CXRå’ŒIU X-RayåŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼ŒS2D-ALIGNè¾¾åˆ°äº†å½“å‰é¢†å…ˆçš„(state-of-the-art)æ€§èƒ½æ°´å¹³ã€‚æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†è¿™ç§å¤šé˜¶æ®µè¾…åŠ©å¼•å¯¼æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸ºæå‡å¤æ‚å¤šæ¨¡æ€ç”Ÿæˆä»»åŠ¡çš„å‡†ç¡®æ€§æä¾›äº†é‡è¦æ–¹å‘ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11066v1",
      "published_date": "2025-11-14 08:34:06 UTC",
      "updated_date": "2025-11-14 08:34:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:21:25.643672+00:00"
    },
    {
      "arxiv_id": "2511.11065v1",
      "title": "From Retinal Pixels to Patients: Evolution of Deep Learning Research in Diabetic Retinopathy Screening",
      "title_zh": "ä»è§†ç½‘è†œåƒç´ åˆ°ä¸´åºŠæ‚£è€…ï¼šç³–å°¿ç—…è§†ç½‘è†œç—…å˜ç­›æŸ¥æ·±åº¦å­¦ä¹ ç ”ç©¶çš„æ¼”è¿›",
      "authors": [
        "Muskaan Chopra",
        "Lorenz Sparrenberg",
        "Armin Berger",
        "Sarthak Khanna",
        "Jan H. Terheyden",
        "Rafet Sifa"
      ],
      "abstract": "Diabetic Retinopathy (DR) remains a leading cause of preventable blindness, with early detection critical for reducing vision loss worldwide. Over the past decade, deep learning has transformed DR screening, progressing from early convolutional neural networks trained on private datasets to advanced pipelines addressing class imbalance, label scarcity, domain shift, and interpretability. This survey provides the first systematic synthesis of DR research spanning 2016-2025, consolidating results from 50+ studies and over 20 datasets. We critically examine methodological advances, including self- and semi-supervised learning, domain generalization, federated training, and hybrid neuro-symbolic models, alongside evaluation protocols, reporting standards, and reproducibility challenges. Benchmark tables contextualize performance across datasets, while discussion highlights open gaps in multi-center validation and clinical trust. By linking technical progress with translational barriers, this work outlines a practical agenda for reproducible, privacy-preserving, and clinically deployable DR AI. Beyond DR, many of the surveyed innovations extend broadly to medical imaging at scale.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿåœ°æ€»ç»“äº†2016å¹´è‡³2025å¹´é—´æ·±åº¦å­¦ä¹ (Deep Learning)åœ¨ç³–å°¿ç—…è§†ç½‘è†œç—…å˜(Diabetic Retinopathy, DR)ç­›æŸ¥é¢†åŸŸçš„ç ”ç©¶è¿›å±•ï¼Œæ•´åˆäº†è¶…è¿‡50é¡¹ç ”ç©¶å’Œ20ä¸ªæ•°æ®é›†çš„æˆæœã€‚ç ”ç©¶è¯¦ç»†åˆ†æäº†ä»æ—©æœŸå·ç§¯ç¥ç»ç½‘ç»œ(CNNs)åˆ°è§£å†³æ ‡ç­¾ç¨€ç¼ºã€é¢†åŸŸåç§»(Domain shift)å’Œå¯è§£é‡Šæ€§(Interpretability)ç­‰å¤æ‚é—®é¢˜çš„æŠ€æœ¯æ¼”è¿›ã€‚æ–‡ç« é‡ç‚¹æ¢è®¨äº†è‡ªç›‘ç£å­¦ä¹ (Self-supervised learning)ã€åŠç›‘ç£å­¦ä¹ (Semi-supervised learning)ã€é¢†åŸŸæ³›åŒ–(Domain generalization)ä»¥åŠè”é‚¦å­¦ä¹ (Federated training)ç­‰æ–¹æ³•è®ºåˆ›æ–°ã€‚é€šè¿‡å¯¹æ¯”å„æ•°æ®é›†çš„åŸºå‡†è¡¨ç°ï¼Œç ”ç©¶æŒ‡å‡ºäº†å¤šä¸­å¿ƒéªŒè¯å’Œä¸´åºŠä¿¡ä»»æ–¹é¢çš„ç°æœ‰å·®è·ï¼Œå¹¶ä¸ºæ„å»ºå¯å¤ç°ã€éšç§ä¿æŠ¤ä¸”å¯ä¸´åºŠéƒ¨ç½²çš„DR AIç³»ç»Ÿæå‡ºäº†å®è·µè®®ç¨‹ã€‚è¿™äº›ç ”ç©¶æˆæœä¸ä»…æ¨åŠ¨äº†DRç­›æŸ¥çš„å‘å±•ï¼Œä¹Ÿä¸ºå¤§è§„æ¨¡åŒ»ç–—å½±åƒAIçš„åº”ç”¨æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted in IEEE BigData 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.11065v1",
      "published_date": "2025-11-14 08:31:42 UTC",
      "updated_date": "2025-11-14 08:31:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:21:25.238948+00:00"
    },
    {
      "arxiv_id": "2511.11062v1",
      "title": "LiteAttention: A Temporal Sparse Attention for Diffusion Transformers",
      "title_zh": "LiteAttentionï¼šé¢å‘æ‰©æ•£ Transformer çš„æ—¶åŸŸç¨€ç–æ³¨æ„åŠ›æœºåˆ¶",
      "authors": [
        "Dor Shmilovich",
        "Tony Wu",
        "Aviad Dahan",
        "Yuval Domb"
      ],
      "abstract": "Diffusion Transformers, particularly for video generation, achieve remarkable quality but suffer from quadratic attention complexity, leading to prohibitive latency. Existing acceleration methods face a fundamental trade-off: dynamically estimating sparse attention patterns at each denoising step incurs high computational overhead and estimation errors, while static sparsity patterns remain fixed and often suboptimal throughout denoising. We identify a key structural property of diffusion attention, namely, its sparsity patterns exhibit strong temporal coherence across denoising steps. Tiles deemed non-essential at step $t$ typically remain so at step $t+Î´$. Leveraging this observation, we introduce LiteAttention, a method that exploits temporal coherence to enable evolutionary computation skips across the denoising sequence. By marking non-essential tiles early and propagating skip decisions forward, LiteAttention eliminates redundant attention computations without repeated profiling overheads, combining the adaptivity of dynamic methods with the efficiency of static ones. We implement a highly optimized LiteAttention kernel on top of FlashAttention and demonstrate substantial speedups on production video diffusion models, with no degradation in quality. The code and implementation details will be publicly released.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è§†é¢‘ç”Ÿæˆé¢†åŸŸ Diffusion Transformers å› æ³¨æ„åŠ›æœºåˆ¶äºŒæ¬¡å¤æ‚åº¦è€Œé¢ä¸´çš„æ˜¾è‘—å»¶è¿Ÿé—®é¢˜ï¼ŒæŒ‡å‡ºç›®å‰çš„åŠ é€Ÿæ–¹æ³•åœ¨åŠ¨æ€è¯„ä¼°å¼€é”€ä¸é™æ€ç¨€ç–æ€§æ¬¡ä¼˜ä¹‹é—´å­˜åœ¨æƒè¡¡ã€‚ä½œè€…å‘ç°æ‰©æ•£æ¨¡å‹ä¸­çš„æ³¨æ„åŠ›ç¨€ç–æ¨¡å¼åœ¨å»å™ªè¿‡ç¨‹ä¸­å…·æœ‰æå¼ºçš„ Temporal Coherenceï¼Œå³åœ¨ç‰¹å®šæ­¥éª¤è¢«è§†ä¸ºéå¿…è¦çš„å—åœ¨åç»­æ­¥éª¤ä¸­é€šå¸¸ä»ä¿æŒéå¿…è¦ã€‚åŸºäºæ­¤å‘ç°ï¼Œç ”ç©¶è€…æå‡ºäº† LiteAttentionï¼Œé€šè¿‡åˆ©ç”¨æ—¶é—´ç›¸å¹²æ€§å®ç°äº†è·¨å»å™ªåºåˆ—çš„æ¼”åŒ–è®¡ç®—è·³è¿‡ã€‚è¯¥æ–¹æ³•é€šè¿‡æ—©æœŸæ ‡è®°éå¿…è¦å—å¹¶å‘å‰ä¼ æ’­è·³è¿‡å†³ç­–ï¼Œæ¶ˆé™¤äº†å†—ä½™çš„æ³¨æ„åŠ›è®¡ç®—åŠé‡å¤çš„å‰–æå¼€é”€ï¼Œå…¼å…·åŠ¨æ€æ–¹æ³•çš„è‡ªé€‚åº”æ€§ä¸é™æ€æ–¹æ³•çš„é«˜æ•ˆæ€§ã€‚å®éªŒè¡¨æ˜ï¼ŒLiteAttention åœ¨ FlashAttention åŸºç¡€ä¸Šå®ç°äº†é«˜åº¦ä¼˜åŒ–çš„ç®—å­ï¼Œåœ¨ä¸ç‰ºç‰²ç”Ÿæˆè´¨é‡çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†ç”Ÿäº§çº§è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11062v1",
      "published_date": "2025-11-14 08:26:55 UTC",
      "updated_date": "2025-11-14 08:26:55 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:21:24.840927+00:00"
    },
    {
      "arxiv_id": "2511.13759v1",
      "title": "Multi-Agent VLMs Guided Self-Training with PNU Loss for Low-Resource Offensive Content Detection",
      "title_zh": "åŸºäº PNU æŸå¤±ä¸å¤šæ™ºèƒ½ä½“ VLMs å¼•å¯¼çš„ä½èµ„æºæ”»å‡»æ€§å†…å®¹æ£€æµ‹è‡ªè®­ç»ƒ",
      "authors": [
        "Han Wang",
        "Deyi Ji",
        "Junyu Lu",
        "Lanyun Zhu",
        "Hailong Zhang",
        "Haiyang Wu",
        "Liqun Liu",
        "Peng Shu",
        "Roy Ka-Wei Lee"
      ],
      "abstract": "Accurate detection of offensive content on social media demands high-quality labeled data; however, such data is often scarce due to the low prevalence of offensive instances and the high cost of manual annotation. To address this low-resource challenge, we propose a self-training framework that leverages abundant unlabeled data through collaborative pseudo-labeling. Starting with a lightweight classifier trained on limited labeled data, our method iteratively assigns pseudo-labels to unlabeled instances with the support of Multi-Agent Vision-Language Models (MA-VLMs). Un-labeled data on which the classifier and MA-VLMs agree are designated as the Agreed-Unknown set, while conflicting samples form the Disagreed-Unknown set. To enhance label reliability, MA-VLMs simulate dual perspectives, moderator and user, capturing both regulatory and subjective viewpoints. The classifier is optimized using a novel Positive-Negative-Unlabeled (PNU) loss, which jointly exploits labeled, Agreed-Unknown, and Disagreed-Unknown data while mitigating pseudo-label noise. Experiments on benchmark datasets demonstrate that our framework substantially outperforms baselines under limited supervision and approaches the performance of large-scale models",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¤¾äº¤åª’ä½“ä¸­æ”»å‡»æ€§å†…å®¹æ£€æµ‹(Offensive Content Detection)åœ¨æ ‡æ³¨æ•°æ®ç¨€ç¼ºå’Œé«˜æ˜‚æˆæœ¬ä¸‹çš„ä½èµ„æºæŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åˆ©ç”¨å¤šæ™ºèƒ½ä½“è§†è§‰è¯­è¨€æ¨¡å‹(MA-VLMs)å¼•å¯¼çš„è‡ªè®­ç»ƒ(Self-training)æ¡†æ¶ã€‚è¯¥æ–¹æ³•é€šè¿‡ååŒä¼ªæ ‡ç­¾(Collaborative Pseudo-labeling)æœºåˆ¶ï¼Œåˆ©ç”¨MA-VLMså¯¹æœªæ ‡æ³¨æ•°æ®è¿›è¡Œè¿­ä»£æ ‡æ³¨ï¼Œå¹¶æ ¹æ®åˆ†ç±»å™¨ä¸MA-VLMsçš„é¢„æµ‹ç»“æœå°†å…¶åˆ’åˆ†ä¸ºä¸€è‡´æœªçŸ¥é›†(Agreed-Unknown)å’Œåˆ†æ­§æœªçŸ¥é›†(Disagreed-Unknown)ã€‚ä¸ºäº†å¢å¼ºæ ‡ç­¾çš„å¯é æ€§ï¼ŒMA-VLMsæ¨¡æ‹Ÿäº†ç›‘ç®¡è€…(Moderator)å’Œç”¨æˆ·(User)çš„åŒé‡è§†è§’ï¼Œä»¥åŒæ—¶æ•æ‰ç›‘ç®¡è§„èŒƒä¸ä¸»è§‚æ„Ÿå—ã€‚ç ”ç©¶è¿›ä¸€æ­¥æå‡ºäº†ä¸€ç§æ–°å‹çš„æ­£è´Ÿæœªæ ‡æ³¨(PNU)æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨è”åˆåˆ©ç”¨å„ç±»æ•°æ®å¹¶ç¼“è§£ä¼ªæ ‡ç­¾å¸¦æ¥çš„å™ªå£°é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨æœ‰é™ç›‘ç£æ¡ä»¶ä¸‹æ˜¾è‘—ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œä¸”æ€§èƒ½è¡¨ç°æ¥è¿‘äºå¤§è§„æ¨¡æ¨¡å‹ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "8 pages, 4 figures, Fortieth AAAI Conference on Artificial Intelligence (AAAI-26)",
      "pdf_url": "https://arxiv.org/pdf/2511.13759v1",
      "published_date": "2025-11-14 08:03:35 UTC",
      "updated_date": "2025-11-14 08:03:35 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:21:28.732314+00:00"
    },
    {
      "arxiv_id": "2511.11048v2",
      "title": "PINGS-X: Physics-Informed Normalized Gaussian Splatting with Axes Alignment for Efficient Super-Resolution of 4D Flow MRI",
      "title_zh": "PINGS-Xï¼šåŸºäºè½´å¯¹é½ç‰©ç†ä¿¡æ¯å½’ä¸€åŒ–é«˜æ–¯æ³¼æº…çš„ 4D Flow MRI é«˜æ•ˆè¶…åˆ†è¾¨ç‡",
      "authors": [
        "Sun Jo",
        "Seok Young Hong",
        "JinHyun Kim",
        "Seungmin Kang",
        "Ahjin Choi",
        "Don-Gwan An",
        "Simon Song",
        "Je Hyeong Hong"
      ],
      "abstract": "4D flow magnetic resonance imaging (MRI) is a reliable, non-invasive approach for estimating blood flow velocities, vital for cardiovascular diagnostics. Unlike conventional MRI focused on anatomical structures, 4D flow MRI requires high spatiotemporal resolution for early detection of critical conditions such as stenosis or aneurysms. However, achieving such resolution typically results in prolonged scan times, creating a trade-off between acquisition speed and prediction accuracy. Recent studies have leveraged physics-informed neural networks (PINNs) for super-resolution of MRI data, but their practical applicability is limited as the prohibitively slow training process must be performed for each patient. To overcome this limitation, we propose PINGS-X, a novel framework modeling high-resolution flow velocities using axes-aligned spatiotemporal Gaussian representations. Inspired by the effectiveness of 3D Gaussian splatting (3DGS) in novel view synthesis, PINGS-X extends this concept through several non-trivial novel innovations: (i) normalized Gaussian splatting with a formal convergence guarantee, (ii) axes-aligned Gaussians that simplify training for high-dimensional data while preserving accuracy and the convergence guarantee, and (iii) a Gaussian merging procedure to prevent degenerate solutions and boost computational efficiency. Experimental results on computational fluid dynamics (CFD) and real 4D flow MRI datasets demonstrate that PINGS-X substantially reduces training time while achieving superior super-resolution accuracy. Our code and datasets are available at https://github.com/SpatialAILab/PINGS-X.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ 4D flow magnetic resonance imaging (MRI) åœ¨å¿ƒè¡€ç®¡è¯Šæ–­ä¸­é¢ä¸´çš„æ‰«ææ—¶é—´ä¸æ—¶ç©ºåˆ†è¾¨ç‡ä¹‹é—´çš„æƒè¡¡é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º PINGS-X çš„æ–°å‹æ¡†æ¶ã€‚ç”±äºç°æœ‰çš„ç‰©ç†ä¿¡æ¯ç¥ç»ç½‘ç»œ (PINNs) åœ¨è¶…åˆ†è¾¨ç‡ä»»åŠ¡ä¸­è®­ç»ƒè¿‡ç¨‹æå…¶ç¼“æ…¢ï¼Œé™åˆ¶äº†å…¶ä¸´åºŠå®ç”¨æ€§ã€‚PINGS-X å€Ÿé‰´äº† 3D Gaussian splatting (3DGS) çš„æ€æƒ³ï¼Œé€šè¿‡è½´å¯¹é½çš„æ—¶ç©ºé«˜æ–¯è¡¨ç¤º (axes-aligned spatiotemporal Gaussian representations) æ¥å»ºæ¨¡é«˜åˆ†è¾¨ç‡è¡€æµé€Ÿåº¦ã€‚è¯¥æ¡†æ¶å¼•å…¥äº†å…·æœ‰æ”¶æ•›ä¿è¯çš„å½’ä¸€åŒ–é«˜æ–¯å–·ç»˜ (normalized Gaussian splatting)ï¼Œå¹¶åˆ©ç”¨è½´å¯¹é½é«˜æ–¯ (axes-aligned Gaussians) ç®€åŒ–äº†é«˜ç»´æ•°æ®çš„è®­ç»ƒï¼ŒåŒæ—¶ç»“åˆé«˜æ–¯åˆå¹¶ç¨‹åº (Gaussian merging procedure) æå‡äº†è®¡ç®—æ•ˆç‡å¹¶é˜²æ­¢é€€åŒ–è§£ã€‚åœ¨è®¡ç®—æµä½“åŠ¨åŠ›å­¦ (CFD) å’ŒçœŸå® 4D flow MRI æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒPINGS-X åœ¨å¤§å¹…ç¼©çŸ­è®­ç»ƒæ—¶é—´çš„åŒæ—¶å®ç°äº†å“è¶Šçš„è¶…åˆ†è¾¨ç‡ç²¾åº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted at AAAI 2026. Supplementary material included after references. 27 pages, 21 figures, 11 tables",
      "pdf_url": "https://arxiv.org/pdf/2511.11048v2",
      "published_date": "2025-11-14 08:01:24 UTC",
      "updated_date": "2026-01-13 07:54:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:21:31.539363+00:00"
    },
    {
      "arxiv_id": "2511.11046v2",
      "title": "Enhancing Graph Representations with Neighborhood-Contextualized Message-Passing",
      "title_zh": "é€šè¿‡é‚»åŸŸä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ¶ˆæ¯ä¼ é€’å¢å¼ºå›¾è¡¨ç¤º",
      "authors": [
        "Brian Godwin Lim",
        "Galvin Brice Lim",
        "Renzo Roel Tan",
        "Irwin King",
        "Kazushi Ikeda"
      ],
      "abstract": "Graph neural networks (GNNs) have become an indispensable tool for analyzing relational data. Classical GNNs are broadly classified into three variants: convolutional, attentional, and message-passing. While the standard message-passing variant is expressive, its typical pair-wise messages only consider the features of the center node and each neighboring node individually. This design fails to incorporate contextual information contained within the broader local neighborhood, potentially hindering its ability to learn complex relationships within the entire set of neighboring nodes. To address this limitation, this work first formalizes the concept of neighborhood-contextualization, rooted in a key property of the attentional variant. This then serves as the foundation for generalizing the message-passing variant to the proposed neighborhood-contextualized message-passing (NCMP) framework. To demonstrate its utility, a simple, practical, and efficient method to parametrize and operationalize NCMP is presented, leading to the development of the proposed Soft-Isomorphic Neighborhood-Contextualized Graph Convolution Network (SINC-GCN). Across a diverse set of synthetic and benchmark GNN datasets, SINC-GCN demonstrates competitive performance against baseline GNN models, highlighting its expressivity and efficiency. Notably, it also delivers substantial and statistically significant performance gains in graph property prediction tasks, further underscoring the distinctive utility of neighborhood-contextualization. Overall, the paper lays the foundation for the NCMP framework as a practical path toward enhancing the graph representational power of classical GNNs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å›¾ç¥ç»ç½‘ç»œ(GNNs)ä¸­æ ‡å‡†Message-Passingå˜ä½“ä»…å¤„ç†ä¸­å¿ƒèŠ‚ç‚¹ä¸å•ä¸ªé‚»å±…èŠ‚ç‚¹æˆå¯¹ä¿¡æ¯è€Œå¿½ç•¥å±€éƒ¨é‚»åŸŸContextual Informationçš„å±€é™æ€§ï¼Œæ­£å¼æå‡ºäº†Neighborhood-contextualizationæ¦‚å¿µã€‚åŸºäºæ­¤ï¼Œä½œè€…å°†Message-Passingæ‰©å±•ä¸ºNCMPæ¡†æ¶ï¼Œå¹¶å¼€å‘äº†åä¸ºSINC-GCNçš„é«˜æ•ˆå‚æ•°åŒ–å®ç°æ¨¡å‹ã€‚é€šè¿‡åœ¨å¤šé¡¹åˆæˆåŠåŸºå‡†æ•°æ®é›†ä¸Šçš„æµ‹è¯•ï¼ŒSINC-GCNè¯æ˜äº†å…¶åœ¨è¡¨è¾¾èƒ½åŠ›ä¸æ•ˆç‡ä¸Šçš„ä¼˜åŠ¿ï¼Œç‰¹åˆ«æ˜¯åœ¨Graph property predictionä»»åŠ¡ä¸­å®ç°äº†ç»Ÿè®¡å­¦æ„ä¹‰ä¸Šçš„æ˜¾è‘—æ€§èƒ½æå‡ã€‚æ•´ä½“è€Œè¨€ï¼ŒNCMPæ¡†æ¶ä¸ºå¢å¼ºç»å…¸GNNsçš„å›¾è¡¨ç¤ºèƒ½åŠ›ï¼ˆGraph representational powerï¼‰å¥ å®šäº†ç†è®ºåŸºç¡€å¹¶æä¾›äº†å®ç”¨è·¯å¾„ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11046v2",
      "published_date": "2025-11-14 08:00:19 UTC",
      "updated_date": "2026-01-08 04:26:02 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:21:39.739090+00:00"
    },
    {
      "arxiv_id": "2511.11043v2",
      "title": "Autonomous Vehicle Path Planning by Searching With Differentiable Simulation",
      "title_zh": "åŸºäºå¯å¾®ä»¿çœŸæœç´¢çš„è‡ªåŠ¨é©¾é©¶è½¦è¾†è·¯å¾„è§„åˆ’",
      "authors": [
        "Asen Nachkov",
        "Jan-Nico Zaech",
        "Danda Pani Paudel",
        "Xi Wang",
        "Luc Van Gool"
      ],
      "abstract": "Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DSS (Differentiable Simulation for Search) æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³è‡ªåŠ¨é©¾é©¶è·¯å¾„è§„åˆ’ä¸­å­¦ä¹ å‹ç»„ä»¶åœ¨å¤æ‚äº¤é€šåœºæ™¯ä¸‹è¡¨ç°ä¸ä½³çš„é—®é¢˜ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¯å¾®åˆ†æ¨¡æ‹Ÿå™¨ Waymax åŒæ—¶ä½œä¸ºçŠ¶æ€é¢„æµ‹å™¨å’Œè¯„ä»·å™¨ï¼Œç»“åˆæ¨¡æ‹Ÿå™¨å†…ç½®çš„ç¡¬ç¼–ç åŠ¨åŠ›å­¦ (hardcoded dynamics) å®ç°äº†æé«˜ç²¾åº¦çš„çŠ¶æ€é¢„æµ‹ã€‚é€šè¿‡åˆ©ç”¨æ¨¡æ‹Ÿå™¨çš„å¯å¾®åˆ†ç‰¹æ€§ï¼ŒDSS èƒ½å¤Ÿåœ¨æ„æƒ³çš„æœªæ¥è½¨è¿¹ä¸Šåˆ©ç”¨æ¢¯åº¦ä¸‹é™ (gradient descent) å¯¹åŠ¨ä½œåºåˆ—è¿›è¡Œé«˜æ•ˆæœç´¢ä¸ä¼˜åŒ–ã€‚å®éªŒè¯æ˜ï¼ŒDSS å°†è§„åˆ’æ¢¯åº¦ (planning gradients) ä¸éšæœºæœç´¢ (stochastic search) ç›¸ç»“åˆï¼Œåœ¨è·Ÿè¸ªå’Œè·¯å¾„è§„åˆ’å‡†ç¡®æ€§ä¸Šæ˜¾è‘—ä¼˜äºåºåˆ—é¢„æµ‹ã€æ¨¡ä»¿å­¦ä¹  (imitation learning) åŠæ— æ¨¡å‹å¼ºåŒ–å­¦ä¹  (model-free RL) ç­‰ä¼ ç»Ÿæ–¹æ³•ã€‚",
      "categories": [
        "cs.AI",
        "cs.RO"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11043v2",
      "published_date": "2025-11-14 07:56:34 UTC",
      "updated_date": "2025-11-24 09:43:11 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:21:38.236778+00:00"
    },
    {
      "arxiv_id": "2511.11041v1",
      "title": "Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB",
      "title_zh": "çº æ­£æ–‡æœ¬åµŒå…¥ä¸­çš„å‡å€¼åå·®ï¼šä¸€ç§åœ¨ MMTEB ä¸Šå®ç°æ— éœ€è®­ç»ƒæå‡çš„æ”¹è¿›é‡å½’ä¸€åŒ–æ–¹æ³•",
      "authors": [
        "Xingyu Ren",
        "Youran Sun",
        "Haoyu Liang"
      ],
      "abstract": "We find that current text embedding models produce outputs with a consistent bias, i.e., each embedding vector $e$ can be decomposed as $\\tilde{e} + Î¼$, where $Î¼$ is almost identical across all sentences. We propose a plug-and-play, training-free and lightweight solution called Renormalization. Through extensive experiments, we show that renormalization consistently and statistically significantly improves the performance of existing models on the Massive Multilingual Text Embedding Benchmark (MMTEB). In particular, across 38 models, renormalization improves performance by 9.7 $Ïƒ$ on retrieval tasks, 3.1 $Ïƒ$ on classification tasks, and 0.8 $Ïƒ$ on other types of tasks. Renormalization has two variants: directly subtracting $Î¼$ from $e$, or subtracting the projection of $e$ onto $Î¼$. We theoretically predict that the latter performs better, and our experiments confirm this prediction.",
      "tldr_zh": "è¯¥ç ”ç©¶å‘ç°å½“å‰çš„ Text Embedding æ¨¡å‹ç”Ÿæˆçš„å‘é‡å­˜åœ¨ä¸€è‡´çš„å‡å€¼åå·®(Mean Bias)ï¼Œå³æ¯ä¸ªåµŒå…¥å‘é‡éƒ½å¯ä»¥åˆ†è§£ä¸ºä¸€ä¸ªç‰¹å®šå‘é‡ä¸å‡ ä¹æ’å®šçš„åå·®å‘é‡ $\\mu$ ä¹‹å’Œã€‚ä¸ºäº†è§£å†³è¿™ä¸€é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åä¸º Renormalization çš„å³æ’å³ç”¨ã€æ— éœ€è®­ç»ƒä¸”è½»é‡çº§çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•åŒ…å«ä¸¤ç§å˜ä½“ï¼Œåˆ†åˆ«æ˜¯ç›´æ¥ä»åŸå§‹å‘é‡ä¸­å‡å» $\\mu$ï¼Œä»¥åŠå‡å»å‘é‡åœ¨ $\\mu$ ä¸Šçš„æŠ•å½±ï¼Œä¸”ç†è®ºé¢„æµ‹å’Œå®éªŒç»“æœå‡è¯å®åè€…è¡¨ç°æ›´ä¼˜ã€‚åœ¨ Massive Multilingual Text Embedding Benchmark (MMTEB) çš„å¹¿æ³›å®éªŒä¸­ï¼ŒRenormalization æ˜¾è‘—ä¸”ç¨³å®šåœ°æå‡äº† 38 ä¸ªç°æœ‰æ¨¡å‹çš„è¡¨ç°ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥æ–¹æ³•åœ¨ Retrieval ä»»åŠ¡ä¸Šå¹³å‡æå‡äº† 9.7 $\\sigma$ï¼Œåœ¨ Classification ä»»åŠ¡ä¸Šæå‡äº† 3.1 $\\sigma$ï¼Œåœ¨å…¶ä»–ä»»åŠ¡ä¸­ä¹Ÿæœ‰ä¸åŒç¨‹åº¦çš„æ”¹è¿›ã€‚è¿™é¡¹ç ”ç©¶è¯æ˜äº†é€šè¿‡ç®€å•çš„å‡ ä½•ä¿®æ­£å³å¯æœ‰æ•ˆæ¶ˆé™¤åµŒå…¥ç©ºé—´çš„ç³»ç»Ÿåå·®ï¼Œä¸ºä¼˜åŒ–é¢„è®­ç»ƒæ¨¡å‹æä¾›äº†ä¸€ç§æä½æˆæœ¬çš„é«˜æ•ˆé€”å¾„ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11041v1",
      "published_date": "2025-11-14 07:51:59 UTC",
      "updated_date": "2025-11-14 07:51:59 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:21:47.834222+00:00"
    },
    {
      "arxiv_id": "2511.11040v1",
      "title": "Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?",
      "title_zh": "å¤šæ™ºèƒ½ä½“è¾©è®ºä¸­çš„å…³é”®å†³ç­–è€…ï¼šè°æŒæ¡ç€æƒåŠ›ï¼Ÿ",
      "authors": [
        "Qian Zhang",
        "Yan Zheng",
        "Jinyi Liu",
        "Hebin Liang",
        "Lanjun Wang"
      ],
      "abstract": "Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, \"Truth Last\", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨å¤šæ™ºèƒ½ä½“è¾©è®º(Multi-Agent Debate, MAD)ä¸­çš„è§’è‰²åˆ†é…ç­–ç•¥ï¼Œæ­ç¤ºäº†ä¸åŒè§‚ç‚¹çš„è§’è‰²ä½ç½®å¯¹æ¨ç†æ€§èƒ½çš„æ˜¾è‘—å½±å“ã€‚ç ”ç©¶å‘ç°äº†ä¸€ç§åä¸º\"Truth Last\"çš„æ–°å‹è§’è‰²åˆ†é…ç­–ç•¥ï¼Œåœ¨æ¨ç†ä»»åŠ¡ä¸­èƒ½å°†MADæ€§èƒ½æå‡é«˜è¾¾22%ã€‚é’ˆå¯¹å®é™…åº”ç”¨ä¸­çœŸå€¼æœªçŸ¥çš„é—®é¢˜ï¼Œä½œè€…æå‡ºäº†Multi-Agent Debate Consistency (MADC)ç­–ç•¥ï¼Œè¯¥ç­–ç•¥é€šè¿‡è·¯å¾„ä¸€è‡´æ€§(path consistency)è¯„ä¼°ç‹¬ç«‹è§’è‰²é—´çš„å…±è¯†ï¼Œå¹¶å°†ä¸€è‡´æ€§å¾—åˆ†æœ€é«˜çš„è§’è‰²æ¨¡æ‹Ÿä¸ºçœŸå€¼ã€‚é€šè¿‡åœ¨åŒ…æ‹¬DeepSeek-R1 Distilled Modelsåœ¨å†…çš„9ç§å¤§è¯­è¨€æ¨¡å‹ä¸Šè¿›è¡ŒéªŒè¯ï¼ŒMADCå±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ï¼Œæœ‰æ•ˆå…‹æœäº†MADçš„æ€§èƒ½ç“¶é¢ˆã€‚è¯¥ç ”ç©¶ä¸ºLLMæ™ºèƒ½ä½“æ‰©å±•(agent scaling)çš„è¿›ä¸€æ­¥ä¼˜åŒ–æä¾›äº†å…³é”®è·¯å¾„å’Œç†è®ºæ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11040v1",
      "published_date": "2025-11-14 07:47:56 UTC",
      "updated_date": "2025-11-14 07:47:56 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:21:45.946710+00:00"
    },
    {
      "arxiv_id": "2512.00040v1",
      "title": "Constrained Network Slice Assignment via Large Language Models",
      "title_zh": "åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å—çº¦æŸç½‘ç»œåˆ‡ç‰‡åˆ†é…",
      "authors": [
        "Sagar Sudhakara",
        "Pankaj Rajak"
      ],
      "abstract": "Modern networks support network slicing, which partitions physical infrastructure into virtual slices tailored to different service requirements (for example, high bandwidth or low latency). Optimally allocating users to slices is a constrained optimization problem that traditionally requires complex algorithms. In this paper, we explore the use of Large Language Models (LLMs) to tackle radio resource allocation for network slicing. We focus on two approaches: (1) using an LLM in a zero-shot setting to directly assign user service requests to slices, and (2) formulating an integer programming model where the LLM provides semantic insight by estimating similarity between requests. Our experiments show that an LLM, even with zero-shot prompting, can produce a reasonable first draft of slice assignments, although it may violate some capacity or latency constraints. We then incorporate the LLM's understanding of service requirements into an optimization solver to generate an improved allocation. The results demonstrate that LLM-guided grouping of requests, based on minimal textual input, achieves performance comparable to traditional methods that use detailed numerical data, in terms of resource utilization and slice isolation. While the LLM alone does not perfectly satisfy all constraints, it significantly reduces the search space and, when combined with exact solvers, provides a promising approach for efficient 5G network slicing resource allocation.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(Large Language Models, LLMs)è§£å†³ç½‘ç»œåˆ‡ç‰‡(network slicing)ä¸­çš„æ— çº¿èµ„æºåˆ†é…é—®é¢˜ã€‚ä½œè€…é‡ç‚¹ç ”ç©¶äº†ä¸¤ç§æ–¹æ¡ˆï¼šä¸€ç§æ˜¯åœ¨Zero-shotè®¾ç½®ä¸‹åˆ©ç”¨LLMç›´æ¥è¿›è¡Œåˆ‡ç‰‡åˆ†é…ï¼Œå¦ä¸€ç§æ˜¯å°†LLMçš„è¯­ä¹‰æ´å¯ŸåŠ›ä¸æ•´æ•°è§„åˆ’(integer programming)æ¨¡å‹ç›¸ç»“åˆã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼ŒZero-shotæ¨¡å¼ä¸‹çš„LLMè™½ç„¶èƒ½ç”Ÿæˆåˆæ­¥åˆ†é…è‰æ¡ˆï¼Œä½†éš¾ä»¥å®Œå…¨æ»¡è¶³å®¹é‡å’Œå»¶è¿Ÿçº¦æŸã€‚é€šè¿‡å¼•å…¥LLMå¯¹æœåŠ¡éœ€æ±‚çš„æ–‡æœ¬ç†è§£å¹¶è¾…åŠ©ä¼˜åŒ–æ±‚è§£å™¨ï¼Œè¯¥æ–¹æ³•åœ¨èµ„æºåˆ©ç”¨ç‡(resource utilization)å’Œåˆ‡ç‰‡éš”ç¦»(slice isolation)æ–¹é¢çš„è¡¨ç°å·²å¯ä¸ä¾èµ–å¤æ‚æ•°å€¼æ•°æ®çš„ä¼ ç»Ÿç®—æ³•ç›¸åª²ç¾ã€‚è¯¥ç ”ç©¶è¯æ˜äº†LLMèƒ½å¤Ÿæ˜¾è‘—ç¼©å°ä¼˜åŒ–é—®é¢˜çš„æœç´¢ç©ºé—´ï¼Œä¸º5Gç½‘ç»œåˆ‡ç‰‡çš„é«˜æ•ˆèµ„æºåˆ†é…æä¾›äº†ä¸€ç§ç»“åˆè¯­ä¹‰ç†è§£ä¸ç²¾ç¡®æ±‚è§£çš„æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "primary_category": "cs.NI",
      "comment": "Accepted at NeurIPS 2025 Workshop on AI and ML for Next-Generation Wireless Communications and Networking (AI4NextG), San Diego, CA",
      "pdf_url": "https://arxiv.org/pdf/2512.00040v1",
      "published_date": "2025-11-14 07:47:42 UTC",
      "updated_date": "2025-11-14 07:47:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:21:50.434690+00:00"
    },
    {
      "arxiv_id": "2511.11038v1",
      "title": "SemanticNN: Compressive and Error-Resilient Semantic Offloading for Extremely Weak Devices",
      "title_zh": "SemanticNNï¼šé¢å‘æå¼±è®¾å¤‡çš„å‹ç¼©å¼æŠ—è¯¯ç è¯­ä¹‰å¸è½½",
      "authors": [
        "Jiaming Huang",
        "Yi Gao",
        "Fuchang Pan",
        "Renjie Li",
        "Wei Dong"
      ],
      "abstract": "With the rapid growth of the Internet of Things (IoT), integrating artificial intelligence (AI) on extremely weak embedded devices has garnered significant attention, enabling improved real-time performance and enhanced data privacy. However, the resource limitations of such devices and unreliable network conditions necessitate error-resilient device-edge collaboration systems. Traditional approaches focus on bit-level transmission correctness, which can be inefficient under dynamic channel conditions. In contrast, we propose SemanticNN, a semantic codec that tolerates bit-level errors in pursuit of semantic-level correctness, enabling compressive and resilient collaborative inference offloading under strict computational and communication constraints. It incorporates a Bit Error Rate (BER)-aware decoder that adapts to dynamic channel conditions and a Soft Quantization (SQ)-based encoder to learn compact representations. Building on this architecture, we introduce Feature-augmentation Learning, a novel training strategy that enhances offloading efficiency. To address encoder-decoder capability mismatches from asymmetric resources, we propose XAI-based Asymmetry Compensation to enhance decoding semantic fidelity. We conduct extensive experiments on STM32 using three models and six datasets across image classification and object detection tasks. Experimental results demonstrate that, under varying transmission error rates, SemanticNN significantly reduces feature transmission volume by 56.82-344.83x while maintaining superior inference accuracy.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹èµ„æºå—é™çš„ç‰©è”ç½‘(IoT)åµŒå…¥å¼è®¾å¤‡ï¼Œæå‡ºäº†SemanticNNï¼Œä¸€ç§æ—¨åœ¨å®ç°å‹ç¼©ä¸”å…·æœ‰å®¹é”™èƒ½åŠ›çš„è¯­ä¹‰å¸è½½(Semantic Offloading)æ¡†æ¶ã€‚ä¸å…³æ³¨ä½çº§(bit-level)ä¼ è¾“æ­£ç¡®æ€§çš„ä¼ ç»Ÿæ–¹æ³•ä¸åŒï¼ŒSemanticNN å…è®¸ä½çº§é”™è¯¯ï¼Œé€šè¿‡è¿½æ±‚è¯­ä¹‰å±‚é¢çš„æ­£ç¡®æ€§æ¥å®ç°åŠ¨æ€ä¿¡é“ç¯å¢ƒä¸‹çš„é«˜æ•ˆåä½œæ¨ç†ã€‚è¯¥æ¡†æ¶ç»“åˆäº†æ„ŸçŸ¥è¯¯ç ç‡(BER-aware)çš„è§£ç å™¨å’ŒåŸºäºè½¯é‡åŒ–(Soft Quantization)çš„ç¼–ç å™¨ï¼Œå¹¶å¼•å…¥ç‰¹å¾å¢å¼ºå­¦ä¹ (Feature-augmentation Learning)ç­–ç•¥ä»¥æå‡å¸è½½æ•ˆç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶é‡‡ç”¨åŸºäºå¯è§£é‡Šäººå·¥æ™ºèƒ½(XAI)çš„å¼‚æ­¥è¡¥å¿æŠ€æœ¯(Asymmetry Compensation)è§£å†³äº†ç«¯äº‘èµ„æºä¸å¯¹ç§°å¯¼è‡´çš„èƒ½åŠ›å¤±é…é—®é¢˜ï¼Œå¢å¼ºäº†è§£ç çš„è¯­ä¹‰ä¿çœŸåº¦ã€‚åœ¨STM32å¹³å°ä¸Šçš„å®éªŒè¯æ˜ï¼ŒSemanticNN åœ¨ä¿æŒä¼˜å¼‚æ¨ç†å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œå°†ç‰¹å¾ä¼ è¾“é‡æ˜¾è‘—é™ä½äº†56.82-344.83å€ï¼Œä¸ºæå¼±è®¾å¤‡ä¸Šçš„æ™ºèƒ½åº”ç”¨æä¾›äº†ç¨³å¥ä¸”é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.DC"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11038v1",
      "published_date": "2025-11-14 07:47:25 UTC",
      "updated_date": "2025-11-14 07:47:25 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:22:08.930705+00:00"
    },
    {
      "arxiv_id": "2511.11034v1",
      "title": "CrossMed: A Multimodal Cross-Task Benchmark for Compositional Generalization in Medical Imaging",
      "title_zh": "CrossMedï¼šåŒ»å­¦å½±åƒç»„åˆæ³›åŒ–çš„å¤šæ¨¡æ€è·¨ä»»åŠ¡åŸºå‡†",
      "authors": [
        "Pooja Singh",
        "Siddhant Ujjain",
        "Tapan Kumar Gandhi",
        "Sandeep Kumar"
      ],
      "abstract": "Recent advances in multimodal large language models have enabled unified processing of visual and textual inputs, offering promising applications in general-purpose medical AI. However, their ability to generalize compositionally across unseen combinations of imaging modality, anatomy, and task type remains underexplored. We introduce CrossMed, a benchmark designed to evaluate compositional generalization (CG) in medical multimodal LLMs using a structured Modality-Anatomy-Task (MAT) schema. CrossMed reformulates four public datasets, CheXpert (X-ray classification), SIIM-ACR (X-ray segmentation), BraTS 2020 (MRI classification and segmentation), and MosMedData (CT classification) into a unified visual question answering (VQA) format, resulting in 20,200 multiple-choice QA instances. We evaluate two open-source multimodal LLMs, LLaVA-Vicuna-7B and Qwen2-VL-7B, on both Related and Unrelated MAT splits, as well as a zero-overlap setting where test triplets share no Modality, Anatomy, or Task with the training data. Models trained on Related splits achieve 83.2 percent classification accuracy and 0.75 segmentation cIoU, while performance drops significantly under Unrelated and zero-overlap conditions, demonstrating the benchmark difficulty. We also show cross-task transfer, where segmentation performance improves by 7 percent cIoU even when trained using classification-only data. Traditional models (ResNet-50 and U-Net) show modest gains, confirming the broad utility of the MAT framework, while multimodal LLMs uniquely excel at compositional generalization. CrossMed provides a rigorous testbed for evaluating zero-shot, cross-task, and modality-agnostic generalization in medical vision-language models.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†CrossMedï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨è¯„ä¼°åŒ»ç–—å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMultimodal LLMsï¼‰ç»„åˆæ³›åŒ–ï¼ˆCompositional Generalization, CGï¼‰èƒ½åŠ›çš„è·¨ä»»åŠ¡åŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†é‡‡ç”¨ç»“æ„åŒ–çš„æ¨¡æ€-è§£å‰–-ä»»åŠ¡ï¼ˆModality-Anatomy-Task, MATï¼‰æ¨¡å¼ï¼Œå°†X-rayã€MRIå’ŒCTç­‰å¤šç§åŒ»å­¦å½±åƒæ•°æ®é›†é‡æ„ä¸ºç»Ÿä¸€çš„è§†è§‰é—®ç­”ï¼ˆVQAï¼‰æ ¼å¼ã€‚é€šè¿‡å¯¹LLaVA-Vicuna-7Bå’ŒQwen2-VL-7Båœ¨ç›¸å…³ã€ä¸ç›¸å…³åŠé›¶é‡å ï¼ˆZero-overlapï¼‰è®¾ç½®ä¸‹çš„è¯„ä¼°ï¼Œç ”ç©¶å‘ç°æ¨¡å‹åœ¨å¤„ç†æœªè§è¿‡çš„MATç»„åˆæ—¶æ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œæ­ç¤ºäº†è¯¥åŸºå‡†çš„æŒ‘æˆ˜æ€§ã€‚å®éªŒè¿˜è¯æ˜äº†æ˜¾è‘—çš„è·¨ä»»åŠ¡è¿ç§»ï¼ˆCross-task transferï¼‰æ•ˆåº”ï¼Œå³åˆ†ç±»æ•°æ®çš„è®­ç»ƒèƒ½æœ‰æ•ˆæå‡åˆ†å‰²ä»»åŠ¡çš„æ€§èƒ½ã€‚CrossMedä¸ºåŒ»ç–—è§†è§‰è¯­è¨€æ¨¡å‹åœ¨é›¶æ ·æœ¬ï¼ˆZero-shotï¼‰å’Œæ¨¡æ€æ— å…³ï¼ˆModality-agnosticï¼‰æ³›åŒ–æ–¹é¢çš„æ€§èƒ½è¯„ä¼°æä¾›äº†ä¸€ä¸ªä¸¥è°¨çš„å®éªŒå¹³å°ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11034v1",
      "published_date": "2025-11-14 07:41:01 UTC",
      "updated_date": "2025-11-14 07:41:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:22:08.395700+00:00"
    },
    {
      "arxiv_id": "2511.11030v4",
      "title": "Algorithms Trained on Normal Chest X-rays Can Predict Health Insurance Types",
      "title_zh": "åŸºäºæ­£å¸¸èƒ¸éƒ¨ X å°„çº¿ç‰‡è®­ç»ƒçš„ç®—æ³•èƒ½å¤Ÿé¢„æµ‹åŒ»ç–—ä¿é™©ç±»å‹",
      "authors": [
        "Chi-Yu Chen",
        "Rawan Abulibdeh",
        "Arash Asgari",
        "SebastiÃ¡n AndrÃ©s Cajas OrdÃ³Ã±ez",
        "Leo Anthony Celi",
        "Deirdre Goode",
        "Hassan Hamidi",
        "Laleh Seyyed-Kalantari",
        "Ned McCague",
        "Thomas Sounack",
        "Po-Chih Kuo"
      ],
      "abstract": "Artificial intelligence is revealing what medicine never intended to encode. Deep vision models, trained on chest X-rays, can now detect not only disease but also invisible traces of social inequality. In this study, we show that state-of-the-art architectures (DenseNet121, SwinV2-B, MedMamba) can predict a patient's health insurance type, a strong proxy for socioeconomic status, from normal chest X-rays with significant accuracy (AUC around 0.70 on MIMIC-CXR-JPG, 0.68 on CheXpert). The signal was unlikely contributed by demographic features by our machine learning study combining age, race, and sex labels to predict health insurance types; it also remains detectable when the model is trained exclusively on a single racial group. Patch-based occlusion reveals that the signal is diffuse rather than localized, embedded in the upper and mid-thoracic regions. This suggests that deep networks may be internalizing subtle traces of clinical environments, equipment differences, or care pathways; learning socioeconomic segregation itself. These findings challenge the assumption that medical images are neutral biological data. By uncovering how models perceive and exploit these hidden social signatures, this work reframes fairness in medical AI: the goal is no longer only to balance datasets or adjust thresholds, but to interrogate and disentangle the social fingerprints embedded in clinical data itself.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†æ·±åº¦è§†è§‰æ¨¡å‹å¦‚ä½•ä»çœ‹ä¼¼ä¸­æ€§çš„åŒ»ç–—å›¾åƒä¸­è¯†åˆ«ç¤¾ä¼šä¸å¹³ç­‰ç‰¹å¾ï¼Œå‘ç°å³ä½¿æ˜¯åŸºäºæ­£å¸¸èƒ¸éƒ¨Xå°„çº¿(Chest X-rays)è®­ç»ƒçš„ç®—æ³•ä¹Ÿèƒ½é«˜ç²¾åº¦é¢„æµ‹æ‚£è€…çš„åŒ»ç–—ä¿é™©ç±»å‹ã€‚ç ”ç©¶é‡‡ç”¨äº† DenseNet121ã€SwinV2-B å’Œ MedMamba ç­‰æ¨¡å‹æ¶æ„ï¼Œåœ¨ MIMIC-CXR-JPG å’Œ CheXpert æ•°æ®é›†ä¸Šåˆ†åˆ«å®ç°äº†çº¦ 0.70 å’Œ 0.68 çš„æ›²çº¿ä¸‹é¢ç§¯(AUC)ã€‚å®éªŒè¿›ä¸€æ­¥è¯å®ï¼Œè¯¥é¢„æµ‹ä¿¡å·å¹¶éç”±å¹´é¾„ã€ç§æ—æˆ–æ€§åˆ«ç­‰äººå£ç»Ÿè®¡å­¦ç‰¹å¾é©±åŠ¨ï¼Œä¸”åœ¨å•ä¸€æ¨¡å‹ä»…é’ˆå¯¹ç‰¹å®šç§æ—ç¾¤ä½“è®­ç»ƒæ—¶ä¾ç„¶å¯è¢«æ£€æµ‹ã€‚é€šè¿‡åŸºäºè¡¥ä¸çš„é®æŒ¡æŠ€æœ¯(Patch-based occlusion)åˆ†æï¼Œç ”ç©¶å‘ç°è¿™äº›ä¿¡å·å¼¥æ•£åœ°åµŒå…¥åœ¨èƒ¸éƒ¨ä¸­ä¸ŠåŒºåŸŸï¼Œåæ˜ äº†æ¨¡å‹å¯¹ä¸´åºŠç¯å¢ƒã€è®¾å¤‡å·®å¼‚æˆ–ç¤¾ä¼šç»æµéš”ç¦»ç—•è¿¹çš„å†…åŒ–ã€‚è¿™ä¸€å‘ç°æŒ‘æˆ˜äº†åŒ»ç–—å›¾åƒä½œä¸ºä¸­æ€§ç”Ÿç‰©æ•°æ®çš„å‡è®¾ï¼Œæç¤ºåŒ»ç–—äººå·¥æ™ºèƒ½(AI)çš„å…¬å¹³æ€§ç ”ç©¶åº”ä»ç®€å•çš„æ•°æ®å¹³è¡¡è½¬å‘å¯¹ä¸´åºŠæ•°æ®ä¸­éšè—ç¤¾ä¼šæŒ‡çº¹çš„æ·±å…¥å®¡è§†ä¸è§£æ„ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Submitting to MIDL 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.11030v4",
      "published_date": "2025-11-14 07:34:29 UTC",
      "updated_date": "2025-12-06 18:13:42 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:22:10.184523+00:00"
    },
    {
      "arxiv_id": "2511.11029v1",
      "title": "Faster Symmetry Breaking Constraints for Abstract Structures",
      "title_zh": "é¢å‘æŠ½è±¡ç»“æ„çš„æ›´å¿«é€Ÿå¯¹ç§°æ€§ç ´ç¼ºçº¦æŸ",
      "authors": [
        "Ã–zgÃ¼r AkgÃ¼n",
        "Mun See Chang",
        "Ian P. Gent",
        "Christopher Jefferson"
      ],
      "abstract": "In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to \"break\" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (AkgÃ¼n et al. 2025).",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹çº¦æŸç¼–ç¨‹(Constraint Programming)ä¸­æŠ½è±¡ç»“æ„(Abstract Structures)çš„å¯¹ç§°æ€§æ¶ˆé™¤(Symmetry Breaking)æ•ˆç‡é—®é¢˜æå‡ºäº†ä¸€ç§æ–°æ–¹æ¡ˆã€‚åœ¨å¤„ç†å¦‚åµŒå¥—é›†åˆç­‰å¤æ‚ç»“æ„æ—¶ï¼Œä¼ ç»Ÿæ–¹æ³•äº§ç”Ÿçš„å¯¹ç§°æ€§æ¶ˆé™¤çº¦æŸå¾€å¾€å› è¿‡äºå¤æ‚è€Œå¯¼è‡´æ€§èƒ½ä½ä¸‹ï¼Œé™åˆ¶äº†æ±‚è§£é€Ÿåº¦ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨ç»“æ„è¡¨ç¤ºå½¢å¼(Representations)çš„ä¸å®Œå…¨å¯¹ç§°æ€§æ¶ˆé™¤æ–¹æ³•ï¼Œä¸“é—¨ç”¨äºå¤„ç†ç”±ä¸å¯åŒºåˆ†å¯¹è±¡(Indistinguishable Objects)å¼•å‘çš„å¸¸è§å¯¹ç§°æ€§é—®é¢˜ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ‰§è¡Œé€Ÿåº¦ä¸Šæ˜¾è‘—ä¼˜äº AkgÃ¼n ç­‰äººåœ¨ 2025 å¹´æå‡ºçš„ç°æœ‰æŠ€æœ¯æ–¹æ¡ˆã€‚é€šè¿‡æ›´æœ‰æ•ˆåœ°æŒ–æ˜åº•å±‚è¡¨ç¤ºçš„ç‰¹æ€§ï¼Œè¯¥ç ”ç©¶ä¸ºæå‡é«˜çº§å»ºæ¨¡è¯­è¨€ä¸­æŠ½è±¡ç»“æ„çš„æœç´¢æ•ˆç‡æä¾›äº†å…³é”®çš„æŠ€æœ¯æ”¯æŒã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11029v1",
      "published_date": "2025-11-14 07:34:23 UTC",
      "updated_date": "2025-11-14 07:34:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:22:13.640804+00:00"
    },
    {
      "arxiv_id": "2511.13758v1",
      "title": "ChemFixer: Correcting Invalid Molecules to Unlock Previously Unseen Chemical Space",
      "title_zh": "ChemFixerï¼šé€šè¿‡ä¿®æ­£æ— æ•ˆåˆ†å­å¼€å¯æ­¤å‰æœªè§çš„åŒ–å­¦ç©ºé—´",
      "authors": [
        "Jun-Hyoung Park",
        "Ho-Jun Song",
        "Seong-Whan Lee"
      ],
      "abstract": "Deep learning-based molecular generation models have shown great potential in efficiently exploring vast chemical spaces by generating potential drug candidates with desired properties. However, these models often produce chemically invalid molecules, which limits the usable scope of the learned chemical space and poses significant challenges for practical applications. To address this issue, we propose ChemFixer, a framework designed to correct invalid molecules into valid ones. ChemFixer is built on a transformer architecture, pre-trained using masking techniques, and fine-tuned on a large-scale dataset of valid/invalid molecular pairs that we constructed. Through comprehensive evaluations across diverse generative models, ChemFixer improved molecular validity while effectively preserving the chemical and biological distributional properties of the original outputs. This indicates that ChemFixer can recover molecules that could not be previously generated, thereby expanding the diversity of potential drug candidates. Furthermore, ChemFixer was effectively applied to a drug-target interaction (DTI) prediction task using limited data, improving the validity of generated ligands and discovering promising ligand-protein pairs. These results suggest that ChemFixer is not only effective in data-limited scenarios, but also extensible to a wide range of downstream tasks. Taken together, ChemFixer shows promise as a practical tool for various stages of deep learning-based drug discovery, enhancing molecular validity and expanding accessible chemical space.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ChemFixerï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¨åœ¨å°†æ— æ•ˆåˆ†å­ä¿®æ­£ä¸ºæœ‰æ•ˆåˆ†å­çš„æ¡†æ¶ï¼Œç”¨ä»¥è§£å†³æ·±åº¦å­¦ä¹ åˆ†å­ç”Ÿæˆæ¨¡å‹å› äº§ç”ŸåŒ–å­¦æ— æ•ˆåˆ†å­è€Œé™åˆ¶æ¢ç´¢èŒƒå›´çš„é—®é¢˜ã€‚ChemFixeråŸºäºTransformeræ¶æ„ï¼Œé€šè¿‡æ©ç æŠ€æœ¯(masking techniques)è¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶åœ¨å¤§è§„æ¨¡çš„æœ‰æ•ˆ/æ— æ•ˆåˆ†å­å¯¹æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚å®éªŒè¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥æ¡†æ¶åœ¨æ˜¾è‘—æé«˜åˆ†å­æœ‰æ•ˆæ€§çš„åŒæ—¶ï¼Œèƒ½æœ‰æ•ˆä¿ç•™åŸå§‹è¾“å‡ºçš„åŒ–å­¦ä¸ç”Ÿç‰©åˆ†å¸ƒç‰¹æ€§ï¼Œä»è€Œè§£é”äº†æ­¤å‰æ— æ³•ç”Ÿæˆçš„åŒ–å­¦ç©ºé—´å¹¶å¢åŠ äº†å€™é€‰è¯ç‰©çš„å¤šæ ·æ€§ã€‚æ­¤å¤–ï¼ŒChemFixeråœ¨æ•°æ®æœ‰é™çš„è¯ç‰©-é¶ç‚¹ç›¸äº’ä½œç”¨(DTI)é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œæå‡äº†ç”Ÿæˆé…ä½“çš„æœ‰æ•ˆæ€§å¹¶åŠ©åŠ›å‘ç°å…·æœ‰å‰æ™¯çš„è›‹ç™½è´¨-é…ä½“å¯¹ã€‚ç ”ç©¶ç»“æœè¯æ˜ï¼ŒChemFixeræ˜¯ä¸€ä¸ªèƒ½å¤Ÿå¢å¼ºåˆ†å­æœ‰æ•ˆæ€§å¹¶å¯æ‰©å±•è‡³å¤šç§ä¸‹æ¸¸ä»»åŠ¡çš„å®ç”¨å·¥å…·ï¼Œä¸ºæ·±åº¦å­¦ä¹ è¾…åŠ©è¯ç‰©ç ”å‘æä¾›äº†é‡è¦æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "This is the author's preprint version of the article accepted to IEEE JBHI. Final published version: https://doi.org/10.1109/JBHI.2025.3593825. High-quality PDF (publisher version): https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=11106678. Note: Some figures may appear distorted due to arXiv's TeXLive rendering",
      "pdf_url": "https://arxiv.org/pdf/2511.13758v1",
      "published_date": "2025-11-14 07:25:03 UTC",
      "updated_date": "2025-11-14 07:25:03 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:22:20.741183+00:00"
    },
    {
      "arxiv_id": "2511.11025v2",
      "title": "AirCopBench: A Benchmark for Multi-drone Collaborative Embodied Perception and Reasoning",
      "title_zh": "AirCopBenchï¼šå¤šæ— äººæœºååŒå…·èº«æ„ŸçŸ¥ä¸æ¨ç†åŸºå‡†",
      "authors": [
        "Jirong Zha",
        "Yuxuan Fan",
        "Tianyu Zhang",
        "Geng Chen",
        "Yingfeng Chen",
        "Chen Gao",
        "Xinlei Chen"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have shown promise in single-agent vision tasks, yet benchmarks for evaluating multi-agent collaborative perception remain scarce. This gap is critical, as multi-drone systems provide enhanced coverage, robustness, and collaboration compared to single-sensor setups. Existing multi-image benchmarks mainly target basic perception tasks using high-quality single-agent images, thus failing to evaluate MLLMs in more complex, egocentric collaborative scenarios, especially under real-world degraded perception conditions.To address these challenges, we introduce AirCopBench, the first comprehensive benchmark designed to evaluate MLLMs in embodied aerial collaborative perception under challenging perceptual conditions. AirCopBench includes 14.6k+ questions derived from both simulator and real-world data, spanning four key task dimensions: Scene Understanding, Object Understanding, Perception Assessment, and Collaborative Decision, across 14 task types. We construct the benchmark using data from challenging degraded-perception scenarios with annotated collaborative events, generating large-scale questions through model-, rule-, and human-based methods under rigorous quality control. Evaluations on 40 MLLMs show significant performance gaps in collaborative perception tasks, with the best model trailing humans by 24.38% on average and exhibiting inconsistent results across tasks. Fine-tuning experiments further confirm the feasibility of sim-to-real transfer in aerial collaborative perception and reasoning.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† AirCopBenchï¼Œè¿™æ˜¯é¦–ä¸ªæ—¨åœ¨è¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ (MLLMs) åœ¨å…·èº«èˆªç©ºåä½œæ„ŸçŸ¥ (embodied aerial collaborative perception) ä¸­è¡¨ç°çš„ç»¼åˆåŸºå‡†æµ‹è¯•ã€‚ä¸ºäº†è§£å†³ç°æœ‰åŸºå‡†ç¼ºä¹å¯¹å¤šæ™ºèƒ½ä½“åä½œå’Œå¤æ‚é€€åŒ–æ„ŸçŸ¥åœºæ™¯è¯„ä¼°çš„é—®é¢˜ï¼Œè¯¥åŸºå‡†åŒ…å«äº†æ¥è‡ªä»¿çœŸå’ŒçœŸå®ä¸–ç•Œçš„ 14.6k ä½™ä¸ªé—®é¢˜ï¼Œæ¶µç›–äº†åœºæ™¯ç†è§£ (Scene Understanding)ã€ç‰©ä½“ç†è§£ (Object Understanding)ã€æ„ŸçŸ¥è¯„ä¼° (Perception Assessment) å’Œåä½œå†³ç­– (Collaborative Decision) å››ä¸ªæ ¸å¿ƒç»´åº¦ã€‚é€šè¿‡å¯¹ 40 ä¸ª MLLMs çš„è¯„ä¼°æ˜¾ç¤ºï¼Œé¡¶å°–æ¨¡å‹åœ¨åä½œæ„ŸçŸ¥ä»»åŠ¡ä¸Šçš„å¹³å‡è¡¨ç°è½åäºäººç±» 24.38%ï¼Œä¸”åœ¨ä¸åŒä»»åŠ¡é—´è¡¨ç°ä¸ä¸€ã€‚æ­¤å¤–ï¼Œå¾®è°ƒå®éªŒè¯å®äº†åœ¨èˆªç©ºåä½œæ„ŸçŸ¥ä¸æ¨ç†ä¸­å®ç°ä»æ¨¡æ‹Ÿåˆ°ç°å® (sim-to-real transfer) è¿ç§»çš„å¯è¡Œæ€§ã€‚è¯¥å·¥ä½œä¸ºå¢å¼ºå¤šæ— äººæœºç³»ç»Ÿçš„é²æ£’åä½œèƒ½åŠ›å¥ å®šäº†åŸºç¡€ï¼Œå¹¶å¡«è¡¥äº†å…·èº«æ„ŸçŸ¥é¢†åŸŸå¤šæ™ºèƒ½ä½“è¯„ä»·çš„ç©ºç™½ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11025v2",
      "published_date": "2025-11-14 07:23:05 UTC",
      "updated_date": "2025-11-22 16:13:06 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:22:28.039145+00:00"
    },
    {
      "arxiv_id": "2511.11020v1",
      "title": "Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis",
      "title_zh": "åŒ»ç–—äººå·¥æ™ºèƒ½æ¶æ„ä¸­çš„æ•°æ®æŠ•æ¯’è„†å¼±æ€§ï¼šå®‰å…¨å¨èƒåˆ†æ",
      "authors": [
        "Farhad Abtahi",
        "Fernando Seoane",
        "IvÃ¡n Pau",
        "Mario Vega-Barbas"
      ],
      "abstract": "Healthcare AI systems face major vulnerabilities to data poisoning that current defenses and regulations cannot adequately address. We analyzed eight attack scenarios in four categories: architectural attacks on convolutional neural networks, large language models, and reinforcement learning agents; infrastructure attacks exploiting federated learning and medical documentation systems; critical resource allocation attacks affecting organ transplantation and crisis triage; and supply chain attacks targeting commercial foundation models. Our findings indicate that attackers with access to only 100-500 samples can compromise healthcare AI regardless of dataset size, often achieving over 60 percent success, with detection taking an estimated 6 to 12 months or sometimes not occurring at all. The distributed nature of healthcare infrastructure creates many entry points where insiders with routine access can launch attacks with limited technical skill. Privacy laws such as HIPAA and GDPR can unintentionally shield attackers by restricting the analyses needed for detection. Supply chain weaknesses allow a single compromised vendor to poison models across 50 to 200 institutions. The Medical Scribe Sybil scenario shows how coordinated fake patient visits can poison data through legitimate clinical workflows without requiring a system breach. Current regulations lack mandatory adversarial robustness testing, and federated learning can worsen risks by obscuring attribution. We recommend multilayer defenses including required adversarial testing, ensemble-based detection, privacy-preserving security mechanisms, and international coordination on AI security standards. We also question whether opaque black-box models are suitable for high-stakes clinical decisions, suggesting a shift toward interpretable systems with verifiable safety guarantees.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŒ»ç–—ä¿å¥ AI ç³»ç»Ÿåœ¨ Data Poisoningï¼ˆæ•°æ®æŠ•æ¯’ï¼‰æ–¹é¢çš„è„†å¼±æ€§è¿›è¡Œäº†å…¨é¢çš„å®‰å…¨å¨èƒåˆ†æï¼Œæ¶µç›–äº† CNNã€LLMã€RL æ™ºèƒ½ä½“åŠ Federated Learning ç­‰å¤šç§æ¶æ„ä¸åŸºç¡€è®¾æ–½ã€‚ç ”ç©¶é€šè¿‡åˆ†æå››å¤§ç±»å…±å…«ç§æ”»å‡»åœºæ™¯å‘ç°ï¼Œæ”»å‡»è€…ä»…éœ€ 100-500 ä¸ªæ ·æœ¬å³å¯åœ¨ä¸è€ƒè™‘æ•°æ®é›†è§„æ¨¡çš„æƒ…å†µä¸‹å®ç°è¶…è¿‡ 60% çš„æ”»å‡»æˆåŠŸç‡ï¼Œä¸”æ£€æµ‹å‘¨æœŸé€šå¸¸é•¿è¾¾ 6 è‡³ 12 ä¸ªæœˆã€‚ç ”ç©¶æŒ‡å‡ºï¼ŒHIPAA å’Œ GDPR ç­‰éšç§æ³•å¾‹åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½å› é™åˆ¶å®‰å…¨åˆ†æè€Œæ— æ„ä¸­æ©æŠ¤æ”»å‡»è€…ï¼Œè€Œä¾›åº”é“¾å¼±ç‚¹åˆ™å¯èƒ½å¯¼è‡´å•ä¸ªä¾›åº”å•†çš„æ¼æ´æ³¢åŠæ•°ç™¾å®¶æœºæ„ã€‚ç‰¹åˆ«æ˜¯ Medical Scribe Sybil åœºæ™¯å±•ç¤ºäº†å¦‚ä½•é€šè¿‡è™šå‡å°±è¯Šåœ¨ä¸çªç ´ç³»ç»Ÿå®‰å…¨æ€§çš„æƒ…å†µä¸‹æ±¡æŸ“åˆæ³•ä¸´åºŠå·¥ä½œæµã€‚ä¸ºæ­¤ï¼Œä½œè€…å»ºè®®å»ºç«‹åŒ…æ‹¬å¼ºåˆ¶æ€§ Adversarial Robustness Testingï¼ˆå¯¹æŠ—é²æ£’æ€§æµ‹è¯•ï¼‰ã€é›†æˆæ£€æµ‹å’Œå›½é™…å®‰å…¨æ ‡å‡†åè°ƒåœ¨å†…çš„å¤šå±‚é˜²å¾¡ä½“ç³»ã€‚è¯¥è®ºæ–‡æœ€åè´¨ç–‘äº†ä¸é€æ˜çš„é»‘ç›’æ¨¡å‹åœ¨å…³é”®ä¸´åºŠå†³ç­–ä¸­çš„é€‚ç”¨æ€§ï¼Œæå€¡å‘å…·æœ‰å¯éªŒè¯å®‰å…¨ä¿è¯çš„å¯è§£é‡Šç³»ç»Ÿè½¬å˜ã€‚",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "primary_category": "cs.CR",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11020v1",
      "published_date": "2025-11-14 07:16:16 UTC",
      "updated_date": "2025-11-14 07:16:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:22:23.333265+00:00"
    },
    {
      "arxiv_id": "2511.11018v1",
      "title": "Automata-Based Steering of Large Language Models for Diverse Structured Generation",
      "title_zh": "åŸºäºè‡ªåŠ¨æœºçš„å¤šæ ·åŒ–ç»“æ„åŒ–ç”Ÿæˆå¤§è¯­è¨€æ¨¡å‹å¼•å¯¼",
      "authors": [
        "Xiaokun Luan",
        "Zeming Wei",
        "Yihao Zhang",
        "Meng Sun"
      ],
      "abstract": "Large language models (LLMs) are increasingly tasked with generating structured outputs. While structured generation methods ensure validity, they often lack output diversity, a critical limitation that we confirm in our preliminary study. We propose a novel method to enhance diversity in automaton-based structured generation. Our approach utilizes automata traversal history to steer LLMs towards novel structural patterns. Evaluations show our method significantly improves structural and content diversity while maintaining comparable generation efficiency. Furthermore, we conduct a case study showcasing the effectiveness of our method in generating diverse test cases for testing open-source libraries.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨æ‰§è¡Œç»“æ„åŒ–ç”Ÿæˆä»»åŠ¡æ—¶æ™®éå­˜åœ¨çš„è¾“å‡ºå¤šæ ·æ€§ä¸è¶³é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºè‡ªåŠ¨æœºå¼•å¯¼ (Automata-Based Steering) çš„æ–°é¢–æ–¹æ³•ã€‚è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨è‡ªåŠ¨æœºéå†å†å² (Automata Traversal History) æ¥å®æ—¶å¹²é¢„ç”Ÿæˆè¿‡ç¨‹ï¼Œå¼•å¯¼ LLMs æ¢ç´¢æ›´ä¸ºä¸°å¯Œå’Œæ–°é¢–çš„ç»“æ„æ¨¡å¼ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ˜¾è‘—æå‡ç»“æ„å¤šæ ·æ€§ä¸å†…å®¹å¤šæ ·æ€§çš„åŒæ—¶ï¼Œä¾ç„¶ä¿æŒäº†ä¸ç°æœ‰æŠ€æœ¯ç›¸å½“çš„ç”Ÿæˆæ•ˆç‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿé€šè¿‡é’ˆå¯¹å¼€æºåº“ç”Ÿæˆå¤šæ ·åŒ–æµ‹è¯•ç”¨ä¾‹çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œè¿›ä¸€æ­¥è¯å®äº†è¯¥æ–¹æ³•åœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­çš„æœ‰æ•ˆæ€§ä¸å®ç”¨ä»·å€¼ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CR",
        "cs.LG",
        "cs.SE"
      ],
      "primary_category": "cs.CL",
      "comment": "ICFEM 2025 (Best Paper Award)",
      "pdf_url": "https://arxiv.org/pdf/2511.11018v1",
      "published_date": "2025-11-14 07:10:23 UTC",
      "updated_date": "2025-11-14 07:10:23 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:22:24.434220+00:00"
    },
    {
      "arxiv_id": "2511.11017v1",
      "title": "AI Agent-Driven Framework for Automated Product Knowledge Graph Construction in E-Commerce",
      "title_zh": "ç”µå­å•†åŠ¡ä¸­åŸºäºAIæ™ºèƒ½ä½“é©±åŠ¨çš„è‡ªåŠ¨åŒ–å•†å“çŸ¥è¯†å›¾è°±æ„å»ºæ¡†æ¶",
      "authors": [
        "Dimitar Peshevski",
        "Riste Stojanov",
        "Dimitar Trajanov"
      ],
      "abstract": "The rapid expansion of e-commerce platforms generates vast amounts of unstructured product data, creating significant challenges for information retrieval, recommendation systems, and data analytics. Knowledge Graphs (KGs) offer a structured, interpretable format to organize such data, yet constructing product-specific KGs remains a complex and manual process. This paper introduces a fully automated, AI agent-driven framework for constructing product knowledge graphs directly from unstructured product descriptions. Leveraging Large Language Models (LLMs), our method operates in three stages using dedicated agents: ontology creation and expansion, ontology refinement, and knowledge graph population. This agent-based approach ensures semantic coherence, scalability, and high-quality output without relying on predefined schemas or handcrafted extraction rules. We evaluate the system on a real-world dataset of air conditioner product descriptions, demonstrating strong performance in both ontology generation and KG population. The framework achieves over 97\\% property coverage and minimal redundancy, validating its effectiveness and practical applicability. Our work highlights the potential of LLMs to automate structured knowledge extraction in retail, providing a scalable path toward intelligent product data integration and utilization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç”µå•†å¹³å°éç»“æ„åŒ–äº§å“æ•°æ®æ¿€å¢å¸¦æ¥çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å®Œå…¨è‡ªåŠ¨åŒ–ã€ç”±AIæ™ºèƒ½ä½“(AI Agents)é©±åŠ¨çš„æ¡†æ¶ï¼Œç”¨äºç›´æ¥ä»äº§å“æè¿°ä¸­æ„å»ºäº§å“çŸ¥è¯†å›¾è°±(Knowledge Graphs)ã€‚è¯¥æ¡†æ¶åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹(LLMs)ï¼Œé€šè¿‡æœ¬ä½“åˆ›å»ºä¸æ‰©å±•(ontology creation and expansion)ã€æœ¬ä½“ç²¾ç‚¼(ontology refinement)ä»¥åŠçŸ¥è¯†å›¾è°±å¡«å……(knowledge graph population)ä¸‰ä¸ªé˜¶æ®µçš„ä¸“é—¨æ™ºèƒ½ä½“ååŒè¿è¡Œã€‚è¿™ç§æ–¹æ³•åœ¨ä¸ä¾èµ–é¢„å®šä¹‰æ¨¡å¼(predefined schemas)æˆ–æ‰‹å·¥è§„åˆ™çš„æƒ…å†µä¸‹ï¼Œç¡®ä¿äº†è¾“å‡ºçš„é«˜è´¨é‡ã€è¯­ä¹‰ä¸€è‡´æ€§ä¸å¯æ‰©å±•æ€§ã€‚å®éªŒåœ¨ç©ºè°ƒäº§å“çš„çœŸå®æ•°æ®é›†ä¸Šè¿›è¡Œï¼Œç»“æœæ˜¾ç¤ºè¯¥æ¡†æ¶å®ç°äº†è¶…è¿‡97%çš„å±æ€§è¦†ç›–ç‡(property coverage)ä¸”å†—ä½™åº¦æä½ï¼ŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚è¯¥å·¥ä½œçªæ˜¾äº†LLMsåœ¨é›¶å”®é¢†åŸŸè‡ªåŠ¨åŒ–ç»“æ„åŒ–çŸ¥è¯†æå–çš„æ½œåŠ›ï¼Œä¸ºæ™ºèƒ½äº§å“æ•°æ®çš„é›†æˆä¸åˆ©ç”¨æä¾›äº†ä¸€æ¡é«˜æ•ˆè·¯å¾„ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Proceedings of the 1st GOBLIN Workshop on Knowledge Graph Technologies",
      "pdf_url": "https://arxiv.org/pdf/2511.11017v1",
      "published_date": "2025-11-14 07:09:13 UTC",
      "updated_date": "2025-11-14 07:09:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:22:28.238703+00:00"
    },
    {
      "arxiv_id": "2511.11007v1",
      "title": "VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models",
      "title_zh": "VisMemï¼šæ½œè§†è§‰è®°å¿†é‡Šæ”¾è§†è§‰è¯­è¨€æ¨¡å‹æ½œèƒ½",
      "authors": [
        "Xinlei Yu",
        "Chengming Xu",
        "Guibin Zhang",
        "Zhangquan Chen",
        "Yudong Zhang",
        "Yongbo He",
        "Peng-Tao Jiang",
        "Jiangning Zhang",
        "Xiaobin Hu",
        "Shuicheng Yan"
      ],
      "abstract": "Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a \"visual processing bottleneck\": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.",
      "tldr_zh": "é’ˆå¯¹è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)åœ¨é•¿æ–‡æœ¬ç”Ÿæˆä¸­å®¹æ˜“ä¸¢å¤±è§†è§‰è¯æ®å¹¶è¡¨ç°å‡ºè§†è§‰å¤„ç†ç“¶é¢ˆ(visual processing bottleneck)çš„é—®é¢˜ï¼Œè¯¥ç ”ç©¶å€Ÿé‰´äººç±»è®¤çŸ¥è®°å¿†ç†è®ºæå‡ºäº†VisMemæ¡†æ¶ã€‚è¯¥æ¡†æ¶ä¸ºæ¨¡å‹è£…å¤‡äº†åŠ¨æ€æ½œç©ºé—´è§†è§‰è®°å¿†(latent vision memories)ï¼ŒåŒ…å«ç”¨äºç»†ç²’åº¦æ„ŸçŸ¥ä¿ç•™çš„çŸ­æœŸæ¨¡å—(short-term module)å’Œç”¨äºæŠ½è±¡è¯­ä¹‰å·©å›ºçš„é•¿æœŸæ¨¡å—(long-term module)ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè¿™äº›è®°å¿†æ¨¡å—ä¼šè¢«æ— ç¼è°ƒç”¨ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿåœ¨æ€è€ƒä¸ç”Ÿæˆè¿‡ç¨‹ä¸­åŒæ—¶ä¿æŒæ„ŸçŸ¥ä¿çœŸåº¦(perceptual fidelity)å’Œè¯­ä¹‰ä¸€è‡´æ€§(semantic consistency)ã€‚å¹¿æ³›çš„å®éªŒè¯æ˜ï¼ŒVisMemåœ¨ç†è§£ã€æ¨ç†å’Œç”Ÿæˆç­‰å¤šæ ·åŒ–ä»»åŠ¡ä¸­ç›¸è¾ƒäºåŸå§‹æ¨¡å‹å®ç°äº†11.8%çš„å¹³å‡æ€§èƒ½æå‡ï¼Œè¡¨ç°ä¼˜äºç°æœ‰çš„åŒç±»æ–¹æ³•ã€‚è¯¥ç ”ç©¶ä¸ºæ½œç©ºé—´è®°å¿†å¢å¼º(latent-space memory enhancement)å»ºç«‹äº†æ–°çš„èŒƒå¼ï¼Œå¹¶å…¬å¼€äº†æºä»£ç ä»¥ä¾›è¿›ä¸€æ­¥ç ”ç©¶ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.11007v1",
      "published_date": "2025-11-14 06:51:34 UTC",
      "updated_date": "2025-11-14 06:51:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:22:33.643645+00:00"
    },
    {
      "arxiv_id": "2511.11006v1",
      "title": "MSMT-FN: Multi-segment Multi-task Fusion Network for Marketing Audio Classification",
      "title_zh": "MSMT-FNï¼šé¢å‘è¥é”€éŸ³é¢‘åˆ†ç±»çš„å¤šç‰‡æ®µå¤šä»»åŠ¡èåˆç½‘ç»œ",
      "authors": [
        "HongYu Liu",
        "Ruijie Wan",
        "Yueju Han",
        "Junxin Li",
        "Liuxing Lu",
        "Chao He",
        "Lihua Cai"
      ],
      "abstract": "Audio classification plays an essential role in sentiment analysis and emotion recognition, especially for analyzing customer attitudes in marketing phone calls. Efficiently categorizing customer purchasing propensity from large volumes of audio data remains challenging. In this work, we propose a novel Multi-Segment Multi-Task Fusion Network (MSMT-FN) that is uniquely designed for addressing this business demand. Evaluations conducted on our proprietary MarketCalls dataset, as well as established benchmarks (CMU-MOSI, CMU-MOSEI, and MELD), show MSMT-FN consistently outperforms or matches state-of-the-art methods. Additionally, our newly curated MarketCalls dataset will be available upon request, and the code base is made accessible at GitHub Repository MSMT-FN, to facilitate further research and advancements in audio classification domain.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è¥é”€ç”µè¯ä¸­å®¢æˆ·è´­ä¹°å€¾å‘(purchasing propensity)çš„é«˜æ•ˆåˆ†ç±»éš¾é¢˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º MSMT-FN çš„å¤šæ®µå¤šä»»åŠ¡èåˆç½‘ç»œ(Multi-segment Multi-task Fusion Network)ã€‚è¯¥æ¡†æ¶ä¸“é—¨è®¾è®¡ç”¨äºéŸ³é¢‘åˆ†ç±»(audio classification)ï¼Œç‰¹åˆ«æ˜¯åœ¨æƒ…æ„Ÿåˆ†æ(sentiment analysis)å’Œæƒ…ç»ªè¯†åˆ«(emotion recognition)é¢†åŸŸï¼Œæ—¨åœ¨ä»æµ·é‡éŸ³é¢‘æ•°æ®ä¸­ç²¾å‡†åˆ†æå®¢æˆ·æ€åº¦ã€‚MSMT-FN é€šè¿‡ç‹¬ç‰¹çš„ç½‘ç»œæ¶æ„åº”å¯¹ä¸šåŠ¡éœ€æ±‚ï¼Œæœ‰æ•ˆæå‡äº†åˆ†ç±»æ•ˆç‡ä¸å‡†ç¡®æ€§ã€‚åœ¨ä¸“æœ‰çš„ MarketCalls æ•°æ®é›†ä»¥åŠ CMU-MOSIã€CMU-MOSEI å’Œ MELD ç­‰å…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹æ€§èƒ½ä¸€è‡´ä¼˜äºæˆ–ç­‰åŒäºå½“å‰çš„å…ˆè¿›æ–¹æ³•ã€‚æ­¤å¤–ï¼Œç ”ç©¶å›¢é˜Ÿè¿˜å…¬å¼€äº†ä»£ç åº“å¹¶æä¾› MarketCalls æ•°æ®é›†è®¿é—®ï¼Œä¸ºéŸ³é¢‘åˆ†ç±»é¢†åŸŸçš„è¿›ä¸€æ­¥å‘å±•åšå‡ºäº†è´¡çŒ®ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "Accepted at The 21st International Conference on Advanced Data Mining and Applications (ADMA 2025). In book: Advanced Data Mining and Applications (pp.306-320)",
      "pdf_url": "https://arxiv.org/pdf/2511.11006v1",
      "published_date": "2025-11-14 06:51:30 UTC",
      "updated_date": "2025-11-14 06:51:30 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:22:45.040900+00:00"
    },
    {
      "arxiv_id": "2601.08833v1",
      "title": "Revisiting Disaggregated Large Language Model Serving for Performance and Energy Implications",
      "title_zh": "å†æ¢å¤§è¯­è¨€æ¨¡å‹åˆ†ç¦»å¼æ¨ç†ï¼šæ€§èƒ½ä¸èƒ½æ•ˆå½±å“ç ”ç©¶",
      "authors": [
        "Jiaxi Li",
        "Yue Zhu",
        "Eun Kyung Lee",
        "Klara Nahrstedt"
      ],
      "abstract": "Different from traditional Large Language Model (LLM) serving that colocates the prefill and decode stages on the same GPU, disaggregated serving dedicates distinct GPUs to prefill and decode workload. Once the prefill GPU completes its task, the KV cache must be transferred to the decode GPU. While existing works have proposed various KV cache transfer paths across different memory and storage tiers, there remains a lack of systematic benchmarking that compares their performance and energy efficiency. Meanwhile, although optimization techniques such as KV cache reuse and frequency scaling have been utilized for disaggregated serving, their performance and energy implications have not been rigorously benchmarked. In this paper, we fill this research gap by re-evaluating prefill-decode disaggregation under different KV transfer mediums and optimization strategies. Specifically, we include a new colocated serving baseline and evaluate disaggregated setups under different KV cache transfer paths. Through GPU profiling using dynamic voltage and frequency scaling (DVFS), we identify and compare the performance-energy Pareto frontiers across all setups to evaluate the potential energy savings enabled by disaggregation. Our results show that performance benefits from prefill-decode disaggregation are not guaranteed and depend on the request load and KV transfer mediums. In addition, stage-wise independent frequency scaling enabled by disaggregation does not lead to energy saving due to inherently higher energy consumption of disaggregated serving.",
      "tldr_zh": "è¯¥ç ”ç©¶é‡æ–°è¯„ä¼°äº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ (LLM) æ¨ç†ä¸­é¢„å¡«å…… (prefill) å’Œè§£ç  (decode) é˜¶æ®µè§£è€¦ (disaggregated) æœåŠ¡çš„æ€§èƒ½å’Œèƒ½æ•ˆè¡¨ç°ã€‚é€šè¿‡å¯¹ä¸åŒ KV cache ä¼ è¾“è·¯å¾„ä»¥åŠ KV cache é‡ç”¨å’Œé¢‘ç‡ç¼©æ”¾ç­‰ä¼˜åŒ–ç­–ç•¥è¿›è¡Œç³»ç»Ÿæ€§åŸºå‡†æµ‹è¯•ï¼Œç ”ç©¶è€…å¡«è¡¥äº†è¯¥é¢†åŸŸåœ¨æ€§èƒ½ä¸èƒ½æºæƒè¡¡åˆ†æä¸Šçš„ç©ºç™½ã€‚å®éªŒå¯¹æ¯”äº†ä¼ ç»Ÿçš„å…±ç½® (colocated) æœåŠ¡åŸºå‡†ä¸å¤šç§è§£è€¦é…ç½®ï¼Œå¹¶åˆ©ç”¨åŠ¨æ€ç”µå‹é¢‘ç‡è°ƒæ•´ (DVFS) æŠ€æœ¯åˆ†æäº†ä¸åŒè®¾ç½®ä¸‹çš„æ€§èƒ½-èƒ½æºå¸•ç´¯æ‰˜å‰æ²¿ (Pareto frontiers)ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè§£è€¦æœåŠ¡å¸¦æ¥çš„æ€§èƒ½æ”¶ç›Šå¹¶ä¸å›ºå®šï¼Œè€Œæ˜¯é«˜åº¦ä¾èµ–äºè¯·æ±‚è´Ÿè½½å’Œä¼ è¾“ä»‹è´¨ã€‚æ­¤å¤–ï¼Œå°½ç®¡è§£è€¦å…è®¸å„é˜¶æ®µç‹¬ç«‹è¿›è¡Œé¢‘ç‡ç¼©æ”¾ï¼Œä½†ç”±äºè§£è€¦æ¨¡å¼æœ¬èº«å›ºæœ‰çš„é«˜èƒ½è€—ï¼Œè¯¥æŠ€æœ¯å¹¶ä¸èƒ½å®ç°é¢„æœŸçš„èŠ‚èƒ½æ•ˆæœï¼Œä¸ºæœªæ¥ä¼˜åŒ– LLM æ¨ç†æ¶æ„æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.PF",
        "cs.AI",
        "cs.AR",
        "cs.DC"
      ],
      "primary_category": "cs.PF",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2601.08833v1",
      "published_date": "2025-11-14 06:42:27 UTC",
      "updated_date": "2025-11-14 06:42:27 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:22:54.037047+00:00"
    },
    {
      "arxiv_id": "2511.11000v2",
      "title": "DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition",
      "title_zh": "DialogGraph-LLMï¼šé¢å‘ç«¯åˆ°ç«¯è¯­éŸ³å¯¹è¯æ„å›¾è¯†åˆ«çš„å›¾æ„ŸçŸ¥å¤§è¯­è¨€æ¨¡å‹",
      "authors": [
        "HongYu Liu",
        "Junxin Li",
        "Changxi Guo",
        "Hao Chen",
        "Yaqian Huang",
        "Yifu Guo",
        "Huan Yang",
        "Lihua Cai"
      ],
      "abstract": "Recognizing speaker intent in long audio dialogues among speakers has a wide range of applications, but is a non-trivial AI task due to complex inter-dependencies in speaker utterances and scarce annotated data. To address these challenges, an end-to-end framework, namely DialogGraph-LLM, is proposed in the current work. DialogGraph-LLM combines a novel Multi-Relational Dialogue Attention Network (MR-DAN) architecture with multimodal foundation models (e.g., Qwen2.5-Omni-7B) for direct acoustic-to-intent inference. An adaptive semi-supervised learning strategy is designed using LLM with a confidence-aware pseudo-label generation mechanism based on dual-threshold filtering using both global and class confidences, and an entropy-based sample selection process that prioritizes high-information unlabeled instances. Extensive evaluations on the proprietary MarketCalls corpus and the publicly available MIntRec 2.0 benchmark demonstrate DialogGraph-LLM's superiority over strong audio and text-driven baselines. The framework demonstrates strong performance and efficiency in intent recognition in real world scenario audio dialogues, proving its practical value for audio-rich domains with limited supervision. Our code is available at https://github.com/david188888/DialogGraph-LLM.",
      "tldr_zh": "é’ˆå¯¹å¤šå‘è¨€è€…é•¿éŸ³é¢‘å¯¹è¯ä¸­ç”±äºè¯è¯­é—´å¤æ‚çš„ç›¸äº’ä¾èµ–å…³ç³»å’Œæ ‡æ³¨æ•°æ®ç¨€ç¼ºå¯¼è‡´çš„æ„å›¾è¯†åˆ«éš¾é¢˜ï¼Œè¯¥ç ”ç©¶æå‡ºäº†åä¸º DialogGraph-LLM çš„ç«¯åˆ°ç«¯æ¡†æ¶ã€‚è¯¥æ¡†æ¶é‡‡ç”¨åˆ›æ–°çš„å¤šå…³ç³»å¯¹è¯æ³¨æ„åŠ›ç½‘ç»œ (Multi-Relational Dialogue Attention Network, MR-DAN) æ¶æ„ï¼Œå¹¶ç»“åˆ Qwen2.5-Omni-7B ç­‰å¤šæ¨¡æ€åŸºåº§æ¨¡å‹ï¼Œå®ç°äº†ç›´æ¥ä»éŸ³é¢‘åˆ°æ„å›¾ (acoustic-to-intent) çš„é«˜æ•ˆæ¨ç†ã€‚ä¸ºäº†åœ¨æœ‰é™ç›‘ç£ä¸‹æå‡æ¨¡å‹æ€§èƒ½ï¼Œç ”ç©¶è®¾è®¡äº†ä¸€ç§è‡ªé€‚åº”åŠç›‘ç£å­¦ä¹  (semi-supervised learning) ç­–ç•¥ï¼Œåˆ©ç”¨ç½®ä¿¡åº¦æ„ŸçŸ¥çš„ä¼ªæ ‡ç­¾ç”Ÿæˆæœºåˆ¶å’ŒåŸºäºç†µçš„æ ·æœ¬é€‰æ‹©è¿‡ç¨‹æ¥æå–é«˜ä¿¡æ¯é‡çš„æœªæ ‡æ³¨å®ä¾‹ã€‚åœ¨ MarketCalls è¯­æ–™åº“å’Œ MIntRec 2.0 åŸºå‡†æµ‹è¯•ä¸Šçš„å¹¿æ³›è¯„ä¼°è¯æ˜ï¼ŒDialogGraph-LLM çš„è¡¨ç°æ˜¾è‘—ä¼˜äºç°æœ‰çš„éŸ³é¢‘åŠæ–‡æœ¬é©±åŠ¨åŸºå‡†æ¨¡å‹ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶åœ¨çœŸå®éŸ³é¢‘å¯¹è¯åœºæ™¯ä¸­å…·æœ‰æå¼ºçš„å®ç”¨ä»·å€¼å’Œæ•ˆç‡ï¼Œä¸ºç¼ºä¹ç›‘ç£æ•°æ®çš„éŸ³é¢‘å¯†é›†å‹é¢†åŸŸæä¾›äº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "primary_category": "cs.SD",
      "comment": "8 pages, 2 figures. To appear in: Proceedings of the 28th European Conference on Artificial Intelligence (ECAI 2025), Frontiers in Artificial Intelligence and Applications, Vol. 413. DOI: 10.3233/FAIA251182",
      "pdf_url": "https://arxiv.org/pdf/2511.11000v2",
      "published_date": "2025-11-14 06:42:04 UTC",
      "updated_date": "2025-11-17 02:49:01 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:22:57.433328+00:00"
    },
    {
      "arxiv_id": "2511.13757v1",
      "title": "VitalBench: A Rigorous Multi-Center Benchmark for Long-Term Vital Sign Prediction in Intraoperative Care",
      "title_zh": "VitalBenchï¼šé¢å‘æœ¯ä¸­ç›‘æŠ¤é•¿æœŸç”Ÿå‘½ä½“å¾é¢„æµ‹çš„ä¸¥è°¨å¤šä¸­å¿ƒåŸºå‡†",
      "authors": [
        "Xiuding Cai",
        "Xueyao Wang",
        "Sen Wang",
        "Yaoyao Zhu",
        "Jiao Chen",
        "Yu Yao"
      ],
      "abstract": "Intraoperative monitoring and prediction of vital signs are critical for ensuring patient safety and improving surgical outcomes. Despite recent advances in deep learning models for medical time-series forecasting, several challenges persist, including the lack of standardized benchmarks, incomplete data, and limited cross-center validation. To address these challenges, we introduce VitalBench, a novel benchmark specifically designed for intraoperative vital sign prediction. VitalBench includes data from over 4,000 surgeries across two independent medical centers, offering three evaluation tracks: complete data, incomplete data, and cross-center generalization. This framework reflects the real-world complexities of clinical practice, minimizing reliance on extensive preprocessing and incorporating masked loss techniques for robust and unbiased model evaluation. By providing a standardized and unified platform for model development and comparison, VitalBench enables researchers to focus on architectural innovation while ensuring consistency in data handling. This work lays the foundation for advancing predictive models for intraoperative vital sign forecasting, ensuring that these models are not only accurate but also robust and adaptable across diverse clinical environments. Our code and data are available at https://github.com/XiudingCai/VitalBench.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† VitalBenchï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºæœ¯ä¸­ç”Ÿå‘½ä½“å¾é¢„æµ‹(intraoperative vital sign prediction)è®¾è®¡çš„ä¸¥è°¨å¤šä¸­å¿ƒåŸºå‡†æµ‹è¯•ã€‚è¯¥åŸºå‡†æ—¨åœ¨è§£å†³åŒ»ç–—æ—¶é—´åºåˆ—é¢„æµ‹ä¸­æ ‡å‡†åŸºå‡†ç¼ºå¤±ã€æ•°æ®ä¸å®Œæ•´åŠè·¨ä¸­å¿ƒéªŒè¯æœ‰é™ç­‰æŒ‘æˆ˜ï¼Œæ¶µç›–äº†æ¥è‡ªä¸¤ä¸ªç‹¬ç«‹åŒ»ç–—ä¸­å¿ƒçš„ 4,000 å¤šåœºæ‰‹æœ¯æ•°æ®ã€‚VitalBench æä¾›äº†å®Œæ•´æ•°æ®ã€ä¸å®Œæ•´æ•°æ®å’Œè·¨ä¸­å¿ƒæ³›åŒ–(cross-center generalization)ä¸‰æ¡è¯„ä¼°èµ›é“ï¼Œå……åˆ†åæ˜ äº†ä¸´åºŠå®è·µä¸­çš„çœŸå®å¤æ‚æ€§ã€‚é€šè¿‡é‡‡ç”¨æ©è”½æŸå¤±(masked loss)æŠ€æœ¯å¹¶å‡å°‘å¯¹é¢„å¤„ç†çš„è¿‡åº¦ä¾èµ–ï¼Œè¯¥æ¡†æ¶ç¡®ä¿äº†æ¨¡å‹è¯„ä¼°çš„é²æ£’æ€§ä¸æ— åæ€§ã€‚è¿™ä¸€å·¥ä½œä¸ºå¼€å‘å‡†ç¡®ã€ç¨³å¥ä¸”é€‚åº”æ€§å¼ºçš„æœ¯ä¸­é¢„æµ‹æ¨¡å‹å¥ å®šäº†åŸºç¡€ï¼Œå¹¶ä¸ºç ”ç©¶äººå‘˜æä¾›äº†æ ‡å‡†åŒ–çš„æ¨¡å‹å¼€å‘ä¸æ¯”è¾ƒå¹³å°ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted by IEEE Sensors Journal",
      "pdf_url": "https://arxiv.org/pdf/2511.13757v1",
      "published_date": "2025-11-14 06:33:39 UTC",
      "updated_date": "2025-11-14 06:33:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:22:58.738969+00:00"
    },
    {
      "arxiv_id": "2511.10985v1",
      "title": "When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets",
      "title_zh": "æ•°æ®å³ç®—æ³•ï¼šåå¥½ä¼˜åŒ–æ•°æ®é›†çš„ç³»ç»Ÿæ€§ç ”ç©¶ä¸ç²¾é€‰",
      "authors": [
        "Aladin Djuhera",
        "Farhan Ahmed",
        "Swanand Ravindra Kadhe",
        "Syed Zawad",
        "Heiko Ludwig",
        "Holger Boche"
      ],
      "abstract": "Aligning large language models (LLMs) is a central objective of post-training, often achieved through reward modeling and reinforcement learning methods. Among these, direct preference optimization (DPO) has emerged as a widely adopted technique that fine-tunes LLMs on preferred completions over less favorable ones. While most frontier LLMs do not disclose their curated preference pairs, the broader LLM community has released several open-source DPO datasets, including TuluDPO, ORPO, UltraFeedback, HelpSteer, and Code-Preference-Pairs. However, systematic comparisons remain scarce, largely due to the high computational cost and the lack of rich quality annotations, making it difficult to understand how preferences were selected, which task types they span, and how well they reflect human judgment on a per-sample level. In this work, we present the first comprehensive, data-centric analysis of popular open-source DPO corpora. We leverage the Magpie framework to annotate each sample for task category, input quality, and preference reward, a reward-model-based signal that validates the preference order without relying on human annotations. This enables a scalable, fine-grained inspection of preference quality across datasets, revealing structural and qualitative discrepancies in reward margins. Building on these insights, we systematically curate a new DPO mixture, UltraMix, that draws selectively from all five corpora while removing noisy or redundant samples. UltraMix is 30% smaller than the best-performing individual dataset yet exceeds its performance across key benchmarks. We publicly release all annotations, metadata, and our curated mixture to facilitate future research in data-centric preference optimization.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLM)å¯¹é½ä¸­ç›´æ¥åå¥½ä¼˜åŒ–(DPO)æ•°æ®é›†ç¼ºä¹ç³»ç»Ÿæ€§æ¯”è¾ƒå’Œç²¾ç»†è´¨é‡è¯„ä¼°çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ•°æ®ä¸­å¿ƒ(Data-Centric)çš„åˆ†ææ–¹æ³•ã€‚ç ”ç©¶è€…åˆ©ç”¨Magpieæ¡†æ¶å¯¹åŒ…æ‹¬TuluDPOã€ORPOã€UltraFeedbackç­‰åœ¨å†…çš„äº”ä¸ªä¸»æµå¼€æºDPOè¯­æ–™åº“è¿›è¡Œäº†ä»»åŠ¡ç±»åˆ«ã€è¾“å…¥è´¨é‡å’Œåå¥½å¥–åŠ±(Preference Reward)çš„å…¨é¢æ ‡æ³¨ï¼Œæ­ç¤ºäº†å„æ•°æ®é›†åœ¨åå¥½æ’åºéªŒè¯ä¸­çš„è´¨é‡å·®å¼‚ã€‚åŸºäºè¿™äº›æ·±å…¥æ´å¯Ÿï¼Œç ”ç©¶å›¢é˜Ÿç³»ç»Ÿæ€§åœ°ç­›é€‰å¹¶æ„å»ºäº†åä¸ºUltraMixçš„æ–°æ•°æ®é›†ï¼Œé€šè¿‡ç§»é™¤å™ªå£°å’Œå†—ä½™æ ·æœ¬å®ç°äº†æ•°æ®çš„é«˜æ•ˆåˆ©ç”¨ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒUltraMixåœ¨è§„æ¨¡ç¼©å‡30%çš„æƒ…å†µä¸‹ï¼Œæ€§èƒ½è¡¨ç°ä¾ç„¶è¶…è¶Šäº†æ­¤å‰è¡¨ç°æœ€ä¼˜çš„å•ä½“æ•°æ®é›†ã€‚è¯¥å·¥ä½œä¸ä»…å‘å¸ƒäº†é«˜è´¨é‡çš„æ•°æ®æ··åˆåŒ…ï¼Œè¿˜å…¬å¼€äº†å®Œæ•´çš„æ ‡æ³¨å…ƒæ•°æ®ï¼Œä¸ºæœªæ¥çš„åå¥½ä¼˜åŒ–ç ”ç©¶å¥ å®šäº†åšå®çš„æ•°æ®åŸºç¡€ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10985v1",
      "published_date": "2025-11-14 06:12:16 UTC",
      "updated_date": "2025-11-14 06:12:16 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:22:59.645477+00:00"
    },
    {
      "arxiv_id": "2511.17568v1",
      "title": "Enhancing Robustness of Offline Reinforcement Learning Under Data Corruption via Sharpness-Aware Minimization",
      "title_zh": "åˆ©ç”¨é”åº¦æ„ŸçŸ¥æœ€å°åŒ–å¢å¼ºç¦»çº¿å¼ºåŒ–å­¦ä¹ åœ¨æ•°æ®æŸåä¸‹çš„é²æ£’æ€§",
      "authors": [
        "Le Xu",
        "Jiayu Chen"
      ],
      "abstract": "Offline reinforcement learning (RL) is vulnerable to real-world data corruption, with even robust algorithms failing under challenging observation and mixture corruptions. We posit this failure stems from data corruption creating sharp minima in the loss landscape, leading to poor generalization. To address this, we are the first to apply Sharpness-Aware Minimization (SAM) as a general-purpose, plug-and-play optimizer for offline RL. SAM seeks flatter minima, guiding models to more robust parameter regions. We integrate SAM into strong baselines for data corruption: IQL, a top-performing offline RL algorithm in this setting, and RIQL, an algorithm designed specifically for data-corruption robustness. We evaluate them on D4RL benchmarks with both random and adversarial corruption. Our SAM-enhanced methods consistently and significantly outperform the original baselines. Visualizations of the reward surface confirm that SAM finds smoother solutions, providing strong evidence for its effectiveness in improving the robustness of offline RL agents.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç¦»çº¿å¼ºåŒ–å­¦ä¹  (Offline Reinforcement Learning) åœ¨é¢å¯¹æ•°æ®æ±¡æŸ“ (Data Corruption) æ—¶è¡¨ç°è„†å¼±çš„é—®é¢˜ï¼ŒæŒ‡å‡ºå…¶å¤±è´¥æ ¹æºåœ¨äºæ±¡æŸ“å¯¼è‡´æŸå¤±æ™¯è§‚ä¸­å‡ºç°äº†æ³›åŒ–æ€§è¾ƒå·®çš„å°–é”æœ€å°å€¼ (Sharp Minima)ã€‚ä¸ºæ­¤ï¼Œè®ºæ–‡é¦–æ¬¡å°† Sharpness-Aware Minimization (SAM) å¼•å…¥è¯¥é¢†åŸŸï¼Œä½œä¸ºä¸€ç§é€šç”¨çš„æ’ä»¶å¼ä¼˜åŒ–å™¨ï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹å¯»æ‰¾æ›´å¹³å¦çš„æœ€å°å€¼æ¥æå‡å‚æ•°åŒºåŸŸçš„é²æ£’æ€§ã€‚ç ”ç©¶äººå‘˜å°† SAM é›†æˆåˆ° IQL å’Œ RIQL ç­‰å¼ºåŸºçº¿ç®—æ³•ä¸­ï¼Œå¹¶åœ¨åŒ…å«éšæœºå’Œå¯¹æŠ—æ€§æ±¡æŸ“çš„ D4RL åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œå¢å¼ºåçš„æ–¹æ³•æ˜¾è‘—ä¼˜äºåŸå§‹åŸºçº¿ï¼Œä¸”å¥–åŠ±è¡¨é¢çš„å¯è§†åŒ–è¯å®äº† SAM èƒ½å¤Ÿæ‰¾åˆ°æ›´å¹³æ»‘çš„è§£ã€‚è¯¥é¡¹å·¥ä½œä¸ºæé«˜å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“åœ¨ç°å®ä¸–ç•Œå—æŸæ•°æ®ç¯å¢ƒä¸‹çš„ç¨³å¥æ€§æä¾›äº†å¼ºæœ‰åŠ›çš„è¯æ®ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Accepted as an Oral Presentation at the AAAI 2026 Student Abstract and Poster Program (SAPP)",
      "pdf_url": "https://arxiv.org/pdf/2511.17568v1",
      "published_date": "2025-11-14 06:11:13 UTC",
      "updated_date": "2025-11-14 06:11:13 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:23:02.638308+00:00"
    },
    {
      "arxiv_id": "2511.10984v2",
      "title": "DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains",
      "title_zh": "DiscoXï¼šä¸“ä¸šé¢†åŸŸç¯‡ç« çº§ç¿»è¯‘ä»»åŠ¡è¯„æµ‹åŸºå‡†",
      "authors": [
        "Xiying Zhao",
        "Zhoufutu Wen",
        "Zhixuan Chen",
        "Jingzhe Ding",
        "Jianpeng Jiao",
        "Shuai Li",
        "Xi Li",
        "Danni Liang",
        "Shengda Long",
        "Qianqian Liu",
        "Xianbo Wu",
        "Hongwan Gao",
        "Xiang Gao",
        "Liang Hu",
        "Jiashuo Liu",
        "Mengyun Liu",
        "Weiran Shi",
        "Chenghao Yang",
        "Qianyu Yang",
        "Xuanliang Zhang",
        "Ge Zhang",
        "Wenhao Huang",
        "Yuwen Tang"
      ],
      "abstract": "The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† DiscoXï¼Œè¿™æ˜¯ä¸€ä¸ªé’ˆå¯¹ä¸“å®¶é¢†åŸŸè¯­ç¯‡çº§ï¼ˆdiscourse-levelï¼‰å’Œä¸“å®¶çº§ï¼ˆexpert-levelï¼‰ä¸­è‹±ç¿»è¯‘çš„æ–°åŸºå‡†ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰è¯„ä¼°æ–¹æ³•ä¸»è¦å…³æ³¨ç‰‡æ®µçº§ï¼ˆsegment-levelï¼‰è€Œå¿½è§†è¯­ç¯‡è¿è´¯æ€§ä¸æœ¯è¯­ç²¾å‡†åº¦çš„é—®é¢˜ã€‚DiscoX åŒ…å«æ¥è‡ª 7 ä¸ªä¸åŒé¢†åŸŸçš„ 200 ç¯‡ä¸“ä¸šæ–‡æœ¬ï¼Œå•ç¯‡å¹³å‡é•¿åº¦è¶…è¿‡ 1700 ä¸ªæ ‡è®°ã€‚ä¸ºäº†é…åˆè¯¥åŸºå‡†ï¼Œç ”ç©¶è€…è¿˜å¼€å‘äº†æ— éœ€å‚è€ƒï¼ˆreference-freeï¼‰çš„è¯„ä¼°ç³»ç»Ÿ Metric-Sï¼Œèƒ½å¤Ÿä»å‡†ç¡®æ€§ã€æµç•…æ€§å’Œé€‚å½“æ€§ç»´åº¦æä¾›ç»†ç²’åº¦è‡ªåŠ¨è¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒMetric-S ä¸äººç±»åˆ¤æ–­é«˜åº¦ä¸€è‡´ï¼Œä¸”å½“å‰æœ€å…ˆè¿›çš„ LLMs åœ¨è¿™äº›ä»»åŠ¡ä¸Šçš„è¡¨ç°ä»æ˜¾è‘—è½åäºäººç±»ä¸“å®¶ã€‚è¯¥åŸºå‡†å’Œè¯„ä¼°ç³»ç»Ÿä¸ºè¯­ç¯‡çº§ç¿»è¯‘æä¾›äº†æ›´ä¸¥è°¨çš„è¯„ä¼°æ¡†æ¶ï¼Œå°†æœ‰åŠ›æ¨åŠ¨æœªæ¥å¤§è¯­è¨€æ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸç¿»è¯‘èƒ½åŠ›çš„æå‡ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "36 pages",
      "pdf_url": "https://arxiv.org/pdf/2511.10984v2",
      "published_date": "2025-11-14 06:09:37 UTC",
      "updated_date": "2025-12-17 16:24:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:23:14.641649+00:00"
    },
    {
      "arxiv_id": "2511.13756v1",
      "title": "Multi-Horizon Time Series Forecasting of non-parametric CDFs with Deep Lattice Networks",
      "title_zh": "åŸºäºæ·±åº¦æ ¼ç‚¹ç½‘ç»œçš„éå‚æ•°ç´¯ç§¯åˆ†å¸ƒå‡½æ•°å¤šæ­¥æ—¶é—´åºåˆ—é¢„æµ‹",
      "authors": [
        "Niklas Erdmann",
        "Lars Bentsen",
        "Roy Stenbro",
        "Heine Nygard Riise",
        "Narada Dilp Warakagoda",
        "Paal E. Engelstad"
      ],
      "abstract": "Probabilistic forecasting is not only a way to add more information to a prediction of the future, but it also builds on weaknesses in point prediction. Sudden changes in a time series can still be captured by a cumulative distribution function (CDF), while a point prediction is likely to miss it entirely. The modeling of CDFs within forecasts has historically been limited to parametric approaches, but due to recent advances, this no longer has to be the case. We aim to advance the fields of probabilistic forecasting and monotonic networks by connecting them and propose an approach that permits the forecasting of implicit, complete, and nonparametric CDFs. For this purpose, we propose an adaptation to deep lattice networks (DLN) for monotonically constrained simultaneous/implicit quantile regression in time series forecasting. Quantile regression usually produces quantile crossovers, which need to be prevented to achieve a legitimate CDF. By leveraging long short term memory units (LSTM) as the embedding layer, and spreading quantile inputs to all sub-lattices of a DLN with an extended output size, we can produce a multi-horizon forecast of an implicit CDF due to the monotonic constraintability of DLNs that prevent quantile crossovers. We compare and evaluate our approach's performance to relevant state of the art within the context of a highly relevant application of time series forecasting: Day-ahead, hourly forecasts of solar irradiance observations. Our experiments show that the adaptation of a DLN performs just as well or even better than an unconstrained approach. Further comparison of the adapted DLN against a scalable monotonic neural network shows that our approach performs better. With this adaptation of DLNs, we intend to create more interest and crossover investigations in techniques of monotonic neural networks and probabilistic forecasting.",
      "tldr_zh": "æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäº Deep Lattice Networks (DLN) çš„æ–°å‹æ–¹æ³•ï¼Œæ—¨åœ¨å®ç°éšå¼ã€å®Œæ•´ä¸”éå‚æ•°åŒ–çš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•° (CDF) çš„å¤šæ­¥æ—¶é—´åºåˆ—é¢„æµ‹ã€‚è¯¥ç ”ç©¶æŒ‡å‡ºæ¦‚ç‡é¢„æµ‹é€šè¿‡æ•æ‰æ—¶é—´åºåˆ—ä¸­çš„çªå˜å¼¥è¡¥äº†ç‚¹é¢„æµ‹ (Point prediction) çš„ä¸è¶³ï¼Œå¹¶é’ˆå¯¹ä¼ ç»Ÿåˆ†ä½æ•°å›å½’å®¹æ˜“å‡ºç°çš„åˆ†ä½æ•°äº¤å‰ (Quantile crossover) é—®é¢˜è¿›è¡Œäº†æ”¹è¿›ã€‚è¯¥æ–¹æ³•åˆ©ç”¨ Long Short Term Memory (LSTM) ä½œä¸ºåµŒå…¥å±‚ï¼Œå¹¶å°†åˆ†ä½æ•°è¾“å…¥åˆ†å¸ƒåˆ°å…·æœ‰æ‰©å±•è¾“å‡ºè§„æ¨¡çš„ DLN å­æ ¼ç½‘ä¸­ã€‚é€šè¿‡åˆ©ç”¨ DLN çš„å•è°ƒçº¦æŸç‰¹æ€§ (Monotonic constraintability)ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿç¡®ä¿åœ¨ç”Ÿæˆéšå¼ CDF æ—¶é˜²æ­¢åˆ†ä½æ•°äº¤å‰ï¼Œä»è€Œä¿è¯é¢„æµ‹ç»“æœçš„åˆæ³•æ€§ã€‚å®éªŒåœ¨æ—¥å‰é€æ—¶å¤ªé˜³è¾ç…§åº¦ (Solar irradiance) é¢„æµ‹ä»»åŠ¡ä¸­éªŒè¯äº†è¯¥æ¨¡å‹çš„æœ‰æ•ˆæ€§ã€‚ç»“æœæ˜¾ç¤ºï¼Œé€‚é…åçš„ DLN æ€§èƒ½ä¼˜äºç°æœ‰çš„å¯æ‰©å±•å•è°ƒç¥ç»ç½‘ç»œ (Monotonic neural network)ï¼Œä¸”åœ¨ä¸æ— çº¦æŸæ–¹æ³•çš„å¯¹æ¯”ä¸­è¡¨ç°å‡ºè‰²ï¼Œä¸ºå•è°ƒç¥ç»ç½‘ç»œåœ¨æ¦‚ç‡é¢„æµ‹é¢†åŸŸçš„åº”ç”¨æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13756v1",
      "published_date": "2025-11-14 06:06:34 UTC",
      "updated_date": "2025-11-14 06:06:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:23:17.833039+00:00"
    },
    {
      "arxiv_id": "2511.10983v1",
      "title": "Binary Verification for Zero-Shot Vision",
      "title_zh": "é›¶æ ·æœ¬è§†è§‰çš„äºŒå…ƒéªŒè¯",
      "authors": [
        "Jeffrey Liu",
        "Rongbin Hu"
      ],
      "abstract": "We propose a training-free, binary verification workflow for zero-shot vision with off-the-shelf VLMs. It comprises two steps: (i) quantization, which turns the open-ended query into a multiple-choice question (MCQ) with a small, explicit list of unambiguous candidates; and (ii) binarization, which asks one True/False question per candidate and resolves deterministically: if exactly one is True, select it; otherwise, revert to an MCQ over the remaining plausible candidates. We evaluate the workflow on referring expression grounding (REC), spatial reasoning (Spatial-Map, Spatial-Grid, Spatial-Maze), and BLINK-Jigsaw. Relative to answering open-ended queries directly, quantization to MCQ yields large gains, and True/False binarization provides a consistent additional boost. Across all tasks, the same workflow produces significant improvements, indicating generality. Our theory formalizes how open-ended vision queries can be quantized to MCQs and further binarized into True/False verifications, establishing a hardness ladder. A simple analysis explains why Boolean resolution boosts accuracy. Together, these components yield a simple and unified workflow that emphasizes inference-time design over task-specific training. It offers a practical, drop-in path to stronger zero-shot vision with today's VLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§é’ˆå¯¹é›¶æ ·æœ¬è§†è§‰(Zero-Shot Vision)çš„å…è®­ç»ƒäºŒè¿›åˆ¶éªŒè¯å·¥ä½œæµï¼Œæ—¨åœ¨ç›´æ¥åˆ©ç”¨ç°æœ‰çš„è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)æå‡æ¨ç†æ€§èƒ½ã€‚è¯¥å·¥ä½œæµç”±ä¸¤ä¸ªæ ¸å¿ƒæ­¥éª¤ç»„æˆï¼šé¦–å…ˆé€šè¿‡é‡åŒ–(Quantization)å°†å¼€æ”¾å¼æŸ¥è¯¢è½¬æ¢ä¸ºå…·æœ‰æ˜ç¡®å€™é€‰é€‰é¡¹çš„å¤šé€‰é¢˜(MCQ)ï¼Œéšåé€šè¿‡äºŒè¿›åˆ¶åŒ–(Binarization)å¯¹æ¯ä¸ªå€™é€‰é¡¹è¿›è¡ŒçœŸ/å‡(True/False)éªŒè¯å¹¶æ ¹æ®ç¡®å®šæ€§é€»è¾‘è§£æç»“æœã€‚å®éªŒåœ¨æŒ‡ç§°è¡¨è¾¾å®šä½(REC)ã€å¤šé¡¹ç©ºé—´æ¨ç†ä»»åŠ¡(Spatial-Map, Spatial-Grid, Spatial-Maze)ä»¥åŠBLINK-Jigsawä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚ç»“æœè¡¨æ˜ï¼Œé‡åŒ–æ­¥éª¤ç›¸æ¯”å¼€æ”¾å¼æŸ¥è¯¢å–å¾—äº†å·¨å¤§æ”¶ç›Šï¼Œè€ŒäºŒè¿›åˆ¶åŒ–éªŒè¯è¿›ä¸€æ­¥æä¾›äº†æŒç»­çš„æ€§èƒ½æå‡ã€‚è¯¥ç ”ç©¶é€šè¿‡ç†è®ºå½¢å¼åŒ–äº†è§†è§‰æŸ¥è¯¢çš„ç¡¬åº¦é˜¶æ¢¯ï¼Œå¹¶è¯æ˜äº†è¿™ç§ç»Ÿä¸€çš„å·¥ä½œæµåœ¨å¤šç§ä»»åŠ¡ä¸­çš„é€šç”¨æ€§ã€‚è¿™é¡¹å·¥ä½œå¼ºè°ƒäº†æ¨ç†ä¾§è®¾è®¡(Inference-time design)çš„é‡è¦æ€§ï¼Œä¸ºæå‡å½“ä»£è§†è§‰è¯­è¨€æ¨¡å‹çš„é›¶æ ·æœ¬èƒ½åŠ›æä¾›äº†ä¸€ç§ç®€å•ä¸”å®ç”¨çš„å³æ’å³ç”¨æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10983v1",
      "published_date": "2025-11-14 06:05:43 UTC",
      "updated_date": "2025-11-14 06:05:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:23:18.330856+00:00"
    },
    {
      "arxiv_id": "2511.10979v1",
      "title": "PAS: A Training-Free Stabilizer for Temporal Encoding in Video LLMs",
      "title_zh": "PASï¼šè§†é¢‘å¤§è¯­è¨€æ¨¡å‹æ—¶åºç¼–ç çš„æ— éœ€è®­ç»ƒç¨³å®šå™¨",
      "authors": [
        "Bowen Sun",
        "Yujun Cai",
        "Ming-Hsuan Yang",
        "Hang Wu",
        "Yiwei Wang"
      ],
      "abstract": "Video LLMs suffer from temporal inconsistency: small shifts in frame timing can flip attention and suppress relevant frames. We trace this instability to the common extension of Rotary Position Embeddings to video through multimodal RoPE. The induced inverse Fourier time kernel exhibits frame-scale ripples that multiply adjacent frames by different factors, which perturbs attention that should otherwise be governed by the raw query key inner product. We present Phase Aggregated Smoothing (PAS), a simple, training-free mechanism that applies small opposed phase offsets across heads and then aggregates their outputs. PAS preserves the per-head spectrum magnitude, while the aggregation effectively smooths the temporal kernel and reduces phase sensitivity without changing the positional encoding structure. Our analysis shows that the RoPE rotated logit can be approximated as a content dot product scaled by a time kernel; smoothing this kernel yields Lipschitz stability of attention to small temporal shifts; multi phase averaging attenuates high frequency ripples while preserving per-head spectra under Nyquist-valid sampling. Experiments on multiple video understanding benchmarks under matched token budgets show consistent improvements with negligible computational overhead. PAS provides a plug and play upgrade for robust temporal encoding in Video LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Video LLMs åœ¨æ—¶é—´ç¼–ç ä¸Šçš„ä¸ä¸€è‡´æ€§é—®é¢˜ï¼ŒæŒ‡å‡ºå¤šæ¨¡æ€ Rotary Position Embeddings (RoPE) æ‰©å±•åˆ°è§†é¢‘é¢†åŸŸæ—¶ï¼Œå…¶è¯±å‘çš„é€†å‚…é‡Œå¶æ—¶é—´æ ¸å­˜åœ¨å¸§çº§æ¶Ÿæ¼ªï¼Œå¯¼è‡´å¾®å°çš„å¸§åç§»ä¼šæ˜¾è‘—å¹²æ‰°æ³¨æ„åŠ›åˆ†å¸ƒã€‚ä¸ºæ­¤ï¼Œä½œè€…æå‡ºäº† Phase Aggregated Smoothing (PAS)ï¼Œè¿™æ˜¯ä¸€ç§æ— éœ€è®­ç»ƒçš„å³æ’å³ç”¨ç¨³å®šæœºåˆ¶ã€‚PAS é€šè¿‡åœ¨ä¸åŒæ³¨æ„åŠ›å¤´ä¹‹é—´åº”ç”¨å¾®å°çš„ç›¸åç›¸ä½åç§»å¹¶èšåˆè¾“å‡ºï¼Œåœ¨ä¸æ”¹å˜ä½ç½®ç¼–ç ç»“æ„çš„å‰æä¸‹æœ‰æ•ˆåœ°å¹³æ»‘äº†æ—¶é—´æ ¸å¹¶é™ä½äº†ç›¸ä½æ•æ„Ÿæ€§ã€‚ç†è®ºåˆ†æè¡¨æ˜ï¼Œè¿™ç§å¤šç›¸ä½å¹³å‡èƒ½åœ¨ä¿ç•™å•å¤´é¢‘è°±ç‰¹å¾çš„åŒæ—¶è¡°å‡é«˜é¢‘æ¶Ÿæ¼ªï¼Œä½¿æ³¨æ„åŠ›æœºåˆ¶å¯¹å¾®å°çš„æ—¶é—´åç§»å…·å¤‡ Lipschitz stabilityã€‚åœ¨å¤šä¸ªè§†é¢‘ç†è§£åŸºå‡†æµ‹è¯•ä¸­çš„å®éªŒç»“æœæ˜¾ç¤ºï¼ŒPAS åœ¨è®¡ç®—å¼€é”€å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡çš„æƒ…å†µä¸‹å®ç°äº†æ€§èƒ½çš„æŒç»­æå‡ã€‚è¯¥æ–¹æ³•ä¸ºå¢å¼º Video LLMs çš„æ—¶é—´ç¼–ç é²æ£’æ€§æä¾›äº†ä¸€ç§ç®€å•ä¸”é«˜æ•ˆçš„å‡çº§æ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "13 pages, 5 figures",
      "pdf_url": "https://arxiv.org/pdf/2511.10979v1",
      "published_date": "2025-11-14 05:56:47 UTC",
      "updated_date": "2025-11-14 05:56:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:23:17.146798+00:00"
    },
    {
      "arxiv_id": "2601.11527v1",
      "title": "Do LLMs Give Good Romantic Relationship Advice? A Study on User Satisfaction and Attitude Change",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹èƒ½å¦æä¾›ä¼˜è´¨çš„æ‹çˆ±å»ºè®®ï¼Ÿä¸€é¡¹å…³äºç”¨æˆ·æ»¡æ„åº¦ä¸æ€åº¦è½¬å˜çš„ç ”ç©¶",
      "authors": [
        "Niva Manchanda",
        "Akshata Kishore Moharir",
        "Isabel Michel",
        "Ratna Kandala"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being used to provide support and advice in personal domains such as romantic relationships, yet little is known about user perceptions of this type of advice. This study investigated how people evaluate advice on LLM-generated romantic relationships. Participants rated advice satisfaction, model reliability, and helpfulness, and completed pre- and post-measures of their general attitudes toward LLMs. Overall, the results showed participants' high satisfaction with LLM-generated advice. Greater satisfaction was, in turn, strongly and positively associated with their perceptions of the models' reliability and helpfulness. Importantly, participants' attitudes toward LLMs improved significantly after exposure to the advice, suggesting that supportive and contextually relevant advice can enhance users' trust and openness toward these AI systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)åœ¨æä¾›æµªæ¼«å…³ç³»å»ºè®®ç­‰ç§äººé¢†åŸŸæ”¯æŒæ—¶çš„ç”¨æˆ·æ„ŸçŸ¥å’Œæ»¡æ„åº¦ã€‚ç ”ç©¶é€šè¿‡è®©å‚ä¸è€…å¯¹LLMç”Ÿæˆçš„å»ºè®®è¿›è¡Œæ»¡æ„åº¦ã€æ¨¡å‹å¯é æ€§å’Œå®ç”¨æ€§è¯„åˆ†ï¼Œå¹¶å¯¹æ¯”äº†ç”¨æˆ·åœ¨æ¥å—å»ºè®®å‰åçš„æ•´ä½“æ€åº¦å˜åŒ–ã€‚ç»“æœæ˜¾ç¤ºï¼Œå‚ä¸è€…å¯¹LLMç”Ÿæˆçš„å»ºè®®è¡¨ç°å‡ºæé«˜çš„æ»¡æ„åº¦ï¼Œä¸”è¿™ç§æ»¡æ„åº¦ä¸ç”¨æˆ·å¯¹æ¨¡å‹å¯é æ€§å’Œå®ç”¨æ€§çš„æ­£å‘æ„ŸçŸ¥æ˜¾è‘—ç›¸å…³ã€‚å…³é”®å‘ç°æ˜¯ï¼Œå‚ä¸è€…åœ¨æ¥è§¦è¿™äº›å»ºè®®åå¯¹LLMsçš„æ€åº¦æ˜¾è‘—æ”¹å–„ã€‚è¿™è¡¨æ˜æä¾›å…·æœ‰æ”¯æŒæ€§å’Œä¸Šä¸‹æ–‡ç›¸å…³æ€§çš„å»ºè®®å¯ä»¥æœ‰æ•ˆå¢å¼ºç”¨æˆ·å¯¹è¿™äº›AIç³»ç»Ÿçš„ä¿¡ä»»åº¦å’Œå¼€æ”¾æ€åº¦ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI",
        "cs.CY"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted at the 39th Conference on Neural Information Processing Systems (NeurIPS 2025) First Workshop on LLM Persona Modeling",
      "pdf_url": "https://arxiv.org/pdf/2601.11527v1",
      "published_date": "2025-11-14 05:19:14 UTC",
      "updated_date": "2025-11-14 05:19:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:23:31.235429+00:00"
    },
    {
      "arxiv_id": "2511.10964v1",
      "title": "How Data Quality Affects Machine Learning Models for Credit Risk Assessment",
      "title_zh": "æ•°æ®è´¨é‡å¯¹ä¿¡ç”¨é£é™©è¯„ä¼°æœºå™¨å­¦ä¹ æ¨¡å‹çš„å½±å“",
      "authors": [
        "Andrea Maurino"
      ],
      "abstract": "Machine Learning (ML) models are being increasingly employed for credit risk evaluation, with their effectiveness largely hinging on the quality of the input data. In this paper we investigate the impact of several data quality issues, including missing values, noisy attributes, outliers, and label errors, on the predictive accuracy of the machine learning model used in credit risk assessment. Utilizing an open-source dataset, we introduce controlled data corruption using the Pucktrick library to assess the robustness of 10 frequently used models like Random Forest, SVM, and Logistic Regression and so on. Our experiments show significant differences in model robustness based on the nature and severity of the data degradation. Moreover, the proposed methodology and accompanying tools offer practical support for practitioners seeking to enhance data pipeline robustness, and provide researchers with a flexible framework for further experimentation in data-centric AI contexts.",
      "tldr_zh": "æœ¬ç ”ç©¶æ¢è®¨äº†æ•°æ®è´¨é‡å¯¹ç”¨äºä¿¡ç”¨é£é™©è¯„ä¼°(Credit Risk Assessment)çš„æœºå™¨å­¦ä¹ (Machine Learning)æ¨¡å‹é¢„æµ‹å‡†ç¡®æ€§çš„å½±å“ã€‚è®ºæ–‡é‡ç‚¹åˆ†æäº†ç¼ºå¤±å€¼(missing values)ã€å™ªå£°å±æ€§(noisy attributes)ã€ç¦»ç¾¤ç‚¹(outliers)å’Œæ ‡ç­¾é”™è¯¯(label errors)ç­‰å…·ä½“æ•°æ®è´¨é‡é—®é¢˜ã€‚ç ”ç©¶äººå‘˜åˆ©ç”¨å¼€æºæ•°æ®é›†å¹¶ç»“åˆ Pucktrick åº“å¼•å…¥å¯æ§çš„æ•°æ®æŸåï¼Œå¯¹éšæœºæ£®æ—(Random Forest)ã€æ”¯æŒå‘é‡æœº(SVM)å’Œé€»è¾‘å›å½’(Logistic Regression)ç­‰10ç§å¸¸ç”¨æ¨¡å‹çš„é²æ£’æ€§è¿›è¡Œäº†ç³»ç»Ÿè¯„ä¼°ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ¨¡å‹çš„é²æ£’æ€§ä¼šå› æ•°æ®é€€åŒ–çš„æ€§è´¨å’Œä¸¥é‡ç¨‹åº¦è€Œäº§ç”Ÿæ˜¾è‘—å·®å¼‚ã€‚è¯¥ç ”ç©¶æå‡ºçš„æ–¹æ³•è®ºå’Œé…å¥—å·¥å…·ä¸ºä»ä¸šè€…å¢å¼ºæ•°æ®æµæ°´çº¿(data pipeline)çš„é²æ£’æ€§æä¾›äº†å®é™…æ”¯æŒã€‚æ­¤å¤–ï¼Œè¿™é¡¹å·¥ä½œè¿˜ä¸ºç ”ç©¶äººå‘˜åœ¨ä»¥æ•°æ®ä¸ºä¸­å¿ƒçš„äººå·¥æ™ºèƒ½(data-centric AI)è¯­å¢ƒä¸‹å¼€å±•è¿›ä¸€æ­¥å®éªŒæä¾›äº†ä¸€ä¸ªçµæ´»çš„æ¡†æ¶ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DB"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10964v1",
      "published_date": "2025-11-14 05:18:43 UTC",
      "updated_date": "2025-11-14 05:18:43 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:23:33.642311+00:00"
    },
    {
      "arxiv_id": "2601.11526v1",
      "title": "Chatsparent: An Interactive System for Detecting and Mitigating Cognitive Fatigue in LLMs",
      "title_zh": "Chatsparentï¼šç”¨äºå¤§è¯­è¨€æ¨¡å‹è®¤çŸ¥ç–²åŠ³æ£€æµ‹ä¸ç¼“è§£çš„äº¤äº’å¼ç³»ç»Ÿ",
      "authors": [
        "Riju Marwah",
        "Vishal Pallagani",
        "Ritvik Garimella",
        "Amit Sheth"
      ],
      "abstract": "LLMs are increasingly being deployed as chatbots, but today's interfaces offer little to no friction: users interact through seamless conversations that conceal when the model is drifting, hallucinating or failing. This lack of transparency fosters blind trust, even as models produce unstable or repetitive outputs. We introduce an interactive demo that surfaces and mitigates cognitive fatigue, a failure mode where LLMs gradually lose coherence during auto-regressive generation. Our system, Chatsparent, instruments real-time, token-level signals of fatigue, including attention-to-prompt decay, embedding drift, and entropy collapse, and visualizes them as a unified fatigue index. When fatigue thresholds are crossed, the interface allows users to activate lightweight interventions such as attention resets, entropy-regularized decoding, and self-reflection checkpoints. The demo streams live text and fatigue signals, allowing users to observe when fatigue arises, how it affects output quality, and how interventions restore stability. By turning passive chatbot interaction into an interactive diagnostic experience, our system empowers users to better understand LLM behavior while improving reliability at inference time.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†Chatsparentï¼Œä¸€ä¸ªç”¨äºæ£€æµ‹å’Œç¼“è§£å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°è®¤çŸ¥ç–²åŠ³(cognitive fatigue)çš„äº¤äº’å¼ç³»ç»Ÿã€‚é’ˆå¯¹ç°æœ‰å¯¹è¯ç•Œé¢é€æ˜åº¦ä¸è¶³å¯¼è‡´ç”¨æˆ·ç›²ç›®ä¿¡ä»»çš„é—®é¢˜ï¼ŒChatsparenté€šè¿‡ç›‘æ§å®æ—¶ä»¤ç‰Œçº§ä¿¡å·ï¼ŒåŒ…æ‹¬æç¤ºè¯æ³¨æ„åŠ›è¡°å‡(attention-to-prompt decay)ã€åµŒå…¥æ¼‚ç§»(embedding drift)å’Œç†µåç¼©(entropy collapse)ï¼Œå¹¶å°†å…¶å¯è§†åŒ–ä¸ºç»Ÿä¸€çš„ç–²åŠ³æŒ‡æ•°ã€‚å½“è¯¥æŒ‡æ•°è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œç³»ç»Ÿå…è®¸ç”¨æˆ·æ¿€æ´»æ³¨æ„åŠ›é‡ç½®(attention resets)ã€ç†µæ­£åˆ™åŒ–è§£ç (entropy-regularized decoding)å’Œè‡ªæˆ‘åæ€æ£€æŸ¥ç‚¹(self-reflection checkpoints)ç­‰è½»é‡çº§å¹²é¢„æ‰‹æ®µæ¥æ¢å¤æ¨¡å‹ç¨³å®šæ€§ã€‚è¿™ç§å°†è¢«åŠ¨äº¤äº’è½¬å˜ä¸ºäº¤äº’å¼è¯Šæ–­ä½“éªŒçš„æ–¹å¼ï¼Œä½¿è®¾è®¡è€…å’Œç”¨æˆ·èƒ½å¤Ÿå®æ—¶è§‚å¯Ÿç–²åŠ³å¦‚ä½•å½±å“è¾“å‡ºè´¨é‡ã€‚è¯¥ç³»ç»Ÿæœ‰æ•ˆåœ°æå‡äº†æ¨ç†æ—¶çš„æ¨¡å‹å¯é æ€§ï¼Œå¹¶èµ‹äºˆç”¨æˆ·æ›´æ·±å±‚æ¬¡ç†è§£LLMè¡Œä¸ºçš„èƒ½åŠ›ã€‚",
      "categories": [
        "cs.HC",
        "cs.AI"
      ],
      "primary_category": "cs.HC",
      "comment": "Accepted to AAAI 2026 Demonstration Track",
      "pdf_url": "https://arxiv.org/pdf/2601.11526v1",
      "published_date": "2025-11-14 05:08:53 UTC",
      "updated_date": "2025-11-14 05:08:53 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:23:38.839490+00:00"
    },
    {
      "arxiv_id": "2511.17567v1",
      "title": "Temporal-adaptive Weight Quantization for Spiking Neural Networks",
      "title_zh": "è„‰å†²ç¥ç»ç½‘ç»œçš„æ—¶é—´è‡ªé€‚åº”æƒé‡é‡åŒ–",
      "authors": [
        "Han Zhang",
        "Qingyan Meng",
        "Jiaqi Wang",
        "Baiyu Chen",
        "Zhengyu Ma",
        "Xiaopeng Fan"
      ],
      "abstract": "Weight quantization in spiking neural networks (SNNs) could further reduce energy consumption. However, quantizing weights without sacrificing accuracy remains challenging. In this study, inspired by astrocyte-mediated synaptic modulation in the biological nervous systems, we propose Temporal-adaptive Weight Quantization (TaWQ), which incorporates weight quantization with temporal dynamics to adaptively allocate ultra-low-bit weights along the temporal dimension. Extensive experiments on static (e.g., ImageNet) and neuromorphic (e.g., CIFAR10-DVS) datasets demonstrate that our TaWQ maintains high energy efficiency (4.12M, 0.63mJ) while incurring a negligible quantization loss of only 0.22% on ImageNet.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹è„‰å†²ç¥ç»ç½‘ç»œ (Spiking Neural Networks) åœ¨æƒé‡é‡åŒ– (Weight quantization) è¿‡ç¨‹ä¸­éš¾ä»¥å…¼é¡¾ç²¾åº¦ä¸èƒ½è€—çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§åä¸º Temporal-adaptive Weight Quantization (TaWQ) çš„æ–°å‹æ–¹æ³•ã€‚å—ç”Ÿç‰©ç¥ç»ç³»ç»Ÿä¸­æ˜Ÿå½¢èƒ¶è´¨ç»†èƒä»‹å¯¼çš„çªè§¦è°ƒèŠ‚ (astrocyte-mediated synaptic modulation) å¯å‘ï¼ŒTaWQ å°†æƒé‡é‡åŒ–ä¸æ—¶é—´åŠ¨åŠ›å­¦ (temporal dynamics) ç›¸ç»“åˆï¼Œèƒ½å¤Ÿæ²¿æ—¶é—´ç»´åº¦ (temporal dimension) è‡ªé€‚åº”åœ°åˆ†é…è¶…ä½æ¯”ç‰¹æƒé‡ (ultra-low-bit weights)ã€‚åœ¨ ImageNet å’Œ CIFAR10-DVS ç­‰é™æ€åŠç¥ç»å½¢æ€æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŒé«˜èƒ½é‡æ•ˆç‡çš„åŒæ—¶ï¼Œåœ¨ ImageNet ä¸Šçš„é‡åŒ–æŸå¤±ä»…ä¸º 0.22%ã€‚è¯¥ç ”ç©¶å±•ç¤ºäº† TaWQ åœ¨ç»´æŒæä½èƒ½è€—çš„åŒæ—¶å®ç°è¿‘ä¹æ— æŸé‡åŒ–çš„æ½œåŠ›ï¼Œä¸ºé«˜æ•ˆç¥ç»å½¢æ€è®¡ç®—çš„å‘å±•æä¾›äº†é‡è¦å‚è€ƒã€‚",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "cs.NE",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17567v1",
      "published_date": "2025-11-14 05:08:14 UTC",
      "updated_date": "2025-11-14 05:08:14 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:23:47.733525+00:00"
    },
    {
      "arxiv_id": "2511.10958v1",
      "title": "Text-guided Weakly Supervised Framework for Dynamic Facial Expression Recognition",
      "title_zh": "é¢å‘åŠ¨æ€é¢éƒ¨è¡¨æƒ…è¯†åˆ«çš„æ–‡æœ¬å¼•å¯¼å¼±ç›‘ç£æ¡†æ¶",
      "authors": [
        "Gunho Jung",
        "Heejo Kong",
        "Seong-Whan Lee"
      ],
      "abstract": "Dynamic facial expression recognition (DFER) aims to identify emotional states by modeling the temporal changes in facial movements across video sequences. A key challenge in DFER is the many-to-one labeling problem, where a video composed of numerous frames is assigned a single emotion label. A common strategy to mitigate this issue is to formulate DFER as a Multiple Instance Learning (MIL) problem. However, MIL-based approaches inherently suffer from the visual diversity of emotional expressions and the complexity of temporal dynamics. To address this challenge, we propose TG-DFER, a text-guided weakly supervised framework that enhances MIL-based DFER by incorporating semantic guidance and coherent temporal modeling. We incorporate a vision-language pre-trained (VLP) model is integrated to provide semantic guidance through fine-grained textual descriptions of emotional context. Furthermore, we introduce visual prompts, which align enriched textual emotion labels with visual instance features, enabling fine-grained reasoning and frame-level relevance estimation. In addition, a multi-grained temporal network is designed to jointly capture short-term facial dynamics and long-range emotional flow, ensuring coherent affective understanding across time. Extensive results demonstrate that TG-DFER achieves improved generalization, interpretability, and temporal sensitivity under weak supervision.",
      "tldr_zh": "è¿™é¡¹ç ”ç©¶æå‡ºäº†TG-DFERï¼Œä¸€ä¸ªæ–‡æœ¬å¼•å¯¼çš„å¼±ç›‘ç£æ¡†æ¶ï¼Œæ—¨åœ¨è§£å†³åŠ¨æ€äººè„¸è¡¨æƒ…è¯†åˆ«(Dynamic Facial Expression Recognition, DFER)ä¸­ç”±äºâ€œå¤šå¯¹ä¸€â€æ ‡ç­¾é—®é¢˜å¯¼è‡´çš„è§†è§‰å¤šæ ·æ€§å’Œæ—¶åºå¤æ‚æ€§æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶é€šè¿‡å¼•å…¥è§†è§‰è¯­è¨€é¢„è®­ç»ƒ(Vision-Language Pre-trained, VLP)æ¨¡å‹ï¼Œåˆ©ç”¨æƒ…æ„Ÿä¸Šä¸‹æ–‡çš„ç»†ç²’åº¦æ–‡æœ¬æè¿°ä¸ºå¤šç¤ºä¾‹å­¦ä¹ (Multiple Instance Learning, MIL)æä¾›è¯­ä¹‰å¼•å¯¼ã€‚ä¸ºäº†å®ç°æ›´ç²¾å‡†çš„æ¨ç†ï¼Œç ”ç©¶å¼•å…¥äº†è§†è§‰æç¤º(Visual Prompts)æœºåˆ¶ï¼Œå°†ä¸°å¯Œçš„æ–‡æœ¬æƒ…æ„Ÿæ ‡ç­¾ä¸è§†è§‰å®ä¾‹ç‰¹å¾å¯¹é½ï¼Œä»¥å®ç°æœ‰æ•ˆçš„å¸§çº§ç›¸å…³æ€§ä¼°è®¡ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªå¤šç²’åº¦æ—¶åºç½‘ç»œï¼Œæ—¨åœ¨åŒæ—¶æ•æ‰çŸ­æœŸé¢éƒ¨åŠ¨æ€å’Œé•¿ç¨‹æƒ…æ„Ÿæµï¼Œç¡®ä¿åœ¨æ—¶é—´ç»´åº¦ä¸Šè·å¾—è¿è´¯çš„æƒ…æ„Ÿç†è§£ã€‚å®éªŒç»“æœè¯æ˜ï¼ŒTG-DFERåœ¨å¼±ç›‘ç£ç¯å¢ƒä¸‹æ˜¾è‘—æå‡äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€å¯è§£é‡Šæ€§ä»¥åŠå¯¹æ—¶åºå˜åŒ–çš„æ•æ„Ÿåº¦ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10958v1",
      "published_date": "2025-11-14 04:49:58 UTC",
      "updated_date": "2025-11-14 04:49:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:23:41.434216+00:00"
    },
    {
      "arxiv_id": "2511.13755v1",
      "title": "Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement",
      "title_zh": "é¢å‘å‡è¡¡å¤šæ¨¡æ€ä¿¡æ¯ç²¾ç‚¼çš„è‡ªé€‚åº”å†—ä½™è°ƒèŠ‚",
      "authors": [
        "Zhe Yang",
        "Wenrui Li",
        "Hongtao Chen",
        "Penghong Wang",
        "Ruiqin Xiong",
        "Xiaopeng Fan"
      ],
      "abstract": "Multimodal learning aims to improve performance by leveraging data from multiple sources. During joint multimodal training, due to modality bias, the advantaged modality often dominates backpropagation, leading to imbalanced optimization. Existing methods still face two problems: First, the long-term dominance of the dominant modality weakens representation-output coupling in the late stages of training, resulting in the accumulation of redundant information. Second, previous methods often directly and uniformly adjust the gradients of the advantaged modality, ignoring the semantics and directionality between modalities. To address these limitations, we propose Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement (RedReg), which is inspired by information bottleneck principle. Specifically, we construct a redundancy phase monitor that uses a joint criterion of effective gain growth rate and redundancy to trigger intervention only when redundancy is high. Furthermore, we design a co-information gating mechanism to estimate the contribution of the current dominant modality based on cross-modal semantics. When the task primarily relies on a single modality, the suppression term is automatically disabled to preserve modality-specific information. Finally, we project the gradient of the dominant modality onto the orthogonal complement of the joint multimodal gradient subspace and suppress the gradient according to redundancy. Experiments show that our method demonstrates superiority among current major methods in most scenarios. Ablation experiments verify the effectiveness of our method. The code is available at https://github.com/xia-zhe/RedReg.git",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†RedRegï¼ˆAdaptive Redundancy Regulation for Balanced Multimodal Information Refinementï¼‰ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å­¦ä¹ ä¸­å› æ¨¡æ€åå·®(modality bias)å¯¼è‡´çš„ä¸å¹³è¡¡ä¼˜åŒ–ä»¥åŠè®­ç»ƒåæœŸå†—ä½™ä¿¡æ¯ç§¯ç´¯çš„é—®é¢˜ã€‚å—ä¿¡æ¯ç“¶é¢ˆ(information bottleneck)åŸç†å¯å‘ï¼ŒRedRegå¼•å…¥äº†å†—ä½™ç›¸ä½ç›‘æµ‹å™¨(redundancy phase monitor)ï¼Œåˆ©ç”¨æœ‰æ•ˆå¢ç›Šå¢é•¿ç‡å’Œå†—ä½™åº¦ä½œä¸ºè”åˆå‡†åˆ™ï¼Œç¡®ä¿ä»…åœ¨å†—ä½™é˜¶æ®µè§¦å‘å¹²é¢„ã€‚æ­¤å¤–ï¼Œè¯¥æ¡†æ¶è®¾è®¡äº†äº’ä¿¡æ¯é—¨æ§æœºåˆ¶(co-information gating mechanism)æ¥è¯„ä¼°ä¸»å¯¼æ¨¡æ€çš„è¯­ä¹‰è´¡çŒ®ï¼Œå¹¶èƒ½åœ¨ä»»åŠ¡ä¾èµ–å•ä¸€æ¨¡æ€æ—¶è‡ªåŠ¨ä¿ç•™æ¨¡æ€ç‰¹å®šä¿¡æ¯ã€‚åœ¨æ¢¯åº¦å¤„ç†ä¸Šï¼ŒRedRegå°†ä¸»å¯¼æ¨¡æ€çš„æ¢¯åº¦æŠ•å½±åˆ°è”åˆå¤šæ¨¡æ€æ¢¯åº¦å­ç©ºé—´çš„æ­£äº¤è¡¥é›†ï¼Œå¹¶ç»“åˆå†—ä½™åº¦å®æ–½åŠ¨æ€æŠ‘åˆ¶ã€‚å®éªŒè¯æ˜ï¼ŒRedRegåœ¨å¤šç§ä¸»æµåœºæ™¯ä¸‹å‡å±•ç°å‡ºä¼˜äºç°æœ‰æ–¹æ³•çš„æ€§èƒ½ï¼Œé€šè¿‡æ¶ˆèå®éªŒè¿›ä¸€æ­¥éªŒè¯äº†å…¶åœ¨å®ç°å¹³è¡¡å¤šæ¨¡æ€ä¿¡æ¯ç²¾ç‚¼æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13755v1",
      "published_date": "2025-11-14 04:44:34 UTC",
      "updated_date": "2025-11-14 04:44:34 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:23:50.747399+00:00"
    },
    {
      "arxiv_id": "2511.11764v1",
      "title": "Demystify, Use, Reflect: Preparing students to be informed LLM-users",
      "title_zh": "æ­ç§˜ã€ä½¿ç”¨ä¸åæ€ï¼šåŸ¹å…»ç†æ€§çš„ LLM ç”¨æˆ·",
      "authors": [
        "Nikitha Donekal Chandrashekar",
        "Sehrish Basir Nizamani",
        "Margaret Ellis",
        "Naren Ramakrishnan"
      ],
      "abstract": "We transitioned our post-CS1 course that introduces various subfields of computer science so that it integrates Large Language Models (LLMs) in a structured, critical, and practical manner. It aims to help students develop the skills needed to engage meaningfully and responsibly with AI. The course now includes explicit instruction on how LLMs work, exposure to current tools, ethical issues, and activities that encourage student reflection on personal use of LLMs as well as the larger evolving landscape of AI-assisted programming. In class, we demonstrate the use and verification of LLM outputs, guide students in the use of LLMs as an ingredient in a larger problem-solving loop, and require students to disclose and acknowledge the nature and extent of LLM assistance. Throughout the course, we discuss risks and benefits of LLMs across CS subfields. In our first iteration of the course, we collected and analyzed data from students pre and post surveys. Student understanding of how LLMs work became more technical, and their verification and use of LLMs shifted to be more discerning and collaborative. These strategies can be used in other courses to prepare students for the AI-integrated future.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†å¦‚ä½•å°†å¤§è¯­è¨€æ¨¡å‹(LLMs)ç»“æ„åŒ–ã€æ‰¹åˆ¤æ€§ä¸”å®ç”¨æ€§åœ°èå…¥è®¡ç®—æœºç§‘å­¦åŸºç¡€è¯¾ç¨‹(post-CS1)ï¼Œæ—¨åœ¨åŸ¹å…»å­¦ç”Ÿè´Ÿè´£ä»»åœ°ä½¿ç”¨AIçš„èƒ½åŠ›ã€‚è¯¾ç¨‹è®¾è®¡å›´ç»•â€œæ­ç§˜ã€ä½¿ç”¨ã€åæ€â€ä¸‰ä¸ªæ ¸å¿ƒï¼Œé€šè¿‡è®²è§£LLMså·¥ä½œåŸç†ã€ä¼¦ç†é—®é¢˜åŠéªŒè¯è¾“å‡ºçš„æ–¹æ³•ï¼Œå¼•å¯¼å­¦ç”Ÿå°†å…¶ä½œä¸ºè§£å†³å¤æ‚é—®é¢˜çš„è¾…åŠ©å·¥å…·ã€‚ç ”ç©¶è¦æ±‚å­¦ç”Ÿåœ¨ä½œä¸šä¸­æŠ«éœ²LLMçš„ä½¿ç”¨èŒƒå›´ï¼Œå¹¶åœ¨ä¸åŒè®¡ç®—æœºå­é¢†åŸŸä¸­è®¨è®ºAIæŠ€æœ¯çš„é£é™©ä¸æ”¶ç›Šã€‚é€šè¿‡å¯¹è¯¾ç¨‹è¿­ä»£å‰åçš„é—®å·æ•°æ®åˆ†æï¼Œç»“æœæ˜¾ç¤ºå­¦ç”Ÿå¯¹LLMsçš„ç†è§£å˜å¾—æ›´åŠ ä¸“ä¸šï¼Œåœ¨éªŒè¯å’Œåä½œä½¿ç”¨æ–¹é¢ä¹Ÿè¡¨ç°å¾—æ›´å…·é‰´åˆ«åŠ›ã€‚è¿™äº›æ•™å­¦å®è·µä¸ºé«˜ç­‰æ•™è‚²åº”å¯¹AIé›†æˆåŒ–è¶‹åŠ¿æä¾›äº†æœ‰æ•ˆçš„è¯¾ç¨‹è®¾è®¡å‚è€ƒã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "2 pages 1 table Submitted to SIGCSE 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.11764v1",
      "published_date": "2025-11-14 04:43:49 UTC",
      "updated_date": "2025-11-14 04:43:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:23:53.230340+00:00"
    },
    {
      "arxiv_id": "2511.10952v2",
      "title": "Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints",
      "title_zh": "è¿è¡Œçº¦æŸä¸­å¯¹é½ä¸åŠ¨æ€å†²çªæ¶ˆè§£çš„éœ€æ±‚",
      "authors": [
        "Steven J. Jones",
        "Robert E. Wray",
        "John E. Laird"
      ],
      "abstract": "Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual \"knowledge\" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†éƒ¨ç½²åçš„è‡ªä¸» AI ç³»ç»Ÿåœ¨å¤„ç†å¤æ‚ä¸”æœªå……åˆ†æ˜ç¡®çš„æƒ…å¢ƒæ—¶ï¼Œå¦‚ä½•è§£å†³æ“ä½œçº¦æŸï¼ˆOperational Constraintsï¼‰ä¹‹é—´çš„å†²çªé—®é¢˜ã€‚è®ºæ–‡æŒ‡å‡ºï¼Œå½“ç°æœ‰ç­–ç•¥ï¼ˆPoliciesï¼‰æ— æ³•å®Œå…¨æ»¡è¶³æ³•å¾‹ã€è§„èŒƒå’Œç›®æ ‡ç­‰çº¦æŸæ—¶ï¼Œæ™ºèƒ½ä½“å¿…é¡»å…·å¤‡è¶…è¶Šé¢„è®­ç»ƒç­–ç•¥çš„èƒ½åŠ›ï¼Œé€šè¿‡æ„å»ºã€è¯„ä¼°å¹¶è¯æ˜å€™é€‰è¡ŒåŠ¨æ–¹æ¡ˆæ¥ç¡®ä¿å†³ç­–ä¸äººç±»é¢„æœŸå¯¹é½ï¼ˆAlignedï¼‰ã€‚ä½œè€…è¯¦ç»†åˆ»ç”»äº†åœ¨æ­¤ç±»èƒŒæ™¯ä¸‹æ™ºèƒ½ä½“å†³ç­–çš„å…·ä½“éœ€æ±‚ï¼Œå¹¶è¯†åˆ«äº†å®ç°ç¨³å¥å†³ç­–æ‰€éœ€çš„æ ¸å¿ƒçŸ¥è¯†ç±»å‹ã€‚é€šè¿‡ç†è®ºåˆ†æä¸å®è¯æ¡ˆä¾‹ç ”ç©¶ï¼Œè¯¥æ–‡å¼ºè°ƒäº†æ™ºèƒ½ä½“æ•´åˆè§„èŒƒæ€§ï¼ˆNormativeï¼‰ã€åŠ¡å®æ€§ï¼ˆPragmaticï¼‰å’Œæƒ…å¢ƒæ€§ï¼ˆSituationalï¼‰ç†è§£çš„é‡è¦æ€§ã€‚è¿™ç§ç»¼åˆæ–¹æ³•èƒ½å¤Ÿä½¿æ™ºèƒ½ä½“åœ¨å¤æ‚çš„ç°å®ç¯å¢ƒä¸­è¯†åˆ«å¹¶è¿½æ±‚æ›´ç¬¦åˆäººç±»ä»·å€¼è§‚çš„è¡ŒåŠ¨è·¯å¾„ï¼Œä»è€Œæå‡è‡ªä¸»ç³»ç»Ÿçš„å¯é æ€§ä¸å¯¹é½ç¨‹åº¦ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "8 pages + references and technical appendix (accepted for oral presentation at AAAI26)",
      "pdf_url": "https://arxiv.org/pdf/2511.10952v2",
      "published_date": "2025-11-14 04:33:15 UTC",
      "updated_date": "2025-11-18 03:36:47 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:23:57.833703+00:00"
    },
    {
      "arxiv_id": "2511.10949v1",
      "title": "Exposing Weak Links in Multi-Agent Systems under Adversarial Prompting",
      "title_zh": "æ­ç¤ºå¯¹æŠ—æ€§æç¤ºä¸‹å¤šæ™ºèƒ½ä½“ç³»ç»Ÿçš„è–„å¼±ç¯èŠ‚",
      "authors": [
        "Nirmit Arora",
        "Sathvik Joel",
        "Ishan Kavathekar",
        "Palak",
        "Rohan Gandhi",
        "Yash Pandya",
        "Tanuja Ganu",
        "Aditya Kanade",
        "Akshay Nambi"
      ],
      "abstract": "LLM-based agents are increasingly deployed in multi-agent systems (MAS). As these systems move toward real-world applications, their security becomes paramount. Existing research largely evaluates single-agent security, leaving a critical gap in understanding the vulnerabilities introduced by multi-agent design. However, existing systems fall short due to lack of unified frameworks and metrics focusing on unique rejection modes in MAS. We present SafeAgents, a unified and extensible framework for fine-grained security assessment of MAS. SafeAgents systematically exposes how design choices such as plan construction strategies, inter-agent context sharing, and fallback behaviors affect susceptibility to adversarial prompting. We introduce Dharma, a diagnostic measure that helps identify weak links within multi-agent pipelines. Using SafeAgents, we conduct a comprehensive study across five widely adopted multi-agent architectures (centralized, decentralized, and hybrid variants) on four datasets spanning web tasks, tool use, and code generation. Our findings reveal that common design patterns carry significant vulnerabilities. For example, centralized systems that delegate only atomic instructions to sub-agents obscure harmful objectives, reducing robustness. Our results highlight the need for security-aware design in MAS. Link to code is https://github.com/microsoft/SafeAgents",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLM)çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ(MAS)åœ¨ç°å®åº”ç”¨ä¸­çš„å®‰å…¨æ€§ï¼Œæå‡ºäº† SafeAgents è¿™ä¸€ç»Ÿä¸€ä¸”å¯æ‰©å±•çš„ç»†ç²’åº¦å®‰å…¨è¯„ä¼°æ¡†æ¶ã€‚ç ”ç©¶è€…é€šè¿‡å¼•å…¥è¯Šæ–­æŒ‡æ ‡ Dharmaï¼Œç³»ç»Ÿåœ°åˆ†æäº†è®¡åˆ’æ„å»ºç­–ç•¥(plan construction strategies)ã€æ™ºèƒ½ä½“é—´ä¸Šä¸‹æ–‡å…±äº«(inter-agent context sharing)ä»¥åŠå›é€€è¡Œä¸º(fallback behaviors)ç­‰è®¾è®¡é€‰æ‹©å¯¹å¯¹æŠ—æ€§æç¤º(adversarial prompting)æ•æ„Ÿåº¦çš„å½±å“ã€‚å®éªŒè¦†ç›–äº†äº”ç§ä¸»æµå¤šæ™ºèƒ½ä½“æ¶æ„ä»¥åŠåŒ…æ‹¬ Web ä»»åŠ¡ã€å·¥å…·è°ƒç”¨å’Œä»£ç ç”Ÿæˆåœ¨å†…çš„å››ä¸ªæ•°æ®é›†ã€‚ç ”ç©¶å‘ç°ï¼Œå¸¸è§çš„ MAS è®¾è®¡æ¨¡å¼å­˜åœ¨æ˜¾è‘—è„†å¼±æ€§ï¼Œä¾‹å¦‚ä¸­å¿ƒåŒ–ç³»ç»Ÿåœ¨å‘å­æ™ºèƒ½ä½“åˆ†å‘åŸå­æŒ‡ä»¤æ—¶ä¼šæ©ç›–æœ‰å®³ç›®æ ‡ï¼Œä»è€Œé™ä½é²æ£’æ€§ã€‚è¯¥æˆæœå¡«è¡¥äº†å¤šæ™ºèƒ½ä½“è®¾è®¡å¼•å…¥å®‰å…¨æ¼æ´çš„ç ”ç©¶ç©ºç™½ï¼Œå¹¶å¼ºè°ƒäº†åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¼€å‘ä¸­é‡‡ç”¨å®‰å…¨æ„ŸçŸ¥è®¾è®¡(security-aware design)çš„é‡è¦æ€§ã€‚",
      "categories": [
        "cs.MA",
        "cs.AI"
      ],
      "primary_category": "cs.MA",
      "comment": "10 pages, 3 figures. Code available at https://github.com/microsoft/SafeAgents",
      "pdf_url": "https://arxiv.org/pdf/2511.10949v1",
      "published_date": "2025-11-14 04:22:49 UTC",
      "updated_date": "2025-11-14 04:22:49 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:23:53.929100+00:00"
    },
    {
      "arxiv_id": "2511.10936v1",
      "title": "GraphToxin: Reconstructing Full Unlearned Graphs from Graph Unlearning",
      "title_zh": "GraphToxinï¼šä»å›¾é—å¿˜ä¸­é‡æ„å®Œæ•´çš„è¢«é—å¿˜å›¾",
      "authors": [
        "Ying Song",
        "Balaji Palanisamy"
      ],
      "abstract": "Graph unlearning has emerged as a promising solution for complying with \"the right to be forgotten\" regulations by enabling the removal of sensitive information upon request. However, this solution is not foolproof. The involvement of multiple parties creates new attack surfaces, and residual traces of deleted data can still remain in the unlearned graph neural networks. These vulnerabilities can be exploited by attackers to recover the supposedly erased samples, thereby undermining the inherent functionality of graph unlearning. In this work, we propose GraphToxin, the first graph reconstruction attack against graph unlearning. Specifically, we introduce a novel curvature matching module to provide a fine-grained guidance for full unlearned graph recovery. We demonstrate that GraphToxin can successfully subvert the regulatory guarantees expected from graph unlearning - it can recover not only a deleted individual's information and personal links but also sensitive content from their connections, thereby posing substantially more detrimental threats. Furthermore, we extend GraphToxin to multiple node removals under both white-box and black-box setting. We highlight the necessity of a worst-case analysis and propose a comprehensive evaluation framework to systematically assess the attack performance under both random and worst-case node removals. This provides a more robust and realistic measure of the vulnerability of graph unlearning methods to graph reconstruction attacks. Our extensive experiments demonstrate the effectiveness and flexibility of GraphToxin. Notably, we show that existing defense mechanisms are largely ineffective against this attack and, in some cases, can even amplify its performance. Given the severe privacy risks posed by GraphToxin, our work underscores the urgent need for the development of more effective and robust defense strategies against this attack.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ Graph unlearning å­˜åœ¨çš„éšç§æ³„éœ²é£é™©ï¼Œæå‡ºäº†é¦–ä¸ªå…¨å›¾é‡æ„æ”»å‡» (graph reconstruction attack) æ¡†æ¶ GraphToxinã€‚è¯¥æ¡†æ¶å¼•å…¥äº†åˆ›æ–°çš„æ›²ç‡åŒ¹é…æ¨¡å— (curvature matching module)ï¼Œé€šè¿‡ç»†ç²’åº¦çš„å¼•å¯¼æ¢å¤è¢«åˆ é™¤çš„èŠ‚ç‚¹ä¿¡æ¯ã€ä¸ªäººé“¾æ¥ä»¥åŠå…³è”æ–¹çš„æ•æ„Ÿå†…å®¹ã€‚GraphToxin æ”¯æŒç™½ç›’ä¸é»‘ç›’è®¾ç½®ä¸‹çš„å¤šèŠ‚ç‚¹ç§»é™¤åœºæ™¯ï¼Œå¹¶é…åˆä¸€å¥—ç»¼åˆè¯„ä¼°æ¡†æ¶ï¼Œç”¨äºç³»ç»Ÿè¡¡é‡éšæœºåŠæœ€åæƒ…å†µ (worst-case) ä¸‹çš„æ”»å‡»æ€§èƒ½ã€‚å®éªŒç»“æœè¯æ˜äº† GraphToxin çš„æœ‰æ•ˆæ€§ä¸çµæ´»æ€§ï¼Œæ­ç¤ºäº†ç°æœ‰é˜²å¾¡æœºåˆ¶åœ¨åº”å¯¹æ­¤ç±»æ”»å‡»æ—¶çš„æ— åŠ›ï¼Œç”šè‡³å¯èƒ½äº§ç”Ÿåæ•ˆæœã€‚æ­¤é¡¹å·¥ä½œæ­ç¤ºäº† Graph unlearning æ–¹æ¡ˆä¸­ä¸¥é‡çš„éšç§å®‰å…¨æ¼æ´ï¼Œå¼ºè°ƒäº†å¼€å‘æ›´å…·é²æ£’æ€§çš„é˜²å¾¡ç­–ç•¥çš„è¿«åˆ‡éœ€æ±‚ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "Submitted to S&P 2026. Code will be available",
      "pdf_url": "https://arxiv.org/pdf/2511.10936v1",
      "published_date": "2025-11-14 03:52:00 UTC",
      "updated_date": "2025-11-14 03:52:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:23:59.437091+00:00"
    },
    {
      "arxiv_id": "2511.10925v1",
      "title": "Multi-Agent Legal Verifier Systems for Data Transfer Planning",
      "title_zh": "é¢å‘æ•°æ®ä¼ è¾“è§„åˆ’çš„å¤šæ™ºèƒ½ä½“æ³•å¾‹éªŒè¯ç³»ç»Ÿ",
      "authors": [
        "Ha-Thanh Nguyen",
        "Wachara Fungwacharakorn",
        "Ken Satoh"
      ],
      "abstract": "Legal compliance in AI-driven data transfer planning is becoming increasingly critical under stringent privacy regulations such as the Japanese Act on the Protection of Personal Information (APPI). We propose a multi-agent legal verifier that decomposes compliance checking into specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol. Evaluated on a stratified dataset of 200 Amended APPI Article 16 cases with clearly defined ground truth labels and multiple performance metrics, the system achieves 72% accuracy, which is 21 percentage points higher than a single-agent baseline, including 90% accuracy on clear compliance cases (vs. 16% for the baseline) while maintaining perfect detection of clear violations. While challenges remain in ambiguous scenarios, these results show that domain specialization and coordinated reasoning can meaningfully improve legal AI performance, providing a scalable and regulation-aware framework for trustworthy and interpretable automated compliance verification.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹AIé©±åŠ¨çš„æ•°æ®ä¼ è¾“è§„åˆ’åœ¨éµå®ˆæ—¥æœ¬ã€Šä¸ªäººä¿¡æ¯ä¿æŠ¤æ³•ã€‹(APPI) ç­‰ä¸¥æ ¼éšç§æ³•è§„æ–¹é¢çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç§å¤šæ™ºèƒ½ä½“æ³•å¾‹éªŒè¯ç³»ç»Ÿ (Multi-agent legal verifier)ã€‚è¯¥ç³»ç»Ÿå°†åˆè§„æ€§æ£€æŸ¥åˆ†è§£ä¸ºæ³•å®šè§£é‡Š (statutory interpretation)ã€ä¸šåŠ¡èƒŒæ™¯è¯„ä¼° (business context evaluation) å’Œé£é™©è¯„ä¼° (risk assessment) ç­‰ä¸“ä¸šæ™ºèƒ½ä½“ï¼Œå¹¶é€šè¿‡ç»“æ„åŒ–çš„åˆæˆåè®® (synthesis protocol) è¿›è¡Œåè°ƒã€‚åœ¨åŒ…å«200ä¸ªä¿®è®¢ç‰ˆAPPIç¬¬16æ¡æ¡ˆä¾‹çš„æ•°æ®é›†ä¸Šè¿›è¡Œçš„è¯„ä¼°æ˜¾ç¤ºï¼Œè¯¥ç³»ç»Ÿå®ç°äº†72%çš„å‡†ç¡®ç‡ï¼Œæ¯”å•æ™ºèƒ½ä½“åŸºçº¿æ¨¡å‹æå‡äº†21ä¸ªç™¾åˆ†ç‚¹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¯¥ç³»ç»Ÿåœ¨æ˜ç¡®åˆè§„çš„æ¡ˆä¾‹ä¸­å‡†ç¡®ç‡é«˜è¾¾90%ï¼Œä¸”å¯¹æ˜ç¡®è¿è§„è¡Œä¸ºä¿æŒäº†å®Œç¾çš„æ£€æµ‹ç‡ã€‚ç ”ç©¶è¯æ˜ï¼Œé€šè¿‡é¢†åŸŸä¸“ä¸šåŒ–å’ŒååŒæ¨ç†ï¼Œå¯ä»¥æ˜¾è‘—æå‡æ³•å¾‹AIåœ¨å¤„ç†å¤æ‚æ³•è§„æ—¶çš„è¡¨ç°ï¼Œä¸ºå®ç°å¯ä¿¡ä¸”å¯è§£é‡Šçš„è‡ªåŠ¨åŒ–åˆè§„éªŒè¯æä¾›äº†ä¸€ä¸ªå…·å¤‡ç›‘ç®¡æ„è¯†çš„å¯æ‰©å±•æ¡†æ¶ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "Presented at NeLaMKRR@KR, 2025 (arXiv:2511.09575)",
      "pdf_url": "https://arxiv.org/pdf/2511.10925v1",
      "published_date": "2025-11-14 03:32:08 UTC",
      "updated_date": "2025-11-14 03:32:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:24:14.037143+00:00"
    },
    {
      "arxiv_id": "2511.13753v1",
      "title": "Robustness of LLM-enabled vehicle trajectory prediction under data security threats",
      "title_zh": "æ•°æ®å®‰å…¨å¨èƒä¸‹åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è½¦è¾†è½¨è¿¹é¢„æµ‹é²æ£’æ€§",
      "authors": [
        "Feilong Wang",
        "Fuqiang Liu"
      ],
      "abstract": "The integration of large language models (LLMs) into automated driving systems has opened new possibilities for reasoning and decision-making by transforming complex driving contexts into language-understandable representations. Recent studies demonstrate that fine-tuned LLMs can accurately predict vehicle trajectories and lane-change intentions by gathering and transforming data from surrounding vehicles. However, the robustness of such LLM-based prediction models for safety-critical driving systems remains unexplored, despite the increasing concerns about the trustworthiness of LLMs. This study addresses this gap by conducting a systematic vulnerability analysis of LLM-enabled vehicle trajectory prediction. We propose a one-feature differential evolution attack that perturbs a single kinematic feature of surrounding vehicles within the LLM's input prompts under a black-box setting. Experiments on the highD dataset reveal that even minor, physically plausible perturbations can significantly disrupt model outputs, underscoring the susceptibility of LLM-based predictors to adversarial manipulation. Further analyses reveal a trade-off between accuracy and robustness, examine the failure mechanism, and explore potential mitigation solutions. The findings provide the very first insights into adversarial vulnerabilities of LLM-driven automated vehicle models in the context of vehicular interactions and highlight the need for robustness-oriented design in future LLM-based intelligent transportation systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäºå¤§è¯­è¨€æ¨¡å‹(LLMs)çš„è½¦è¾†è½¨è¿¹é¢„æµ‹ç³»ç»Ÿåœ¨é¢å¯¹æ•°æ®å®‰å…¨å¨èƒæ—¶çš„é²æ£’æ€§ï¼Œæ—¨åœ¨å¡«è¡¥è‡ªåŠ¨é©¾é©¶é¢†åŸŸLLMå¯ä¿¡åº¦ç ”ç©¶çš„ç©ºç™½ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§å•ç‰¹å¾å·®åˆ†è¿›åŒ–æ”»å‡»(one-feature differential evolution attack)æ–¹æ³•ï¼Œåœ¨é»‘ç›’è®¾ç½®ä¸‹é€šè¿‡å¾®æ‰°è¾“å…¥æç¤ºè¯ä¸­å‘¨è¾¹è½¦è¾†çš„å•ä¸ªè¿åŠ¨ç‰¹å¾æ¥åˆ†æç³»ç»Ÿæ¼æ´ã€‚åœ¨highDæ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œå³ä½¿æ˜¯ç‰©ç†ä¸Šåˆç†çš„å¾®å°æ‰°åŠ¨ä¹Ÿè¶³ä»¥æ˜¾è‘—å¹²æ‰°æ¨¡å‹è¾“å‡ºï¼Œåæ˜ å‡ºLLMé¢„æµ‹å™¨ææ˜“å—åˆ°å¯¹æŠ—æ€§æ“çºµçš„å½±å“ã€‚è¿›ä¸€æ­¥åˆ†ææ­ç¤ºäº†æ¨¡å‹å‡†ç¡®æ€§ä¸é²æ£’æ€§ä¹‹é—´çš„æƒè¡¡å…³ç³»ï¼Œå¹¶æ¢è®¨äº†å¤±æ•ˆæœºåˆ¶åŠæ½œåœ¨çš„ç¼“è§£æ–¹æ¡ˆã€‚è¯¥å·¥ä½œé¦–æ¬¡æ·±å…¥æ­ç¤ºäº†LLMé©±åŠ¨çš„è‡ªåŠ¨é©¾é©¶æ¨¡å‹åœ¨è½¦è¾†äº¤äº’èƒŒæ™¯ä¸‹çš„å¯¹æŠ—æ€§æ¼æ´ï¼Œå¼ºè°ƒäº†æœªæ¥åœ¨è®¾è®¡æ™ºèƒ½äº¤é€šç³»ç»Ÿæ—¶å¿…é¡»é‡è§†é²æ£’æ€§ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR"
      ],
      "primary_category": "cs.LG",
      "comment": "20 pages, 2 figures, 11 tables, working paper",
      "pdf_url": "https://arxiv.org/pdf/2511.13753v1",
      "published_date": "2025-11-14 03:26:51 UTC",
      "updated_date": "2025-11-14 03:26:51 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:24:15.134088+00:00"
    },
    {
      "arxiv_id": "2511.10913v1",
      "title": "Synthetic Voices, Real Threats: Evaluating Large Text-to-Speech Models in Generating Harmful Audio",
      "title_zh": "åˆæˆè¯­éŸ³ï¼Œç°å®å¨èƒï¼šå¤§è§„æ¨¡æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ç”Ÿæˆæœ‰å®³éŸ³é¢‘è¯„ä¼°",
      "authors": [
        "Guangke Chen",
        "Yuhui Wang",
        "Shouling Ji",
        "Xiapu Luo",
        "Ting Wang"
      ],
      "abstract": "Modern text-to-speech (TTS) systems, particularly those built on Large Audio-Language Models (LALMs), generate high-fidelity speech that faithfully reproduces input text and mimics specified speaker identities. While prior misuse studies have focused on speaker impersonation, this work explores a distinct content-centric threat: exploiting TTS systems to produce speech containing harmful content. Realizing such threats poses two core challenges: (1) LALM safety alignment frequently rejects harmful prompts, yet existing jailbreak attacks are ill-suited for TTS because these systems are designed to faithfully vocalize any input text, and (2) real-world deployment pipelines often employ input/output filters that block harmful text and audio.\n  We present HARMGEN, a suite of five attacks organized into two families that address these challenges. The first family employs semantic obfuscation techniques (Concat, Shuffle) that conceal harmful content within text. The second leverages audio-modality exploits (Read, Spell, Phoneme) that inject harmful content through auxiliary audio channels while maintaining benign textual prompts. Through evaluation across five commercial LALMs-based TTS systems and three datasets spanning two languages, we demonstrate that our attacks substantially reduce refusal rates and increase the toxicity of generated speech.\n  We further assess both reactive countermeasures deployed by audio-streaming platforms and proactive defenses implemented by TTS providers. Our analysis reveals critical vulnerabilities: deepfake detectors underperform on high-fidelity audio; reactive moderation can be circumvented by adversarial perturbations; while proactive moderation detects 57-93% of attacks. Our work highlights a previously underexplored content-centric misuse vector for TTS and underscore the need for robust cross-modal safeguards throughout training and deployment.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†ç°ä»£å¤§è§„æ¨¡æ–‡æœ¬è½¬è¯­éŸ³(Text-to-Speech, TTS)ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯åŸºäºå¤§è§„æ¨¡éŸ³é¢‘è¯­è¨€æ¨¡å‹(Large Audio-Language Models, LALMs)çš„ç³»ç»Ÿåœ¨ç”Ÿæˆæœ‰å®³éŸ³é¢‘å†…å®¹æ–¹é¢çš„å®‰å…¨å¨èƒã€‚ä½œè€…æå‡ºäº† HARMGEN æ”»å‡»å¥—ä»¶ï¼Œé€šè¿‡è¯­ä¹‰æ¨¡ç³Š(Semantic Obfuscation)å’ŒéŸ³é¢‘æ¨¡æ€åˆ©ç”¨(Audio-modality exploits)ä¸¤å¤§ç±»æ”»å‡»æ–¹æ³•ï¼ŒæˆåŠŸç»•è¿‡äº†ç³»ç»Ÿçš„å®‰å…¨å¯¹é½å’Œè¾“å…¥è¾“å‡ºè¿‡æ»¤æœºåˆ¶ã€‚å®éªŒè¯„ä¼°æ¶µç›–äº†äº”ä¸ªå•†ä¸šåŒ– LALMs ç³»ç»Ÿï¼Œè¯æ˜äº† HARMGEN èƒ½æ˜¾è‘—é™ä½ç³»ç»Ÿçš„æ‹’ç»ç‡å¹¶å¢åŠ ç”ŸæˆéŸ³é¢‘çš„æ¯’æ€§ã€‚ç ”ç©¶è¿˜åˆ†æäº†ç°æœ‰çš„ä¸»åŠ¨ä¸è¢«åŠ¨é˜²å¾¡æ‰‹æ®µï¼ŒæŒ‡å‡ºæ·±åº¦ä¼ªé€ (Deepfake)æ£€æµ‹å™¨åœ¨é«˜è´¨é‡è¯­éŸ³ä¸Šè¡¨ç°æ¬ ä½³ï¼Œä¸”ç°æœ‰çš„å®¡æ ¸æµç¨‹æ˜“è¢«è§„é¿ã€‚è¯¥å·¥ä½œå¼ºè°ƒäº† TTS é¢†åŸŸä¸­æ­¤å‰è¢«å¿½è§†çš„ä»¥å†…å®¹ä¸ºä¸­å¿ƒçš„è¯¯ç”¨é£é™©ï¼Œå¹¶å‘¼ååœ¨æ¨¡å‹è®­ç»ƒå’Œéƒ¨ç½²ä¸­å»ºç«‹æ›´å®Œå–„çš„è·¨æ¨¡æ€å®‰å…¨é˜²æŠ¤(Cross-modal safeguards)ã€‚",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CR",
        "cs.MM",
        "eess.AS"
      ],
      "primary_category": "cs.SD",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10913v1",
      "published_date": "2025-11-14 03:00:04 UTC",
      "updated_date": "2025-11-14 03:00:04 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:24:17.881980+00:00"
    },
    {
      "arxiv_id": "2511.10912v1",
      "title": "Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D",
      "title_zh": "å¤§è¯­è¨€æ¨¡å‹åœ¨ç½•è§ç—…è¯Šæ–­ä¸­çš„è¯„ä¼°ï¼šåŸºäº House M.D çš„æ¡ˆä¾‹ç ”ç©¶",
      "authors": [
        "Arsh Gupta",
        "Ajay Narayanan Sridhar",
        "Bonam Mingole",
        "Amulya Yadav"
      ],
      "abstract": "Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.",
      "tldr_zh": "è¯¥ç ”ç©¶è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) åœ¨å¤„ç†å™è¿°æ€§åŒ»ç–—æ¡ˆä¾‹è¿›è¡Œç½•è§ç—…è¯Šæ–­ (Rare Disease Diagnosis) æ–¹é¢çš„èƒ½åŠ›ï¼Œå¡«è¡¥äº†è¯¥é¢†åŸŸç ”ç©¶çš„ç©ºç™½ã€‚ç ”ç©¶å›¢é˜Ÿä»ç»è¿‡åŒ»å­¦æ•™è‚²éªŒè¯çš„ç”µè§†å‰§ã€Šè±ªæ–¯åŒ»ç”Ÿã€‹(House M.D.) ä¸­æå–å¹¶æ„å»ºäº†ä¸€ä¸ªåŒ…å« 176 ä¸ªç—‡çŠ¶-è¯Šæ–­å¯¹ (Symptom-diagnosis pairs) çš„æ–°å‹æ•°æ®é›†ã€‚å®éªŒå¯¹ GPT-4o miniã€GPT-5 miniã€Gemini 2.5 Flash å’Œ Gemini 2.5 Pro ç­‰æ¨¡å‹è¿›è¡Œäº†è¯„ä¼°ï¼Œç»“æœæ˜¾ç¤ºå…¶è¯Šæ–­å‡†ç¡®ç‡åœ¨ 16.48% åˆ° 38.64% ä¹‹é—´ã€‚å°½ç®¡å½“å‰æ¨¡å‹åœ¨ç½•è§ç—…è¯Šæ–­ä¸­ä»é¢ä¸´ä¸¥å³»æŒ‘æˆ˜ï¼Œä½†æ–°ä¸€ä»£æ¨¡å‹ç›¸æ¯”æ—§ç‰ˆå®ç°äº† 2.3 å€çš„æ€§èƒ½æå‡ï¼Œå±•ç¤ºäº†æ˜¾è‘—çš„å‘å±•æ½œåŠ›ã€‚è¯¥ç ”ç©¶å»ºç«‹çš„ç»è¿‡æ•™è‚²éªŒè¯çš„åŸºå‡† (Benchmark) ä¸ºå™è¿°æ€§åŒ»ç–—æ¨ç†æä¾›äº†åŸºç¡€æ€§èƒ½æŒ‡æ ‡ï¼Œå¹¶ä¸ºæ¨åŠ¨ AI è¾…åŠ©è¯Šæ–­ç ”ç©¶æä¾›äº†ä¸€ä¸ªå…¬å¼€çš„è¯„ä¼°æ¡†æ¶ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10912v1",
      "published_date": "2025-11-14 02:54:58 UTC",
      "updated_date": "2025-11-14 02:54:58 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:24:20.538733+00:00"
    },
    {
      "arxiv_id": "2511.15722v1",
      "title": "Spatial Reasoning in Multimodal Large Language Models: A Survey of Tasks, Benchmarks and Methods",
      "title_zh": "å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„ç©ºé—´æ¨ç†ï¼šä»»åŠ¡ã€åŸºå‡†åŠæ–¹æ³•ç»¼è¿°",
      "authors": [
        "Weichen Liu",
        "Qiyao Xue",
        "Haoming Wang",
        "Xiangyu Yin",
        "Boyuan Yang",
        "Wei Gao"
      ],
      "abstract": "Spatial reasoning, which requires ability to perceive and manipulate spatial relationships in the 3D world, is a fundamental aspect of human intelligence, yet remains a persistent challenge for Multimodal large language models (MLLMs). While existing surveys often categorize recent progress based on input modality (e.g., text, image, video, or 3D), we argue that spatial ability is not solely determined by the input format. Instead, our survey introduces a taxonomy that organizes spatial intelligence from cognitive aspect and divides tasks in terms of reasoning complexity, linking them to several cognitive functions. We map existing benchmarks across text only, vision language, and embodied settings onto this taxonomy, and review evaluation metrics and methodologies for assessing spatial reasoning ability. This cognitive perspective enables more principled cross-task comparisons and reveals critical gaps between current model capabilities and human-like reasoning. In addition, we analyze methods for improving spatial ability, spanning both training-based and reasoning-based approaches. This dual perspective analysis clarifies their respective strengths, uncovers complementary mechanisms. By surveying tasks, benchmarks, and recent advances, we aim to provide new researchers with a comprehensive understanding of the field and actionable directions for future research.",
      "tldr_zh": "è¯¥ç»¼è¿°ç³»ç»Ÿåœ°æ¢è®¨äº†å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(Multimodal Large Language Models, MLLMs)ä¸­çš„ç©ºé—´æ¨ç†(Spatial reasoning)èƒ½åŠ›ï¼Œå¹¶æŒ‡å‡ºè¿™ä»æ˜¯å½“å‰æ¨¡å‹é¢ä¸´çš„ä¸€é¡¹æŒç»­æŒ‘æˆ˜ã€‚ç ”ç©¶è€…æå‡ºäº†ä¸€ç§åŸºäºè®¤çŸ¥è§†è§’çš„æ–°å‹åˆ†ç±»ä½“ç³»ï¼Œä»æ¨ç†å¤æ‚åº¦å‡ºå‘ç»„ç»‡ç©ºé—´æ™ºèƒ½ä»»åŠ¡ï¼Œå°†å…¶ä¸ç‰¹å®šçš„è®¤çŸ¥åŠŸèƒ½ç›¸å…³è”ã€‚è¯¥ç»¼è¿°å…¨é¢æ¢³ç†äº†æ¶µç›–çº¯æ–‡æœ¬ã€è§†è§‰è¯­è¨€å’Œå…·èº«æ™ºèƒ½(Embodied AI)åœºæ™¯çš„åŸºå‡†æµ‹è¯•(Benchmarks)ï¼Œå¹¶å¯¹è¯„ä¼°æŒ‡æ ‡å’Œæ–¹æ³•è®ºè¿›è¡Œäº†æ·±åº¦è¯„è¿°ã€‚é€šè¿‡è¿™ç§è®¤çŸ¥è§†è§’ï¼Œç ”ç©¶æ­ç¤ºäº†å½“å‰æ¨¡å‹æ€§èƒ½ä¸äººç±»æ°´å¹³ç©ºé—´æ¨ç†ä¹‹é—´çš„æ˜¾è‘—å·®è·ã€‚æ–‡ç« è¿›ä¸€æ­¥åˆ†æäº†æå‡ç©ºé—´èƒ½åŠ›çš„è®­ç»ƒåŸºç¡€(Training-based)ä¸æ¨ç†åŸºç¡€(Reasoning-based)æ–¹æ³•ï¼Œæ¢è®¨äº†å…¶å„è‡ªä¼˜åŠ¿ä¸äº’è¡¥æ½œåŠ›ã€‚è¿™é¡¹å·¥ä½œæ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜æä¾›è¯¥é¢†åŸŸçš„å…¨é¢ç†è§£ï¼Œå¹¶ä¸ºæœªæ¥ç ”ç©¶æŒ‡æ˜å…·æœ‰æ“ä½œæ€§çš„å‘å±•æ–¹å‘ã€‚",
      "categories": [
        "cs.AI"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.15722v1",
      "published_date": "2025-11-14 02:43:17 UTC",
      "updated_date": "2025-11-14 02:43:17 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:24:25.137206+00:00"
    },
    {
      "arxiv_id": "2511.10903v1",
      "title": "Automated Analysis of Learning Outcomes and Exam Questions Based on Bloom's Taxonomy",
      "title_zh": "åŸºäºå¸ƒé²å§†åˆ†ç±»æ³•çš„å­¦ä¹ æˆæœä¸è¯•é¢˜è‡ªåŠ¨åŒ–åˆ†æ",
      "authors": [
        "Ramya Kumar",
        "Dhruv Gulwani",
        "Sonit Singh"
      ],
      "abstract": "This paper explores the automatic classification of exam questions and learning outcomes according to Bloom's Taxonomy. A small dataset of 600 sentences labeled with six cognitive categories - Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation - was processed using traditional machine learning (ML) models (Naive Bayes, Logistic Regression, Support Vector Machines), recurrent neural network architectures (LSTM, BiLSTM, GRU, BiGRU), transformer-based models (BERT and RoBERTa), and large language models (OpenAI, Gemini, Ollama, Anthropic). Each model was evaluated under different preprocessing and augmentation strategies (for example, synonym replacement, word embeddings, etc.). Among traditional ML approaches, Support Vector Machines (SVM) with data augmentation achieved the best overall performance, reaching 94 percent accuracy, recall, and F1 scores with minimal overfitting. In contrast, the RNN models and BERT suffered from severe overfitting, while RoBERTa initially overcame it but began to show signs as training progressed. Finally, zero-shot evaluations of large language models (LLMs) indicated that OpenAI and Gemini performed best among the tested LLMs, achieving approximately 0.72-0.73 accuracy and comparable F1 scores. These findings highlight the challenges of training complex deep models on limited data and underscore the value of careful data augmentation and simpler algorithms (such as augmented SVM) for Bloom's Taxonomy classification.",
      "tldr_zh": "è¯¥ç ”ç©¶æ¢è®¨äº†åŸºäº Bloom's Taxonomy å¯¹è€ƒè¯•é¢˜ç›®å’Œå­¦ä¹ æˆæœè¿›è¡Œè‡ªåŠ¨åˆ†ç±»çš„æ–¹æ³•ï¼Œæ—¨åœ¨å°†æ–‡æœ¬åˆ’åˆ†ä¸º Knowledge, Comprehension, Application, Analysis, Synthesis å’Œ Evaluation å…­ä¸ªè®¤çŸ¥ç±»åˆ«ã€‚ç ”ç©¶å›¢é˜Ÿåœ¨åŒ…å« 600 ä¸ªå¥å­çš„å°å‹æ•°æ®é›†ä¸Šï¼Œå¯¹æ¯”äº†ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ (SVM, Naive Bayes ç­‰)ã€å¾ªç¯ç¥ç»ç½‘ç»œ (LSTM, GRU ç­‰)ã€Transformer æ¨¡å‹ (BERT, RoBERTa) ä»¥åŠå¤šç§å¤§è¯­è¨€æ¨¡å‹ (LLMs)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œç»“åˆæ•°æ®å¢å¼ºæŠ€æœ¯çš„ Support Vector Machines (SVM) è¡¨ç°æœ€ä¸ºä¼˜å¼‚ï¼Œåœ¨å‡†ç¡®ç‡ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ä¸Šå‡è¾¾åˆ°äº† 94%ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRNN å’Œ BERT ç­‰å¤æ‚æ·±åº¦æ¨¡å‹åœ¨æœ‰é™æ•°æ®ä¸‹å‡ºç°äº†ä¸¥é‡çš„ overfitting ç°è±¡ï¼Œè€Œ RoBERTa çš„è¡¨ç°ä¹Ÿéšè®­ç»ƒè¿›è¡Œè€Œä¸‹æ»‘ã€‚åœ¨ LLMs çš„ zero-shot è¯„ä¼°ä¸­ï¼ŒOpenAI å’Œ Gemini å–å¾—äº† 0.72-0.73 çš„å‡†ç¡®ç‡ï¼Œä½å±…æµ‹è¯•æ¨¡å‹å‰åˆ—ã€‚è¯¥ç ”ç©¶æœ€ç»ˆå¼ºè°ƒäº†åœ¨å¤„ç†æ­¤ç±»ç‰¹å®šæ•™è‚²åˆ†ç±»ä»»åŠ¡æ—¶ï¼Œé’ˆå¯¹å°è§„æ¨¡æ•°æ®è¿›è¡Œæœ‰æ•ˆçš„æ•°æ®å¢å¼ºä»¥åŠé€‰ç”¨è¾ƒç®€å•çš„ç®—æ³•ï¼ˆå¦‚å¢å¼ºå‹ SVMï¼‰æ¯”ç›²ç›®ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹æ›´å…·ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "7 Pages",
      "pdf_url": "https://arxiv.org/pdf/2511.10903v1",
      "published_date": "2025-11-14 02:31:12 UTC",
      "updated_date": "2025-11-14 02:31:12 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:24:28.334368+00:00"
    },
    {
      "arxiv_id": "2511.10900v2",
      "title": "Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering",
      "title_zh": "é¢å‘æ€¥æ•‘åŒ»ç–—æœåŠ¡é—®ç­”çš„ä¸“å®¶å¼•å¯¼æç¤ºä¸æ£€ç´¢å¢å¼ºç”Ÿæˆ",
      "authors": [
        "Xueren Ge",
        "Sahil Murtaza",
        "Anthony Cortez",
        "Homa Alemzadeh"
      ],
      "abstract": "Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨åŒ»ç–—é—®ç­”ä¸­å¸¸å¿½è§†é¢†åŸŸç‰¹å®šä¸“ä¸šçŸ¥è¯†çš„é—®é¢˜ï¼Œæ¨å‡ºäº† EMSQAï¼Œä¸€ä¸ªåŒ…å« 2.43 ä¸‡é“å¤šé€‰é¢˜ã€æ¶µç›– 10 ä¸ªä¸´åºŠé¢†åŸŸå’Œ 4 ä¸ªè®¤è¯çº§åˆ«çš„ç´§æ€¥åŒ»ç–—æœåŠ¡æ•°æ®é›†ï¼Œå¹¶é™„å¸¦ 4 ä¸‡ä»½é¢†åŸŸå¯¹é½çš„çŸ¥è¯†åº“ã€‚ç ”ç©¶å›¢é˜ŸåŸºäºæ­¤æå‡ºäº† Expert-CoT æç¤ºç­–ç•¥ï¼Œé€šè¿‡åœ¨ Chain-of-Thought æ¨ç†ä¸­åŠ å…¥ç‰¹å®šä¸´åºŠé¢†åŸŸå’Œè®¤è¯çº§åˆ«ä½œä¸ºçº¦æŸæ¡ä»¶æ¥å¼•å¯¼æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç ”ç©¶å¼•å…¥äº† ExpertRAG æ£€ç´¢å¢å¼ºç”Ÿæˆæµæ°´çº¿ï¼Œå°†å›å¤å†…å®¹é”šå®šåœ¨ä¸ä¸»é¢˜å¯¹é½çš„æ–‡æ¡£åŠçœŸå®æ‚£è€…æ•°æ®ä¸­ã€‚å®éªŒè¯æ˜ï¼ŒExpert-CoT ç›¸æ¯”ä¼ ç»Ÿ CoT å‡†ç¡®ç‡æå‡è¾¾ 2.05%ï¼Œè€Œ Expert-CoT ä¸ ExpertRAG çš„ç»“åˆåˆ™æ¯”æ ‡å‡† RAG åŸºå‡†æå‡äº† 4.59%ã€‚æœ€ç»ˆï¼Œä¸“å®¶çŸ¥è¯†å¢å¼ºåçš„ 32B è§„æ¨¡æ¨¡å‹æˆåŠŸé€šè¿‡äº†æ‰€æœ‰è®¡ç®—æœºè‡ªé€‚åº” EMS è®¤è¯æ¨¡æ‹Ÿè€ƒè¯•ï¼Œå±•ç°äº†å…¶åœ¨å¤æ‚åŒ»ç–—å†³ç­–åœºæ™¯ä¸‹çš„å“è¶Šæ€§èƒ½ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted by AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.10900v2",
      "published_date": "2025-11-14 02:21:48 UTC",
      "updated_date": "2025-11-18 21:50:44 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:24:29.435676+00:00"
    },
    {
      "arxiv_id": "2511.10896v1",
      "title": "CLIPPan: Adapting CLIP as A Supervisor for Unsupervised Pansharpening",
      "title_zh": "CLIPPanï¼šå°† CLIP é€‚é…ä¸ºæ— ç›‘ç£å…¨è‰²é”åŒ–çš„ç›‘ç£æœºåˆ¶",
      "authors": [
        "Lihua Jian",
        "Jiabo Liu",
        "Shaowu Wu",
        "Lihui Chen"
      ],
      "abstract": "Despite remarkable advancements in supervised pansharpening neural networks, these methods face domain adaptation challenges of resolution due to the intrinsic disparity between simulated reduced-resolution training data and real-world full-resolution scenarios.To bridge this gap, we propose an unsupervised pansharpening framework, CLIPPan, that enables model training at full resolution directly by taking CLIP, a visual-language model, as a supervisor. However, directly applying CLIP to supervise pansharpening remains challenging due to its inherent bias toward natural images and limited understanding of pansharpening tasks. Therefore, we first introduce a lightweight fine-tuning pipeline that adapts CLIP to recognize low-resolution multispectral, panchromatic, and high-resolution multispectral images, as well as to understand the pansharpening process. Then, building on the adapted CLIP, we formulate a novel \\textit{loss integrating semantic language constraints}, which aligns image-level fusion transitions with protocol-aligned textual prompts (e.g., Wald's or Khan's descriptions), thus enabling CLIPPan to use language as a powerful supervisory signal and guide fusion learning without ground truth. Extensive experiments demonstrate that CLIPPan consistently improves spectral and spatial fidelity across various pansharpening backbones on real-world datasets, setting a new state of the art for unsupervised full-resolution pansharpening.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›‘ç£å­¦ä¹ å¼å…¨è‰²é”åŒ–(Supervised Pansharpening)ç¥ç»ç½‘ç»œåœ¨å¤„ç†çœŸå®åœºæ™¯æ—¶é¢ä¸´çš„åˆ†è¾¨ç‡é¢†åŸŸè‡ªé€‚åº”æŒ‘æˆ˜ï¼Œæå‡ºäº†åä¸ºCLIPPançš„æ— ç›‘ç£æ¡†æ¶ã€‚CLIPPané€šè¿‡å¼•å…¥è§†è§‰è¯­è¨€æ¨¡å‹CLIPä½œä¸ºç›‘ç£è€…ï¼Œå®ç°äº†ç›´æ¥åœ¨å…¨åˆ†è¾¨ç‡å›¾åƒä¸Šçš„æ¨¡å‹è®­ç»ƒï¼Œä»è€Œæœ‰æ•ˆå¼¥åˆäº†æ¨¡æ‹Ÿæ•°æ®ä¸çœŸå®æ•°æ®é—´çš„å·®è·ã€‚ä¸ºè§£å†³CLIPå¯¹å…¨è‰²é”åŒ–ä»»åŠ¡ç†è§£ä¸è¶³çš„é—®é¢˜ï¼Œç ”ç©¶å›¢é˜Ÿè®¾è®¡äº†è½»é‡åŒ–å¾®è°ƒæµæ°´çº¿ï¼Œå¹¶æå‡ºä¸€ç§èåˆè¯­ä¹‰è¯­è¨€çº¦æŸçš„æŸå¤±å‡½æ•°ï¼Œå°†å›¾åƒèåˆè½¬æ¢è¿‡ç¨‹ä¸æ–‡æœ¬æç¤ºè¿›è¡Œå¯¹é½ã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨è¯­è¨€ä¿¡å·ä½œä¸ºå¼ºå¤§çš„ç›‘ç£æ‰‹æ®µï¼Œåœ¨æ— éœ€åœ°é¢çœŸå€¼(Ground Truth)çš„æƒ…å†µä¸‹å¼•å¯¼æ¨¡å‹è¿›è¡Œé«˜æ•ˆçš„èåˆå­¦ä¹ ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒCLIPPanåœ¨å¤šç§éª¨å¹²ç½‘ç»œä¸Šå‡æ˜¾è‘—æå‡äº†å…‰è°±å’Œç©ºé—´ä¿çœŸåº¦ï¼Œåœ¨çœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè¾¾åˆ°äº†æ— ç›‘ç£å…¨åˆ†è¾¨ç‡å…¨è‰²é”åŒ–é¢†åŸŸçš„é¢†å…ˆæ°´å¹³ã€‚",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV"
      ],
      "primary_category": "eess.IV",
      "comment": "Accepted to AAAI 2026",
      "pdf_url": "https://arxiv.org/pdf/2511.10896v1",
      "published_date": "2025-11-14 02:16:19 UTC",
      "updated_date": "2025-11-14 02:16:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:24:36.344885+00:00"
    },
    {
      "arxiv_id": "2511.10894v2",
      "title": "DINOv3 as a Frozen Encoder for CRPS-Oriented Probabilistic Rainfall Nowcasting",
      "title_zh": "DINOv3 ä½œä¸ºé¢å‘ CRPS æ¦‚ç‡æ€§é™æ°´ä¸´è¿‘é¢„æŠ¥çš„å†»ç»“ç¼–ç å™¨",
      "authors": [
        "Luciano Araujo Dourado Filho",
        "Almir Moreira da Silva Neto",
        "Anthony Miyaguchi",
        "Rodrigo Pereira David",
        "Rodrigo Tripodi Calumby",
        "LukÃ¡Å¡ Picek"
      ],
      "abstract": "This paper proposes a competitive and computationally efficient approach to probabilistic rainfall nowcasting. A video projector (V-JEPA Vision Transformer) associated to a lightweight probabilistic head is attached to a pre-trained satellite vision encoder (DINOv3-SAT493M) to map encoder tokens into a discrete empirical CDF (eCDF) over 4-hour accumulated rainfall. The projector-head is optimized end-to-end over the Ranked Probability Score (RPS). As an alternative, 3D-UNET baselines trained with an aggregate Rank Probability Score and a per-pixel Gamma-Hurdle objective are used. On the Weather4Cast 2025 benchmark, the proposed method achieved a promising performance, with a CRPS of 3.5102, which represents $\\approx$ 26% in effectiveness gain against the best 3D-UNET.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹æ¦‚ç‡æ€§é™æ°´ä¸´è¿‘é¢„æŠ¥(probabilistic rainfall nowcasting)æå‡ºäº†ä¸€ç§æå…·ç«äº‰åŠ›ä¸”è®¡ç®—æ•ˆç‡é«˜çš„æ–¹æ¡ˆã€‚è¯¥æ–¹æ³•é‡‡ç”¨é¢„è®­ç»ƒçš„å«æ˜Ÿè§†è§‰ç¼–ç å™¨ DINOv3-SAT493M ä½œä¸ºå†»ç»“ç¼–ç å™¨ï¼Œå¹¶ç»“åˆè§†é¢‘æŠ•å½±ä»ª(V-JEPA Vision Transformer)ä¸è½»é‡çº§æ¦‚ç‡é¢„æµ‹å¤´(probabilistic head)ï¼Œå°†ç¼–ç å™¨ä»¤ç‰Œæ˜ å°„ä¸º4å°æ—¶ç´¯ç§¯é™æ°´çš„ç¦»æ•£ç»éªŒç´¯ç§¯åˆ†å¸ƒå‡½æ•°(eCDF)ã€‚æ•´ä¸ªæŠ•å½±æ¶æ„åœ¨åˆ†çº§æ¦‚ç‡è¯„åˆ†(Ranked Probability Score, RPS)çš„åŸºç¡€ä¸Šè¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–ï¼Œä»¥ç¡®ä¿é¢„æµ‹ç»“æœçš„æ¦‚ç‡ä¸€è‡´æ€§ã€‚åœ¨ Weather4Cast 2025 åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¯¥æ–¹æ³•å–å¾—äº† 3.5102 çš„è¿ç»­åˆ†çº§æ¦‚ç‡è¯„åˆ†(CRPS)ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œä¸æœ€ä¼˜çš„ 3D-UNET åŸºçº¿æ¨¡å‹ç›¸æ¯”ï¼Œè¯¥æ–¹æ¡ˆå®ç°äº†çº¦ 26% çš„æœ‰æ•ˆæ€§å¢ç›Šã€‚è¿™è¯æ˜äº†åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒè§†è§‰æ¨¡å‹ç»“åˆè½»é‡çº§é¢„æµ‹å¤´åœ¨å¤„ç†å¤æ‚æ°”è±¡é¢„æµ‹ä»»åŠ¡ä¸­çš„æ˜¾è‘—ä¼˜åŠ¿ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10894v2",
      "published_date": "2025-11-14 02:14:08 UTC",
      "updated_date": "2025-11-19 17:48:19 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:24:37.432409+00:00"
    },
    {
      "arxiv_id": "2511.10892v1",
      "title": "MCN-CL: Multimodal Cross-Attention Network and Contrastive Learning for Multimodal Emotion Recognition",
      "title_zh": "MCN-CLï¼šèåˆå¤šæ¨¡æ€äº¤å‰æ³¨æ„åŠ›ç½‘ç»œä¸å¯¹æ¯”å­¦ä¹ çš„å¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«",
      "authors": [
        "Feng Li",
        "Ke Wu",
        "Yongwei Li"
      ],
      "abstract": "Multimodal emotion recognition plays a key role in many domains, including mental health monitoring, educational interaction, and human-computer interaction. However, existing methods often face three major challenges: unbalanced category distribution, the complexity of dynamic facial action unit time modeling, and the difficulty of feature fusion due to modal heterogeneity. With the explosive growth of multimodal data in social media scenarios, the need for building an efficient cross-modal fusion framework for emotion recognition is becoming increasingly urgent. To this end, this paper proposes Multimodal Cross-Attention Network and Contrastive Learning (MCN-CL) for multimodal emotion recognition. It uses a triple query mechanism and hard negative mining strategy to remove feature redundancy while preserving important emotional cues, effectively addressing the issues of modal heterogeneity and category imbalance. Experiment results on the IEMOCAP and MELD datasets show that our proposed method outperforms state-of-the-art approaches, with Weighted F1 scores improving by 3.42% and 5.73%, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº† MCN-CL (Multimodal Cross-Attention Network and Contrastive Learning)ï¼Œä¸€ç§ä¸“ä¸ºå¤šæ¨¡æ€æƒ…æ„Ÿè¯†åˆ«è®¾è®¡çš„è·¨æ³¨æ„åŠ›ç½‘ç»œä¸å¯¹æ¯”å­¦ä¹ æ¡†æ¶ã€‚è¯¥æ–¹æ³•æ—¨åœ¨åº”å¯¹ç±»åˆ«åˆ†å¸ƒä¸å¹³è¡¡ (unbalanced category distribution)ã€åŠ¨æ€é¢éƒ¨åŠ¨ä½œå•å…ƒ (facial action unit) å»ºæ¨¡å¤æ‚ä»¥åŠæ¨¡æ€å¼‚æ„æ€§ (modal heterogeneity) å¸¦æ¥çš„ç‰¹å¾èåˆæŒ‘æˆ˜ã€‚é€šè¿‡é‡‡ç”¨ä¸‰é‡æŸ¥è¯¢æœºåˆ¶ (triple query mechanism) å’Œéš¾è´Ÿæ ·æœ¬æŒ–æ˜ç­–ç•¥ (hard negative mining strategy)ï¼ŒMCN-CL èƒ½å¤Ÿæœ‰æ•ˆæ¶ˆé™¤ç‰¹å¾å†—ä½™å¹¶ä¿ç•™æ ¸å¿ƒæƒ…æ„Ÿçº¿ç´¢ï¼Œä»è€Œè§£å†³äº†æ¨¡æ€å¼‚æ„å’Œç±»åˆ«ä¸å‡çš„é—®é¢˜ã€‚åœ¨ IEMOCAP å’Œ MELD æ•°æ®é›†ä¸Šçš„å®éªŒè¡¨æ˜ï¼Œè¯¥æ–¹æ³•çš„ Weighted F1 åˆ†æ•°åˆ†åˆ«æå‡äº† 3.42% å’Œ 5.73%ï¼Œæ€§èƒ½æ˜¾è‘—ä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æŠ€æœ¯ã€‚è¿™ä¸€æˆæœè¯æ˜äº† MCN-CL åœ¨å¤„ç†å¤æ‚ç¤¾äº¤åª’ä½“å¤šæ¨¡æ€æ•°æ®å’Œæå‡æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®æ€§æ–¹é¢çš„å“è¶Šæ•ˆèƒ½ï¼Œä¸ºæ„å»ºé«˜æ•ˆçš„è·¨æ¨¡æ€èåˆæ¡†æ¶æä¾›äº†æ–°æ€è·¯ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "Accepted by 32nd International Conference on MultiMedia Modeling (MMM 2026)",
      "pdf_url": "https://arxiv.org/pdf/2511.10892v1",
      "published_date": "2025-11-14 02:13:31 UTC",
      "updated_date": "2025-11-14 02:13:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:24:42.743764+00:00"
    },
    {
      "arxiv_id": "2511.10890v1",
      "title": "LLM enhanced graph inference for long-term disease progression modelling",
      "title_zh": "é¢å‘é•¿æœŸç–¾ç—…è¿›å±•å»ºæ¨¡çš„å¤§è¯­è¨€æ¨¡å‹å¢å¼ºå›¾æ¨ç†",
      "authors": [
        "Tiantian He",
        "An Zhao",
        "Elinor Thompson",
        "Anna Schroder",
        "Ahmed Abdulaal",
        "Frederik Barkhof",
        "Daniel C. Alexander"
      ],
      "abstract": "Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Models, LLMsï¼‰ä½œä¸ºä¸“å®¶æŒ‡å¯¼æ¥å¢å¼ºå›¾æ¨ç†çš„æ–°å‹æ¡†æ¶ï¼Œæ—¨åœ¨æ”¹è¿›ç¥ç»é€€è¡Œæ€§ç–¾ç—…çš„é•¿æœŸè¿›å±•å»ºæ¨¡ã€‚é’ˆå¯¹å½“å‰æ–¹æ³•åœ¨å¤§è„‘è¿æ¥æ€§è¡¨ç¤ºä¸Šçš„è¿‡åº¦ç®€åŒ–ä»¥åŠçº¯æ•°æ®é©±åŠ¨å­¦ä¹ ä¸­çš„æ ‡è¯†æ€§ï¼ˆidentifiabilityï¼‰éš¾é¢˜ï¼Œè¯¥æ¡†æ¶é€šè¿‡ LLMs ç»¼åˆå¤šæ¨¡æ€å…³ç³»å’Œå¤šç§ç–¾ç—…é©±åŠ¨æœºåˆ¶ï¼Œä»è€Œä»ä¸è§„åˆ™é‡‡æ ·çš„çºµå‘æ‚£è€…æ•°æ®ä¸­å­¦ä¹ ç–¾ç—…è¿›å±•ã€‚è¯¥æ–¹æ³•å®ç°äº†å¯¹ä¸ªä½“æ°´å¹³é•¿æœŸç–¾ç—…è½¨è¿¹æ„å»ºä¸æ•æ‰è„‘åŒºäº¤äº’çš„ç”Ÿç‰©çº¦æŸå›¾ç»“æ„çš„åŒæ­¥ä¼˜åŒ–ã€‚åœ¨é˜¿å°”èŒ¨æµ·é»˜ç—…ï¼ˆAlzheimer's Disease, ADï¼‰é˜Ÿåˆ—çš„ tau-PET å½±åƒæ•°æ®å®éªŒä¸­ï¼Œè¯¥æ¡†æ¶å±•ç°å‡ºä¼˜äºä¼ ç»Ÿæ–¹æ³•çš„é¢„æµ‹å‡†ç¡®æ€§å’Œå¯è§£é‡Šæ€§ã€‚ç ”ç©¶ç»“æœä¸ä»…æå‡äº†ç—…ç†ä¼ æ’­ä¼°ç®—çš„ç²¾ç¡®åº¦ï¼Œè¿˜æ­ç¤ºäº†ä¼ ç»Ÿè¿æ¥æ€§æµ‹é‡ä¹‹å¤–çš„é¢å¤–ç–¾ç—…é©±åŠ¨å› ç´ ã€‚",
      "categories": [
        "cs.AI",
        "stat.ML"
      ],
      "primary_category": "cs.AI",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10890v1",
      "published_date": "2025-11-14 02:03:10 UTC",
      "updated_date": "2025-11-14 02:03:10 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:25:01.939301+00:00"
    },
    {
      "arxiv_id": "2511.13752v1",
      "title": "Motor Imagery Classification Using Feature Fusion of Spatially Weighted Electroencephalography",
      "title_zh": "åŸºäºç©ºé—´åŠ æƒè„‘ç”µç‰¹å¾èåˆçš„è¿åŠ¨æƒ³è±¡åˆ†ç±»",
      "authors": [
        "Abdullah Al Shiam",
        "Md. Khademul Islam Molla",
        "Abu Saleh Musa Miah",
        "Md. Abdus Samad Kamal"
      ],
      "abstract": "A Brain Computer Interface (BCI) connects the human brain to the outside world, providing a direct communication channel. Electroencephalography (EEG) signals are commonly used in BCIs to reflect cognitive patterns related to motor function activities. However, due to the multichannel nature of EEG signals, explicit information processing is crucial to lessen computational complexity in BCI systems. This study proposes an innovative method based on brain region-specific channel selection and multi-domain feature fusion to improve classification accuracy. The novelty of the proposed approach lies in region-based channel selection, where EEG channels are grouped according to their functional relevance to distinct brain regions. By selecting channels based on specific regions involved in motor imagery (MI) tasks, this technique eliminates irrelevant channels, reducing data dimensionality and improving computational efficiency. This also ensures that the extracted features are more reflective of the brain actual activity related to motor tasks. Three distinct feature extraction methods Common Spatial Pattern (CSP), Fuzzy C-means clustering, and Tangent Space Mapping (TSM), are applied to each group of channels based on their brain region. Each method targets different characteristics of the EEG signal: CSP focuses on spatial patterns, Fuzzy C means identifies clusters within the data, and TSM captures non-linear patterns in the signal. The combined feature vector is used to classify motor imagery tasks (left hand, right hand, and right foot) using Support Vector Machine (SVM). The proposed method was validated on publicly available benchmark EEG datasets (IVA and I) from the BCI competition III and IV. The results show that the approach outperforms existing methods, achieving classification accuracies of 90.77% and 84.50% for datasets IVA and I, respectively.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºç‰¹å®šå¤§è„‘åŒºåŸŸé€šé“é€‰æ‹©å’Œå¤šåŸŸç‰¹å¾èåˆçš„åˆ›æ–°æ–¹æ³•ï¼Œæ—¨åœ¨æé«˜è¿åŠ¨æƒ³è±¡(Motor Imagery)ä»»åŠ¡çš„åˆ†ç±»å‡†ç¡®ç‡å¹¶é™ä½è®¡ç®—å¤æ‚åº¦ã€‚é€šè¿‡å°†Electroencephalography (EEG)é€šé“æŒ‰åŠŸèƒ½åŒºåŸŸåˆ†ç»„å¹¶ç­›é€‰ç›¸å…³é€šé“ï¼Œè¯¥æŠ€æœ¯æœ‰æ•ˆå‡å°‘äº†æ•°æ®ç»´åº¦å¹¶çªå‡ºäº†ä¸è¿åŠ¨ä»»åŠ¡ç›´æ¥ç›¸å…³çš„è„‘ç”µç‰¹å¾ã€‚ç ”ç©¶ç»“åˆäº†Common Spatial Pattern (CSP)ã€Fuzzy C-meansèšç±»å’ŒTangent Space Mapping (TSM)ä¸‰ç§æ–¹æ³•æ¥æå–ç©ºé—´ã€èšç±»åŠéçº¿æ€§ç‰¹å¾ï¼Œéšååˆ©ç”¨æ”¯æŒå‘é‡æœº(SVM)å¯¹å¤šé¡¹è¿åŠ¨æƒ³è±¡ä»»åŠ¡è¿›è¡Œåˆ†ç±»ã€‚åœ¨BCI Competition IIIå’ŒIVçš„å…¬å¼€åŸºå‡†æ•°æ®é›†ä¸Šçš„å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ–¹æ³•åœ¨æ•°æ®é›†IVAå’ŒIä¸Šåˆ†åˆ«å®ç°äº†90.77%å’Œ84.50%çš„åˆ†ç±»å‡†ç¡®ç‡ã€‚è¯¥ç»“æœè¯æ˜è¯¥æ–¹æ³•åœ¨åˆ†ç±»æ€§èƒ½ä¸Šä¼˜äºç°æœ‰æŠ€æœ¯ï¼Œä¸ºæ„å»ºæ›´é«˜æ•ˆã€ç²¾ç¡®çš„è„‘æœºæ¥å£(BCI)ç³»ç»Ÿæä¾›äº†æœ‰åŠ›æ”¯æŒã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.13752v1",
      "published_date": "2025-11-14 01:36:08 UTC",
      "updated_date": "2025-11-14 01:36:08 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:25:05.536693+00:00"
    },
    {
      "arxiv_id": "2511.10881v1",
      "title": "A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge",
      "title_zh": "ä»å‚æ•°åŒ–çŸ¥è¯†è§†è§’å¯¹å¤§è¯­è¨€æ¨¡å‹å¦å®šåè§çš„å¤šç»´åˆ†æ",
      "authors": [
        "Jongyoon Song",
        "Sangwon Yu",
        "Sungroh Yoon"
      ],
      "abstract": "Negative bias refers to the tendency of large language models (LLMs) to excessively generate negative responses in binary decision tasks (e.g., yes-no question answering). Previous research has focused on detecting and addressing negative attention heads that induce negative bias. However, the underlying detailed factors influencing negative bias remain underexplored. In this paper, we demonstrate that LLMs exhibit format-level negative bias, meaning the prompt format more influences their responses than the semantics of the negative response. For the fine-grained study of the negative bias, we introduce a pipeline for constructing the evaluation set, which systematically categorizes the dataset into three subsets based on the model's parametric knowledge: correct, incorrect, and insufficient relevant knowledge. Through analysis of this evaluation set, we identify a shortcut behavior in which models tend to generate negative responses when they lack sufficient knowledge to answer a yes-no question, leading to negative bias. We further examine how negative bias changes under various prompting scenarios related to parametric knowledge. We observe that providing relevant context and offering an \"I don't know\" option generally reduces negative bias, whereas chain-of-thought prompting tends to amplify the bias. Finally, we demonstrate that the degree of negative bias can vary depending on the type of prompt, which influences the direction of the response. Our work reveals the various factors that influence negative bias, providing critical insights for mitigating it in LLMs.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹ (LLMs) åœ¨äºŒå…ƒå†³ç­–ä»»åŠ¡ä¸­è¿‡åº¦ç”Ÿæˆå¦å®šå›ç­”çš„ Negative bias ç°è±¡è¿›è¡Œäº†å¤šç»´åº¦åˆ†æã€‚ç ”ç©¶å‘ç° LLMs è¡¨ç°å‡º Format-level negative biasï¼Œå³æç¤ºè¯æ ¼å¼ç›¸æ¯”å¦å®šè¯­ä¹‰å¯¹æ¨¡å‹å›ç­”çš„å½±å“æ›´å¤§ã€‚ä¸ºäº†è¿›è¡Œç»†ç²’åº¦ç ”ç©¶ï¼Œä½œè€…æå‡ºäº†ä¸€å¥—æ ¹æ® Parametric knowledgeï¼ˆæ­£ç¡®ã€é”™è¯¯åŠçŸ¥è¯†ä¸è¶³ï¼‰å¯¹è¯„ä¼°é›†è¿›è¡Œç³»ç»Ÿåˆ†ç±»çš„æ„å»ºæµç¨‹ã€‚åˆ†ææ­ç¤ºäº†æ¨¡å‹åœ¨ç¼ºä¹è¶³å¤ŸçŸ¥è¯†å›ç­” Yes-no question æ—¶å€¾å‘äºäº§ç”Ÿå¦å®šå›ç­”çš„ Shortcut behaviorï¼Œè¿™æ˜¯å¯¼è‡´ Negative bias çš„é‡è¦åŸå› ã€‚ç ”ç©¶è¿˜é€šè¿‡å¤šç§æç¤ºåœºæ™¯æµ‹è¯•å‘ç°ï¼Œæä¾›ç›¸å…³ä¸Šä¸‹æ–‡æˆ–å¢åŠ â€œI don't knowâ€é€‰é¡¹èƒ½æœ‰æ•ˆç¼“è§£åè§ï¼Œè€Œ Chain-of-thought æç¤ºåˆ™å¾€å¾€ä¼šæ”¾å¤§åè§ã€‚è¯¥é¡¹å·¥ä½œç³»ç»Ÿæ€§åœ°æ­ç¤ºäº†å½±å“ Negative bias çš„å¤šç§å› ç´ ï¼Œä¸ºæœªæ¥ç¼“è§£ LLMs ä¸­çš„æ­¤ç±»åè§æä¾›äº†å…³é”®è§è§£ã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "Accepted to IEEE Transactions on Audio, Speech and Language Processing",
      "pdf_url": "https://arxiv.org/pdf/2511.10881v1",
      "published_date": "2025-11-14 01:18:18 UTC",
      "updated_date": "2025-11-14 01:18:18 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:25:03.334552+00:00"
    },
    {
      "arxiv_id": "2511.10872v1",
      "title": "Incorporating Spatial Information into Goal-Conditioned Hierarchical Reinforcement Learning via Graph Representations",
      "title_zh": "é€šè¿‡å›¾è¡¨ç¤ºå°†ç©ºé—´ä¿¡æ¯èå…¥ç›®æ ‡æ¡ä»¶å±‚çº§å¼ºåŒ–å­¦ä¹ ",
      "authors": [
        "Shuyuan Zhang",
        "Zihan Wang",
        "Xiao-Wen Chang",
        "Doina Precup"
      ],
      "abstract": "The integration of graphs with Goal-conditioned Hierarchical Reinforcement Learning (GCHRL) has recently gained attention, as intermediate goals (subgoals) can be effectively sampled from graphs that naturally represent the overall task structure in most RL tasks. However, existing approaches typically rely on domain-specific knowledge to construct these graphs, limiting their applicability to new tasks. Other graph-based approaches create graphs dynamically during exploration but struggle to fully utilize them, because they have problems passing the information in the graphs to newly visited states. Additionally, current GCHRL methods face challenges such as sample inefficiency and poor subgoal representation. This paper proposes a solution to these issues by developing a graph encoder-decoder to evaluate unseen states. Our proposed method, Graph-Guided sub-Goal representation Generation RL (G4RL), can be incorporated into any existing GCHRL method when operating in environments with primarily symmetric and reversible transitions to enhance performance across this class of problems. We show that the graph encoder-decoder can be effectively implemented using a network trained on the state graph generated during exploration. Empirical results indicate that leveraging high and low-level intrinsic rewards from the graph encoder-decoder significantly enhances the performance of state-of-the-art GCHRL approaches with an extra small computational cost in dense and sparse reward environments.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹ç›®æ ‡é©±åŠ¨çš„åˆ†å±‚å¼ºåŒ–å­¦ä¹ (Goal-conditioned Hierarchical Reinforcement Learning, GCHRL)ä¸­å›¾è¡¨ç¤ºåˆ©ç”¨ä¸è¶³ã€æ ·æœ¬æ•ˆç‡ä½ä»¥åŠå­ç›®æ ‡è¡¨ç¤ºè¾ƒå·®ç­‰æŒ‘æˆ˜ï¼Œæå‡ºäº†G4RL (Graph-Guided sub-Goal representation Generation RL)æ¡†æ¶ã€‚è¯¥æ–¹æ³•æ ¸å¿ƒåœ¨äºæ„å»ºäº†ä¸€ä¸ªåŸºäºæ¢ç´¢è¿‡ç¨‹ä¸­ç”Ÿæˆçš„å›¾è¿›è¡Œè®­ç»ƒçš„å›¾ç¼–ç å™¨-è§£ç å™¨(graph encoder-decoder)ï¼Œä»è€Œå®ç°å¯¹æœªè§çŠ¶æ€çš„æœ‰æ•ˆè¯„ä¼°å¹¶æå–ç©ºé—´ä¿¡æ¯ã€‚G4RLå¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰çš„GCHRLæ–¹æ³•ä¸­ï¼Œç‰¹åˆ«é€‚ç”¨äºå…·æœ‰å¯¹ç§°å’Œå¯é€†è½¬ç§»ç‰¹å¾çš„ä»»åŠ¡ç¯å¢ƒã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåˆ©ç”¨è¯¥å›¾ç¼–ç å™¨-è§£ç å™¨ç”Ÿæˆçš„é«˜ä½å±‚å†…åœ¨å¥–åŠ±(intrinsic rewards)ï¼Œèƒ½åœ¨ä¿æŒæå°è®¡ç®—å¼€é”€çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡æœ€å…ˆè¿›GCHRLç®—æ³•åœ¨å¯†é›†å’Œç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­çš„æ€§èƒ½ã€‚",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "primary_category": "cs.LG",
      "comment": "Transactions on Machine Learning Research (2025)",
      "pdf_url": "https://arxiv.org/pdf/2511.10872v1",
      "published_date": "2025-11-14 00:58:39 UTC",
      "updated_date": "2025-11-14 00:58:39 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:25:05.937058+00:00"
    },
    {
      "arxiv_id": "2511.10866v1",
      "title": "Short-Window Sliding Learning for Real-Time Violence Detection via LLM-based Auto-Labeling",
      "title_zh": "åŸºäº LLM è‡ªåŠ¨æ ‡æ³¨çš„çŸ­çª—å£æ»‘åŠ¨å­¦ä¹ å®æ—¶æš´åŠ›æ£€æµ‹",
      "authors": [
        "Seoik Jung",
        "Taekyung Song",
        "Yangro Lee",
        "Sungjun Lee"
      ],
      "abstract": "This paper proposes a Short-Window Sliding Learning framework for real-time violence detection in CCTV footages. Unlike conventional long-video training approaches, the proposed method divides videos into 1-2 second clips and applies Large Language Model (LLM)-based auto-caption labeling to construct fine-grained datasets. Each short clip fully utilizes all frames to preserve temporal continuity, enabling precise recognition of rapid violent events. Experiments demonstrate that the proposed method achieves 95.25\\% accuracy on RWF-2000 and significantly improves performance on long videos (UCF-Crime: 83.25\\%), confirming its strong generalization and real-time applicability in intelligent surveillance systems.",
      "tldr_zh": "è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§åä¸ºShort-Window Sliding Learningçš„æ¡†æ¶ï¼Œç”¨äºCCTVç›‘æ§ä¸­çš„å®æ—¶æš´åŠ›æ£€æµ‹ã€‚ä¸åŒäºä¼ ç»Ÿçš„é•¿è§†é¢‘è®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å°†è§†é¢‘åˆ‡åˆ†ä¸º1-2ç§’çš„çŸ­ç‰‡æ®µï¼Œå¹¶åˆ©ç”¨åŸºäºLarge Language Model (LLM) çš„è‡ªåŠ¨æ ‡æ³¨æŠ€æœ¯æ„å»ºç»†ç²’åº¦æ•°æ®é›†ã€‚æ¯ä¸ªçŸ­ç‰‡æ®µé€šè¿‡åˆ©ç”¨æ‰€æœ‰è§†é¢‘å¸§æ¥ä¿æŒæ—¶é—´è¿ç»­æ€§ï¼Œä»è€Œå®ç°å¯¹å¿«é€Ÿæš´åŠ›äº‹ä»¶çš„ç²¾ç¡®è¯†åˆ«ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•åœ¨RWF-2000æ•°æ®é›†ä¸Šè¾¾åˆ°äº†95.25%çš„å‡†ç¡®ç‡ï¼Œå¹¶åœ¨é•¿è§†é¢‘æ•°æ®é›†UCF-Crimeä¸Šå–å¾—äº†83.25%çš„å‡†ç¡®ç‡ã€‚è¿™è¯æ˜äº†è¯¥æ¡†æ¶åœ¨æ™ºèƒ½ç›‘æ§ç³»ç»Ÿä¸­å…·æœ‰æå¼ºçš„æ³›åŒ–èƒ½åŠ›å’Œå®æ—¶åº”ç”¨æ½œåŠ›ã€‚",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "primary_category": "cs.CV",
      "comment": "5 pages, 2 figures. Accepted paper for the IEIE (Institute of Electronics and Information Engineers) Fall Conference 2025. Presentation on Nov 27, 2025",
      "pdf_url": "https://arxiv.org/pdf/2511.10866v1",
      "published_date": "2025-11-14 00:29:31 UTC",
      "updated_date": "2025-11-14 00:29:31 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:25:09.940152+00:00"
    },
    {
      "arxiv_id": "2511.17565v1",
      "title": "Generative Caching for Structurally Similar Prompts and Responses",
      "title_zh": "é¢å‘ç»“æ„ç›¸ä¼¼æç¤ºè¯ä¸å“åº”çš„ç”Ÿæˆå¼ç¼“å­˜",
      "authors": [
        "Sarthak Chakraborty",
        "Suman Nath",
        "Xuchao Zhang",
        "Chetan Bansal",
        "Indranil Gupta"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being used to plan, reason, and execute tasks across diverse scenarios. In use cases like repeatable workflows and agentic settings, prompts are often reused with minor variations while having a similar structure for recurring tasks. This opens up opportunities for caching. However, exact prompt matching fails on such structurally similar prompts, while semantic caching may produce incorrect responses by ignoring critical differences. To address this, we introduce \\ourmethod{}, a generative cache that produces variation-aware responses for structurally similar prompts. \\ourmethod{} identifies reusable response patterns across similar prompt structures and synthesizes customized outputs for new requests. We show that \\ourmethod{} achieves 83\\% cache hit rate, while having minimal incorrect hits on datasets without prompt repetition. In agentic workflows, it improves cache hit rate by $\\sim$20\\% and reduces end-to-end execution latency by $\\sim$34\\% compared to standard prompt matching.",
      "tldr_zh": "è¯¥ç ”ç©¶é’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹(LLMs)åœ¨é‡å¤å·¥ä½œæµå’Œæ™ºèƒ½ä½“(agentic)è®¾ç½®ä¸­æç¤ºè¯ç»“æ„ç›¸ä¼¼ä½†å­˜åœ¨å¾®å°å˜åŠ¨çš„æƒ…å†µï¼Œæå‡ºäº†Generative Cachingæ¡†æ¶ã€‚ä¼ ç»Ÿçš„ç²¾ç¡®åŒ¹é…(exact prompt matching)åœ¨è¿™äº›åœºæ™¯ä¸‹ä¼šå¤±æ•ˆï¼Œè€Œè¯­ä¹‰ç¼“å­˜(semantic caching)åˆ™å¯èƒ½å› å¿½ç•¥å…³é”®å·®å¼‚è€Œäº§ç”Ÿé”™è¯¯å“åº”ã€‚Generative Cachingé€šè¿‡è¯†åˆ«ç›¸ä¼¼æç¤ºç»“æ„ä¸­çš„å¯é‡ç”¨å“åº”æ¨¡å¼ï¼Œå¹¶ä¸ºæ–°è¯·æ±‚åˆæˆå®šåˆ¶åŒ–è¾“å‡ºï¼Œå®ç°äº†å…·æœ‰å˜ä½“æ„ŸçŸ¥(variation-aware)èƒ½åŠ›çš„ç¼“å­˜æœºåˆ¶ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•è¾¾åˆ°äº†83%çš„ç¼“å­˜å‘½ä¸­ç‡ï¼Œä¸”åœ¨æ— é‡å¤çš„æ•°æ®é›†ä¸Šè¯¯æŠ¥æä½ã€‚åœ¨æ™ºèƒ½ä½“å·¥ä½œæµä¸­ï¼Œç›¸æ¯”æ ‡å‡†æç¤ºåŒ¹é…ï¼Œè¯¥æ–¹æ³•å°†ç¼“å­˜å‘½ä¸­ç‡æå‡äº†çº¦20%ï¼Œå¹¶å°†ç«¯åˆ°ç«¯æ‰§è¡Œå»¶è¿Ÿé™ä½äº†çº¦34%ï¼Œä¸ºæå‡å¤§è§„æ¨¡LLMåº”ç”¨çš„æ•ˆç‡æä¾›äº†æœ‰æ•ˆæ–¹æ¡ˆã€‚",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "primary_category": "cs.CL",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.17565v1",
      "published_date": "2025-11-14 00:22:00 UTC",
      "updated_date": "2025-11-14 00:22:00 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:25:14.837257+00:00"
    },
    {
      "arxiv_id": "2511.10862v1",
      "title": "Generative Artificial Intelligence Adoption Among Bangladeshi Journalists: Exploring Journalists' Awareness, Acceptance, Usage, and Organizational Stance on Generative AI",
      "title_zh": "Bangladeshi æ–°é—»å·¥ä½œè€…å¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½çš„é‡‡ç”¨ï¼šæ¢ç©¶è®°è€…çš„è®¤çŸ¥ã€æ¥å—åº¦ã€ä½¿ç”¨ç°çŠ¶åŠæœºæ„ç«‹åœº",
      "authors": [
        "H. M. Murtuza",
        "Md Oliullah"
      ],
      "abstract": "Newsrooms and journalists across the world are adopting Generative AI (GenAI). Drawing on in-depth interviews with 23 journalists, this study identifies Bangladeshi journalists' awareness, acceptance, usage patterns, and their media organizations' stance toward GenAI. This study finds Bangladeshi journalists' high reliance on GenAI like their Western colleagues despite limited institutional support and the near absence of AI policy. Despite this contrast, concerns over GenAI's implications in journalism between the West and non-West were mostly identical. Moreover, this study contributes to the Unified Theory of Acceptance and Use of Technology (UTAUT) by proposing two changes regarding GenAI adoption among journalists in non-Western settings. First, this study identifies the non-contribution of facilitating conditions in shaping behavioral intent in GenAI adoption in non-Western contexts. Second, social influence works in a horizontal order through informal peer pressure or professional motivation in the absence of formal institutional hierarchical pressure. Voluntariness in the context of Bangladeshi journalists is underpinned by their professional compulsion. Therefore, this study contributes to understanding how contextual factors shape technology adoption trajectories in non-Western journalism.",
      "tldr_zh": "è¯¥ç ”ç©¶é€šè¿‡å¯¹23åå­ŸåŠ æ‹‰å›½è®°è€…çš„æ·±åº¦è®¿è°ˆï¼Œæ¢è®¨äº†éè¥¿æ–¹èƒŒæ™¯ä¸‹æ–°é—»ä»ä¸šè€…å¯¹ç”Ÿæˆå¼äººå·¥æ™ºèƒ½(Generative AI)çš„è®¤çŸ¥ã€æ¥å—åº¦ã€ä½¿ç”¨æ¨¡å¼åŠåª’ä½“æœºæ„çš„ç«‹åœºã€‚ç ”ç©¶å‘ç°ï¼Œå°½ç®¡å­ŸåŠ æ‹‰å›½æ–°é—»æœºæ„æ™®éç¼ºä¹åˆ¶åº¦æ”¯æŒå’ŒAIæ”¿ç­–ï¼Œä½†è®°è€…å¯¹Generative AIçš„ä¾èµ–ç¨‹åº¦æé«˜ï¼Œä¸”å…¶å¯¹æŠ€æœ¯è´Ÿé¢å½±å“çš„æ‹…å¿§ä¸è¥¿æ–¹åŒè¡Œé«˜åº¦ç›¸ä¼¼ã€‚é€šè¿‡æ‰©å±•æŠ€æœ¯æ¥å—å’Œä½¿ç”¨ç»Ÿä¸€ç†è®º(UTAUT)ï¼Œè¯¥ç ”ç©¶å‘ç°åœ¨éè¥¿æ–¹è¯­å¢ƒä¸‹ï¼Œä¾¿åˆ©æ¡ä»¶(Facilitating conditions)å¯¹è¡Œä¸ºæ„å›¾çš„å½±å“å¹¶ä¸æ˜¾è‘—ã€‚æ­¤å¤–ï¼Œç¤¾ä¼šå½±å“(Social influence)åœ¨ç¼ºä¹æ­£å¼å±‚çº§å‹åŠ›æ—¶ï¼Œä¸»è¦é€šè¿‡æ°´å¹³æ–¹å‘çš„åŒä¼´å‹åŠ›æˆ–ä¸“ä¸šåŠ¨æœºå‘æŒ¥ä½œç”¨ï¼Œè€Œè®°è€…çš„è‡ªæ„¿æ€§(Voluntariness)åˆ™æ·±å—å…¶èŒä¸šå¼ºè¿«æ„Ÿé©±åŠ¨ã€‚è¯¥ç ”ç©¶æ­ç¤ºäº†ç‰¹å®šèƒŒæ™¯å› ç´ å¦‚ä½•é‡å¡‘éè¥¿æ–¹æ–°é—»ä¸šçš„æŠ€æœ¯åº”ç”¨è½¨è¿¹ã€‚",
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "primary_category": "cs.CY",
      "comment": "",
      "pdf_url": "https://arxiv.org/pdf/2511.10862v1",
      "published_date": "2025-11-14 00:14:38 UTC",
      "updated_date": "2025-11-14 00:14:38 UTC",
      "processing_status": "completed",
      "attempts": 1,
      "max_attempts": 3,
      "error": null,
      "completed_steps": [
        "translation",
        "tldr"
      ],
      "llm_backup_calls": 0,
      "last_update": "2026-01-26T06:25:24.837833+00:00"
    }
  ],
  "processing_status": "completed",
  "error": null,
  "raw_papers_fetched": true,
  "papers_count": 167,
  "processed_papers_count": 167,
  "failed_papers_count": 0,
  "llm_backup_calls": 0,
  "summary_generated": true,
  "daily_data_saved": true,
  "last_update": "2026-01-26T06:26:13.061013+00:00"
}
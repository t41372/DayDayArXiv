[
  {
    "arxiv_id": "2410.01131v2",
    "title": "nGPT: Normalized Transformer with Representation Learning on the Hypersphere",
    "authors": [
      "Ilya Loshchilov",
      "Cheng-Ping Hsieh",
      "Simeng Sun",
      "Boris Ginsburg"
    ],
    "abstract": "We propose a novel neural network architecture, the normalized Transformer\n(nGPT) with representation learning on the hypersphere. In nGPT, all vectors\nforming the embeddings, MLP, attention matrices and hidden states are unit norm\nnormalized. The input stream of tokens travels on the surface of a hypersphere,\nwith each layer contributing a displacement towards the target output\npredictions. These displacements are defined by the MLP and attention blocks,\nwhose vector components also reside on the same hypersphere. Experiments show\nthat nGPT learns much faster, reducing the number of training steps required to\nachieve the same accuracy by a factor of 4 to 20, depending on the sequence\nlength.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.01131v2",
    "published_date": "2024-10-01 23:50:09 UTC",
    "updated_date": "2025-04-23 18:41:43 UTC"
  },
  {
    "arxiv_id": "2410.02831v1",
    "title": "Skill Issues: An Analysis of CS:GO Skill Rating Systems",
    "authors": [
      "Mikel Bober-Irizar",
      "Naunidh Dua",
      "Max McGuinness"
    ],
    "abstract": "The meteoric rise of online games has created a need for accurate skill\nrating systems for tracking improvement and fair matchmaking. Although many\nskill rating systems are deployed, with various theoretical foundations, less\nwork has been done at analysing the real-world performance of these algorithms.\nIn this paper, we perform an empirical analysis of Elo, Glicko2 and TrueSkill\nthrough the lens of surrogate modelling, where skill ratings influence future\nmatchmaking with a configurable acquisition function. We look both at overall\nperformance and data efficiency, and perform a sensitivity analysis based on a\nlarge dataset of Counter-Strike: Global Offensive matches.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02831v1",
    "published_date": "2024-10-01 23:19:31 UTC",
    "updated_date": "2024-10-01 23:19:31 UTC"
  },
  {
    "arxiv_id": "2410.01855v2",
    "title": "Explainable Diagnosis Prediction through Neuro-Symbolic Integration",
    "authors": [
      "Qiuhao Lu",
      "Rui Li",
      "Elham Sagheb",
      "Andrew Wen",
      "Jinlian Wang",
      "Liwei Wang",
      "Jungwei W. Fan",
      "Hongfang Liu"
    ],
    "abstract": "Diagnosis prediction is a critical task in healthcare, where timely and\naccurate identification of medical conditions can significantly impact patient\noutcomes. Traditional machine learning and deep learning models have achieved\nnotable success in this domain but often lack interpretability which is a\ncrucial requirement in clinical settings. In this study, we explore the use of\nneuro-symbolic methods, specifically Logical Neural Networks (LNNs), to develop\nexplainable models for diagnosis prediction. Essentially, we design and\nimplement LNN-based models that integrate domain-specific knowledge through\nlogical rules with learnable thresholds. Our models, particularly\n$M_{\\text{multi-pathway}}$ and $M_{\\text{comprehensive}}$, demonstrate superior\nperformance over traditional models such as Logistic Regression, SVM, and\nRandom Forest, achieving higher accuracy (up to 80.52\\%) and AUROC scores (up\nto 0.8457) in the case study of diabetes prediction. The learned weights and\nthresholds within the LNN models provide direct insights into feature\ncontributions, enhancing interpretability without compromising predictive\npower. These findings highlight the potential of neuro-symbolic approaches in\nbridging the gap between accuracy and explainability in healthcare AI\napplications. By offering transparent and adaptable diagnostic models, our work\ncontributes to the advancement of precision medicine and supports the\ndevelopment of equitable healthcare solutions. Future research will focus on\nextending these methods to larger and more diverse datasets to further validate\ntheir applicability across different medical conditions and populations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Proceedings of AMIA Informatics Summit 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.01855v2",
    "published_date": "2024-10-01 22:47:24 UTC",
    "updated_date": "2025-01-07 23:47:06 UTC"
  },
  {
    "arxiv_id": "2410.01111v1",
    "title": "Learning to Build by Building Your Own Instructions",
    "authors": [
      "Aaron Walsman",
      "Muru Zhang",
      "Adam Fishman",
      "Ali Farhadi",
      "Dieter Fox"
    ],
    "abstract": "Structural understanding of complex visual objects is an important unsolved\ncomponent of artificial intelligence. To study this, we develop a new technique\nfor the recently proposed Break-and-Make problem in LTRON where an agent must\nlearn to build a previously unseen LEGO assembly using a single interactive\nsession to gather information about its components and their structure. We\nattack this problem by building an agent that we call \\textbf{\\ours} that is\nable to make its own visual instruction book. By disassembling an unseen\nassembly and periodically saving images of it, the agent is able to create a\nset of instructions so that it has the information necessary to rebuild it.\nThese instructions form an explicit memory that allows the model to reason\nabout the assembly process one step at a time, avoiding the need for long-term\nimplicit memory. This in turn allows us to train on much larger LEGO assemblies\nthan has been possible in the past. To demonstrate the power of this model, we\nrelease a new dataset of procedurally built LEGO vehicles that contain an\naverage of 31 bricks each and require over one hundred steps to disassemble and\nreassemble. We train these models using online imitation learning which allows\nthe model to learn from its own mistakes. Finally, we also provide some small\nimprovements to LTRON and the Break-and-Make problem that simplify the learning\nenvironment and improve usability.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.01111v1",
    "published_date": "2024-10-01 22:39:58 UTC",
    "updated_date": "2024-10-01 22:39:58 UTC"
  },
  {
    "arxiv_id": "2410.01109v2",
    "title": "Mixing It Up: The Cocktail Effect of Multi-Task Fine-Tuning on LLM Performance -- A Case Study in Finance",
    "authors": [
      "Meni Brief",
      "Oded Ovadia",
      "Gil Shenderovitz",
      "Noga Ben Yoash",
      "Rachel Lemberg",
      "Eitam Sheetrit"
    ],
    "abstract": "The application of large language models (LLMs) in domain-specific contexts,\nincluding finance, has expanded rapidly. Domain-specific LLMs are typically\nevaluated based on their performance in various downstream tasks relevant to\nthe domain. In this work, we present a detailed analysis of fine-tuning LLMs\nfor such tasks. Somewhat counterintuitively, we find that in domain-specific\ncases, fine-tuning exclusively on the target task is not always the most\neffective strategy. Instead, multi-task finetuning - where models are trained\non a cocktail of related tasks - can significantly enhance performance. We\ndemonstrate how this approach enables a small model, such as Phi-3-Mini, to\nachieve state-of-the-art results, even surpassing the much larger GPT-4-o model\non financial benchmarks. Our study involves a large-scale experiment,\nconducting over 200 training experiments using several widely adopted LLMs as\nbaselines, and empirically confirms the benefits of multi-task fine-tuning.\nAdditionally, we explore the use of general instruction data as a form of\nregularization, suggesting that it helps minimize performance degradation. We\nalso investigate the inclusion of mathematical data, finding improvements in\nnumerical reasoning that transfer effectively to financial tasks. Finally, we\nnote that while fine-tuning for downstream tasks leads to targeted improvements\nin task performance, it does not necessarily result in broader gains in domain\nknowledge or complex domain reasoning abilities.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.01109v2",
    "published_date": "2024-10-01 22:35:56 UTC",
    "updated_date": "2024-12-04 20:57:05 UTC"
  },
  {
    "arxiv_id": "2410.01108v1",
    "title": "Augmentation through Laundering Attacks for Audio Spoof Detection",
    "authors": [
      "Hashim Ali",
      "Surya Subramani",
      "Hafiz Malik"
    ],
    "abstract": "Recent text-to-speech (TTS) developments have made voice cloning (VC) more\nrealistic, affordable, and easily accessible. This has given rise to many\npotential abuses of this technology, including Joe Biden's New Hampshire\ndeepfake robocall. Several methodologies have been proposed to detect such\nclones. However, these methodologies have been trained and evaluated on\nrelatively clean databases. Recently, ASVspoof 5 Challenge introduced a new\ncrowd-sourced database of diverse acoustic conditions including various\nspoofing attacks and codec conditions. This paper is our submission to the\nASVspoof 5 Challenge and aims to investigate the performance of Audio Spoof\nDetection, trained using data augmentation through laundering attacks, on the\nASVSpoof 5 database. The results demonstrate that our system performs worst on\nA18, A19, A20, A26, and A30 spoofing attacks and in the codec and compression\nconditions of C08, C09, and C10.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.SD"
    ],
    "primary_category": "eess.AS",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.01108v1",
    "published_date": "2024-10-01 22:34:51 UTC",
    "updated_date": "2024-10-01 22:34:51 UTC"
  },
  {
    "arxiv_id": "2410.01104v2",
    "title": "softmax is not enough (for sharp out-of-distribution)",
    "authors": [
      "Petar Veličković",
      "Christos Perivolaropoulos",
      "Federico Barbero",
      "Razvan Pascanu"
    ],
    "abstract": "A key property of reasoning systems is the ability to make sharp decisions on\ntheir input data. For contemporary AI systems, a key carrier of sharp behaviour\nis the softmax function, with its capability to perform differentiable\nquery-key lookups. It is a common belief that the predictive power of networks\nleveraging softmax arises from \"circuits\" which sharply perform certain kinds\nof computations consistently across many diverse inputs. However, for these\ncircuits to be robust, they would need to generalise well to arbitrary valid\ninputs. In this paper, we dispel this myth: even for tasks as simple as finding\nthe maximum key, any learned circuitry must disperse as the number of items\ngrows at test time. We attribute this to a fundamental limitation of the\nsoftmax function to robustly approximate sharp functions, prove this phenomenon\ntheoretically, and propose adaptive temperature as an ad-hoc technique for\nimproving the sharpness of softmax at inference time.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "Comments welcome. 15 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.01104v2",
    "published_date": "2024-10-01 22:22:35 UTC",
    "updated_date": "2024-10-07 13:13:41 UTC"
  },
  {
    "arxiv_id": "2410.01103v1",
    "title": "Approximately Aligned Decoding",
    "authors": [
      "Daniel Melcer",
      "Sujan Gonugondla",
      "Pramuditha Perera",
      "Haifeng Qian",
      "Wen-Hao Chiang",
      "Yanjun Wang",
      "Nihal Jain",
      "Pranav Garg",
      "Xiaofei Ma",
      "Anoop Deoras"
    ],
    "abstract": "It is common to reject undesired outputs of Large Language Models (LLMs);\nhowever, current methods to do so require an excessive amount of computation,\nor severely distort the distribution of outputs. We present a method to balance\nthe distortion of the output distribution with computational efficiency,\nallowing for the generation of long sequences of text with difficult-to-satisfy\nconstraints, with less amplification of low probability outputs compared to\nexisting methods. We show through a series of experiments that the\ntask-specific performance of our method is comparable to methods that do not\ndistort the output distribution, while being much more computationally\nefficient.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "9 pages main, 22 pages total",
    "pdf_url": "http://arxiv.org/pdf/2410.01103v1",
    "published_date": "2024-10-01 22:22:13 UTC",
    "updated_date": "2024-10-01 22:22:13 UTC"
  },
  {
    "arxiv_id": "2410.01098v2",
    "title": "Exploring Gen-AI applications in building research and industry: A review",
    "authors": [
      "Hanlong Wan",
      "Jian Zhang",
      "Yan Chen",
      "Weili Xu",
      "Fan Feng"
    ],
    "abstract": "This paper investigates the transformative potential of Generative AI\n(Gen-AI) technologies, particularly large language models, within the building\nindustry. By leveraging these advanced AI tools, the study explores their\napplication across key areas such as automated compliance checking and building\ndesign assistance. The research highlights how Gen-AI can automate\nlabor-intensive processes, significantly improving efficiency and reducing\ncosts in building practices. The paper first discusses the two widely applied\nfundamental models-Transformer and Diffusion model-and summarizes current\npathways for accessing Gen-AI models and the most common techniques for\ncustomizing them. It then explores applications for text generation, such as\ncompliance checking, control support, data mining, and building simulation\ninput file editing. Additionally, it examines image generation, including\ndirect generation through diffusion models and indirect generation through\nlanguage model-supported template creation based on existing Computer-Aided\nDesign or other design tools with rendering. The paper concludes with a\ncomprehensive analysis of the current capabilities of Gen-AI in the building\nindustry, outlining future directions for research and development, with the\ngoal of paving the way for smarter, more effective, and responsive design,\nconstruction, and operational practices.",
    "categories": [
      "cs.AI",
      "cs.SY",
      "eess.IV",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "comment": "This is a pre-peer review and copy editing version of an article\n  published in Building Simulation. The final authenticated version is\n  available online at:https://doi.org/10.1007/s12273-025-1279-x",
    "pdf_url": "http://arxiv.org/pdf/2410.01098v2",
    "published_date": "2024-10-01 21:59:08 UTC",
    "updated_date": "2025-05-11 04:14:59 UTC"
  },
  {
    "arxiv_id": "2410.01096v1",
    "title": "Mechanic Maker: Accessible Game Development Via Symbolic Learning Program Synthesis",
    "authors": [
      "Megan Sumner",
      "Vardan Saini",
      "Matthew Guzdial"
    ],
    "abstract": "Game development is a highly technical practice that traditionally requires\nprogramming skills. This serves as a barrier to entry for would-be developers\nor those hoping to use games as part of their creative expression. While there\nhave been prior game development tools focused on accessibility, they generally\nstill require programming, or have major limitations in terms of the kinds of\ngames they can make. In this paper we introduce Mechanic Maker, a tool for\ncreating a wide-range of game mechanics without programming. It instead relies\non a backend symbolic learning system to synthesize game mechanics from\nexamples. We conducted a user study to evaluate the benefits of the tool for\nparticipants with a variety of programming and game development experience. Our\nresults demonstrated that participants' ability to use the tool was unrelated\nto programming ability. We conclude that tools like ours could help democratize\ngame development, making the practice accessible regardless of programming\nskills.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "11 pages, 8 figures, AAAI Conference on Artificial Intelligence and\n  Interactive Digital Entertainment",
    "pdf_url": "http://arxiv.org/pdf/2410.01096v1",
    "published_date": "2024-10-01 21:58:28 UTC",
    "updated_date": "2024-10-01 21:58:28 UTC"
  },
  {
    "arxiv_id": "2410.01091v2",
    "title": "Efficient and Private Marginal Reconstruction with Local Non-Negativity",
    "authors": [
      "Brett Mullins",
      "Miguel Fuentes",
      "Yingtai Xiao",
      "Daniel Kifer",
      "Cameron Musco",
      "Daniel Sheldon"
    ],
    "abstract": "Differential privacy is the dominant standard for formal and quantifiable\nprivacy and has been used in major deployments that impact millions of people.\nMany differentially private algorithms for query release and synthetic data\ncontain steps that reconstruct answers to queries from answers to other queries\nthat have been measured privately. Reconstruction is an important subproblem\nfor such mechanisms to economize the privacy budget, minimize error on\nreconstructed answers, and allow for scalability to high-dimensional datasets.\nIn this paper, we introduce a principled and efficient postprocessing method\nReM (Residuals-to-Marginals) for reconstructing answers to marginal queries.\nOur method builds on recent work on efficient mechanisms for marginal query\nrelease, based on making measurements using a residual query basis that admits\nefficient pseudoinversion, which is an important primitive used in\nreconstruction. An extension GReM-LNN (Gaussian Residuals-to-Marginals with\nLocal Non-negativity) reconstructs marginals under Gaussian noise satisfying\nconsistency and non-negativity, which often reduces error on reconstructed\nanswers. We demonstrate the utility of ReM and GReM-LNN by applying them to\nimprove existing private query answering mechanisms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "To appear at NeurIPS 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.01091v2",
    "published_date": "2024-10-01 21:39:28 UTC",
    "updated_date": "2024-12-07 05:41:06 UTC"
  },
  {
    "arxiv_id": "2410.01066v2",
    "title": "From Natural Language to SQL: Review of LLM-based Text-to-SQL Systems",
    "authors": [
      "Ali Mohammadjafari",
      "Anthony S. Maida",
      "Raju Gottumukkala"
    ],
    "abstract": "LLMs when used with Retrieval Augmented Generation (RAG), are greatly\nimproving the SOTA of translating natural language queries to structured and\ncorrect SQL. Unlike previous reviews, this survey provides a comprehensive\nstudy of the evolution of LLM-based text-to-SQL systems, from early rule-based\nmodels to advanced LLM approaches that use (RAG) systems. We discuss\nbenchmarks, evaluation methods, and evaluation metrics. Also, we uniquely study\nthe use of Graph RAGs for better contextual accuracy and schema linking in\nthese systems. Finally, we highlight key challenges such as computational\nefficiency, model robustness, and data privacy toward improvements of LLM-based\ntext-to-SQL systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 5 figures, 5 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.01066v2",
    "published_date": "2024-10-01 20:46:25 UTC",
    "updated_date": "2025-02-04 03:46:27 UTC"
  },
  {
    "arxiv_id": "2410.01064v1",
    "title": "Truth or Deceit? A Bayesian Decoding Game Enhances Consistency and Reliability",
    "authors": [
      "Weitong Zhang",
      "Chengqi Zang",
      "Bernhard Kainz"
    ],
    "abstract": "Large Language Models (LLMs) often produce outputs that -- though plausible\n-- can lack consistency and reliability, particularly in ambiguous or complex\nscenarios. Challenges arise from ensuring that outputs align with both factual\ncorrectness and human intent. This is problematic in existing approaches that\ntrade improved consistency for lower accuracy. To mitigate these challenges, we\npropose a novel game-theoretic approach to enhance consistency and reliability\nduring the decoding stage of LLM output generation. Our method models the\ndecoding process as a multistage Bayesian decoding game. This ensures\nconsistency through Correctness Alignment and enhances reliability via\nAmbiguity Calibration. The model dynamically converges to a consensus on the\nmost reliable outputs and distinguishes {Valid, Specious} outputs without human\nfeedback or additional training. Our game design allows smaller models to\noutperform much larger models through game mechanisms (e.g., 78.1 LLaMA13B vs\n76.6 PaLM540B), as well as integrating various LL strategies and models,\ndemonstrating the potential of game-theoretic tools to improve the truthfulness\nand reliability of LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.01064v1",
    "published_date": "2024-10-01 20:46:10 UTC",
    "updated_date": "2024-10-01 20:46:10 UTC"
  },
  {
    "arxiv_id": "2410.01044v1",
    "title": "RATIONALYST: Pre-training Process-Supervision for Improving Reasoning",
    "authors": [
      "Dongwei Jiang",
      "Guoxuan Wang",
      "Yining Lu",
      "Andrew Wang",
      "Jingyu Zhang",
      "Chuyu Liu",
      "Benjamin Van Durme",
      "Daniel Khashabi"
    ],
    "abstract": "The reasoning steps generated by LLMs might be incomplete, as they mimic\nlogical leaps common in everyday communication found in their pre-training\ndata: underlying rationales are frequently left implicit (unstated). To address\nthis challenge, we introduce RATIONALYST, a model for process-supervision of\nreasoning based on pre-training on a vast collection of rationale annotations\nextracted from unlabeled data. We extract 79k rationales from web-scale\nunlabelled dataset (the Pile) and a combination of reasoning datasets with\nminimal human intervention. This web-scale pre-training for reasoning allows\nRATIONALYST to consistently generalize across diverse reasoning tasks,\nincluding mathematical, commonsense, scientific, and logical reasoning.\nFine-tuned from LLaMa-3-8B, RATIONALYST improves the accuracy of reasoning by\nan average of 3.9% on 7 representative reasoning benchmarks. It also\ndemonstrates superior performance compared to significantly larger verifiers\nlike GPT-4 and similarly sized models fine-tuned on matching training sets.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "Our code, data, and model can be found at this repository:\n  https://github.com/JHU-CLSP/Rationalyst",
    "pdf_url": "http://arxiv.org/pdf/2410.01044v1",
    "published_date": "2024-10-01 20:05:51 UTC",
    "updated_date": "2024-10-01 20:05:51 UTC"
  },
  {
    "arxiv_id": "2410.01036v1",
    "title": "MOSEL: 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages",
    "authors": [
      "Marco Gaido",
      "Sara Papi",
      "Luisa Bentivogli",
      "Alessio Brutti",
      "Mauro Cettolo",
      "Roberto Gretter",
      "Marco Matassoni",
      "Mohamed Nabih",
      "Matteo Negri"
    ],
    "abstract": "The rise of foundation models (FMs), coupled with regulatory efforts\naddressing their risks and impacts, has sparked significant interest in\nopen-source models. However, existing speech FMs (SFMs) fall short of full\ncompliance with the open-source principles, even if claimed otherwise, as no\nexisting SFM has model weights, code, and training data publicly available\nunder open-source terms. In this work, we take the first step toward filling\nthis gap by focusing on the 24 official languages of the European Union (EU).\nWe collect suitable training data by surveying automatic speech recognition\ndatasets and unlabeled speech corpora under open-source compliant licenses, for\na total of 950k hours. Additionally, we release automatic transcripts for 441k\nhours of unlabeled data under the permissive CC-BY license, thereby\nfacilitating the creation of open-source SFMs for the EU languages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted at EMNLP 2024 Main Conference",
    "pdf_url": "http://arxiv.org/pdf/2410.01036v1",
    "published_date": "2024-10-01 19:54:10 UTC",
    "updated_date": "2024-10-01 19:54:10 UTC"
  },
  {
    "arxiv_id": "2410.03750v1",
    "title": "SQFT: Low-cost Model Adaptation in Low-precision Sparse Foundation Models",
    "authors": [
      "Juan Pablo Muñoz",
      "Jinjie Yuan",
      "Nilesh Jain"
    ],
    "abstract": "Large pre-trained models (LPMs), such as large language models, have become\nubiquitous and are employed in many applications. These models are often\nadapted to a desired domain or downstream task through a fine-tuning stage.\nThis paper proposes SQFT, an end-to-end solution for low-precision sparse\nparameter-efficient fine-tuning of LPMs, allowing for effective model\nmanipulation in resource-constrained environments. Additionally, an innovative\nstrategy enables the merging of sparse weights with low-rank adapters without\nlosing sparsity and accuracy, overcoming the limitations of previous\napproaches. SQFT also addresses the challenge of having quantized weights and\nadapters with different numerical precisions, enabling merging in the desired\nnumerical format without sacrificing accuracy. Multiple adaptation scenarios,\nmodels, and comprehensive sparsity levels demonstrate the effectiveness of\nSQFT. Models and code are available at\nhttps://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "To be published in EMNLP-24 Findings",
    "pdf_url": "http://arxiv.org/pdf/2410.03750v1",
    "published_date": "2024-10-01 19:49:35 UTC",
    "updated_date": "2024-10-01 19:49:35 UTC"
  },
  {
    "arxiv_id": "2410.19738v1",
    "title": "Integrating Reasoning Systems for Trustworthy AI, Proceedings of the 4th Workshop on Logic and Practice of Programming (LPOP)",
    "authors": [
      "Anil Nerode",
      "Yanhong A. Liu"
    ],
    "abstract": "This proceedings contains abstracts and position papers for the work to be\npresented at the fourth Logic and Practice of Programming (LPOP) Workshop. The\nworkshop is to be held in Dallas, Texas, USA, and as a hybrid event, on October\n13, 2024, in conjunction with the 40th International Conference on Logic\nProgramming (ICLP). The focus of this workshop is integrating reasoning systems\nfor trustworthy AI, especially including integrating diverse models of\nprogramming with rules and constraints.",
    "categories": [
      "cs.AI",
      "cs.LO",
      "cs.PL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.19738v1",
    "published_date": "2024-10-01 19:36:08 UTC",
    "updated_date": "2024-10-01 19:36:08 UTC"
  },
  {
    "arxiv_id": "2410.01023v2",
    "title": "Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!",
    "authors": [
      "Jiwan Chung",
      "Seungwon Lim",
      "Jaehyun Jeon",
      "Seungbeen Lee",
      "Youngjae Yu"
    ],
    "abstract": "Humans possess multimodal literacy, allowing them to actively integrate\ninformation from various modalities to form reasoning. Faced with challenges\nlike lexical ambiguity in text, we supplement this with other modalities, such\nas thumbnail images or textbook illustrations. Is it possible for machines to\nachieve a similar multimodal understanding capability? In response, we present\nUnderstanding Pun with Image Explanations (UNPIE), a novel benchmark designed\nto assess the impact of multimodal inputs in resolving lexical ambiguities.\nPuns serve as the ideal subject for this evaluation due to their intrinsic\nambiguity. Our dataset includes 1,000 puns, each accompanied by an image that\nexplains both meanings. We pose three multimodal challenges with the\nannotations to assess different aspects of multimodal literacy; Pun Grounding,\nDisambiguation, and Reconstruction. The results indicate that various Socratic\nModels and Visual-Language Models improve over the text-only models when given\nvisual context, particularly as the complexity of the tasks increases.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted as main paper in EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.01023v2",
    "published_date": "2024-10-01 19:32:57 UTC",
    "updated_date": "2024-10-23 02:55:20 UTC"
  },
  {
    "arxiv_id": "2410.02829v1",
    "title": "LLMs May Not Be Human-Level Players, But They Can Be Testers: Measuring Game Difficulty with LLM Agents",
    "authors": [
      "Chang Xiao",
      "Brenda Z. Yang"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have demonstrated their\npotential as autonomous agents across various tasks. One emerging application\nis the use of LLMs in playing games. In this work, we explore a practical\nproblem for the gaming industry: Can LLMs be used to measure game difficulty?\nWe propose a general game-testing framework using LLM agents and test it on two\nwidely played strategy games: Wordle and Slay the Spire. Our results reveal an\ninteresting finding: although LLMs may not perform as well as the average human\nplayer, their performance, when guided by simple, generic prompting techniques,\nshows a statistically significant and strong correlation with difficulty\nindicated by human players. This suggests that LLMs could serve as effective\nagents for measuring game difficulty during the development process. Based on\nour experiments, we also outline general principles and guidelines for\nincorporating LLMs into the game testing process.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02829v1",
    "published_date": "2024-10-01 18:40:43 UTC",
    "updated_date": "2024-10-01 18:40:43 UTC"
  },
  {
    "arxiv_id": "2410.03747v1",
    "title": "Distributed AI Platform for the 6G RAN",
    "authors": [
      "Ganesh Ananthanarayanan",
      "Xenofon Foukas",
      "Bozidar Radunovic",
      "Yongguang Zhang"
    ],
    "abstract": "Cellular Radio Access Networks (RANs) are rapidly evolving towards 6G, driven\nby the need to reduce costs and introduce new revenue streams for operators and\nenterprises. In this context, AI emerges as a key enabler in solving complex\nRAN problems spanning both the management and application domains.\nUnfortunately, and despite the undeniable promise of AI, several practical\nchallenges still remain, hindering the widespread adoption of AI applications\nin the RAN space. This article attempts to shed light to these challenges and\nargues that existing approaches in addressing them are inadequate for realizing\nthe vision of a truly AI-native 6G network. Motivated by this lack of\nsolutions, it proposes a generic distributed AI platform architecture, tailored\nto the needs of an AI-native RAN and discusses its alignment with ongoing\nstandardization efforts.",
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "primary_category": "cs.NI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03747v1",
    "published_date": "2024-10-01 18:35:25 UTC",
    "updated_date": "2024-10-01 18:35:25 UTC"
  },
  {
    "arxiv_id": "2410.00983v2",
    "title": "Robust Guided Diffusion for Offline Black-Box Optimization",
    "authors": [
      "Can Sam Chen",
      "Christopher Beckham",
      "Zixuan Liu",
      "Xue Liu",
      "Christopher Pal"
    ],
    "abstract": "Offline black-box optimization aims to maximize a black-box function using an\noffline dataset of designs and their measured properties. Two main approaches\nhave emerged: the forward approach, which learns a mapping from input to its\nvalue, thereby acting as a proxy to guide optimization, and the inverse\napproach, which learns a mapping from value to input for conditional\ngeneration. (a) Although proxy-free~(classifier-free) diffusion shows promise\nin robustly modeling the inverse mapping, it lacks explicit guidance from\nproxies, essential for generating high-performance samples beyond the training\ndistribution. Therefore, we propose \\textit{proxy-enhanced sampling} which\nutilizes the explicit guidance from a trained proxy to bolster proxy-free\ndiffusion with enhanced sampling control. (b) Yet, the trained proxy is\nsusceptible to out-of-distribution issues. To address this, we devise the\nmodule \\textit{diffusion-based proxy refinement}, which seamlessly integrates\ninsights from proxy-free diffusion back into the proxy for refinement. To sum\nup, we propose \\textit{\\textbf{R}obust \\textbf{G}uided \\textbf{D}iffusion for\nOffline Black-box Optimization}~(\\textbf{RGD}), combining the advantages of\nproxy~(explicit guidance) and proxy-free diffusion~(robustness) for effective\nconditional generation. RGD achieves state-of-the-art results on various\ndesign-bench tasks, underscoring its efficacy. Our code is at\nhttps://github.com/GGchen1997/RGD.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "19 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.00983v2",
    "published_date": "2024-10-01 18:14:25 UTC",
    "updated_date": "2024-12-30 20:00:57 UTC"
  },
  {
    "arxiv_id": "2410.00980v1",
    "title": "Heterogeneous sound classification with the Broad Sound Taxonomy and Dataset",
    "authors": [
      "Panagiota Anastasopoulou",
      "Jessica Torrey",
      "Xavier Serra",
      "Frederic Font"
    ],
    "abstract": "Automatic sound classification has a wide range of applications in machine\nlistening, enabling context-aware sound processing and understanding. This\npaper explores methodologies for automatically classifying heterogeneous sounds\ncharacterized by high intra-class variability. Our study evaluates the\nclassification task using the Broad Sound Taxonomy, a two-level taxonomy\ncomprising 28 classes designed to cover a heterogeneous range of sounds with\nsemantic distinctions tailored for practical user applications. We construct a\ndataset through manual annotation to ensure accuracy, diverse representation\nwithin each class and relevance in real-world scenarios. We compare a variety\nof both traditional and modern machine learning approaches to establish a\nbaseline for the task of heterogeneous sound classification. We investigate the\nrole of input features, specifically examining how acoustically derived sound\nrepresentations compare to embeddings extracted with pre-trained deep neural\nnetworks that capture both acoustic and semantic information about sounds.\nExperimental results illustrate that audio embeddings encoding acoustic and\nsemantic information achieve higher accuracy in the classification task. After\ncareful analysis of classification errors, we identify some underlying reasons\nfor failure and propose actions to mitigate them. The paper highlights the need\nfor deeper exploration of all stages of classification, understanding the data\nand adopting methodologies capable of effectively handling data complexity and\ngeneralizing in real-world sound environments.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "DCASE2024, post-print, 5 pages, 2 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.00980v1",
    "published_date": "2024-10-01 18:09:02 UTC",
    "updated_date": "2024-10-01 18:09:02 UTC"
  },
  {
    "arxiv_id": "2410.00979v2",
    "title": "Towards Full-parameter and Parameter-efficient Self-learning For Endoscopic Camera Depth Estimation",
    "authors": [
      "Shuting Zhao",
      "Chenkang Du",
      "Kristin Qi",
      "Xinrong Chen",
      "Xinhan Di"
    ],
    "abstract": "Adaptation methods are developed to adapt depth foundation models to\nendoscopic depth estimation recently. However, such approaches typically\nunder-perform training since they limit the parameter search to a low-rank\nsubspace and alter the training dynamics. Therefore, we propose a\nfull-parameter and parameter-efficient learning framework for endoscopic depth\nestimation. At the first stage, the subspace of attention, convolution and\nmulti-layer perception are adapted simultaneously within different sub-spaces.\nAt the second stage, a memory-efficient optimization is proposed for subspace\ncomposition and the performance is further improved in the united sub-space.\nInitial experiments on the SCARED dataset demonstrate that results at the first\nstage improves the performance from 10.2% to 4.1% for Sq Rel, Abs Rel, RMSE and\nRMSE log in the comparison with the state-of-the-art models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "WiCV @ ECCV 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.00979v2",
    "published_date": "2024-10-01 18:08:56 UTC",
    "updated_date": "2024-10-10 03:48:46 UTC"
  },
  {
    "arxiv_id": "2410.00897v1",
    "title": "The Gradient of Health Data Privacy",
    "authors": [
      "Baihan Lin"
    ],
    "abstract": "In the era of digital health and artificial intelligence, the management of\npatient data privacy has become increasingly complex, with significant\nimplications for global health equity and patient trust. This paper introduces\na novel \"privacy gradient\" approach to health data governance, offering a more\nnuanced and adaptive framework than traditional binary privacy models. Our\nmultidimensional concept considers factors such as data sensitivity,\nstakeholder relationships, purpose of use, and temporal aspects, allowing for\ncontext-sensitive privacy protections. Through policy analyses, ethical\nconsiderations, and case studies spanning adolescent health, integrated care,\nand genomic research, we demonstrate how this approach can address critical\nprivacy challenges in diverse healthcare settings worldwide. The privacy\ngradient model has the potential to enhance patient engagement, improve care\ncoordination, and accelerate medical research while safeguarding individual\nprivacy rights. We provide policy recommendations for implementing this\napproach, considering its impact on healthcare systems, research\ninfrastructures, and global health initiatives. This work aims to inform\npolicymakers, healthcare leaders, and digital health innovators, contributing\nto a more equitable, trustworthy, and effective global health data ecosystem in\nthe digital age.",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "q-bio.OT"
    ],
    "primary_category": "cs.CY",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00897v1",
    "published_date": "2024-10-01 17:35:18 UTC",
    "updated_date": "2024-10-01 17:35:18 UTC"
  },
  {
    "arxiv_id": "2410.00880v1",
    "title": "GEMS: Generative Expert Metric System through Iterative Prompt Priming",
    "authors": [
      "Ti-Chung Cheng",
      "Carmen Badea",
      "Christian Bird",
      "Thomas Zimmermann",
      "Robert DeLine",
      "Nicole Forsgren",
      "Denae Ford"
    ],
    "abstract": "Across domains, metrics and measurements are fundamental to identifying\nchallenges, informing decisions, and resolving conflicts. Despite the abundance\nof data available in this information age, not only can it be challenging for a\nsingle expert to work across multi-disciplinary data, but non-experts can also\nfind it unintuitive to create effective measures or transform theories into\ncontext-specific metrics that are chosen appropriately. This technical report\naddresses this challenge by examining software communities within large\nsoftware corporations, where different measures are used as proxies to locate\ncounterparts within the organization to transfer tacit knowledge. We propose a\nprompt-engineering framework inspired by neural activities, demonstrating that\ngenerative models can extract and summarize theories and perform basic\nreasoning, thereby transforming concepts into context-aware metrics to support\nsoftware communities given software repository data. While this research zoomed\nin on software communities, we believe the framework's applicability extends\nacross various fields, showcasing expert-theory-inspired metrics that aid in\ntriaging complex challenges.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "29 pages, 3 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.00880v1",
    "published_date": "2024-10-01 17:14:54 UTC",
    "updated_date": "2024-10-01 17:14:54 UTC"
  },
  {
    "arxiv_id": "2410.00872v1",
    "title": "Do Music Generation Models Encode Music Theory?",
    "authors": [
      "Megan Wei",
      "Michael Freeman",
      "Chris Donahue",
      "Chen Sun"
    ],
    "abstract": "Music foundation models possess impressive music generation capabilities.\nWhen people compose music, they may infuse their understanding of music into\ntheir work, by using notes and intervals to craft melodies, chords to build\nprogressions, and tempo to create a rhythmic feel. To what extent is this true\nof music generation models? More specifically, are fundamental Western music\ntheory concepts observable within the \"inner workings\" of these models? Recent\nwork proposed leveraging latent audio representations from music generation\nmodels towards music information retrieval tasks (e.g. genre classification,\nemotion recognition), which suggests that high-level musical characteristics\nare encoded within these models. However, probing individual music theory\nconcepts (e.g. tempo, pitch class, chord quality) remains under-explored. Thus,\nwe introduce SynTheory, a synthetic MIDI and audio music theory dataset,\nconsisting of tempos, time signatures, notes, intervals, scales, chords, and\nchord progressions concepts. We then propose a framework to probe for these\nmusic theory concepts in music foundation models (Jukebox and MusicGen) and\nassess how strongly they encode these concepts within their internal\nrepresentations. Our findings suggest that music theory concepts are\ndiscernible within foundation models and that the degree to which they are\ndetectable varies by model size and layer.",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "primary_category": "cs.SD",
    "comment": "Accepted at ISMIR 2024. Dataset:\n  https://huggingface.co/datasets/meganwei/syntheory Code:\n  https://github.com/brown-palm/syntheory Website:\n  https://brown-palm.github.io/music-theory",
    "pdf_url": "http://arxiv.org/pdf/2410.00872v1",
    "published_date": "2024-10-01 17:06:30 UTC",
    "updated_date": "2024-10-01 17:06:30 UTC"
  },
  {
    "arxiv_id": "2410.00871v2",
    "title": "MAP: Unleashing Hybrid Mamba-Transformer Vision Backbone's Potential with Masked Autoregressive Pretraining",
    "authors": [
      "Yunze Liu",
      "Li Yi"
    ],
    "abstract": "Hybrid Mamba-Transformer networks have recently garnered broad attention.\nThese networks can leverage the scalability of Transformers while capitalizing\non Mamba's strengths in long-context modeling and computational efficiency.\nHowever, the challenge of effectively pretraining such hybrid networks remains\nan open question. Existing methods, such as Masked Autoencoders (MAE) or\nautoregressive (AR) pretraining, primarily focus on single-type network\narchitectures. In contrast, pretraining strategies for hybrid architectures\nmust be effective for both Mamba and Transformer components. Based on this, we\npropose Masked Autoregressive Pretraining (MAP) to pretrain a hybrid\nMamba-Transformer vision backbone network. This strategy combines the strengths\nof both MAE and Autoregressive pretraining, improving the performance of Mamba\nand Transformer modules within a unified paradigm. Experimental results show\nthat the hybrid Mamba-Transformer vision backbone network pretrained with MAP\nsignificantly outperforms other pretraining strategies, achieving\nstate-of-the-art performance. We validate the method's effectiveness on both 2D\nand 3D datasets and provide detailed ablation studies to support the design\nchoices for each component. The code and checkpoints are available at\nhttps://github.com/yunzeliu/MAP",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00871v2",
    "published_date": "2024-10-01 17:05:08 UTC",
    "updated_date": "2025-03-15 15:21:48 UTC"
  },
  {
    "arxiv_id": "2410.02828v1",
    "title": "PyRIT: A Framework for Security Risk Identification and Red Teaming in Generative AI System",
    "authors": [
      "Gary D. Lopez Munoz",
      "Amanda J. Minnich",
      "Roman Lutz",
      "Richard Lundeen",
      "Raja Sekhar Rao Dheekonda",
      "Nina Chikanov",
      "Bolor-Erdene Jagdagdorj",
      "Martin Pouliot",
      "Shiven Chawla",
      "Whitney Maxwell",
      "Blake Bullwinkel",
      "Katherine Pratt",
      "Joris de Gruyter",
      "Charlotte Siska",
      "Pete Bryan",
      "Tori Westerhoff",
      "Chang Kawaguchi",
      "Christian Seifert",
      "Ram Shankar Siva Kumar",
      "Yonatan Zunger"
    ],
    "abstract": "Generative Artificial Intelligence (GenAI) is becoming ubiquitous in our\ndaily lives. The increase in computational power and data availability has led\nto a proliferation of both single- and multi-modal models. As the GenAI\necosystem matures, the need for extensible and model-agnostic risk\nidentification frameworks is growing. To meet this need, we introduce the\nPython Risk Identification Toolkit (PyRIT), an open-source framework designed\nto enhance red teaming efforts in GenAI systems. PyRIT is a model- and\nplatform-agnostic tool that enables red teamers to probe for and identify novel\nharms, risks, and jailbreaks in multimodal generative AI models. Its composable\narchitecture facilitates the reuse of core building blocks and allows for\nextensibility to future models and modalities. This paper details the\nchallenges specific to red teaming generative AI systems, the development and\nfeatures of PyRIT, and its practical applications in real-world scenarios.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02828v1",
    "published_date": "2024-10-01 17:00:59 UTC",
    "updated_date": "2024-10-01 17:00:59 UTC"
  },
  {
    "arxiv_id": "2410.00807v1",
    "title": "WiGNet: Windowed Vision Graph Neural Network",
    "authors": [
      "Gabriele Spadaro",
      "Marco Grangetto",
      "Attilio Fiandrotti",
      "Enzo Tartaglione",
      "Jhony H. Giraldo"
    ],
    "abstract": "In recent years, Graph Neural Networks (GNNs) have demonstrated strong\nadaptability to various real-world challenges, with architectures such as\nVision GNN (ViG) achieving state-of-the-art performance in several computer\nvision tasks. However, their practical applicability is hindered by the\ncomputational complexity of constructing the graph, which scales quadratically\nwith the image size. In this paper, we introduce a novel Windowed vision Graph\nneural Network (WiGNet) model for efficient image processing. WiGNet explores a\ndifferent strategy from previous works by partitioning the image into windows\nand constructing a graph within each window. Therefore, our model uses graph\nconvolutions instead of the typical 2D convolution or self-attention mechanism.\nWiGNet effectively manages computational and memory complexity for large image\nsizes. We evaluate our method in the ImageNet-1k benchmark dataset and test the\nadaptability of WiGNet using the CelebA-HQ dataset as a downstream task with\nhigher-resolution images. In both of these scenarios, our method achieves\ncompetitive results compared to previous vision GNNs while keeping memory and\ncomputational complexity at bay. WiGNet offers a promising solution toward the\ndeployment of vision GNNs in real-world applications. We publicly released the\ncode at https://github.com/EIDOSLAB/WiGNet.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00807v1",
    "published_date": "2024-10-01 15:54:07 UTC",
    "updated_date": "2024-10-01 15:54:07 UTC"
  },
  {
    "arxiv_id": "2410.00944v1",
    "title": "GAMMA-PD: Graph-based Analysis of Multi-Modal Motor Impairment Assessments in Parkinson's Disease",
    "authors": [
      "Favour Nerrise",
      "Alice Louise Heiman",
      "Ehsan Adeli"
    ],
    "abstract": "The rapid advancement of medical technology has led to an exponential\nincrease in multi-modal medical data, including imaging, genomics, and\nelectronic health records (EHRs). Graph neural networks (GNNs) have been widely\nused to represent this data due to their prominent performance in capturing\npairwise relationships. However, the heterogeneity and complexity of\nmulti-modal medical data still pose significant challenges for standard GNNs,\nwhich struggle with learning higher-order, non-pairwise relationships. This\npaper proposes GAMMA-PD (Graph-based Analysis of Multi-modal Motor Impairment\nAssessments in Parkinson's Disease), a novel heterogeneous hypergraph fusion\nframework for multi-modal clinical data analysis. GAMMA-PD integrates imaging\nand non-imaging data into a \"hypernetwork\" (patient population graph) by\npreserving higher-order information and similarity between patient profiles and\nsymptom subtypes. We also design a feature-based attention-weighted mechanism\nto interpret feature-level contributions towards downstream decision tasks. We\nevaluate our approach with clinical data from the Parkinson's Progression\nMarkers Initiative (PPMI) and a private dataset. We demonstrate gains in\npredicting motor impairment symptoms in Parkinson's disease. Our end-to-end\nframework also learns associations between subsets of patient characteristics\nto generate clinically relevant explanations for disease and symptom profiles.\nThe source code is available at https://github.com/favour-nerrise/GAMMA-PD.",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG",
      "eess.IV",
      "q-bio.NC"
    ],
    "primary_category": "q-bio.QM",
    "comment": "Accepted by the 6th Workshop on GRaphs in biomedicAl Image anaLysis\n  (GRAIL) at the 27th International Conference on Medical Image Computing and\n  Computer Assisted Intervention (MICCAI 2024). 12 pages, 3 figures, 2 tables,\n  Source Code: https://github.com/favour-nerrise/GAMMA-PD",
    "pdf_url": "http://arxiv.org/pdf/2410.00944v1",
    "published_date": "2024-10-01 15:51:33 UTC",
    "updated_date": "2024-10-01 15:51:33 UTC"
  },
  {
    "arxiv_id": "2410.11617v1",
    "title": "M$^{2}$M: Learning controllable Multi of experts and multi-scale operators are the Partial Differential Equations need",
    "authors": [
      "Aoming Liang",
      "Zhaoyang Mu",
      "Pengxiao Lin",
      "Cong Wang",
      "Mingming Ge",
      "Ling Shao",
      "Dixia Fan",
      "Hao Tang"
    ],
    "abstract": "Learning the evolutionary dynamics of Partial Differential Equations (PDEs)\nis critical in understanding dynamic systems, yet current methods\ninsufficiently learn their representations. This is largely due to the\nmulti-scale nature of the solution, where certain regions exhibit rapid\noscillations while others evolve more slowly. This paper introduces a framework\nof multi-scale and multi-expert (M$^2$M) neural operators designed to simulate\nand learn PDEs efficiently. We employ a divide-and-conquer strategy to train a\nmulti-expert gated network for the dynamic router policy. Our method\nincorporates a controllable prior gating mechanism that determines the\nselection rights of experts, enhancing the model's efficiency. To optimize the\nlearning process, we have implemented a PI (Proportional, Integral) control\nstrategy to adjust the allocation rules precisely. This universal controllable\napproach allows the model to achieve greater accuracy. We test our approach on\nbenchmark 2D Navier-Stokes equations and provide a custom multi-scale dataset.\nM$^2$M can achieve higher simulation accuracy and offer improved\ninterpretability compared to baseline methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "30 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.11617v1",
    "published_date": "2024-10-01 15:42:09 UTC",
    "updated_date": "2024-10-01 15:42:09 UTC"
  },
  {
    "arxiv_id": "2410.00774v1",
    "title": "Adaptive Motion Generation Using Uncertainty-Driven Foresight Prediction",
    "authors": [
      "Hyogo Hiruma",
      "Hiroshi Ito",
      "Tetusya Ogata"
    ],
    "abstract": "Uncertainty of environments has long been a difficult characteristic to\nhandle, when performing real-world robot tasks. This is because the uncertainty\nproduces unexpected observations that cannot be covered by manual scripting.\nLearning based robot controlling methods are a promising approach for\ngenerating flexible motions against unknown situations, but still tend to\nsuffer under uncertainty due to its deterministic nature. In order to\nadaptively perform the target task under such conditions, the robot control\nmodel must be able to accurately understand the possible uncertainty, and to\nexploratively derive the optimal action that minimizes such uncertainty. This\npaper extended an existing predictive learning based robot control method,\nwhich employ foresight prediction using dynamic internal simulation. The\nforesight module refines the model's hidden states by sampling multiple\npossible futures and replace with the one that led to the lower future\nuncertainty. The adaptiveness of the model was evaluated on a door opening\ntask. The door can be opened either by pushing, pulling, or sliding, but robot\ncannot visually distinguish which way, and is required to adapt on the fly. The\nresults showed that the proposed model adaptively diverged its motion through\ninteraction with the door, whereas conventional methods failed to stably\ndiverge. The models were analyzed on Lyapunov exponents of RNN hidden states\nwhich reflect the possible divergence at each time step during task execution.\nThe result indicated that the foresight module biased the model to consider\nfuture consequences, which lead to embedding uncertainties at the policy of the\nrobot controller, rather than the resultant observation. This is beneficial for\nimplementing adaptive behaviors, which indices derivation of diverse motion\nduring exploration.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00774v1",
    "published_date": "2024-10-01 15:13:27 UTC",
    "updated_date": "2024-10-01 15:13:27 UTC"
  },
  {
    "arxiv_id": "2410.00773v1",
    "title": "BabelBench: An Omni Benchmark for Code-Driven Analysis of Multimodal and Multistructured Data",
    "authors": [
      "Xuwu Wang",
      "Qiwen Cui",
      "Yunzhe Tao",
      "Yiran Wang",
      "Ziwei Chai",
      "Xiaotian Han",
      "Boyi Liu",
      "Jianbo Yuan",
      "Jing Su",
      "Guoyin Wang",
      "Tingkai Liu",
      "Liyu Chen",
      "Tianyi Liu",
      "Tao Sun",
      "Yufeng Zhang",
      "Sirui Zheng",
      "Quanzeng You",
      "Yang Yang",
      "Hongxia Yang"
    ],
    "abstract": "Large language models (LLMs) have become increasingly pivotal across various\ndomains, especially in handling complex data types. This includes structured\ndata processing, as exemplified by ChartQA and ChatGPT-Ada, and multimodal\nunstructured data processing as seen in Visual Question Answering (VQA). These\nareas have attracted significant attention from both industry and academia.\nDespite this, there remains a lack of unified evaluation methodologies for\nthese diverse data handling scenarios. In response, we introduce BabelBench, an\ninnovative benchmark framework that evaluates the proficiency of LLMs in\nmanaging multimodal multistructured data with code execution. BabelBench\nincorporates a dataset comprising 247 meticulously curated problems that\nchallenge the models with tasks in perception, commonsense reasoning, logical\nreasoning, and so on. Besides the basic capabilities of multimodal\nunderstanding, structured data processing as well as code generation, these\ntasks demand advanced capabilities in exploration, planning, reasoning and\ndebugging. Our experimental findings on BabelBench indicate that even\ncutting-edge models like ChatGPT 4 exhibit substantial room for improvement.\nThe insights derived from our comprehensive analysis offer valuable guidance\nfor future research within the community. The benchmark data can be found at\nhttps://github.com/FFD8FFE/babelbench.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00773v1",
    "published_date": "2024-10-01 15:11:24 UTC",
    "updated_date": "2024-10-01 15:11:24 UTC"
  },
  {
    "arxiv_id": "2410.00726v3",
    "title": "LTLf Synthesis on First-Order Agent Programs in Nondeterministic Environments",
    "authors": [
      "Till Hofmann",
      "Jens Claßen"
    ],
    "abstract": "We investigate the synthesis of policies for high-level agent programs\nexpressed in Golog, a language based on situation calculus that incorporates\nnondeterministic programming constructs. Unlike traditional approaches for\nprogram realization that assume full agent control or rely on incremental\nsearch, we address scenarios where environmental nondeterminism significantly\ninfluences program outcomes. Our synthesis problem involves deriving a policy\nthat successfully realizes a given Golog program while ensuring the\nsatisfaction of a temporal specification, expressed in Linear Temporal Logic on\nfinite traces (LTLf), across all possible environmental behaviors. By\nleveraging an expressive class of first-order action theories, we construct a\nfinite game arena that encapsulates program executions and tracks the\nsatisfaction of the temporal goal. A game-theoretic approach is employed to\nderive such a policy. Experimental results demonstrate this approach's\nfeasibility in domains with unbounded objects and non-local effects. This work\nbridges agent programming and temporal logic synthesis, providing a framework\nfor robust agent behavior in nondeterministic environments.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "comment": "AAAI'25",
    "pdf_url": "http://arxiv.org/pdf/2410.00726v3",
    "published_date": "2024-10-01 14:15:14 UTC",
    "updated_date": "2025-03-01 20:35:15 UTC"
  },
  {
    "arxiv_id": "2410.00704v1",
    "title": "Contrastive Abstraction for Reinforcement Learning",
    "authors": [
      "Vihang Patil",
      "Markus Hofmarcher",
      "Elisabeth Rumetshofer",
      "Sepp Hochreiter"
    ],
    "abstract": "Learning agents with reinforcement learning is difficult when dealing with\nlong trajectories that involve a large number of states. To address these\nlearning problems effectively, the number of states can be reduced by abstract\nrepresentations that cluster states. In principle, deep reinforcement learning\ncan find abstract states, but end-to-end learning is unstable. We propose\ncontrastive abstraction learning to find abstract states, where we assume that\nsuccessive states in a trajectory belong to the same abstract state. Such\nabstract states may be basic locations, achieved subgoals, inventory, or health\nconditions. Contrastive abstraction learning first constructs clusters of state\nrepresentations by contrastive learning and then applies modern Hopfield\nnetworks to determine the abstract states. The first phase of contrastive\nabstraction learning is self-supervised learning, where contrastive learning\nforces states with sequential proximity to have similar representations. The\nsecond phase uses modern Hopfield networks to map similar state representations\nto the same fixed point, i.e.\\ to an abstract state. The level of abstraction\ncan be adjusted by determining the number of fixed points of the modern\nHopfield network. Furthermore, \\textit{contrastive abstraction learning} does\nnot require rewards and facilitates efficient reinforcement learning for a wide\nrange of downstream tasks. Our experiments demonstrate the effectiveness of\ncontrastive abstraction learning for reinforcement learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00704v1",
    "published_date": "2024-10-01 13:56:09 UTC",
    "updated_date": "2024-10-01 13:56:09 UTC"
  },
  {
    "arxiv_id": "2410.00700v3",
    "title": "Mining Your Own Secrets: Diffusion Classifier Scores for Continual Personalization of Text-to-Image Diffusion Models",
    "authors": [
      "Saurav Jha",
      "Shiqi Yang",
      "Masato Ishii",
      "Mengjie Zhao",
      "Christian Simon",
      "Muhammad Jehanzeb Mirza",
      "Dong Gong",
      "Lina Yao",
      "Shusuke Takahashi",
      "Yuki Mitsufuji"
    ],
    "abstract": "Personalized text-to-image diffusion models have grown popular for their\nability to efficiently acquire a new concept from user-defined text\ndescriptions and a few images. However, in the real world, a user may wish to\npersonalize a model on multiple concepts but one at a time, with no access to\nthe data from previous concepts due to storage/privacy concerns. When faced\nwith this continual learning (CL) setup, most personalization methods fail to\nfind a balance between acquiring new concepts and retaining previous ones -- a\nchallenge that continual personalization (CP) aims to solve. Inspired by the\nsuccessful CL methods that rely on class-specific information for\nregularization, we resort to the inherent class-conditioned density estimates,\nalso known as diffusion classifier (DC) scores, for continual personalization\nof text-to-image diffusion models. Namely, we propose using DC scores for\nregularizing the parameter-space and function-space of text-to-image diffusion\nmodels, to achieve continual personalization. Using several diverse evaluation\nsetups, datasets, and metrics, we show that our proposed regularization-based\nCP methods outperform the state-of-the-art C-LoRA, and other baselines.\nFinally, by operating in the replay-free CL setup and on low-rank adapters, our\nmethod incurs zero storage and parameter overhead, respectively, over the\nstate-of-the-art. Our project page:\nhttps://srvcodes.github.io/continual_personalization/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.00700v3",
    "published_date": "2024-10-01 13:54:29 UTC",
    "updated_date": "2025-02-10 04:06:39 UTC"
  },
  {
    "arxiv_id": "2410.00690v2",
    "title": "Beyond Minimax Rates in Group Distributionally Robust Optimization via a Novel Notion of Sparsity",
    "authors": [
      "Quan Nguyen",
      "Nishant A. Mehta",
      "Cristóbal Guzmán"
    ],
    "abstract": "The minimax sample complexity of group distributionally robust optimization\n(GDRO) has been determined up to a $\\log(K)$ factor, where $K$ is the number of\ngroups. In this work, we venture beyond the minimax perspective via a novel\nnotion of sparsity that we dub $(\\lambda, \\beta)$-sparsity. In short, this\ncondition means that at any parameter $\\theta$, there is a set of at most\n$\\beta$ groups whose risks at $\\theta$ all are at least $\\lambda$ larger than\nthe risks of the other groups. To find an $\\epsilon$-optimal $\\theta$, we show\nvia a novel algorithm and analysis that the $\\epsilon$-dependent term in the\nsample complexity can swap a linear dependence on $K$ for a linear dependence\non the potentially much smaller $\\beta$. This improvement leverages recent\nprogress in sleeping bandits, showing a fundamental connection between the\ntwo-player zero-sum game optimization framework for GDRO and per-action regret\nbounds in sleeping bandits. We next show an adaptive algorithm which, up to log\nfactors, gets a sample complexity bound that adapts to the best $(\\lambda,\n\\beta)$-sparsity condition that holds. We also show how to get a dimension-free\nsemi-adaptive sample complexity bound with a computationally efficient method.\nFinally, we demonstrate the practicality of the $(\\lambda, \\beta)$-sparsity\ncondition and the improved sample efficiency of our algorithms on both\nsynthetic and real-life datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "44 pages. V2: updated a semi-adaptive approach and experimental\n  results",
    "pdf_url": "http://arxiv.org/pdf/2410.00690v2",
    "published_date": "2024-10-01 13:45:55 UTC",
    "updated_date": "2025-01-31 02:51:17 UTC"
  },
  {
    "arxiv_id": "2410.00689v2",
    "title": "Multimodal Auto Validation For Self-Refinement in Web Agents",
    "authors": [
      "Ruhana Azam",
      "Tamer Abuelsaad",
      "Aditya Vempaty",
      "Ashish Jagmohan"
    ],
    "abstract": "As our world digitizes, web agents that can automate complex and monotonous\ntasks are becoming essential in streamlining workflows. This paper introduces\nan approach to improving web agent performance through multi-modal validation\nand self-refinement. We present a comprehensive study of different modalities\n(text, vision) and the effect of hierarchy for the automatic validation of web\nagents, building upon the state-of-the-art Agent-E web automation framework. We\nalso introduce a self-refinement mechanism for web automation, using the\ndeveloped auto-validator, that enables web agents to detect and self-correct\nworkflow failures. Our results show significant gains on Agent-E's (a SOTA web\nagent) prior state-of-art performance, boosting task-completion rates from\n76.2\\% to 81.24\\% on the subset of the WebVoyager benchmark. The approach\npresented in this paper paves the way for more reliable digital assistants in\ncomplex, real-world scenarios.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00689v2",
    "published_date": "2024-10-01 13:43:55 UTC",
    "updated_date": "2024-10-11 15:42:52 UTC"
  },
  {
    "arxiv_id": "2410.00683v1",
    "title": "Efficient Technical Term Translation: A Knowledge Distillation Approach for Parenthetical Terminology Translation",
    "authors": [
      "Jiyoon Myung",
      "Jihyeon Park",
      "Jungki Son",
      "Kyungro Lee",
      "Joohyung Han"
    ],
    "abstract": "This paper addresses the challenge of accurately translating technical terms,\nwhich are crucial for clear communication in specialized fields. We introduce\nthe Parenthetical Terminology Translation (PTT) task, designed to mitigate\npotential inaccuracies by displaying the original term in parentheses alongside\nits translation. To implement this approach, we generated a representative PTT\ndataset using a collaborative approach with large language models and applied\nknowledge distillation to fine-tune traditional Neural Machine Translation\n(NMT) models and small-sized Large Language Models (sLMs). Additionally, we\ndeveloped a novel evaluation metric to assess both overall translation accuracy\nand the correct parenthetical presentation of terms. Our findings indicate that\nsLMs did not consistently outperform NMT models, with fine-tuning proving more\neffective than few-shot prompting, particularly in models with continued\npre-training in the target language. These insights contribute to the\nadvancement of more reliable terminology translation methodologies.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Paper accepted in EMNLPW 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.00683v1",
    "published_date": "2024-10-01 13:40:28 UTC",
    "updated_date": "2024-10-01 13:40:28 UTC"
  },
  {
    "arxiv_id": "2410.00681v1",
    "title": "Advanced Arabic Alphabet Sign Language Recognition Using Transfer Learning and Transformer Models",
    "authors": [
      "Mazen Balat",
      "Rewaa Awaad",
      "Hend Adel",
      "Ahmed B. Zaky",
      "Salah A. Aly"
    ],
    "abstract": "This paper presents an Arabic Alphabet Sign Language recognition approach,\nusing deep learning methods in conjunction with transfer learning and\ntransformer-based models. We study the performance of the different variants on\ntwo publicly available datasets, namely ArSL2018 and AASL. This task will make\nfull use of state-of-the-art CNN architectures like ResNet50, MobileNetV2, and\nEfficientNetB7, and the latest transformer models such as Google ViT and\nMicrosoft Swin Transformer. These pre-trained models have been fine-tuned on\nthe above datasets in an attempt to capture some unique features of Arabic sign\nlanguage motions. Experimental results present evidence that the suggested\nmethodology can receive a high recognition accuracy, by up to 99.6\\% and\n99.43\\% on ArSL2018 and AASL, respectively. That is far beyond the previously\nreported state-of-the-art approaches. This performance opens up even more\navenues for communication that may be more accessible to Arabic-speaking deaf\nand hard-of-hearing, and thus encourages an inclusive society.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 8 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.00681v1",
    "published_date": "2024-10-01 13:39:26 UTC",
    "updated_date": "2024-10-01 13:39:26 UTC"
  },
  {
    "arxiv_id": "2410.12820v1",
    "title": "A transformer-based deep reinforcement learning approach to spatial navigation in a partially observable Morris Water Maze",
    "authors": [
      "Marte Eggen",
      "Inga Strümke"
    ],
    "abstract": "Navigation is a fundamental cognitive skill extensively studied in\nneuroscientific experiments and has lately gained substantial interest in\nartificial intelligence research. Recreating the task solved by rodents in the\nwell-established Morris Water Maze (MWM) experiment, this work applies a\ntransformer-based architecture using deep reinforcement learning -- an approach\npreviously unexplored in this context -- to navigate a 2D version of the maze.\nSpecifically, the agent leverages a decoder-only transformer architecture\nserving as a deep Q-network performing effective decision making in the\npartially observable environment. We demonstrate that the proposed architecture\nenables the agent to efficiently learn spatial navigation strategies,\novercoming challenges associated with a limited field of vision, corresponding\nto the visual information available to a rodent in the MWM. Demonstrating the\npotential of transformer-based models for enhancing navigation performance in\npartially observable environments, this work suggests promising avenues for\nfuture research in artificial agents whose behavior resembles that of\nbiological agents. Finally, the flexibility of the transformer architecture in\nsupporting varying input sequence lengths opens opportunities for gaining\nincreased understanding of the artificial agent's inner representation of the\nenvironment.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.12820v1",
    "published_date": "2024-10-01 13:22:56 UTC",
    "updated_date": "2024-10-01 13:22:56 UTC"
  },
  {
    "arxiv_id": "2410.00659v1",
    "title": "Multimodal Coherent Explanation Generation of Robot Failures",
    "authors": [
      "Pradip Pramanick",
      "Silvia Rossi"
    ],
    "abstract": "The explainability of a robot's actions is crucial to its acceptance in\nsocial spaces. Explaining why a robot fails to complete a given task is\nparticularly important for non-expert users to be aware of the robot's\ncapabilities and limitations. So far, research on explaining robot failures has\nonly considered generating textual explanations, even though several studies\nhave shown the benefits of multimodal ones. However, a simple combination of\nmultiple modalities may lead to semantic incoherence between the information\nacross different modalities - a problem that is not well-studied. An incoherent\nmultimodal explanation can be difficult to understand, and it may even become\ninconsistent with what the robot and the human observe and how they perform\nreasoning with the observations. Such inconsistencies may lead to wrong\nconclusions about the robot's capabilities. In this paper, we introduce an\napproach to generate coherent multimodal explanations by checking the logical\ncoherence of explanations from different modalities, followed by refinements as\nrequired. We propose a classification approach for coherence assessment, where\nwe evaluate if an explanation logically follows another. Our experiments\nsuggest that fine-tuning a neural network that was pre-trained to recognize\ntextual entailment, performs well for coherence assessment of multimodal\nexplanations. Code & data: https://pradippramanick.github.io/coherent-explain/.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00659v1",
    "published_date": "2024-10-01 13:15:38 UTC",
    "updated_date": "2024-10-01 13:15:38 UTC"
  },
  {
    "arxiv_id": "2410.00654v1",
    "title": "Explainable Multi-Stakeholder Job Recommender Systems",
    "authors": [
      "Roan Schellingerhout"
    ],
    "abstract": "Public opinion on recommender systems has become increasingly wary in recent\nyears. In line with this trend, lawmakers have also started to become more\ncritical of such systems, resulting in the introduction of new laws focusing on\naspects such as privacy, fairness, and explainability for recommender systems\nand AI at large. These concepts are especially crucial in high-risk domains\nsuch as recruitment. In recruitment specifically, decisions carry substantial\nweight, as the outcomes can significantly impact individuals' careers and\ncompanies' success. Additionally, there is a need for a multi-stakeholder\napproach, as these systems are used by job seekers, recruiters, and companies\nsimultaneously, each with its own requirements and expectations. In this paper,\nI summarize my current research on the topic of explainable, multi-stakeholder\njob recommender systems and set out a number of future research directions.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "5 pages, 1 figure, to be published in ACM RecSys 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.00654v1",
    "published_date": "2024-10-01 13:12:30 UTC",
    "updated_date": "2024-10-01 13:12:30 UTC"
  },
  {
    "arxiv_id": "2410.00649v1",
    "title": "LASMP: Language Aided Subset Sampling Based Motion Planner",
    "authors": [
      "Saswati Bhattacharjee",
      "Anirban Sinha",
      "Chinwe Ekenna"
    ],
    "abstract": "This paper presents the Language Aided Subset Sampling Based Motion Planner\n(LASMP), a system that helps mobile robots plan their movements by using\nnatural language instructions. LASMP uses a modified version of the Rapidly\nExploring Random Tree (RRT) method, which is guided by user-provided commands\nprocessed through a language model (RoBERTa). The system improves efficiency by\nfocusing on specific areas of the robot's workspace based on these\ninstructions, making it faster and less resource-intensive. Compared to\ntraditional RRT methods, LASMP reduces the number of nodes needed by 55% and\ncuts random sample queries by 80%, while still generating safe, collision-free\npaths. Tested in both simulated and real-world environments, LASMP has shown\nbetter performance in handling complex indoor scenarios. The results highlight\nthe potential of combining language processing with motion planning to make\nrobot navigation more efficient.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 9 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.00649v1",
    "published_date": "2024-10-01 13:03:15 UTC",
    "updated_date": "2024-10-01 13:03:15 UTC"
  },
  {
    "arxiv_id": "2410.00630v1",
    "title": "Cafca: High-quality Novel View Synthesis of Expressive Faces from Casual Few-shot Captures",
    "authors": [
      "Marcel C. Bühler",
      "Gengyan Li",
      "Erroll Wood",
      "Leonhard Helminger",
      "Xu Chen",
      "Tanmay Shah",
      "Daoye Wang",
      "Stephan Garbin",
      "Sergio Orts-Escolano",
      "Otmar Hilliges",
      "Dmitry Lagun",
      "Jérémy Riviere",
      "Paulo Gotardo",
      "Thabo Beeler",
      "Abhimitra Meka",
      "Kripasindhu Sarkar"
    ],
    "abstract": "Volumetric modeling and neural radiance field representations have\nrevolutionized 3D face capture and photorealistic novel view synthesis.\nHowever, these methods often require hundreds of multi-view input images and\nare thus inapplicable to cases with less than a handful of inputs. We present a\nnovel volumetric prior on human faces that allows for high-fidelity expressive\nface modeling from as few as three input views captured in the wild. Our key\ninsight is that an implicit prior trained on synthetic data alone can\ngeneralize to extremely challenging real-world identities and expressions and\nrender novel views with fine idiosyncratic details like wrinkles and eyelashes.\nWe leverage a 3D Morphable Face Model to synthesize a large training set,\nrendering each identity with different expressions, hair, clothing, and other\nassets. We then train a conditional Neural Radiance Field prior on this\nsynthetic dataset and, at inference time, fine-tune the model on a very sparse\nset of real images of a single subject. On average, the fine-tuning requires\nonly three inputs to cross the synthetic-to-real domain gap. The resulting\npersonalized 3D model reconstructs strong idiosyncratic facial expressions and\noutperforms the state-of-the-art in high-quality novel view synthesis of faces\nfrom sparse inputs in terms of perceptual and photo-metric quality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Siggraph Asia Conference Papers 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.00630v1",
    "published_date": "2024-10-01 12:24:50 UTC",
    "updated_date": "2024-10-01 12:24:50 UTC"
  },
  {
    "arxiv_id": "2410.00589v1",
    "title": "GERA: Geometric Embedding for Efficient Point Registration Analysis",
    "authors": [
      "Geng Li",
      "Haozhi Cao",
      "Mingyang Liu",
      "Shenghai Yuan",
      "Jianfei Yang"
    ],
    "abstract": "Point cloud registration aims to provide estimated transformations to align\npoint clouds, which plays a crucial role in pose estimation of various\nnavigation systems, such as surgical guidance systems and autonomous vehicles.\nDespite the impressive performance of recent models on benchmark datasets, many\nrely on complex modules like KPConv and Transformers, which impose significant\ncomputational and memory demands. These requirements hinder their practical\napplication, particularly in resource-constrained environments such as mobile\nrobotics. In this paper, we propose a novel point cloud registration network\nthat leverages a pure MLP architecture, constructing geometric information\noffline. This approach eliminates the computational and memory burdens\nassociated with traditional complex feature extractors and significantly\nreduces inference time and resource consumption. Our method is the first to\nreplace 3D coordinate inputs with offline-constructed geometric encoding,\nimproving generalization and stability, as demonstrated by Maximum Mean\nDiscrepancy (MMD) comparisons. This efficient and accurate geometric\nrepresentation marks a significant advancement in point cloud analysis,\nparticularly for applications requiring fast and reliability.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00589v1",
    "published_date": "2024-10-01 11:19:56 UTC",
    "updated_date": "2024-10-01 11:19:56 UTC"
  },
  {
    "arxiv_id": "2410.12817v1",
    "title": "Interactive Explainable Anomaly Detection for Industrial Settings",
    "authors": [
      "Daniel Gramelt",
      "Timon Höfer",
      "Ute Schmid"
    ],
    "abstract": "Being able to recognise defects in industrial objects is a key element of\nquality assurance in production lines. Our research focuses on visual anomaly\ndetection in RGB images. Although Convolutional Neural Networks (CNNs) achieve\nhigh accuracies in this task, end users in industrial environments receive the\nmodel's decisions without additional explanations. Therefore, it is of interest\nto enrich the model's outputs with further explanations to increase confidence\nin the model and speed up anomaly detection. In our work, we focus on (1)\nCNN-based classification models and (2) the further development of a\nmodel-agnostic explanation algorithm for black-box classifiers. Additionally,\n(3) we demonstrate how we can establish an interactive interface that allows\nusers to further correct the model's output. We present our NearCAIPI\nInteraction Framework, which improves AI through user interaction, and show how\nthis approach increases the system's trustworthiness. We also illustrate how\nNearCAIPI can integrate human feedback into an interactive process chain.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.12817v1",
    "published_date": "2024-10-01 11:06:38 UTC",
    "updated_date": "2024-10-01 11:06:38 UTC"
  },
  {
    "arxiv_id": "2410.00564v3",
    "title": "Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining",
    "authors": [
      "Jie Cheng",
      "Ruixi Qiao",
      "Yingwei Ma",
      "Binhua Li",
      "Gang Xiong",
      "Qinghai Miao",
      "Yongbin Li",
      "Yisheng Lv"
    ],
    "abstract": "A significant aspiration of offline reinforcement learning (RL) is to develop\na generalist agent with high capabilities from large and heterogeneous\ndatasets. However, prior approaches that scale offline RL either rely heavily\non expert trajectories or struggle to generalize to diverse unseen tasks.\nInspired by the excellent generalization of world model in conditional video\ngeneration, we explore the potential of image observation-based world model for\nscaling offline RL and enhancing generalization on novel tasks. In this paper,\nwe introduce JOWA: Jointly-Optimized World-Action model, an offline model-based\nRL agent pretrained on multiple Atari games with 6 billion tokens data to learn\ngeneral-purpose representation and decision-making ability. Our method jointly\noptimizes a world-action model through a shared transformer backbone, which\nstabilize temporal difference learning with large models during pretraining.\nMoreover, we propose a provably efficient and parallelizable planning algorithm\nto compensate for the Q-value estimation error and thus search out better\npolicies. Experimental results indicate that our largest agent, with 150\nmillion parameters, achieves 78.9% human-level performance on pretrained games\nusing only 10% subsampled offline data, outperforming existing state-of-the-art\nlarge-scale offline RL baselines by 31.6% on averange. Furthermore, JOWA scales\nfavorably with model capacity and can sample-efficiently transfer to novel\ngames using only 5k offline fine-tuning data (approximately 4 trajectories) per\ngame, demonstrating superior generalization. We will release codes and model\nweights at https://github.com/CJReinforce/JOWA",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted by ICLR 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.00564v3",
    "published_date": "2024-10-01 10:25:03 UTC",
    "updated_date": "2025-03-03 02:59:29 UTC"
  },
  {
    "arxiv_id": "2410.00558v1",
    "title": "AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation",
    "authors": [
      "Ziyang Luo",
      "Xin Li",
      "Hongzhan Lin",
      "Jing Ma",
      "Lidong Bing"
    ],
    "abstract": "The impressive performance of proprietary LLMs like GPT4 in code generation\nhas led to a trend to replicate these capabilities in open-source models\nthrough knowledge distillation (e.g. Code Evol-Instruct). However, these\nefforts often neglect the crucial aspect of response quality, relying heavily\non teacher models for direct response distillation. This paradigm, especially\nfor complex instructions, can degrade the quality of synthesized data,\ncompromising the knowledge distillation process. To this end, our study\nintroduces the Adaptive Modular Response Evolution (AMR-Evol) framework, which\nemploys a two-stage process to refine response distillation. The first stage,\nmodular decomposition, breaks down the direct response into more manageable\nsub-modules. The second stage, adaptive response evolution, automatically\nevolves the response with the related function modules. Our experiments with\nthree popular code benchmarks (HumanEval, MBPP, and EvalPlus) attest to the\nsuperiority of the AMR-Evol framework over baseline response distillation\nmethods. By comparing with the open-source Code LLMs trained on a similar scale\nof data, we observed performance enhancements: more than +3.0 points on\nHumanEval-Plus and +1.0 points on MBPP-Plus, which underscores the\neffectiveness of our framework. Our codes are available at\nhttps://github.com/ChiYeungLaw/AMR-Evol.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.00558v1",
    "published_date": "2024-10-01 10:12:38 UTC",
    "updated_date": "2024-10-01 10:12:38 UTC"
  },
  {
    "arxiv_id": "2410.00536v1",
    "title": "Arges: Spatio-Temporal Transformer for Ulcerative Colitis Severity Assessment in Endoscopy Videos",
    "authors": [
      "Krishna Chaitanya",
      "Pablo F. Damasceno",
      "Shreyas Fadnavis",
      "Pooya Mobadersany",
      "Chaitanya Parmar",
      "Emily Scherer",
      "Natalia Zemlianskaia",
      "Lindsey Surace",
      "Louis R. Ghanem",
      "Oana Gabriela Cula",
      "Tommaso Mansi",
      "Kristopher Standish"
    ],
    "abstract": "Accurate assessment of disease severity from endoscopy videos in ulcerative\ncolitis (UC) is crucial for evaluating drug efficacy in clinical trials.\nSeverity is often measured by the Mayo Endoscopic Subscore (MES) and Ulcerative\nColitis Endoscopic Index of Severity (UCEIS) score. However, expert MES/UCEIS\nannotation is time-consuming and susceptible to inter-rater variability,\nfactors addressable by automation. Automation attempts with frame-level labels\nface challenges in fully-supervised solutions due to the prevalence of\nvideo-level labels in clinical trials. CNN-based weakly-supervised models (WSL)\nwith end-to-end (e2e) training lack generalization to new disease scores and\nignore spatio-temporal information crucial for accurate scoring. To address\nthese limitations, we propose \"Arges\", a deep learning framework that utilizes\na transformer with positional encoding to incorporate spatio-temporal\ninformation from frame features to estimate disease severity scores in\nendoscopy video. Extracted features are derived from a foundation model\n(ArgesFM), pre-trained on a large diverse dataset from multiple clinical trials\n(61M frames, 3927 videos). We evaluate four UC disease severity scores,\nincluding MES and three UCEIS component scores. Test set evaluation indicates\nsignificant improvements, with F1 scores increasing by 4.1% for MES and 18.8%,\n6.6%, 3.8% for the three UCEIS component scores compared to state-of-the-art\nmethods. Prospective validation on previously unseen clinical trial data\nfurther demonstrates the model's successful generalization.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "eess.IV",
    "comment": "12 pages, 2 figures, 5 tables, accepted at MLMI, MICCAI",
    "pdf_url": "http://arxiv.org/pdf/2410.00536v1",
    "published_date": "2024-10-01 09:23:14 UTC",
    "updated_date": "2024-10-01 09:23:14 UTC"
  },
  {
    "arxiv_id": "2410.00535v3",
    "title": "The Causal Information Bottleneck and Optimal Causal Variable Abstractions",
    "authors": [
      "Francisco N. F. Q. Simoes",
      "Mehdi Dastani",
      "Thijs van Ommen"
    ],
    "abstract": "To effectively study complex causal systems, it is often useful to construct\nabstractions of parts of the system by discarding irrelevant details while\npreserving key features. The Information Bottleneck (IB) method is a widely\nused approach to construct variable abstractions by compressing random\nvariables while retaining predictive power over a target variable. Traditional\nmethods like IB are purely statistical and ignore underlying causal structures,\nmaking them ill-suited for causal tasks. We propose the Causal Information\nBottleneck (CIB), a causal extension of the IB, which compresses a set of\nchosen variables while maintaining causal control over a target variable. This\nmethod produces abstractions of (sets of) variables which are causally\ninterpretable, give us insight about the interactions between the abstracted\nvariables and the target variable, and can be used when reasoning about\ninterventions. We present experimental results demonstrating that the learned\nabstractions accurately capture causal relations as intended.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.IT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "Submitted to UAI 2025. Code available at\n  github.com/francisco-simoes/cib-optimization-psagd",
    "pdf_url": "http://arxiv.org/pdf/2410.00535v3",
    "published_date": "2024-10-01 09:21:29 UTC",
    "updated_date": "2025-02-11 13:59:11 UTC"
  },
  {
    "arxiv_id": "2410.00531v1",
    "title": "TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices",
    "authors": [
      "Zonghang Li",
      "Wenjiao Feng",
      "Mohsen Guizani",
      "Hongfang Yu"
    ],
    "abstract": "Large model inference is shifting from cloud to edge due to concerns about\nthe privacy of user interaction data. However, edge devices often struggle with\nlimited computing power, memory, and bandwidth, requiring collaboration across\nmultiple devices to run and speed up LLM inference. Pipeline parallelism, the\nmainstream solution, is inefficient for single-user scenarios, while tensor\nparallelism struggles with frequent communications. In this paper, we argue\nthat tensor parallelism can be more effective than pipeline on low-resource\ndevices, and present a compute- and memory-efficient tensor parallel inference\nsystem, named TPI-LLM, to serve 70B-scale models. TPI-LLM keeps sensitive raw\ndata local in the users' devices and introduces a sliding window memory\nscheduler to dynamically manage layer weights during inference, with disk I/O\nlatency overlapped with the computation and communication. This allows larger\nmodels to run smoothly on memory-limited devices. We analyze the communication\nbottleneck and find that link latency, not bandwidth, emerges as the main\nissue, so a star-based allreduce algorithm is implemented. Through extensive\nexperiments on both emulated and real testbeds, TPI-LLM demonstrated over 80%\nless time-to-first-token and token latency compared to Accelerate, and over 90%\ncompared to Transformers and Galaxy, while cutting the peak memory footprint of\nLlama 2-70B by 90%, requiring only 3.1 GB of memory for 70B-scale models.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "68T50",
      "I.2.11"
    ],
    "primary_category": "cs.DC",
    "comment": "This paper is currently under review. Find the code at\n  https://github.com/Lizonghang/TPI-LLM",
    "pdf_url": "http://arxiv.org/pdf/2410.00531v1",
    "published_date": "2024-10-01 09:18:56 UTC",
    "updated_date": "2024-10-01 09:18:56 UTC"
  },
  {
    "arxiv_id": "2410.00519v1",
    "title": "Exploring the Learning Capabilities of Language Models using LEVERWORLDS",
    "authors": [
      "Eitan Wagner",
      "Amir Feder",
      "Omri Abend"
    ],
    "abstract": "Learning a model of a stochastic setting often involves learning both general\nstructure rules and specific properties of the instance. This paper\ninvestigates the interplay between learning the general and the specific in\nvarious learning methods, with emphasis on sample efficiency. We design a\nframework called {\\sc LeverWorlds}, which allows the generation of simple\nphysics-inspired worlds that follow a similar generative process with different\ndistributions, and their instances can be expressed in natural language. These\nworlds allow for controlled experiments to assess the sample complexity of\ndifferent learning methods. We experiment with classic learning algorithms as\nwell as Transformer language models, both with fine-tuning and In-Context\nLearning (ICL). Our general finding is that (1) Transformers generally succeed\nin the task; but (2) they are considerably less sample efficient than classic\nmethods that make stronger assumptions about the structure, such as Maximum\nLikelihood Estimation and Logistic Regression. This finding is in tension with\nthe recent tendency to use Transformers as general-purpose estimators. We\npropose an approach that leverages the ICL capabilities of contemporary\nlanguage models to apply simple algorithms for this type of data. Our\nexperiments show that models currently struggle with the task but show\npromising potential.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00519v1",
    "published_date": "2024-10-01 09:02:13 UTC",
    "updated_date": "2024-10-01 09:02:13 UTC"
  },
  {
    "arxiv_id": "2410.00517v1",
    "title": "Human-Robot Collaborative Minimum Time Search through Sub-priors in Ant Colony Optimization",
    "authors": [
      "Oscar Gil Viyuela",
      "Alberto Sanfeliu"
    ],
    "abstract": "Human-Robot Collaboration (HRC) has evolved into a highly promising issue\nowing to the latest breakthroughs in Artificial Intelligence (AI) and\nHuman-Robot Interaction (HRI), among other reasons. This emerging growth\nincreases the need to design multi-agent algorithms that can manage also human\npreferences. This paper presents an extension of the Ant Colony Optimization\n(ACO) meta-heuristic to solve the Minimum Time Search (MTS) task, in the case\nwhere humans and robots perform an object searching task together. The proposed\nmodel consists of two main blocks. The first one is a convolutional neural\nnetwork (CNN) that provides the prior probabilities about where an object may\nbe from a segmented image. The second one is the Sub-prior MTS-ACO algorithm\n(SP-MTS-ACO), which takes as inputs the prior probabilities and the particular\nsearch preferences of the agents in different sub-priors to generate search\nplans for all agents. The model has been tested in real experiments for the\njoint search of an object through a Vizanti web-based visualization in a tablet\ncomputer. The designed interface allows the communication between a human and\nour humanoid robot named IVO. The obtained results show an improvement in the\nsearch perception of the users without loss of efficiency.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00517v1",
    "published_date": "2024-10-01 08:57:28 UTC",
    "updated_date": "2024-10-01 08:57:28 UTC"
  },
  {
    "arxiv_id": "2410.00516v1",
    "title": "Enhancing Sentinel-2 Image Resolution: Evaluating Advanced Techniques based on Convolutional and Generative Neural Networks",
    "authors": [
      "Patrick Kramer",
      "Alexander Steinhardt",
      "Barbara Pedretscher"
    ],
    "abstract": "This paper investigates the enhancement of spatial resolution in Sentinel-2\nbands that contain spectral information using advanced super-resolution\ntechniques by a factor of 2. State-of-the-art CNN models are compared with\nenhanced GAN approaches in terms of quality and feasibility. Therefore, a\nrepresentative dataset comprising Sentinel-2 low-resolution images and\ncorresponding high-resolution aerial orthophotos is required. Literature study\nreveals no feasible dataset for the land type of interest (forests), for which\nreason an adequate dataset had to be generated in addition, accounting for\naccurate alignment and image source optimization. The results reveal that while\nCNN-based approaches produce satisfactory outcomes, they tend to yield blurry\nimages. In contrast, GAN-based models not only provide clear and detailed\nimages, but also demonstrate superior performance in terms of quantitative\nassessment, underlying the potential of the framework beyond the specific land\ntype investigated.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.IV",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "pdf_url": "http://arxiv.org/pdf/2410.00516v1",
    "published_date": "2024-10-01 08:56:46 UTC",
    "updated_date": "2024-10-01 08:56:46 UTC"
  },
  {
    "arxiv_id": "2410.00513v1",
    "title": "Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing",
    "authors": [
      "Deokhyung Kang",
      "Seonjeong Hwang",
      "Yunsu Kim",
      "Gary Geunbae Lee"
    ],
    "abstract": "Recent efforts have aimed to utilize multilingual pretrained language models\n(mPLMs) to extend semantic parsing (SP) across multiple languages without\nrequiring extensive annotations. However, achieving zero-shot cross-lingual\ntransfer for SP remains challenging, leading to a performance gap between\nsource and target languages. In this study, we propose Cross-Lingual\nBack-Parsing (CBP), a novel data augmentation methodology designed to enhance\ncross-lingual transfer for SP. Leveraging the representation geometry of the\nmPLMs, CBP synthesizes target language utterances from source meaning\nrepresentations. Our methodology effectively performs cross-lingual data\naugmentation in challenging zero-resource settings, by utilizing only labeled\ndata in the source language and monolingual corpora. Extensive experiments on\ntwo cross-language SP benchmarks (Mschema2QA and Xspider) demonstrate that CBP\nbrings substantial gains in the target language. Further analysis of the\nsynthesized utterances shows that our method successfully generates target\nlanguage utterances with high slot value alignment rates while preserving\nsemantic integrity. Our codes and data are publicly available at\nhttps://github.com/deokhk/CBP.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.00513v1",
    "published_date": "2024-10-01 08:53:38 UTC",
    "updated_date": "2024-10-01 08:53:38 UTC"
  },
  {
    "arxiv_id": "2410.00511v1",
    "title": "Pre-training with Synthetic Patterns for Audio",
    "authors": [
      "Yuchi Ishikawa",
      "Tatsuya Komatsu",
      "Yoshimitsu Aoki"
    ],
    "abstract": "In this paper, we propose to pre-train audio encoders using synthetic\npatterns instead of real audio data. Our proposed framework consists of two key\nelements. The first one is Masked Autoencoder (MAE), a self-supervised learning\nframework that learns from reconstructing data from randomly masked\ncounterparts. MAEs tend to focus on low-level information such as visual\npatterns and regularities within data. Therefore, it is unimportant what is\nportrayed in the input, whether it be images, audio mel-spectrograms, or even\nsynthetic patterns. This leads to the second key element, which is synthetic\ndata. Synthetic data, unlike real audio, is free from privacy and licensing\ninfringement issues. By combining MAEs and synthetic patterns, our framework\nenables the model to learn generalized feature representations without real\ndata, while addressing the issues related to real audio. To evaluate the\nefficacy of our framework, we conduct extensive experiments across a total of\n13 audio tasks and 17 synthetic datasets. The experiments provide insights into\nwhich types of synthetic patterns are effective for audio. Our results\ndemonstrate that our framework achieves performance comparable to models\npre-trained on AudioSet-2M and partially outperforms image-based pre-training\nmethods.",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "eess.AS",
    "comment": "Submitted to ICASSP'25",
    "pdf_url": "http://arxiv.org/pdf/2410.00511v1",
    "published_date": "2024-10-01 08:52:35 UTC",
    "updated_date": "2024-10-01 08:52:35 UTC"
  },
  {
    "arxiv_id": "2410.00508v2",
    "title": "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization",
    "authors": [
      "Mingye Zhu",
      "Yi Liu",
      "Quan Wang",
      "Junbo Guo",
      "Zhendong Mao"
    ],
    "abstract": "Recent breakthroughs in preference alignment have significantly improved\nLarge Language Models' ability to generate texts that align with human\npreferences and values. However, current alignment metrics typically emphasize\nthe post-hoc overall improvement, while overlooking a critical aspect:\nregression, which refers to the backsliding on previously correctly-handled\ndata after updates. This potential pitfall may arise from excessive fine-tuning\non already well-aligned data, which subsequently leads to over-alignment and\ndegeneration. To address this challenge, we propose FlipGuard, a constrained\noptimization approach to detect and mitigate update regression with focal\nattention. Specifically, FlipGuard identifies performance degradation using a\ncustomized reward characterization and strategically enforces a constraint to\nencourage conditional congruence with the pre-aligned model during training.\nComprehensive experiments demonstrate that FlipGuard effectively alleviates\nupdate regression while demonstrating excellent overall performance, with the\nadded benefit of knowledge preservation while aligning preferences.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted by EMNLP 2024 Main track",
    "pdf_url": "http://arxiv.org/pdf/2410.00508v2",
    "published_date": "2024-10-01 08:46:59 UTC",
    "updated_date": "2024-10-14 10:34:32 UTC"
  },
  {
    "arxiv_id": "2410.03743v1",
    "title": "Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging",
    "authors": [
      "Yiming Ju",
      "Ziyi Ni",
      "Xingrun Xing",
      "Zhixiong Zeng",
      "hanyu Zhao",
      "Siqi Fan",
      "Zheng Zhang"
    ],
    "abstract": "Supervised fine-tuning (SFT) is crucial for adapting Large Language Models\n(LLMs) to specific tasks. In this work, we demonstrate that the order of\ntraining data can lead to significant training imbalances, potentially\nresulting in performance degradation. Consequently, we propose to mitigate this\nimbalance by merging SFT models fine-tuned with different data orders, thereby\nenhancing the overall effectiveness of SFT. Additionally, we introduce a novel\ntechnique, \"parameter-selection merging,\" which outperforms traditional\nweighted-average methods on five datasets. Further, through analysis and\nablation studies, we validate the effectiveness of our method and identify the\nsources of performance improvements.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.03743v1",
    "published_date": "2024-10-01 08:44:31 UTC",
    "updated_date": "2024-10-01 08:44:31 UTC"
  },
  {
    "arxiv_id": "2410.02827v1",
    "title": "Effective Intrusion Detection for UAV Communications using Autoencoder-based Feature Extraction and Machine Learning Approach",
    "authors": [
      "Tuan-Cuong Vuong",
      "Cong Chi Nguyen",
      "Van-Cuong Pham",
      "Thi-Thanh-Huyen Le",
      "Xuan-Nam Tran",
      "Thien Van Luong"
    ],
    "abstract": "This paper proposes a novel intrusion detection method for unmanned aerial\nvehicles (UAV) in the presence of recent actual UAV intrusion dataset. In\nparticular, in the first stage of our method, we design an autoencoder\narchitecture for effectively extracting important features, which are then fed\ninto various machine learning models in the second stage for detecting and\nclassifying attack types. To the best of our knowledge, this is the first\nattempt to propose such the autoencoder-based machine learning intrusion\ndetection method for UAVs using actual dataset, while most of existing works\nonly consider either simulated datasets or datasets irrelevant to UAV\ncommunications. Our experiment results show that the proposed method\noutperforms the baselines such as feature selection schemes in both binary and\nmulti-class classification tasks.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.RO",
    "comment": "4 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.02827v1",
    "published_date": "2024-10-01 08:44:23 UTC",
    "updated_date": "2024-10-01 08:44:23 UTC"
  },
  {
    "arxiv_id": "2410.00503v2",
    "title": "Drone Stereo Vision for Radiata Pine Branch Detection and Distance Measurement: Utilizing Deep Learning and YOLO Integration",
    "authors": [
      "Yida Lin",
      "Bing Xue",
      "Mengjie Zhang",
      "Sam Schofield",
      "Richard Green"
    ],
    "abstract": "This research focuses on the development of a drone equipped with pruning\ntools and a stereo vision camera to accurately detect and measure the spatial\npositions of tree branches. YOLO is employed for branch segmentation, while two\ndepth estimation approaches, monocular and stereo, are investigated. In\ncomparison to SGBM, deep learning techniques produce more refined and accurate\ndepth maps. In the absence of ground-truth data, a fine-tuning process using\ndeep neural networks is applied to approximate optimal depth values. This\nmethodology facilitates precise branch detection and distance measurement,\naddressing critical challenges in the automation of pruning operations. The\nresults demonstrate notable advancements in both accuracy and efficiency,\nunderscoring the potential of deep learning to drive innovation and enhance\nautomation in the agricultural sector.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00503v2",
    "published_date": "2024-10-01 08:34:00 UTC",
    "updated_date": "2024-10-06 07:34:52 UTC"
  },
  {
    "arxiv_id": "2410.00502v1",
    "title": "Multi-Target Cross-Lingual Summarization: a novel task and a language-neutral approach",
    "authors": [
      "Diogo Pernes",
      "Gonçalo M. Correia",
      "Afonso Mendes"
    ],
    "abstract": "Cross-lingual summarization aims to bridge language barriers by summarizing\ndocuments in different languages. However, ensuring semantic coherence across\nlanguages is an overlooked challenge and can be critical in several contexts.\nTo fill this gap, we introduce multi-target cross-lingual summarization as the\ntask of summarizing a document into multiple target languages while ensuring\nthat the produced summaries are semantically similar. We propose a principled\nre-ranking approach to this problem and a multi-criteria evaluation protocol to\nassess semantic coherence across target languages, marking a first step that\nwill hopefully stimulate further research on this problem.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024 (Findings)",
    "pdf_url": "http://arxiv.org/pdf/2410.00502v1",
    "published_date": "2024-10-01 08:33:57 UTC",
    "updated_date": "2024-10-01 08:33:57 UTC"
  },
  {
    "arxiv_id": "2410.00490v1",
    "title": "Learning Adaptive Hydrodynamic Models Using Neural ODEs in Complex Conditions",
    "authors": [
      "Cong Wang",
      "Aoming Liang",
      "Fei Han",
      "Xinyu Zeng",
      "Zhibin Li",
      "Dixia Fan",
      "Jens Kober"
    ],
    "abstract": "Reinforcement learning-based quadruped robots excel across various terrains\nbut still lack the ability to swim in water due to the complex underwater\nenvironment. This paper presents the development and evaluation of a\ndata-driven hydrodynamic model for amphibious quadruped robots, aiming to\nenhance their adaptive capabilities in complex and dynamic underwater\nenvironments. The proposed model leverages Neural Ordinary Differential\nEquations (ODEs) combined with attention mechanisms to accurately process and\ninterpret real-time sensor data. The model enables the quadruped robots to\nunderstand and predict complex environmental patterns, facilitating robust\ndecision-making strategies. We harness real-time sensor data, capturing various\nenvironmental and internal state parameters to train and evaluate our model. A\nsignificant focus of our evaluation involves testing the quadruped robot's\nperformance across different hydrodynamic conditions and assessing its\ncapabilities at varying speeds and fluid dynamic conditions. The outcomes\nsuggest that the model can effectively learn and adapt to varying conditions,\nenabling the prediction of force states and enhancing autonomous robotic\nbehaviors in various practical scenarios.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "8 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.00490v1",
    "published_date": "2024-10-01 08:18:36 UTC",
    "updated_date": "2024-10-01 08:18:36 UTC"
  },
  {
    "arxiv_id": "2410.00483v1",
    "title": "MCGM: Mask Conditional Text-to-Image Generative Model",
    "authors": [
      "Rami Skaik",
      "Leonardo Rossi",
      "Tomaso Fontanini",
      "Andrea Prati"
    ],
    "abstract": "Recent advancements in generative models have revolutionized the field of\nartificial intelligence, enabling the creation of highly-realistic and detailed\nimages. In this study, we propose a novel Mask Conditional Text-to-Image\nGenerative Model (MCGM) that leverages the power of conditional diffusion\nmodels to generate pictures with specific poses. Our model builds upon the\nsuccess of the Break-a-scene [1] model in generating new scenes using a single\nimage with multiple subjects and incorporates a mask embedding injection that\nallows the conditioning of the generation process. By introducing this\nadditional level of control, MCGM offers a flexible and intuitive approach for\ngenerating specific poses for one or more subjects learned from a single image,\nempowering users to influence the output based on their requirements. Through\nextensive experimentation and evaluation, we demonstrate the effectiveness of\nour proposed model in generating high-quality images that meet predefined mask\nconditions and improving the current Break-a-scene generative model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "17 pages, 13 figures, presented at the 5th International Conference\n  on Artificial Intelligence and Machine Learning (CAIML 2024)",
    "pdf_url": "http://arxiv.org/pdf/2410.00483v1",
    "published_date": "2024-10-01 08:13:47 UTC",
    "updated_date": "2024-10-01 08:13:47 UTC"
  },
  {
    "arxiv_id": "2410.00475v4",
    "title": "Probabilistic Analysis of Copyright Disputes and Generative AI Safety",
    "authors": [
      "Hiroaki Chiba-Okabe"
    ],
    "abstract": "This paper presents a probabilistic approach to analyzing copyright\ninfringement disputes. Under this approach, evidentiary principles shaped by\ncase law are formalized in probabilistic terms, allowing for a mathematical\nexamination of issues arising in such disputes. The usefulness of this approach\nis showcased through its application to the ``inverse ratio rule'' -- a\ncontroversial legal doctrine adopted by some courts. Although this rule has\nfaced significant criticism, a formal proof demonstrates its validity, provided\nit is properly defined. Furthermore, the paper employs the probabilistic\napproach to study the copyright safety of generative AI. Specifically, the Near\nAccess-Free (NAF) condition, previously proposed as a strategy for mitigating\nthe heightened copyright infringement risks of generative AI, is evaluated. The\nanalysis reveals that, while the NAF condition mitigates some infringement\nrisks, its justifiability and efficacy are questionable in certain contexts.\nThese findings illustrate how taking a probabilistic perspective can enhance\nour understanding of copyright jurisprudence and its interaction with\ngenerative AI technology.",
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "primary_category": "cs.CY",
    "comment": "9 pages",
    "pdf_url": "http://arxiv.org/pdf/2410.00475v4",
    "published_date": "2024-10-01 08:05:19 UTC",
    "updated_date": "2025-01-25 02:14:36 UTC"
  },
  {
    "arxiv_id": "2410.00467v3",
    "title": "Dynamic Planning for LLM-based Graphical User Interface Automation",
    "authors": [
      "Shaoqing Zhang",
      "Zhuosheng Zhang",
      "Kehai Chen",
      "Xinbei Ma",
      "Muyun Yang",
      "Tiejun Zhao",
      "Min Zhang"
    ],
    "abstract": "The advent of large language models (LLMs) has spurred considerable interest\nin advancing autonomous LLMs-based agents, particularly in intriguing\napplications within smartphone graphical user interfaces (GUIs). When presented\nwith a task goal, these agents typically emulate human actions within a GUI\nenvironment until the task is completed. However, a key challenge lies in\ndevising effective plans to guide action prediction in GUI tasks, though\nplanning have been widely recognized as effective for decomposing complex tasks\ninto a series of steps. Specifically, given the dynamic nature of environmental\nGUIs following action execution, it is crucial to dynamically adapt plans based\non environmental feedback and action history.We show that the widely-used ReAct\napproach fails due to the excessively long historical dialogues. To address\nthis challenge, we propose a novel approach called Dynamic Planning of Thoughts\n(D-PoT) for LLM-based GUI agents.D-PoT involves the dynamic adjustment of\nplanning based on the environmental feedback and execution history.\nExperimental results reveal that the proposed D-PoT significantly surpassed the\nstrong GPT-4V baseline by +12.7% (34.66% $\\rightarrow$ 47.36%) in accuracy. The\nanalysis highlights the generality of dynamic planning in different backbone\nLLMs, as well as the benefits in mitigating hallucinations and adapting to\nunseen tasks. Code is available at https://github.com/sqzhang-lazy/D-PoT.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "comment": "EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.00467v3",
    "published_date": "2024-10-01 07:49:24 UTC",
    "updated_date": "2024-12-19 14:29:31 UTC"
  },
  {
    "arxiv_id": "2410.03742v2",
    "title": "Beyond Scalar Reward Model: Learning Generative Judge from Preference Data",
    "authors": [
      "Ziyi Ye",
      "Xiangsheng Li",
      "Qiuchi Li",
      "Qingyao Ai",
      "Yujia Zhou",
      "Wei Shen",
      "Dong Yan",
      "Yiqun Liu"
    ],
    "abstract": "Learning from preference feedback is a common practice for aligning large\nlanguage models~(LLMs) with human value. Conventionally, preference data is\nlearned and encoded into a scalar reward model that connects a value head with\nan LLM to produce a scalar score as preference or reward. However, scalar\nmodels lack interpretability and are known to be susceptible to biases in\ndatasets. This paper investigates leveraging the generation capability of LLMs\nto address both limitations in one shot. Specifically, we prompt the\npre-trained LLM to generate positive and negative judgments, both supported\nwith rationales in natural language form. The self-generated contrastive\njudgment pairs are used to train the generative judge with Direct Preference\nOptimization (DPO). This proposal of training the generative Judge using\nself-generated Contrastive judgments (Con-J) ensures natural interpretability\ndue to the generated rationales together with the judgments, as well as high\nrobustness against bias without the need for an additional reward head.\nExperimental results show that the performance of Con-J is comparable to the\nscalar reward model trained on the same collection of preference data, and\ndemonstrate its superior interpretability and robustness in encoding human\npreferences.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03742v2",
    "published_date": "2024-10-01 07:38:58 UTC",
    "updated_date": "2024-10-13 10:21:29 UTC"
  },
  {
    "arxiv_id": "2410.01847v2",
    "title": "Bayes-CATSI: A variational Bayesian deep learning framework for medical time series data imputation",
    "authors": [
      "Omkar Kulkarni",
      "Rohitash Chandra"
    ],
    "abstract": "Medical time series datasets feature missing values that need data imputation\nmethods, however, conventional machine learning models fall short due to a lack\nof uncertainty quantification in predictions. Among these models, the CATSI\n(Context-Aware Time Series Imputation) stands out for its effectiveness by\nincorporating a context vector into the imputation process, capturing the\nglobal dependencies of each patient. In this paper, we propose a Bayesian\nContext-Aware Time Series Imputation (Bayes-CATSI) framework which leverages\nuncertainty quantification offered by variational inference. We consider the\ntime series derived from electroencephalography (EEG), electrooculography\n(EOG), electromyography (EMG), electrocardiology (EKG). Variational Inference\nassumes the shape of the posterior distribution and through minimization of the\nKullback-Leibler(KL) divergence it finds variational densities that are closest\nto the true posterior distribution. Thus , we integrate the variational\nBayesian deep learning layers into the CATSI model. Our results show that\nBayes-CATSI not only provides uncertainty quantification but also achieves\nsuperior imputation performance compared to the CATSI model. Specifically, an\ninstance of Bayes-CATSI outperforms CATSI by 9.57 %. We provide an open-source\ncode implementation for applying Bayes-CATSI to other medical data imputation\nproblems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.01847v2",
    "published_date": "2024-10-01 07:37:52 UTC",
    "updated_date": "2024-10-04 01:55:55 UTC"
  },
  {
    "arxiv_id": "2410.00451v3",
    "title": "Unleashing the Unseen: Harnessing Benign Datasets for Jailbreaking Large Language Models",
    "authors": [
      "Wei Zhao",
      "Zhe Li",
      "Yige Li",
      "Jun Sun"
    ],
    "abstract": "Despite significant ongoing efforts in safety alignment, large language\nmodels (LLMs) such as GPT-4 and LLaMA 3 remain vulnerable to jailbreak attacks\nthat can induce harmful behaviors, including through the use of adversarial\nsuffixes. Building on prior research, we hypothesize that these adversarial\nsuffixes are not mere bugs but may represent features that can dominate the\nLLM's behavior. To evaluate this hypothesis, we conduct several experiments.\nFirst, we demonstrate that benign features can be effectively made to function\nas adversarial suffixes, i.e., we develop a feature extraction method to\nextract sample-agnostic features from benign dataset in the form of suffixes\nand show that these suffixes may effectively compromise safety alignment.\nSecond, we show that adversarial suffixes generated from jailbreak attacks may\ncontain meaningful features, i.e., appending the same suffix to different\nprompts results in responses exhibiting specific characteristics. Third, we\nshow that such benign-yet-safety-compromising features can be easily introduced\nthrough fine-tuning using only benign datasets. As a result, we are able to\ncompletely eliminate GPT's safety alignment in a blackbox setting through\nfinetuning with only benign data. Our code and data is available at\n\\url{https://github.com/suffix-maybe-feature/adver-suffix-maybe-features}.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00451v3",
    "published_date": "2024-10-01 07:11:55 UTC",
    "updated_date": "2024-12-19 05:32:59 UTC"
  },
  {
    "arxiv_id": "2410.00441v2",
    "title": "ReXplain: Translating Radiology into Patient-Friendly Video Reports",
    "authors": [
      "Luyang Luo",
      "Jenanan Vairavamurthy",
      "Xiaoman Zhang",
      "Abhinav Kumar",
      "Ramon R. Ter-Oganesyan",
      "Stuart T. Schroff",
      "Dan Shilo",
      "Rydhwana Hossain",
      "Mike Moritz",
      "Pranav Rajpurkar"
    ],
    "abstract": "Radiology reports, designed for efficient communication between medical\nexperts, often remain incomprehensible to patients. This inaccessibility could\npotentially lead to anxiety, decreased engagement in treatment decisions, and\npoorer health outcomes, undermining patient-centered care. We present ReXplain\n(Radiology eXplanation), an innovative AI-driven system that translates\nradiology findings into patient-friendly video reports. ReXplain uniquely\nintegrates a large language model for medical text simplification and\ntext-anatomy association, an image segmentation model for anatomical region\nidentification, and an avatar generation tool for engaging interface\nvisualization. ReXplain enables producing comprehensive explanations with plain\nlanguage, highlighted imagery, and 3D organ renderings in the form of video\nreports. To evaluate the utility of ReXplain-generated explanations, we\nconducted two rounds of user feedback collection from six board-certified\nradiologists. The results of this proof-of-concept study indicate that ReXplain\ncould accurately deliver radiological information and effectively simulate\none-on-one consultation, shedding light on enhancing patient-centered radiology\nwith potential clinical usage. This work demonstrates a new paradigm in\nAI-assisted medical communication, potentially improving patient engagement and\nsatisfaction in radiology care, and opens new avenues for research in\nmultimodal medical communication.",
    "categories": [
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.AI",
    "comment": "12 pages. The project page is\n  https://www.rajpurkarlab.hms.harvard.edu/rexplain",
    "pdf_url": "http://arxiv.org/pdf/2410.00441v2",
    "published_date": "2024-10-01 06:41:18 UTC",
    "updated_date": "2024-12-17 22:28:04 UTC"
  },
  {
    "arxiv_id": "2410.03741v1",
    "title": "Towards Democratization of Subspeciality Medical Expertise",
    "authors": [
      "Jack W. O'Sullivan",
      "Anil Palepu",
      "Khaled Saab",
      "Wei-Hung Weng",
      "Yong Cheng",
      "Emily Chu",
      "Yaanik Desai",
      "Aly Elezaby",
      "Daniel Seung Kim",
      "Roy Lan",
      "Wilson Tang",
      "Natalie Tapaskar",
      "Victoria Parikh",
      "Sneha S. Jain",
      "Kavita Kulkarni",
      "Philip Mansfield",
      "Dale Webster",
      "Juraj Gottweis",
      "Joelle Barral",
      "Mike Schaekermann",
      "Ryutaro Tanno",
      "S. Sara Mahdavi",
      "Vivek Natarajan",
      "Alan Karthikesalingam",
      "Euan Ashley",
      "Tao Tu"
    ],
    "abstract": "The scarcity of subspecialist medical expertise, particularly in rare,\ncomplex and life-threatening diseases, poses a significant challenge for\nhealthcare delivery. This issue is particularly acute in cardiology where\ntimely, accurate management determines outcomes. We explored the potential of\nAMIE (Articulate Medical Intelligence Explorer), a large language model\n(LLM)-based experimental AI system optimized for diagnostic dialogue, to\npotentially augment and support clinical decision-making in this challenging\ncontext. We curated a real-world dataset of 204 complex cases from a\nsubspecialist cardiology practice, including results for electrocardiograms,\nechocardiograms, cardiac MRI, genetic tests, and cardiopulmonary stress tests.\nWe developed a ten-domain evaluation rubric used by subspecialists to evaluate\nthe quality of diagnosis and clinical management plans produced by general\ncardiologists or AMIE, the latter enhanced with web-search and self-critique\ncapabilities. AMIE was rated superior to general cardiologists for 5 of the 10\ndomains (with preference ranging from 9% to 20%), and equivalent for the rest.\nAccess to AMIE's response improved cardiologists' overall response quality in\n63.7% of cases while lowering quality in just 3.4%. Cardiologists' responses\nwith access to AMIE were superior to cardiologist responses without access to\nAMIE for all 10 domains. Qualitative examinations suggest AMIE and general\ncardiologist could complement each other, with AMIE thorough and sensitive,\nwhile general cardiologist concise and specific. Overall, our results suggest\nthat specialized medical LLMs have the potential to augment general\ncardiologists' capabilities by bridging gaps in subspecialty expertise, though\nfurther research and validation are essential for wide clinical utility.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03741v1",
    "published_date": "2024-10-01 06:34:31 UTC",
    "updated_date": "2024-10-01 06:34:31 UTC"
  },
  {
    "arxiv_id": "2410.00432v1",
    "title": "Scalable Multi-Task Transfer Learning for Molecular Property Prediction",
    "authors": [
      "Chanhui Lee",
      "Dae-Woong Jeong",
      "Sung Moon Ko",
      "Sumin Lee",
      "Hyunseung Kim",
      "Soorin Yim",
      "Sehui Han",
      "Sungwoong Kim",
      "Sungbin Lim"
    ],
    "abstract": "Molecules have a number of distinct properties whose importance and\napplication vary. Often, in reality, labels for some properties are hard to\nachieve despite their practical importance. A common solution to such data\nscarcity is to use models of good generalization with transfer learning. This\ninvolves domain experts for designing source and target tasks whose features\nare shared. However, this approach has limitations: i). Difficulty in accurate\ndesign of source-target task pairs due to the large number of tasks, and ii).\ncorresponding computational burden verifying many trials and errors of transfer\nlearning design, thereby iii). constraining the potential of foundation\nmodeling of multi-task molecular property prediction. We address the\nlimitations of the manual design of transfer learning via data-driven bi-level\noptimization. The proposed method enables scalable multi-task transfer learning\nfor molecular property prediction by automatically obtaining the optimal\ntransfer ratios. Empirically, the proposed method improved the prediction\nperformance of 40 molecular properties and accelerated training convergence.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00432v1",
    "published_date": "2024-10-01 06:28:14 UTC",
    "updated_date": "2024-10-01 06:28:14 UTC"
  },
  {
    "arxiv_id": "2410.00428v3",
    "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV Cache Management",
    "authors": [
      "Yi Xiong",
      "Hao Wu",
      "Changxu Shao",
      "Ziqing Wang",
      "Rui Zhang",
      "Yuhong Guo",
      "Junping Zhao",
      "Ke Zhang",
      "Zhenxuan Pan"
    ],
    "abstract": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "I.2.11; C.4"
    ],
    "primary_category": "cs.DC",
    "comment": "11 pages, 7 figures, 1 table",
    "pdf_url": "http://arxiv.org/pdf/2410.00428v3",
    "published_date": "2024-10-01 06:23:17 UTC",
    "updated_date": "2024-10-09 11:40:31 UTC"
  },
  {
    "arxiv_id": "2410.00425v1",
    "title": "ManiSkill3: GPU Parallelized Robotics Simulation and Rendering for Generalizable Embodied AI",
    "authors": [
      "Stone Tao",
      "Fanbo Xiang",
      "Arth Shukla",
      "Yuzhe Qin",
      "Xander Hinrichsen",
      "Xiaodi Yuan",
      "Chen Bao",
      "Xinsong Lin",
      "Yulin Liu",
      "Tse-kai Chan",
      "Yuan Gao",
      "Xuanlin Li",
      "Tongzhou Mu",
      "Nan Xiao",
      "Arnav Gurha",
      "Zhiao Huang",
      "Roberto Calandra",
      "Rui Chen",
      "Shan Luo",
      "Hao Su"
    ],
    "abstract": "Simulation has enabled unprecedented compute-scalable approaches to robot\nlearning. However, many existing simulation frameworks typically support a\nnarrow range of scenes/tasks and lack features critical for scaling\ngeneralizable robotics and sim2real. We introduce and open source ManiSkill3,\nthe fastest state-visual GPU parallelized robotics simulator with contact-rich\nphysics targeting generalizable manipulation. ManiSkill3 supports GPU\nparallelization of many aspects including simulation+rendering, heterogeneous\nsimulation, pointclouds/voxels visual input, and more. Simulation with\nrendering on ManiSkill3 can run 10-1000x faster with 2-3x less GPU memory usage\nthan other platforms, achieving up to 30,000+ FPS in benchmarked environments\ndue to minimal python/pytorch overhead in the system, simulation on the GPU,\nand the use of the SAPIEN parallel rendering system. Tasks that used to take\nhours to train can now take minutes. We further provide the most comprehensive\nrange of GPU parallelized environments/tasks spanning 12 distinct domains\nincluding but not limited to mobile manipulation for tasks such as drawing,\nhumanoids, and dextrous manipulation in realistic scenes designed by artists or\nreal-world digital twins. In addition, millions of demonstration frames are\nprovided from motion planning, RL, and teleoperation. ManiSkill3 also provides\na comprehensive set of baselines that span popular RL and\nlearning-from-demonstrations algorithms.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website: http://maniskill.ai/",
    "pdf_url": "http://arxiv.org/pdf/2410.00425v1",
    "published_date": "2024-10-01 06:10:39 UTC",
    "updated_date": "2024-10-01 06:10:39 UTC"
  },
  {
    "arxiv_id": "2410.00418v3",
    "title": "Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration",
    "authors": [
      "Guy Ohayon",
      "Tomer Michaeli",
      "Michael Elad"
    ],
    "abstract": "Photo-realistic image restoration algorithms are typically evaluated by\ndistortion measures (e.g., PSNR, SSIM) and by perceptual quality measures\n(e.g., FID, NIQE), where the desire is to attain the lowest possible distortion\nwithout compromising on perceptual quality. To achieve this goal, current\nmethods commonly attempt to sample from the posterior distribution, or to\noptimize a weighted sum of a distortion loss (e.g., MSE) and a perceptual\nquality loss (e.g., GAN). Unlike previous works, this paper is concerned\nspecifically with the optimal estimator that minimizes the MSE under a\nconstraint of perfect perceptual index, namely where the distribution of the\nreconstructed images is equal to that of the ground-truth ones. A recent\ntheoretical result shows that such an estimator can be constructed by optimally\ntransporting the posterior mean prediction (MMSE estimate) to the distribution\nof the ground-truth images. Inspired by this result, we introduce\nPosterior-Mean Rectified Flow (PMRF), a simple yet highly effective algorithm\nthat approximates this optimal estimator. In particular, PMRF first predicts\nthe posterior mean, and then transports the result to a high-quality image\nusing a rectified flow model that approximates the desired optimal transport\nmap. We investigate the theoretical utility of PMRF and demonstrate that it\nconsistently outperforms previous methods on a variety of image restoration\ntasks.",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "eess.SP"
    ],
    "primary_category": "eess.IV",
    "comment": "Accepted to ICLR 2025. Code and demo are available at\n  https://pmrf-ml.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2410.00418v3",
    "published_date": "2024-10-01 05:54:07 UTC",
    "updated_date": "2025-02-04 07:00:12 UTC"
  },
  {
    "arxiv_id": "2410.02826v1",
    "title": "LinkThief: Combining Generalized Structure Knowledge with Node Similarity for Link Stealing Attack against GNN",
    "authors": [
      "Yuxing Zhang",
      "Siyuan Meng",
      "Chunchun Chen",
      "Mengyao Peng",
      "Hongyan Gu",
      "Xinli Huang"
    ],
    "abstract": "Graph neural networks(GNNs) have a wide range of applications in\nmultimedia.Recent studies have shown that Graph neural networks(GNNs) are\nvulnerable to link stealing attacks,which infers the existence of edges in the\ntarget GNN's training graph.Existing attacks are usually based on the\nassumption that links exist between two nodes that share similar\nposteriors;however,they fail to focus on links that do not hold under this\nassumption.To this end,we propose LinkThief,an improved link stealing attack\nthat combines generalized structure knowledge with node similarity,in a\nscenario where the attackers' background knowledge contains partially leaked\ntarget graph and shadow graph.Specifically,to equip the attack model with\ninsights into the link structure spanning both the shadow graph and the target\ngraph,we introduce the idea of creating a Shadow-Target Bridge Graph and\nextracting edge subgraph structure features from it.Through theoretical\nanalysis from the perspective of privacy theft,we first explore how to\nimplement the aforementioned ideas.Building upon the findings,we design the\nBridge Graph Generator to construct the Shadow-Target Bridge Graph.Then,the\nsubgraph around the link is sampled by the Edge Subgraph Preparation\nModule.Finally,the Edge Structure Feature Extractor is designed to obtain\ngeneralized structure knowledge,which is combined with node similarity to form\nthe features provided to the attack model.Extensive experiments validate the\ncorrectness of theoretical analysis and demonstrate that LinkThief still\neffectively steals links without extra assumptions.",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.02826v1",
    "published_date": "2024-10-01 05:34:03 UTC",
    "updated_date": "2024-10-01 05:34:03 UTC"
  },
  {
    "arxiv_id": "2410.00403v1",
    "title": "TikGuard: A Deep Learning Transformer-Based Solution for Detecting Unsuitable TikTok Content for Kids",
    "authors": [
      "Mazen Balat",
      "Mahmoud Essam Gabr",
      "Hend Bakr",
      "Ahmed B. Zaky"
    ],
    "abstract": "The rise of short-form videos on platforms like TikTok has brought new\nchallenges in safeguarding young viewers from inappropriate content.\nTraditional moderation methods often fall short in handling the vast and\nrapidly changing landscape of user-generated videos, increasing the risk of\nchildren encountering harmful material. This paper introduces TikGuard, a\ntransformer-based deep learning approach aimed at detecting and flagging\ncontent unsuitable for children on TikTok. By using a specially curated\ndataset, TikHarm, and leveraging advanced video classification techniques,\nTikGuard achieves an accuracy of 86.7%, showing a notable improvement over\nexisting methods in similar contexts. While direct comparisons are limited by\nthe uniqueness of the TikHarm dataset, TikGuard's performance highlights its\npotential in enhancing content moderation, contributing to a safer online\nexperience for minors. This study underscores the effectiveness of transformer\nmodels in video classification and sets a foundation for future research in\nthis area.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "NILES2024",
    "pdf_url": "http://arxiv.org/pdf/2410.00403v1",
    "published_date": "2024-10-01 05:00:05 UTC",
    "updated_date": "2024-10-01 05:00:05 UTC"
  },
  {
    "arxiv_id": "2410.00393v1",
    "title": "Revisiting Essential and Nonessential Settings of Evidential Deep Learning",
    "authors": [
      "Mengyuan Chen",
      "Junyu Gao",
      "Changsheng Xu"
    ],
    "abstract": "Evidential Deep Learning (EDL) is an emerging method for uncertainty\nestimation that provides reliable predictive uncertainty in a single forward\npass, attracting significant attention. Grounded in subjective logic, EDL\nderives Dirichlet concentration parameters from neural networks to construct a\nDirichlet probability density function (PDF), modeling the distribution of\nclass probabilities. Despite its success, EDL incorporates several nonessential\nsettings: In model construction, (1) a commonly ignored prior weight parameter\nis fixed to the number of classes, while its value actually impacts the balance\nbetween the proportion of evidence and its magnitude in deriving predictive\nscores. In model optimization, (2) the empirical risk features a\nvariance-minimizing optimization term that biases the PDF towards a Dirac delta\nfunction, potentially exacerbating overconfidence. (3) Additionally, the\nstructural risk typically includes a KL-divergence-minimizing regularization,\nwhose optimization direction extends beyond the intended purpose and\ncontradicts common sense, diminishing the information carried by the evidence\nmagnitude. Therefore, we propose Re-EDL, a simplified yet more effective\nvariant of EDL, by relaxing the nonessential settings and retaining the\nessential one, namely, the adoption of projected probability from subjective\nlogic. Specifically, Re-EDL treats the prior weight as an adjustable\nhyperparameter rather than a fixed scalar, and directly optimizes the\nexpectation of the Dirichlet PDF provided by deprecating both the\nvariance-minimizing optimization term and the divergence regularization term.\nExtensive experiments and state-of-the-art performance validate the\neffectiveness of our method. The source code is available at\nhttps://github.com/MengyuanChen21/Re-EDL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "22 pages, under review",
    "pdf_url": "http://arxiv.org/pdf/2410.00393v1",
    "published_date": "2024-10-01 04:27:07 UTC",
    "updated_date": "2024-10-01 04:27:07 UTC"
  },
  {
    "arxiv_id": "2410.00387v1",
    "title": "Boosting the Capabilities of Compact Models in Low-Data Contexts with Large Language Models and Retrieval-Augmented Generation",
    "authors": [
      "Bhargav Shandilya",
      "Alexis Palmer"
    ],
    "abstract": "The data and compute requirements of current language modeling technology\npose challenges for the processing and analysis of low-resource languages.\nDeclarative linguistic knowledge has the potential to partially bridge this\ndata scarcity gap by providing models with useful inductive bias in the form of\nlanguage-specific rules. In this paper, we propose a retrieval augmented\ngeneration (RAG) framework backed by a large language model (LLM) to correct\nthe output of a smaller model for the linguistic task of morphological\nglossing. We leverage linguistic information to make up for the lack of data\nand trainable parameters, while allowing for inputs from written descriptive\ngrammars interpreted and distilled through an LLM.\n  The results demonstrate that significant leaps in performance and efficiency\nare possible with the right combination of: a) linguistic inputs in the form of\ngrammars, b) the interpretive power of LLMs, and c) the trainability of smaller\ntoken classification networks. We show that a compact, RAG-supported model is\nhighly effective in data-scarce settings, achieving a new state-of-the-art for\nthis task and our target languages. Our work also offers documentary linguists\na more reliable and more usable tool for morphological glossing by providing\nwell-reasoned explanations and confidence scores for each output.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "13 pages, 1 figure, 5 tables, submitted to COLING 2025",
    "pdf_url": "http://arxiv.org/pdf/2410.00387v1",
    "published_date": "2024-10-01 04:20:14 UTC",
    "updated_date": "2024-10-01 04:20:14 UTC"
  },
  {
    "arxiv_id": "2410.00385v2",
    "title": "STGformer: Efficient Spatiotemporal Graph Transformer for Traffic Forecasting",
    "authors": [
      "Hongjun Wang",
      "Jiyuan Chen",
      "Tong Pan",
      "Zheng Dong",
      "Lingyu Zhang",
      "Renhe Jiang",
      "Xuan Song"
    ],
    "abstract": "Traffic forecasting is a cornerstone of smart city management, enabling\nefficient resource allocation and transportation planning. Deep learning, with\nits ability to capture complex nonlinear patterns in spatiotemporal (ST) data,\nhas emerged as a powerful tool for traffic forecasting. While graph neural\nnetworks (GCNs) and transformer-based models have shown promise, their\ncomputational demands often hinder their application to real-world road\nnetworks, particularly those with large-scale spatiotemporal interactions. To\naddress these challenges, we propose a novel spatiotemporal graph transformer\n(STGformer) architecture. STGformer effectively balances the strengths of GCNs\nand Transformers, enabling efficient modeling of both global and local traffic\npatterns while maintaining a manageable computational footprint. Unlike\ntraditional approaches that require multiple attention layers, STG attention\nblock captures high-order spatiotemporal interactions in a single layer,\nsignificantly reducing computational cost. In particular, STGformer achieves a\n100x speedup and a 99.8\\% reduction in GPU memory usage compared to STAEformer\nduring batch inference on a California road graph with 8,600 sensors. We\nevaluate STGformer on the LargeST benchmark and demonstrate its superiority\nover state-of-the-art Transformer-based methods such as PDFormer and\nSTAEformer, which underline STGformer's potential to revolutionize traffic\nforecasting by overcoming the computational and memory limitations of existing\napproaches, making it a promising foundation for future spatiotemporal modeling\ntasks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00385v2",
    "published_date": "2024-10-01 04:15:48 UTC",
    "updated_date": "2024-10-15 05:44:29 UTC"
  },
  {
    "arxiv_id": "2410.00381v2",
    "title": "Downscaling Extreme Precipitation with Wasserstein Regularized Diffusion",
    "authors": [
      "Yuhao Liu",
      "James Doss-Gollin",
      "Qiushi Dai",
      "Guha Balakrishnan",
      "Ashok Veeraraghavan"
    ],
    "abstract": "Understanding the risks posed by extreme rainfall events necessitates both\nhigh-resolution products (to assess localized hazards) and extensive historical\nrecords (to capture rare occurrences). Radar and mesonet networks provide\nkilometer-scale precipitation fields, but with limited historical records and\ngeographical coverage. Conversely, global gauge and blended products span\ndecades, yet their coarse 30-50 km grids obscure local extremes. This work\nintroduces Wasserstein Regularized Diffusion (WassDiff), a generative\ndownscaling framework that integrates diffusion modeling with a\ndistribution-matching (Wasserstein) regularizer, suppressing bias throughout\nthe entire generative denoising process. Conditioned on 55 km CPC gauge-based\nprecipitation and the 31 km ERA5 reanalysis, WassDiff generates 1 km\nprecipitation estimates that remain well-calibrated to targets across the full\nintensity range, including the extremes. Comprehensive evaluations demonstrate\nthat WassDiff outperforms existing state-of-the-art downscaling methods,\ndelivering lower reconstruction error and reduced bias. Case studies further\ndemonstrate its ability to reproduce realistic fine-scale structures and\naccurate peak intensities from extreme weather phenomena, such as tropical\nstorms and cold fronts. By unlocking decades of high-resolution rainfall\ninformation from globally available coarse records, WassDiff offers a practical\npathway toward more accurate flood-risk assessments and climate-adaptation\nplanning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 10 figures, 4 tables",
    "pdf_url": "http://arxiv.org/pdf/2410.00381v2",
    "published_date": "2024-10-01 04:12:40 UTC",
    "updated_date": "2025-04-29 20:41:38 UTC"
  },
  {
    "arxiv_id": "2410.00379v1",
    "title": "CXPMRG-Bench: Pre-training and Benchmarking for X-ray Medical Report Generation on CheXpert Plus Dataset",
    "authors": [
      "Xiao Wang",
      "Fuling Wang",
      "Yuehang Li",
      "Qingchuan Ma",
      "Shiao Wang",
      "Bo Jiang",
      "Chuanfu Li",
      "Jin Tang"
    ],
    "abstract": "X-ray image-based medical report generation (MRG) is a pivotal area in\nartificial intelligence which can significantly reduce diagnostic burdens and\npatient wait times. Despite significant progress, we believe that the task has\nreached a bottleneck due to the limited benchmark datasets and the existing\nlarge models' insufficient capability enhancements in this specialized domain.\nSpecifically, the recently released CheXpert Plus dataset lacks comparative\nevaluation algorithms and their results, providing only the dataset itself.\nThis situation makes the training, evaluation, and comparison of subsequent\nalgorithms challenging. Thus, we conduct a comprehensive benchmarking of\nexisting mainstream X-ray report generation models and large language models\n(LLMs), on the CheXpert Plus dataset. We believe that the proposed benchmark\ncan provide a solid comparative basis for subsequent algorithms and serve as a\nguide for researchers to quickly grasp the state-of-the-art models in this\nfield. More importantly, we propose a large model for the X-ray image report\ngeneration using a multi-stage pre-training strategy, including self-supervised\nautoregressive generation and Xray-report contrastive learning, and supervised\nfine-tuning. Extensive experimental results indicate that the autoregressive\npre-training based on Mamba effectively encodes X-ray images, and the\nimage-text contrastive pre-training further aligns the feature spaces,\nachieving better experimental results. Source code can be found on\n\\url{https://github.com/Event-AHU/Medical_Image_Analysis}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "In Peer Review",
    "pdf_url": "http://arxiv.org/pdf/2410.00379v1",
    "published_date": "2024-10-01 04:07:01 UTC",
    "updated_date": "2024-10-01 04:07:01 UTC"
  },
  {
    "arxiv_id": "2410.12812v1",
    "title": "Optimizing and Evaluating Enterprise Retrieval-Augmented Generation (RAG): A Content Design Perspective",
    "authors": [
      "Sarah Packowski",
      "Inge Halilovic",
      "Jenifer Schlotfeldt",
      "Trish Smith"
    ],
    "abstract": "Retrieval-augmented generation (RAG) is a popular technique for using large\nlanguage models (LLMs) to build customer-support, question-answering solutions.\nIn this paper, we share our team's practical experience building and\nmaintaining enterprise-scale RAG solutions that answer users' questions about\nour software based on product documentation. Our experience has not always\nmatched the most common patterns in the RAG literature. This paper focuses on\nsolution strategies that are modular and model-agnostic. For example, our\nexperience over the past few years - using different search methods and LLMs,\nand many knowledge base collections - has been that simple changes to the way\nwe create knowledge base content can have a huge impact on our RAG solutions'\nsuccess. In this paper, we also discuss how we monitor and evaluate results.\nCommon RAG benchmark evaluation techniques have not been useful for evaluating\nresponses to novel user questions, so we have found a flexible, \"human in the\nlead\" approach is required.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "6 pages, 4 figures, to be published in ICAAI 2024 conference\n  proceedings",
    "pdf_url": "http://arxiv.org/pdf/2410.12812v1",
    "published_date": "2024-10-01 03:54:45 UTC",
    "updated_date": "2024-10-01 03:54:45 UTC"
  },
  {
    "arxiv_id": "2410.00373v1",
    "title": "Robust Traffic Forecasting against Spatial Shift over Years",
    "authors": [
      "Hongjun Wang",
      "Jiyuan Chen",
      "Tong Pan",
      "Zheng Dong",
      "Lingyu Zhang",
      "Renhe Jiang",
      "Xuan Song"
    ],
    "abstract": "Recent advancements in Spatiotemporal Graph Neural Networks (ST-GNNs) and\nTransformers have demonstrated promising potential for traffic forecasting by\neffectively capturing both temporal and spatial correlations. The\ngeneralization ability of spatiotemporal models has received considerable\nattention in recent scholarly discourse. However, no substantive datasets\nspecifically addressing traffic out-of-distribution (OOD) scenarios have been\nproposed. Existing ST-OOD methods are either constrained to testing on extant\ndata or necessitate manual modifications to the dataset. Consequently, the\ngeneralization capacity of current spatiotemporal models in OOD scenarios\nremains largely underexplored. In this paper, we investigate state-of-the-art\nmodels using newly proposed traffic OOD benchmarks and, surprisingly, find that\nthese models experience a significant decline in performance. Through\nmeticulous analysis, we attribute this decline to the models' inability to\nadapt to previously unobserved spatial relationships. To address this\nchallenge, we propose a novel Mixture of Experts (MoE) framework, which learns\na set of graph generators (i.e., graphons) during training and adaptively\ncombines them to generate new graphs based on novel environmental conditions to\nhandle spatial distribution shifts during testing. We further extend this\nconcept to the Transformer architecture, achieving substantial improvements.\nOur method is both parsimonious and efficacious, and can be seamlessly\nintegrated into any spatiotemporal model, outperforming current\nstate-of-the-art approaches in addressing spatial dynamics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00373v1",
    "published_date": "2024-10-01 03:49:29 UTC",
    "updated_date": "2024-10-01 03:49:29 UTC"
  },
  {
    "arxiv_id": "2410.00366v1",
    "title": "Easydiagnos: a framework for accurate feature selection for automatic diagnosis in smart healthcare",
    "authors": [
      "Prasenjit Maji",
      "Amit Kumar Mondal",
      "Hemanta Kumar Mondal",
      "Saraju P. Mohanty"
    ],
    "abstract": "The rapid advancements in artificial intelligence (AI) have revolutionized\nsmart healthcare, driving innovations in wearable technologies, continuous\nmonitoring devices, and intelligent diagnostic systems. However, security,\nexplainability, robustness, and performance optimization challenges remain\ncritical barriers to widespread adoption in clinical environments. This\nresearch presents an innovative algorithmic method using the Adaptive Feature\nEvaluator (AFE) algorithm to improve feature selection in healthcare datasets\nand overcome problems. AFE integrating Genetic Algorithms (GA), Explainable\nArtificial Intelligence (XAI), and Permutation Combination Techniques (PCT),\nthe algorithm optimizes Clinical Decision Support Systems (CDSS), thereby\nenhancing predictive accuracy and interpretability. The proposed method is\nvalidated across three diverse healthcare datasets using six distinct machine\nlearning algorithms, demonstrating its robustness and superiority over\nconventional feature selection techniques. The results underscore the\ntransformative potential of AFE in smart healthcare, enabling personalized and\ntransparent patient care. Notably, the AFE algorithm, when combined with a\nMulti-layer Perceptron (MLP), achieved an accuracy of up to 98.5%, highlighting\nits capability to improve clinical decision-making processes in real-world\nhealthcare applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00366v1",
    "published_date": "2024-10-01 03:28:56 UTC",
    "updated_date": "2024-10-01 03:28:56 UTC"
  },
  {
    "arxiv_id": "2410.00362v1",
    "title": "FedPT: Federated Proxy-Tuning of Large Language Models on Resource-Constrained Edge Devices",
    "authors": [
      "Zhidong Gao",
      "Yu Zhang",
      "Zhenxiao Zhang",
      "Yanmin Gong",
      "Yuanxiong Guo"
    ],
    "abstract": "Despite demonstrating superior performance across a variety of linguistic\ntasks, pre-trained large language models (LMs) often require fine-tuning on\nspecific datasets to effectively address different downstream tasks. However,\nfine-tuning these LMs for downstream tasks necessitates collecting data from\nindividuals, which raises significant privacy concerns. Federated learning (FL)\nhas emerged as the de facto solution, enabling collaborative model training\nwithout sharing raw data. While promising, federated fine-tuning of large LMs\nfaces significant challenges, including restricted access to model parameters\nand high computation, communication, and memory overhead. To address these\nchallenges, this paper introduces \\textbf{Fed}erated\n\\textbf{P}roxy-\\textbf{T}uning (FedPT), a novel framework for federated\nfine-tuning of black-box large LMs, requiring access only to their predictions\nover the output vocabulary instead of their parameters. Specifically, devices\nin FedPT first collaboratively tune a smaller LM, and then the server combines\nthe knowledge learned by the tuned small LM with the knowledge learned by the\nlarger pre-trained LM to construct a large proxy-tuned LM that can reach the\nperformance of directly tuned large LMs. The experimental results demonstrate\nthat FedPT can significantly reduce computation, communication, and memory\noverhead while maintaining competitive performance compared to directly\nfederated fine-tuning of large LMs. FedPT offers a promising solution for\nefficient, privacy-preserving fine-tuning of large LMs on resource-constrained\ndevices, broadening the accessibility and applicability of state-of-the-art\nlarge LMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "29 pages, 19 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.00362v1",
    "published_date": "2024-10-01 03:20:39 UTC",
    "updated_date": "2024-10-01 03:20:39 UTC"
  },
  {
    "arxiv_id": "2410.00359v1",
    "title": "Self-controller: Controlling LLMs with Multi-round Step-by-step Self-awareness",
    "authors": [
      "Xiao Peng",
      "Xufan Geng"
    ],
    "abstract": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "10 pages, 6 figures",
    "pdf_url": "http://arxiv.org/pdf/2410.00359v1",
    "published_date": "2024-10-01 03:14:12 UTC",
    "updated_date": "2024-10-01 03:14:12 UTC"
  },
  {
    "arxiv_id": "2410.00340v3",
    "title": "Sparse Attention Decomposition Applied to Circuit Tracing",
    "authors": [
      "Gabriel Franco",
      "Mark Crovella"
    ],
    "abstract": "Many papers have shown that attention heads work in conjunction with each\nother to perform complex tasks. It's frequently assumed that communication\nbetween attention heads is via the addition of specific features to token\nresiduals. In this work we seek to isolate and identify the features used to\neffect communication and coordination among attention heads in GPT-2 small. Our\nkey leverage on the problem is to show that these features are very often\nsparsely coded in the singular vectors of attention head matrices. We\ncharacterize the dimensionality and occurrence of these signals across the\nattention heads in GPT-2 small when used for the Indirect Object Identification\n(IOI) task. The sparse encoding of signals, as provided by attention head\nsingular vectors, allows for efficient separation of signals from the residual\nbackground and straightforward identification of communication paths between\nattention heads. We explore the effectiveness of this approach by tracing\nportions of the circuits used in the IOI task. Our traces reveal considerable\ndetail not present in previous studies, shedding light on the nature of\nredundant paths present in GPT-2. And our traces go beyond previous work by\nidentifying features used to communicate between attention heads when\nperforming IOI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00340v3",
    "published_date": "2024-10-01 02:34:08 UTC",
    "updated_date": "2024-10-28 21:54:44 UTC"
  },
  {
    "arxiv_id": "2410.03739v2",
    "title": "Grammar Induction from Visual, Speech and Text",
    "authors": [
      "Yu Zhao",
      "Hao Fei",
      "Shengqiong Wu",
      "Meishan Zhang",
      "Min Zhang",
      "Tat-seng Chua"
    ],
    "abstract": "Grammar Induction could benefit from rich heterogeneous signals, such as\ntext, vision, and acoustics. In the process, features from distinct modalities\nessentially serve complementary roles to each other. With such intuition, this\nwork introduces a novel \\emph{unsupervised visual-audio-text grammar induction}\ntask (named \\textbf{VAT-GI}), to induce the constituent grammar trees from\nparallel images, text, and speech inputs. Inspired by the fact that language\ngrammar natively exists beyond the texts, we argue that the text has not to be\nthe predominant modality in grammar induction. Thus we further introduce a\n\\emph{textless} setting of VAT-GI, wherein the task solely relies on visual and\nauditory inputs. To approach the task, we propose a visual-audio-text\ninside-outside recursive autoencoder (\\textbf{VaTiora}) framework, which\nleverages rich modal-specific and complementary features for effective grammar\nparsing. Besides, a more challenging benchmark data is constructed to assess\nthe generalization ability of VAT-GI system. Experiments on two benchmark\ndatasets demonstrate that our proposed VaTiora system is more effective in\nincorporating the various multimodal signals, and also presents new\nstate-of-the-art performance of VAT-GI.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.03739v2",
    "published_date": "2024-10-01 02:24:18 UTC",
    "updated_date": "2025-02-20 07:46:29 UTC"
  },
  {
    "arxiv_id": "2410.00334v1",
    "title": "Preserving Generalization of Language models in Few-shot Continual Relation Extraction",
    "authors": [
      "Quyen Tran",
      "Nguyen Xuan Thanh",
      "Nguyen Hoang Anh",
      "Nam Le Hai",
      "Trung Le",
      "Linh Van Ngo",
      "Thien Huu Nguyen"
    ],
    "abstract": "Few-shot Continual Relations Extraction (FCRE) is an emerging and dynamic\narea of study where models can sequentially integrate knowledge from new\nrelations with limited labeled data while circumventing catastrophic forgetting\nand preserving prior knowledge from pre-trained backbones. In this work, we\nintroduce a novel method that leverages often-discarded language model heads.\nBy employing these components via a mutual information maximization strategy,\nour approach helps maintain prior knowledge from the pre-trained backbone and\nstrategically aligns the primary classification head, thereby enhancing model\nperformance. Furthermore, we explore the potential of Large Language Models\n(LLMs), renowned for their wealth of knowledge, in addressing FCRE challenges.\nOur comprehensive experimental results underscore the efficacy of the proposed\nmethod and offer valuable insights for future work.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Accepted to EMNLP 2024",
    "pdf_url": "http://arxiv.org/pdf/2410.00334v1",
    "published_date": "2024-10-01 02:22:34 UTC",
    "updated_date": "2024-10-01 02:22:34 UTC"
  },
  {
    "arxiv_id": "2410.00332v5",
    "title": "Vision Language Models Know Law of Conservation without Understanding More-or-Less",
    "authors": [
      "Dezhi Luo",
      "Haiyun Lyu",
      "Qingying Gao",
      "Haoran Sun",
      "Yijiang Li",
      "Hokin Deng"
    ],
    "abstract": "Understanding law of conservation is a critical milestone in human cognitive\ndevelopment considered to be supported by the apprehension of quantitative\nconcepts and the reversibility of operations. To assess whether this critical\ncomponent of human intelligence has emerged in Vision Language Models, we have\ncurated the ConserveBench, a battery of 365 cognitive experiments across four\ndimensions of physical quantities: volume, solid quantity, length, and number.\nThe former two involve transformational tasks which require reversibility\nunderstanding. The latter two involve non-transformational tasks which assess\nquantity understanding. Surprisingly, we find that while Vision Language Models\nare generally good at transformational tasks, they tend to fail at\nnon-transformational tasks. There is a dissociation between understanding the\nreversibility of operations and understanding the concept of quantity, which\nboth are believed to be the cornerstones of understanding law of conservation\nin humans. $\\href{https://growing-ai-like-a-child.github.io/}{Website}$",
    "categories": [
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at the ICLR 2025 Workshop on Bidirectional Human-AI\n  Alignment (BiAlign)",
    "pdf_url": "http://arxiv.org/pdf/2410.00332v5",
    "published_date": "2024-10-01 02:15:49 UTC",
    "updated_date": "2025-04-13 04:31:37 UTC"
  },
  {
    "arxiv_id": "2410.00327v1",
    "title": "EnzymeFlow: Generating Reaction-specific Enzyme Catalytic Pockets through Flow Matching and Co-Evolutionary Dynamics",
    "authors": [
      "Chenqing Hua",
      "Yong Liu",
      "Dinghuai Zhang",
      "Odin Zhang",
      "Sitao Luan",
      "Kevin K. Yang",
      "Guy Wolf",
      "Doina Precup",
      "Shuangjia Zheng"
    ],
    "abstract": "Enzyme design is a critical area in biotechnology, with applications ranging\nfrom drug development to synthetic biology. Traditional methods for enzyme\nfunction prediction or protein binding pocket design often fall short in\ncapturing the dynamic and complex nature of enzyme-substrate interactions,\nparticularly in catalytic processes. To address the challenges, we introduce\nEnzymeFlow, a generative model that employs flow matching with hierarchical\npre-training and enzyme-reaction co-evolution to generate catalytic pockets for\nspecific substrates and catalytic reactions. Additionally, we introduce a\nlarge-scale, curated, and validated dataset of enzyme-reaction pairs,\nspecifically designed for the catalytic pocket generation task, comprising a\ntotal of $328,192$ pairs. By incorporating evolutionary dynamics and\nreaction-specific adaptations, EnzymeFlow becomes a powerful model for\ndesigning enzyme pockets, which is capable of catalyzing a wide range of\nbiochemical reactions. Experiments on the new dataset demonstrate the model's\neffectiveness in designing high-quality, functional enzyme catalytic pockets,\npaving the way for advancements in enzyme engineering and synthetic biology. We\nprovide EnzymeFlow code at https://github.com/WillHua127/EnzymeFlow with\nnotebook demonstration at\nhttps://github.com/WillHua127/EnzymeFlow/blob/main/enzymeflow_demo.ipynb.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2410.00327v1",
    "published_date": "2024-10-01 02:04:01 UTC",
    "updated_date": "2024-10-01 02:04:01 UTC"
  },
  {
    "arxiv_id": "2410.00324v5",
    "title": "Vision Language Models See What You Want but not What You See",
    "authors": [
      "Qingying Gao",
      "Yijiang Li",
      "Haiyun Lyu",
      "Haoran Sun",
      "Dezhi Luo",
      "Hokin Deng"
    ],
    "abstract": "Knowing others' intentions and taking others' perspectives are two core\ncomponents of human intelligence that are considered to be instantiations of\ntheory-of-mind. Infiltrating machines with these abilities is an important step\ntowards building human-level artificial intelligence. Here, to investigate\nintentionality understanding and level-2 perspective-taking in Vision Language\nModels (VLMs), we constructed the IntentBench and PerspectBench, which together\ncontains over 300 cognitive experiments grounded in real-world scenarios and\nclassic cognitive tasks. We found VLMs achieving high performance on\nintentionality understanding but low performance on level-2 perspective-taking.\nThis suggests a potential dissociation between simulation-based and\ntheory-based theory-of-mind abilities in VLMs, highlighting the concern that\nthey are not capable of using model-based reasoning to infer others' mental\nstates. See $\\href{https://growing-ai-like-a-child.github.io/}{Website}$",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at the ICLR 2025 Workshop on Bidirectional Human-AI\n  Alignment (BiAlign)",
    "pdf_url": "http://arxiv.org/pdf/2410.00324v5",
    "published_date": "2024-10-01 01:52:01 UTC",
    "updated_date": "2025-04-13 05:41:27 UTC"
  },
  {
    "arxiv_id": "2410.00318v3",
    "title": "Probing Mechanical Reasoning in Large Vision Language Models",
    "authors": [
      "Haoran Sun",
      "Qingying Gao",
      "Haiyun Lyu",
      "Dezhi Luo",
      "Yijiang Li",
      "Hokin Deng"
    ],
    "abstract": "Mechanical reasoning is a hallmark of human intelligence, defined by its\nubiquitous yet irreplaceable role in human activities ranging from routine\ntasks to civil engineering. Embedding machines with mechanical reasoning is\ntherefore an important step towards building human-level artificial\nintelligence. Here, we leveraged 155 cognitive experiments to test the\nunderstanding of system stability, gears and pulley systems, leverage\nprinciple, inertia and motion, and fluid mechanics in 26 Vision Language Models\n(VLMs). Results indicate that VLMs consistently perform worse than humans on\nall domains, while demonstrate significant difficulty in reasoning about gear\nsystems and fluid mechanics. Notably, their performance on these tasks do not\nimprove as number of parameters increase, suggesting that current\nattention-based architecture may fail to grasp certain underlying mechanisms\nrequired for mechanical reasoning, particularly those pertaining to mental\nsimulations.",
    "categories": [
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.AI",
    "comment": "Published at the ICLR 2025 Workshop on Bidirectional Human-AI\n  Alignment (BiAlign)",
    "pdf_url": "http://arxiv.org/pdf/2410.00318v3",
    "published_date": "2024-10-01 01:33:10 UTC",
    "updated_date": "2025-04-13 05:53:58 UTC"
  },
  {
    "arxiv_id": "2410.00316v1",
    "title": "EmoKnob: Enhance Voice Cloning with Fine-Grained Emotion Control",
    "authors": [
      "Haozhe Chen",
      "Run Chen",
      "Julia Hirschberg"
    ],
    "abstract": "While recent advances in Text-to-Speech (TTS) technology produce natural and\nexpressive speech, they lack the option for users to select emotion and control\nintensity. We propose EmoKnob, a framework that allows fine-grained emotion\ncontrol in speech synthesis with few-shot demonstrative samples of arbitrary\nemotion. Our framework leverages the expressive speaker representation space\nmade possible by recent advances in foundation voice cloning models. Based on\nthe few-shot capability of our emotion control framework, we propose two\nmethods to apply emotion control on emotions described by open-ended text,\nenabling an intuitive interface for controlling a diverse array of nuanced\nemotions. To facilitate a more systematic emotional speech synthesis field, we\nintroduce a set of evaluation metrics designed to rigorously assess the\nfaithfulness and recognizability of emotion control frameworks. Through\nobjective and subjective evaluations, we show that our emotion control\nframework effectively embeds emotions into speech and surpasses emotion\nexpressiveness of commercial TTS services.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "comment": "EMNLP 2024 Main",
    "pdf_url": "http://arxiv.org/pdf/2410.00316v1",
    "published_date": "2024-10-01 01:29:54 UTC",
    "updated_date": "2024-10-01 01:29:54 UTC"
  },
  {
    "arxiv_id": "2410.00312v1",
    "title": "Contrastive Representation Learning for Predicting Solar Flares from Extremely Imbalanced Multivariate Time Series Data",
    "authors": [
      "Onur Vural",
      "Shah Muhammad Hamdi",
      "Soukaina Filali Boubrahimi"
    ],
    "abstract": "Major solar flares are abrupt surges in the Sun's magnetic flux, presenting\nsignificant risks to technological infrastructure. In view of this, effectively\npredicting major flares from solar active region magnetic field data through\nmachine learning methods becomes highly important in space weather research.\nMagnetic field data can be represented in multivariate time series modality\nwhere the data displays an extreme class imbalance due to the rarity of major\nflare events. In time series classification-based flare prediction, the use of\ncontrastive representation learning methods has been relatively limited. In\nthis paper, we introduce CONTREX, a novel contrastive representation learning\napproach for multivariate time series data, addressing challenges of temporal\ndependencies and extreme class imbalance. Our method involves extracting\ndynamic features from the multivariate time series instances, deriving two\nextremes from positive and negative class feature vectors that provide maximum\nseparation capability, and training a sequence representation embedding module\nwith the original multivariate time series data guided by our novel contrastive\nreconstruction loss to generate embeddings aligned with the extreme points.\nThese embeddings capture essential time series characteristics and enhance\ndiscriminative power. Our approach shows promising solar flare prediction\nresults on the Space Weather Analytics for Solar Flares (SWAN-SF) multivariate\ntime series benchmark dataset against baseline methods.",
    "categories": [
      "astro-ph.SR",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "astro-ph.SR",
    "comment": "This work has been accepted at ICMLA 2024 on September 7, 2024, as a\n  short paper for poster presentation",
    "pdf_url": "http://arxiv.org/pdf/2410.00312v1",
    "published_date": "2024-10-01 01:20:47 UTC",
    "updated_date": "2024-10-01 01:20:47 UTC"
  },
  {
    "arxiv_id": "2410.00309v1",
    "title": "Ask, Pose, Unite: Scaling Data Acquisition for Close Interactions with Vision Language Models",
    "authors": [
      "Laura Bravo-Sánchez",
      "Jaewoo Heo",
      "Zhenzhen Weng",
      "Kuan-Chieh Wang",
      "Serena Yeung-Levy"
    ],
    "abstract": "Social dynamics in close human interactions pose significant challenges for\nHuman Mesh Estimation (HME), particularly due to the complexity of physical\ncontacts and the scarcity of training data. Addressing these challenges, we\nintroduce a novel data generation method that utilizes Large Vision Language\nModels (LVLMs) to annotate contact maps which guide test-time optimization to\nproduce paired image and pseudo-ground truth meshes. This methodology not only\nalleviates the annotation burden but also enables the assembly of a\ncomprehensive dataset specifically tailored for close interactions in HME. Our\nAsk Pose Unite (APU) dataset, comprising over 6.2k human mesh pairs in contact\ncovering diverse interaction types, is curated from images depicting\nnaturalistic person-to-person scenes. We empirically show that using our\ndataset to train a diffusion-based contact prior, used as guidance during\noptimization, improves mesh estimation on unseen interactions. Our work\naddresses longstanding challenges of data scarcity for close interactions in\nHME enhancing the field's capabilities of handling complex interaction\nscenarios.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Project webpage: https://laubravo.github.io/apu_website/",
    "pdf_url": "http://arxiv.org/pdf/2410.00309v1",
    "published_date": "2024-10-01 01:14:24 UTC",
    "updated_date": "2024-10-01 01:14:24 UTC"
  },
  {
    "arxiv_id": "2410.03738v2",
    "title": "ERASMO: Leveraging Large Language Models for Enhanced Clustering Segmentation",
    "authors": [
      "Fillipe dos Santos Silva",
      "Gabriel Kenzo Kakimoto",
      "Julio Cesar dos Reis",
      "Marcelo S. Reis"
    ],
    "abstract": "Cluster analysis plays a crucial role in various domains and applications,\nsuch as customer segmentation in marketing. These contexts often involve\nmultimodal data, including both tabular and textual datasets, making it\nchallenging to represent hidden patterns for obtaining meaningful clusters.\nThis study introduces ERASMO, a framework designed to fine-tune a pretrained\nlanguage model on textually encoded tabular data and generate embeddings from\nthe fine-tuned model. ERASMO employs a textual converter to transform tabular\ndata into a textual format, enabling the language model to process and\nunderstand the data more effectively. Additionally, ERASMO produces\ncontextually rich and structurally representative embeddings through techniques\nsuch as random feature sequence shuffling and number verbalization. Extensive\nexperimental evaluations were conducted using multiple datasets and baseline\napproaches. Our results demonstrate that ERASMO fully leverages the specific\ncontext of each tabular dataset, leading to more precise and nuanced embeddings\nfor accurate clustering. This approach enhances clustering performance by\ncapturing complex relationship patterns within diverse tabular data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T50 (Natural language processing), 68T01 (General topics in\n  artificial intelligence)"
    ],
    "primary_category": "cs.CL",
    "comment": "15 pages, 10 figures, published in BRACIS 2024 conference",
    "pdf_url": "http://arxiv.org/pdf/2410.03738v2",
    "published_date": "2024-10-01 00:37:16 UTC",
    "updated_date": "2025-02-04 15:06:50 UTC"
  }
]
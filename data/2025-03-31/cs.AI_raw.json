[
  {
    "arxiv_id": "2503.24388v1",
    "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy",
    "authors": [
      "Zhonghan Zhao",
      "Wenwei Zhang",
      "Haian Huang",
      "Kuikun Liu",
      "Jianfei Gao",
      "Gaoang Wang",
      "Kai Chen"
    ],
    "abstract": "Reasoning before action and imagining potential outcomes (i.e., world models)\nare essential for embodied agents operating in complex open-world environments.\nYet, prior work either incorporates only one of these abilities in an\nend-to-end agent or integrates multiple specialized models into an agent\nsystem, limiting the learning efficiency and generalization of the policy.\nThus, this paper makes the first attempt to synergize Reasoning and Imagination\nin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end\nmanner, we construct a data pipeline that progressively integrates and enriches\nthe content of imagination and reasoning in the trajectories collected from\nexisting agents. The joint learning of reasoning and next image generation\nexplicitly models the inherent correlation between reasoning, action, and\ndynamics of environments, and thus exhibits more than $17\\times$ sample\nefficiency improvements and generalization in comparison with previous works.\nDuring inference, RIG first reasons about the next action, produces potential\naction, and then predicts the action outcomes, which offers the agent a chance\nto review and self-correct based on the imagination before taking real actions.\nExperimental results show that the synergy of reasoning and imagination not\nonly improves the robustness, generalization, and interoperability of\ngeneralist policy but also enables test-time scaling to enhance overall\nperformance.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24388v1",
    "published_date": "2025-03-31 17:59:52 UTC",
    "updated_date": "2025-03-31 17:59:52 UTC"
  },
  {
    "arxiv_id": "2503.24381v1",
    "title": "UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving",
    "authors": [
      "Yuping Wang",
      "Xiangyu Huang",
      "Xiaokang Sun",
      "Mingxuan Yan",
      "Shuo Xing",
      "Zhengzhong Tu",
      "Jiachen Li"
    ],
    "abstract": "We introduce UniOcc, a comprehensive, unified benchmark for occupancy\nforecasting (i.e., predicting future occupancies based on historical\ninformation) and current-frame occupancy prediction from camera images. UniOcc\nunifies data from multiple real-world datasets (i.e., nuScenes, Waymo) and\nhigh-fidelity driving simulators (i.e., CARLA, OpenCOOD), which provides 2D/3D\noccupancy labels with per-voxel flow annotations and support for cooperative\nautonomous driving. In terms of evaluation, unlike existing studies that rely\non suboptimal pseudo labels for evaluation, UniOcc incorporates novel metrics\nthat do not depend on ground-truth occupancy, enabling robust assessment of\nadditional aspects of occupancy quality. Through extensive experiments on\nstate-of-the-art models, we demonstrate that large-scale, diverse training data\nand explicit flow information significantly enhance occupancy prediction and\nforecasting performance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "comment": "14 pages; Dataset: https://huggingface.co/datasets/tasl-lab/uniocc;\n  Code: https://github.com/tasl-lab/UniOcc",
    "pdf_url": "http://arxiv.org/pdf/2503.24381v1",
    "published_date": "2025-03-31 17:59:24 UTC",
    "updated_date": "2025-03-31 17:59:24 UTC"
  },
  {
    "arxiv_id": "2503.24379v1",
    "title": "Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation",
    "authors": [
      "Shengqiong Wu",
      "Weicai Ye",
      "Jiahao Wang",
      "Quande Liu",
      "Xintao Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Kun Gai",
      "Shuicheng Yan",
      "Hao Fei",
      "Tat-Seng Chua"
    ],
    "abstract": "To address the bottleneck of accurate user intent interpretation within the\ncurrent video generation community, we present Any2Caption, a novel framework\nfor controllable video generation under any condition. The key idea is to\ndecouple various condition interpretation steps from the video synthesis step.\nBy leveraging modern multimodal large language models (MLLMs), Any2Caption\ninterprets diverse inputs--text, images, videos, and specialized cues such as\nregion, motion, and camera poses--into dense, structured captions that offer\nbackbone video generators with better guidance. We also introduce Any2CapIns, a\nlarge-scale dataset with 337K instances and 407K conditions for\nany-condition-to-caption instruction tuning. Comprehensive evaluations\ndemonstrate significant improvements of our system in controllability and video\nquality across various aspects of existing video generation models. Project\nPage: https://sqwu.top/Any2Cap/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "Project Page: https://sqwu.top/Any2Cap/",
    "pdf_url": "http://arxiv.org/pdf/2503.24379v1",
    "published_date": "2025-03-31 17:59:01 UTC",
    "updated_date": "2025-03-31 17:59:01 UTC"
  },
  {
    "arxiv_id": "2503.24378v1",
    "title": "ACPBench Hard: Unrestrained Reasoning about Action, Change, and Planning",
    "authors": [
      "Harsha Kokel",
      "Michael Katz",
      "Kavitha Srinivas",
      "Shirin Sohrabi"
    ],
    "abstract": "The ACPBench dataset provides atomic reasoning tasks required for efficient\nplanning. The dataset is aimed at distilling the complex plan generation task\ninto separate atomic reasoning tasks in their easiest possible form, boolean or\nmultiple-choice questions, where the model has to choose the right answer from\nthe provided options. While the aim of ACPBench is to test the simplest form of\nreasoning about action and change, when tasked with planning, a model does not\ntypically have options to choose from and thus the reasoning required for\nplanning dictates an open-ended, generative form for these tasks. To that end,\nwe introduce ACPBench Hard, a generative version of ACPBench, with open-ended\nquestions which the model needs to answer. Models that perform well on these\ntasks could in principle be integrated into a planner or be used directly as a\npolicy. We discuss the complexity of these tasks as well as the complexity of\nvalidating the correctness of their answers and present validation algorithms\nfor each task. Equipped with these validators, we test the performance of a\nvariety of models on our tasks and find that for most of these tasks the\nperformance of even the largest models is still subpar. Our experiments show\nthat no model outperforms another in these tasks and with a few exceptions all\ntested language models score below 65%, indicating that even the current\nfrontier language models have a long way to go before they can reliably reason\nabout planning. In fact, even the so-called reasoning models struggle with\nsolving these reasoning tasks. ACPBench Hard collection is available at the\nfollowing link: https://ibm.github.io/ACPBench",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Accepted to LM4Plan@AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.24378v1",
    "published_date": "2025-03-31 17:58:25 UTC",
    "updated_date": "2025-03-31 17:58:25 UTC"
  },
  {
    "arxiv_id": "2503.24377v1",
    "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models",
    "authors": [
      "Rui Wang",
      "Hongru Wang",
      "Boyang Xue",
      "Jianhui Pang",
      "Shudong Liu",
      "Yi Chen",
      "Jiahao Qiu",
      "Derek Fai Wong",
      "Heng Ji",
      "Kam-Fai Wong"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their ability to perform complex reasoning tasks, transitioning from\nfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).\nWhile System 2 reasoning improves task accuracy, it often incurs substantial\ncomputational costs due to its slow thinking nature and inefficient or\nunnecessary reasoning behaviors. In contrast, System 1 reasoning is\ncomputationally efficient but leads to suboptimal performance. Consequently, it\nis critical to balance the trade-off between performance (benefits) and\ncomputational costs (budgets), giving rise to the concept of reasoning economy.\nIn this survey, we provide a comprehensive analysis of reasoning economy in\nboth the post-training and test-time inference stages of LLMs, encompassing i)\nthe cause of reasoning inefficiency, ii) behavior analysis of different\nreasoning patterns, and iii) potential solutions to achieve reasoning economy.\nBy offering actionable insights and highlighting open challenges, we aim to\nshed light on strategies for improving the reasoning economy of LLMs, thereby\nserving as a valuable resource for advancing research in this evolving area. We\nalso provide a public repository to continually track developments in this\nfast-evolving field.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "In Progress; Paper list Repo:\n  https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers",
    "pdf_url": "http://arxiv.org/pdf/2503.24377v1",
    "published_date": "2025-03-31 17:58:07 UTC",
    "updated_date": "2025-03-31 17:58:07 UTC"
  },
  {
    "arxiv_id": "2503.24376v1",
    "title": "Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1",
    "authors": [
      "Yi Chen",
      "Yuying Ge",
      "Rui Wang",
      "Yixiao Ge",
      "Lu Qiu",
      "Ying Shan",
      "Xihui Liu"
    ],
    "abstract": "Recent advancements in Chain of Thought (COT) generation have significantly\nimproved the reasoning capabilities of Large Language Models (LLMs), with\nreinforcement learning (RL) emerging as an effective post-training approach.\nMultimodal Large Language Models (MLLMs) inherit this reasoning potential but\nremain underexplored in tasks requiring both perception and logical reasoning.\nTo address this, we introduce SEED-Bench-R1, a benchmark designed to\nsystematically evaluate post-training methods for MLLMs in video understanding.\nIt includes intricate real-world videos and complex everyday planning tasks in\nthe format of multiple-choice questions, requiring sophisticated perception and\nreasoning. SEED-Bench-R1 assesses generalization through a three-level\nhierarchy: in-distribution, cross-environment, and cross-environment-task\nscenarios, equipped with a large-scale training dataset with easily verifiable\nground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL\nwith supervised fine-tuning (SFT), demonstrating RL's data efficiency and\nsuperior performance on both in-distribution and out-of-distribution tasks,\neven outperforming SFT on general video understanding benchmarks like\nLongVideoBench. Our detailed analysis reveals that RL enhances visual\nperception but often produces less logically coherent reasoning chains. We\nidentify key limitations such as inconsistent reasoning and overlooked visual\ncues, and suggest future improvements in base model reasoning, reward modeling,\nand RL robustness against noisy signals.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "comment": "Technical Report (In Progress); Code released at:\n  https://github.com/TencentARC/SEED-Bench-R1",
    "pdf_url": "http://arxiv.org/pdf/2503.24376v1",
    "published_date": "2025-03-31 17:55:23 UTC",
    "updated_date": "2025-03-31 17:55:23 UTC"
  },
  {
    "arxiv_id": "2503.24370v1",
    "title": "Effectively Controlling Reasoning Models through Thinking Intervention",
    "authors": [
      "Tong Wu",
      "Chong Xiang",
      "Jiachen T. Wang",
      "Prateek Mittal"
    ],
    "abstract": "Reasoning-enhanced large language models (LLMs) explicitly generate\nintermediate reasoning steps prior to generating final answers, helping the\nmodel excel in complex problem-solving. In this paper, we demonstrate that this\nemerging generation framework offers a unique opportunity for more fine-grained\ncontrol over model behavior. We propose Thinking Intervention, a novel paradigm\ndesigned to explicitly guide the internal reasoning processes of LLMs by\nstrategically inserting or revising specific thinking tokens. We conduct\ncomprehensive evaluations across multiple tasks, including instruction\nfollowing on IFEval, instruction hierarchy on SEP, and safety alignment on\nXSTest and SORRY-Bench. Our results demonstrate that Thinking Intervention\nsignificantly outperforms baseline prompting approaches, achieving up to 6.7%\naccuracy gains in instruction-following scenarios, 15.4% improvements in\nreasoning about instruction hierarchies, and a 40.0% increase in refusal rates\nfor unsafe prompts using open-source DeepSeek R1 models. Overall, our work\nopens a promising new research avenue for controlling reasoning LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24370v1",
    "published_date": "2025-03-31 17:50:13 UTC",
    "updated_date": "2025-03-31 17:50:13 UTC"
  },
  {
    "arxiv_id": "2503.24365v1",
    "title": "Which LIME should I trust? Concepts, Challenges, and Solutions",
    "authors": [
      "Patrick Knab",
      "Sascha Marton",
      "Udo Schlegel",
      "Christian Bartelt"
    ],
    "abstract": "As neural networks become dominant in essential systems, Explainable\nArtificial Intelligence (XAI) plays a crucial role in fostering trust and\ndetecting potential misbehavior of opaque models. LIME (Local Interpretable\nModel-agnostic Explanations) is among the most prominent model-agnostic\napproaches, generating explanations by approximating the behavior of black-box\nmodels around specific instances. Despite its popularity, LIME faces challenges\nrelated to fidelity, stability, and applicability to domain-specific problems.\nNumerous adaptations and enhancements have been proposed to address these\nissues, but the growing number of developments can be overwhelming,\ncomplicating efforts to navigate LIME-related research. To the best of our\nknowledge, this is the first survey to comprehensively explore and collect\nLIME's foundational concepts and known limitations. We categorize and compare\nits various enhancements, offering a structured taxonomy based on intermediate\nsteps and key issues. Our analysis provides a holistic overview of advancements\nin LIME, guiding future research and helping practitioners identify suitable\napproaches. Additionally, we provide a continuously updated interactive website\n(https://patrick-knab.github.io/which-lime-to-trust/), offering a concise and\naccessible overview of the survey.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Accepted at the 3rd World Conference on eXplainable Artificial\n  Intelligence (XAI 2025)",
    "pdf_url": "http://arxiv.org/pdf/2503.24365v1",
    "published_date": "2025-03-31 17:44:39 UTC",
    "updated_date": "2025-03-31 17:44:39 UTC"
  },
  {
    "arxiv_id": "2503.24361v1",
    "title": "Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic Manipulation",
    "authors": [
      "Abhiram Maddukuri",
      "Zhenyu Jiang",
      "Lawrence Yunliang Chen",
      "Soroush Nasiriany",
      "Yuqi Xie",
      "Yu Fang",
      "Wenqi Huang",
      "Zu Wang",
      "Zhenjia Xu",
      "Nikita Chernyadev",
      "Scott Reed",
      "Ken Goldberg",
      "Ajay Mandlekar",
      "Linxi Fan",
      "Yuke Zhu"
    ],
    "abstract": "Large real-world robot datasets hold great potential to train generalist\nrobot models, but scaling real-world human data collection is time-consuming\nand resource-intensive. Simulation has great potential in supplementing\nlarge-scale data, especially with recent advances in generative AI and\nautomated data generation tools that enable scalable creation of robot behavior\ndatasets. However, training a policy solely in simulation and transferring it\nto the real world often demands substantial human effort to bridge the reality\ngap. A compelling alternative is to co-train the policy on a mixture of\nsimulation and real-world datasets. Preliminary studies have recently shown\nthis strategy to substantially improve the performance of a policy over one\ntrained on a limited amount of real-world data. Nonetheless, the community\nlacks a systematic understanding of sim-and-real co-training and what it takes\nto reap the benefits of simulation data for real-robot learning. This work\npresents a simple yet effective recipe for utilizing simulation data to solve\nvision-based robotic manipulation tasks. We derive this recipe from\ncomprehensive experiments that validate the co-training strategy on various\nsimulation and real-world datasets. Using two domains--a robot arm and a\nhumanoid--across diverse tasks, we demonstrate that simulation data can enhance\nreal-world task performance by an average of 38%, even with notable differences\nbetween the simulation and real-world data. Videos and additional results can\nbe found at https://co-training.github.io/",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.RO",
    "comment": "Project website: https://co-training.github.io/",
    "pdf_url": "http://arxiv.org/pdf/2503.24361v1",
    "published_date": "2025-03-31 17:39:38 UTC",
    "updated_date": "2025-03-31 17:39:38 UTC"
  },
  {
    "arxiv_id": "2503.24358v1",
    "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
    "authors": [
      "Hao Wang",
      "Ligong Han",
      "Kai Xu",
      "Akash Srivastava"
    ],
    "abstract": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IT",
      "math.IT"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24358v1",
    "published_date": "2025-03-31 17:37:32 UTC",
    "updated_date": "2025-03-31 17:37:32 UTC"
  },
  {
    "arxiv_id": "2503.24354v1",
    "title": "ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion",
    "authors": [
      "Rana Muhammad Shahroz Khan",
      "Dongwen Tang",
      "Pingzhi Li",
      "Kai Wang",
      "Tianlong Chen"
    ],
    "abstract": "Parameter generation has emerged as a novel paradigm for neural network\ndevelopment, offering an alternative to traditional neural network training by\nsynthesizing high-quality model weights directly. In the context of Low-Rank\nAdaptation (LoRA) for evolving ($\\textit{i.e.}$, constantly updated) large\nlanguage models (LLMs), this approach promises efficient adaptation without\ncostly retraining. However, existing methods face critical limitations in\nsimultaneously achieving scalability and controllability. In this paper, we\nintroduce $\\texttt{ORAL}$, a novel $\\textbf{conditional recurrent diffusion}$\nframework that addresses these challenges. $\\texttt{ORAL}$ incorporates a novel\nconditioning mechanism that integrates model architecture and textual task\nspecifications, enabling the generation of task-specific LoRA parameters that\ncan seamlessly transfer across evolving foundation models. Our approach\nsuccessfully scales to billions-of-parameter LLMs and maintains\ncontrollability. Through extensive experiments across seven language tasks,\nfour vision tasks, and three multimodal tasks using five pre-trained LLMs, we\ndemonstrate that $\\texttt{ORAL}$ generates high-quality LoRA parameters that\nachieve comparable or superior performance to vanilla trained counterparts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24354v1",
    "published_date": "2025-03-31 17:34:59 UTC",
    "updated_date": "2025-03-31 17:34:59 UTC"
  },
  {
    "arxiv_id": "2503.24328v1",
    "title": "Contextual Preference Collaborative Measure Framework Based on Belief System",
    "authors": [
      "Hang Yu",
      "Wei Wei",
      "Zheng Tan",
      "Jing-lei Liu"
    ],
    "abstract": "To reduce the human intervention in the preference measure process,this\narticle proposes a preference collaborative measure framework based on an\nupdated belief system,which is also capable of improving the accuracy and\nefficiency of preferen-ce measure algorithms.Firstly,the distance of rules and\nthe average internal distance of rulesets are proposed for specifying the\nrelationship between the rules.For discovering the most representative\npreferences that are common in all users,namely common preference,a algorithm\nbased on average internal distance of ruleset,PRA algorithm,is proposed,which\naims to finish the discoveryprocess with minimum information loss\nrate.Furthermore,the concept of Common belief is proposed to update the belief\nsystem,and the common preferences are the evidences of updated belief\nsystem.Then,under the belief system,the proposed belief degree and deviation\ndegree are used to determine whether a rule confirms the belief system or not\nand classify the preference rules into two kinds(generalized or\npersonalized),and eventually filters out Top-K interesting rules relying on\nbelief degree and deviation degree.Based on above,a scalable interestingness\ncalculation framework that can apply various formulas is proposed for\naccurately calculating interestingness in different conditions.At last,IMCos\nalgorithm and IMCov algorithm are proposed as exemplars to verify the accuracy\nand efficiency of the framework by using weighted cosine similarity and\ncorrelation coefficients as belief degree.In experiments,the proposed\nalgorithms are compared to two state-of-the-art algorithms and the results show\nthat IMCos and IMCov outperform than the other two in most aspects.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "comment": "in Chinese language",
    "pdf_url": "http://arxiv.org/pdf/2503.24328v1",
    "published_date": "2025-03-31 17:17:45 UTC",
    "updated_date": "2025-03-31 17:17:45 UTC"
  },
  {
    "arxiv_id": "2503.24325v1",
    "title": "Pro-Routing: Proactive Routing of Autonomous Multi-Capacity Robots for Pickup-and-Delivery Tasks",
    "authors": [
      "Daniel Garces",
      "Stephanie Gil"
    ],
    "abstract": "We consider a multi-robot setting, where we have a fleet of multi-capacity\nautonomous robots that must service spatially distributed pickup-and-delivery\nrequests with fixed maximum wait times. Requests can be either scheduled ahead\nof time or they can enter the system in real-time. In this setting, stability\nfor a routing policy is defined as the cost of the policy being uniformly\nbounded over time. Most previous work either solve the problem offline to\ntheoretically maintain stability or they consider dynamically arriving requests\nat the expense of the theoretical guarantees on stability. In this paper, we\naim to bridge this gap by proposing a novel proactive rollout-based routing\nframework that adapts to real-time demand while still provably maintaining the\nstability of the learned routing policy. We derive provable stability\nguarantees for our method by proposing a fleet sizing algorithm that obtains a\nsufficiently large fleet that ensures stability by construction. To validate\nour theoretical results, we consider a case study on real ride requests for\nHarvard's evening Van System. We also evaluate the performance of our framework\nusing the currently deployed smaller fleet size. In this smaller setup, we\ncompare against the currently deployed routing algorithm, greedy heuristics,\nand Monte-Carlo-Tree-Search-based algorithms. Our empirical results show that\nour framework maintains stability when we use the sufficiently large fleet size\nfound in our theoretical results. For the smaller currently deployed fleet\nsize, our method services 6% more requests than the closest baseline while\nreducing median passenger wait times by 33%.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "25 pages, 7 figures, and 1 table",
    "pdf_url": "http://arxiv.org/pdf/2503.24325v1",
    "published_date": "2025-03-31 17:14:07 UTC",
    "updated_date": "2025-03-31 17:14:07 UTC"
  },
  {
    "arxiv_id": "2503.24310v1",
    "title": "BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models",
    "authors": [
      "Alok Abhishek",
      "Lisa Erickson",
      "Tushar Bandopadhyay"
    ],
    "abstract": "In this research, we introduce BEATS, a novel framework for evaluating Bias,\nEthics, Fairness, and Factuality in Large Language Models (LLMs). Building upon\nthe BEATS framework, we present a bias benchmark for LLMs that measure\nperformance across 29 distinct metrics. These metrics span a broad range of\ncharacteristics, including demographic, cognitive, and social biases, as well\nas measures of ethical reasoning, group fairness, and factuality related\nmisinformation risk. These metrics enable a quantitative assessment of the\nextent to which LLM generated responses may perpetuate societal prejudices that\nreinforce or expand systemic inequities. To achieve a high score on this\nbenchmark a LLM must show very equitable behavior in their responses, making it\na rigorous standard for responsible AI evaluation. Empirical results based on\ndata from our experiment show that, 37.65\\% of outputs generated by industry\nleading models contained some form of bias, highlighting a substantial risk of\nusing these models in critical decision making systems. BEATS framework and\nbenchmark offer a scalable and statistically rigorous methodology to benchmark\nLLMs, diagnose factors driving biases, and develop mitigation strategies. With\nthe BEATS framework, our goal is to help the development of more socially\nresponsible and ethically aligned AI models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "68T01 (Primary), 68T50 (Secondary)",
      "I.2.0; I.2.7"
    ],
    "primary_category": "cs.CL",
    "comment": "32 pages, 33 figures, preprint version",
    "pdf_url": "http://arxiv.org/pdf/2503.24310v1",
    "published_date": "2025-03-31 16:56:52 UTC",
    "updated_date": "2025-03-31 16:56:52 UTC"
  },
  {
    "arxiv_id": "2503.24307v1",
    "title": "A Systematic Evaluation of LLM Strategies for Mental Health Text Analysis: Fine-tuning vs. Prompt Engineering vs. RAG",
    "authors": [
      "Arshia Kermani",
      "Veronica Perez-Rosas",
      "Vangelis Metsis"
    ],
    "abstract": "This study presents a systematic comparison of three approaches for the\nanalysis of mental health text using large language models (LLMs): prompt\nengineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA\n3, we evaluate these approaches on emotion classification and mental health\ncondition detection tasks across two datasets. Fine-tuning achieves the highest\naccuracy (91% for emotion classification, 80% for mental health conditions) but\nrequires substantial computational resources and large training sets, while\nprompt engineering and RAG offer more flexible deployment with moderate\nperformance (40-68% accuracy). Our findings provide practical insights for\nimplementing LLM-based solutions in mental health applications, highlighting\nthe trade-offs between accuracy, computational requirements, and deployment\nflexibility.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24307v1",
    "published_date": "2025-03-31 16:54:04 UTC",
    "updated_date": "2025-03-31 16:54:04 UTC"
  },
  {
    "arxiv_id": "2503.24305v2",
    "title": "Evaluating machine learning models for predicting pesticides toxicity to honey bees",
    "authors": [
      "Jakub Adamczyk",
      "Jakub Poziemski",
      "Pawel Siedlecki"
    ],
    "abstract": "Small molecules play a critical role in the biomedical, environmental, and\nagrochemical domains, each with distinct physicochemical requirements and\nsuccess criteria. Although biomedical research benefits from extensive datasets\nand established benchmarks, agrochemical data remain scarce, particularly with\nrespect to species-specific toxicity. This work focuses on ApisTox, the most\ncomprehensive dataset of experimentally validated chemical toxicity to the\nhoney bee (Apis mellifera), an ecologically vital pollinator. We evaluate\nApisTox using a diverse suite of machine learning approaches, including\nmolecular fingerprints, graph kernels, and graph neural networks, as well as\npretrained models. Comparative analysis with medicinal datasets from the\nMoleculeNet benchmark reveals that ApisTox represents a distinct chemical\nspace. Performance degradation on non-medicinal datasets, such as ApisTox,\ndemonstrates their limited generalizability of current state-of-the-art\nalgorithms trained solely on biomedical data. Our study highlights the need for\nmore diverse datasets and for targeted model development geared toward the\nagrochemical domain.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24305v2",
    "published_date": "2025-03-31 16:51:12 UTC",
    "updated_date": "2025-04-01 07:57:14 UTC"
  },
  {
    "arxiv_id": "2503.24299v1",
    "title": "Shape Expressions with Inheritance",
    "authors": [
      "Iovka Boneva",
      "Jose Emilio Labra Gayo",
      "Eric Prud'hommeaux",
      "Katherine Thornton",
      "Andra Waagmeester"
    ],
    "abstract": "We formally introduce an inheritance mechanism for the Shape Expressions\nlanguage (ShEx). It is inspired by inheritance in object-oriented programming\nlanguages, and provides similar advantages such as reuse, modularity, and more\nflexible data modelling. Using an example, we explain the main features of the\ninheritance mechanism. We present its syntax and formal semantics. The\nsemantics is an extension of the semantics of ShEx 2.1. It also directly yields\na validation algorithm as an extension of the previous ShEx validation\nalgorithms, while maintaining the same algorithmic complexity.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "Accepted in Extended Semantic Web Conference, ESWC, 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.24299v1",
    "published_date": "2025-03-31 16:42:44 UTC",
    "updated_date": "2025-03-31 16:42:44 UTC"
  },
  {
    "arxiv_id": "2503.24284v1",
    "title": "Value of Information-based Deceptive Path Planning Under Adversarial Interventions",
    "authors": [
      "Wesley A. Suttle",
      "Jesse Milzman",
      "Mustafa O. Karabag",
      "Brian M. Sadler",
      "Ufuk Topcu"
    ],
    "abstract": "Existing methods for deceptive path planning (DPP) address the problem of\ndesigning paths that conceal their true goal from a passive, external observer.\nSuch methods do not apply to problems where the observer has the ability to\nperform adversarial interventions to impede the path planning agent. In this\npaper, we propose a novel Markov decision process (MDP)-based model for the DPP\nproblem under adversarial interventions and develop new value of information\n(VoI) objectives to guide the design of DPP policies. Using the VoI objectives\nwe propose, path planning agents deceive the adversarial observer into choosing\nsuboptimal interventions by selecting trajectories that are of low\ninformational value to the observer. Leveraging connections to the linear\nprogramming theory for MDPs, we derive computationally efficient solution\nmethods for synthesizing policies for performing DPP under adversarial\ninterventions. In our experiments, we illustrate the effectiveness of the\nproposed solution method in achieving deceptiveness under adversarial\ninterventions and demonstrate the superior performance of our approach to both\nexisting DPP methods and conservative path planning approaches on illustrative\ngridworld problems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "comment": "10 pages, 4 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.24284v1",
    "published_date": "2025-03-31 16:31:29 UTC",
    "updated_date": "2025-03-31 16:31:29 UTC"
  },
  {
    "arxiv_id": "2503.24278v1",
    "title": "AutoEval: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World",
    "authors": [
      "Zhiyuan Zhou",
      "Pranav Atreya",
      "You Liang Tan",
      "Karl Pertsch",
      "Sergey Levine"
    ],
    "abstract": "Scalable and reproducible policy evaluation has been a long-standing\nchallenge in robot learning. Evaluations are critical to assess progress and\nbuild better policies, but evaluation in the real world, especially at a scale\nthat would provide statistically reliable results, is costly in terms of human\ntime and hard to obtain. Evaluation of increasingly generalist robot policies\nrequires an increasingly diverse repertoire of evaluation environments, making\nthe evaluation bottleneck even more pronounced. To make real-world evaluation\nof robotic policies more practical, we propose AutoEval, a system to\nautonomously evaluate generalist robot policies around the clock with minimal\nhuman intervention. Users interact with AutoEval by submitting evaluation jobs\nto the AutoEval queue, much like how software jobs are submitted with a cluster\nscheduling system, and AutoEval will schedule the policies for evaluation\nwithin a framework supplying automatic success detection and automatic scene\nresets. We show that AutoEval can nearly fully eliminate human involvement in\nthe evaluation process, permitting around the clock evaluations, and the\nevaluation results correspond closely to ground truth evaluations conducted by\nhand. To facilitate the evaluation of generalist policies in the robotics\ncommunity, we provide public access to multiple AutoEval scenes in the popular\nBridgeData robot setup with WidowX robot arms. In the future, we hope that\nAutoEval scenes can be set up across institutions to form a diverse and\ndistributed evaluation network.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24278v1",
    "published_date": "2025-03-31 16:23:44 UTC",
    "updated_date": "2025-03-31 16:23:44 UTC"
  },
  {
    "arxiv_id": "2503.24277v1",
    "title": "Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality",
    "authors": [
      "Sewoong Lee",
      "Adam Davies",
      "Marc E. Canby",
      "Julia Hockenmaier"
    ],
    "abstract": "Sparse autoencoders (SAEs) have emerged as a workhorse of modern mechanistic\ninterpretability, but leading SAE approaches with top-$k$ style activation\nfunctions lack theoretical grounding for selecting the hyperparameter $k$. SAEs\nare based on the linear representation hypothesis (LRH), which assumes that the\nrepresentations of large language models (LLMs) are linearly encoded, and the\nsuperposition hypothesis (SH), which states that there can be more features in\nthe model than its dimensionality. We show that, based on the formal\ndefinitions of the LRH and SH, the magnitude of sparse feature vectors (the\nlatent representations learned by SAEs of the dense embeddings of LLMs) can be\napproximated using their corresponding dense vector with a closed-form error\nbound. To visualize this, we propose the ZF plot, which reveals a previously\nunknown relationship between LLM hidden embeddings and SAE feature vectors,\nallowing us to make the first empirical measurement of the extent to which\nfeature vectors of pre-trained SAEs are over- or under-activated for a given\ninput. Correspondingly, we introduce Approximate Feature Activation (AFA),\nwhich approximates the magnitude of the ground-truth sparse feature vector, and\npropose a new evaluation metric derived from AFA to assess the alignment\nbetween inputs and activations. We also leverage AFA to introduce a novel SAE\narchitecture, the top-AFA SAE, leading to SAEs that: (a) are more in line with\ntheoretical justifications; and (b) obviate the need to tune SAE sparsity\nhyperparameters. Finally, we empirically demonstrate that top-AFA SAEs achieve\nreconstruction loss comparable to that of state-of-the-art top-k SAEs, without\nrequiring the hyperparameter $k$ to be tuned. Our code is available at:\nhttps://github.com/SewoongLee/top-afa-sae.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24277v1",
    "published_date": "2025-03-31 16:22:11 UTC",
    "updated_date": "2025-03-31 16:22:11 UTC"
  },
  {
    "arxiv_id": "2503.24270v2",
    "title": "Visual Acoustic Fields",
    "authors": [
      "Yuelei Li",
      "Hyunjin Kim",
      "Fangneng Zhan",
      "Ri-Zhao Qiu",
      "Mazeyu Ji",
      "Xiaojun Shan",
      "Xueyan Zou",
      "Paul Liang",
      "Hanspeter Pfister",
      "Xiaolong Wang"
    ],
    "abstract": "Objects produce different sounds when hit, and humans can intuitively infer\nhow an object might sound based on its appearance and material properties.\nInspired by this intuition, we propose Visual Acoustic Fields, a framework that\nbridges hitting sounds and visual signals within a 3D space using 3D Gaussian\nSplatting (3DGS). Our approach features two key modules: sound generation and\nsound localization. The sound generation module leverages a conditional\ndiffusion model, which takes multiscale features rendered from a\nfeature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, the\nsound localization module enables querying the 3D scene, represented by the\nfeature-augmented 3DGS, to localize hitting positions based on the sound\nsources. To support this framework, we introduce a novel pipeline for\ncollecting scene-level visual-sound sample pairs, achieving alignment between\ncaptured images, impact locations, and corresponding sounds. To the best of our\nknowledge, this is the first dataset to connect visual and acoustic signals in\na 3D context. Extensive experiments on our dataset demonstrate the\neffectiveness of Visual Acoustic Fields in generating plausible impact sounds\nand accurately localizing impact sources. Our project page is at\nhttps://yuelei0428.github.io/projects/Visual-Acoustic-Fields/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24270v2",
    "published_date": "2025-03-31 16:16:10 UTC",
    "updated_date": "2025-04-01 03:16:38 UTC"
  },
  {
    "arxiv_id": "2503.24262v1",
    "title": "New Statistical Framework for Extreme Error Probability in High-Stakes Domains for Reliable Machine Learning",
    "authors": [
      "Umberto Michelucci",
      "Francesca Venturini"
    ],
    "abstract": "Machine learning is vital in high-stakes domains, yet conventional validation\nmethods rely on averaging metrics like mean squared error (MSE) or mean\nabsolute error (MAE), which fail to quantify extreme errors. Worst-case\nprediction failures can have substantial consequences, but current frameworks\nlack statistical foundations for assessing their probability. In this work a\nnew statistical framework, based on Extreme Value Theory (EVT), is presented\nthat provides a rigorous approach to estimating worst-case failures. Applying\nEVT to synthetic and real-world datasets, this method is shown to enable robust\nestimation of catastrophic failure probabilities, overcoming the fundamental\nlimitations of standard cross-validation. This work establishes EVT as a\nfundamental tool for assessing model reliability, ensuring safer AI deployment\nin new technologies where uncertainty quantification is central to\ndecision-making or scientific analysis.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24262v1",
    "published_date": "2025-03-31 16:08:11 UTC",
    "updated_date": "2025-03-31 16:08:11 UTC"
  },
  {
    "arxiv_id": "2503.24258v1",
    "title": "Beyond a Single Mode: GAN Ensembles for Diverse Medical Data Generation",
    "authors": [
      "Lorenzo Tronchin",
      "Tommy LÃ¶fstedt",
      "Paolo Soda",
      "Valerio Guarrasi"
    ],
    "abstract": "The advancement of generative AI, particularly in medical imaging, confronts\nthe trilemma of ensuring high fidelity, diversity, and efficiency in synthetic\ndata generation. While Generative Adversarial Networks (GANs) have shown\npromise across various applications, they still face challenges like mode\ncollapse and insufficient coverage of real data distributions. This work\nexplores the use of GAN ensembles to overcome these limitations, specifically\nin the context of medical imaging. By solving a multi-objective optimisation\nproblem that balances fidelity and diversity, we propose a method for selecting\nan optimal ensemble of GANs tailored for medical data. The selected ensemble is\ncapable of generating diverse synthetic medical images that are representative\nof true data distributions and computationally efficient. Each model in the\nensemble brings a unique contribution, ensuring minimal redundancy. We\nconducted a comprehensive evaluation using three distinct medical datasets,\ntesting 22 different GAN architectures with various loss functions and\nregularisation techniques. By sampling models at different training epochs, we\ncrafted 110 unique configurations. The results highlight the capability of GAN\nensembles to enhance the quality and utility of synthetic medical images,\nthereby improving the efficacy of downstream tasks such as diagnostic\nmodelling.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24258v1",
    "published_date": "2025-03-31 16:06:01 UTC",
    "updated_date": "2025-03-31 16:06:01 UTC"
  },
  {
    "arxiv_id": "2503.24237v1",
    "title": "Spatio-temporal Prediction of Fine-Grained Origin-Destination Matrices with Applications in Ridesharing",
    "authors": [
      "Run Yang",
      "Runpeng Dai",
      "Siran Gao",
      "Xiaocheng Tang",
      "Fan Zhou",
      "Hongtu Zhu"
    ],
    "abstract": "Accurate spatial-temporal prediction of network-based travelers' requests is\ncrucial for the effective policy design of ridesharing platforms. Having\nknowledge of the total demand between various locations in the upcoming time\nslots enables platforms to proactively prepare adequate supplies, thereby\nincreasing the likelihood of fulfilling travelers' requests and redistributing\nidle drivers to areas with high potential demand to optimize the global\nsupply-demand equilibrium. This paper delves into the prediction of\nOrigin-Destination (OD) demands at a fine-grained spatial level, especially\nwhen confronted with an expansive set of local regions. While this task holds\nimmense practical value, it remains relatively unexplored within the research\ncommunity. To fill this gap, we introduce a novel prediction model called\nOD-CED, which comprises an unsupervised space coarsening technique to alleviate\ndata sparsity and an encoder-decoder architecture to capture both semantic and\ngeographic dependencies. Through practical experimentation, OD-CED has\ndemonstrated remarkable results. It achieved an impressive reduction of up to\n45% reduction in root-mean-square error and 60% in weighted mean absolute\npercentage error over traditional statistical methods when dealing with OD\nmatrices exhibiting a sparsity exceeding 90%.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24237v1",
    "published_date": "2025-03-31 15:52:27 UTC",
    "updated_date": "2025-03-31 15:52:27 UTC"
  },
  {
    "arxiv_id": "2503.24235v1",
    "title": "What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models",
    "authors": [
      "Qiyuan Zhang",
      "Fuyuan Lyu",
      "Zexu Sun",
      "Lei Wang",
      "Weixu Zhang",
      "Zhihan Guo",
      "Yufei Wang",
      "Irwin King",
      "Xue Liu",
      "Chen Ma"
    ],
    "abstract": "As enthusiasm for scaling computation (data and parameters) in the\npretraining era gradually diminished, test-time scaling (TTS), also referred to\nas ``test-time computing'' has emerged as a prominent research focus. Recent\nstudies demonstrate that TTS can further elicit the problem-solving\ncapabilities of large language models (LLMs), enabling significant\nbreakthroughs not only in specialized reasoning tasks, such as mathematics and\ncoding, but also in general tasks like open-ended Q&A. However, despite the\nexplosion of recent efforts in this area, there remains an urgent need for a\ncomprehensive survey offering a systemic understanding. To fill this gap, we\npropose a unified, multidimensional framework structured along four core\ndimensions of TTS research: what to scale, how to scale, where to scale, and\nhow well to scale. Building upon this taxonomy, we conduct an extensive review\nof methods, application scenarios, and assessment aspects, and present an\norganized decomposition that highlights the unique functional roles of\nindividual techniques within the broader TTS landscape. From this analysis, we\ndistill the major developmental trajectories of TTS to date and offer hands-on\nguidelines for practical deployment. Furthermore, we identify several open\nchallenges and offer insights into promising future directions, including\nfurther scaling, clarifying the functional essence of techniques, generalizing\nto more tasks, and more attributions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24235v1",
    "published_date": "2025-03-31 15:46:15 UTC",
    "updated_date": "2025-03-31 15:46:15 UTC"
  },
  {
    "arxiv_id": "2503.24228v1",
    "title": "PAARS: Persona Aligned Agentic Retail Shoppers",
    "authors": [
      "Saab Mansour",
      "Leonardo Perelli",
      "Lorenzo Mainetti",
      "George Davidson",
      "Stefano D'Amato"
    ],
    "abstract": "In e-commerce, behavioral data is collected for decision making which can be\ncostly and slow. Simulation with LLM powered agents is emerging as a promising\nalternative for representing human population behavior. However, LLMs are known\nto exhibit certain biases, such as brand bias, review rating bias and limited\nrepresentation of certain groups in the population, hence they need to be\ncarefully benchmarked and aligned to user behavior. Ultimately, our goal is to\nsynthesise an agent population and verify that it collectively approximates a\nreal sample of humans. To this end, we propose a framework that: (i) creates\nsynthetic shopping agents by automatically mining personas from anonymised\nhistorical shopping data, (ii) equips agents with retail-specific tools to\nsynthesise shopping sessions and (iii) introduces a novel alignment suite\nmeasuring distributional differences between humans and shopping agents at the\ngroup (i.e. population) level rather than the traditional \"individual\" level.\nExperimental results demonstrate that using personas improves performance on\nthe alignment suite, though a gap remains to human behaviour. We showcase an\ninitial application of our framework for automated agentic A/B testing and\ncompare the findings to human results. Finally, we discuss applications,\nlimitations and challenges setting the stage for impactful future work.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24228v1",
    "published_date": "2025-03-31 15:41:51 UTC",
    "updated_date": "2025-03-31 15:41:51 UTC"
  },
  {
    "arxiv_id": "2503.24219v1",
    "title": "MB-ORES: A Multi-Branch Object Reasoner for Visual Grounding in Remote Sensing",
    "authors": [
      "Karim Radouane",
      "Hanane Azzag",
      "Mustapha lebbah"
    ],
    "abstract": "We propose a unified framework that integrates object detection (OD) and\nvisual grounding (VG) for remote sensing (RS) imagery. To support conventional\nOD and establish an intuitive prior for VG task, we fine-tune an open-set\nobject detector using referring expression data, framing it as a partially\nsupervised OD task. In the first stage, we construct a graph representation of\neach image, comprising object queries, class embeddings, and proposal\nlocations. Then, our task-aware architecture processes this graph to perform\nthe VG task. The model consists of: (i) a multi-branch network that integrates\nspatial, visual, and categorical features to generate task-aware proposals, and\n(ii) an object reasoning network that assigns probabilities across proposals,\nfollowed by a soft selection mechanism for final referring object localization.\nOur model demonstrates superior performance on the OPT-RSVG and DIOR-RSVG\ndatasets, achieving significant improvements over state-of-the-art methods\nwhile retaining classical OD capabilities. The code will be available in our\nrepository: \\url{https://github.com/rd20karim/MB-ORES}.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24219v1",
    "published_date": "2025-03-31 15:36:41 UTC",
    "updated_date": "2025-03-31 15:36:41 UTC"
  },
  {
    "arxiv_id": "2503.24215v1",
    "title": "All You Need is Sally-Anne: ToM in AI Strongly Supported After Surpassing Tests for 3-Year-Olds",
    "authors": [
      "Nitay Alon",
      "Joseph Barnby",
      "Reuth Mirsky",
      "Stefan Sarkadi"
    ],
    "abstract": "Theory of Mind (ToM) is a hallmark of human cognition, allowing individuals\nto reason about others' beliefs and intentions. Engineers behind recent\nadvances in Artificial Intelligence (AI) have claimed to demonstrate comparable\ncapabilities. This paper presents a model that surpasses traditional ToM tests\ndesigned for 3-year-old children, providing strong support for the presence of\nToM in AI systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24215v1",
    "published_date": "2025-03-31 15:32:10 UTC",
    "updated_date": "2025-03-31 15:32:10 UTC"
  },
  {
    "arxiv_id": "2503.24210v1",
    "title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting",
    "authors": [
      "Seungjun Lee",
      "Gim Hee Lee"
    ],
    "abstract": "Reconstructing sharp 3D representations from blurry multi-view images are\nlong-standing problem in computer vision. Recent works attempt to enhance\nhigh-quality novel view synthesis from the motion blur by leveraging\nevent-based cameras, benefiting from high dynamic range and microsecond\ntemporal resolution. However, they often reach sub-optimal visual quality in\neither restoring inaccurate color or losing fine-grained details. In this\npaper, we present DiET-GS, a diffusion prior and event stream-assisted motion\ndeblurring 3DGS. Our framework effectively leverages both blur-free event\nstreams and diffusion prior in a two-stage training strategy. Specifically, we\nintroduce the novel framework to constraint 3DGS with event double integral,\nachieving both accurate color and well-defined details. Additionally, we\npropose a simple technique to leverage diffusion prior to further enhance the\nedge details. Qualitative and quantitative results on both synthetic and\nreal-world data demonstrate that our DiET-GS is capable of producing\nsignificantly better quality of novel views compared to the existing baselines.\nOur project page is https://diet-gs.github.io",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "comment": "CVPR 2025. Project Page: https://diet-gs.github.io",
    "pdf_url": "http://arxiv.org/pdf/2503.24210v1",
    "published_date": "2025-03-31 15:27:07 UTC",
    "updated_date": "2025-03-31 15:27:07 UTC"
  },
  {
    "arxiv_id": "2503.24199v1",
    "title": "Agent-Based Simulations of Online Political Discussions: A Case Study on Elections in Germany",
    "authors": [
      "Abdul Sittar",
      "Simon MÃ¼nker",
      "Fabio Sartori",
      "Andreas Reitenbach",
      "Achim Rettinger",
      "Michael MÃ¤s",
      "Alenka GuÄek",
      "Marko Grobelnik"
    ],
    "abstract": "User engagement on social media platforms is influenced by historical\ncontext, time constraints, and reward-driven interactions. This study presents\nan agent-based simulation approach that models user interactions, considering\npast conversation history, motivation, and resource constraints. Utilizing\nGerman Twitter data on political discourse, we fine-tune AI models to generate\nposts and replies, incorporating sentiment analysis, irony detection, and\noffensiveness classification. The simulation employs a myopic best-response\nmodel to govern agent behavior, accounting for decision-making based on\nexpected rewards. Our results highlight the impact of historical context on\nAI-generated responses and demonstrate how engagement evolves under varying\nconstraints.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "comment": "15 pages, 3, ESWC, Workshop Paper",
    "pdf_url": "http://arxiv.org/pdf/2503.24199v1",
    "published_date": "2025-03-31 15:17:04 UTC",
    "updated_date": "2025-03-31 15:17:04 UTC"
  },
  {
    "arxiv_id": "2503.24191v1",
    "title": "Output Constraints as Attack Surface: Exploiting Structured Generation to Bypass LLM Safety Mechanisms",
    "authors": [
      "Shuoming Zhang",
      "Jiacheng Zhao",
      "Ruiyuan Xu",
      "Xiaobing Feng",
      "Huimin Cui"
    ],
    "abstract": "Content Warning: This paper may contain unsafe or harmful content generated\nby LLMs that may be offensive to readers. Large Language Models (LLMs) are\nextensively used as tooling platforms through structured output APIs to ensure\nsyntax compliance so that robust integration with existing softwares like agent\nsystems, could be achieved. However, the feature enabling functionality of\ngrammar-guided structured output presents significant security vulnerabilities.\nIn this work, we reveal a critical control-plane attack surface orthogonal to\ntraditional data-plane vulnerabilities. We introduce Constrained Decoding\nAttack (CDA), a novel jailbreak class that weaponizes structured output\nconstraints to bypass safety mechanisms. Unlike prior attacks focused on input\nprompts, CDA operates by embedding malicious intent in schema-level grammar\nrules (control-plane) while maintaining benign surface prompts (data-plane). We\ninstantiate this with a proof-of-concept Chain Enum Attack, achieves 96.2%\nattack success rates across proprietary and open-weight LLMs on five safety\nbenchmarks with a single query, including GPT-4o and Gemini-2.0-flash. Our\nfindings identify a critical security blind spot in current LLM architectures\nand urge a paradigm shift in LLM safety to address control-plane\nvulnerabilities, as current mechanisms focused solely on data-plane threats\nleave critical systems exposed.",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "primary_category": "cs.CR",
    "comment": "15 pages, 13 figures, 4 tables Work In Progress",
    "pdf_url": "http://arxiv.org/pdf/2503.24191v1",
    "published_date": "2025-03-31 15:08:06 UTC",
    "updated_date": "2025-03-31 15:08:06 UTC"
  },
  {
    "arxiv_id": "2503.24165v1",
    "title": "Predicting Targeted Therapy Resistance in Non-Small Cell Lung Cancer Using Multimodal Machine Learning",
    "authors": [
      "Peiying Hua",
      "Andrea Olofson",
      "Faraz Farhadi",
      "Liesbeth Hondelink",
      "Gregory Tsongalis",
      "Konstantin Dragnev",
      "Dagmar Hoegemann Savellano",
      "Arief Suriawinata",
      "Laura Tafe",
      "Saeed Hassanpour"
    ],
    "abstract": "Lung cancer is the primary cause of cancer death globally, with non-small\ncell lung cancer (NSCLC) emerging as its most prevalent subtype. Among NSCLC\npatients, approximately 32.3% have mutations in the epidermal growth factor\nreceptor (EGFR) gene. Osimertinib, a third-generation EGFR-tyrosine kinase\ninhibitor (TKI), has demonstrated remarkable efficacy in the treatment of NSCLC\npatients with activating and T790M resistance EGFR mutations. Despite its\nestablished efficacy, drug resistance poses a significant challenge for\npatients to fully benefit from osimertinib. The absence of a standard tool to\naccurately predict TKI resistance, including that of osimertinib, remains a\ncritical obstacle. To bridge this gap, in this study, we developed an\ninterpretable multimodal machine learning model designed to predict patient\nresistance to osimertinib among late-stage NSCLC patients with activating EGFR\nmutations, achieving a c-index of 0.82 on a multi-institutional dataset. This\nmachine learning model harnesses readily available data routinely collected\nduring patient visits and medical assessments to facilitate precision lung\ncancer management and informed treatment decisions. By integrating various data\ntypes such as histology images, next generation sequencing (NGS) data,\ndemographics data, and clinical records, our multimodal model can generate\nwell-informed recommendations. Our experiment results also demonstrated the\nsuperior performance of the multimodal model over single modality models\n(c-index 0.82 compared with 0.75 and 0.77), thus underscoring the benefit of\ncombining multiple modalities in patient outcome prediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24165v1",
    "published_date": "2025-03-31 14:47:02 UTC",
    "updated_date": "2025-03-31 14:47:02 UTC"
  },
  {
    "arxiv_id": "2503.24150v1",
    "title": "Learning a Canonical Basis of Human Preferences from Binary Ratings",
    "authors": [
      "Kailas Vodrahalli",
      "Wei Wei",
      "James Zou"
    ],
    "abstract": "Recent advances in generative AI have been driven by alignment techniques\nsuch as reinforcement learning from human feedback (RLHF). RLHF and related\ntechniques typically involve constructing a dataset of binary or ranked choice\nhuman preferences and subsequently fine-tuning models to align with these\npreferences. This paper shifts the focus to understanding the preferences\nencoded in such datasets and identifying common human preferences. We find that\na small subset of 21 preference categories (selected from a set of nearly 5,000\ndistinct preferences) captures >89% of preference variation across individuals.\nThis small set of preferences is analogous to a canonical basis of human\npreferences, similar to established findings that characterize human variation\nin psychology or facial recognition studies. Through both synthetic and\nempirical evaluations, we confirm that our low-rank, canonical set of human\npreferences generalizes across the entire dataset and within specific topics.\nWe further demonstrate our preference basis' utility in model evaluation, where\nour preference categories offer deeper insights into model alignment, and in\nmodel training, where we show that fine-tuning on preference-defined subsets\nsuccessfully aligns the model accordingly.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "comment": "25 pages, 11 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.24150v1",
    "published_date": "2025-03-31 14:35:48 UTC",
    "updated_date": "2025-03-31 14:35:48 UTC"
  },
  {
    "arxiv_id": "2503.24145v1",
    "title": "Resonance: Drawing from Memories to Imagine Positive Futures through AI-Augmented Journaling",
    "authors": [
      "Wazeer Zulfikar",
      "Treyden Chiaravalloti",
      "Jocelyn Shen",
      "Rosalind Picard",
      "Pattie Maes"
    ],
    "abstract": "People inherently use experiences of their past while imagining their future,\na capability that plays a crucial role in mental health. Resonance is an\nAI-powered journaling tool designed to augment this ability by offering\nAI-generated, action-oriented suggestions for future activities based on the\nuser's own past memories. Suggestions are offered when a new memory is logged\nand are followed by a prompt for the user to imagine carrying out the\nsuggestion. In a two-week randomized controlled study (N=55), we found that\nusing Resonance significantly improved mental health outcomes, reducing the\nusers' PHQ8 scores, a measure of current depression, and increasing their daily\npositive affect, particularly when they would likely act on the suggestion.\nNotably, the effectiveness of the suggestions was higher when they were\npersonal, novel, and referenced the user's logged memories. Finally, through\nopen-ended feedback, we discuss the factors that encouraged or hindered the use\nof the tool.",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "primary_category": "cs.HC",
    "comment": "17 pages, 13 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.24145v1",
    "published_date": "2025-03-31 14:30:47 UTC",
    "updated_date": "2025-03-31 14:30:47 UTC"
  },
  {
    "arxiv_id": "2503.24130v1",
    "title": "Graph Neural Network-Based Predictive Modeling for Robotic Plaster Printing",
    "authors": [
      "Diego Machain Rivera",
      "Selen Ercan Jenny",
      "Ping Hsun Tsai",
      "Ena Lloret-Fritschi",
      "Luis Salamanca",
      "Fernando Perez-Cruz",
      "Konstantinos E. Tatsis"
    ],
    "abstract": "This work proposes a Graph Neural Network (GNN) modeling approach to predict\nthe resulting surface from a particle based fabrication process. The latter\nconsists of spray-based printing of cementitious plaster on a wall and is\nfacilitated with the use of a robotic arm. The predictions are computed using\nthe robotic arm trajectory features, such as position, velocity and direction,\nas well as the printing process parameters. The proposed approach, based on a\nparticle representation of the wall domain and the end effector, allows for the\nadoption of a graph-based solution. The GNN model consists of an\nencoder-processor-decoder architecture and is trained using data from\nlaboratory tests, while the hyperparameters are optimized by means of a\nBayesian scheme. The aim of this model is to act as a simulator of the printing\nprocess, and ultimately used for the generation of the robotic arm trajectory\nand the optimization of the printing parameters, towards the materialization of\nan autonomous plastering process. The performance of the proposed model is\nassessed in terms of the prediction error against unseen ground truth data,\nwhich shows its generality in varied scenarios, as well as in comparison with\nthe performance of an existing benchmark model. The results demonstrate a\nsignificant improvement over the benchmark model, with notably better\nperformance and enhanced error scaling across prediction steps.",
    "categories": [
      "cs.CE",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24130v1",
    "published_date": "2025-03-31 14:15:00 UTC",
    "updated_date": "2025-03-31 14:15:00 UTC"
  },
  {
    "arxiv_id": "2503.24110v1",
    "title": "Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to Embodied Cognition",
    "authors": [
      "FranÃ§ois Olivier",
      "Zied Bouraoui"
    ],
    "abstract": "Despite advances in embodied AI, agent reasoning systems still struggle to\ncapture the fundamental conceptual structures that humans naturally use to\nunderstand and interact with their environment. To address this, we propose a\nnovel framework that bridges embodied cognition theory and agent systems by\nleveraging a formal characterization of image schemas, which are defined as\nrecurring patterns of sensorimotor experience that structure human cognition.\nBy customizing LLMs to translate natural language descriptions into formal\nrepresentations based on these sensorimotor patterns, we will be able to create\na neurosymbolic system that grounds the agent's understanding in fundamental\nconceptual structures. We argue that such an approach enhances both efficiency\nand interpretability while enabling more intuitive human-agent interactions\nthrough shared embodied understanding.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24110v1",
    "published_date": "2025-03-31 14:01:39 UTC",
    "updated_date": "2025-03-31 14:01:39 UTC"
  },
  {
    "arxiv_id": "2503.24108v1",
    "title": "PolypSegTrack: Unified Foundation Model for Colonoscopy Video Analysis",
    "authors": [
      "Anwesa Choudhuri",
      "Zhongpai Gao",
      "Meng Zheng",
      "Benjamin Planche",
      "Terrence Chen",
      "Ziyan Wu"
    ],
    "abstract": "Early detection, accurate segmentation, classification and tracking of polyps\nduring colonoscopy are critical for preventing colorectal cancer. Many existing\ndeep-learning-based methods for analyzing colonoscopic videos either require\ntask-specific fine-tuning, lack tracking capabilities, or rely on\ndomain-specific pre-training. In this paper, we introduce\n\\textit{PolypSegTrack}, a novel foundation model that jointly addresses polyp\ndetection, segmentation, classification and unsupervised tracking in\ncolonoscopic videos. Our approach leverages a novel conditional mask loss,\nenabling flexible training across datasets with either pixel-level segmentation\nmasks or bounding box annotations, allowing us to bypass task-specific\nfine-tuning. Our unsupervised tracking module reliably associates polyp\ninstances across frames using object queries, without relying on any\nheuristics. We leverage a robust vision foundation model backbone that is\npre-trained unsupervisedly on natural images, thereby removing the need for\ndomain-specific pre-training. Extensive experiments on multiple polyp\nbenchmarks demonstrate that our method significantly outperforms existing\nstate-of-the-art approaches in detection, segmentation, classification, and\ntracking.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24108v1",
    "published_date": "2025-03-31 14:00:21 UTC",
    "updated_date": "2025-03-31 14:00:21 UTC"
  },
  {
    "arxiv_id": "2503.24062v1",
    "title": "Artificial Conversations, Real Results: Fostering Language Detection with Synthetic Data",
    "authors": [
      "Fatemeh Mohammadi",
      "Tommaso Romano",
      "Samira Maghool",
      "Paolo Ceravolo"
    ],
    "abstract": "Collecting high-quality training data is essential for fine-tuning Large\nLanguage Models (LLMs). However, acquiring such data is often costly and\ntime-consuming, especially for non-English languages such as Italian. Recently,\nresearchers have begun to explore the use of LLMs to generate synthetic\ndatasets as a viable alternative. This study proposes a pipeline for generating\nsynthetic data and a comprehensive approach for investigating the factors that\ninfluence the validity of synthetic data generated by LLMs by examining how\nmodel performance is affected by metrics such as prompt strategy, text length\nand target position in a specific task, i.e. inclusive language detection in\nItalian job advertisements. Our results show that, in most cases and across\ndifferent metrics, the fine-tuned models trained on synthetic data consistently\noutperformed other models on both real and synthetic test datasets. The study\ndiscusses the practical implications and limitations of using synthetic data\nfor language detection tasks with LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24062v1",
    "published_date": "2025-03-31 13:22:34 UTC",
    "updated_date": "2025-03-31 13:22:34 UTC"
  },
  {
    "arxiv_id": "2503.24047v1",
    "title": "Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents",
    "authors": [
      "Shuo Ren",
      "Pu Jian",
      "Zhenjiang Ren",
      "Chunlin Leng",
      "Can Xie",
      "Jiajun Zhang"
    ],
    "abstract": "As scientific research becomes increasingly complex, innovative tools are\nneeded to manage vast data, facilitate interdisciplinary collaboration, and\naccelerate discovery. Large language models (LLMs) are now evolving into\nLLM-based scientific agents that automate critical tasks, ranging from\nhypothesis generation and experiment design to data analysis and simulation.\nUnlike general-purpose LLMs, these specialized agents integrate domain-specific\nknowledge, advanced tool sets, and robust validation mechanisms, enabling them\nto handle complex data types, ensure reproducibility, and drive scientific\nbreakthroughs. This survey provides a focused review of the architectures,\ndesign, benchmarks, applications, and ethical considerations surrounding\nLLM-based scientific agents. We highlight why they differ from general agents\nand the ways in which they advance research across various scientific fields.\nBy examining their development and challenges, this survey offers a\ncomprehensive roadmap for researchers and practitioners to harness these agents\nfor more efficient, reliable, and ethically sound scientific discovery.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "comment": "34 pages, 10 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.24047v1",
    "published_date": "2025-03-31 13:11:28 UTC",
    "updated_date": "2025-03-31 13:11:28 UTC"
  },
  {
    "arxiv_id": "2503.24028v1",
    "title": "Pay More Attention to the Robustness of Prompt for Instruction Data Mining",
    "authors": [
      "Qiang Wang",
      "Dawei Feng",
      "Xu Zhang",
      "Ao Shen",
      "Yang Xu",
      "Bo Ding",
      "Huaimin Wang"
    ],
    "abstract": "Instruction tuning has emerged as a paramount method for tailoring the\nbehaviors of LLMs. Recent work has unveiled the potential for LLMs to achieve\nhigh performance through fine-tuning with a limited quantity of high-quality\ninstruction data. Building upon this approach, we further explore the impact of\nprompt's robustness on the selection of high-quality instruction data. This\npaper proposes a pioneering framework of high-quality online instruction data\nmining for instruction tuning, focusing on the impact of prompt's robustness on\nthe data mining process. Our notable innovation, is to generate the adversarial\ninstruction data by conducting the attack for the prompt of online instruction\ndata. Then, we introduce an Adversarial Instruction-Following Difficulty metric\nto measure how much help the adversarial instruction data can provide to the\ngeneration of the corresponding response. Apart from it, we propose a novel\nAdversarial Instruction Output Embedding Consistency approach to select\nhigh-quality online instruction data. We conduct extensive experiments on two\nbenchmark datasets to assess the performance. The experimental results serve to\nunderscore the effectiveness of our proposed two methods. Moreover, the results\nunderscore the critical practical significance of considering prompt's\nrobustness.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24028v1",
    "published_date": "2025-03-31 12:53:08 UTC",
    "updated_date": "2025-03-31 12:53:08 UTC"
  },
  {
    "arxiv_id": "2503.24016v1",
    "title": "Bayesian Predictive Coding",
    "authors": [
      "Alexander Tschantz",
      "Magnus Koudahl",
      "Hampus Linander",
      "Lancelot Da Costa",
      "Conor Heins",
      "Jeff Beck",
      "Christopher Buckley"
    ],
    "abstract": "Predictive coding (PC) is an influential theory of information processing in\nthe brain, providing a biologically plausible alternative to backpropagation.\nIt is motivated in terms of Bayesian inference, as hidden states and parameters\nare optimised via gradient descent on variational free energy. However,\nimplementations of PC rely on maximum \\textit{a posteriori} (MAP) estimates of\nhidden states and maximum likelihood (ML) estimates of parameters, limiting\ntheir ability to quantify epistemic uncertainty. In this work, we investigate a\nBayesian extension to PC that estimates a posterior distribution over network\nparameters. This approach, termed Bayesian Predictive coding (BPC), preserves\nthe locality of PC and results in closed-form Hebbian weight updates. Compared\nto PC, our BPC algorithm converges in fewer epochs in the full-batch setting\nand remains competitive in the mini-batch setting. Additionally, we demonstrate\nthat BPC offers uncertainty quantification comparable to existing methods in\nBayesian deep learning, while also improving convergence properties. Together,\nthese results suggest that BPC provides a biologically plausible method for\nBayesian learning in the brain, as well as an attractive approach to\nuncertainty quantification in deep learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24016v1",
    "published_date": "2025-03-31 12:40:50 UTC",
    "updated_date": "2025-03-31 12:40:50 UTC"
  },
  {
    "arxiv_id": "2503.24009v1",
    "title": "Learning 3D-Gaussian Simulators from RGB Videos",
    "authors": [
      "Mikel Zhobro",
      "Andreas RenÃ© Geist",
      "Georg Martius"
    ],
    "abstract": "Learning physics simulations from video data requires maintaining spatial and\ntemporal consistency, a challenge often addressed with strong inductive biases\nor ground-truth 3D information -- limiting scalability and generalization. We\nintroduce 3DGSim, a 3D physics simulator that learns object dynamics end-to-end\nfrom multi-view RGB videos. It encodes images into a 3D Gaussian particle\nrepresentation, propagates dynamics via a transformer, and renders frames using\n3D Gaussian splatting. By jointly training inverse rendering with a dynamics\ntransformer using a temporal encoding and merging layer, 3DGSimembeds physical\nproperties into point-wise latent vectors without enforcing explicit\nconnectivity constraints. This enables the model to capture diverse physical\nbehaviors, from rigid to elastic and cloth-like interactions, along with\nrealistic lighting effects that also generalize to unseen multi-body\ninteractions and novel scene edits.",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.GR",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24009v1",
    "published_date": "2025-03-31 12:33:59 UTC",
    "updated_date": "2025-03-31 12:33:59 UTC"
  },
  {
    "arxiv_id": "2503.24008v1",
    "title": "H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding",
    "authors": [
      "Qi Wu",
      "Quanlong Zheng",
      "Yanhao Zhang",
      "Junlin Xie",
      "Jinguo Luo",
      "Kuo Wang",
      "Peng Liu",
      "Qingsong Xie",
      "Ru Zhen",
      "Haonan Lu",
      "Zhenyu Yang"
    ],
    "abstract": "With the rapid development of multimodal models, the demand for assessing\nvideo understanding capabilities has been steadily increasing. However,\nexisting benchmarks for evaluating video understanding exhibit significant\nlimitations in coverage, task diversity, and scene adaptability. These\nshortcomings hinder the accurate assessment of models' comprehensive video\nunderstanding capabilities. To tackle this challenge, we propose a hierarchical\nand holistic video understanding (H2VU) benchmark designed to evaluate both\ngeneral video and online streaming video comprehension. This benchmark\ncontributes three key features:\n  Extended video duration: Spanning videos from brief 3-second clips to\ncomprehensive 1.5-hour recordings, thereby bridging the temporal gaps found in\ncurrent benchmarks. Comprehensive assessment tasks: Beyond traditional\nperceptual and reasoning tasks, we have introduced modules for\ncountercommonsense comprehension and trajectory state tracking. These additions\ntest the models' deep understanding capabilities beyond mere prior knowledge.\nEnriched video data: To keep pace with the rapid evolution of current AI\nagents, we have expanded first-person streaming video datasets. This expansion\nallows for the exploration of multimodal models' performance in understanding\nstreaming videos from a first-person perspective. Extensive results from H2VU\nreveal that existing multimodal large language models (MLLMs) possess\nsubstantial potential for improvement in our newly proposed evaluation tasks.\nWe expect that H2VU will facilitate advancements in video understanding\nresearch by offering a comprehensive and in-depth analysis of MLLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24008v1",
    "published_date": "2025-03-31 12:32:51 UTC",
    "updated_date": "2025-03-31 12:32:51 UTC"
  },
  {
    "arxiv_id": "2503.24007v1",
    "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
    "authors": [
      "Yosuke Yamaguchi",
      "Issei Suemitsu",
      "Wenpeng Wei"
    ],
    "abstract": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.24007v1",
    "published_date": "2025-03-31 12:32:23 UTC",
    "updated_date": "2025-03-31 12:32:23 UTC"
  },
  {
    "arxiv_id": "2503.24000v1",
    "title": "Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving",
    "authors": [
      "Wei Gao",
      "Xinyu Zhou",
      "Peng Sun",
      "Tianwei Zhang",
      "Yonggang Wen"
    ],
    "abstract": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "21 pages, 18 figures, published to MLSys2025",
    "pdf_url": "http://arxiv.org/pdf/2503.24000v1",
    "published_date": "2025-03-31 12:23:31 UTC",
    "updated_date": "2025-03-31 12:23:31 UTC"
  },
  {
    "arxiv_id": "2503.23993v1",
    "title": "DenseFormer: Learning Dense Depth Map from Sparse Depth and Image via Conditional Diffusion Model",
    "authors": [
      "Ming Yuan",
      "Sichao Wang",
      "Chuang Zhang",
      "Lei He",
      "Qing Xu",
      "Jianqiang Wang"
    ],
    "abstract": "The depth completion task is a critical problem in autonomous driving,\ninvolving the generation of dense depth maps from sparse depth maps and RGB\nimages. Most existing methods employ a spatial propagation network to\niteratively refine the depth map after obtaining an initial dense depth. In\nthis paper, we propose DenseFormer, a novel method that integrates the\ndiffusion model into the depth completion task. By incorporating the denoising\nmechanism of the diffusion model, DenseFormer generates the dense depth map by\nprogressively refining an initial random depth distribution through multiple\niterations. We propose a feature extraction module that leverages a feature\npyramid structure, along with multi-layer deformable attention, to effectively\nextract and integrate features from sparse depth maps and RGB images, which\nserve as the guiding condition for the diffusion process. Additionally, this\npaper presents a depth refinement module that applies multi-step iterative\nrefinement across various ranges to the dense depth results generated by the\ndiffusion process. The module utilizes image features enriched with multi-scale\ninformation and sparse depth input to further enhance the accuracy of the\npredicted depth map. Extensive experiments on the KITTI outdoor scene dataset\ndemonstrate that DenseFormer outperforms classical depth completion methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23993v1",
    "published_date": "2025-03-31 12:11:01 UTC",
    "updated_date": "2025-03-31 12:11:01 UTC"
  },
  {
    "arxiv_id": "2503.23989v1",
    "title": "Rubric Is All You Need: Enhancing LLM-based Code Evaluation With Question-Specific Rubrics",
    "authors": [
      "Aditya Pathak",
      "Rachit Gandhi",
      "Vaibhav Uttam",
      "Devansh",
      "Yashwanth Nakka",
      "Aaryan Raj Jindal",
      "Pratyush Ghosh",
      "Arnav Ramamoorthy",
      "Shreyash Verma",
      "Aditya Mittal",
      "Aashna Ased",
      "Chirag Khatri",
      "Jagat Sesh Challa",
      "Dhruv Kumar"
    ],
    "abstract": "Since the disruption in LLM technology brought about by the release of GPT-3\nand ChatGPT, LLMs have shown remarkable promise in programming-related tasks.\nWhile code generation remains a popular field of research, code evaluation\nusing LLMs remains a problem with no conclusive solution. In this paper, we\nfocus on LLM-based code evaluation and attempt to fill in the existing gaps. We\npropose multi-agentic novel approaches using question-specific rubrics tailored\nto the problem statement, arguing that these perform better for logical\nassessment than the existing approaches that use question-agnostic rubrics. To\naddress the lack of suitable evaluation datasets, we introduce two datasets: a\nData Structures and Algorithms dataset containing 150 student submissions from\na popular Data Structures and Algorithms practice website, and an Object\nOriented Programming dataset comprising 80 student submissions from\nundergraduate computer science courses. In addition to using standard metrics\n(Spearman Correlation, Cohen's Kappa), we additionally propose a new metric\ncalled as Leniency, which quantifies evaluation strictness relative to expert\nassessment. Our comprehensive analysis demonstrates that question-specific\nrubrics significantly enhance logical assessment of code in educational\nsettings, providing better feedback aligned with instructional goals beyond\nmere syntactic correctness.",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "Under Review",
    "pdf_url": "http://arxiv.org/pdf/2503.23989v1",
    "published_date": "2025-03-31 11:59:43 UTC",
    "updated_date": "2025-03-31 11:59:43 UTC"
  },
  {
    "arxiv_id": "2503.23988v1",
    "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an Exploratory Study Using Low Computing Power Environments",
    "authors": [
      "Elayne Lemos",
      "Rodrigo Oliveira",
      "Jairson Rodrigues",
      "Rosalvo F. Oliveira Neto"
    ],
    "abstract": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.PF",
      "68T07, 68U01",
      "C.4; I.2.0; B.8.2"
    ],
    "primary_category": "cs.DC",
    "comment": "15 pages, 7 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.23988v1",
    "published_date": "2025-03-31 11:58:37 UTC",
    "updated_date": "2025-03-31 11:58:37 UTC"
  },
  {
    "arxiv_id": "2503.23982v1",
    "title": "Deep Nets as Hamiltonians",
    "authors": [
      "Mike Winer",
      "Boris Hanin"
    ],
    "abstract": "Neural networks are complex functions of both their inputs and parameters.\nMuch prior work in deep learning theory analyzes the distribution of network\noutputs at a fixed a set of inputs (e.g. a training dataset) over random\ninitializations of the network parameters. The purpose of this article is to\nconsider the opposite situation: we view a randomly initialized Multi-Layer\nPerceptron (MLP) as a Hamiltonian over its inputs. For typical realizations of\nthe network parameters, we study the properties of the energy landscape induced\nby this Hamiltonian, focusing on the structure of near-global minimum in the\nlimit of infinite width. Specifically, we use the replica trick to perform an\nexact analytic calculation giving the entropy (log volume of space) at a given\nenergy. We further derive saddle point equations that describe the overlaps\nbetween inputs sampled iid from the Gibbs distribution induced by the random\nMLP. For linear activations we solve these saddle point equations exactly. But\nwe also solve them numerically for a variety of depths and activation\nfunctions, including $\\tanh, \\sin, \\text{ReLU}$, and shaped non-linearities. We\nfind even at infinite width a rich range of behaviors. For some\nnon-linearities, such as $\\sin$, for instance, we find that the landscapes of\nrandom MLPs exhibit full replica symmetry breaking, while shallow $\\tanh$ and\nReLU networks or deep shaped MLPs are instead replica symmetric.",
    "categories": [
      "cond-mat.dis-nn",
      "cond-mat.stat-mech",
      "cs.AI",
      "cs.LG",
      "math.PR"
    ],
    "primary_category": "cond-mat.dis-nn",
    "comment": "19+7 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.23982v1",
    "published_date": "2025-03-31 11:51:10 UTC",
    "updated_date": "2025-03-31 11:51:10 UTC"
  },
  {
    "arxiv_id": "2503.23972v1",
    "title": "Noise-based reward-modulated learning",
    "authors": [
      "JesÃºs GarcÃ­a FernÃ¡ndez",
      "Nasir Ahmad",
      "Marcel van Gerven"
    ],
    "abstract": "Recent advances in reinforcement learning (RL) have led to significant\nimprovements in task performance. However, training neural networks in an RL\nregime is typically achieved in combination with backpropagation, limiting\ntheir applicability in resource-constrained environments or when using\nnon-differentiable neural networks. While noise-based alternatives like\nreward-modulated Hebbian learning (RMHL) have been proposed, their performance\nhas remained limited, especially in scenarios with delayed rewards, which\nrequire retrospective credit assignment over time. Here, we derive a novel\nnoise-based learning rule that addresses these challenges. Our approach\ncombines directional derivative theory with Hebbian-like updates to enable\nefficient, gradient-free learning in RL. It features stochastic noisy neurons\nwhich can approximate gradients, and produces local synaptic updates modulated\nby a global reward signal. Drawing on concepts from neuroscience, our method\nuses reward prediction error as its optimization target to generate\nincreasingly advantageous behavior, and incorporates an eligibility trace to\nfacilitate temporal credit assignment in environments with delayed rewards. Its\nformulation relies on local information alone, making it compatible with\nimplementations in neuromorphic hardware. Experimental validation shows that\nour approach significantly outperforms RMHL and is competitive with BP-based\nbaselines, highlighting the promise of noise-based, biologically inspired\nlearning for low-power and real-time applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23972v1",
    "published_date": "2025-03-31 11:35:23 UTC",
    "updated_date": "2025-03-31 11:35:23 UTC"
  },
  {
    "arxiv_id": "2503.23956v1",
    "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference",
    "authors": [
      "Kai Huang",
      "Hao Zou",
      "Bochen Wang",
      "Ye Xi",
      "Zhen Xie",
      "Hao Wang"
    ],
    "abstract": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23956v1",
    "published_date": "2025-03-31 11:13:18 UTC",
    "updated_date": "2025-03-31 11:13:18 UTC"
  },
  {
    "arxiv_id": "2503.23948v1",
    "title": "AI2Agent: An End-to-End Framework for Deploying AI Projects as Autonomous Agents",
    "authors": [
      "Jiaxiang Chen",
      "Jingwei Shi",
      "Lei Gan",
      "Jiale Zhang",
      "Qingyu Zhang",
      "Dongqian Zhang",
      "Xin Pang",
      "Zhucong Li",
      "Yinghui Xu"
    ],
    "abstract": "As AI technology advances, it is driving innovation across industries,\nincreasing the demand for scalable AI project deployment. However, deployment\nremains a critical challenge due to complex environment configurations,\ndependency conflicts, cross-platform adaptation, and debugging difficulties,\nwhich hinder automation and adoption. This paper introduces AI2Agent, an\nend-to-end framework that automates AI project deployment through\nguideline-driven execution, self-adaptive debugging, and case \\& solution\naccumulation. AI2Agent dynamically analyzes deployment challenges, learns from\npast cases, and iteratively refines its approach, significantly reducing human\nintervention. To evaluate its effectiveness, we conducted experiments on 30 AI\ndeployment cases, covering TTS, text-to-image generation, image editing, and\nother AI applications. Results show that AI2Agent significantly reduces\ndeployment time and improves success rates. The code and demo video are now\npublicly accessible.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23948v1",
    "published_date": "2025-03-31 10:58:34 UTC",
    "updated_date": "2025-03-31 10:58:34 UTC"
  },
  {
    "arxiv_id": "2503.23934v1",
    "title": "Green MLOps to Green GenOps: An Empirical Study of Energy Consumption in Discriminative and Generative AI Operations",
    "authors": [
      "AdriÃ¡n SÃ¡nchez-MompÃ³",
      "Ioannis Mavromatis",
      "Peizheng Li",
      "Konstantinos Katsaros",
      "Aftab Khan"
    ],
    "abstract": "This study presents an empirical investigation into the energy consumption of\nDiscriminative and Generative AI models within real-world MLOps pipelines. For\nDiscriminative models, we examine various architectures and hyperparameters\nduring training and inference and identify energy-efficient practices. For\nGenerative AI, Large Language Models (LLMs) are assessed, focusing primarily on\nenergy consumption across different model sizes and varying service requests.\nOur study employs software-based power measurements, ensuring ease of\nreplication across diverse configurations, models, and datasets. We analyse\nmultiple models and hardware setups to uncover correlations among various\nmetrics, identifying key contributors to energy consumption. The results\nindicate that for Discriminative models, optimising architectures,\nhyperparameters, and hardware can significantly reduce energy consumption\nwithout sacrificing performance. For LLMs, energy efficiency depends on\nbalancing model size, reasoning complexity, and request-handling capacity, as\nlarger models do not necessarily consume more energy when utilisation remains\nlow. This analysis provides practical guidelines for designing green and\nsustainable ML operations, emphasising energy consumption and carbon footprint\nreductions while maintaining performance. This paper can serve as a benchmark\nfor accurately estimating total energy use across different types of AI models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "Published to MDPI Information - Artificial Intelligence Section",
    "pdf_url": "http://arxiv.org/pdf/2503.23934v1",
    "published_date": "2025-03-31 10:28:04 UTC",
    "updated_date": "2025-03-31 10:28:04 UTC"
  },
  {
    "arxiv_id": "2503.23923v1",
    "title": "What the F*ck Is Artificial General Intelligence?",
    "authors": [
      "Michael Timothy Bennett"
    ],
    "abstract": "Artificial general intelligence (AGI) is an established field of research.\nYet Melanie Mitchell and others have questioned if the term still has meaning.\nAGI has been subject to so much hype and speculation it has become something of\na Rorschach test. Mitchell points out that the debate will only be settled\nthrough long term, scientific investigation. To that end here is a short,\naccessible and provocative overview of AGI. I compare definitions of\nintelligence, settling on intelligence in terms of adaptation and AGI as an\nartificial scientist. Taking my queue from Sutton's Bitter Lesson I describe\ntwo foundational tools used to build adaptive systems: search and\napproximation. I compare pros, cons, hybrids and architectures like o3,\nAlphaGo, AERA, NARS and Hyperon. I then discuss overall meta-approaches to\nmaking systems behave more intelligently. I divide them into scale-maxing,\nsimp-maxing, w-maxing based on the Bitter Lesson, Ockham's and Bennett's\nRazors. These maximise resources, simplicity of form, and the weakness of\nconstraints on functionality. I discuss examples including AIXI, the free\nenergy principle and The Embiggening of language models. I conclude that though\nscale-maxed approximation dominates, AGI will be a fusion of tools and\nmeta-approaches. The Embiggening was enabled by improvements in hardware. Now\nthe bottlenecks are sample and energy efficiency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "Preprint; 10 pages;",
    "pdf_url": "http://arxiv.org/pdf/2503.23923v1",
    "published_date": "2025-03-31 10:15:37 UTC",
    "updated_date": "2025-03-31 10:15:37 UTC"
  },
  {
    "arxiv_id": "2503.23907v1",
    "title": "HumanAesExpert: Advancing a Multi-Modality Foundation Model for Human Image Aesthetic Assessment",
    "authors": [
      "Zhichao Liao",
      "Xiaokun Liu",
      "Wenyu Qin",
      "Qingyu Li",
      "Qiulin Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Long Zeng",
      "Pingfa Feng"
    ],
    "abstract": "Image Aesthetic Assessment (IAA) is a long-standing and challenging research\ntask. However, its subset, Human Image Aesthetic Assessment (HIAA), has been\nscarcely explored, even though HIAA is widely used in social media, AI\nworkflows, and related domains. To bridge this research gap, our work pioneers\na holistic implementation framework tailored for HIAA. Specifically, we\nintroduce HumanBeauty, the first dataset purpose-built for HIAA, which\ncomprises 108k high-quality human images with manual annotations. To achieve\ncomprehensive and fine-grained HIAA, 50K human images are manually collected\nthrough a rigorous curation process and annotated leveraging our trailblazing\n12-dimensional aesthetic standard, while the remaining 58K with overall\naesthetic labels are systematically filtered from public datasets. Based on the\nHumanBeauty database, we propose HumanAesExpert, a powerful Vision Language\nModel for aesthetic evaluation of human images. We innovatively design an\nExpert head to incorporate human knowledge of aesthetic sub-dimensions while\njointly utilizing the Language Modeling (LM) and Regression head. This approach\nempowers our model to achieve superior proficiency in both overall and\nfine-grained HIAA. Furthermore, we introduce a MetaVoter, which aggregates\nscores from all three heads, to effectively balance the capabilities of each\nhead, thereby realizing improved assessment precision. Extensive experiments\ndemonstrate that our HumanAesExpert models deliver significantly better\nperformance in HIAA than other state-of-the-art models. Our datasets, models,\nand codes are publicly released to advance the HIAA community. Project webpage:\nhttps://humanaesexpert.github.io/HumanAesExpert/",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23907v1",
    "published_date": "2025-03-31 09:58:11 UTC",
    "updated_date": "2025-03-31 09:58:11 UTC"
  },
  {
    "arxiv_id": "2503.23897v1",
    "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
    "authors": [
      "Yufei Wang",
      "Lanqing Guo",
      "Zhihao Li",
      "Jiaxing Huang",
      "Pichao Wang",
      "Bihan Wen",
      "Jian Wang"
    ],
    "abstract": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23897v1",
    "published_date": "2025-03-31 09:46:56 UTC",
    "updated_date": "2025-03-31 09:46:56 UTC"
  },
  {
    "arxiv_id": "2503.23895v1",
    "title": "Better wit than wealth: Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement",
    "authors": [
      "Yuqiao Tan",
      "Shizhu He",
      "Huanxuan Liao",
      "Jun Zhao",
      "Kang Liu"
    ],
    "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving relevant documents from external sources and incorporating them into\nthe context. While it improves reliability by providing factual texts, it\nsignificantly increases inference costs as context length grows and introduces\nchallenging issue of RAG hallucination, primarily caused by the lack of\ncorresponding parametric knowledge in LLMs. An efficient solution is to enhance\nthe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by\nembedding document into LLMs parameters to perform test-time knowledge\nenhancement, effectively reducing inference costs through offline training.\nHowever, its high training and storage costs, along with limited generalization\nability, significantly restrict its practical adoption. To address these\nchallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that\nleverages a lightweight parameter translator model to efficiently convert\ndocuments into parametric knowledge. DyPRAG not only reduces inference,\ntraining, and storage costs but also dynamically generates parametric\nknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge\nconflicts in a plug-and-play manner at test-time. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and generalization capabilities\nof DyPRAG, offering a powerful and practical RAG paradigm which enables\nsuperior knowledge fusion and mitigates RAG hallucination in real-world\napplications. Our code is available at https://github.com/Trae1ounG/DyPRAG.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "preprint",
    "pdf_url": "http://arxiv.org/pdf/2503.23895v1",
    "published_date": "2025-03-31 09:46:35 UTC",
    "updated_date": "2025-03-31 09:46:35 UTC"
  },
  {
    "arxiv_id": "2503.23893v1",
    "title": "DiffScale: Continuous Downscaling and Bias Correction of Subseasonal Wind Speed Forecasts using Diffusion Models",
    "authors": [
      "Maximilian Springenberg",
      "Noelia Otero",
      "Yuxin Xue",
      "Jackie Ma"
    ],
    "abstract": "Renewable resources are strongly dependent on local and large-scale weather\nsituations. Skillful subseasonal to seasonal (S2S) forecasts -- beyond two\nweeks and up to two months -- can offer significant socioeconomic advantages to\nthe energy sector. This study aims to enhance wind speed predictions using a\ndiffusion model with classifier-free guidance to downscale S2S forecasts of\nsurface wind speed. We propose DiffScale, a diffusion model that super-resolves\nspatial information for continuous downscaling factors and lead times.\nLeveraging weather priors as guidance for the generative process of diffusion\nmodels, we adopt the perspective of conditional probabilities on sampling\nsuper-resolved S2S forecasts. We aim to directly estimate the density\nassociated with the target S2S forecasts at different spatial resolutions and\nlead times without auto-regression or sequence prediction, resulting in an\nefficient and flexible model. Synthetic experiments were designed to\nsuper-resolve wind speed S2S forecasts from the European Center for\nMedium-Range Weather Forecast (ECMWF) from a coarse resolution to a finer\nresolution of ERA5 reanalysis data, which serves as a high-resolution target.\nThe innovative aspect of DiffScale lies in its flexibility to downscale\narbitrary scaling factors, enabling it to generalize across various grid\nresolutions and lead times -without retraining the model- while correcting\nmodel errors, making it a versatile tool for improving S2S wind speed\nforecasts. We achieve a significant improvement in prediction quality,\noutperforming baselines up to week 3.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "comment": "28 pages, 18 figures, preprint under review",
    "pdf_url": "http://arxiv.org/pdf/2503.23893v1",
    "published_date": "2025-03-31 09:44:28 UTC",
    "updated_date": "2025-03-31 09:44:28 UTC"
  },
  {
    "arxiv_id": "2503.23888v1",
    "title": "MuseFace: Text-driven Face Editing via Diffusion-based Mask Generation Approach",
    "authors": [
      "Xin Zhang",
      "Siting Huang",
      "Xiangyang Luo",
      "Yifan Xie",
      "Weijiang Yu",
      "Heng Chang",
      "Fei Ma",
      "Fei Yu"
    ],
    "abstract": "Face editing modifies the appearance of face, which plays a key role in\ncustomization and enhancement of personal images. Although much work have\nachieved remarkable success in text-driven face editing, they still face\nsignificant challenges as none of them simultaneously fulfill the\ncharacteristics of diversity, controllability and flexibility. To address this\nchallenge, we propose MuseFace, a text-driven face editing framework, which\nrelies solely on text prompt to enable face editing. Specifically, MuseFace\nintegrates a Text-to-Mask diffusion model and a semantic-aware face editing\nmodel, capable of directly generating fine-grained semantic masks from text and\nperforming face editing. The Text-to-Mask diffusion model provides\n\\textit{diversity} and \\textit{flexibility} to the framework, while the\nsemantic-aware face editing model ensures \\textit{controllability} of the\nframework. Our framework can create fine-grained semantic masks, making precise\nface editing possible, and significantly enhancing the controllability and\nflexibility of face editing models. Extensive experiments demonstrate that\nMuseFace achieves superior high-fidelity performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "6 pages, 5 figures,IEEE International Conference on Multimedia & Expo\n  2025",
    "pdf_url": "http://arxiv.org/pdf/2503.23888v1",
    "published_date": "2025-03-31 09:41:09 UTC",
    "updated_date": "2025-03-31 09:41:09 UTC"
  },
  {
    "arxiv_id": "2503.23886v1",
    "title": "SchemaAgent: A Multi-Agents Framework for Generating Relational Database Schema",
    "authors": [
      "Qin Wang",
      "Youhuan Li",
      "Yansong Feng",
      "Si Chen",
      "Ziming Li",
      "Pan Zhang",
      "Zhichao Shi",
      "Yuequn Dou",
      "chuchu Gao",
      "Zebin Huang",
      "Zihui Si",
      "Yixuan Chen",
      "Zhaohai Sun",
      "Ke Tang",
      "Wenqiang Jin"
    ],
    "abstract": "The relational database design would output a schema based on user's\nrequirements, which defines table structures and their interrelated relations.\nTranslating requirements into accurate schema involves several non-trivial\nsubtasks demanding both database expertise and domain-specific knowledge. This\nposes unique challenges for automated design of relational databases. Existing\nefforts are mostly based on customized rules or conventional deep learning\nmodels, often producing suboptimal schema. Recently, large language models\n(LLMs) have significantly advanced intelligent application development across\nvarious domains. In this paper, we propose SchemaAgent, a unified LLM-based\nmulti-agent framework for the automated generation of high-quality database\nschema. SchemaAgent is the first to apply LLMs for schema generation, which\nemulates the workflow of manual schema design by assigning specialized roles to\nagents and enabling effective collaboration to refine their respective\nsubtasks. Schema generation is a streamlined workflow, where directly applying\nthe multi-agent framework may cause compounding impact of errors. To address\nthis, we incorporate dedicated roles for reflection and inspection, alongside\nan innovative error detection and correction mechanism to identify and rectify\nissues across various phases. For evaluation, we present a benchmark named\n\\textit{RSchema}, which contains more than 500 pairs of requirement description\nand schema. Experimental results on this benchmark demonstrate the superiority\nof our approach over mainstream LLMs for relational database schema generation.",
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "primary_category": "cs.DB",
    "comment": "19 pages, 16 figures",
    "pdf_url": "http://arxiv.org/pdf/2503.23886v1",
    "published_date": "2025-03-31 09:39:19 UTC",
    "updated_date": "2025-03-31 09:39:19 UTC"
  },
  {
    "arxiv_id": "2503.23875v1",
    "title": "GenSwarm: Scalable Multi-Robot Code-Policy Generation and Deployment via Language Models",
    "authors": [
      "Wenkang Ji",
      "Huaben Chen",
      "Mingyang Chen",
      "Guobin Zhu",
      "Lufeng Xu",
      "Roderich GroÃ",
      "Rui Zhou",
      "Ming Cao",
      "Shiyu Zhao"
    ],
    "abstract": "The development of control policies for multi-robot systems traditionally\nfollows a complex and labor-intensive process, often lacking the flexibility to\nadapt to dynamic tasks. This has motivated research on methods to automatically\ncreate control policies. However, these methods require iterative processes of\nmanually crafting and refining objective functions, thereby prolonging the\ndevelopment cycle. This work introduces \\textit{GenSwarm}, an end-to-end system\nthat leverages large language models to automatically generate and deploy\ncontrol policies for multi-robot tasks based on simple user instructions in\nnatural language. As a multi-language-agent system, GenSwarm achieves zero-shot\nlearning, enabling rapid adaptation to altered or unseen tasks. The white-box\nnature of the code policies ensures strong reproducibility and\ninterpretability. With its scalable software and hardware architectures,\nGenSwarm supports efficient policy deployment on both simulated and real-world\nmulti-robot systems, realizing an instruction-to-execution end-to-end\nfunctionality that could prove valuable for robotics specialists and\nnon-specialists alike.The code of the proposed GenSwarm system is available\nonline: https://github.com/WindyLab/GenSwarm.",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23875v1",
    "published_date": "2025-03-31 09:26:34 UTC",
    "updated_date": "2025-03-31 09:26:34 UTC"
  },
  {
    "arxiv_id": "2503.23862v2",
    "title": "Learned Image Compression and Restoration for Digital Pathology",
    "authors": [
      "SeonYeong Lee",
      "EonSeung Seong",
      "DongEon Lee",
      "SiYeoul Lee",
      "Yubin Cho",
      "Chunsu Park",
      "Seonho Kim",
      "MinKyung Seo",
      "YoungSin Ko",
      "MinWoo Kim"
    ],
    "abstract": "Digital pathology images play a crucial role in medical diagnostics, but\ntheir ultra-high resolution and large file sizes pose significant challenges\nfor storage, transmission, and real-time visualization. To address these\nissues, we propose CLERIC, a novel deep learning-based image compression\nframework designed specifically for whole slide images (WSIs). CLERIC\nintegrates a learnable lifting scheme and advanced convolutional techniques to\nenhance compression efficiency while preserving critical pathological details.\nOur framework employs a lifting-scheme transform in the analysis stage to\ndecompose images into low- and high-frequency components, enabling more\nstructured latent representations. These components are processed through\nparallel encoders incorporating Deformable Residual Blocks (DRB) and Recurrent\nResidual Blocks (R2B) to improve feature extraction and spatial adaptability.\nThe synthesis stage applies an inverse lifting transform for effective image\nreconstruction, ensuring high-fidelity restoration of fine-grained tissue\nstructures. We evaluate CLERIC on a digital pathology image dataset and compare\nits performance against state-of-the-art learned image compression (LIC)\nmodels. Experimental results demonstrate that CLERIC achieves superior\nrate-distortion (RD) performance, significantly reducing storage requirements\nwhile maintaining high diagnostic image quality. Our study highlights the\npotential of deep learning-based compression in digital pathology, facilitating\nefficient data management and long-term storage while ensuring seamless\nintegration into clinical workflows and AI-assisted diagnostic systems. Code\nand models are available at: https://github.com/pnu-amilab/CLERIC.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23862v2",
    "published_date": "2025-03-31 09:09:09 UTC",
    "updated_date": "2025-04-01 03:06:51 UTC"
  },
  {
    "arxiv_id": "2503.23830v1",
    "title": "OrchMLLM: Orchestrate Multimodal Data with Batch Post-Balancing to Accelerate Multimodal Large Language Model Training",
    "authors": [
      "Yijie Zheng",
      "Bangjun Xiao",
      "Lei Shi",
      "Xiaoyang Li",
      "Faming Wu",
      "Tianyu Li",
      "Xuefeng Xiao",
      "Yang Zhang",
      "Yuxuan Wang",
      "Shouda Liu"
    ],
    "abstract": "Multimodal large language models (MLLMs), such as GPT-4o, are garnering\nsignificant attention. During the exploration of MLLM training, we identified\nModality Composition Incoherence, a phenomenon that the proportion of a certain\nmodality varies dramatically across different examples. It exacerbates the\nchallenges of addressing mini-batch imbalances, which lead to uneven GPU\nutilization between Data Parallel (DP) instances and severely degrades the\nefficiency and scalability of MLLM training, ultimately affecting training\nspeed and hindering further research on MLLMs.\n  To address these challenges, we introduce OrchMLLM, a comprehensive framework\ndesigned to mitigate the inefficiencies in MLLM training caused by Modality\nComposition Incoherence. First, we propose Batch Post-Balancing Dispatcher, a\ntechnique that efficiently eliminates mini-batch imbalances in sequential data.\nAdditionally, we integrate MLLM Global Orchestrator into the training framework\nto orchestrate multimodal data and tackle the issues arising from Modality\nComposition Incoherence. We evaluate OrchMLLM across various MLLM sizes,\ndemonstrating its efficiency and scalability. Experimental results reveal that\nOrchMLLM achieves a Model FLOPs Utilization (MFU) of $41.6\\%$ when training an\n84B MLLM with three modalities on $2560$ H100 GPUs, outperforming Megatron-LM\nby up to $3.1\\times$ in throughput.",
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "primary_category": "cs.DC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23830v1",
    "published_date": "2025-03-31 08:24:23 UTC",
    "updated_date": "2025-03-31 08:24:23 UTC"
  },
  {
    "arxiv_id": "2503.23820v2",
    "title": "When Counterfactual Reasoning Fails: Chaos and Real-World Complexity",
    "authors": [
      "Yahya Aalaila",
      "Gerrit GroÃmann",
      "Sumantrak Mukherjee",
      "Jonas Wahl",
      "Sebastian Vollmer"
    ],
    "abstract": "Counterfactual reasoning, a cornerstone of human cognition and\ndecision-making, is often seen as the 'holy grail' of causal learning, with\napplications ranging from interpreting machine learning models to promoting\nalgorithmic fairness. While counterfactual reasoning has been extensively\nstudied in contexts where the underlying causal model is well-defined,\nreal-world causal modeling is often hindered by model and parameter\nuncertainty, observational noise, and chaotic behavior. The reliability of\ncounterfactual analysis in such settings remains largely unexplored. In this\nwork, we investigate the limitations of counterfactual reasoning within the\nframework of Structural Causal Models. Specifically, we empirically investigate\n\\emph{counterfactual sequence estimation} and highlight cases where it becomes\nincreasingly unreliable. We find that realistic assumptions, such as low\ndegrees of model uncertainty or chaotic dynamics, can result in\ncounterintuitive outcomes, including dramatic deviations between predicted and\ntrue counterfactual trajectories. This work urges caution when applying\ncounterfactual reasoning in settings characterized by chaos and uncertainty.\nFurthermore, it raises the question of whether certain systems may pose\nfundamental limitations on the ability to answer counterfactual questions about\ntheir behavior.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23820v2",
    "published_date": "2025-03-31 08:14:51 UTC",
    "updated_date": "2025-04-01 08:57:37 UTC"
  },
  {
    "arxiv_id": "2503.23819v1",
    "title": "Conformal uncertainty quantification to evaluate predictive fairness of foundation AI model for skin lesion classes across patient demographics",
    "authors": [
      "Swarnava Bhattacharyya",
      "Umapada Pal",
      "Tapabrata Chakraborti"
    ],
    "abstract": "Deep learning based diagnostic AI systems based on medical images are\nstarting to provide similar performance as human experts. However these data\nhungry complex systems are inherently black boxes and therefore slow to be\nadopted for high risk applications like healthcare. This problem of lack of\ntransparency is exacerbated in the case of recent large foundation models,\nwhich are trained in a self supervised manner on millions of data points to\nprovide robust generalisation across a range of downstream tasks, but the\nembeddings generated from them happen through a process that is not\ninterpretable, and hence not easily trustable for clinical applications. To\naddress this timely issue, we deploy conformal analysis to quantify the\npredictive uncertainty of a vision transformer (ViT) based foundation model\nacross patient demographics with respect to sex, age and ethnicity for the\ntasks of skin lesion classification using several public benchmark datasets.\nThe significant advantage of this method is that conformal analysis is method\nindependent and it not only provides a coverage guarantee at population level\nbut also provides an uncertainty score for each individual. We used a\nmodel-agnostic dynamic F1-score-based sampling during model training, which\nhelped to stabilize the class imbalance and we investigate the effects on\nuncertainty quantification (UQ) with or without this bias mitigation step. Thus\nwe show how this can be used as a fairness metric to evaluate the robustness of\nthe feature embeddings of the foundation model (Google DermFoundation) and thus\nadvance the trustworthiness and fairness of clinical AI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23819v1",
    "published_date": "2025-03-31 08:06:00 UTC",
    "updated_date": "2025-03-31 08:06:00 UTC"
  },
  {
    "arxiv_id": "2503.23803v1",
    "title": "Thinking Longer, Not Larger: Enhancing Software Engineering Agents via Scaling Test-Time Compute",
    "authors": [
      "Yingwei Ma",
      "Binhua Li",
      "Yihong Dong",
      "Xue Jiang",
      "Rongyu Cao",
      "Jue Chen",
      "Fei Huang",
      "Yongbin Li"
    ],
    "abstract": "Recent advancements in software engineering agents have demonstrated\npromising capabilities in automating program improvements. However, their\nreliance on closed-source or resource-intensive models introduces significant\ndeployment challenges in private environments, prompting a critical question:\n\\textit{How can personally deployable open-source LLMs achieve comparable code\nreasoning performance?}\n  To this end, we propose a unified Test-Time Compute scaling framework that\nleverages increased inference-time computation instead of larger models. Our\nframework incorporates two complementary strategies: internal TTC and external\nTTC. Internally, we introduce a \\textit{development-contextualized trajectory\nsynthesis} method leveraging real-world software repositories to bootstrap\nmulti-stage reasoning processes, such as fault localization and patch\ngeneration. We further enhance trajectory quality through rejection sampling,\nrigorously evaluating trajectories along accuracy and complexity. Externally,\nwe propose a novel \\textit{development-process-based search} strategy guided by\nreward models and execution verification. This approach enables targeted\ncomputational allocation at critical development decision points, overcoming\nlimitations of existing \"end-point only\" verification methods.\n  Evaluations on SWE-bench Verified demonstrate our \\textbf{32B model achieves\na 46\\% issue resolution rate}, surpassing significantly larger models such as\nDeepSeek R1 671B and OpenAI o1. Additionally, we provide the empirical\nvalidation of the test-time scaling phenomenon within SWE agents, revealing\nthat \\textbf{models dynamically allocate more tokens to increasingly\nchallenging problems}, effectively enhancing reasoning capabilities. We\npublicly release all training data, models, and code to facilitate future\nresearch. https://github.com/yingweima2022/SWE-Reasoner",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "primary_category": "cs.SE",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23803v1",
    "published_date": "2025-03-31 07:31:32 UTC",
    "updated_date": "2025-03-31 07:31:32 UTC"
  },
  {
    "arxiv_id": "2503.23798v1",
    "title": "Adaptive Layer-skipping in Pre-trained LLMs",
    "authors": [
      "Xuan Luo",
      "Weizhi Wang",
      "Xifeng Yan"
    ],
    "abstract": "Various layer-skipping methods have been proposed to accelerate token\ngeneration in large language models (LLMs). However, they have overlooked a\nfundamental question: How do computational demands vary across the generation\nof different tokens? In this work, we introduce FlexiDepth, a method that\ndynamically adjusts the number of Transformer layers used in text generation.\nBy incorporating a plug-in router and adapter, FlexiDepth enables adaptive\nlayer-skipping in LLMs without modifying their original parameters. Introducing\nFlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32,\nand meanwhile maintains the full 100\\% benchmark performance. Experimental\nresults with FlexiDepth demonstrate that computational demands in LLMs\nsignificantly vary based on token type. Specifically, generating repetitive\ntokens or fixed phrases requires fewer layers, whereas producing tokens\ninvolving computation or high uncertainty requires more layers. Interestingly,\nthis adaptive allocation pattern aligns with human intuition. To advance\nresearch in this area, we open sourced FlexiDepth and a dataset documenting\nFlexiDepth's layer allocation patterns for future exploration.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23798v1",
    "published_date": "2025-03-31 07:20:58 UTC",
    "updated_date": "2025-03-31 07:20:58 UTC"
  },
  {
    "arxiv_id": "2503.23786v1",
    "title": "MGD-SAM2: Multi-view Guided Detail-enhanced Segment Anything Model 2 for High-Resolution Class-agnostic Segmentation",
    "authors": [
      "Haoran Shen",
      "Peixian Zhuang",
      "Jiahao Kou",
      "Yuxin Zeng",
      "Haoying Xu",
      "Jiangyun Li"
    ],
    "abstract": "Segment Anything Models (SAMs), as vision foundation models, have\ndemonstrated remarkable performance across various image analysis tasks.\nDespite their strong generalization capabilities, SAMs encounter challenges in\nfine-grained detail segmentation for high-resolution class-independent\nsegmentation (HRCS), due to the limitations in the direct processing of\nhigh-resolution inputs and low-resolution mask predictions, and the reliance on\naccurate manual prompts. To address these limitations, we propose MGD-SAM2\nwhich integrates SAM2 with multi-view feature interaction between a global\nimage and local patches to achieve precise segmentation. MGD-SAM2 incorporates\nthe pre-trained SAM2 with four novel modules: the Multi-view Perception Adapter\n(MPAdapter), the Multi-view Complementary Enhancement Module (MCEM), the\nHierarchical Multi-view Interaction Module (HMIM), and the Detail Refinement\nModule (DRM). Specifically, we first introduce MPAdapter to adapt the SAM2\nencoder for enhanced extraction of local details and global semantics in HRCS\nimages. Then, MCEM and HMIM are proposed to further exploit local texture and\nglobal context by aggregating multi-view features within and across\nmulti-scales. Finally, DRM is designed to generate gradually restored\nhigh-resolution mask predictions, compensating for the loss of fine-grained\ndetails resulting from directly upsampling the low-resolution prediction maps.\nExperimental results demonstrate the superior performance and strong\ngeneralization of our model on multiple high-resolution and normal-resolution\ndatasets. Code will be available at https://github.com/sevenshr/MGD-SAM2.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23786v1",
    "published_date": "2025-03-31 07:02:32 UTC",
    "updated_date": "2025-03-31 07:02:32 UTC"
  },
  {
    "arxiv_id": "2503.23781v1",
    "title": "DebFlow: Automating Agent Creation via Agent Debate",
    "authors": [
      "Jinwei Su",
      "Yinghui Xia",
      "Ronghua Shi",
      "Jianhui Wang",
      "Jianuo Huang",
      "Yijin Wang",
      "Tianyu Shi",
      "Yang Jingsong",
      "Lewei He"
    ],
    "abstract": "Large language models (LLMs) have demonstrated strong potential and\nimpressive performance in automating the generation and optimization of\nworkflows. However, existing approaches are marked by limited reasoning\ncapabilities, high computational demands, and significant resource\nrequirements. To address these issues, we propose DebFlow, a framework that\nemploys a debate mechanism to optimize workflows and integrates reflexion to\nimprove based on previous experiences. We evaluated our method across six\nbenchmark datasets, including HotpotQA, MATH, and ALFWorld. Our approach\nachieved a 3\\% average performance improvement over the latest baselines,\ndemonstrating its effectiveness in diverse problem domains. In particular,\nduring training, our framework reduces resource consumption by 37\\% compared to\nthe state-of-the-art baselines. Additionally, we performed ablation studies.\nRemoving the Debate component resulted in a 4\\% performance drop across two\nbenchmark datasets, significantly greater than the 2\\% drop observed when the\nReflection component was removed. These findings strongly demonstrate the\ncritical role of Debate in enhancing framework performance, while also\nhighlighting the auxiliary contribution of reflexion to overall optimization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23781v1",
    "published_date": "2025-03-31 06:56:13 UTC",
    "updated_date": "2025-03-31 06:56:13 UTC"
  },
  {
    "arxiv_id": "2503.23779v1",
    "title": "WinoWhat: A Parallel Corpus of Paraphrased WinoGrande Sentences with Common Sense Categorization",
    "authors": [
      "Ine Gevers",
      "Victor De Marez",
      "Luna De Bruyne",
      "Walter Daelemans"
    ],
    "abstract": "In this study, we take a closer look at how Winograd schema challenges can be\nused to evaluate common sense reasoning in LLMs. Specifically, we evaluate\ngenerative models of different sizes on the popular WinoGrande benchmark. We\nrelease WinoWhat, a new corpus, in which each instance of the WinoGrande\nvalidation set is paraphrased. Additionally, we evaluate the performance on the\nchallenge across five common sense knowledge categories, giving more\nfine-grained insights on what types of knowledge are more challenging for LLMs.\nSurprisingly, all models perform significantly worse on WinoWhat, implying that\nLLM reasoning capabilities are overestimated on WinoGrande. To verify whether\nthis is an effect of benchmark memorization, we match benchmark instances to\nLLM trainingdata and create two test-suites. We observe that memorization has a\nminimal effect on model performance on WinoGrande.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23779v1",
    "published_date": "2025-03-31 06:53:53 UTC",
    "updated_date": "2025-03-31 06:53:53 UTC"
  },
  {
    "arxiv_id": "2503.23764v2",
    "title": "WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation for Efficient Medical Image Segmentation",
    "authors": [
      "Md Mahfuz Al Hasan",
      "Mahdi Zaman",
      "Abdul Jawad",
      "Alberto Santamaria-Pang",
      "Ho Hin Lee",
      "Ivan Tarapov",
      "Kyle See",
      "Md Shah Imran",
      "Antika Roy",
      "Yaser Pourmohammadi Fallah",
      "Navid Asadizanjani",
      "Reza Forghani"
    ],
    "abstract": "Transformer-based architectures have advanced medical image analysis by\neffectively modeling long-range dependencies, yet they often struggle in 3D\nsettings due to substantial memory overhead and insufficient capture of\nfine-grained local features. We address these limitations with WaveFormer, a\nnovel 3D-transformer that: i) leverages the fundamental frequency-domain\nproperties of features for contextual representation, and ii) is inspired by\nthe top-down mechanism of the human visual recognition system, making it a\nbiologically motivated architecture. By employing discrete wavelet\ntransformations (DWT) at multiple scales, WaveFormer preserves both global\ncontext and high-frequency details while replacing heavy upsampling layers with\nefficient wavelet-based summarization and reconstruction. This significantly\nreduces the number of parameters, which is critical for real-world deployment\nwhere computational resources and training times are constrained. Furthermore,\nthe model is generic and easily adaptable to diverse applications. Evaluations\non BraTS2023, FLARE2021, and KiTS2023 demonstrate performance on par with\nstate-of-the-art methods while offering substantially lower computational\ncomplexity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23764v2",
    "published_date": "2025-03-31 06:28:41 UTC",
    "updated_date": "2025-04-01 02:13:23 UTC"
  },
  {
    "arxiv_id": "2503.23740v1",
    "title": "LANID: LLM-assisted New Intent Discovery",
    "authors": [
      "Lu Fan",
      "Jiashu Pu",
      "Rongsheng Zhang",
      "Xiao-Ming Wu"
    ],
    "abstract": "Task-oriented Dialogue Systems (TODS) often face the challenge of\nencountering new intents. New Intent Discovery (NID) is a crucial task that\naims to identify these novel intents while maintaining the capability to\nrecognize existing ones. Previous efforts to adapt TODS to new intents have\nstruggled with inadequate semantic representation or have depended on external\nknowledge, which is often not scalable or flexible. Recently, Large Language\nModels (LLMs) have demonstrated strong zero-shot capabilities; however, their\nscale can be impractical for real-world applications that involve extensive\nqueries. To address the limitations of existing NID methods by leveraging LLMs,\nwe propose LANID, a framework that enhances the semantic representation of\nlightweight NID encoders with the guidance of LLMs. Specifically, LANID employs\nthe $K$-nearest neighbors and Density-Based Spatial Clustering of Applications\nwith Noise (DBSCAN) algorithms to sample selective utterance pairs from the\ntraining set. It then queries an LLM to ascertain the relationships between\nthese pairs. The data produced from this process is utilized to design a\ncontrastive fine-tuning task, which is then used to train a small encoder with\na contrastive triplet loss. Our experimental results demonstrate the efficacy\nof the proposed method across three distinct NID datasets, surpassing strong\nbaselines in both unsupervised and semi-supervised settings. Our code is\navailable at https://github.com/floatSDSDS/LANID.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "comment": "Published in LREC-COLING 2024",
    "pdf_url": "http://arxiv.org/pdf/2503.23740v1",
    "published_date": "2025-03-31 05:34:32 UTC",
    "updated_date": "2025-03-31 05:34:32 UTC"
  },
  {
    "arxiv_id": "2503.23731v1",
    "title": "Investigation of intelligent barbell squat coaching system based on computer vision and machine learning",
    "authors": [
      "Yinq-Rong Chern",
      "Yuhao Lee",
      "Hsiao-Ching Lin",
      "Guan-Ting Chen",
      "Ying-Hsien Chen",
      "Fu-Sung Lin",
      "Chih-Yao Chuang",
      "Jenn-Jier James Lien",
      "Chih-Hsien Huang"
    ],
    "abstract": "Purpose: Research has revealed that strength training can reduce the\nincidence of chronic diseases and physical deterioration at any age. Therefore,\nhaving a movement diagnostic system is crucial for training alone. Hence, this\nstudy developed an artificial intelligence and computer vision-based barbell\nsquat coaching system with a real-time mode that immediately diagnoses the\nissue and provides feedback after each squat. In addition, a replay mode allows\nusers to examine their previous squats and check their comments. Initially,\nfour primary characteristics of the barbell squat were identified: body joint\nangles, dorsiflexion, the ratio of knee-to-hip movement, and barbell stability.\nMethods: We collect 8,151 squats from 77 participants, categorizing them as\ngood squats and six issues. Then, we trained the diagnosis models with three\nmachine-learning architectures. Furthermore, this research applied the SHapley\nAdditive exPlanations (SHAP) method to enhance the accuracy of issue prediction\nand reduce the computation time by feature selection. Results: The F1 score of\nthe six issues reached 86.86%, 69.01%, 77.42%, 90.74%, 95.83%, and 100%. Each\nsquat diagnosis took less than 0.5 seconds. Finally, this study examined the\nefficacy of the proposed system with two groups of participants trained with\nand without the system. Subsequently, participants trained with the system\nexhibited substantial improvements in their squat technique, as assessed both\nby the system itself and by a professional weightlifting coach. Conclusion:\nThis is a comprehensive study that integrates artificial intelligence, computer\nvision and multivariable processing technologies, aimed at building a\nreal-time, user-friendly barbell squat feedback and training system.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23731v1",
    "published_date": "2025-03-31 05:08:52 UTC",
    "updated_date": "2025-03-31 05:08:52 UTC"
  },
  {
    "arxiv_id": "2503.23730v1",
    "title": "KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language",
    "authors": [
      "Yoonshik Kim",
      "Jaeyoon Jung"
    ],
    "abstract": "The recent emergence of Large Vision-Language Models(VLMs) has resulted in a\nvariety of different benchmarks for evaluating such models. Despite this, we\nobserve that most existing evaluation methods suffer from the fact that they\neither require the model to choose from pre-determined responses, sacrificing\nopen-endedness, or evaluate responses using a judge model, resulting in\nsubjective and unreliable evaluation. In addition, we observe a lack of\nbenchmarks for VLMs in the Korean language, which are necessary as a separate\nmetric from more common English language benchmarks, as the performance of\ngenerative language models can differ significantly based on the language being\nused. Therefore, we present KOFFVQA, a general-purpose free-form visual\nquestion answering benchmark in the Korean language for the evaluation of VLMs.\nOur benchmark consists of 275 carefully crafted questions each paired with an\nimage and grading criteria covering 10 different aspects of VLM performance.\nThe grading criteria eliminate the problem of unreliability by allowing the\njudge model to grade each response based on a pre-determined set of rules. By\ndefining the evaluation criteria in an objective manner, even a small\nopen-source model can be used to evaluate models on our benchmark reliably. In\naddition to evaluating a large number of existing VLMs on our benchmark, we\nalso experimentally verify that our method of using pre-existing grading\ncriteria for evaluation is much more reliable than existing methods. Our\nevaluation code is available at https://github.com/maum-ai/KOFFVQA",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "comment": "Accepted to CVPRW 2025, Workshop on Benchmarking and Expanding AI\n  Multimodal Approaches",
    "pdf_url": "http://arxiv.org/pdf/2503.23730v1",
    "published_date": "2025-03-31 05:04:25 UTC",
    "updated_date": "2025-03-31 05:04:25 UTC"
  },
  {
    "arxiv_id": "2503.23721v1",
    "title": "Unimodal-driven Distillation in Multimodal Emotion Recognition with Dynamic Fusion",
    "authors": [
      "Jiagen Li",
      "Rui Yu",
      "Huihao Huang",
      "Huaicheng Yan"
    ],
    "abstract": "Multimodal Emotion Recognition in Conversations (MERC) identifies emotional\nstates across text, audio and video, which is essential for intelligent\ndialogue systems and opinion analysis. Existing methods emphasize heterogeneous\nmodal fusion directly for cross-modal integration, but often suffer from\ndisorientation in multimodal learning due to modal heterogeneity and lack of\ninstructive guidance. In this work, we propose SUMMER, a novel heterogeneous\nmultimodal integration framework leveraging Mixture of Experts with\nHierarchical Cross-modal Fusion and Interactive Knowledge Distillation. Key\ncomponents include a Sparse Dynamic Mixture of Experts (SDMoE) for capturing\ndynamic token-wise interactions, a Hierarchical Cross-Modal Fusion (HCMF) for\neffective fusion of heterogeneous modalities, and Interactive Knowledge\nDistillation (IKD), which uses a pre-trained unimodal teacher to guide\nmultimodal fusion in latent and logit spaces. Experiments on IEMOCAP and MELD\nshow SUMMER outperforms state-of-the-art methods, particularly in recognizing\nminority and semantically similar emotions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23721v1",
    "published_date": "2025-03-31 04:43:10 UTC",
    "updated_date": "2025-03-31 04:43:10 UTC"
  },
  {
    "arxiv_id": "2503.23713v1",
    "title": "GNN-Based Candidate Node Predictor for Influence Maximization in Temporal Graphs",
    "authors": [
      "Priyanka Gautam",
      "Balasubramaniam Natarajan",
      "Sai Munikoti",
      "S M Ferdous",
      "Mahantesh Halappanavar"
    ],
    "abstract": "In an age where information spreads rapidly across social media, effectively\nidentifying influential nodes in dynamic networks is critical. Traditional\ninfluence maximization strategies often fail to keep up with rapidly evolving\nrelationships and structures, leading to missed opportunities and\ninefficiencies. To address this, we propose a novel learning-based approach\nintegrating Graph Neural Networks (GNNs) with Bidirectional Long Short-Term\nMemory (BiLSTM) models. This hybrid framework captures both structural and\ntemporal dynamics, enabling accurate prediction of candidate nodes for seed set\nselection. The bidirectional nature of BiLSTM allows our model to analyze\npatterns from both past and future network states, ensuring adaptability to\nchanges over time. By dynamically adapting to graph evolution at each time\nsnapshot, our approach improves seed set calculation efficiency, achieving an\naverage of 90% accuracy in predicting potential seed nodes across diverse\nnetworks. This significantly reduces computational overhead by optimizing the\nnumber of nodes evaluated for seed selection. Our method is particularly\neffective in fields like viral marketing and social network analysis, where\nunderstanding temporal dynamics is crucial.",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "primary_category": "cs.SI",
    "comment": "9 pages, 5 figures, Accepted in AAAI25 to AI4TS Workshop@AAAI 2025",
    "pdf_url": "http://arxiv.org/pdf/2503.23713v1",
    "published_date": "2025-03-31 04:28:37 UTC",
    "updated_date": "2025-03-31 04:28:37 UTC"
  },
  {
    "arxiv_id": "2503.23708v1",
    "title": "Towards Benchmarking and Assessing the Safety and Robustness of Autonomous Driving on Safety-critical Scenarios",
    "authors": [
      "Jingzheng Li",
      "Xianglong Liu",
      "Shikui Wei",
      "Zhijun Chen",
      "Bing Li",
      "Qing Guo",
      "Xianqi Yang",
      "Yanjun Pu",
      "Jiakai Wang"
    ],
    "abstract": "Autonomous driving has made significant progress in both academia and\nindustry, including performance improvements in perception task and the\ndevelopment of end-to-end autonomous driving systems. However, the safety and\nrobustness assessment of autonomous driving has not received sufficient\nattention. Current evaluations of autonomous driving are typically conducted in\nnatural driving scenarios. However, many accidents often occur in edge cases,\nalso known as safety-critical scenarios. These safety-critical scenarios are\ndifficult to collect, and there is currently no clear definition of what\nconstitutes a safety-critical scenario. In this work, we explore the safety and\nrobustness of autonomous driving in safety-critical scenarios. First, we\nprovide a definition of safety-critical scenarios, including static traffic\nscenarios such as adversarial attack scenarios and natural distribution shifts,\nas well as dynamic traffic scenarios such as accident scenarios. Then, we\ndevelop an autonomous driving safety testing platform to comprehensively\nevaluate autonomous driving systems, encompassing not only the assessment of\nperception modules but also system-level evaluations. Our work systematically\nconstructs a safety verification process for autonomous driving, providing\ntechnical support for the industry to establish standardized test framework and\nreduce risks in real-world road deployment.",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "primary_category": "cs.RO",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23708v1",
    "published_date": "2025-03-31 04:13:32 UTC",
    "updated_date": "2025-03-31 04:13:32 UTC"
  },
  {
    "arxiv_id": "2503.23668v2",
    "title": "MolGround: A Benchmark for Molecular Grounding",
    "authors": [
      "Jiaxin Wu",
      "Ting Zhang",
      "Rubing Chen",
      "Wengyu Zhang",
      "Chen Jason Zhang",
      "Xiaoyong Wei",
      "Li Qing"
    ],
    "abstract": "Current molecular understanding approaches predominantly focus on the\ndescriptive aspect of human perception, providing broad, topic-level insights.\nHowever, the referential aspect -- linking molecular concepts to specific\nstructural components -- remains largely unexplored. To address this gap, we\npropose a molecular grounding benchmark designed to evaluate a model's\nreferential abilities. We align molecular grounding with established\nconventions in NLP, cheminformatics, and molecular science, showcasing the\npotential of NLP techniques to advance molecular understanding within the AI\nfor Science movement. Furthermore, we constructed the largest molecular\nunderstanding benchmark to date, comprising 79k QA pairs, and developed a\nmulti-agent grounding prototype as proof of concept. This system outperforms\nexisting models, including GPT-4o, and its grounding outputs have been\nintegrated to enhance traditional tasks such as molecular captioning and ATC\n(Anatomical, Therapeutic, Chemical) classification.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23668v2",
    "published_date": "2025-03-31 02:23:16 UTC",
    "updated_date": "2025-04-01 06:49:07 UTC"
  },
  {
    "arxiv_id": "2503.23641v1",
    "title": "Remarks on the Polyak-Lojasiewicz inequality and the convergence of gradient systems",
    "authors": [
      "Arthur Castello B. de Oliveira",
      "Leilei Cui",
      "Eduardo D. Sontag"
    ],
    "abstract": "This work explores generalizations of the Polyak-Lojasiewicz inequality (PLI)\nand their implications for the convergence behavior of gradient flows in\noptimization problems. Motivated by the continuous-time linear quadratic\nregulator (CT-LQR) policy optimization problem -- where only a weaker version\nof the PLI is characterized in the literature -- this work shows that while\nweaker conditions are sufficient for global convergence to, and optimality of\nthe set of critical points of the cost function, the \"profile\" of the gradient\nflow solution can change significantly depending on which \"flavor\" of\ninequality the cost satisfies. After a general theoretical analysis, we focus\non fitting the CT-LQR policy optimization problem to the proposed framework,\nshowing that, in fact, it can never satisfy a PLI in its strongest form. We\nfollow up our analysis with a brief discussion on the difference between\ncontinuous- and discrete-time LQR policy optimization, and end the paper with\nsome intuition on the extension of this framework to optimization problems with\nL1 regularization and solved through proximal gradient flows.",
    "categories": [
      "math.OC",
      "cs.AI",
      "cs.SY",
      "eess.SY"
    ],
    "primary_category": "math.OC",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23641v1",
    "published_date": "2025-03-31 00:59:56 UTC",
    "updated_date": "2025-03-31 00:59:56 UTC"
  },
  {
    "arxiv_id": "2503.23633v1",
    "title": "GIScience in the Era of Artificial Intelligence: A Research Agenda Towards Autonomous GIS",
    "authors": [
      "Zhenlong Li",
      "Huan Ning",
      "Song Gao",
      "Krzysztof Janowicz",
      "Wenwen Li",
      "Samantha T. Arundel",
      "Chaowei Yang",
      "Budhendra Bhaduri",
      "Shaowen Wang",
      "A-Xing Zhu",
      "Mark Gahegan",
      "Shashi Shekhar",
      "Xinyue Ye",
      "Grant McKenzie",
      "Guido Cervone",
      "Michael E. Hodgson"
    ],
    "abstract": "The advent of generative AI exemplified by large language models (LLMs) opens\nnew ways to represent and compute geographic information and transcend the\nprocess of geographic knowledge production, driving geographic information\nsystems (GIS) towards autonomous GIS. Leveraging LLMs as the decision core,\nautonomous GIS can independently generate and execute geoprocessing workflows\nto perform spatial analysis. In this vision paper, we elaborate on the concept\nof autonomous GIS and present a framework that defines its five autonomous\ngoals, five levels of autonomy, five core functions, and three operational\nscales. We demonstrate how autonomous GIS could perform geospatial data\nretrieval, spatial analysis, and map making with four proof-of-concept GIS\nagents. We conclude by identifying critical challenges and future research\ndirections, including fine-tuning and self-growing decision cores, autonomous\nmodeling, and examining the ethical and practical implications of autonomous\nGIS. By establishing the groundwork for a paradigm shift in GIScience, this\npaper envisions a future where GIS moves beyond traditional workflows to\nautonomously reason, derive, innovate, and advance solutions to pressing global\nchallenges.",
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23633v1",
    "published_date": "2025-03-31 00:12:48 UTC",
    "updated_date": "2025-03-31 00:12:48 UTC"
  },
  {
    "arxiv_id": "2503.23631v1",
    "title": "Intrinsically-Motivated Humans and Agents in Open-World Exploration",
    "authors": [
      "Aly Lidayan",
      "Yuqing Du",
      "Eliza Kosoy",
      "Maria Rufova",
      "Pieter Abbeel",
      "Alison Gopnik"
    ],
    "abstract": "What drives exploration? Understanding intrinsic motivation is a\nlong-standing challenge in both cognitive science and artificial intelligence;\nnumerous objectives have been proposed and used to train agents, yet there\nremains a gap between human and agent exploration. We directly compare adults,\nchildren, and AI agents in a complex open-ended environment, Crafter, and study\nhow common intrinsic objectives: Entropy, Information Gain, and Empowerment,\nrelate to their behavior. We find that only Entropy and Empowerment are\nconsistently positively correlated with human exploration progress, indicating\nthat these objectives may better inform intrinsic reward design for agents.\nFurthermore, across agents and humans we observe that Entropy initially\nincreases rapidly, then plateaus, while Empowerment increases continuously,\nsuggesting that state diversity may provide more signal in early exploration,\nwhile advanced exploration should prioritize control. Finally, we find\npreliminary evidence that private speech utterances, and particularly goal\nverbalizations, may aid exploration in children.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "comment": "",
    "pdf_url": "http://arxiv.org/pdf/2503.23631v1",
    "published_date": "2025-03-31 00:09:00 UTC",
    "updated_date": "2025-03-31 00:09:00 UTC"
  },
  {
    "arxiv_id": "2503.23630v1",
    "title": "Finding Interest Needle in Popularity Haystack: Improving Retrieval by Modeling Item Exposure",
    "authors": [
      "Amit Jaspal",
      "Rahul Agarwal"
    ],
    "abstract": "Recommender systems operate in closed feedback loops, where user interactions\nreinforce popularity bias, leading to over-recommendation of already popular\nitems while under-exposing niche or novel content. Existing bias mitigation\nmethods, such as Inverse Propensity Scoring (IPS) and Off- Policy Correction\n(OPC), primarily operate at the ranking stage or during training, lacking\nexplicit real-time control over exposure dynamics. In this work, we introduce\nan exposure- aware retrieval scoring approach, which explicitly models item\nexposure probability and adjusts retrieval-stage ranking at inference time.\nUnlike prior work, this method decouples exposure effects from engagement\nlikelihood, enabling controlled trade-offs between fairness and engagement in\nlarge-scale recommendation platforms. We validate our approach through online\nA/B experiments in a real-world video recommendation system, demonstrating a\n25% increase in uniquely retrieved items and a 40% reduction in the dominance\nof over-popular content, all while maintaining overall user engagement levels.\nOur results establish a scalable, deployable solution for mitigating popularity\nbias at the retrieval stage, offering a new paradigm for bias-aware\npersonalization.",
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "primary_category": "cs.IR",
    "comment": "2 pages",
    "pdf_url": "http://arxiv.org/pdf/2503.23630v1",
    "published_date": "2025-03-31 00:04:01 UTC",
    "updated_date": "2025-03-31 00:04:01 UTC"
  }
]